,labels,text,predicates,subj/obj,triple_A,triple_B,triple_C,triple_D,topic,paper_idx,idx
0,research-problem,Recurrent Neural Network Grammars,[],[],[],[],[],[],constituency_parsing,0,2
1,model,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .","[('introduce', (5, 6)), ('of', (21, 22)), ('explicitly models', (24, 26)), ('among', (30, 31))]","[('recurrent neural network grammars ( RNNGs', (6, 12)), ('sentences', (22, 23)), ('nested , hierarchical relationships', (26, 30)), ('words and phrases', (31, 34))]","[['sentences', 'explicitly models', 'nested , hierarchical relationships'], ['nested , hierarchical relationships', 'among', 'words and phrases']]",[],"[['Model', 'introduce', 'recurrent neural network grammars ( RNNGs']]",[],constituency_parsing,0,12
2,model,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .","[('give', (1, 2)), ('of', (4, 5)), ('one for', (8, 10))]","[('two variants', (2, 4)), ('algorithm', (6, 7)), ('parsing', (10, 11)), ('generation', (27, 28))]","[['two variants', 'of', 'algorithm'], ['two variants', 'one for', 'parsing'], ['algorithm', 'one for', 'parsing']]","[['two variants', 'has', 'algorithm']]","[['Model', 'give', 'two variants']]",[],constituency_parsing,0,15
3,model,"Similar to previously published discriminative bottomup transition - based parsers , greedy prediction with our model yields a linear -","[('with', (13, 14)), ('yields', (16, 17))]","[('greedy prediction', (11, 13)), ('our', (14, 15))]","[['greedy prediction', 'with', 'our']]",[],[],"[['Model', 'has', 'greedy prediction']]",constituency_parsing,0,20
4,model,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,"[('present', (1, 2)), ('which uses', (7, 9)), ('from', (10, 11)), ('to solve', (14, 16)), ('in', (18, 19))]","[('simple importance sampling algorithm', (3, 7)), ('samples', (9, 10)), ('discriminative parser', (12, 14)), ('inference problems', (16, 18)), ('generative model', (20, 22))]","[['simple importance sampling algorithm', 'which uses', 'samples'], ['samples', 'from', 'discriminative parser'], ['samples', 'to solve', 'inference problems'], ['discriminative parser', 'to solve', 'inference problems'], ['inference problems', 'in', 'generative model']]",[],"[['Model', 'present', 'simple importance sampling algorithm']]",[],constituency_parsing,0,25
5,research-problem,Transition Sequences from Trees,[],[],[],[],[],[],constituency_parsing,0,85
6,hyperparameters,For training we used stochastic gradient descent with a learning rate of 0.1 .,"[('used', (3, 4)), ('with', (7, 8)), ('of', (11, 12))]","[('training', (1, 2)), ('stochastic gradient descent', (4, 7)), ('learning rate', (9, 11)), ('0.1', (12, 13))]","[['training', 'used', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'learning rate'], ['learning rate', 'of', '0.1']]",[],[],[],constituency_parsing,0,164
7,results,"Despite slightly higher language modeling perplexity on PTB 23 , the fixed RNNG still outperforms a highly optimized sequential LSTM baseline . :",[],"[('fixed RNNG', (11, 13)), ('outperforms', (14, 15)), ('highly optimized sequential LSTM baseline', (16, 21))]",[],"[['fixed RNNG', 'has', 'outperforms'], ['outperforms', 'has', 'highly optimized sequential LSTM baseline']]",[],[],constituency_parsing,0,222
8,research-problem,Cloze - driven Pretraining of Self - attention Networks,[],[],[],[],[],[],constituency_parsing,1,2
9,research-problem,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .,[],"[('pretraining', (6, 7))]",[],[],[],[],constituency_parsing,1,4
10,approach,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .","[('show', (5, 6)), ('of', (18, 19))]","[('even larger performance gains', (7, 11)), ('both directions', (16, 18)), ('large language - model - inspired self - attention cloze model', (20, 31))]","[['both directions', 'of', 'large language - model - inspired self - attention cloze model']]",[],"[['Approach', 'show', 'even larger performance gains']]",[],constituency_parsing,1,11
11,approach,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,"[('introducing', (4, 5)), ('where', (11, 12)), ('must predict', (14, 16)), ('given', (19, 20))]","[('cloze - style training objective', (6, 11)), ('model', (13, 14)), ('center word', (17, 19)), ('left - to - right and right - to - left context representations', (20, 33))]","[['cloze - style training objective', 'where', 'model'], ['model', 'must predict', 'center word'], ['center word', 'given', 'left - to - right and right - to - left context representations']]","[['cloze - style training objective', 'has', 'model']]","[['Approach', 'introducing', 'cloze - style training objective']]",[],constituency_parsing,1,13
12,experimental-setup,We subsample up to 18B tokens .,"[('subsample', (1, 2))]","[('up to 18B tokens', (2, 6))]",[],[],"[['Experimental setup', 'subsample', 'up to 18B tokens']]",[],constituency_parsing,1,117
13,experimental-setup,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .","[('use', (2, 3)), ('in', (6, 7)), ('contains', (12, 13)), ('with', (18, 19)), ('followed by', (22, 24)), ('with', (28, 29)), ('with', (32, 33)), ('if', (41, 42)), ('exceeds', (44, 45))]","[('CNN', (0, 1)), ('adaptive softmax', (4, 6)), ('output', (8, 9)), ('headband', (11, 12)), ('60K most frequent types', (14, 18)), ('160 K band', (25, 28)), ('dimensionality 256', (29, 31)), ('momentum', (34, 35)), ('0.99', (36, 37)), ('renormalize', (39, 40)), ('gradients', (40, 41)), ('norm', (43, 44)), ('0.1', (45, 46))]","[['CNN', 'use', 'adaptive softmax'], ['adaptive softmax', 'in', 'output'], ['headband', 'contains', '60K most frequent types'], ['160 K band', 'with', 'dimensionality 256'], ['160 K band', 'with', 'momentum'], ['gradients', 'if', 'norm'], ['norm', 'exceeds', '0.1']]","[['momentum', 'has', '0.99'], ['renormalize', 'has', 'gradients']]",[],"[['Experimental setup', 'has', 'CNN']]",constituency_parsing,1,128
14,experimental-setup,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,"[('from', (7, 8)), ('for', (13, 14)), ('annealed using', (19, 21)), ('with', (26, 27)), ('to', (30, 31))]","[('learning rate', (1, 3)), ('linearly warmed up', (4, 7)), ('10 ? 7 to 1', (8, 13)), ('16 K steps', (14, 17)), ('cosine learning rate schedule', (22, 26)), ('single phase', (28, 30)), ('0.0001', (31, 32))]","[['linearly warmed up', 'from', '10 ? 7 to 1'], ['10 ? 7 to 1', 'for', '16 K steps'], ['learning rate', 'annealed using', 'cosine learning rate schedule'], ['cosine learning rate schedule', 'with', 'single phase'], ['single phase', 'to', '0.0001']]","[['learning rate', 'has', 'linearly warmed up']]",[],"[['Experimental setup', 'has', 'learning rate']]",constituency_parsing,1,129
15,experimental-setup,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,"[('on', (3, 4)), ('with', (8, 9)), ('interconnected by', (16, 18))]","[('DGX - 1 machines', (4, 8)), ('8 NVIDIA V100 GPUs', (9, 13)), ('Infiniband', (18, 19))]","[['DGX - 1 machines', 'with', '8 NVIDIA V100 GPUs']]",[],"[['Experimental setup', 'on', 'DGX - 1 machines']]",[],constituency_parsing,1,130
16,experimental-setup,We also use the NCCL2 library and the torch .,"[('use', (2, 3))]","[('NCCL2 library', (4, 6)), ('torch', (8, 9))]",[],[],"[['Experimental setup', 'use', 'NCCL2 library']]",[],constituency_parsing,1,131
17,experimental-setup,"We train models with 16 bit floating point precision , following .","[('train', (1, 2)), ('with', (3, 4))]","[('models', (2, 3)), ('16 bit floating point precision', (4, 9))]","[['models', 'with', '16 bit floating point precision']]",[],"[['Experimental setup', 'train', 'models']]",[],constituency_parsing,1,133
18,results,"All our models outperform the uni-directional transformer ( OpenAI GPT ) of , however , our model is about 50 % larger than their model .","[('than', (22, 23))]","[('outperform', (3, 4)), ('uni-directional transformer ( OpenAI GPT )', (5, 11)), ('our model', (15, 17)), ('about 50 % larger', (18, 22))]",[],"[['outperform', 'has', 'uni-directional transformer ( OpenAI GPT )']]",[],[],constituency_parsing,1,151
19,results,"Our CNN base model performs as well as STILTs in aggregate , however , on some tasks involving sentence - pairs , STILTs performs much better ( MRPC , RTE ) ; there is a similar trend for BERT .","[('performs as', (4, 6)), ('in', (9, 10)), ('performs', (23, 24))]","[('Our CNN base model', (0, 4)), ('STILTs', (8, 9)), ('aggregate', (10, 11)), ('much', (24, 25))]","[['STILTs', 'in', 'aggregate']]",[],[],"[['Results', 'has', 'Our CNN base model']]",constituency_parsing,1,153
20,results,Structured Prediction,[],[],[],[],[],[],constituency_parsing,1,159
21,research-problem,An Empirical Study of Building a Strong Baseline for Constituency Parsing,[],[],[],[],[],[],constituency_parsing,2,2
22,research-problem,This paper investigates the construction of a strong baseline based on general purpose sequence - to - sequence models for constituency parsing .,"[('investigates', (2, 3)), ('based on', (9, 11)), ('for', (19, 20))]","[('general purpose sequence - to - sequence models', (11, 19)), ('constituency parsing', (20, 22))]","[['general purpose sequence - to - sequence models', 'for', 'constituency parsing']]",[],"[['Research problem', 'investigates', 'general purpose sequence - to - sequence models']]",[],constituency_parsing,2,4
23,research-problem,"We incorporate several techniques that were mainly developed in natural language generation tasks , e.g. , machine translation and summarization , and demonstrate that the sequenceto - sequence model achieves the current top - notch parsers ' performance without requiring explicit task - specific knowledge or architecture of constituent parsing .","[('incorporate', (1, 2)), ('demonstrate', (22, 23))]","[('sequenceto - sequence model', (25, 29))]",[],[],"[['Research problem', 'incorporate', 'sequenceto - sequence model']]",[],constituency_parsing,2,5
24,research-problem,"Sequence - to - sequence ( Seq2seq ) models have successfully improved many well - studied NLP tasks , especially for natural language generation ( NLG ) tasks , such as machine translation ( MT ) and abstractive summarization .",[],"[('Sequence - to - sequence ( Seq2seq )', (0, 8))]",[],[],[],[],constituency_parsing,2,7
25,model,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,"[('of', (14, 15))]","[('Seq2seq approach', (6, 8)), ('constituency parsing', (15, 17))]",[],[],[],"[['Model', 'has', 'Seq2seq approach']]",constituency_parsing,2,13
26,research-problem,Constituency Parsing by Seq2seq,[],"[('Constituency Parsing', (0, 2))]",[],[],[],[],constituency_parsing,2,19
27,experiments,Model ensemble,[],[],[],[],[],[],constituency_parsing,2,83
28,experiments,Ensembling several independently trained models together significantly improves many NLP tasks .,"[('Ensembling', (0, 1))]","[('several independently trained models together', (1, 6)), ('significantly improves', (6, 8))]",[],"[['several independently trained models together', 'has', 'significantly improves']]",[],[],constituency_parsing,2,84
29,experiments,Language model ( LM ) reranking,[],[],[],[],[],[],constituency_parsing,2,87
30,hyperparameters,"For data pre-processing , all the parse trees were transformed into linearized forms , which include standard UNK replacement for OOV words and POS - tag normalization by XX - tags .","[('For', (0, 1)), ('transformed into', (9, 11)), ('which', (14, 15)), ('include', (15, 16)), ('for', (19, 20)), ('by', (27, 28))]","[('data pre-processing', (1, 3)), ('all the parse trees', (4, 8)), ('linearized forms', (11, 13)), ('standard UNK replacement', (16, 19)), ('OOV words', (20, 22)), ('POS - tag normalization', (23, 27)), ('XX - tags', (28, 31))]","[['standard UNK replacement', 'For', 'OOV words'], ['all the parse trees', 'transformed into', 'linearized forms'], ['linearized forms', 'which', 'POS - tag normalization'], ['linearized forms', 'include', 'standard UNK replacement'], ['linearized forms', 'include', 'POS - tag normalization'], ['standard UNK replacement', 'for', 'OOV words'], ['POS - tag normalization', 'by', 'XX - tags']]","[['data pre-processing', 'has', 'all the parse trees']]","[['Hyperparameters', 'For', 'data pre-processing']]",[],constituency_parsing,2,95
31,results,Ensembling and Reranking : shows the results of our models with model ensembling and LM- reranking .,[],"[('Ensembling and Reranking', (0, 3))]",[],[],[],"[['Results', 'has', 'Ensembling and Reranking']]",constituency_parsing,2,103
32,results,( 1 ) Smaller mini-batch size M and gradient clipping G provided the better performance .,"[('provided', (11, 12))]","[('Smaller mini-batch size M and gradient clipping G', (3, 11)), ('better performance', (13, 15))]","[['Smaller mini-batch size M and gradient clipping G', 'provided', 'better performance']]",[],[],"[['Results', 'has', 'Smaller mini-batch size M and gradient clipping G']]",constituency_parsing,2,111
33,ablation-analysis,"( 2 ) Larger layer size , hidden state dimension , and beam size have little impact on the performance ; our setting , L = 2 , H = 200 , and B = 5 looks adequate in terms of speed / performance trade - off .","[('have', (14, 15)), ('on', (17, 18)), ('looks', (36, 37)), ('in terms of', (38, 41))]","[('Larger layer size', (3, 6)), ('little impact', (15, 17)), ('performance', (19, 20)), ('our', (21, 22)), ('adequate', (37, 38)), ('speed / performance trade - off', (41, 47))]","[['little impact', 'on', 'performance'], ['adequate', 'in terms of', 'speed / performance trade - off']]","[['Larger layer size', 'has', 'little impact'], ['performance', 'has', 'our'], ['our', 'has', 'adequate']]",[],"[['Ablation analysis', 'has', 'Larger layer size']]",constituency_parsing,2,113
34,results,( e ) shows the results of utilizing subword splits .,"[('of', (6, 7))]","[('results', (5, 6)), ('utilizing subword splits', (7, 10))]","[['results', 'of', 'utilizing subword splits']]","[['results', 'has', 'utilizing subword splits']]",[],"[['Results', 'has', 'results']]",constituency_parsing,2,116
35,results,"Thus , using subword information as features is one promising approach for leveraging subword information into constituency parsing .","[('using', (2, 3)), ('as', (5, 6)), ('is', (7, 8)), ('for leveraging', (11, 13)), ('into', (15, 16))]","[('subword information', (3, 5)), ('features', (6, 7)), ('promising approach', (9, 11)), ('subword information', (13, 15)), ('constituency parsing', (16, 18))]","[['subword information', 'as', 'features'], ['promising approach', 'for leveraging', 'subword information'], ['subword information', 'into', 'constituency parsing']]","[['subword information', 'has', 'features']]","[['Results', 'using', 'subword information']]",[],constituency_parsing,2,119
36,results,Our Seq2seq approach successfully achieved the competitive level as the current top - notch methods : RNNG and its variants .,"[('successfully achieved', (3, 5)), ('as', (8, 9))]","[('Our Seq2seq approach', (0, 3)), ('competitive level', (6, 8)), ('current', (10, 11)), ('RNNG', (16, 17))]","[['Our Seq2seq approach', 'successfully achieved', 'competitive level'], ['competitive level', 'as', 'current']]","[['current', 'name', 'RNNG']]",[],"[['Results', 'has', 'Our Seq2seq approach']]",constituency_parsing,2,126
37,research-problem,Grammar as a Foreign Language,[],[],[],[],[],[],constituency_parsing,3,2
38,research-problem,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .,[],"[('Syntactic constituency parsing', (0, 3))]",[],[],[],[],constituency_parsing,3,4
39,results,But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,"[('gets to', (5, 7)), ('achieves', (17, 18)), ('matching', (19, 20)), ('on', (25, 26))]","[('single attention model', (2, 5)), ('88.3', (7, 8)), ('ensemble of 5 LSTM', (10, 14)), ('90.5', (18, 19)), ('single - model BerkeleyParser', (21, 25))]","[['single attention model', 'gets to', '88.3'], ['ensemble of 5 LSTM', 'achieves', '90.5'], ['90.5', 'matching', 'single - model BerkeleyParser']]","[['single attention model', 'has', '88.3']]",[],[],constituency_parsing,3,118
40,results,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .","[('trained on', (1, 3)), ('achieves', (16, 17))]","[('large high - confidence corpus', (4, 9)), ('single LSTM + A model', (11, 16)), ('92.5', (17, 18)), ('outperforms', (20, 21)), ('best single model', (24, 27)), ('best ensemble result', (31, 34))]","[['single LSTM + A model', 'achieves', '92.5']]","[['large high - confidence corpus', 'has', 'single LSTM + A model'], ['outperforms', 'has', 'best single model']]","[['Results', 'trained on', 'large high - confidence corpus']]",[],constituency_parsing,3,119
41,results,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,"[('further improves', (7, 9)), ('to', (11, 12))]","[('ensemble of 5 LSTM+ A models', (1, 7)), ('score', (10, 11)), ('92.8', (12, 13))]","[['ensemble of 5 LSTM+ A models', 'further improves', 'score'], ['score', 'to', '92.8']]","[['ensemble of 5 LSTM+ A models', 'has', 'score']]",[],[],constituency_parsing,3,120
42,results,"The difference between the F 1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser , 1.7 for the baseline LSTM , and 0.7 for LSTM + A .","[('between', (2, 3)), ('on', (7, 8)), ('of length', (9, 11)), ('upto', (11, 12)), ('upto', (15, 16)), ('is', (17, 18)), ('for', (19, 20))]","[('difference', (1, 2)), ('F 1 score', (4, 7)), ('sentences', (8, 9)), ('30', (12, 13)), ('70', (16, 17)), ('1.3', (18, 19)), ('BerkeleyParser', (21, 22)), ('1.7', (23, 24)), ('baseline LSTM', (26, 28)), ('0.7', (30, 31)), ('LSTM + A', (32, 35))]","[['difference', 'between', 'F 1 score'], ['F 1 score', 'on', 'sentences'], ['F 1 score', 'upto', '70'], ['70', 'is', '1.3'], ['1.3', 'for', 'BerkeleyParser'], ['1.3', 'for', 'baseline LSTM'], ['1.3', 'for', '0.7']]",[],[],"[['Results', 'has', 'difference']]",constituency_parsing,3,130
43,results,"Surprisingly , LSTM +A shows less degradation with length than BerkeleyParser - a full O ( n 3 ) chart parser that uses a lot more memory .","[('shows', (4, 5)), ('with', (7, 8)), ('than', (9, 10))]","[('LSTM +A', (2, 4)), ('less degradation', (5, 7)), ('length', (8, 9)), ('BerkeleyParser', (10, 11))]","[['LSTM +A', 'shows', 'less degradation'], ['less degradation', 'with', 'length'], ['length', 'than', 'BerkeleyParser']]","[['LSTM +A', 'has', 'less degradation']]",[],"[['Results', 'has', 'LSTM +A']]",constituency_parsing,3,132
44,experiments,"As described in the previous section , we initialized the word - vector embedding with pre-trained word vectors obtained from word2 vec .","[('initialized', (8, 9)), ('with', (14, 15)), ('obtained from', (18, 20))]","[('word - vector embedding', (10, 14)), ('pre-trained word vectors', (15, 18)), ('word2 vec', (20, 22))]","[['word - vector embedding', 'with', 'pre-trained word vectors'], ['pre-trained word vectors', 'obtained from', 'word2 vec']]",[],[],[],constituency_parsing,3,145
45,results,LSTM + A trained on the high - confidence corpus ( which only includes text from news ) achieved an F 1 score of 95.7 on QTB and 84.6 on WEB .,"[('trained on', (3, 5)), ('achieved', (18, 19)), ('of', (23, 24)), ('on', (25, 26)), ('on', (29, 30))]","[('LSTM + A', (0, 3)), ('high - confidence corpus', (6, 10)), ('F 1 score', (20, 23)), ('95.7', (24, 25)), ('QTB', (26, 27)), ('84.6', (28, 29)), ('WEB', (30, 31))]","[['LSTM + A', 'trained on', 'high - confidence corpus'], ['high - confidence corpus', 'achieved', 'F 1 score'], ['F 1 score', 'of', '95.7'], ['F 1 score', 'of', '84.6'], ['95.7', 'on', 'QTB'], ['95.7', 'on', '84.6'], ['84.6', 'on', 'WEB']]",[],[],"[['Results', 'has', 'LSTM + A']]",constituency_parsing,3,157
46,results,Our score on WEB is higher both than the best score reported in ( 83.5 ) and the best score we achieved with an in - house reimplementation of Berkeley Parser trained on human - annotated data ( 84.4 ) .,"[('on', (2, 3)), ('higher both than', (5, 8)), ('achieved', (21, 22)), ('of', (28, 29)), ('trained on', (31, 33))]","[('Our score', (0, 2)), ('WEB', (3, 4)), ('best score', (9, 11)), ('( 83.5 )', (13, 16)), ('best score', (18, 20)), ('in - house reimplementation', (24, 28)), ('Berkeley Parser', (29, 31)), ('human - annotated data ( 84.4 )', (33, 40))]","[['Our score', 'on', 'WEB'], ['Our score', 'higher both than', 'best score'], ['in - house reimplementation', 'of', 'Berkeley Parser'], ['Berkeley Parser', 'trained on', 'human - annotated data ( 84.4 )']]",[],[],[],constituency_parsing,3,158
47,results,We managed to achieve a slightly higher score ( 84.8 ) with the in - house Berkeley Parser trained on a large corpus .,"[('achieve', (3, 4)), ('with', (11, 12)), ('trained on', (18, 20))]","[('slightly higher score ( 84.8 )', (5, 11)), ('in - house Berkeley Parser', (13, 18)), ('large corpus', (21, 23))]","[['slightly higher score ( 84.8 )', 'with', 'in - house Berkeley Parser'], ['in - house Berkeley Parser', 'trained on', 'large corpus']]",[],"[['Results', 'achieve', 'slightly higher score ( 84.8 )']]",[],constituency_parsing,3,159
48,results,"On QTB , the 95.7 score of LSTM + A is also lower than the best score of our in - house BerkeleyParser ( 96.2 ) .","[('On', (0, 1)), ('of', (6, 7)), ('lower than', (12, 14)), ('of', (17, 18))]","[('QTB', (1, 2)), ('95.7 score', (4, 6)), ('LSTM + A', (7, 10)), ('best score', (15, 17)), ('our', (18, 19)), ('in - house BerkeleyParser ( 96.2 )', (19, 26))]","[['95.7 score', 'of', 'LSTM + A'], ['best score', 'of', 'our'], ['best score', 'of', 'in - house BerkeleyParser ( 96.2 )'], ['LSTM + A', 'lower than', 'best score'], ['best score', 'of', 'our'], ['best score', 'of', 'in - house BerkeleyParser ( 96.2 )']]","[['QTB', 'has', '95.7 score'], ['our', 'has', 'in - house BerkeleyParser ( 96.2 )']]","[['Results', 'On', 'QTB']]",[],constituency_parsing,3,160
49,research-problem,Improving Neural Parsing by Disentangling Model Combination and Reranking Effects,[],"[('Neural Parsing', (1, 3))]",[],[],[],[],constituency_parsing,4,2
50,hyperparameters,"For the LSTM generative model ( LM ) , we use the pre-trained model released by Choe and .","[('For', (0, 1)), ('use', (10, 11))]","[('LSTM generative model ( LM )', (2, 8)), ('pre-trained model', (12, 14))]","[['LSTM generative model ( LM )', 'use', 'pre-trained model']]",[],"[['Hyperparameters', 'For', 'LSTM generative model ( LM )']]",[],constituency_parsing,4,80
51,hyperparameters,We use actionsynchronous beam search ( Section 2.1 ) with beam size K = 100 for RD and word - synchronous beam ( Section 2.2 ) with K w = 100 and K a = 1000 for the generative models RG and LM .,"[('use', (1, 2)), ('with', (9, 10)), ('for', (15, 16)), ('with', (26, 27)), ('for', (36, 37))]","[('actionsynchronous beam search', (2, 5)), ('beam size K = 100', (10, 15)), ('RD', (16, 17)), ('word - synchronous beam', (18, 22)), ('K w', (27, 29)), ('generative models', (38, 40)), ('RG and LM', (40, 43))]","[['actionsynchronous beam search', 'with', 'beam size K = 100'], ['actionsynchronous beam search', 'with', 'word - synchronous beam'], ['word - synchronous beam', 'with', 'K w'], ['beam size K = 100', 'for', 'RD'], ['word - synchronous beam', 'with', 'K w']]","[['generative models', 'has', 'RG and LM']]","[['Hyperparameters', 'use', 'actionsynchronous beam search']]",[],constituency_parsing,4,84
52,results,"In comparison , we found higher performance for the LM model when using a candidate list from the RD parser : 93.66 F1 versus 92.79 F1 on the development data .","[('found', (4, 5)), ('for', (7, 8)), ('when using', (11, 13)), ('from', (16, 17)), ('versus', (23, 24)), ('on', (26, 27))]","[('higher performance', (5, 7)), ('LM model', (9, 11)), ('candidate list', (14, 16)), ('RD parser', (18, 20)), ('93.66 F1', (21, 23)), ('92.79 F1', (24, 26)), ('development data', (28, 30))]","[['higher performance', 'for', 'LM model'], ['higher performance', 'when using', 'candidate list'], ['LM model', 'when using', 'candidate list'], ['candidate list', 'from', 'RD parser'], ['candidate list', 'versus', '92.79 F1'], ['93.66 F1', 'versus', '92.79 F1'], ['92.79 F1', 'on', 'development data']]","[['RD parser', 'has', '93.66 F1']]","[['Results', 'found', 'higher performance']]",[],constituency_parsing,4,88
53,results,"We find that combining the scores of both models improves on using the score of either model alone , regardless of the source of candidates .","[('combining', (3, 4)), ('of', (6, 7)), ('of', (14, 15))]","[('scores', (5, 6)), ('both models', (7, 9)), ('improves', (9, 10)), ('using', (11, 12)), ('score', (13, 14)), ('either model alone', (15, 18))]","[['scores', 'of', 'both models'], ['score', 'of', 'either model alone'], ['score', 'of', 'either model alone']]","[['using', 'has', 'score']]","[['Results', 'combining', 'scores']]",[],constituency_parsing,4,111
54,results,Score combination also more than compensates for the decrease in performance we saw previously when adding in candidates from the generative model : RD ?,"[('more than compensates for', (3, 7)), ('when', (14, 15)), ('adding in', (15, 17)), ('from', (18, 19))]","[('Score combination', (0, 2)), ('decrease in', (8, 10)), ('performance', (10, 11)), ('candidates', (17, 18)), ('generative model', (20, 22)), ('RD', (23, 24))]","[['Score combination', 'more than compensates for', 'decrease in'], ['candidates', 'from', 'generative model']]","[['decrease in', 'has', 'performance']]",[],"[['Results', 'has', 'Score combination']]",constituency_parsing,4,113
55,experiments,"The same trends we observed on the development data , on which the interpolation parameters were tuned , hold here : score combination improves results for all models ( row 3 vs. row 2 ; row 6 vs. row 5 ) , with candidate augmentation from the generative models giving a further increase ( rows 4 and 7 ) .","[('improves', (23, 24)), ('for', (25, 26)), ('with', (42, 43)), ('from', (45, 46))]","[('score combination', (21, 23)), ('results', (24, 25)), ('all models', (26, 28)), ('candidate augmentation', (43, 45)), ('generative models', (47, 49)), ('further increase', (51, 53))]","[['score combination', 'improves', 'results'], ['results', 'for', 'all models'], ['results', 'with', 'candidate augmentation'], ['all models', 'with', 'candidate augmentation'], ['candidate augmentation', 'from', 'generative models']]","[['score combination', 'has', 'results']]",[],[],constituency_parsing,4,122
56,experiments,"As in the PTB training data setting , using all models for candidates and score combinations is best , achieving 94.66 F1 ( row 9 ) .","[('using', (8, 9)), ('for', (11, 12)), ('is', (16, 17)), ('achieving', (19, 20))]","[('PTB training data setting', (3, 7)), ('all models', (9, 11)), ('candidates and score combinations', (12, 16)), ('best', (17, 18)), ('94.66 F1', (20, 22))]","[['PTB training data setting', 'using', 'all models'], ['all models', 'for', 'candidates and score combinations'], ['candidates and score combinations', 'is', 'best'], ['candidates and score combinations', 'achieving', '94.66 F1'], ['best', 'achieving', '94.66 F1']]",[],[],[],constituency_parsing,4,130
57,experiments,"Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .","[('using', (2, 3)), ('with', (20, 21)), ('of', (23, 24))]","[('Performance', (0, 1)), ('only the ensembled RD models', (3, 8)), ('lower', (13, 14)), ('rescoring', (15, 16)), ('single RD model', (17, 20)), ('score combinations', (21, 23)), ('single models', (24, 26)), ('RD + RG', (28, 31)), ('RD + LM', (36, 39))]","[['Performance', 'using', 'only the ensembled RD models'], ['single RD model', 'with', 'score combinations'], ['score combinations', 'of', 'single models']]","[['rescoring', 'has', 'single RD model']]",[],[],constituency_parsing,4,134
58,experiments,"In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .","[('In', (0, 1)), ('with', (6, 7)), ('achieves', (9, 10)), ('of', (15, 16))]","[('PTB setting', (2, 4)), ('ensembling', (5, 6)), ('score combination', (7, 9)), ('best over all result', (11, 15)), ('94.25', (16, 17))]","[['ensembling', 'with', 'score combination'], ['ensembling', 'achieves', 'best over all result'], ['score combination', 'achieves', 'best over all result'], ['best over all result', 'of', '94.25']]","[['PTB setting', 'has', 'ensembling']]",[],[],constituency_parsing,4,135
59,research-problem,In- Order Transition - based Constituent Parsing,[],[],[],[],[],[],constituency_parsing,5,2
60,research-problem,"Furthermore , the system achieves 93.6 F 1 with supervised reranking and 94.2 F 1 with semi-supervised reranking , which are the best results on the WSJ benchmark .","[('achieves', (4, 5)), ('with', (8, 9)), ('with', (15, 16))]","[('system', (3, 4)), ('93.6 F 1', (5, 8)), ('supervised reranking', (9, 11)), ('94.2 F 1', (12, 15)), ('semi-supervised reranking', (16, 18))]","[['system', 'achieves', '93.6 F 1'], ['system', 'achieves', '94.2 F 1'], ['93.6 F 1', 'with', 'supervised reranking'], ['93.6 F 1', 'with', '94.2 F 1'], ['93.6 F 1', 'with', 'semi-supervised reranking'], ['94.2 F 1', 'with', 'semi-supervised reranking'], ['94.2 F 1', 'with', 'semi-supervised reranking']]",[],[],[],constituency_parsing,5,9
61,research-problem,Transition - based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences .,"[('employs', (5, 6))]","[('Transition - based constituent parsing', (0, 5))]",[],[],[],[],constituency_parsing,5,11
62,model,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .","[('propose', (5, 6)), ('for', (10, 11)), ('mitigating', (14, 15)), ('of', (16, 17)), ('by finding', (26, 28)), ('between', (30, 31))]","[('novel transition system', (7, 10)), ('constituent parsing', (11, 13)), ('issues', (15, 16)), ('bottom - up and top - down systems', (18, 26)), ('compromise', (29, 30)), ('bottom - up constituent information', (31, 36)), ('top - down lookahead information', (37, 42))]","[['novel transition system', 'for', 'constituent parsing'], ['novel transition system', 'mitigating', 'issues'], ['issues', 'of', 'bottom - up and top - down systems'], ['bottom - up and top - down systems', 'by finding', 'compromise'], ['compromise', 'between', 'bottom - up constituent information'], ['compromise', 'between', 'top - down lookahead information']]",[],"[['Model', 'propose', 'novel transition system']]",[],constituency_parsing,5,32
63,code,We release our code at https://github.com/LeonCrashCode/InOrderParser .,[],"[('https://github.com/LeonCrashCode/InOrderParser', (5, 6))]",[],[],[],[],constituency_parsing,5,49
64,hyperparameters,is a regularization hyperparameter (? = 10 ?6 ) . We use stochastic gradient descent with a 0.1 initialized learning rate with a 0.05 learning rate decay .,"[('is', (0, 1)), ('use', (11, 12)), ('with', (15, 16)), ('with', (21, 22))]","[('regularization hyperparameter (? = 10 ?6 )', (2, 9)), ('stochastic gradient descent', (12, 15)), ('0.1 initialized learning rate', (17, 21)), ('0.05 learning rate decay', (23, 27))]","[['regularization hyperparameter (? = 10 ?6 )', 'use', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', '0.1 initialized learning rate'], ['0.1 initialized learning rate', 'with', '0.05 learning rate decay'], ['0.1 initialized learning rate', 'with', '0.05 learning rate decay']]",[],"[['Hyperparameters', 'is', 'regularization hyperparameter (? = 10 ?6 )']]",[],constituency_parsing,5,116
65,results,The bottom - up system performs slightly better than the top - down system .,"[('performs', (5, 6)), ('than', (8, 9))]","[('bottom - up system', (1, 5)), ('slightly better', (6, 8)), ('top - down system', (10, 14))]","[['bottom - up system', 'performs', 'slightly better'], ['slightly better', 'than', 'top - down system']]",[],[],"[['Results', 'has', 'bottom - up system']]",constituency_parsing,5,133
66,results,The inorder system outperforms both the bottom - up and the top - down system .,[],"[('inorder system', (1, 3)), ('outperforms', (3, 4)), ('bottom - up and the top - down system', (6, 15))]",[],"[['inorder system', 'has', 'outperforms'], ['outperforms', 'has', 'bottom - up and the top - down system']]",[],"[['Results', 'has', 'inorder system']]",constituency_parsing,5,134
67,results,shows the parsing results on the English test dataset .,"[('on', (4, 5))]","[('English test dataset', (6, 9))]",[],[],"[['Results', 'on', 'English test dataset']]",[],constituency_parsing,5,135
68,results,"We find that the bottom - up parser and the top - down parser have similar results under the greedy setting , and the in - order parser outperforms both of them .","[('find that', (1, 3)), ('have', (14, 15)), ('under', (17, 18))]","[('bottom - up parser and the top - down parser', (4, 14)), ('similar results', (15, 17)), ('greedy setting', (19, 21)), ('in - order parser', (24, 28)), ('outperforms', (28, 29))]","[['bottom - up parser and the top - down parser', 'have', 'similar results'], ['similar results', 'under', 'greedy setting']]","[['bottom - up parser and the top - down parser', 'has', 'similar results'], ['in - order parser', 'has', 'outperforms']]","[['Results', 'find that', 'bottom - up parser and the top - down parser']]",[],constituency_parsing,5,136
69,results,"Also , with supervised reranking , the in - order parser achieves the best results .","[('with', (2, 3)), ('achieves', (11, 12))]","[('supervised reranking', (3, 5)), ('in - order parser', (7, 11)), ('best results', (13, 15))]","[['in - order parser', 'achieves', 'best results']]","[['supervised reranking', 'has', 'in - order parser']]","[['Results', 'with', 'supervised reranking']]",[],constituency_parsing,5,137
70,results,English constituent results,[],[],[],[],[],[],constituency_parsing,5,138
71,results,"With the fully - supervise setting 5 , the inorder parser outperforms the state - of - the - art discrete parser , the state - of - the - art neural parsers","[('With', (0, 1)), ('outperforms', (11, 12))]","[('fully - supervise setting', (2, 6)), ('inorder parser', (9, 11)), ('state - of - the - art discrete parser', (13, 22))]","[['inorder parser', 'outperforms', 'state - of - the - art discrete parser']]","[['fully - supervise setting', 'has', 'inorder parser']]",[],[],constituency_parsing,5,140
72,results,Chinese dependency results,[],[],[],[],[],[],constituency_parsing,5,145
73,results,UAS LAS,[],[],[],[],[],[],constituency_parsing,5,147
74,results,"The in - order parser performs the best on all constituent types , demonstrating that the in - order parser can benefit from both bottom - up and top - down information .","[('performs', (5, 6)), ('on', (8, 9))]","[('in - order parser', (1, 5)), ('best', (7, 8)), ('all constituent types', (9, 12))]","[['in - order parser', 'performs', 'best'], ['best', 'on', 'all constituent types']]","[['in - order parser', 'has', 'best']]",[],"[['Results', 'has', 'in - order parser']]",constituency_parsing,5,170
75,experiments,Parsing as Language Modeling,[],[],[],[],[],[],constituency_parsing,6,2
76,research-problem,"When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .","[('converted to', (3, 5)), ('are', (11, 12))]","[('trees', (1, 2)), ('Stanford dependencies', (5, 7)), ('UAS and LAS', (8, 11)), ('95.9 % and 94.1 %', (12, 17))]","[['trees', 'converted to', 'Stanford dependencies'], ['UAS and LAS', 'are', '95.9 % and 94.1 %']]",[],[],[],constituency_parsing,6,5
77,model,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .","[('present', (14, 15)), ('achieves', (22, 23)), ('with', (31, 32))]","[('neural - net parse reranker', (16, 21)), ('very good results', (23, 26)), ('93.8 F 1', (27, 30)), ('comparatively simple architecture', (33, 36))]","[['neural - net parse reranker', 'achieves', 'very good results'], ['very good results', 'with', 'comparatively simple architecture']]","[['very good results', 'has', '93.8 F 1']]","[['Model', 'present', 'neural - net parse reranker']]",[],constituency_parsing,6,8
78,experiments,Language Modeling,[],[],[],[],[],[],constituency_parsing,6,14
79,experiments,Parsing as Language Modeling,[],[],[],[],[],[],constituency_parsing,6,18
80,hyperparameters,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,"[('applied to', (2, 4)), ('when', (10, 11))]","[('Dropout', (0, 1)), ('non-recurrent connections', (4, 6)), ('gradients', (7, 8)), ('clipped', (9, 10)), ('norm', (12, 13)), ('bigger', (14, 15)), ('20', (16, 17))]","[['Dropout', 'applied to', 'non-recurrent connections'], ['clipped', 'when', 'norm']]","[['norm', 'has', 'bigger'], ['bigger', 'has', '20']]",[],"[['Hyperparameters', 'has', 'Dropout']]",constituency_parsing,6,54
81,hyperparameters,The learning rate is 0.25 0.85 max where is an epoch number .,"[('is', (3, 4))]","[('learning rate', (1, 3)), ('0.25 0.85 max', (4, 7))]","[['learning rate', 'is', '0.25 0.85 max']]","[['learning rate', 'has', '0.25 0.85 max']]",[],"[['Hyperparameters', 'has', 'learning rate']]",constituency_parsing,6,55
82,research-problem,What Do Recurrent Neural Network Grammars Learn About Syntax ?,[],"[('Syntax', (8, 9))]",[],[],[],[],constituency_parsing,7,2
83,research-problem,We find that explicit modeling of composition is crucial for achieving the best performance .,"[('of', (5, 6))]","[('explicit modeling', (3, 5)), ('composition', (6, 7)), ('best performance', (12, 14))]","[['explicit modeling', 'of', 'composition']]",[],[],[],constituency_parsing,7,7
84,research-problem,"Through the attention mechanism , we find that headedness plays a central role in phrasal representation ( with the model 's latent attention largely agreeing with predictions made by hand - crafted head rules , albeit with some important differences ) .","[('Through', (0, 1)), ('find', (6, 7)), ('plays', (9, 10)), ('in', (13, 14)), ('made by', (27, 29))]","[('attention mechanism', (2, 4)), ('headedness', (8, 9)), ('phrasal representation', (14, 16)), ('hand - crafted head rules', (29, 34))]","[['attention mechanism', 'find', 'headedness']]",[],"[['Research problem', 'Through', 'attention mechanism']]",[],constituency_parsing,7,8
85,research-problem,"By training grammars without nonterminal labels , we find that phrasal representations depend minimally on nonterminals , providing support for the endocentricity hypothesis .","[('training', (1, 2)), ('without', (3, 4)), ('find that', (8, 10)), ('depend minimally on', (12, 15)), ('providing support for', (17, 20))]","[('grammars', (2, 3)), ('nonterminal labels', (4, 6)), ('phrasal representations', (10, 12)), ('nonterminals', (15, 16)), ('endocentricity hypothesis', (21, 23))]","[['grammars', 'without', 'nonterminal labels'], ['grammars', 'find that', 'phrasal representations'], ['phrasal representations', 'depend minimally on', 'nonterminals'], ['phrasal representations', 'providing support for', 'endocentricity hypothesis']]",[],"[['Research problem', 'training', 'grammars']]",[],constituency_parsing,7,9
86,model,"In this paper , we focus on a recently proposed class of probability distributions , recurrent neural network grammars ( RNNGs ; ) , designed to model syntactic derivations of sentences .","[('focus on', (5, 7)), ('designed to model', (24, 27)), ('of', (29, 30))]","[('probability distributions', (12, 14)), ('recurrent neural network grammars ( RNNGs ; )', (15, 23)), ('syntactic derivations', (27, 29)), ('sentences', (30, 31))]","[['probability distributions', 'designed to model', 'syntactic derivations'], ['syntactic derivations', 'of', 'sentences']]","[['probability distributions', 'name', 'recurrent neural network grammars ( RNNGs ; )']]","[['Model', 'focus on', 'probability distributions']]",[],constituency_parsing,7,11
87,model,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .","[('focus on', (1, 3)), ('as', (4, 5))]","[('RNNGs', (3, 4)), ('generative probabilistic models over', (5, 9))]","[['RNNGs', 'as', 'generative probabilistic models over']]",[],"[['Model', 'focus on', 'RNNGs']]",[],constituency_parsing,7,12
88,model,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,"[('manipulates', (2, 3)), ('of', (6, 7)), ('to test', (8, 10))]","[('inductive bias', (4, 6)), ('RNNGs', (7, 8)), ('linguistic hypotheses', (10, 12))]","[['inductive bias', 'of', 'RNNGs'], ['inductive bias', 'to test', 'linguistic hypotheses'], ['RNNGs', 'to test', 'linguistic hypotheses']]",[],"[['Model', 'manipulates', 'inductive bias']]",[],constituency_parsing,7,21
89,model,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .","[('augment', (6, 7)), ('with', (11, 12)), ('leading to', (18, 20)), ('to incorporate', (25, 27)), ('into', (29, 30))]","[('RNNG composition function', (8, 11)), ('novel gated attention mechanism', (13, 17)), ('GA - RNNG', (21, 24)), ('more interpretability', (27, 29)), ('model', (31, 32))]","[['RNNG composition function', 'with', 'novel gated attention mechanism'], ['novel gated attention mechanism', 'leading to', 'GA - RNNG'], ['novel gated attention mechanism', 'to incorporate', 'more interpretability'], ['more interpretability', 'into', 'model']]","[['RNNG composition function', 'name', 'novel gated attention mechanism'], ['novel gated attention mechanism', 'name', 'GA - RNNG']]","[['Model', 'augment', 'RNNG composition function']]",[],constituency_parsing,7,23
90,model,"Unlike previous works that rely on hand - crafted rules to compose more fine - grained phrase representations , the RNNG implicitly parameterizes the information passed through compositions of phrases ( in ? and the neural network architecture ) , hence weakening the strong independence assumptions in classical probabilistic context - free grammars .","[('parameterizes', (22, 23)), ('passed through', (25, 27)), ('of', (28, 29))]","[('information', (24, 25)), ('compositions', (27, 28)), ('phrases ( in ? and the neural network architecture )', (29, 39))]","[['information', 'passed through', 'compositions'], ['compositions', 'of', 'phrases ( in ? and the neural network architecture )']]",[],"[['Model', 'parameterizes', 'information']]",[],constituency_parsing,7,36
91,model,"To generate a sentence x and its phrase - structure tree y , the RNNG samples a sequence of actions to construct y top - down .","[('To generate', (0, 2)), ('samples', (15, 16)), ('to', (20, 21))]","[('sentence x', (3, 5)), ('RNNG', (14, 15)), ('sequence of actions', (17, 20)), ('construct', (21, 22))]","[['RNNG', 'samples', 'sequence of actions'], ['sequence of actions', 'to', 'construct']]",[],"[['Model', 'To generate', 'sentence x']]",[],constituency_parsing,7,38
92,model,The RNNG uses three different actions :,"[('uses', (2, 3))]","[('RNNG', (1, 2)), ('three different actions', (3, 6))]","[['RNNG', 'uses', 'three different actions']]","[['RNNG', 'has', 'three different actions']]",[],"[['Model', 'has', 'RNNG']]",constituency_parsing,7,40
93,model,"The RNNG consists of a stack , buffer of generated words , and list of past actions that lead to the current configuration .","[('consists of', (2, 4)), ('of', (8, 9)), ('lead to', (18, 20))]","[('RNNG', (1, 2)), ('stack', (5, 6)), ('buffer', (7, 8)), ('generated words', (9, 11)), ('list of past actions', (13, 17)), ('current configuration', (21, 23))]","[['RNNG', 'consists of', 'stack'], ['RNNG', 'consists of', 'list of past actions'], ['buffer', 'of', 'generated words'], ['list of past actions', 'lead to', 'current configuration']]",[],[],"[['Model', 'has', 'RNNG']]",constituency_parsing,7,44
94,model,"At each timestep , the model encodes the stack , buffer , and past actions , with a separate LSTM for each component as features to define a distribution over the next action to take ( conditioned on the full algorithmic state ) .","[('At', (0, 1)), ('encodes', (6, 7)), ('with', (16, 17)), ('for', (20, 21)), ('as', (23, 24)), ('to define', (25, 27)), ('over', (29, 30)), ('conditioned on', (36, 38))]","[('each timestep', (1, 3)), ('stack , buffer , and past actions', (8, 15)), ('separate LSTM', (18, 20)), ('each component', (21, 23)), ('features', (24, 25)), ('distribution', (28, 29)), ('next action to take', (31, 35)), ('full algorithmic state', (39, 42))]","[['stack , buffer , and past actions', 'with', 'separate LSTM'], ['separate LSTM', 'for', 'each component'], ['each component', 'as', 'features'], ['features', 'to define', 'distribution'], ['distribution', 'over', 'next action to take'], ['distribution', 'conditioned on', 'full algorithmic state'], ['next action to take', 'conditioned on', 'full algorithmic state']]",[],"[['Model', 'At', 'each timestep']]",[],constituency_parsing,7,48
95,model,Both inference problems can be solved using an importance sampling procedure .,"[('solved using', (5, 7))]","[('Both inference problems', (0, 3)), ('importance sampling procedure', (8, 11))]","[['Both inference problems', 'solved using', 'importance sampling procedure']]",[],[],"[['Model', 'has', 'Both inference problems']]",constituency_parsing,7,56
96,hyperparameters,The generative model did not use any pretrained word embeddings or POS tags ; a discriminative variant of the standard RNNG was used to obtain tree samples for the generative model .,"[('did not use', (3, 6)), ('to obtain', (23, 25))]","[('generative', (1, 2)), ('pretrained word embeddings or POS tags', (7, 13))]",[],[],[],"[['Hyperparameters', 'has', 'generative']]",constituency_parsing,7,80
97,results,Do the phrasal representations learned by RN - NGs depend on individual lexical heads or multiple heads ?,"[('learned by', (4, 6)), ('depend on', (9, 11))]","[('Do the phrasal representations', (0, 4)), ('RN - NGs', (6, 9)), ('individual lexical heads', (11, 14))]","[['Do the phrasal representations', 'learned by', 'RN - NGs'], ['Do the phrasal representations', 'depend on', 'individual lexical heads'], ['RN - NGs', 'depend on', 'individual lexical heads']]",[],[],"[['Results', 'has', 'Do the phrasal representations']]",constituency_parsing,7,105
98,results,NPs .,[],"[('NPs', (0, 1))]",[],[],[],[],constituency_parsing,7,149
99,experiments,"The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .","[('better for', (4, 6)), ('much better for', (15, 18)), ('with respect to', (29, 32))]","[('conversion accuracy', (1, 3)), ('nouns ( ? 50 % error )', (6, 13)), ('determiners ( 30 % ) and particles ( 6 % )', (18, 29)), ('Collins head rules', (33, 36))]","[['conversion accuracy', 'better for', 'nouns ( ? 50 % error )'], ['conversion accuracy', 'much better for', 'determiners ( 30 % ) and particles ( 6 % )'], ['determiners ( 30 % ) and particles ( 6 % )', 'with respect to', 'Collins head rules']]",[],[],[],constituency_parsing,7,176
100,results,"On test data ( with the usual split ) , the GA - RNNG achieves 94.2 % , while the U - GA - RNNG achieves 93.5 % .","[('On', (0, 1)), ('achieves', (14, 15)), ('achieves', (25, 26))]","[('test data', (1, 3)), ('usual', (6, 7)), ('GA - RNNG', (11, 14)), ('94.2 %', (15, 17)), ('U - GA - RNNG', (20, 25)), ('93.5 %', (26, 28))]","[['GA - RNNG', 'achieves', '94.2 %'], ['U - GA - RNNG', 'achieves', '93.5 %']]","[['test data', 'has', 'usual'], ['usual', 'has', 'GA - RNNG']]","[['Results', 'On', 'test data']]",[],constituency_parsing,7,192
101,research-problem,Constituency Parsing with a Self - Attentive Encoder,[],"[('Constituency Parsing', (0, 2))]",[],[],[],[],constituency_parsing,8,2
102,research-problem,We demonstrate that replacing an LSTM encoder with a self - attentive architecture can lead to improvements to a state - of the - art discriminative constituency parser .,"[('demonstrate', (1, 2)), ('replacing', (3, 4)), ('with', (7, 8)), ('lead to', (14, 16)), ('to', (17, 18))]","[('LSTM encoder', (5, 7)), ('self - attentive architecture', (9, 13)), ('improvements', (16, 17)), ('state - of the - art discriminative constituency parser', (19, 28))]","[['LSTM encoder', 'with', 'self - attentive architecture'], ['LSTM encoder', 'lead to', 'improvements'], ['self - attentive architecture', 'lead to', 'improvements'], ['improvements', 'to', 'state - of the - art discriminative constituency parser']]",[],"[['Research problem', 'demonstrate', 'LSTM encoder']]",[],constituency_parsing,8,4
103,model,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .","[('introduce', (5, 6)), ('combines', (9, 10)), ('built using', (12, 14)), ('with', (21, 22)), ('customized for', (24, 26))]","[('parser', (7, 8)), ('encoder', (11, 12)), ('decoder', (23, 24)), ('parsing', (26, 27))]","[['parser', 'combines', 'encoder'], ['decoder', 'customized for', 'parsing']]",[],"[['Model', 'introduce', 'parser']]",[],constituency_parsing,8,16
104,model,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .","[('uses', (9, 10)), ('performs', (15, 16)), ('than', (17, 18))]","[('character LSTM', (11, 13)), ('better', (16, 17)), ('other lexical representationseven', (18, 21))]","[['character LSTM', 'performs', 'better'], ['better', 'than', 'other lexical representationseven']]",[],"[['Model', 'uses', 'character LSTM']]",[],constituency_parsing,8,24
105,results,The model presented above achieves a score of 92.67 F1 on the Penn Treebank WSJ development set .,"[('achieves', (4, 5)), ('of', (7, 8)), ('on', (10, 11))]","[('score', (6, 7)), ('92.67 F1', (8, 10)), ('Penn Treebank WSJ development set', (12, 17))]","[['score', 'of', '92.67 F1'], ['92.67 F1', 'on', 'Penn Treebank WSJ development set']]",[],"[['Results', 'achieves', 'score']]",[],constituency_parsing,8,89
106,results,"For comparison , a model that uses the same decode procedure with an LSTM - based encoder achieves a development set score of 92.24 .","[('uses', (6, 7)), ('with', (11, 12)), ('achieves', (17, 18)), ('of', (22, 23))]","[('same decode procedure', (8, 11)), ('LSTM - based encoder', (13, 17)), ('development set score', (19, 22)), ('92.24', (23, 24))]","[['same decode procedure', 'with', 'LSTM - based encoder'], ['LSTM - based encoder', 'achieves', 'development set score'], ['development set score', 'of', '92.24']]",[],[],[],constituency_parsing,8,91
107,results,Content vs. Position Attention,[],[],[],[],[],[],constituency_parsing,8,96
108,ablation-analysis,"We can see that our model learns to use a combination of the two attention types , with positionbased attention being the most important .","[('see', (2, 3)), ('learns to use', (6, 9)), ('of', (11, 12)), ('with', (17, 18))]","[('our model', (4, 6)), ('combination', (10, 11)), ('two attention types', (13, 16)), ('positionbased attention', (18, 20))]","[['our model', 'learns to use', 'combination'], ['combination', 'of', 'two attention types'], ['two attention types', 'with', 'positionbased attention']]","[['combination', 'has', 'two attention types']]","[['Ablation analysis', 'see', 'our model']]",[],constituency_parsing,8,99
109,results,"We also see that content - based attention is more useful at later layers in the network , which is consistent with the idea that the initial layers of our model act similarly to a dilated convolutional network while the upper layers have a greater balance between the two attention types .","[('see', (2, 3)), ('is', (8, 9)), ('at', (11, 12)), ('in', (14, 15))]","[('content - based attention', (4, 8)), ('more useful', (9, 11)), ('later layers', (12, 14)), ('network', (16, 17))]","[['content - based attention', 'is', 'more useful'], ['more useful', 'at', 'later layers'], ['later layers', 'in', 'network']]","[['content - based attention', 'has', 'more useful']]","[['Results', 'see', 'content - based attention']]",[],constituency_parsing,8,100
110,research-problem,Improving Coreference Resolution by Learning Entity - Level Distributed Representations,[],"[('Coreference Resolution', (1, 3))]",[],[],[],[],coreference_resolution,0,2
111,research-problem,A long - standing challenge in coreference resolution has been the incorporation of entity - level information - features defined over clusters of mentions instead of mention pairs .,[],"[('coreference resolution', (6, 8))]",[],[],[],[],coreference_resolution,0,4
112,research-problem,"Coreference resolution , the task of identifying which mentions in a text refer to the same realworld entity , is fundamentally a clustering problem .",[],"[('Coreference resolution', (0, 2))]",[],[],[],[],coreference_resolution,0,10
113,model,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .","[('train', (6, 7)), ('to build', (11, 13)), ('of', (15, 16)), ('of', (17, 18))]","[('deep neural network', (8, 11)), ('distributed representations', (13, 15)), ('pairs', (16, 17)), ('coreference clusters', (18, 20))]","[['deep neural network', 'to build', 'distributed representations'], ['distributed representations', 'of', 'pairs'], ['pairs', 'of', 'coreference clusters'], ['pairs', 'of', 'coreference clusters']]",[],"[['Model', 'train', 'deep neural network']]",[],coreference_resolution,0,16
114,model,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .","[('captures', (1, 2)), ('with', (6, 7)), ('instead of', (15, 17))]","[('entity - level information', (2, 6)), ('large number of learned , continuous features', (8, 15)), ('small number of hand - crafted categorical ones', (18, 26))]","[['entity - level information', 'with', 'large number of learned , continuous features'], ['large number of learned , continuous features', 'instead of', 'small number of hand - crafted categorical ones']]",[],"[['Model', 'captures', 'entity - level information']]",[],coreference_resolution,0,17
115,model,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .","[('At', (0, 1)), ('builds up', (4, 6)), ('starting with', (10, 12)), ('in', (14, 15))]","[('test time', (1, 3)), ('coreference clusters', (6, 8)), ('incrementally', (8, 9)), ('each mention', (12, 14)), ('merging', (20, 21)), ('pair of clusters', (22, 25))]","[['test time', 'builds up', 'coreference clusters'], ['coreference clusters', 'starting with', 'each mention']]","[['coreference clusters', 'has', 'incrementally'], ['merging', 'has', 'pair of clusters']]","[['Model', 'At', 'test time']]",[],coreference_resolution,0,19
116,model,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,"[('with', (4, 5)), ('combines', (15, 16)), ('of', (18, 19))]","[('decisions', (3, 4)), ('novel easy - first cluster - ranking procedure', (6, 14)), ('strengths', (17, 18)), ('cluster - ranking ( Rahman and and easy - first coreference algorithms', (19, 31))]","[['decisions', 'with', 'novel easy - first cluster - ranking procedure'], ['novel easy - first cluster - ranking procedure', 'combines', 'strengths'], ['strengths', 'of', 'cluster - ranking ( Rahman and and easy - first coreference algorithms']]","[['decisions', 'has', 'novel easy - first cluster - ranking procedure']]",[],"[['Model', 'has', 'decisions']]",coreference_resolution,0,20
117,research-problem,Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made .,[],"[('Training incremental coreference systems', (0, 4))]",[],[],[],[],coreference_resolution,0,21
118,model,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,"[('using', (4, 5)), ('inspired by', (12, 14)), ('to train', (15, 17))]","[('learning - to - search algorithm', (6, 12)), ('SEARN', (14, 15)), ('our neural network', (17, 20))]","[['learning - to - search algorithm', 'inspired by', 'SEARN'], ['SEARN', 'to train', 'our neural network']]",[],"[['Model', 'using', 'learning - to - search algorithm']]",[],coreference_resolution,0,22
119,model,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,"[('lead to', (28, 30))]","[('which action ( a cluster merge ) available', (7, 15)), ('current state', (17, 19)), ('partially completed coreference clustering', (21, 25)), ('high - scoring coreference partition', (31, 36))]","[['which action ( a cluster merge ) available', 'lead to', 'high - scoring coreference partition']]","[['current state', 'name', 'partially completed coreference clustering']]",[],"[['Model', 'has', 'which action ( a cluster merge ) available']]",coreference_resolution,0,23
120,hyperparameters,"We initialized our word embeddings with 50 dimensional ones produced by word2vec on the Gigaword corpus for English and 64 dimensional ones provided by Polyglot ( Al - Rfou et al. , 2013 ) for Chinese .","[('initialized', (1, 2)), ('with', (5, 6)), ('produced by', (9, 11)), ('on', (12, 13)), ('for', (16, 17)), ('provided by', (22, 24))]","[('our word embeddings', (2, 5)), ('50 dimensional ones', (6, 9)), ('word2vec', (11, 12)), ('Gigaword corpus', (14, 16)), ('English', (17, 18)), ('64 dimensional ones', (19, 22)), ('Chinese', (35, 36))]","[['our word embeddings', 'with', '50 dimensional ones'], ['50 dimensional ones', 'produced by', 'word2vec'], ['word2vec', 'on', 'Gigaword corpus'], ['word2vec', 'on', '64 dimensional ones'], ['Gigaword corpus', 'for', 'English']]",[],"[['Hyperparameters', 'initialized', 'our word embeddings']]",[],coreference_resolution,0,78
121,hyperparameters,Averaged word embeddings were held fixed during training while the embeddings used for single words were updated .,"[('held', (4, 5)), ('during', (6, 7)), ('used for', (11, 13))]","[('Averaged word embeddings', (0, 3)), ('fixed', (5, 6)), ('training', (7, 8)), ('embeddings', (10, 11)), ('single words', (13, 15)), ('updated', (16, 17))]","[['Averaged word embeddings', 'held', 'fixed'], ['fixed', 'during', 'training'], ['embeddings', 'used for', 'single words']]","[['Averaged word embeddings', 'has', 'fixed']]",[],"[['Hyperparameters', 'has', 'Averaged word embeddings']]",coreference_resolution,0,79
122,hyperparameters,"We set our hidden layer sizes to M 1 = 1000 , M 2 = d = 500 and minimized the training objective using RMS - Prop .","[('set', (1, 2)), ('to', (6, 7)), ('minimized', (19, 20)), ('using', (23, 24))]","[('hidden layer sizes', (3, 6)), ('M 1 = 1000', (7, 11)), ('M 2 = d = 500', (12, 18)), ('training objective', (21, 23)), ('RMS - Prop', (24, 27))]","[['hidden layer sizes', 'to', 'M 1 = 1000'], ['hidden layer sizes', 'minimized', 'training objective'], ['training objective', 'using', 'RMS - Prop']]","[['hidden layer sizes', 'has', 'M 1 = 1000']]","[['Hyperparameters', 'set', 'hidden layer sizes']]",[],coreference_resolution,0,80
123,hyperparameters,"To regularize the network , we applied L2 regularization to the model weights and dropout with a rate of 0.5 on the word embeddings and the output of each hidden layer .","[('To regularize', (0, 2)), ('applied', (6, 7)), ('to', (9, 10)), ('with', (15, 16)), ('of', (18, 19)), ('on', (20, 21)), ('of', (27, 28))]","[('network', (3, 4)), ('L2 regularization', (7, 9)), ('model weights', (11, 13)), ('dropout', (14, 15)), ('rate', (17, 18)), ('0.5', (19, 20)), ('word embeddings', (22, 24)), ('output', (26, 27)), ('each hidden layer', (28, 31))]","[['network', 'applied', 'L2 regularization'], ['L2 regularization', 'to', 'model weights'], ['dropout', 'with', 'rate'], ['rate', 'of', '0.5'], ['output', 'of', 'each hidden layer'], ['0.5', 'on', 'word embeddings'], ['output', 'of', 'each hidden layer']]","[['rate', 'has', '0.5']]","[['Hyperparameters', 'To regularize', 'network']]",[],coreference_resolution,0,81
124,hyperparameters,"As in , we found that pretraining is crucial for the mentionranking model 's success .","[('found', (4, 5)), ('crucial for', (8, 10))]","[('pretraining', (6, 7)), (""mentionranking model 's success"", (11, 15))]","[['pretraining', 'crucial for', ""mentionranking model 's success""]]",[],"[['Hyperparameters', 'found', 'pretraining']]",[],coreference_resolution,0,83
125,experiments,Mention - Ranking Model Experiments,[],[],[],[],[],[],coreference_resolution,0,188
126,ablation-analysis,"We find the small number of non-embedding features substantially improves model performance , especially the distance and string matching features .","[('find', (1, 2)), ('especially', (13, 14))]","[('small number of non-embedding features', (3, 8)), ('substantially improves', (8, 10)), ('model performance', (10, 12)), ('distance and string matching features', (15, 20))]","[['model performance', 'especially', 'distance and string matching features']]","[['small number of non-embedding features', 'has', 'substantially improves'], ['substantially improves', 'has', 'model performance']]","[['Ablation analysis', 'find', 'small number of non-embedding features']]",[],coreference_resolution,0,192
127,ablation-analysis,Cluster - Ranking Model Experiments,[],[],[],[],[],[],coreference_resolution,0,200
128,ablation-analysis,Using pretrained weights greatly improves performance .,"[('Using', (0, 1))]","[('pretrained weights', (1, 3)), ('greatly improves', (3, 5)), ('performance', (5, 6))]",[],"[['pretrained weights', 'has', 'greatly improves'], ['greatly improves', 'has', 'performance']]","[['Ablation analysis', 'Using', 'pretrained weights']]",[],coreference_resolution,0,205
129,ablation-analysis,We find the easy - first approach slightly outperforms using a left - to - right ordering of mentions .,"[('find', (1, 2)), ('using', (9, 10)), ('of', (17, 18))]","[('easy - first approach', (3, 7)), ('slightly outperforms', (7, 9)), ('left - to - right ordering', (11, 17)), ('mentions', (18, 19))]","[['slightly outperforms', 'using', 'left - to - right ordering'], ['left - to - right ordering', 'of', 'mentions']]","[['easy - first approach', 'has', 'slightly outperforms']]","[['Ablation analysis', 'find', 'easy - first approach']]",[],coreference_resolution,0,211
130,results,Our mention - ranking model surpasses all previous systems .,"[('surpasses', (5, 6))]","[('mention - ranking model', (1, 5)), ('all previous systems', (6, 9))]","[['mention - ranking model', 'surpasses', 'all previous systems']]",[],[],"[['Results', 'has', 'mention - ranking model']]",coreference_resolution,0,229
131,results,"The cluster - ranking model improves results further across both languages and all evaluation metrics , demonstrating the utility of incorporating entity - level information .","[('improves', (5, 6)), ('further across', (7, 9))]","[('cluster - ranking model', (1, 5)), ('results', (6, 7)), ('both languages and all evaluation metrics', (9, 15))]","[['cluster - ranking model', 'improves', 'results'], ['results', 'further across', 'both languages and all evaluation metrics']]","[['cluster - ranking model', 'has', 'results']]",[],"[['Results', 'has', 'cluster - ranking model']]",coreference_resolution,0,231
132,results,"However , it is worth noting that in practice the much more complicated cluster - ranking model brings only fairly modest gains in performance .","[('brings', (17, 18)), ('in', (22, 23))]","[('much more complicated cluster - ranking model', (10, 17)), ('fairly modest gains', (19, 22)), ('performance', (23, 24))]","[['much more complicated cluster - ranking model', 'brings', 'fairly modest gains'], ['fairly modest gains', 'in', 'performance']]",[],[],[],coreference_resolution,0,236
133,research-problem,End - to - end Deep Reinforcement Learning Based Coreference Resolution,[],[],[],[],[],[],coreference_resolution,1,2
134,research-problem,Recent neural network models have significantly advanced the task of coreference resolution .,[],"[('coreference resolution', (10, 12))]",[],[],[],[],coreference_resolution,1,4
135,research-problem,"Coreference resolution is one of the most fundamental tasks in natural language processing ( NLP ) , which has a significant impact on many downstream applications including information extraction ) , question answering , and entity linking .",[],"[('Coreference resolution', (0, 2))]",[],[],[],[],coreference_resolution,1,12
136,model,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .","[('propose', (5, 6)), ('to resolve', (17, 19))]","[('goal - directed endto - end deep reinforcement learning framework', (7, 17)), ('coreference', (19, 20))]","[['goal - directed endto - end deep reinforcement learning framework', 'to resolve', 'coreference']]",[],"[['Model', 'propose', 'goal - directed endto - end deep reinforcement learning framework']]",[],coreference_resolution,1,23
137,model,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .","[('leverage', (3, 4)), ('includes', (14, 15)), ('scoring', (19, 20)), ('generating', (25, 26)), ('over', (29, 30)), ('from', (35, 36)), ('to', (39, 40))]","[('neural architecture', (5, 7)), ('our policy network', (9, 12)), ('learning span representation', (15, 18)), ('potential entity mentions', (20, 23)), ('probability distribution', (27, 29)), ('all possible coreference linking actions', (30, 35)), ('current mention', (37, 39))]","[['our policy network', 'includes', 'learning span representation'], ['learning span representation', 'scoring', 'potential entity mentions'], ['our policy network', 'generating', 'probability distribution'], ['probability distribution', 'over', 'all possible coreference linking actions'], ['all possible coreference linking actions', 'from', 'current mention']]","[['neural architecture', 'name', 'our policy network']]","[['Model', 'leverage', 'neural architecture']]",[],coreference_resolution,1,24
138,model,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .","[('introduce', (3, 4)), ('to encourage', (8, 10)), ('prevent', (12, 13)), ('from', (15, 16)), ('to', (18, 19))]","[('entropy regularization term', (5, 8)), ('exploration', (10, 11)), ('policy', (14, 15)), ('prematurely converging', (16, 18)), ('bad local optimum', (20, 23))]","[['entropy regularization term', 'to encourage', 'exploration'], ['entropy regularization term', 'prevent', 'policy'], ['policy', 'from', 'prematurely converging'], ['prematurely converging', 'to', 'bad local optimum']]",[],"[['Model', 'introduce', 'entropy regularization term']]",[],coreference_resolution,1,26
139,model,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .","[('update', (3, 4)), ('based on', (9, 11)), ('associated with', (13, 15)), ('of', (16, 17)), ('computed on', (22, 24))]","[('regularized policy network parameters', (5, 9)), ('rewards', (12, 13)), ('sequences', (15, 16)), ('whole input document', (25, 28))]","[['regularized policy network parameters', 'based on', 'rewards'], ['rewards', 'associated with', 'sequences']]",[],"[['Model', 'update', 'regularized policy network parameters']]",[],coreference_resolution,1,27
140,hyperparameters,"First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .","[('pretrain', (3, 4)), ('for', (11, 12)), ('use', (17, 18)), ('for', (21, 22))]","[('our model', (4, 6)), ('around 200 K steps', (12, 16)), ('learned parameters', (19, 21)), ('initialization', (22, 23))]","[['learned parameters', 'for', 'initialization'], ['our model', 'use', 'learned parameters'], ['learned parameters', 'for', 'initialization']]",[],"[['Hyperparameters', 'pretrain', 'our model']]",[],coreference_resolution,1,104
141,hyperparameters,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .","[('set', (3, 4)), ('tune', (14, 15)), ('expr in', (19, 21)), ('set it to', (37, 40)), ('based on', (43, 45))]","[('number of sampled trajectories N s = 100', (5, 13)), ('regularization parameter ?', (16, 19)), ('{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }', (21, 36)), ('10 ? 4', (40, 43)), ('development set', (46, 48))]","[['number of sampled trajectories N s = 100', 'tune', 'regularization parameter ?'], ['regularization parameter ?', 'expr in', '{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }'], ['regularization parameter ?', 'set it to', '10 ? 4'], ['10 ? 4', 'based on', 'development set']]",[],"[['Hyperparameters', 'set', 'number of sampled trajectories N s = 100']]",[],coreference_resolution,1,105
142,results,"In , we compare our model with the coreference systems that have produced significant improvement over the last 3 years on the OntoNotes benchmark .","[('produced', (12, 13)), ('on', (20, 21))]","[('coreference systems', (8, 10)), ('significant improvement', (13, 15)), ('OntoNotes benchmark', (22, 24))]","[['significant improvement', 'on', 'OntoNotes benchmark']]",[],[],[],coreference_resolution,1,110
143,results,"Built on top of the model in but excluding ELMo , our base reinforced model improves the average F 1 score around 2 points ( statistical significant t- test with p < 0.05 ) compared with .","[('Built on top of', (0, 4)), ('excluding', (8, 9)), ('improves', (15, 16)), ('around', (21, 22))]","[('model', (5, 6)), ('ELMo', (9, 10)), ('our base reinforced model', (11, 15)), ('average F 1 score', (17, 21)), ('2 points', (22, 24))]","[['our base reinforced model', 'improves', 'average F 1 score'], ['average F 1 score', 'around', '2 points']]","[['model', 'has', 'ELMo']]","[['Results', 'Built on top of', 'model']]",[],coreference_resolution,1,115
144,results,"Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .","[('using', (4, 5)), ('to encourage', (7, 9)), ('can improve', (10, 12)), ('by', (14, 15))]","[('entropy regularization', (5, 7)), ('exploration', (9, 10)), ('result', (13, 14)), ('1 point', (15, 17))]","[['entropy regularization', 'to encourage', 'exploration'], ['exploration', 'can improve', 'result'], ['result', 'by', '1 point']]",[],[],[],coreference_resolution,1,117
145,results,"Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .","[('introducing', (2, 3)), ('to', (9, 10)), ('can further boosts', (13, 16))]","[('context - dependent ELMo embedding', (4, 9)), ('our base model', (10, 13)), ('performance', (17, 18))]","[['context - dependent ELMo embedding', 'to', 'our base model'], ['context - dependent ELMo embedding', 'can further boosts', 'performance'], ['our base model', 'can further boosts', 'performance']]",[],"[['Results', 'introducing', 'context - dependent ELMo embedding']]",[],coreference_resolution,1,118
146,results,"We also notice that our full model 's improvement is mainly from higher precision scores and reasonably good recall scores , which indicates that our reinforced model combined with more active exploration produces better coreference scores to reduce false positive coreference links .","[('notice', (2, 3)), ('mainly from', (10, 12))]","[(""our full model 's improvement"", (4, 9)), ('higher precision scores', (12, 15)), ('reasonably good recall scores', (16, 20))]","[[""our full model 's improvement"", 'mainly from', 'higher precision scores']]",[],[],[],coreference_resolution,1,119
147,results,"Overall , our full model achieves the state - of the - art performance of 73.8 % F1 - score when using ELMo and entropy regularization ( compared to models marked with * in , and our approach simultaneously obtains the best F1 -score of 70.5 % when using fixed word embedding only .","[('achieves', (5, 6)), ('of', (14, 15)), ('when using', (20, 22)), ('of', (44, 45)), ('when using', (47, 49))]","[('our full model', (2, 5)), ('state - of the - art performance', (7, 14)), ('73.8 % F1 - score', (15, 20)), ('ELMo and entropy regularization', (22, 26)), ('our', (36, 37)), ('best F1 -score', (41, 44)), ('70.5 %', (45, 47)), ('fixed word embedding', (49, 52))]","[['our full model', 'achieves', 'state - of the - art performance'], ['state - of the - art performance', 'of', '73.8 % F1 - score'], ['73.8 % F1 - score', 'when using', 'ELMo and entropy regularization'], ['best F1 -score', 'of', '70.5 %'], ['70.5 %', 'when using', 'fixed word embedding']]",[],[],"[['Results', 'has', 'our full model']]",coreference_resolution,1,120
148,research-problem,Deep Reinforcement Learning for Mention - Ranking Coreference Models,[],"[('Mention - Ranking Coreference', (4, 8))]",[],[],[],[],coreference_resolution,2,2
149,model,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .","[('explore using', (5, 7)), ('to directly optimize', (12, 15)), ('for', (18, 19))]","[('two variants of reinforcement learning', (7, 12)), ('coreference system', (16, 18)), ('coreference evaluation metrics', (19, 22))]","[['two variants of reinforcement learning', 'to directly optimize', 'coreference system'], ['coreference system', 'for', 'coreference evaluation metrics']]",[],"[['Model', 'explore using', 'two variants of reinforcement learning']]",[],coreference_resolution,2,15
150,model,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .","[('modify', (4, 5)), ('by incorporating', (11, 13)), ('associated with', (15, 17)), ('into', (20, 21))]","[('max-margin coreference objective', (6, 9)), ('reward', (14, 15)), ('each coreference decision', (17, 20)), (""loss 's slack rescaling"", (22, 26))]","[['max-margin coreference objective', 'by incorporating', 'reward'], ['reward', 'associated with', 'each coreference decision'], ['reward', 'into', ""loss 's slack rescaling""]]",[],"[['Model', 'modify', 'max-margin coreference objective']]",[],coreference_resolution,2,16
151,model,Our model is a neural mention - ranking model .,"[('is', (2, 3))]","[('neural mention - ranking model', (4, 9))]",[],[],"[['Model', 'is', 'neural mention - ranking model']]",[],coreference_resolution,2,18
152,experiments,Reinforcement Learning,[],[],[],[],[],[],coreference_resolution,2,70
153,results,"We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .","[('find that', (1, 3)), ('does', (4, 5)), ('than', (7, 8)), ('performs', (15, 16))]","[('REINFORCE', (3, 4)), ('slightly better', (5, 7)), ('heuristic loss', (9, 11)), ('reward rescaling', (13, 15)), ('significantly better', (16, 18))]","[['REINFORCE', 'does', 'slightly better'], ['slightly better', 'than', 'heuristic loss'], ['reward rescaling', 'performs', 'significantly better']]","[['REINFORCE', 'has', 'slightly better']]","[['Results', 'find that', 'REINFORCE']]",[],coreference_resolution,2,110
154,ablation-analysis,"During training it optimizes the model 's performance in expectation , but at test - time it takes the most probable sequence of actions .","[('During', (0, 1)), ('optimizes', (3, 4)), ('in', (8, 9)), ('at', (12, 13)), ('takes', (17, 18)), ('of', (22, 23))]","[('training', (1, 2)), (""model 's performance"", (5, 8)), ('expectation', (9, 10)), ('test - time', (13, 16)), ('most probable sequence', (19, 22)), ('actions', (23, 24))]","[['training', 'optimizes', ""model 's performance""], [""model 's performance"", 'in', 'expectation'], ['training', 'at', 'test - time'], ['test - time', 'takes', 'most probable sequence'], ['most probable sequence', 'of', 'actions']]",[],"[['Ablation analysis', 'During', 'training']]",[],coreference_resolution,2,112
155,results,"The reward - rescaled max - margin loss combines the best of both worlds , resulting in superior performance .","[('combines', (8, 9)), ('resulting in', (15, 17))]","[('reward - rescaled max - margin loss', (1, 8)), ('best of both worlds', (10, 14)), ('superior performance', (17, 19))]","[['reward - rescaled max - margin loss', 'combines', 'best of both worlds'], ['best of both worlds', 'resulting in', 'superior performance']]",[],[],"[['Results', 'has', 'reward - rescaled max - margin loss']]",coreference_resolution,2,115
156,research-problem,Higher - order Coreference Resolution with Coarse - to - fine Inference,[],"[('Higher', (0, 1))]",[],[],[],[],coreference_resolution,3,2
157,model,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,"[('introduce', (1, 2)), ('of', (4, 5)), ('uses', (10, 11))]","[('approximation', (3, 4)), ('higher - order inference', (5, 9)), ('span - ranking architecture', (12, 16))]","[['approximation', 'of', 'higher - order inference'], ['higher - order inference', 'uses', 'span - ranking architecture']]",[],"[['Model', 'introduce', 'approximation']]",[],coreference_resolution,3,15
158,model,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .","[('propose', (13, 14)), ('learned with', (21, 23))]","[('coarseto - fine approach', (15, 19)), ('single endto - end objective', (24, 29))]","[['coarseto - fine approach', 'learned with', 'single endto - end objective']]",[],"[['Model', 'propose', 'coarseto - fine approach']]",[],coreference_resolution,3,19
159,experiments,1 . increasing the maximum span width from 10 to 30 words .,"[('increasing', (2, 3)), ('from', (7, 8))]","[('maximum span width', (4, 7)), ('10 to 30 words', (8, 12))]","[['maximum span width', 'from', '10 to 30 words']]","[['maximum span width', 'has', '10 to 30 words']]",[],[],coreference_resolution,3,103
160,code,1 https://github.com/kentonl/e2e-coref,[],[],[],[],[],[],coreference_resolution,3,104
161,experiments,2 . using 3 highway LSTMs instead of 1 .,"[('using', (2, 3)), ('instead of', (6, 8))]","[('3 highway LSTMs', (3, 6)), ('1', (8, 9))]","[['3 highway LSTMs', 'instead of', '1']]",[],[],[],coreference_resolution,3,105
162,hyperparameters,3 . using Glo Ve word embeddings with a window size of 2 for the headword embeddings and a window size of 10 for the LSTM inputs .,"[('using', (2, 3)), ('with', (7, 8)), ('of', (11, 12)), ('for', (13, 14)), ('for', (23, 24))]","[('Glo Ve word embeddings', (3, 7)), ('window size', (9, 11)), ('2', (12, 13)), ('headword embeddings', (15, 17)), ('window size', (19, 21)), ('10', (22, 23)), ('LSTM inputs', (25, 27))]","[['Glo Ve word embeddings', 'with', 'window size'], ['Glo Ve word embeddings', 'with', 'window size'], ['window size', 'of', '2'], ['window size', 'of', '10'], ['2', 'for', 'headword embeddings'], ['10', 'for', 'LSTM inputs'], ['10', 'for', 'LSTM inputs']]","[['Glo Ve word embeddings', 'has', 'window size'], ['window size', 'has', '10']]","[['Hyperparameters', 'using', 'Glo Ve word embeddings']]",[],coreference_resolution,3,106
163,results,"On the development set , the second - order model ( N = 2 ) outperforms the first - order model by 0.8 F1 , but the third order model only provides an additional 0.1 F1 improvement .","[('On', (0, 1)), ('by', (21, 22)), ('provides', (31, 32))]","[('development set', (2, 4)), ('second - order model ( N = 2 )', (6, 15)), ('outperforms', (15, 16)), ('first - order model', (17, 21)), ('0.8 F1', (22, 24)), ('third order model', (27, 30)), ('additional 0.1 F1 improvement', (33, 37))]","[['first - order model', 'by', '0.8 F1'], ['third order model', 'provides', 'additional 0.1 F1 improvement']]","[['development set', 'has', 'second - order model ( N = 2 )'], ['second - order model ( N = 2 )', 'has', 'outperforms'], ['outperforms', 'has', 'first - order model']]","[['Results', 'On', 'development set']]",[],coreference_resolution,3,110
164,results,"The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .","[('is', (6, 7)), ('from augmented with', (12, 15)), ('achieves', (22, 23))]","[('span - ranking model', (8, 12)), ('ELMo and hyperparameter tuning', (16, 20)), ('72.3 F1', (23, 25))]","[['span - ranking model', 'from augmented with', 'ELMo and hyperparameter tuning'], ['span - ranking model', 'achieves', '72.3 F1']]",[],[],[],coreference_resolution,3,118
165,results,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .","[('achieves', (3, 4)), ('setting', (7, 8)), ('for', (14, 15))]","[('Our full approach', (0, 3)), ('73.0 F1', (4, 6)), ('new state of the art', (9, 14)), ('coreference resolution', (15, 17))]","[['Our full approach', 'achieves', '73.0 F1'], ['73.0 F1', 'setting', 'new state of the art'], ['new state of the art', 'for', 'coreference resolution']]",[],[],"[['Results', 'has', 'Our full approach']]",coreference_resolution,3,119
166,results,We also observe further improvement by including the second - order inference ( Section 3 ) .,"[('observe', (2, 3)), ('by including', (5, 7))]","[('further improvement', (3, 5)), ('second - order inference', (8, 12))]","[['further improvement', 'by including', 'second - order inference']]",[],"[['Results', 'observe', 'further improvement']]",[],coreference_resolution,3,123
167,results,"The improvement is largely driven by the over all increase in precision , which is expected since the higher - order inference mainly serves to rule out inconsistent clusters .","[('largely driven by', (3, 6)), ('in', (10, 11))]","[('improvement', (1, 2)), ('over all increase', (7, 10)), ('precision', (11, 12))]","[['improvement', 'largely driven by', 'over all increase'], ['over all increase', 'in', 'precision']]",[],[],"[['Results', 'has', 'improvement']]",coreference_resolution,3,124
168,research-problem,A Mention - Ranking Model for Abstract Anaphora Resolution,[],[],[],[],[],[],coreference_resolution,4,2
169,research-problem,"Resolving abstract anaphora is an important , but difficult task for text understanding .",[],"[('Resolving abstract anaphora', (0, 3))]",[],[],[],[],coreference_resolution,4,4
170,model,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .","[('inspired by', (3, 5)), ('for', (10, 11)), ('combines it with', (14, 17)), ('for learning', (21, 23)), ('between', (24, 25))]","[('mention - ranking model', (6, 10)), ('coreference resolution', (11, 13)), ('Siamese Net', (18, 20)), ('similarity', (23, 24)), ('sentences', (25, 26))]","[['mention - ranking model', 'for', 'coreference resolution'], ['Siamese Net', 'for learning', 'similarity'], ['similarity', 'between', 'sentences']]",[],"[['Model', 'inspired by', 'mention - ranking model']]",[],coreference_resolution,4,28
171,model,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .","[('Given', (0, 1)), ('learns', (42, 43)), ('for', (44, 45)), ('in', (51, 52))]","[('anaphoric sentence', (2, 4)), ('representations', (43, 44)), ('candidate and the anaphoric sentence', (46, 51)), ('shared space', (53, 55))]","[['representations', 'for', 'candidate and the anaphoric sentence'], ['candidate and the anaphoric sentence', 'in', 'shared space']]",[],"[['Model', 'Given', 'anaphoric sentence']]",[],coreference_resolution,4,29
172,model,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,"[('combined into', (3, 5)), ('to calculate', (9, 11)), ('characterizes', (14, 15))]","[('joint representation', (6, 8)), ('score', (12, 13)), ('relation', (16, 17))]","[['joint representation', 'to calculate', 'score'], ['score', 'characterizes', 'relation']]",[],[],[],coreference_resolution,4,30
173,code,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora.,[],"[('data extraction', (11, 13))]",[],[],[],[],coreference_resolution,4,41
174,results,"PS BL always performs worse than the KZH13 model on the ASN , so we report it only for ARRAU - AA .","[('performs', (3, 4)), ('than', (5, 6)), ('on', (9, 10))]","[('PS BL', (0, 2)), ('worse', (4, 5)), ('KZH13 model', (7, 9)), ('ASN', (11, 12))]","[['PS BL', 'performs', 'worse'], ['worse', 'than', 'KZH13 model'], ['KZH13 model', 'on', 'ASN']]",[],[],"[['Results', 'has', 'PS BL']]",coreference_resolution,4,160
175,hyperparameters,"Embeddings for tags are initialized with values drawn from the uniform distribution U ? 1 ? d+t , 1 ? d+t , where t is the number of tags 16 and d ? { 50 , qlog - U ( 30 , 100 ) } the size of the tag embeddings .","[('for', (1, 2)), ('drawn from', (7, 9))]","[('Embeddings', (0, 1)), ('tags', (2, 3)), ('initialized', (4, 5)), ('values', (6, 7)), ('uniform distribution U ? 1 ? d+t , 1 ? d+t', (10, 21)), ('tag embeddings', (49, 51))]","[['Embeddings', 'for', 'tags'], ['values', 'drawn from', 'uniform distribution U ? 1 ? d+t , 1 ? d+t']]",[],[],"[['Hyperparameters', 'has', 'Embeddings']]",coreference_resolution,4,174
176,hyperparameters,"The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .","[('of', (2, 3)), ('set to', (8, 10))]","[('size', (1, 2)), ('LSTMs hidden states', (4, 7)), ('{ 100 , qlog - U ( 30 , 150 ) }', (10, 22))]","[['size', 'of', 'LSTMs hidden states'], ['LSTMs hidden states', 'set to', '{ 100 , qlog - U ( 30 , 150 ) }']]",[],[],"[['Hyperparameters', 'has', 'size']]",coreference_resolution,4,177
177,hyperparameters,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .","[('initialized', (1, 2)), ('of', (5, 6)), ('with', (8, 9))]","[('weight matrices', (3, 5)), ('LSTMs', (7, 8)), ('random orthogonal matrices', (9, 12))]","[['weight matrices', 'of', 'LSTMs'], ['LSTMs', 'with', 'random orthogonal matrices']]",[],"[['Hyperparameters', 'initialized', 'weight matrices']]",[],coreference_resolution,4,178
178,hyperparameters,The first feed - forward layer size is set to a value in Optimization .,"[('set to', (8, 10)), ('in', (12, 13))]","[('first', (1, 2)), ('feed - forward layer size', (2, 7)), ('value', (11, 12)), ('Optimization', (13, 14))]","[['feed - forward layer size', 'set to', 'value'], ['value', 'in', 'Optimization']]","[['first', 'has', 'feed - forward layer size'], ['feed - forward layer size', 'has', 'value']]",[],"[['Hyperparameters', 'has', 'first']]",coreference_resolution,4,179
179,hyperparameters,"We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .","[('trained', (1, 2)), ('in', (4, 5)), ('using', (6, 7)), ('with', (15, 16))]","[('our model', (2, 4)), ('minibatches', (5, 6)), ('Adam ( Kingma and Ba , 2015 )', (7, 15)), ('learning rate', (17, 19)), ('10 ? 4', (20, 23)), ('maximal batch size', (24, 27)), ('64', (27, 28))]","[['our model', 'in', 'minibatches'], ['minibatches', 'using', 'Adam ( Kingma and Ba , 2015 )'], ['Adam ( Kingma and Ba , 2015 )', 'with', 'learning rate'], ['Adam ( Kingma and Ba , 2015 )', 'with', 'maximal batch size']]","[['learning rate', 'has', '10 ? 4'], ['maximal batch size', 'has', '64']]","[['Hyperparameters', 'trained', 'our model']]",[],coreference_resolution,4,180
180,hyperparameters,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .","[('clip', (1, 2)), ('by', (3, 4)), ('with', (7, 8)), ('in', (11, 12))]","[('gradients', (2, 3)), ('global norm', (4, 6)), ('clipping value', (9, 11)), ('{ 1.0 , U ( 1 , 100 ) }', (12, 22))]","[['gradients', 'by', 'global norm'], ['global norm', 'with', 'clipping value'], ['clipping value', 'in', '{ 1.0 , U ( 1 , 100 ) }']]",[],"[['Hyperparameters', 'clip', 'gradients']]",[],coreference_resolution,4,181
181,hyperparameters,We train for 10 epochs and choose the model that performs best on the devset .,"[('train for', (1, 3)), ('choose', (6, 7)), ('on', (12, 13))]","[('10 epochs', (3, 5)), ('model', (8, 9)), ('performs', (10, 11)), ('best', (11, 12)), ('devset', (14, 15))]","[['performs', 'on', 'devset'], ['best', 'on', 'devset']]","[['model', 'has', 'performs'], ['performs', 'has', 'best']]","[['Hyperparameters', 'train for', '10 epochs']]",[],coreference_resolution,4,182
182,hyperparameters,"We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) }.","[('used', (1, 2)), ('with', (7, 8))]","[('l 2 - regularization', (3, 7)), ('? ? { 10 ?5', (8, 13))]","[['l 2 - regularization', 'with', '? ? { 10 ?5']]","[['l 2 - regularization', 'has', '? ? { 10 ?5']]","[['Hyperparameters', 'used', 'l 2 - regularization']]",[],coreference_resolution,4,184
183,hyperparameters,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .","[('with', (1, 2)), ('applied to', (18, 20)), ('of', (22, 23))]","[('Dropout', (0, 1)), ('keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) }', (3, 17)), ('outputs', (21, 22)), ('LSTMs', (24, 25)), ('both feed - forward layers', (26, 31)), ('input', (35, 36)), ('k p ? U (0.8 , 1.0 )', (37, 45))]","[['Dropout', 'with', 'keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) }'], ['input', 'with', 'k p ? U (0.8 , 1.0 )'], ['keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) }', 'applied to', 'outputs'], ['outputs', 'of', 'LSTMs']]",[],[],"[['Hyperparameters', 'has', 'Dropout']]",coreference_resolution,4,185
184,results,6 Results and analysis 6.1 Results on shell noun resolution dataset provides the results of the mentionranking model ( MR - LSTM ) on the ASN corpus using default HPs .,"[('on', (23, 24)), ('using', (27, 28))]","[('shell noun resolution dataset', (7, 11)), ('mentionranking model ( MR - LSTM )', (16, 23)), ('ASN corpus', (25, 27)), ('default HPs', (28, 30))]","[['mentionranking model ( MR - LSTM )', 'on', 'ASN corpus'], ['ASN corpus', 'using', 'default HPs']]",[],[],[],coreference_resolution,4,186
185,results,"In terms of s@1 score , MR - LSTM outperforms both KZH13 's results and TAG BL without even necessitating HP tuning .","[('In terms of', (0, 3)), ('without even necessitating', (17, 20))]","[('s@1 score', (3, 5)), ('MR - LSTM', (6, 9)), ('outperforms', (9, 10)), (""KZH13 's results"", (11, 14)), ('TAG BL', (15, 17)), ('HP tuning', (20, 22))]","[['TAG BL', 'without even necessitating', 'HP tuning']]","[['s@1 score', 'has', 'MR - LSTM'], ['MR - LSTM', 'has', 'outperforms'], ['outperforms', 'has', ""KZH13 's results""]]","[['Results', 'In terms of', 's@1 score']]",[],coreference_resolution,4,188
186,results,"From we observe : ( 1 ) with HPs tuned on ARRAU - AA , we obtain results well beyond KZH13 , ( 2 ) all ablated model variants perform worse than the full model , ( 3 ) a large performance drop when omitting syntactic information ( tag , cut ) suggests that the model makes good use of it .","[('with', (7, 8)), ('tuned on', (9, 11)), ('obtain', (16, 17)), ('perform', (29, 30)), ('than', (31, 32)), ('when omitting', (43, 45))]","[('HPs', (8, 9)), ('ARRAU - AA', (11, 14)), ('results', (17, 18)), ('well beyond', (18, 20)), ('KZH13', (20, 21)), ('all ablated model variants', (25, 29)), ('worse', (30, 31)), ('full model', (33, 35)), ('large performance drop', (40, 43)), ('syntactic information ( tag , cut )', (45, 52))]","[['HPs', 'tuned on', 'ARRAU - AA'], ['HPs', 'obtain', 'results'], ['all ablated model variants', 'perform', 'worse'], ['worse', 'than', 'full model'], ['large performance drop', 'when omitting', 'syntactic information ( tag , cut )']]","[['results', 'has', 'well beyond'], ['well beyond', 'has', 'KZH13']]","[['Results', 'with', 'HPs']]",[],coreference_resolution,4,190
187,results,Results on the ARRAU corpus,"[('on', (1, 2))]",[],[],[],[],[],coreference_resolution,4,199
188,results,"The MR - LSTM is more successful in resolving nominal than pronominal anaphors , although the training data provides only pronominal ones .","[('in', (7, 8)), ('than', (10, 11))]","[('MR - LSTM', (1, 4)), ('more successful', (5, 7)), ('resolving', (8, 9)), ('nominal', (9, 10)), ('pronominal anaphors', (11, 13))]","[['more successful', 'in', 'resolving'], ['nominal', 'than', 'pronominal anaphors']]","[['MR - LSTM', 'has', 'more successful'], ['resolving', 'has', 'nominal']]",[],"[['Results', 'has', 'MR - LSTM']]",coreference_resolution,4,200
189,results,"Contrary to shell noun resolution , omitting syntactic information boosts performance in ARRAU - AA .","[('Contrary to', (0, 2)), ('omitting', (6, 7)), ('boosts', (9, 10)), ('in', (11, 12))]","[('syntactic information', (7, 9)), ('performance', (10, 11)), ('ARRAU - AA', (12, 15))]","[['syntactic information', 'boosts', 'performance'], ['performance', 'in', 'ARRAU - AA']]",[],"[['Results', 'Contrary to', 'syntactic information']]",[],coreference_resolution,4,206
190,results,"This is what we can observe from row 2 vs. row 6 in Table 5 : the MR - LSTM without context embedding ( ctx ) achieves a comparable s@ 2 score with the variant that omits syntactic information , but better s@3 - 4 scores .","[('achieves', (26, 27)), ('with', (32, 33))]","[('MR - LSTM without context embedding ( ctx )', (17, 26)), ('comparable s@ 2 score', (28, 32)), ('variant', (34, 35)), ('better s@3 - 4 scores', (41, 46))]","[['MR - LSTM without context embedding ( ctx )', 'achieves', 'comparable s@ 2 score'], ['comparable s@ 2 score', 'with', 'variant']]",[],[],[],coreference_resolution,4,209
191,research-problem,Learning Global Features for Coreference Resolution,[],[],[],[],[],[],coreference_resolution,5,2
192,model,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .","[('propose to', (4, 6)), ('of', (8, 9)), ('by embedding', (11, 13)), ('using', (15, 16))]","[('learn', (6, 7)), ('representations', (7, 8)), ('mention clusters', (9, 11)), ('sequentially', (14, 15)), ('recurrent neural network', (17, 20))]","[['representations', 'of', 'mention clusters'], ['representations', 'by embedding', 'sequentially'], ['sequentially', 'using', 'recurrent neural network']]","[['learn', 'has', 'representations']]","[['Model', 'propose to', 'learn']]",[],coreference_resolution,5,13
193,model,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .","[('learns', (11, 12)), ('from', (15, 16)), ('present in', (19, 21))]","[('manually defined cluster features', (4, 8)), ('global representation', (13, 15)), ('individual mentions', (17, 19)), ('each cluster', (21, 23))]","[['global representation', 'from', 'individual mentions'], ['individual mentions', 'present in', 'each cluster']]",[],[],"[['Model', 'has', 'manually defined cluster features']]",coreference_resolution,5,14
194,model,We incorporate these representations into a mention - ranking style coreference system .,"[('incorporate', (1, 2)), ('into', (4, 5))]","[('mention - ranking style coreference system', (6, 12))]",[],[],"[['Model', 'incorporate', 'mention - ranking style coreference system']]",[],coreference_resolution,5,15
195,model,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .","[('train', (1, 2)), ('as', (4, 5)), ('with', (8, 9))]","[('model', (3, 4)), ('local classifier', (6, 8)), ('fixed context', (9, 11))]","[['model', 'as', 'local classifier'], ['local classifier', 'with', 'fixed context']]","[['model', 'has', 'local classifier']]","[['Model', 'train', 'model']]",[],coreference_resolution,5,17
196,results,"In we present our main results on the CoNLL English test set , and compare with other recent stateof - the - art systems .","[('on', (6, 7))]","[('CoNLL English test set', (8, 12))]",[],[],"[['Results', 'on', 'CoNLL English test set']]",[],coreference_resolution,5,192
197,results,"We see a statistically significant improvement of over 0.8 Co NLL points over the previous state of the art , and the highest F 1 scores to date on all three CoNLL metrics .","[('see', (1, 2)), ('of', (6, 7)), ('over', (12, 13)), ('on', (28, 29))]","[('statistically significant improvement', (3, 6)), ('over 0.8 Co NLL points', (7, 12)), ('previous state of the art', (14, 19)), ('highest F 1 scores', (22, 26))]","[['statistically significant improvement', 'of', 'over 0.8 Co NLL points'], ['over 0.8 Co NLL points', 'over', 'previous state of the art']]",[],"[['Results', 'see', 'statistically significant improvement']]",[],coreference_resolution,5,193
198,results,We now consider in more detail the impact of global features and RNNs on performance .,"[('consider', (2, 3)), ('of', (8, 9)), ('on', (13, 14))]","[('impact', (7, 8)), ('global features and RNNs', (9, 13)), ('performance', (14, 15))]","[['impact', 'of', 'global features and RNNs'], ['global features and RNNs', 'on', 'performance']]",[],"[['Results', 'consider', 'impact']]",[],coreference_resolution,5,194
199,results,"In we see that the RNN improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .","[('see', (2, 3)), ('improves', (6, 7))]","[('RNN', (5, 6)), ('performance over all', (7, 10))]","[['RNN', 'improves', 'performance over all']]",[],"[['Results', 'see', 'RNN']]",[],coreference_resolution,5,203
200,results,"While WL errors also decrease for both these mention - categories under the RNN model , FN errors increase .","[('for', (5, 6)), ('under', (11, 12))]","[('WL errors', (1, 3)), ('decrease', (4, 5)), ('both', (6, 7)), ('RNN model', (13, 15)), ('FN errors', (16, 18)), ('increase', (18, 19))]","[['decrease', 'for', 'both']]","[['WL errors', 'has', 'decrease'], ['FN errors', 'has', 'increase']]",[],[],coreference_resolution,5,204
201,results,"Importantly , the RNN performance is significantly better than that of the Avg baseline , which barely improves over mention - ranking , even with oracle history .","[('than', (8, 9)), ('over', (18, 19)), ('even with', (23, 25))]","[('RNN performance', (3, 5)), ('significantly better', (6, 8)), ('Avg baseline', (12, 14)), ('barely improves', (16, 18)), ('mention - ranking', (19, 22)), ('oracle history', (25, 27))]","[['significantly better', 'than', 'Avg baseline'], ['barely improves', 'over', 'mention - ranking'], ['mention - ranking', 'even with', 'oracle history']]","[['RNN performance', 'has', 'significantly better']]",[],"[['Results', 'has', 'RNN performance']]",coreference_resolution,5,205
202,results,"We also note that while RNN performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting , we are still able to recover a significant portion of the possible performance improvement .","[('degrades', (7, 8)), ('in', (8, 9)), ('when moving from', (13, 16)), ('upperbound to', (19, 21)), ('able to recover', (28, 31)), ('of', (34, 35))]","[('RNN performance', (5, 7)), ('both precision and recall', (9, 13)), ('oracle history', (17, 19)), ('greedy setting', (22, 24)), ('significant portion', (32, 34)), ('possible performance improvement', (36, 39))]","[['both precision and recall', 'when moving from', 'oracle history'], ['oracle history', 'upperbound to', 'greedy setting'], ['significant portion', 'of', 'possible performance improvement']]","[['significant portion', 'has', 'possible performance improvement']]",[],[],coreference_resolution,5,207
203,research-problem,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution,[],[],[],[],[],[],coreference_resolution,6,2
204,research-problem,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .","[('present', (5, 6)), ('that learns', (10, 12)), ('for improving', (16, 18))]","[('word embedding model', (7, 10)), ('cross - sentence dependency', (12, 16)), ('end - to - end co-reference resolution ( E2E - CR )', (18, 30))]","[['word embedding model', 'that learns', 'cross - sentence dependency'], ['cross - sentence dependency', 'for improving', 'end - to - end co-reference resolution ( E2E - CR )']]",[],"[['Research problem', 'present', 'word embedding model']]",[],coreference_resolution,6,4
205,model,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .","[('propose', (46, 47)), ('for', (52, 53))]","[('cross - sentence encoder', (48, 52)), ('end - to - end co-reference ( E2E - CR )', (53, 64))]","[['cross - sentence encoder', 'for', 'end - to - end co-reference ( E2E - CR )']]",[],"[['Model', 'propose', 'cross - sentence encoder']]",[],coreference_resolution,6,24
206,model,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .","[('Borrowing', (0, 1)), ('of', (3, 4)), ('containing', (14, 15)), ('from', (19, 20)), ('added to', (23, 25))]","[('idea', (2, 3)), ('external memory module', (5, 8)), ('external memory block', (11, 14)), ('syntactic and semantic information', (15, 19)), ('context sentences', (20, 22)), ('standard LSTM model', (26, 29))]","[['idea', 'of', 'external memory module'], ['external memory block', 'containing', 'syntactic and semantic information'], ['syntactic and semantic information', 'from', 'context sentences'], ['syntactic and semantic information', 'added to', 'standard LSTM model']]","[['idea', 'has', 'external memory module'], ['external memory module', 'has', 'external memory block']]","[['Model', 'Borrowing', 'idea']]",[],coreference_resolution,6,25
207,model,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .","[('With', (0, 1)), ('as', (15, 16)), ('calculate', (21, 22)), ('of', (24, 25)), ('by taking', (27, 29)), ('into', (35, 36))]","[('context', (2, 3)), ('proposed', (7, 8)), ('encode', (12, 13)), ('input sentences', (13, 15)), ('batch', (17, 18)), ('representations', (23, 24)), ('input words', (25, 27)), ('target sentences and context sentences', (30, 35)), ('consideration', (36, 37))]","[['input sentences', 'as', 'batch'], ['proposed', 'calculate', 'representations'], ['representations', 'of', 'input words'], ['input words', 'by taking', 'target sentences and context sentences'], ['target sentences and context sentences', 'into', 'consideration']]","[['context', 'has', 'proposed'], ['encode', 'has', 'input sentences']]","[['Model', 'With', 'context']]",[],coreference_resolution,6,26
208,experiments,Language Representation Learning,[],[],[],[],[],[],coreference_resolution,6,39
209,research-problem,LSTMs with Cross - Sentence Attention,[],[],[],[],[],[],coreference_resolution,6,68
210,hyperparameters,"In practice , the LSTM modules applied in our model have 200 output units .","[('applied in', (6, 8)), ('have', (10, 11))]","[('LSTM modules', (4, 6)), ('our model', (8, 10)), ('200 output units', (11, 14))]","[['LSTM modules', 'applied in', 'our model'], ['LSTM modules', 'have', '200 output units'], ['our model', 'have', '200 output units']]",[],[],"[['Hyperparameters', 'has', 'LSTM modules']]",coreference_resolution,6,87
211,hyperparameters,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .","[('calculate', (4, 5)), ('using', (9, 10)), ('with', (13, 14)), ('consisting of', (17, 19))]","[('ASL', (1, 2)), ('cross - sentence dependency', (5, 9)), ('multilayer perceptron', (11, 13)), ('one hidden layer', (14, 17)), ('150 hidden units', (19, 22))]","[['ASL', 'calculate', 'cross - sentence dependency'], ['cross - sentence dependency', 'using', 'multilayer perceptron'], ['multilayer perceptron', 'with', 'one hidden layer'], ['one hidden layer', 'consisting of', '150 hidden units']]",[],[],[],coreference_resolution,6,88
212,hyperparameters,The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,"[('set as', (5, 7)), ('every', (12, 13))]","[('initial learning rate', (1, 4)), ('0.001', (7, 8)), ('decays', (9, 10)), ('0.001 %', (10, 12)), ('100 steps', (13, 15))]","[['initial learning rate', 'set as', '0.001'], ['0.001 %', 'every', '100 steps']]","[['initial learning rate', 'has', '0.001'], ['decays', 'has', '0.001 %']]",[],"[['Hyperparameters', 'has', 'initial learning rate']]",coreference_resolution,6,89
213,hyperparameters,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .","[('optimized with', (3, 5))]","[('Adam algorithm', (6, 8))]",[],[],"[['Hyperparameters', 'optimized with', 'Adam algorithm']]",[],coreference_resolution,6,90
214,hyperparameters,"In co-reference prediction , we select 250 candidate antecedents as our baseline model .","[('In', (0, 1)), ('select', (5, 6)), ('as', (9, 10))]","[('co-reference prediction', (1, 3)), ('250 candidate antecedents', (6, 9)), ('our baseline model', (10, 13))]","[['co-reference prediction', 'select', '250 candidate antecedents'], ['250 candidate antecedents', 'as', 'our baseline model']]",[],"[['Hyperparameters', 'In', 'co-reference prediction']]",[],coreference_resolution,6,92
215,results,"Comparing with the baseline model that achieved 67.2 % F1 score , the ASL model improved the performance by 0.6 % and achieved 67.8 % average F1 .","[('Comparing with', (0, 2)), ('achieved', (6, 7)), ('improved', (15, 16)), ('by', (18, 19)), ('achieved', (22, 23))]","[('baseline model', (3, 5)), ('67.2 % F1 score', (7, 11)), ('ASL model', (13, 15)), ('performance', (17, 18)), ('0.6 %', (19, 21)), ('67.8 % average F1', (23, 27))]","[['baseline model', 'achieved', '67.2 % F1 score'], ['ASL model', 'improved', 'performance'], ['performance', 'by', '0.6 %'], ['ASL model', 'achieved', '67.8 % average F1']]","[['baseline model', 'has', '67.2 % F1 score']]","[['Results', 'Comparing with', 'baseline model']]",[],coreference_resolution,6,97
216,results,- Uh- huh .,[],"[('Uh- huh', (1, 3))]",[],[],[],"[['Results', 'has', 'Uh- huh']]",coreference_resolution,6,100
217,results,"show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .","[('show', (0, 1)), ('consider', (5, 6)), ('encodes', (17, 18)), ('from', (20, 21))]","[('cross - sentence dependency', (6, 10)), ('significantly outperform', (10, 12)), ('baseline model', (13, 15)), ('each sentence', (18, 20)), ('input document', (22, 24))]","[['significantly outperform', 'encodes', 'each sentence'], ['baseline model', 'encodes', 'each sentence'], ['each sentence', 'from', 'input document']]","[['cross - sentence dependency', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline model']]","[['Results', 'show', 'cross - sentence dependency']]",[],coreference_resolution,6,108
218,results,"With the proposed context gate , ASL takes knowledge from context sentences if local inputs are not informative enough .","[('takes knowledge', (7, 9)), ('from', (9, 10)), ('if', (12, 13)), ('are', (15, 16))]","[('ASL', (6, 7)), ('context sentences', (10, 12)), ('local inputs', (13, 15)), ('not informative enough', (16, 19))]","[['local inputs', 'are', 'not informative enough']]",[],[],"[['Results', 'has', 'ASL']]",coreference_resolution,6,122
219,research-problem,Coreference Resolution with Entity Equalization,[],[],[],[],[],[],coreference_resolution,7,2
220,research-problem,"The problem of entity - level representation ( also referred to as high - order coreference models ) has attracted considerable interest recently , with methods ranging from imitation learning to iterative refinement .",[],"[('entity - level representation', (3, 7)), ('high', (12, 13))]",[],[],[],[],coreference_resolution,7,16
221,model,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .","[('provides', (6, 7)), ('facilitates', (21, 22))]","[('entity - level representation', (8, 12)), ('end - to - end optimization', (22, 28))]","[['entity - level representation', 'facilitates', 'end - to - end optimization']]",[],"[['Model', 'provides', 'entity - level representation']]",[],coreference_resolution,7,18
222,model,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .","[('motivated by', (15, 17)), ('of', (21, 22))]","[('BERT embeddings', (12, 14)), ('impressive empirical performance', (18, 21)), ('BERT', (22, 23))]","[['BERT embeddings', 'motivated by', 'impressive empirical performance'], ['impressive empirical performance', 'of', 'BERT']]",[],[],[],coreference_resolution,7,23
223,model,We show that this can be done by using BERT in a fully convolutional manner .,"[('done by using', (6, 9)), ('in', (10, 11))]","[('BERT', (9, 10)), ('fully convolutional manner', (12, 15))]","[['BERT', 'in', 'fully convolutional manner']]",[],"[['Model', 'done by using', 'BERT']]",[],coreference_resolution,7,25
224,model,"Our work is the first to use BERT for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .","[('first to use', (4, 7)), ('for', (8, 9)), ('demonstrate', (17, 18)), ('results in', (20, 22)), ('over', (24, 25))]","[('BERT', (7, 8)), ('task of coreference resolution', (10, 14)), ('significant improvement', (22, 24)), ('current state - of - the - art', (25, 33))]","[['BERT', 'for', 'task of coreference resolution'], ['significant improvement', 'over', 'current state - of - the - art']]",[],"[['Model', 'first to use', 'BERT']]",[],coreference_resolution,7,26
225,baselines,Using BERT,"[('Using', (0, 1))]",[],[],[],[],[],coreference_resolution,7,81
226,results,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg.","[('is', (2, 3)), ('from with', (8, 10)), ('achieves', (21, 22))]","[('span - ranking model', (4, 8)), ('ELMo input features and second - order span representations', (10, 19))]","[['span - ranking model', 'from with', 'ELMo input features and second - order span representations']]",[],[],[],coreference_resolution,7,119
227,results,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,"[('Replacing', (2, 3)), ('with', (6, 7)), ('achieves', (9, 10))]","[('F1', (0, 1)), ('ELMo features', (4, 6)), ('BERT features', (7, 9)), ('76. 25 % average F1', (10, 15))]","[['F1', 'Replacing', 'ELMo features'], ['ELMo features', 'with', 'BERT features'], ['ELMo features', 'achieves', '76. 25 % average F1'], ['BERT features', 'achieves', '76. 25 % average F1']]",[],[],[],coreference_resolution,7,120
228,results,"Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .","[('Removing', (0, 1)), ('while using', (8, 10)), ('achieves', (12, 13)), ('achieving', (17, 18)), ('on', (23, 24))]","[('second - order span - representations', (2, 8)), ('BERT features', (10, 12)), ('76.37 % F1', (13, 16)), ('higher recall and lower precision', (18, 23)), ('all evaluation metrics', (24, 27))]","[['second - order span - representations', 'while using', 'BERT features'], ['BERT features', 'achieves', '76.37 % F1'], ['76.37 % F1', 'achieving', 'higher recall and lower precision'], ['higher recall and lower precision', 'on', 'all evaluation metrics']]",[],"[['Results', 'Removing', 'second - order span - representations']]",[],coreference_resolution,7,121
229,results,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .","[('Replacing', (0, 1)), ('achieves', (7, 8)), ('consistently achieving', (16, 18))]","[('secondorder span representations', (1, 4)), ('Entity Equalization', (5, 7)), ('76. 64 % average F1', (8, 13)), ('highest F 1 score', (19, 23))]","[['secondorder span representations', 'achieves', '76. 64 % average F1'], ['Entity Equalization', 'achieves', '76. 64 % average F1'], ['secondorder span representations', 'consistently achieving', 'highest F 1 score']]",[],"[['Results', 'Replacing', 'secondorder span representations']]",[],coreference_resolution,7,122
230,results,"Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .","[('set', (2, 3)), ('for', (9, 10)), ('improving', (13, 14)), ('by', (20, 21))]","[('new state of the art', (4, 9)), ('coreference resolution', (10, 12)), ('previous state of the art', (15, 20)), ('3.6 % average F1', (21, 25))]","[['new state of the art', 'for', 'coreference resolution'], ['new state of the art', 'improving', 'previous state of the art'], ['previous state of the art', 'by', '3.6 % average F1']]",[],"[['Results', 'set', 'new state of the art']]",[],coreference_resolution,7,123
231,research-problem,End - to - end Neural Coreference Resolution,[],"[('End - to - end', (0, 5))]",[],[],[],[],coreference_resolution,8,2
232,research-problem,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .,"[('introduce', (1, 2)), ('show', (13, 14)), ('without using', (21, 23))]","[('first end - to - end coreference resolution model', (3, 12)), ('significantly outperforms', (16, 18)), ('all previous work', (18, 21)), ('syntactic parser or handengineered mention detector', (24, 30))]","[['significantly outperforms', 'without using', 'syntactic parser or handengineered mention detector'], ['all previous work', 'without using', 'syntactic parser or handengineered mention detector']]","[['first end - to - end coreference resolution model', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all previous work']]","[['Research problem', 'introduce', 'first end - to - end coreference resolution model']]",[],coreference_resolution,8,4
233,model,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .,"[('present', (1, 2)), ('learned', (17, 18)), ('given', (21, 22))]","[('first state - of - the - art neural coreference resolution model', (3, 15)), ('end - toend', (18, 21)), ('only gold mention clusters', (22, 26))]","[['first state - of - the - art neural coreference resolution model', 'learned', 'end - toend'], ['end - toend', 'given', 'only gold mention clusters']]",[],"[['Model', 'present', 'first state - of - the - art neural coreference resolution model']]",[],coreference_resolution,8,10
234,model,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .","[('demonstrate', (1, 2)), ('by training', (24, 26)), ('jointly learns', (35, 37)), ('are', (39, 40))]","[('first time', (4, 6)), ('resources', (8, 9)), ('not required', (10, 12)), ('performance', (16, 17)), ('improved significantly', (19, 21)), ('end - to - end neural model', (27, 34)), ('which spans', (37, 39)), ('entity mentions', (40, 42)), ('best cluster', (45, 47))]","[['improved significantly', 'by training', 'end - to - end neural model'], ['end - to - end neural model', 'jointly learns', 'which spans'], ['which spans', 'are', 'entity mentions']]","[['first time', 'has', 'resources'], ['resources', 'has', 'not required'], ['performance', 'has', 'improved significantly'], ['which spans', 'has', 'entity mentions']]","[['Model', 'demonstrate', 'first time']]",[],coreference_resolution,8,12
235,model,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,"[('over', (3, 4)), ('spans up to', (8, 11)), ('directly optimizes', (15, 17)), ('of', (20, 21)), ('from', (23, 24))]","[('Our model reasons', (0, 3)), ('maximum length', (12, 14)), ('marginal likelihood', (18, 20)), ('gold coreference clusters', (24, 27))]","[['Our model reasons', 'directly optimizes', 'marginal likelihood']]",[],[],"[['Model', 'has', 'Our model reasons']]",coreference_resolution,8,13
236,model,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .","[('includes', (1, 2)), ('decides', (8, 9)), ('for', (10, 11)), ('which of', (14, 16)), ('is', (23, 24))]","[('span - ranking model', (3, 7)), ('each span', (11, 13)), ('previous spans', (17, 19)), ('good antecedent', (25, 27))]","[['each span', 'which of', 'previous spans'], ['each span', 'is', 'good antecedent'], ['previous spans', 'is', 'good antecedent']]",[],"[['Model', 'includes', 'span - ranking model']]",[],coreference_resolution,8,14
237,model,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .","[('are', (6, 7)), ('representing', (9, 10)), ('in', (13, 14)), ('combine', (18, 19)), ('with', (24, 25)), ('over', (31, 32))]","[('vector embeddings', (7, 9)), ('spans of text', (10, 13)), ('document', (15, 16)), ('context - dependent boundary representations', (19, 24)), ('head - finding attention mechanism', (26, 31)), ('span', (33, 34))]","[['vector embeddings', 'representing', 'spans of text'], ['spans of text', 'in', 'document'], ['vector embeddings', 'combine', 'context - dependent boundary representations'], ['context - dependent boundary representations', 'with', 'head - finding attention mechanism'], ['head - finding attention mechanism', 'over', 'span']]",[],"[['Model', 'are', 'vector embeddings']]",[],coreference_resolution,8,15
238,model,The head - finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions .,"[('reveals', (7, 8)), ('contribute most to', (11, 14))]","[('head - finding attention mechanism', (1, 6)), ('mentioninternal words', (9, 11)), ('coreference decisions', (14, 16))]","[['head - finding attention mechanism', 'reveals', 'mentioninternal words'], ['mentioninternal words', 'contribute most to', 'coreference decisions']]",[],[],"[['Model', 'has', 'head - finding attention mechanism']]",coreference_resolution,8,24
239,hyperparameters,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .","[('of', (6, 7))]","[('word embeddings area fixed concatenation', (1, 6)), ('300 - dimensional GloVe embeddings', (7, 12)), ('50 - dimensional embeddings', (13, 17)), ('normalized', (20, 21)), ('unit vectors', (23, 25))]","[['word embeddings area fixed concatenation', 'of', '300 - dimensional GloVe embeddings'], ['word embeddings area fixed concatenation', 'of', '50 - dimensional embeddings']]",[],[],"[['Hyperparameters', 'has', 'word embeddings area fixed concatenation']]",coreference_resolution,8,118
240,hyperparameters,The hidden states in the LSTMs have 200 dimensions .,"[('in', (3, 4)), ('have', (6, 7))]","[('hidden states', (1, 3)), ('LSTMs', (5, 6)), ('200 dimensions', (7, 9))]","[['hidden states', 'in', 'LSTMs'], ['hidden states', 'have', '200 dimensions'], ['LSTMs', 'have', '200 dimensions']]",[],[],"[['Hyperparameters', 'has', 'hidden states']]",coreference_resolution,8,123
241,hyperparameters,We encode speaker information as a binary feature indicating whether a pair of spans are from the same speaker .,"[('encode', (1, 2)), ('as', (4, 5)), ('indicating whether', (8, 10)), ('are', (14, 15)), ('from', (15, 16))]","[('speaker information', (2, 4)), ('binary feature', (6, 8)), ('pair of spans', (11, 14))]","[['speaker information', 'as', 'binary feature'], ['binary feature', 'indicating whether', 'pair of spans']]",[],"[['Hyperparameters', 'encode', 'speaker information']]",[],coreference_resolution,8,126
242,hyperparameters,"All features ( speaker , genre , span distance , mention width ) are represented as learned 20 - dimensional embeddings .","[('represented as', (14, 16))]","[('features ( speaker , genre , span distance , mention width )', (1, 13)), ('learned 20 - dimensional embeddings', (16, 21))]","[['features ( speaker , genre , span distance , mention width )', 'represented as', 'learned 20 - dimensional embeddings']]",[],[],"[['Hyperparameters', 'has', 'features ( speaker , genre , span distance , mention width )']]",coreference_resolution,8,128
243,hyperparameters,"We prune the spans such that the maximum span width L = 10 , the number of spans per word ? = 0.4 , and the maximum number of antecedents K = 250 .","[('prune', (1, 2)), ('such that', (4, 6)), ('=', (11, 12))]","[('spans', (3, 4)), ('maximum span width L', (7, 11)), ('10', (12, 13)), ('number of spans per word ?', (15, 21)), ('0.4', (22, 23)), ('maximum number of antecedents K', (26, 31))]","[['spans', 'such that', 'maximum span width L'], ['spans', 'such that', 'maximum number of antecedents K'], ['maximum span width L', '=', '10']]","[['spans', 'has', 'maximum span width L'], ['maximum span width L', 'has', '10'], ['number of spans per word ?', 'has', '0.4']]","[['Hyperparameters', 'prune', 'spans']]",[],coreference_resolution,8,130
244,hyperparameters,We use ADAM for learning with a minibatch size of 1 .,"[('use', (1, 2)), ('for', (3, 4)), ('with', (5, 6)), ('of', (9, 10))]","[('ADAM', (2, 3)), ('learning', (4, 5)), ('minibatch size', (7, 9)), ('1', (10, 11))]","[['ADAM', 'for', 'learning'], ['learning', 'with', 'minibatch size'], ['minibatch size', 'of', '1']]",[],"[['Hyperparameters', 'use', 'ADAM']]",[],coreference_resolution,8,133
245,hyperparameters,We apply 0.2 dropout to all hidden layers and feature embeddings .,"[('apply', (1, 2)), ('to', (4, 5))]","[('0.2 dropout', (2, 4)), ('all hidden layers and feature embeddings', (5, 11))]","[['0.2 dropout', 'to', 'all hidden layers and feature embeddings']]",[],"[['Hyperparameters', 'apply', '0.2 dropout']]",[],coreference_resolution,8,136
246,hyperparameters,The learning rate is decayed by 0.1 % every 100 steps .,"[('by', (5, 6)), ('every', (8, 9))]","[('learning rate', (1, 3)), ('decayed', (4, 5)), ('0.1 %', (6, 8)), ('100 steps', (9, 11))]","[['decayed', 'by', '0.1 %'], ['0.1 %', 'every', '100 steps']]","[['learning rate', 'has', 'decayed']]",[],"[['Hyperparameters', 'has', 'learning rate']]",coreference_resolution,8,138
247,hyperparameters,Ensembling is performed for both the span pruning and antecedent decisions .,"[('performed for both', (2, 5))]","[('Ensembling', (0, 1)), ('span pruning', (6, 8)), ('antecedent decisions', (9, 11))]","[['Ensembling', 'performed for both', 'span pruning'], ['Ensembling', 'performed for both', 'antecedent decisions']]",[],[],"[['Hyperparameters', 'has', 'Ensembling']]",coreference_resolution,8,144
248,results,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .","[('improves', (6, 7)), ('by', (17, 18)), ('improves', (26, 27)), ('by', (28, 29))]","[('our single model', (3, 6)), ('state - of - the - art average F1', (8, 17)), ('1.5', (18, 19)), ('our 5 - model ensemble', (21, 26)), ('3.1', (29, 30))]","[['our single model', 'improves', 'state - of - the - art average F1'], ['our single model', 'improves', 'our 5 - model ensemble'], ['state - of - the - art average F1', 'by', '1.5']]",[],[],[],coreference_resolution,8,154
249,results,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .","[('come from', (4, 6)), ('in', (7, 8))]","[('most significant gains', (1, 4)), ('improvements', (6, 7)), ('recall', (8, 9))]","[['most significant gains', 'come from', 'improvements'], ['improvements', 'in', 'recall']]",[],[],"[['Results', 'has', 'most significant gains']]",coreference_resolution,8,155
250,ablation-analysis,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .","[('are', (9, 10)), ('for', (12, 13))]","[('distance between', (1, 3)), ('spans and the width of spans', (3, 9)), ('crucial signals', (10, 12)), ('coreference resolution', (13, 15))]","[['spans and the width of spans', 'are', 'crucial signals'], ['crucial signals', 'for', 'coreference resolution']]","[['distance between', 'has', 'spans and the width of spans']]",[],"[['Ablation analysis', 'has', 'distance between']]",coreference_resolution,8,162
251,ablation-analysis,"With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .","[('With', (0, 1)), ('see', (5, 6)), ('of', (8, 9)), ('suggesting', (12, 13))]","[('oracle mentions', (1, 3)), ('improvement', (7, 8)), ('17.5 F1', (9, 11))]","[['oracle mentions', 'see', 'improvement'], ['improvement', 'of', '17.5 F1']]","[['oracle mentions', 'has', 'improvement']]","[['Ablation analysis', 'With', 'oracle mentions']]",[],coreference_resolution,8,183
252,ablation-analysis,Mention Precision,[],[],[],[],[],[],coreference_resolution,8,194
253,ablation-analysis,"For spans with 2 - 5 words , 75 - 90 % of the predictions are constituents , indicating that the vast majority of the mentions are syntactically plausible .","[('For', (0, 1)), ('with', (2, 3)), ('of', (12, 13)), ('are', (15, 16))]","[('spans', (1, 2)), ('2 - 5 words', (3, 7)), ('75 - 90 %', (8, 12)), ('predictions', (14, 15)), ('constituents', (16, 17))]","[['spans', 'with', '2 - 5 words'], ['75 - 90 %', 'of', 'predictions'], ['predictions', 'are', 'constituents']]","[['spans', 'has', '2 - 5 words'], ['2 - 5 words', 'has', '75 - 90 %']]","[['Ablation analysis', 'For', 'spans']]",[],coreference_resolution,8,198
254,research-problem,BERT for Coreference Resolution : Baselines and Analysis,[],"[('Coreference Resolution', (2, 4))]",[],[],[],[],coreference_resolution,9,2
255,research-problem,"We apply BERT to coreference resolution , achieving strong improvements on the OntoNotes ( + 3.9 F1 ) and GAP ( + 11.5 F1 ) benchmarks .","[('apply', (1, 2)), ('to', (3, 4)), ('achieving', (7, 8)), ('on', (10, 11))]","[('BERT', (2, 3)), ('coreference resolution', (4, 6)), ('strong improvements', (8, 10)), ('OntoNotes ( + 3.9 F1 ) and', (12, 19)), ('GAP ( + 11.5 F1 ) benchmarks', (19, 26))]","[['BERT', 'to', 'coreference resolution'], ['BERT', 'achieving', 'strong improvements'], ['coreference resolution', 'achieving', 'strong improvements'], ['strong improvements', 'on', 'OntoNotes ( + 3.9 F1 ) and'], ['strong improvements', 'on', 'GAP ( + 11.5 F1 ) benchmarks']]",[],"[['Research problem', 'apply', 'BERT']]",[],coreference_resolution,9,4
256,model,We present two ways of extending the c 2f - coref model in .,"[('present', (1, 2)), ('of', (4, 5))]","[('two ways', (2, 4)), ('extending', (5, 6)), ('c 2f - coref model', (7, 12))]","[['two ways', 'of', 'extending']]","[['two ways', 'has', 'extending'], ['extending', 'has', 'c 2f - coref model']]","[['Model', 'present', 'two ways']]",[],coreference_resolution,9,12
257,model,The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT .,"[('uses', (3, 4)), ('acts as', (9, 11)), ('for', (14, 15))]","[('independent variant', (1, 3)), ('non-overlapping segments', (4, 6)), ('independent instance', (12, 14)), ('BERT', (15, 16))]","[['independent variant', 'uses', 'non-overlapping segments'], ['non-overlapping segments', 'acts as', 'independent instance'], ['independent instance', 'for', 'BERT']]",[],[],"[['Model', 'has', 'independent variant']]",coreference_resolution,9,13
258,model,The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens .,"[('splits', (3, 4)), ('into', (6, 7)), ('so as to provide', (9, 13)), ('with', (15, 16)), ('beyond', (17, 18))]","[('overlap variant', (1, 3)), ('document', (5, 6)), ('overlapping segments', (7, 9)), ('model', (14, 15)), ('context', (16, 17)), ('512 tokens', (18, 20))]","[['overlap variant', 'splits', 'document'], ['document', 'into', 'overlapping segments'], ['overlapping segments', 'so as to provide', 'model'], ['model', 'with', 'context'], ['context', 'beyond', '512 tokens']]",[],[],"[['Model', 'has', 'overlap variant']]",coreference_resolution,9,14
259,code,1 https://github.com/mandarjoshi90/coref,[],[],[],[],[],[],coreference_resolution,9,16
260,experiments,We also find that BERT - large benefits from using longer context windows ( 384 word pieces ) while BERT - base performs better with shorter contexts ( 128 word pieces ) .,"[('find', (2, 3)), ('benefits from using', (7, 10)), ('performs', (22, 23)), ('with', (24, 25))]","[('BERT - large', (4, 7)), ('longer context windows ( 384 word pieces )', (10, 18)), ('BERT - base', (19, 22)), ('better', (23, 24)), ('shorter contexts ( 128 word pieces )', (25, 32))]","[['BERT - large', 'benefits from using', 'longer context windows ( 384 word pieces )'], ['BERT - base', 'performs', 'better'], ['better', 'with', 'shorter contexts ( 128 word pieces )']]",[],[],[],coreference_resolution,9,21
261,baselines,We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,"[('extend', (1, 2)), ('of', (6, 7))]","[('original Tensorflow implementations', (3, 6)), ('c 2f - coref 3 and BERT', (7, 14))]","[['original Tensorflow implementations', 'of', 'c 2f - coref 3 and BERT']]",[],[],[],coreference_resolution,9,63
262,hyperparameters,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .","[('fine tune', (1, 3)), ('on', (5, 6)), ('for', (10, 11)), ('using', (13, 14)), ('of', (16, 17)), ('of', (22, 23)), ('with', (31, 32)), ('for', (34, 35))]","[('all models', (3, 5)), ('OntoNotes English data', (7, 10)), ('20 epochs', (11, 13)), ('dropout', (15, 16)), ('0.3', (17, 18)), ('learning rates', (20, 22)), ('1 10 ?5 and 2 10 ? 4', (23, 31)), ('linear decay', (32, 34)), ('BERT parameters', (36, 38)), ('task parameters', (40, 42))]","[['all models', 'on', 'OntoNotes English data'], ['OntoNotes English data', 'for', '20 epochs'], ['20 epochs', 'using', 'dropout'], ['20 epochs', 'using', 'learning rates'], ['dropout', 'of', '0.3'], ['learning rates', 'of', '1 10 ?5 and 2 10 ? 4'], ['1 10 ?5 and 2 10 ? 4', 'with', 'linear decay'], ['1 10 ?5 and 2 10 ? 4', 'with', 'task parameters'], ['linear decay', 'for', 'BERT parameters']]","[['dropout', 'has', '0.3']]","[['Hyperparameters', 'fine tune', 'all models']]",[],coreference_resolution,9,64
263,results,"We trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for BERT - base and BERT - large respectively .","[('trained', (1, 2)), ('with', (4, 5)), ('of', (8, 9)), ('trained on', (20, 22)), ('performed', (27, 28)), ('for', (30, 31))]","[('separate models', (2, 4)), ('max segment len', (5, 8)), ('128 , 256 , 384 , and 512', (9, 17)), ('128 and 384 word pieces', (22, 27)), ('best', (29, 30)), ('BERT - base', (31, 34))]","[['separate models', 'with', 'max segment len'], ['max segment len', 'of', '128 , 256 , 384 , and 512'], ['128 and 384 word pieces', 'performed', 'best'], ['best', 'for', 'BERT - base']]",[],"[['Results', 'trained', 'separate models']]",[],coreference_resolution,9,66
264,ablation-analysis,"In addition to being more computationally efficient than e2e -coref , c2 f - coref iteratively refines span representations using attention for higher - order reasoning .","[('refines', (16, 17)), ('using', (19, 20)), ('for', (21, 22))]","[('span representations', (17, 19)), ('attention', (20, 21)), ('higher - order reasoning', (22, 26))]","[['span representations', 'using', 'attention'], ['attention', 'for', 'higher - order reasoning']]",[],"[['Ablation analysis', 'refines', 'span representations']]",[],coreference_resolution,9,69
265,hyperparameters,GAP ) is a human - labeled corpus of ambiguous pronoun - name pairs derived from Wikipedia snippets .,"[('is', (2, 3)), ('of', (8, 9)), ('derived from', (14, 16))]","[('GAP', (0, 1)), ('human - labeled corpus', (4, 8)), ('ambiguous pronoun - name pairs', (9, 14)), ('Wikipedia snippets', (16, 18))]","[['GAP', 'is', 'human - labeled corpus'], ['human - labeled corpus', 'of', 'ambiguous pronoun - name pairs'], ['ambiguous pronoun - name pairs', 'derived from', 'Wikipedia snippets']]","[['GAP', 'has', 'human - labeled corpus']]",[],"[['Hyperparameters', 'has', 'GAP']]",coreference_resolution,9,71
266,results,"Examples in the GAP dataset fit within a single BERT segment , thus eliminating the need for cross - segment inference .","[('in', (1, 2)), ('fit within', (5, 7)), ('eliminating', (13, 14))]","[('GAP dataset', (3, 5)), ('single BERT segment', (8, 11)), ('cross - segment inference', (17, 21))]","[['GAP dataset', 'fit within', 'single BERT segment']]",[],"[['Results', 'in', 'GAP dataset']]",[],coreference_resolution,9,72
267,experiments,"Following , we trained our BERT - based c 2f - coref model on OntoNotes .","[('trained', (3, 4)), ('on', (13, 14))]","[('BERT - based c 2f - coref model', (5, 13)), ('OntoNotes', (14, 15))]","[['BERT - based c 2f - coref model', 'on', 'OntoNotes']]",[],[],[],coreference_resolution,9,73
268,results,Table 2 shows that BERT improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .,"[('shows', (2, 3)), ('improves', (5, 6)), ('by', (11, 12)), ('for', (17, 18))]","[('BERT', (4, 5)), ('c 2 f - coref', (6, 11)), ('9 % and 11.5 %', (12, 17)), ('base and large models', (19, 23))]","[['BERT', 'improves', 'c 2 f - coref'], ['c 2 f - coref', 'by', '9 % and 11.5 %'], ['9 % and 11.5 %', 'for', 'base and large models']]",[],"[['Results', 'shows', 'BERT']]",[],coreference_resolution,9,76
269,results,Document Level : OntoNotes,[],[],[],[],[],[],coreference_resolution,9,78
270,results,shows that BERT - base offers an improvement of 0.9 % over the ELMo - based c2 fcoref model .,"[('shows', (0, 1)), ('offers', (5, 6)), ('of', (8, 9)), ('over', (11, 12))]","[('BERT - base', (2, 5)), ('improvement', (7, 8)), ('0.9 %', (9, 11)), ('ELMo - based c2 fcoref model', (13, 19))]","[['BERT - base', 'offers', 'improvement'], ['improvement', 'of', '0.9 %'], ['0.9 %', 'over', 'ELMo - based c2 fcoref model']]","[['BERT - base', 'has', 'improvement']]","[['Results', 'shows', 'BERT - base']]",[],coreference_resolution,9,83
271,experiments,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .","[('improves', (6, 7)), ('by', (12, 13)), ('of', (17, 18))]","[('BERT - large', (0, 3)), ('c 2 f - coref', (7, 12)), ('much larger margin', (14, 17)), ('3.9 %', (18, 20))]","[['BERT - large', 'improves', 'c 2 f - coref'], ['c 2 f - coref', 'by', 'much larger margin'], ['much larger margin', 'of', '3.9 %']]",[],[],[],coreference_resolution,9,87
272,results,We also observe that the overlap variant offers no improvement over independent .,"[('observe', (2, 3)), ('offers', (7, 8)), ('over', (10, 11))]","[('overlap variant', (5, 7)), ('no improvement', (8, 10)), ('independent', (11, 12))]","[['overlap variant', 'offers', 'no improvement'], ['no improvement', 'over', 'independent']]","[['overlap variant', 'has', 'no improvement']]","[['Results', 'observe', 'overlap variant']]",[],coreference_resolution,9,88
273,results,"Also concurrent , Span BERT , another self - supervised method , pretrains span representations achieving state of the art results ( Avg. F1 79.6 ) with the independent variant .","[('pretrains', (12, 13)), ('achieving', (15, 16)), ('with', (26, 27))]","[('Span BERT', (3, 5)), ('span representations', (13, 15)), ('state of the art results ( Avg. F1 79.6 )', (16, 26)), ('independent variant', (28, 30))]","[['span representations', 'achieving', 'state of the art results ( Avg. F1 79.6 )'], ['state of the art results ( Avg. F1 79.6 )', 'with', 'independent variant']]",[],[],[],coreference_resolution,9,91
274,results,"BERT - large improves over BERT - base in a variety of ways including pronoun resolution and lexical matching ( e.g. , racetrack and track ) .","[('improves over', (3, 5)), ('including', (13, 14))]","[('BERT - large', (0, 3)), ('BERT - base', (5, 8)), ('pronoun resolution', (14, 16)), ('lexical matching', (17, 19))]","[['BERT - large', 'improves over', 'BERT - base']]",[],[],"[['Results', 'has', 'BERT - large']]",coreference_resolution,9,98
275,experiments,Longer documents in OntoNotes generally contain larger and more spread - out clusters .,"[('in', (2, 3)), ('contain', (5, 6))]","[('Longer documents', (0, 2)), ('OntoNotes', (3, 4)), ('larger and more spread - out clusters', (6, 13))]","[['Longer documents', 'in', 'OntoNotes'], ['Longer documents', 'contain', 'larger and more spread - out clusters'], ['OntoNotes', 'contain', 'larger and more spread - out clusters']]",[],[],[],coreference_resolution,9,104
276,experiments,These observations suggest that future research in pretraining methods should look at more effectively encoding document - level context using sparse representations .,"[('using', (19, 20))]","[('more effectively encoding document - level context', (12, 19)), ('sparse representations', (20, 22))]","[['more effectively encoding document - level context', 'using', 'sparse representations']]",[],[],[],coreference_resolution,9,109
277,research-problem,A Hierarchical Model for Data - to - Text Generation,[],[],[],[],[],[],data-to-text_generation,0,2
278,research-problem,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .",[],"[('Transcribing structured data into natural language descriptions', (0, 7))]",[],[],[],[],data-to-text_generation,0,4
279,research-problem,Recent datato - text models mostly rely on an encoder - decoder architecture in which the data - structure is first encoded sequentially into a fixed - size vectorial representation by an encoder .,[],[],[],[],[],[],data-to-text_generation,0,19
280,model,"Then , a decoder generates words conditioned on this representation .","[('generates', (4, 5)), ('conditioned on', (6, 8))]","[('decoder', (3, 4)), ('words', (5, 6))]","[['decoder', 'generates', 'words']]",[],[],"[['Model', 'has', 'decoder']]",data-to-text_generation,0,20
281,model,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .","[('propose', (6, 7)), ('assuming that', (13, 15)), ('should be', (16, 18))]","[('new structured - data encoder', (8, 13)), ('structures', (15, 16)), ('hierarchically captured', (18, 20))]","[['new structured - data encoder', 'assuming that', 'structures'], ['structures', 'should be', 'hierarchically captured']]",[],"[['Model', 'propose', 'new structured - data encoder']]",[],data-to-text_generation,0,36
282,model,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .","[('focuses on', (2, 4)), ('of', (6, 7)), ('chosen to be', (16, 19))]","[('encoding', (5, 6)), ('data - structure', (8, 11)), ('decoder', (14, 15)), ('classical module', (20, 22))]","[['encoding', 'of', 'data - structure'], ['decoder', 'chosen to be', 'classical module']]",[],"[['Model', 'focuses on', 'encoding']]",[],data-to-text_generation,0,37
283,model,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .","[('model', (2, 3)), ('of', (6, 7)), ('using', (9, 10)), ('first encoding', (16, 18)), ('on the basis of', (20, 24)), ('then', (27, 28)), ('encoding', (28, 29)), ('on the basis of', (32, 36)), ('introduce', (41, 42)), ('in', (45, 46)), ('to ensure', (52, 54)), ('of', (56, 57)), ('in comparison to', (61, 64)), ('integrate', (75, 76)), ('to compute', (80, 82)), ('fed into', (85, 87))]","[('general structure', (4, 6)), ('two - level architecture', (11, 15)), ('all entities', (18, 20)), ('their elements', (24, 26)), ('data structure', (30, 32)), ('its entities', (36, 38)), ('Transformer encoder', (43, 45)), ('data - to - text models', (46, 52)), ('robust encoding', (54, 56)), ('each element / entities', (57, 61)), ('all others', (64, 66)), ('hierarchical attention mechanism', (77, 80)), ('hierarchical context', (83, 85)), ('decoder', (88, 89))]","[['two - level architecture', 'first encoding', 'all entities'], ['all entities', 'on the basis of', 'their elements'], ['data structure', 'on the basis of', 'its entities'], ['Transformer encoder', 'in', 'data - to - text models'], ['data - to - text models', 'to ensure', 'robust encoding'], ['robust encoding', 'of', 'each element / entities'], ['each element / entities', 'in comparison to', 'all others'], ['two - level architecture', 'integrate', 'hierarchical attention mechanism'], ['hierarchical attention mechanism', 'to compute', 'hierarchical context'], ['hierarchical context', 'fed into', 'decoder']]",[],"[['Model', 'model', 'general structure']]",[],data-to-text_generation,0,39
284,baselines,"Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .","[('is', (1, 2)), ('with', (7, 8)), ('first generated with', (15, 18)), ('replaced by', (22, 24)), ('extracted from', (26, 28)), ('by', (30, 31))]","[('Li', (0, 1)), ('standard encoder - decoder', (3, 7)), ('delayed copy mechanism', (9, 12)), ('text', (13, 14)), ('placeholders', (18, 19)), ('salient records', (24, 26)), ('table', (29, 30)), ('pointer network', (32, 34))]","[['Li', 'is', 'standard encoder - decoder'], ['standard encoder - decoder', 'with', 'delayed copy mechanism'], ['placeholders', 'replaced by', 'salient records'], ['salient records', 'extracted from', 'table'], ['table', 'by', 'pointer network']]","[['Li', 'has', 'standard encoder - decoder'], ['standard encoder - decoder', 'has', 'delayed copy mechanism'], ['delayed copy mechanism', 'has', 'text']]",[],"[['Baselines', 'has', 'Li']]",data-to-text_generation,0,192
285,baselines,"It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .","[('consists in', (1, 3)), ('with', (9, 10)), ('aimed at updating', (13, 16)), ('during', (18, 19))]","[('standard encoder - decoder', (4, 8)), ('added module', (11, 13)), ('record representations', (16, 18)), ('generation process', (20, 22))]","[['standard encoder - decoder', 'with', 'added module'], ['added module', 'aimed at updating', 'record representations'], ['record representations', 'during', 'generation process']]",[],[],[],data-to-text_generation,0,195
286,baselines,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .","[('At', (0, 1)), ('computes', (9, 10)), ('should', (12, 13))]","[('each decoding step', (1, 4)), ('gated recurrent network', (6, 9)), ('new representation', (20, 22))]",[],"[['each decoding step', 'has', 'gated recurrent network']]",[],[],data-to-text_generation,0,196
287,hyperparameters,"For the encoder module , both the low - level and high - level encoders use a two - layers multi-head self - attention with two heads .","[('For', (0, 1)), ('use', (15, 16)), ('with', (24, 25))]","[('encoder module', (2, 4)), ('two - layers multi-head self - attention', (17, 24)), ('two heads', (25, 27))]","[['two - layers multi-head self - attention', 'with', 'two heads']]",[],"[['Hyperparameters', 'For', 'encoder module']]",[],data-to-text_generation,0,206
288,hyperparameters,"To fit with the small number of record keys in our dataset , their embedding size is fixed to 20 .","[('To fit with', (0, 3)), ('in', (9, 10)), ('fixed to', (17, 19))]","[('small number of record keys', (4, 9)), ('our dataset', (10, 12)), ('embedding size', (14, 16)), ('20', (19, 20))]","[['small number of record keys', 'in', 'our dataset'], ['embedding size', 'fixed to', '20']]","[['small number of record keys', 'has', 'our dataset']]","[['Hyperparameters', 'To fit with', 'small number of record keys']]",[],data-to-text_generation,0,207
289,hyperparameters,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,"[('of', (2, 3)), ('of', (10, 11)), ('set to', (16, 18))]","[('size', (1, 2)), ('record value embeddings and hidden layers', (4, 10)), ('Transformer encoders', (12, 14)), ('300', (18, 19))]","[['size', 'of', 'record value embeddings and hidden layers'], ['record value embeddings and hidden layers', 'of', 'Transformer encoders'], ['size', 'of', 'record value embeddings and hidden layers'], ['record value embeddings and hidden layers', 'of', 'Transformer encoders'], ['Transformer encoders', 'set to', '300']]",[],[],"[['Hyperparameters', 'has', 'size']]",data-to-text_generation,0,208
290,hyperparameters,We use dropout at rate 0.5 .,"[('use', (1, 2)), ('at', (3, 4))]","[('dropout', (2, 3)), ('rate 0.5', (4, 6))]","[['dropout', 'at', 'rate 0.5']]",[],"[['Hyperparameters', 'use', 'dropout']]",[],data-to-text_generation,0,209
291,hyperparameters,The models are trained with a batch size of 64 .,"[('trained with', (3, 5)), ('of', (8, 9))]","[('batch size', (6, 8)), ('64', (9, 10))]","[['batch size', 'of', '64']]",[],"[['Hyperparameters', 'trained with', 'batch size']]",[],data-to-text_generation,0,210
292,hyperparameters,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .","[('trained with', (3, 5)), ('is', (13, 14)), ('reduced by', (18, 20)), ('every', (21, 22))]","[('Adam optimizer', (6, 8)), ('initial learning rate', (10, 13)), ('0.001', (14, 15)), ('half', (20, 21)), ('10 K steps', (22, 25))]","[['initial learning rate', 'is', '0.001'], ['initial learning rate', 'reduced by', 'half'], ['half', 'every', '10 K steps']]","[['initial learning rate', 'has', '0.001']]","[['Hyperparameters', 'trained with', 'Adam optimizer']]",[],data-to-text_generation,0,212
293,hyperparameters,We used beam search with beam size of 5 during inference .,"[('used', (1, 2)), ('with', (4, 5)), ('of', (7, 8)), ('during', (9, 10))]","[('beam search', (2, 4)), ('beam size', (5, 7)), ('5', (8, 9)), ('inference', (10, 11))]","[['beam search', 'with', 'beam size'], ['beam size', 'of', '5'], ['beam size', 'during', 'inference'], ['5', 'during', 'inference']]",[],"[['Hyperparameters', 'used', 'beam search']]",[],data-to-text_generation,0,213
294,hyperparameters,All the models are implemented in Open NMT - py .,"[('implemented in', (4, 6))]","[('Open NMT - py', (6, 10))]",[],[],"[['Hyperparameters', 'implemented in', 'Open NMT - py']]",[],data-to-text_generation,0,214
295,code,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,[],[],[],[],[],[],data-to-text_generation,0,215
296,ablation-analysis,"Second , the comparison between scenario Hierarchical - kv and Hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted CS - R% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .","[('between', (4, 5)), ('shows', (12, 13)), ('omitting', (14, 15)), ('of', (18, 19)), ('in', (22, 23)), ('is', (26, 27)), ('excepted', (39, 40))]","[('comparison', (3, 4)), ('scenario Hierarchical - kv and Hierarchical -k', (5, 12)), ('entirely the influence', (15, 18)), ('record values', (20, 22)), ('attention mechanism', (24, 26)), ('more effective', (27, 29))]","[['comparison', 'between', 'scenario Hierarchical - kv and Hierarchical -k'], ['scenario Hierarchical - kv and Hierarchical -k', 'omitting', 'entirely the influence'], ['entirely the influence', 'of', 'record values'], ['record values', 'in', 'attention mechanism'], ['attention mechanism', 'is', 'more effective']]","[['comparison', 'has', 'scenario Hierarchical - kv and Hierarchical -k']]",[],"[['Ablation analysis', 'has', 'comparison']]",data-to-text_generation,0,229
297,ablation-analysis,"Scores of Hierarchical -k are sharp , with all of the weight on the correct record ( PTS QTR1 , 26 ) whereas scores of Hierarchical - kv are more distributed over all PTS QTR records , ultimately failing to retrieve the correct one .","[('are', (4, 5)), ('with', (7, 8)), ('on', (12, 13)), ('distributed over', (30, 32))]","[('Scores of Hierarchical -k', (0, 4)), ('sharp', (5, 6)), ('all of the weight', (8, 12)), ('correct record', (14, 16)), ('scores of Hierarchical - kv', (23, 28)), ('more', (29, 30)), ('all PTS QTR records', (32, 36)), ('failing', (38, 39)), ('correct one', (42, 44))]","[['Scores of Hierarchical -k', 'are', 'sharp'], ['scores of Hierarchical - kv', 'are', 'more'], ['sharp', 'with', 'all of the weight'], ['all of the weight', 'on', 'correct record'], ['scores of Hierarchical - kv', 'distributed over', 'all PTS QTR records'], ['more', 'distributed over', 'all PTS QTR records']]","[['Scores of Hierarchical -k', 'has', 'sharp']]",[],"[['Ablation analysis', 'has', 'Scores of Hierarchical -k']]",data-to-text_generation,0,234
298,ablation-analysis,over all models ; our best model Hierarchical -k reaching 17.5 vs. 16.5 against the best baseline .,"[('reaching', (9, 10)), ('against', (13, 14))]","[('our best model', (4, 7)), ('Hierarchical -k', (7, 9)), ('17.5 vs. 16.5', (10, 13)), ('best baseline', (15, 17))]","[['Hierarchical -k', 'reaching', '17.5 vs. 16.5'], ['17.5 vs. 16.5', 'against', 'best baseline']]","[['our best model', 'has', 'Hierarchical -k']]",[],"[['Ablation analysis', 'has', 'our best model']]",data-to-text_generation,0,235
299,ablation-analysis,"Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman , reinforcing the crucial role of structure in data semantics and saliency .","[('achieve', (3, 4)), ('on', (7, 8))]","[('Our hierarchical models', (0, 3)), ('significantly better scores', (4, 7)), ('all metrics', (8, 10)), ('flat architecture Wiseman', (14, 17))]","[['Our hierarchical models', 'achieve', 'significantly better scores'], ['significantly better scores', 'on', 'all metrics']]",[],[],"[['Ablation analysis', 'has', 'Our hierarchical models']]",data-to-text_generation,0,244
300,ablation-analysis,"However , Wiseman achieves only 75 . 62 % of precision , effectively mentioning on average a total of 22.25 records ( wrong or accurate ) , where our model Hierarchical -k scores a precision of 89 . 46 % , leading to 23.66 total mentions , just slightly above Wiseman .","[('achieves', (3, 4)), ('of', (9, 10)), ('scores', (32, 33)), ('leading to', (41, 43))]","[('only 75 . 62 %', (4, 9)), ('precision', (10, 11))]","[['only 75 . 62 %', 'of', 'precision']]",[],[],[],data-to-text_generation,0,247
301,ablation-analysis,This suggests that introducing the Transformer architecture is promising way to implicitly account for data structure .,"[('introducing', (3, 4)), ('is', (7, 8)), ('to implicitly account for', (10, 14))]","[('Transformer architecture', (5, 7)), ('promising way', (8, 10)), ('data structure', (14, 16))]","[['Transformer architecture', 'is', 'promising way'], ['Transformer architecture', 'to implicitly account for', 'data structure'], ['promising way', 'to implicitly account for', 'data structure']]","[['Transformer architecture', 'has', 'promising way']]","[['Ablation analysis', 'introducing', 'Transformer architecture']]",[],data-to-text_generation,0,252
302,ablation-analysis,"Our hierarchical models outperform the two - step decoders of Li and Puduppully - plan on both BLEU and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .","[('of', (9, 10)), ('on', (15, 16))]","[('Our hierarchical models', (0, 3)), ('outperform', (3, 4)), ('two - step decoders', (5, 9)), ('Li and Puduppully - plan', (10, 15)), ('BLEU and all qualitative metrics', (17, 22))]","[['two - step decoders', 'of', 'Li and Puduppully - plan'], ['Li and Puduppully - plan', 'on', 'BLEU and all qualitative metrics']]","[['Our hierarchical models', 'has', 'outperform'], ['outperform', 'has', 'two - step decoders']]",[],"[['Ablation analysis', 'has', 'Our hierarchical models']]",data-to-text_generation,0,253
303,ablation-analysis,"While our models sensibly outperform in precision at factual mentions , the baseline Puduppully - plan reaches 34.28 mentions on average , showing that incorporating modules dedicated to entity extraction leads to over- focusing on entities ; contrasting with our models that learn to generate more balanced descriptions .","[('sensibly outperform', (3, 5)), ('at', (7, 8)), ('reaches', (16, 17))]","[('precision', (6, 7)), ('factual mentions', (8, 10)), ('baseline Puduppully - plan', (12, 16)), ('34.28 mentions', (17, 19))]","[['precision', 'at', 'factual mentions'], ['baseline Puduppully - plan', 'reaches', '34.28 mentions']]",[],[],[],data-to-text_generation,0,254
304,ablation-analysis,The comparison with Puduppully - updt shows that dynamically updating the encoding across the generation process can lead to better Content Ordering ( CO ) and RG - P% .,"[('comparison', (1, 2)), ('with', (2, 3)), ('shows', (6, 7)), ('across', (12, 13)), ('lead to', (17, 19))]","[('Puduppully - updt', (3, 6)), ('dynamically updating', (8, 10)), ('encoding', (11, 12)), ('generation process', (14, 16)), ('better Content Ordering ( CO )', (19, 25))]","[['Puduppully - updt', 'shows', 'dynamically updating'], ['encoding', 'across', 'generation process'], ['encoding', 'lead to', 'better Content Ordering ( CO )']]","[['Puduppully - updt', 'has', 'dynamically updating'], ['dynamically updating', 'has', 'encoding']]","[['Ablation analysis', 'comparison', 'Puduppully - updt']]",[],data-to-text_generation,0,255
305,ablation-analysis,"In contrast , our model encodes saliency among records / entities more effectively ( CS metric ) .","[('encodes', (5, 6)), ('among', (7, 8))]","[('saliency', (6, 7)), ('records / entities', (8, 11)), ('more effectively', (11, 13))]","[['saliency', 'among', 'records / entities']]",[],[],[],data-to-text_generation,0,259
306,ablation-analysis,"In this work we have proposed a hierarchical encoder for structured data , which 1 ) leverages the structure to form efficient representation of its input ; 2 ) has strong synergy with the hierarchical attention of its associated decoder .","[('proposed', (5, 6)), ('for', (9, 10)), ('leverages', (16, 17)), ('to form', (19, 21)), ('of', (23, 24)), ('with', (32, 33)), ('of', (36, 37))]","[('hierarchical encoder', (7, 9)), ('structured data', (10, 12)), ('structure', (18, 19)), ('efficient representation', (21, 23)), ('its input', (24, 26)), ('strong synergy', (30, 32)), ('hierarchical attention', (34, 36)), ('its associated decoder', (37, 40))]","[['hierarchical encoder', 'for', 'structured data'], ['structure', 'to form', 'efficient representation'], ['efficient representation', 'of', 'its input'], ['strong synergy', 'with', 'hierarchical attention'], ['hierarchical attention', 'of', 'its associated decoder']]",[],"[['Ablation analysis', 'proposed', 'hierarchical encoder']]",[],data-to-text_generation,0,266
307,research-problem,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation,[],[],[],[],[],[],data-to-text_generation,1,2
308,approach,Our work focuses on language generators whose inputs are structured meaning representations ( MRs ) .,"[('focuses on', (2, 4)), ('are', (8, 9))]","[('language generators', (4, 6)), ('inputs', (7, 8)), ('structured meaning representations ( MRs )', (9, 15))]","[['inputs', 'are', 'structured meaning representations ( MRs )']]","[['language generators', 'has', 'inputs']]","[['Approach', 'focuses on', 'language generators']]",[],data-to-text_generation,1,12
309,approach,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .","[('present', (2, 3)), ('train and test on', (12, 16)), ('in', (20, 21))]","[('neural ensemble natural language generator', (4, 9)), ('three large unaligned datasets', (16, 20)), ('restaurant , television , and laptop domains', (22, 29))]","[['neural ensemble natural language generator', 'train and test on', 'three large unaligned datasets'], ['three large unaligned datasets', 'in', 'restaurant , television , and laptop domains']]",[],"[['Approach', 'present', 'neural ensemble natural language generator']]",[],data-to-text_generation,1,28
310,experimental-setup,We built our ensemble model using the seq2seq framework for TensorFlow .,"[('built', (1, 2)), ('using', (5, 6)), ('for', (9, 10))]","[('our ensemble model', (2, 5)), ('seq2seq framework', (7, 9)), ('TensorFlow', (10, 11))]","[['our ensemble model', 'using', 'seq2seq framework'], ['seq2seq framework', 'for', 'TensorFlow']]",[],"[['Experimental setup', 'built', 'our ensemble model']]",[],data-to-text_generation,1,158
311,experimental-setup,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .","[('use', (4, 5)), ('with', (9, 10)), ('use', (19, 20))]","[('bidirectional LSTM encoder', (6, 9)), ('512 cells per layer', (10, 14)), ('CNN', (17, 18)), ('pooling encoder', (21, 23))]","[['bidirectional LSTM encoder', 'with', '512 cells per layer']]",[],"[['Experimental setup', 'use', 'bidirectional LSTM encoder']]",[],data-to-text_generation,1,159
312,experimental-setup,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,"[('was', (5, 6)), ('with', (12, 13))]","[('decoder', (1, 2)), ('4 - layer RNN decoder', (7, 12)), ('512 LSTM cells per', (13, 17)), ('layer', (17, 18))]","[['decoder', 'was', '4 - layer RNN decoder'], ['4 - layer RNN decoder', 'with', '512 LSTM cells per']]","[['decoder', 'has', '4 - layer RNN decoder'], ['512 LSTM cells per', 'has', 'layer']]",[],"[['Experimental setup', 'has', 'decoder']]",data-to-text_generation,1,160
313,experimental-setup,"After experimenting with different beam search parameters , we settled on the beam width of 10 .","[('experimenting with', (1, 3)), ('settled on', (9, 11))]","[('different beam search parameters', (3, 7)), ('beam width of 10', (12, 16))]","[['different beam search parameters', 'settled on', 'beam width of 10']]","[['different beam search parameters', 'has', 'beam width of 10']]","[['Experimental setup', 'experimenting with', 'different beam search parameters']]",[],data-to-text_generation,1,162
314,experiments,"The length penalty providing the best results on the E2E dataset was 0.6 , whereas for the TV and Laptop datasets it was 0.9 and 1.0 , respectively .","[('providing', (3, 4)), ('on', (7, 8)), ('was', (11, 12)), ('was', (22, 23))]","[('length penalty', (1, 3)), ('best results', (5, 7)), ('E2E dataset', (9, 11)), ('0.6', (12, 13)), ('TV and Laptop datasets', (17, 21)), ('0.9 and 1.0', (23, 26))]","[['length penalty', 'providing', 'best results'], ['best results', 'on', 'E2E dataset'], ['E2E dataset', 'was', '0.6'], ['TV and Laptop datasets', 'was', '0.9 and 1.0']]","[['length penalty', 'has', 'best results'], ['TV and Laptop datasets', 'has', '0.9 and 1.0']]",[],[],data-to-text_generation,1,164
315,experiments,Experiments on the E2E Dataset,"[('on', (1, 2))]",[],[],[],[],[],data-to-text_generation,1,165
316,results,The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo - samples in the training set .,"[('show', (3, 4)), ('benefit from', (13, 15)), ('in', (19, 20))]","[('both the LSTM and the CNN models', (5, 12)), ('additional pseudo - samples', (15, 19)), ('training set', (21, 23))]","[['both the LSTM and the CNN models', 'benefit from', 'additional pseudo - samples'], ['additional pseudo - samples', 'in', 'training set']]",[],"[['Results', 'show', 'both the LSTM and the CNN models']]",[],data-to-text_generation,1,172
317,results,Testing our ensembling approach reveals that reranking predictions pooled from different models produces an ensemble model that is over all more robust than the individual submodels .,"[('Testing', (0, 1)), ('reveals', (4, 5)), ('pooled from', (8, 10)), ('produces', (12, 13)), ('that is', (16, 18)), ('than', (22, 23))]","[('ensembling approach', (2, 4)), ('reranking predictions', (6, 8)), ('different models', (10, 12)), ('ensemble model', (14, 16)), ('over all more robust', (18, 22)), ('individual submodels', (24, 26))]","[['ensembling approach', 'reveals', 'reranking predictions'], ['reranking predictions', 'pooled from', 'different models'], ['different models', 'produces', 'ensemble model'], ['ensemble model', 'that is', 'over all more robust'], ['over all more robust', 'than', 'individual submodels']]","[['ensembling approach', 'has', 'reranking predictions']]","[['Results', 'Testing', 'ensembling approach']]",[],data-to-text_generation,1,175
318,results,"Analyzing the outputs , we also observed that the CNN model surpassed the two LSTM models in the ability to realize the "" fast food "" and "" pub "" values reliably , both of which were hardly present in the validation set but very frequent in the test set .","[('observed', (6, 7)), ('surpassed', (11, 12)), ('in the', (16, 18)), ('to realize', (19, 21))]","[('CNN model', (9, 11)), ('two LSTM models', (13, 16)), ('ability', (18, 19)), ('"" fast food "" and "" pub "" values', (22, 31)), ('reliably', (31, 32))]","[['CNN model', 'surpassed', 'two LSTM models'], ['two LSTM models', 'in the', 'ability'], ['two LSTM models', 'to realize', '"" fast food "" and "" pub "" values'], ['ability', 'to realize', '"" fast food "" and "" pub "" values']]","[['"" fast food "" and "" pub "" values', 'has', 'reliably']]","[['Results', 'observed', 'CNN model']]",[],data-to-text_generation,1,179
319,results,"We observe , however , that a hybrid ensemble model manages to perform the best in terms of the error rate , as well as the naturalness .","[('observe', (1, 2)), ('manages to perform', (10, 13)), ('in terms of', (15, 18)), ('as well as', (22, 25))]","[('hybrid ensemble model', (7, 10)), ('best', (14, 15)), ('error rate', (19, 21)), ('naturalness', (26, 27))]","[['hybrid ensemble model', 'manages to perform', 'best'], ['best', 'in terms of', 'error rate'], ['best', 'as well as', 'naturalness']]",[],"[['Results', 'observe', 'hybrid ensemble model']]",[],data-to-text_generation,1,206
320,experiments,"As shows , our ensemble model performs competitively with the baseline on the TV dataset , and it outperforms it on the Laptop dataset by a wide margin .","[('performs', (6, 7)), ('with', (8, 9)), ('on', (11, 12)), ('on', (20, 21)), ('by', (24, 25))]","[('our ensemble model', (3, 6)), ('competitively', (7, 8)), ('baseline', (10, 11)), ('TV dataset', (13, 15)), ('outperforms', (18, 19)), ('Laptop dataset', (22, 24)), ('wide margin', (26, 28))]","[['our ensemble model', 'performs', 'competitively'], ['competitively', 'with', 'baseline'], ['baseline', 'on', 'TV dataset'], ['outperforms', 'on', 'Laptop dataset'], ['outperforms', 'on', 'Laptop dataset'], ['outperforms', 'by', 'wide margin'], ['Laptop dataset', 'by', 'wide margin']]","[['outperforms', 'has', 'Laptop dataset']]",[],[],data-to-text_generation,1,212
321,research-problem,Deep Graph Convolutional Encoders for Structured Data to Text Generation,[],[],[],[],[],[],data-to-text_generation,2,2
322,research-problem,Recent neural generation approaches build on encoder - decoder architectures proposed for machine translation .,[],"[('machine translation', (12, 14))]",[],[],[],[],data-to-text_generation,2,11
323,model,In this work we focus on two generation scenarios where the source data is graph structured .,"[('focus on', (4, 6)), ('where', (9, 10)), ('is', (13, 14))]","[('two generation scenarios', (6, 9)), ('source data', (11, 13)), ('graph structured', (14, 16))]","[['two generation scenarios', 'where', 'source data'], ['source data', 'is', 'graph structured']]",[],"[['Model', 'focus on', 'two generation scenarios']]",[],data-to-text_generation,2,14
324,research-problem,"One is the generation of multi-sentence descriptions of Knowledge Base ( KB ) entities from RDF graphs ) , namely the WebNLG task .","[('generation of', (3, 5)), ('namely', (19, 20))]","[('multi-sentence descriptions of Knowledge Base ( KB ) entities', (5, 14)), ('WebNLG task', (21, 23))]","[['multi-sentence descriptions of Knowledge Base ( KB ) entities', 'namely', 'WebNLG task']]",[],"[['Research problem', 'generation of', 'multi-sentence descriptions of Knowledge Base ( KB ) entities']]",[],data-to-text_generation,2,15
325,model,They rely on recurrent data encoders with memory and gating mechanisms ( LSTM ; ) .,"[('rely on', (1, 3)), ('with', (6, 7))]","[('recurrent data encoders', (3, 6)), ('memory and gating mechanisms', (7, 11))]","[['recurrent data encoders', 'with', 'memory and gating mechanisms']]",[],"[['Model', 'rely on', 'recurrent data encoders']]",[],data-to-text_generation,2,23
326,model,"In this work , we compare with a model that explicitly encodes structure and is trained end - to - end .","[('explicitly encodes', (10, 12)), ('trained', (15, 16))]","[('structure', (12, 13)), ('end - to - end', (16, 21))]",[],[],"[['Model', 'explicitly encodes', 'structure']]",[],data-to-text_generation,2,25
327,model,"Concretely , we use a Graph Convolutional Network ( GCN ; ) as our encoder .","[('use', (3, 4)), ('as', (12, 13))]","[('Graph Convolutional Network ( GCN ; )', (5, 12)), ('our encoder', (13, 15))]","[['Graph Convolutional Network ( GCN ; )', 'as', 'our encoder']]",[],"[['Model', 'use', 'Graph Convolutional Network ( GCN ; )']]",[],data-to-text_generation,2,26
328,research-problem,"Formally , we address the task of text generation from graph - structured data considering as input a directed labeled graph X = ( V , E ) where V is a set of nodes and E is a set of edges between nodes in V .","[('considering as input', (14, 17)), ('where', (28, 29)), ('is', (30, 31)), ('between', (42, 43))]","[('text generation from graph - structured data', (7, 14)), ('directed', (18, 19)), ('V', (29, 30)), ('set', (32, 33)), ('nodes', (43, 44))]","[['text generation from graph - structured data', 'considering as input', 'directed'], ['V', 'is', 'set']]",[],[],[],data-to-text_generation,2,37
329,research-problem,The WebNLG task aims at the generation of entity descriptions from a set of RDF triples related to an entity of a given category .,[],"[('WebNLG', (1, 2))]",[],[],[],[],data-to-text_generation,2,72
330,baselines,Both take as input a linearised version of the source graph .,"[('take as input', (1, 4)), ('of', (7, 8))]","[('linearised version', (5, 7)), ('source graph', (9, 11))]","[['linearised version', 'of', 'source graph']]",[],"[['Baselines', 'take as input', 'linearised version']]",[],data-to-text_generation,2,103
331,hyperparameters,"For the WebNLG baseline , we use the linearis ation scripts provided by .","[('For', (0, 1)), ('use', (6, 7))]","[('WebNLG baseline', (2, 4)), ('linearis ation scripts', (8, 11))]","[['WebNLG baseline', 'use', 'linearis ation scripts']]",[],"[['Hyperparameters', 'For', 'WebNLG baseline']]",[],data-to-text_generation,2,104
332,results,We obtained the best results with an encoder with four GCN layers with residual connections .,"[('obtained', (1, 2)), ('with', (5, 6)), ('with', (8, 9)), ('with', (12, 13))]","[('best results', (3, 5)), ('encoder', (7, 8)), ('four GCN layers', (9, 12)), ('residual connections', (13, 15))]","[['best results', 'with', 'encoder'], ['encoder', 'with', 'four GCN layers'], ['four GCN layers', 'with', 'residual connections'], ['encoder', 'with', 'four GCN layers'], ['four GCN layers', 'with', 'residual connections'], ['four GCN layers', 'with', 'residual connections']]","[['best results', 'has', 'encoder']]","[['Results', 'obtained', 'best results']]",[],data-to-text_generation,2,115
333,tasks,Encoder ( decoder ) embeddings and hidden dimensions were set to 300 .,"[('set to', (9, 11))]","[('Encoder ( decoder ) embeddings and hidden dimensions', (0, 8)), ('300', (11, 12))]","[['Encoder ( decoder ) embeddings and hidden dimensions', 'set to', '300']]",[],[],"[['Tasks', 'has', 'Encoder ( decoder ) embeddings and hidden dimensions']]",data-to-text_generation,2,126
334,results,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,"[('is', (3, 4)), ('than', (7, 8)), ('with', (10, 11)), ('of', (14, 15))]","[('GCN model', (1, 3)), ('more stable', (5, 7)), ('baseline', (9, 10)), ('standard deviation', (12, 14)), ('.004 vs . 010', (15, 19))]","[['GCN model', 'is', 'more stable'], ['more stable', 'than', 'baseline'], ['baseline', 'with', 'standard deviation'], ['standard deviation', 'of', '.004 vs . 010']]","[['GCN model', 'has', 'more stable']]",[],"[['Results', 'has', 'GCN model']]",data-to-text_generation,2,135
335,results,The GCN EC model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points ; and MELBOURNE by .014 BLEU points .,"[('that uses', (6, 8)), ('by', (19, 20)), ('by', (26, 27))]","[('GCN EC model', (1, 4)), ('outperforms', (4, 5)), ('PKUWRITER', (5, 6)), ('ensemble of 7 models', (9, 13)), ('further reinforcement learning step', (15, 19)), ('.047 BLEU points', (20, 23)), ('MELBOURNE', (25, 26)), ('.014 BLEU points', (27, 30))]","[['outperforms', 'that uses', 'further reinforcement learning step'], ['PKUWRITER', 'that uses', 'ensemble of 7 models'], ['PKUWRITER', 'that uses', 'further reinforcement learning step'], ['PKUWRITER', 'that uses', 'MELBOURNE'], ['further reinforcement learning step', 'by', '.047 BLEU points'], ['MELBOURNE', 'by', '.014 BLEU points']]","[['GCN EC model', 'has', 'outperforms'], ['outperforms', 'has', 'PKUWRITER']]",[],"[['Results', 'has', 'GCN EC model']]",data-to-text_generation,2,137
336,baselines,GCN EC is behind ADAPT which relies on sub-word encoding .,"[('behind', (3, 4)), ('relies on', (6, 8))]","[('GCN EC', (0, 2)), ('ADAPT', (4, 5)), ('sub-word encoding', (8, 10))]","[['GCN EC', 'behind', 'ADAPT'], ['ADAPT', 'relies on', 'sub-word encoding']]","[['GCN EC', 'has', 'ADAPT']]",[],"[['Baselines', 'has', 'GCN EC']]",data-to-text_generation,2,138
337,results,SR11 Deep task,[],[],[],[],[],[],data-to-text_generation,2,139
338,ablation-analysis,In ( BLEU ) we report an ablation study on the impact of the number of layers and the type of skip connections on the WebNLG dataset .,"[('on', (9, 10)), ('on', (23, 24))]","[('impact of the number of layers and the type of skip connections', (11, 23)), ('WebNLG dataset', (25, 27))]","[['impact of the number of layers and the type of skip connections', 'on', 'WebNLG dataset'], ['impact of the number of layers and the type of skip connections', 'on', 'WebNLG dataset']]",[],"[['Ablation analysis', 'on', 'impact of the number of layers and the type of skip connections']]",[],data-to-text_generation,2,162
339,ablation-analysis,The first thing we notice is the importance of skip connections between GCN layers .,"[('notice', (4, 5)), ('of', (8, 9)), ('between', (11, 12))]","[('importance', (7, 8)), ('skip connections', (9, 11)), ('GCN layers', (12, 14))]","[['importance', 'of', 'skip connections'], ['skip connections', 'between', 'GCN layers']]",[],"[['Ablation analysis', 'notice', 'importance']]",[],data-to-text_generation,2,163
340,ablation-analysis,Residual and dense connections lead to similar results .,"[('lead to', (4, 6))]","[('Residual and dense connections', (0, 4)), ('similar results', (6, 8))]","[['Residual and dense connections', 'lead to', 'similar results']]",[],[],"[['Ablation analysis', 'has', 'Residual and dense connections']]",data-to-text_generation,2,164
341,ablation-analysis,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .","[('produce', (9, 10)), ('than', (18, 19))]","[('Dense connections', (0, 2)), ('models', (10, 11)), ('bigger', (11, 12)), ('slightly less accurate', (14, 17)), ('residual connections', (19, 21))]","[['Dense connections', 'produce', 'models'], ['slightly less accurate', 'than', 'residual connections']]","[['models', 'has', 'bigger']]",[],"[['Ablation analysis', 'has', 'Dense connections']]",data-to-text_generation,2,165
342,research-problem,Pragmatically Informative Text Generation,[],[],[],[],[],[],data-to-text_generation,3,2
343,research-problem,We improve the informativeness of models for conditional text generation using techniques from computational pragmatics .,"[('improve', (1, 2)), ('for', (6, 7))]","[('informativeness', (3, 4)), ('conditional text generation', (7, 10))]","[['informativeness', 'for', 'conditional text generation']]",[],"[['Research problem', 'improve', 'informativeness']]",[],data-to-text_generation,3,4
344,model,"In this paper , we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations ) and summarization .","[('show', (5, 6))]","[('pragmatic reasoning', (7, 9))]",[],[],"[['Model', 'show', 'pragmatic reasoning']]",[],data-to-text_generation,3,12
345,model,Reconstructor - based pragmatic system ( S R 1 ),[],"[('Reconstructor - based pragmatic system', (0, 5))]",[],[],[],"[['Model', 'has', 'Reconstructor - based pragmatic system']]",data-to-text_generation,3,20
346,experiments,"We also report two extractive baselines : Lead - 3 , which uses the first three sentences of the document as the summary , and Inputs , the concatenation of the extracted sentences used as inputs to our models ( i.e. , i ( 1 ) , . . . , i ( P ) ) .","[('report', (2, 3)), ('of', (29, 30)), ('used as', (33, 35)), ('to', (36, 37))]","[('two extractive baselines', (3, 6)), ('Lead - 3', (7, 10)), ('first three sentences', (14, 17)), ('Inputs', (25, 26)), ('concatenation', (28, 29)), ('extracted sentences', (31, 33)), ('inputs', (35, 36)), ('our models', (37, 39))]","[['concatenation', 'of', 'extracted sentences'], ['concatenation', 'used as', 'inputs'], ['extracted sentences', 'used as', 'inputs'], ['inputs', 'to', 'our models']]","[['two extractive baselines', 'name', 'Lead - 3']]",[],[],data-to-text_generation,3,93
347,experiments,"The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .","[('obtain', (3, 4)), ('of', (5, 6)), ('in', (7, 8)), ('over', (13, 14)), ('with', (20, 21)), ('outperforming', (28, 29))]","[('pragmatic methods', (1, 3)), ('improvements', (4, 5)), ('0.2-0.5', (6, 7)), ('ROUGE scores', (8, 10)), ('0.2-1.8 METEOR', (11, 13)), ('base S 0 model', (15, 19)), ('distractor - based approach', (22, 26)), ('SD 1', (26, 28)), ('reconstructorbased approach S R 1', (30, 35))]","[['pragmatic methods', 'obtain', 'improvements'], ['pragmatic methods', 'obtain', '0.2-1.8 METEOR'], ['improvements', 'of', '0.2-0.5'], ['improvements', 'of', '0.2-1.8 METEOR'], ['0.2-0.5', 'in', 'ROUGE scores'], ['0.2-1.8 METEOR', 'over', 'base S 0 model'], ['base S 0 model', 'with', 'distractor - based approach'], ['SD 1', 'outperforming', 'reconstructorbased approach S R 1']]","[['pragmatic methods', 'has', 'improvements'], ['distractor - based approach', 'has', 'SD 1']]",[],[],data-to-text_generation,3,94
348,research-problem,Data - to - Text Generation with Content Selection and Planning,[],"[('Data - to - Text Generation', (0, 6))]",[],[],[],[],data-to-text_generation,4,2
349,model,"In this paper , we address these shortcomings by explicitly modeling content selection and planning within a neural data - to - text architecture .","[('explicitly modeling', (9, 11)), ('within', (15, 16))]","[('content selection and planning', (11, 15)), ('neural data - to - text architecture', (17, 24))]","[['content selection and planning', 'within', 'neural data - to - text architecture']]",[],"[['Model', 'explicitly modeling', 'content selection and planning']]",[],data-to-text_generation,4,19
350,hyperparameters,"For each stage , we utilize beam search to approximately obtain the best results .","[('utilize', (5, 6)), ('to approximately obtain', (8, 11))]","[('each stage', (1, 3)), ('beam search', (6, 8)), ('best results', (12, 14))]","[['each stage', 'utilize', 'beam search'], ['beam search', 'to approximately obtain', 'best results']]",[],[],[],data-to-text_generation,4,134
351,hyperparameters,Input feeding was employed for the text decoder .,"[('employed for', (3, 5))]","[('Input feeding', (0, 2)), ('text decoder', (6, 8))]","[['Input feeding', 'employed for', 'text decoder']]",[],[],"[['Hyperparameters', 'has', 'Input feeding']]",data-to-text_generation,4,159
352,hyperparameters,We applied dropout ) at a rate of 0.3 .,"[('applied', (1, 2)), ('at', (4, 5)), ('of', (7, 8))]","[('dropout', (2, 3)), ('rate', (6, 7)), ('0.3', (8, 9))]","[['dropout', 'at', 'rate'], ['rate', 'of', '0.3']]",[],"[['Hyperparameters', 'applied', 'dropout']]",[],data-to-text_generation,4,160
353,hyperparameters,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .","[('trained for', (2, 4)), ('with', (6, 7)), ('selected from', (22, 24))]","[('25 epochs', (4, 6)), ('Adagrad optimizer', (8, 10)), ('initial learning rate', (12, 15)), ('0.15', (16, 17)), ('learning rate decay', (18, 21)), ('{ 0.5 , 0.97 }', (24, 29)), ('batch size', (31, 33)), ('5', (34, 35))]","[['25 epochs', 'with', 'Adagrad optimizer'], ['learning rate decay', 'selected from', '{ 0.5 , 0.97 }']]","[['initial learning rate', 'has', '0.15'], ['batch size', 'has', '5']]","[['Hyperparameters', 'trained for', '25 epochs']]",[],data-to-text_generation,4,161
354,hyperparameters,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .","[('For', (0, 1)), ('made use of', (5, 8)), ('set', (11, 12)), ('to', (15, 16))]","[('text decoding', (1, 3)), ('BPTT', (8, 9)), ('truncation size', (13, 15)), ('100', (16, 17))]","[['text decoding', 'made use of', 'BPTT'], ['truncation size', 'to', '100']]","[['truncation size', 'has', '100']]","[['Hyperparameters', 'For', 'text decoding']]",[],data-to-text_generation,4,162
355,hyperparameters,We set the beam size to 5 during inference .,"[('set', (1, 2)), ('to', (5, 6)), ('during', (7, 8))]","[('beam size', (3, 5)), ('5', (6, 7)), ('inference', (8, 9))]","[['beam size', 'to', '5'], ['beam size', 'during', 'inference'], ['5', 'during', 'inference']]","[['beam size', 'has', '5']]","[['Hyperparameters', 'set', 'beam size']]",[],data-to-text_generation,4,163
356,hyperparameters,All models are implemented in Open NMT - py .,"[('implemented in', (3, 5))]","[('Open NMT - py', (5, 9))]",[],[],"[['Hyperparameters', 'implemented in', 'Open NMT - py']]",[],data-to-text_generation,4,164
357,results,"As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .","[('improves upon', (6, 8))]","[('NCP', (5, 6)), ('vanilla encoderdecoder models ( ED + JC , ED + CC )', (8, 20))]","[['NCP', 'improves upon', 'vanilla encoderdecoder models ( ED + JC , ED + CC )']]",[],[],"[['Results', 'has', 'NCP']]",data-to-text_generation,4,179
358,results,"In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .","[('achieves', (4, 5)), ('with', (7, 8))]","[('NCP', (3, 4)), ('comparable scores', (5, 7)), ('joint or conditional copy mechanism', (9, 14))]","[['NCP', 'achieves', 'comparable scores'], ['comparable scores', 'with', 'joint or conditional copy mechanism']]",[],[],"[['Results', 'has', 'NCP']]",data-to-text_generation,4,180
359,results,"Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .","[('achieves', (5, 6)), ('in terms of', (13, 16))]","[('NCP + CC', (2, 5)), ('best content selection and content ordering scores', (6, 13)), ('BLEU', (16, 17))]","[['NCP + CC', 'achieves', 'best content selection and content ordering scores'], ['best content selection and content ordering scores', 'in terms of', 'BLEU']]",[],[],"[['Results', 'has', 'NCP + CC']]",data-to-text_generation,4,181
360,results,"Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .","[('Compared to', (0, 2)), ('in', (6, 7)), ('achieve', (12, 13)), ('of', (16, 17)), ('in terms of', (20, 23)), ('improves', (30, 31))]","[('best reported system', (3, 6)), ('Wiseman et al.', (7, 10)), ('absolute improvement', (14, 16)), ('approximately 12 %', (17, 20)), ('relation generation', (23, 25)), ('content selection precision', (26, 29)), ('5 %', (32, 34)), ('recall', (35, 36)), ('15 %', (37, 39)), ('content ordering', (40, 42)), ('increases', (42, 43)), ('3 %', (44, 46)), ('BLEU', (48, 49)), ('1.5 points', (50, 52))]","[['best reported system', 'in', 'Wiseman et al.'], ['best reported system', 'achieve', 'absolute improvement'], ['absolute improvement', 'of', 'approximately 12 %'], ['approximately 12 %', 'in terms of', 'relation generation']]","[['best reported system', 'has', 'Wiseman et al.'], ['content ordering', 'has', 'increases']]","[['Results', 'Compared to', 'best reported system']]",[],data-to-text_generation,4,182
361,results,The results of the oracle system ( NCP + OR ) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output .,"[('of', (2, 3)), ('show', (11, 12)), ('correlate with', (19, 21)), ('in', (31, 32)), ('result in', (36, 38))]","[('oracle system ( NCP + OR )', (4, 11)), ('content selection and ordering', (13, 17)), ('quality of', (22, 24)), ('content plan', (25, 27)), ('any improvements', (29, 31)), ('our planning component', (32, 35)), ('better output', (38, 40))]","[['oracle system ( NCP + OR )', 'show', 'content selection and ordering'], ['content selection and ordering', 'correlate with', 'quality of'], ['content selection and ordering', 'correlate with', 'any improvements'], ['any improvements', 'in', 'our planning component'], ['any improvements', 'result in', 'better output'], ['our planning component', 'result in', 'better output']]","[['quality of', 'has', 'content plan']]","[['Results', 'of', 'oracle system ( NCP + OR )']]",[],data-to-text_generation,4,183
362,results,"As far as the template - based system is concerned , we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics .","[('observe', (12, 13)), ('obtains', (15, 16)), ('scores high', (22, 24)), ('on', (24, 25))]","[('template - based system', (4, 8)), ('low BLEU and CS precision', (16, 21)), ('CS recall and RG metrics', (25, 30))]","[['template - based system', 'observe', 'low BLEU and CS precision'], ['template - based system', 'obtains', 'low BLEU and CS precision']]",[],[],[],data-to-text_generation,4,184
363,results,84.5 % of the records in NCP + CC are non-duplicates compared to who obtain 72.9 % showing that our model is less repetitive .,"[('of', (2, 3)), ('in', (5, 6)), ('are', (9, 10)), ('compared to', (11, 13)), ('showing', (17, 18))]","[('84.5 %', (0, 2)), ('records', (4, 5)), ('NCP + CC', (6, 9)), ('non-duplicates', (10, 11)), ('obtain', (14, 15)), ('72.9 %', (15, 17))]","[['84.5 %', 'of', 'records'], ['records', 'in', 'NCP + CC'], ['NCP + CC', 'are', 'non-duplicates'], ['non-duplicates', 'compared to', 'obtain']]","[['obtain', 'has', '72.9 %']]",[],"[['Results', 'has', '84.5 %']]",data-to-text_generation,4,187
364,results,"We see in that content selection and planning individually contribute to performance improvements over the baseline ( ED + CC ) , and accuracy further increases when both components are taken into account .","[('individually', (8, 9)), ('contribute to', (9, 11)), ('over', (13, 14)), ('further', (24, 25)), ('when', (26, 27)), ('taken', (30, 31))]","[('content selection and planning', (4, 8)), ('performance improvements', (11, 13)), ('baseline ( ED + CC )', (15, 21)), ('accuracy', (23, 24)), ('increases', (25, 26)), ('both components', (27, 29))]","[['content selection and planning', 'contribute to', 'performance improvements'], ['performance improvements', 'over', 'baseline ( ED + CC )'], ['accuracy', 'further', 'increases'], ['increases', 'when', 'both components']]","[['accuracy', 'has', 'increases']]",[],[],data-to-text_generation,4,189
365,results,"Compared to the full system ( NCP + CC ) , content selection precision and recall are higher ( by 4.5 % and 2 % , respectively ) as well as content ordering ( by 1.8 % ) .","[('Compared to', (0, 2)), ('by', (19, 20)), ('as well as', (28, 31)), ('by', (34, 35))]","[('full system ( NCP + CC )', (3, 10)), ('content selection precision and recall', (11, 16)), ('higher', (17, 18)), ('4.5 % and 2 %', (20, 25)), ('content ordering', (31, 33)), ('1.8 %', (35, 37))]","[['higher', 'by', '4.5 % and 2 %'], ['content selection precision and recall', 'as well as', 'content ordering'], ['higher', 'as well as', 'content ordering'], ['content ordering', 'by', '1.8 %']]","[['full system ( NCP + CC )', 'has', 'content selection precision and recall'], ['content selection precision and recall', 'has', 'higher']]","[['Results', 'Compared to', 'full system ( NCP + CC )']]",[],data-to-text_generation,4,191
366,results,"CS precision is higher than 85 % , CS recall is higher than 93 % , and CO higher than 84 % .","[('higher than', (3, 5)), ('higher than', (11, 13)), ('higher than', (18, 20))]","[('CS precision', (0, 2)), ('85 %', (5, 7)), ('CS recall', (8, 10)), ('93 %', (13, 15)), ('CO', (17, 18)), ('84 %', (20, 22))]","[['CS precision', 'higher than', '85 %'], ['CS recall', 'higher than', '93 %'], ['CO', 'higher than', '84 %']]","[['CO', 'has', '84 %']]",[],"[['Results', 'has', 'CS precision']]",data-to-text_generation,4,194
367,results,"NCP achieves higher accuracy in all metrics including relation generation , content selection , content ordering , and BLEU compared to .","[('achieves', (1, 2)), ('in', (4, 5)), ('including', (7, 8))]","[('NCP', (0, 1)), ('higher accuracy', (2, 4)), ('all metrics', (5, 7)), ('relation generation', (8, 10)), ('content selection', (11, 13)), ('content ordering', (14, 16)), ('BLEU', (18, 19))]","[['NCP', 'achieves', 'higher accuracy'], ['higher accuracy', 'in', 'all metrics'], ['all metrics', 'including', 'relation generation'], ['all metrics', 'including', 'content selection'], ['all metrics', 'including', 'content ordering'], ['all metrics', 'including', 'BLEU']]",[],[],"[['Results', 'has', 'NCP']]",data-to-text_generation,4,197
368,results,"We find that NCP + CC over all performs best , however there is a significant gap between automatically generated summaries and human - authored ones .","[('find', (1, 2)), ('performs', (8, 9))]","[('NCP + CC over all', (3, 8)), ('best', (9, 10))]","[['NCP + CC over all', 'performs', 'best']]",[],"[['Results', 'find', 'NCP + CC over all']]",[],data-to-text_generation,4,243
369,research-problem,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation,[],[],[],[],[],[],data-to-text_generation,5,2
370,model,The system is given a set of RDF triplets describing facts ( entities and relations between them ) and has to produce a fluent text that is faithful to the facts .,"[('given', (3, 4)), ('describing', (9, 10)), ('faithful to', (27, 29))]","[('set of RDF triplets', (5, 9)), ('facts ( entities and relations', (10, 15)), ('fluent text', (23, 25)), ('facts', (30, 31))]","[['set of RDF triplets', 'describing', 'facts ( entities and relations'], ['fluent text', 'faithful to', 'facts']]",[],"[['Model', 'given', 'set of RDF triplets']]",[],data-to-text_generation,5,18
371,model,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .","[('propose', (2, 3)), ('whose', (12, 13)), ('fed into', (15, 17))]","[('explicit , symbolic , text planning stage', (4, 11)), ('output', (13, 14)), ('neural generation system', (18, 21))]","[['explicit , symbolic , text planning stage', 'whose', 'output'], ['output', 'fed into', 'neural generation system']]","[['explicit , symbolic , text planning stage', 'has', 'output']]","[['Model', 'propose', 'explicit , symbolic , text planning stage']]",[],data-to-text_generation,5,49
372,model,The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,"[('determines', (3, 4)), ('expresses it', (8, 10))]","[('text planner', (1, 3)), ('information structure', (5, 7)), ('unambiguously', (10, 11)), ('sequence of ordered trees', (17, 21))]","[['text planner', 'determines', 'information structure'], ['text planner', 'expresses it', 'unambiguously']]",[],[],"[['Model', 'has', 'text planner']]",data-to-text_generation,5,50
373,model,This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts .,"[('performed', (3, 4)), ('guaranteed to', (7, 9)), ('with regards to', (13, 16))]","[('symbolically', (4, 5)), ('remain', (9, 10)), ('input facts', (17, 19))]",[],[],[],[],data-to-text_generation,5,51
374,code,We release our code and the corpus extended with matching plans in https://github.com/AmitMY/ chimera .,[],"[('https://github.com/AmitMY/ chimera', (12, 14))]",[],[],[],[],data-to-text_generation,5,57
375,hyperparameters,We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase .,"[('map', (1, 2)), ('of', (6, 7)), ('by splitting on', (8, 11))]","[('DBPedia relations to sequences', (2, 6)), ('tokens', (7, 8)), ('underscores', (11, 12)), ('CamelCase', (13, 14))]","[['DBPedia relations to sequences', 'of', 'tokens'], ['tokens', 'by splitting on', 'underscores'], ['tokens', 'by splitting on', 'CamelCase']]",[],"[['Hyperparameters', 'map', 'DBPedia relations to sequences']]",[],data-to-text_generation,5,189
376,hyperparameters,"Concretely , we use the Open NMT toolkit with the copy attn flag .","[('use', (3, 4)), ('with', (8, 9))]","[('Open NMT toolkit', (5, 8)), ('copy attn flag', (10, 13))]","[['Open NMT toolkit', 'with', 'copy attn flag']]","[['Open NMT toolkit', 'has', 'copy attn flag']]","[['Hyperparameters', 'use', 'Open NMT toolkit']]",[],data-to-text_generation,5,190
377,hyperparameters,"The pretrained embeddings are used to initialize the relation tokens in the plans , as well as the tokens in the reference texts .","[('in', (10, 11)), ('as well as', (14, 17)), ('in', (19, 20))]","[('pretrained embeddings', (1, 3)), ('initialize', (6, 7)), ('relation tokens', (8, 10)), ('plans', (12, 13)), ('tokens', (18, 19)), ('reference texts', (21, 23))]","[['relation tokens', 'in', 'plans'], ['tokens', 'in', 'reference texts'], ['relation tokens', 'as well as', 'tokens'], ['tokens', 'in', 'reference texts']]","[['initialize', 'has', 'relation tokens']]",[],"[['Hyperparameters', 'has', 'pretrained embeddings']]",data-to-text_generation,5,192
378,baselines,"We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .","[('compare', (1, 2)), ('in', (6, 7)), ('on', (23, 24))]","[('best submissions', (4, 6)), ('WebNLG challenge', (8, 10)), ('Melbourne', (11, 12)), ('end - to - end system', (14, 20)), ('best', (22, 23)), ('all categories', (24, 26))]","[['best submissions', 'in', 'WebNLG challenge'], ['best', 'on', 'all categories']]","[['best submissions', 'has', 'WebNLG challenge'], ['WebNLG challenge', 'has', 'Melbourne'], ['Melbourne', 'has', 'end - to - end system']]",[],[],data-to-text_generation,5,208
379,baselines,"It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .","[('uses', (1, 2)), ('with', (9, 10))]","[('set encoder', (3, 5)), ('LSTM decoder', (7, 9)), ('attention', (10, 11)), ('copy - attention mechanism', (13, 17)), ('neural checklist model', (19, 22)), ('entity dropout', (27, 29))]","[['LSTM decoder', 'with', 'attention'], ['LSTM decoder', 'with', 'neural checklist model']]",[],[],[],data-to-text_generation,5,210
380,baselines,6 Experiments and Results,[],[],[],[],[],[],data-to-text_generation,5,213
381,results,"BestPlan reduces all error types compared to StrongNeural , by 85 % , 56 % and 90 % respectively .","[('reduces', (1, 2)), ('compared to', (5, 7)), ('by', (9, 10))]","[('BestPlan', (0, 1)), ('all error types', (2, 5)), ('StrongNeural', (7, 8)), ('85 % , 56 % and 90 %', (10, 18))]","[['BestPlan', 'reduces', 'all error types'], ['all error types', 'compared to', 'StrongNeural'], ['StrongNeural', 'by', '85 % , 56 % and 90 %']]",[],[],"[['Results', 'has', 'BestPlan']]",data-to-text_generation,5,228
382,results,"BestPlan performed on - par with StrongNeural , and surpassed the previous state - of - the - art UPF - FORGe .","[('performed', (1, 2)), ('with', (5, 6)), ('surpassed', (9, 10))]","[('BestPlan', (0, 1)), ('on - par', (2, 5)), ('StrongNeural', (6, 7)), ('previous state - of - the - art UPF - FORGe', (11, 22))]","[['BestPlan', 'performed', 'on - par'], ['on - par', 'with', 'StrongNeural'], ['BestPlan', 'surpassed', 'previous state - of - the - art UPF - FORGe']]",[],[],"[['Results', 'has', 'BestPlan']]",data-to-text_generation,5,244
383,research-problem,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation,[],[],[],[],[],[],data-to-text_generation,6,2
384,research-problem,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .",[],"[('natural language generation', (19, 22))]",[],[],[],[],data-to-text_generation,6,4
385,research-problem,"Sequence - to - sequence frameworks have proved to be very effective in natural language generation ( NLG ) tasks , as well as in machine translation and in language modeling .",[],"[('Sequence - to - sequence frameworks', (0, 6)), ('natural language generation ( NLG )', (13, 19))]",[],[],[],[],data-to-text_generation,6,16
386,model,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .","[('present', (15, 16)), ('with', (26, 27)), ('results in', (30, 32))]","[('character - level sequence - to - sequence model', (17, 26)), ('attention mechanism', (27, 29)), ('completely neural end - to - end architecture', (33, 41))]","[['character - level sequence - to - sequence model', 'with', 'attention mechanism'], ['attention mechanism', 'results in', 'completely neural end - to - end architecture']]",[],"[['Model', 'present', 'character - level sequence - to - sequence model']]",[],data-to-text_generation,6,23
387,model,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .","[('does not require', (10, 13))]","[('delexicalization', (13, 14)), ('tokenization', (15, 16)), ('lowercasing', (17, 18))]",[],[],[],[],data-to-text_generation,6,24
388,model,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .","[('shows', (5, 6)), ('with respect to', (10, 13)), ('consisting in', (33, 35)), ('between', (38, 39)), ('disengages', (45, 46)), ('to learn', (48, 50)), ('improves', (67, 68)), ('enhancing', (73, 74))]","[('two important features', (6, 9)), ('state - of - art architecture', (14, 20)), ('character - wise copy mechanism', (27, 32)), ('soft switch', (36, 38)), ('generation and copy mode', (39, 43)), ('model', (47, 48)), ('rare and unhelpful self - correspondences', (50, 56)), ('peculiar training procedure', (62, 65)), ('internal representation capabilities', (69, 72)), ('recall', (74, 75))]","[['two important features', 'with respect to', 'state - of - art architecture'], ['character - wise copy mechanism', 'consisting in', 'soft switch'], ['soft switch', 'between', 'generation and copy mode'], ['generation and copy mode', 'disengages', 'model'], ['model', 'to learn', 'rare and unhelpful self - correspondences'], ['state - of - art architecture', 'enhancing', 'recall'], ['internal representation capabilities', 'enhancing', 'recall']]","[['two important features', 'has', 'state - of - art architecture']]","[['Model', 'shows', 'two important features']]",[],data-to-text_generation,6,27
389,experimental-setup,"We developed our system using the PyTorch framework 2 , release 0.4.1 3 .","[('developed', (1, 2)), ('using', (4, 5))]","[('our system', (2, 4)), ('PyTorch framework', (6, 8))]","[['our system', 'using', 'PyTorch framework']]",[],"[['Experimental setup', 'developed', 'our system']]",[],data-to-text_generation,6,105
390,experimental-setup,"The training has been carried out as described in subsection 2.3 : this training procedure needs the two GRUs to have the same dimensions , in terms of input size , hidden size , number of layers and presence of a bias term .","[('to have', (19, 21)), ('in terms of', (25, 28))]","[('same dimensions', (22, 24)), ('input size', (28, 30)), ('hidden size', (31, 33)), ('number of layers', (34, 37)), ('presence of', (38, 40)), ('bias term', (41, 43))]","[['same dimensions', 'in terms of', 'input size'], ['same dimensions', 'in terms of', 'presence of']]","[['presence of', 'has', 'bias term']]","[['Experimental setup', 'to have', 'same dimensions']]",[],data-to-text_generation,6,106
391,ablation-analysis,"Moreover , they both have to be bidirectional , even if the decoder ignores the backward part of its current GRU .","[('have to be', (4, 7))]","[('bidirectional', (7, 8))]",[],[],[],[],data-to-text_generation,6,107
392,experimental-setup,"We minimize the negative log - likelihood loss using teacher forcing and Adam , the latter being an optimizer that computes individual adaptive learning rates .","[('minimize', (1, 2)), ('using', (8, 9)), ('computes', (20, 21))]","[('negative log - likelihood loss', (3, 8)), ('teacher forcing and Adam', (9, 13)), ('individual adaptive learning rates', (21, 25))]","[['negative log - likelihood loss', 'using', 'teacher forcing and Adam']]",[],"[['Experimental setup', 'minimize', 'negative log - likelihood loss']]",[],data-to-text_generation,6,108
393,baselines,We also propose a new formulation of P ( c ) that helps the model to learn when it is necessary to start a copying phase :,"[('propose', (2, 3)), ('of', (6, 7))]","[('new formulation', (4, 6)), ('P ( c )', (7, 11)), ('model', (14, 15)), ('necessary', (20, 21)), ('copying phase', (24, 26))]","[['new formulation', 'of', 'P ( c )']]",[],"[['Baselines', 'propose', 'new formulation']]",[],data-to-text_generation,6,110
394,baselines,"The second one is TGen , a word - based model , still derived from , but integrating a beam search mechanism and a reranker over the top k outputs , in order to dis advantage utterances that do not verbalize all the information contained in the MR .","[('integrating', (17, 18)), ('over', (25, 26)), ('to dis advantage', (33, 36)), ('that do not verbalize', (37, 41)), ('contained in', (44, 46))]","[('TGen', (4, 5)), ('beam search mechanism and a reranker', (19, 25)), ('top k outputs', (27, 30)), ('utterances', (36, 37)), ('all the information', (41, 44)), ('MR', (47, 48))]","[['beam search mechanism and a reranker', 'over', 'top k outputs'], ['beam search mechanism and a reranker', 'to dis advantage', 'utterances'], ['utterances', 'that do not verbalize', 'all the information'], ['all the information', 'contained in', 'MR']]",[],[],"[['Baselines', 'has', 'TGen']]",data-to-text_generation,6,121
395,ablation-analysis,"We used the official code provided in the E2E NLG Challenge website for TGen , and we developed our models and EDA in PyTorch , training them on NVIDIA GPUs .","[('used', (1, 2)), ('provided in', (5, 7)), ('for', (12, 13)), ('developed', (17, 18)), ('in', (22, 23)), ('training them on', (25, 28))]","[('official code', (3, 5)), ('E2E NLG Challenge website', (8, 12)), ('TGen', (13, 14)), ('our models and EDA', (18, 22)), ('PyTorch', (23, 24)), ('NVIDIA GPUs', (28, 30))]","[['official code', 'provided in', 'E2E NLG Challenge website'], ['E2E NLG Challenge website', 'for', 'TGen'], ['official code', 'developed', 'our models and EDA'], ['our models and EDA', 'in', 'PyTorch'], ['our models and EDA', 'training them on', 'NVIDIA GPUs']]",[],"[['Ablation analysis', 'used', 'official code']]",[],data-to-text_generation,6,123
396,results,"A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .","[('is', (4, 5)), ('always obtains', (9, 11)), ('with respect to', (14, 17)), ('on', (18, 19)), ('on', (33, 34))]","[('our model EDA_CS', (6, 9)), ('higher metric values', (11, 14)), ('TGen', (17, 18)), ('Hotel and Restaurant datasets', (20, 24)), ('three out of five higher metrics values', (26, 33)), ('E2E dataset', (35, 37))]","[['our model EDA_CS', 'always obtains', 'higher metric values'], ['higher metric values', 'with respect to', 'TGen'], ['TGen', 'on', 'Hotel and Restaurant datasets'], ['TGen', 'on', 'E2E dataset'], ['three out of five higher metrics values', 'on', 'E2E dataset'], ['three out of five higher metrics values', 'on', 'E2E dataset']]",[],"[['Results', 'is', 'our model EDA_CS']]",[],data-to-text_generation,6,137
397,results,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .","[('in the case of', (2, 6)), ('achieves', (10, 11))]","[('E2E +', (6, 8)), ('TGen', (9, 10)), ('three out of five higher metrics values', (11, 18))]","[['TGen', 'achieves', 'three out of five higher metrics values']]","[['E2E +', 'has', 'TGen']]",[],[],data-to-text_generation,6,138
398,results,"A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .","[('allows to obtain', (10, 13)), ('with respect to', (15, 18)), ('in', (20, 21)), ('on', (24, 25))]","[('approach', (7, 8)), ('better performance', (13, 15)), ('training', (18, 19)), ('EDA_CS', (19, 20)), ('standard way', (22, 24)), ('Hotel and Restaurant datasets', (26, 30)), ('outperforms', (43, 44)), ('EDA_CS', (44, 45)), ('one case', (47, 49))]","[['approach', 'allows to obtain', 'better performance'], ['better performance', 'with respect to', 'training'], ['training', 'in', 'standard way'], ['EDA_CS', 'in', 'standard way'], ['standard way', 'on', 'Hotel and Restaurant datasets']]","[['training', 'has', 'EDA_CS'], ['outperforms', 'has', 'EDA_CS']]",[],[],data-to-text_generation,6,140
399,results,"Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .","[('shows', (4, 5)), ('of', (8, 9)), ('with respect to', (13, 16)), ('when compared to', (19, 22))]","[('EDA_CS TL', (2, 4)), ('bleu increment', (6, 8)), ('at least 14 %', (9, 13)), (""TGen 's score"", (16, 19)), ('Hotel and Restaurant datasets', (23, 27))]","[['EDA_CS TL', 'shows', 'bleu increment'], ['bleu increment', 'of', 'at least 14 %'], ['at least 14 %', 'with respect to', ""TGen 's score""], [""TGen 's score"", 'when compared to', 'Hotel and Restaurant datasets']]","[['EDA_CS TL', 'has', 'bleu increment']]",[],"[['Results', 'has', 'EDA_CS TL']]",data-to-text_generation,6,141
400,results,"Finally , the baseline model , EDA , is largely outperformed by all other examined methods .","[('by', (11, 12))]","[('baseline model', (3, 5)), ('EDA', (6, 7)), ('largely outperformed', (9, 11)), ('all other examined methods', (12, 16))]","[['largely outperformed', 'by', 'all other examined methods']]","[['baseline model', 'name', 'EDA']]",[],"[['Results', 'has', 'baseline model']]",data-to-text_generation,6,142
401,results,"We highlight that EDA_CS 's model 's good results are achieved even if it consists in a fully end - to - end model which does not benefit from the delexicalizationrelexicalization procedure , differently from TGen .","[('highlight', (1, 2)), ('are', (9, 10))]","[(""EDA_CS 's model 's"", (3, 7)), ('good results', (7, 9)), ('achieved', (10, 11))]","[['good results', 'are', 'achieved']]","[[""EDA_CS 's model 's"", 'has', 'good results'], ['good results', 'has', 'achieved']]","[['Results', 'highlight', ""EDA_CS 's model 's""]]",[],data-to-text_generation,6,144
402,research-problem,An improved neural network model for joint POS tagging and dependency parsing,"[('for', (5, 6))]",[],[],[],[],[],dependency_parsing,0,2
403,code,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,[],"[('https://github.com/datquocnguyen/jPTDP', (11, 12))]",[],[],[],[],dependency_parsing,0,9
404,model,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .","[('present', (5, 6)), ('for jointly', (13, 15))]","[('POS tagging and dependency paring', (16, 21))]",[],[],"[['Model', 'present', 'POS tagging and dependency paring']]",[],dependency_parsing,0,18
405,results,"As mentioned in Section 4 , our model generally outperforms j PTDP v1.0 with 2.5 + % LAS improvements on universal dependencies ( UD ) treebanks .","[('with', (13, 14)), ('on', (19, 20))]","[('j PTDP v1.0', (10, 13)), ('2.5 + % LAS improvements', (14, 19)), ('universal dependencies ( UD ) treebanks', (20, 26))]","[['j PTDP v1.0', 'with', '2.5 + % LAS improvements'], ['2.5 + % LAS improvements', 'on', 'universal dependencies ( UD ) treebanks']]",[],[],[],dependency_parsing,0,71
406,code,"Our model is released as jPTDP v2.0 , available at https://github.com/datquocnguyen/",[],[],[],[],[],[],dependency_parsing,0,73
407,hyperparameters,Our jPTDP v 2.0 is implemented using DYNET v2.0 with a fixed random seed .,"[('implemented using', (5, 7)), ('with', (9, 10))]","[('DYNET v2.0', (7, 9)), ('fixed random seed', (11, 14))]","[['DYNET v2.0', 'with', 'fixed random seed']]",[],"[['Hyperparameters', 'implemented using', 'DYNET v2.0']]",[],dependency_parsing,0,75
408,hyperparameters,"Word embeddings are initialized either randomly or by pre-trained word vectors , while character and POS tag embeddings are randomly initialized .","[('initialized', (3, 4)), ('are', (18, 19))]","[('Word embeddings', (0, 2)), ('randomly', (5, 6)), ('pre-trained word vectors', (8, 11)), ('character and POS tag embeddings', (13, 18)), ('randomly initialized', (19, 21))]","[['Word embeddings', 'initialized', 'randomly'], ['character and POS tag embeddings', 'are', 'randomly initialized']]","[['character and POS tag embeddings', 'has', 'randomly initialized']]",[],"[['Hyperparameters', 'has', 'Word embeddings']]",dependency_parsing,0,77
409,hyperparameters,"For learning character - level word embeddings , we use one - layer BiLSTM seq , and set the size of LSTM hidden states to be equal to the vector size of character embeddings .","[('For', (0, 1)), ('use', (9, 10)), ('set', (17, 18)), ('of', (20, 21)), ('to be equal to', (24, 28))]","[('learning character - level word embeddings', (1, 7)), ('one - layer BiLSTM seq', (10, 15)), ('size', (19, 20)), ('LSTM hidden states', (21, 24)), ('vector size of', (29, 32)), ('character embeddings', (32, 34))]","[['learning character - level word embeddings', 'use', 'one - layer BiLSTM seq'], ['learning character - level word embeddings', 'set', 'size'], ['size', 'of', 'LSTM hidden states'], ['LSTM hidden states', 'to be equal to', 'vector size of']]","[['vector size of', 'has', 'character embeddings']]","[['Hyperparameters', 'For', 'learning character - level word embeddings']]",[],dependency_parsing,0,78
410,hyperparameters,We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,"[('apply', (1, 2)), ('with', (3, 4)), ('to', (9, 10))]","[('dropout', (2, 3)), ('67 % keep probability', (5, 9)), ('inputs of BiLSTMs and MLPs', (11, 16))]","[['dropout', 'with', '67 % keep probability'], ['67 % keep probability', 'to', 'inputs of BiLSTMs and MLPs']]","[['dropout', 'has', '67 % keep probability']]","[['Hyperparameters', 'apply', 'dropout']]",[],dependency_parsing,0,79
411,hyperparameters,"Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .","[('apply', (5, 6)), ('to learn', (8, 10)), ('for', (12, 13)), ('replace', (17, 18)), ('in', (28, 29)), ('with', (32, 33)), ('with', (39, 40))]","[('word dropout', (6, 8)), ('embedding', (11, 12)), ('unknown words', (13, 15)), ('each word token', (18, 21)), ('appearing # ( w ) times', (22, 28)), ('training set', (30, 32)), ('special "" unk "" symbol', (34, 39)), ('probability punk ( w )', (40, 45))]","[['each word token', 'apply', 'appearing # ( w ) times'], ['word dropout', 'to learn', 'embedding'], ['embedding', 'for', 'unknown words'], ['embedding', 'replace', 'each word token'], ['appearing # ( w ) times', 'in', 'training set'], ['each word token', 'with', 'special "" unk "" symbol'], ['training set', 'with', 'special "" unk "" symbol'], ['special "" unk "" symbol', 'with', 'probability punk ( w )']]","[['each word token', 'has', 'appearing # ( w ) times']]","[['Hyperparameters', 'apply', 'word dropout']]",[],dependency_parsing,0,80
412,hyperparameters,"We optimize the objective loss using Adam ( Kingma and Ba , 2014 ) with an initial learning rate at 0.001 and no mini-batches .","[('optimize', (1, 2)), ('using', (5, 6)), ('with', (14, 15)), ('at', (19, 20))]","[('objective loss', (3, 5)), ('Adam ( Kingma and Ba , 2014 )', (6, 14)), ('initial learning rate', (16, 19)), ('0.001', (20, 21))]","[['objective loss', 'using', 'Adam ( Kingma and Ba , 2014 )'], ['Adam ( Kingma and Ba , 2014 )', 'with', 'initial learning rate'], ['initial learning rate', 'at', '0.001']]","[['initial learning rate', 'has', '0.001']]","[['Hyperparameters', 'optimize', 'objective loss']]",[],dependency_parsing,0,82
413,experiments,"For training , we run for 30 epochs , and restart the Adam optimizer and anneal its initial learning rate at a proportion of 0.5 every 10 epochs .","[('For', (0, 1)), ('run for', (4, 6)), ('restart', (10, 11)), ('anneal', (15, 16)), ('at', (20, 21)), ('of', (23, 24)), ('every', (25, 26))]","[('training', (1, 2)), ('30 epochs', (6, 8)), ('Adam optimizer', (12, 14)), ('initial learning rate', (17, 20)), ('proportion', (22, 23)), ('0.5', (24, 25)), ('10 epochs', (26, 28))]","[['training', 'run for', '30 epochs'], ['initial learning rate', 'at', 'proportion'], ['proportion', 'of', '0.5'], ['0.5', 'every', '10 epochs']]",[],[],[],dependency_parsing,0,83
414,hyperparameters,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .","[('use', (9, 10))]","[('100 - dimensional word embeddings', (10, 15)), ('50 - dimensional character embeddings', (16, 21)), ('100 dimensional POS tag embeddings', (22, 27))]",[],[],"[['Hyperparameters', 'use', '100 - dimensional word embeddings']]",[],dependency_parsing,0,86
415,hyperparameters,We also fix the number of hidden nodes in MLPs at 100 .,"[('fix', (2, 3)), ('in', (8, 9)), ('at', (10, 11))]","[('number of hidden nodes', (4, 8)), ('MLPs', (9, 10)), ('100', (11, 12))]","[['number of hidden nodes', 'in', 'MLPs'], ['number of hidden nodes', 'at', '100'], ['MLPs', 'at', '100']]",[],"[['Hyperparameters', 'fix', 'number of hidden nodes']]",[],dependency_parsing,0,87
416,hyperparameters,"Due to limited computational resource , for experiments presented in Section 3 , we perform a minimal grid search of hyper - parameters to select the number of BiLSTM pos and BiLSTM dep layers from { 1 , 2 } and the size of LSTM hidden states in each layer from { 128 , 256 } .","[('perform', (14, 15)), ('of', (19, 20)), ('to select', (23, 25)), ('from', (34, 35)), ('in', (47, 48)), ('from', (50, 51))]","[('minimal grid search', (16, 19)), ('hyper - parameters', (20, 23)), ('number of BiLSTM pos and BiLSTM dep layers', (26, 34)), ('{ 1 , 2 }', (35, 40)), ('size of LSTM hidden states', (42, 47)), ('each layer', (48, 50)), ('{ 128 , 256 }', (51, 56))]","[['minimal grid search', 'of', 'hyper - parameters'], ['hyper - parameters', 'to select', 'number of BiLSTM pos and BiLSTM dep layers'], ['hyper - parameters', 'to select', 'size of LSTM hidden states'], ['number of BiLSTM pos and BiLSTM dep layers', 'from', '{ 1 , 2 }'], ['each layer', 'from', '{ 128 , 256 }'], ['size of LSTM hidden states', 'in', 'each layer'], ['each layer', 'from', '{ 128 , 256 }']]",[],"[['Hyperparameters', 'perform', 'minimal grid search']]",[],dependency_parsing,0,88
417,hyperparameters,"For experiments presented in sections 4 and 5 , we fix the number of BiLSTM layers at 2 and the size of hidden states at 128 .","[('fix', (10, 11)), ('at', (16, 17)), ('at', (24, 25))]","[('number of BiLSTM layers', (12, 16)), ('2', (17, 18)), ('size of', (20, 22)), ('hidden states', (22, 24)), ('128', (25, 26))]","[['number of BiLSTM layers', 'at', '2'], ['hidden states', 'at', '128'], ['hidden states', 'at', '128']]","[['size of', 'has', 'hidden states']]","[['Hyperparameters', 'fix', 'number of BiLSTM layers']]",[],dependency_parsing,0,89
418,experiments,Word embeddings are initialized by 100 dimensional Glo Ve word vectors pre-trained on Wikipedia and Gigaword .,"[('initialized by', (3, 5)), ('pre-trained on', (11, 13))]","[('Word embeddings', (0, 2)), ('100 dimensional Glo Ve word vectors', (5, 11)), ('Wikipedia and Gigaword', (13, 16))]","[['Word embeddings', 'initialized by', '100 dimensional Glo Ve word vectors'], ['100 dimensional Glo Ve word vectors', 'pre-trained on', 'Wikipedia and Gigaword']]",[],[],[],dependency_parsing,0,94
419,experiments,"As mentioned in Section 2.5 , we perform a minimal grid search of hyper - parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256 - dimensional LSTM hidden states ( in , we present scores obtained on the development set when using 2 BiLSTM layers ) .","[('perform', (7, 8)), ('of', (12, 13)), ('find that', (17, 19)), ('on', (23, 24)), ('obtained when using', (28, 31))]","[('minimal', (9, 10)), ('highest mixed accuracy', (20, 23)), ('development set', (25, 27)), ('2 BiLSTM layers', (31, 34))]","[['minimal', 'find that', 'highest mixed accuracy'], ['highest mixed accuracy', 'on', 'development set'], ['highest mixed accuracy', 'obtained when using', '2 BiLSTM layers']]","[['minimal', 'has', 'highest mixed accuracy']]",[],[],dependency_parsing,0,96
420,experiments,"Clearly , our model produces very competitive parsing results .","[('produces', (4, 5))]","[('our model', (2, 4)), ('very competitive parsing results', (5, 9))]","[['our model', 'produces', 'very competitive parsing results']]",[],[],[],dependency_parsing,0,99
421,experiments,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .","[('obtains', (5, 6)), ('at', (9, 10))]","[('UAS score', (7, 9)), ('94.51 %', (10, 12)), ('LAS score', (14, 16)), ('92.87 %', (17, 19))]","[['UAS score', 'at', '94.51 %'], ['LAS score', 'at', '92.87 %']]",[],[],[],dependency_parsing,0,100
422,experiments,"Our model also does better than the previous transition - based joint models in , and , while obtaining similar UAS and LAS scores to the joint model JMT proposed by .","[('does', (3, 4)), ('than', (5, 6)), ('to', (24, 25))]","[('Our model', (0, 2)), ('better', (4, 5)), ('previous transition - based joint models', (7, 13)), ('similar UAS and LAS scores', (19, 24)), ('joint model JMT', (26, 29))]","[['Our model', 'does', 'better'], ['better', 'than', 'previous transition - based joint models'], ['similar UAS and LAS scores', 'to', 'joint model JMT']]","[['Our model', 'has', 'better']]",[],[],dependency_parsing,0,101
423,experiments,We achieve 0.9 % lower parsing scores than the state - of - the - art dependency parser of .,"[('achieve', (1, 2)), ('than', (7, 8))]","[('0.9 % lower parsing scores', (2, 7)), ('state - of - the - art dependency parser', (9, 18))]","[['0.9 % lower parsing scores', 'than', 'state - of - the - art dependency parser']]",[],[],[],dependency_parsing,0,102
424,experiments,"While also a BiLSTM - and graph - based model , it uses a more sophisticated attention mechanism "" biaffine "" for better decoding dependency arcs and relation types .","[('uses', (12, 13)), ('for better decoding', (21, 24))]","[('BiLSTM - and graph - based model', (3, 10)), ('more sophisticated attention mechanism', (14, 18)), ('biaffine', (19, 20)), ('dependency arcs and relation types', (24, 29))]","[['BiLSTM - and graph - based model', 'uses', 'more sophisticated attention mechanism'], ['more sophisticated attention mechanism', 'for better decoding', 'dependency arcs and relation types'], ['biaffine', 'for better decoding', 'dependency arcs and relation types']]","[['more sophisticated attention mechanism', 'name', 'biaffine']]",[],[],dependency_parsing,0,103
425,experiments,"In future work , we will extend our model with the biaffine attention mechanism to investigate the benefit for our model .","[('extend', (6, 7)), ('with', (9, 10)), ('to investigate', (14, 16)), ('for', (18, 19))]","[('our model', (7, 9)), ('biaffine attention mechanism', (11, 14)), ('benefit', (17, 18)), ('our model', (19, 21))]","[['our model', 'with', 'biaffine attention mechanism'], ['biaffine attention mechanism', 'to investigate', 'benefit'], ['benefit', 'for', 'our model']]",[],[],[],dependency_parsing,0,104
426,experiments,"We also obtain a state - of - the - art POS tagging accuracy at 97.97 % on the test Section 23 , which is about 0.4 + % higher than those by , and .","[('obtain', (2, 3)), ('at', (14, 15)), ('on', (17, 18))]","[('state - of - the - art', (4, 11)), ('POS tagging accuracy', (11, 14)), ('97.97 %', (15, 17)), ('test Section', (19, 21))]","[['POS tagging accuracy', 'at', '97.97 %'], ['POS tagging accuracy', 'on', 'test Section'], ['97.97 %', 'on', 'test Section']]","[['state - of - the - art', 'has', 'POS tagging accuracy']]",[],[],dependency_parsing,0,106
427,experiments,4 4 UniMelb in the CoNLL 2018 shared task on UD parsing,"[('on', (9, 10))]",[],[],[],[],[],dependency_parsing,0,108
428,experiments,"For each big or small treebank , we train a joint model for universal POS tagging and dependency parsing , using a fixed random seed and a fixed set .","[('For', (0, 1)), ('train', (8, 9)), ('for', (12, 13)), ('using', (20, 21))]","[('each big or small treebank', (1, 6)), ('joint model', (10, 12)), ('universal POS tagging and dependency parsing', (13, 19)), ('fixed random seed', (22, 25))]","[['joint model', 'For', 'universal POS tagging and dependency parsing'], ['each big or small treebank', 'train', 'joint model'], ['joint model', 'for', 'universal POS tagging and dependency parsing'], ['universal POS tagging and dependency parsing', 'using', 'fixed random seed']]",[],[],[],dependency_parsing,0,114
429,experiments,"Here , we utilize the tokenization , word and sentence segmentation predicted by UD - Pipe 1.2 .","[('utilize', (3, 4)), ('predicted by', (11, 13))]","[('tokenization , word and sentence segmentation', (5, 11)), ('UD - Pipe', (13, 16))]","[['tokenization , word and sentence segmentation', 'predicted by', 'UD - Pipe']]","[['tokenization , word and sentence segmentation', 'has', 'UD - Pipe']]",[],[],dependency_parsing,0,121
430,experiments,The final test runs are carried out on the TIRA platform .,"[('carried out on', (5, 8))]","[('final test runs', (1, 4)), ('TIRA platform', (9, 11))]","[['final test runs', 'carried out on', 'TIRA platform']]",[],[],[],dependency_parsing,0,125
431,experiments,presents our results in the CoNLL 2018 shared task on multilingual parsing from raw texts to universal dependencies .,[],"[('multilingual parsing', (10, 12))]",[],[],[],[],dependency_parsing,0,126
432,experiments,"Over all 82 test sets , we outperform the baseline UDPipe 1.2 with 0.6 % absolute higher average UPOS F1 score and 2.5 + % higher average UAS and LAS F1 scores .","[('Over', (0, 1)), ('with', (12, 13))]","[('all 82 test sets', (1, 5)), ('outperform', (7, 8)), ('baseline UDPipe 1.2', (9, 12)), ('0.6 % absolute higher average UPOS F1 score', (13, 21)), ('2.5 + % higher average UAS and LAS F1 scores', (22, 32))]","[['outperform', 'with', '2.5 + % higher average UAS and LAS F1 scores'], ['baseline UDPipe 1.2', 'with', '0.6 % absolute higher average UPOS F1 score'], ['baseline UDPipe 1.2', 'with', '2.5 + % higher average UAS and LAS F1 scores']]","[['all 82 test sets', 'has', 'outperform'], ['outperform', 'has', 'baseline UDPipe 1.2']]",[],[],dependency_parsing,0,127
433,experiments,"In particular , for the "" big "" category consisting of 61 treebank test sets , we obtain 0.8 % higher UPOS and 3.1 % higher UAS and 3.6 % higher LAS than UDPipe 1.2 .","[('for', (3, 4)), ('consisting of', (9, 11)), ('obtain', (17, 18)), ('than', (32, 33))]","[('"" big "" category', (5, 9)), ('61 treebank test sets', (11, 15)), ('0.8 % higher', (18, 21)), ('UPOS', (21, 22)), ('3.1 % higher', (23, 26)), ('UAS', (26, 27)), ('3.6 % higher LAS', (28, 32)), ('UDPipe 1.2', (33, 35))]","[['"" big "" category', 'consisting of', '61 treebank test sets'], ['"" big "" category', 'obtain', '0.8 % higher'], ['"" big "" category', 'obtain', '3.1 % higher'], ['"" big "" category', 'obtain', '3.6 % higher LAS'], ['3.6 % higher LAS', 'than', 'UDPipe 1.2']]","[['0.8 % higher', 'has', 'UPOS'], ['3.1 % higher', 'has', 'UAS']]",[],[],dependency_parsing,0,128
434,experiments,Our ( UniMelb ) official LAS - based rank is at 14 th place while the baseline UDPipe 1.2 is at 18 th place over total 26 participating systems .,"[('is at', (9, 11)), ('is at', (19, 21)), ('over', (24, 25))]","[('Our ( UniMelb ) official LAS - based rank', (0, 9)), ('14 th place', (11, 14)), ('baseline UDPipe 1.2', (16, 19)), ('18 th place', (21, 24)), ('total 26 participating systems', (25, 29))]","[['Our ( UniMelb ) official LAS - based rank', 'is at', '14 th place'], ['baseline UDPipe 1.2', 'is at', '18 th place'], ['baseline UDPipe 1.2', 'is at', '18 th place'], ['18 th place', 'over', 'total 26 participating systems']]","[['Our ( UniMelb ) official LAS - based rank', 'has', '14 th place']]",[],[],dependency_parsing,0,129
435,experiments,"In , we also present our average UPOS , UAS and LAS accuracies with respect to ( w.r.t. ) gold - standard tokenization , word and sentence segmentation .","[('present', (4, 5)), ('with respect to', (13, 16))]","[('gold - standard tokenization', (19, 23)), ('word and sentence segmentation', (24, 28))]",[],[],[],[],dependency_parsing,0,136
436,experiments,"In particular , we achieved the highest F 1 scores for both biomedical event extraction and opinion analysis .","[('achieved', (4, 5)), ('for', (10, 11))]","[('highest F 1 scores', (6, 10)), ('biomedical event extraction', (12, 15)), ('opinion analysis', (16, 18))]","[['highest F 1 scores', 'for', 'biomedical event extraction'], ['highest F 1 scores', 'for', 'opinion analysis']]",[],[],[],dependency_parsing,0,151
437,research-problem,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,[],"[('Dependency Parsing', (3, 5))]",[],[],[],[],dependency_parsing,1,2
438,research-problem,We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) .,"[('based on', (12, 14))]","[('dependency parsing', (8, 10))]",[],[],[],[],dependency_parsing,1,4
439,model,"The focus of this paper is on feature representation for dependency parsing , using recent techniques from the neural - networks ( "" deep learning "" ) literature .",[],"[('feature representation for', (7, 10)), ('dependency parsing', (10, 12))]",[],"[['feature representation for', 'has', 'dependency parsing']]",[],[],dependency_parsing,1,10
440,model,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .","[('centered around', (7, 9)), ('which are', (16, 18))]","[('BiRNNs', (9, 10)), ('BiLSTMs', (14, 15)), ('strong and trainable sequence models', (18, 23))]","[['BiLSTMs', 'which are', 'strong and trainable sequence models']]",[],"[['Model', 'centered around', 'BiRNNs']]",[],dependency_parsing,1,24
441,model,"The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an "" infinite "" window around it .","[('excels at', (2, 4)), ('representing', (4, 5)), ('in', (6, 7)), ('capturing', (19, 20))]","[('BiLSTM', (1, 2)), ('elements', (5, 6)), ('sequence ( i.e. , words )', (8, 14)), ('contexts', (17, 18)), ('element', (21, 22)), ('"" infinite "" window', (24, 28))]","[['elements', 'in', 'sequence ( i.e. , words )'], ['contexts', 'capturing', 'element']]",[],[],"[['Model', 'has', 'BiLSTM']]",dependency_parsing,1,25
442,model,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .","[('represent', (1, 2)), ('by', (4, 5)), ('use', (10, 11)), ('of', (13, 14)), ('of', (17, 18)), ('as', (21, 22)), ('passed to', (29, 31))]","[('each word', (2, 4)), ('concatenation', (12, 13)), ('minimal set', (15, 17)), ('our feature function', (22, 25)), ('non-linear scoring function ( multi - layer perceptron )', (32, 41))]","[['concatenation', 'of', 'minimal set']]","[['concatenation', 'has', 'minimal set']]","[['Model', 'represent', 'each word']]",[],dependency_parsing,1,26
443,model,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .","[('In', (0, 1)), ('jointly train', (6, 8)), ('on top of', (13, 16)), ('propagating', (19, 20)), ('from', (21, 22))]","[('graphbased parser', (2, 4)), ('structured - prediction model', (9, 13)), ('BiLSTM', (17, 18)), ('errors', (20, 21)), ('structured objective', (23, 25)), ('BiLSTM feature - encoder', (31, 35))]","[['graphbased parser', 'jointly train', 'structured - prediction model'], ['structured - prediction model', 'on top of', 'BiLSTM'], ['structured - prediction model', 'propagating', 'errors'], ['BiLSTM', 'propagating', 'errors'], ['errors', 'from', 'structured objective']]",[],"[['Model', 'In', 'graphbased parser']]",[],dependency_parsing,1,30
444,results,"For Chinese , we use the Penn Chinese Treebank 5.1 ( CTB5 ) , using the train / test / dev splits of with gold partof - speech tags , also following .","[('use', (4, 5)), ('using', (14, 15)), ('of with', (22, 24))]","[('Chinese', (1, 2)), ('Penn Chinese Treebank 5.1 ( CTB5 )', (6, 13)), ('train / test / dev splits', (16, 22)), ('gold partof - speech tags', (24, 29))]","[['Chinese', 'use', 'Penn Chinese Treebank 5.1 ( CTB5 )'], ['Penn Chinese Treebank 5.1 ( CTB5 )', 'using', 'train / test / dev splits'], ['train / test / dev splits', 'of with', 'gold partof - speech tags']]",[],[],[],dependency_parsing,1,278
445,hyperparameters,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .","[('implemented in', (3, 5)), ('using', (7, 8)), ('for', (12, 13))]","[('parsers', (1, 2)), ('python', (5, 6)), ('neural network training', (13, 16))]","[['parsers', 'implemented in', 'python']]",[],[],"[['Hyperparameters', 'has', 'parsers']]",dependency_parsing,1,281
446,code,The code is available at the github repository https://github.com/elikip / bist -parser .,[],"[('https://github.com/elikip / bist -parser', (8, 12))]",[],[],[],[],dependency_parsing,1,282
447,hyperparameters,"We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .","[('use', (1, 2)), ('implemented in', (5, 7)), ('optimize using', (10, 12))]","[('LSTM variant', (3, 5)), ('PyCNN', (7, 8)), ('Adam optimizer', (13, 15))]","[['LSTM variant', 'implemented in', 'PyCNN'], ['LSTM variant', 'optimize using', 'Adam optimizer']]",[],"[['Hyperparameters', 'use', 'LSTM variant']]",[],dependency_parsing,1,283
448,code,11 https://github.com/clab/cnn/tree/,[],[],[],[],[],[],dependency_parsing,1,286
449,hyperparameters,The word and POS embeddings e ( w i ) and e ( p i ) are initialized to random values and trained together with the rest of the parsers ' networks .,"[('initialized to', (17, 19)), ('trained together with', (22, 25))]","[('word and POS embeddings e ( w i ) and e ( p i )', (1, 16)), ('random values', (19, 21)), (""rest of the parsers ' networks"", (26, 32))]","[['word and POS embeddings e ( w i ) and e ( p i )', 'initialized to', 'random values'], ['word and POS embeddings e ( w i ) and e ( p i )', 'trained together with', ""rest of the parsers ' networks""]]",[],[],"[['Hyperparameters', 'has', 'word and POS embeddings e ( w i ) and e ( p i )']]",dependency_parsing,1,288
450,results,"We train the parsers for up to 30 iterations , and choose the best model according to the UAS accuracy on the development set .","[('train', (1, 2)), ('for', (4, 5)), ('choose', (11, 12)), ('according to', (15, 17)), ('on', (20, 21))]","[('parsers', (3, 4)), ('up to 30 iterations', (5, 9)), ('best model', (13, 15)), ('UAS accuracy', (18, 20)), ('development set', (22, 24))]","[['parsers', 'for', 'up to 30 iterations'], ['best model', 'according to', 'UAS accuracy'], ['UAS accuracy', 'on', 'development set']]",[],"[['Results', 'train', 'parsers']]",[],dependency_parsing,1,295
451,results,"When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .","[('When not using', (0, 3)), ('with', (14, 15)), ('thatare', (21, 22)), ('not using', (22, 24)), ('including', (27, 28))]","[('external embeddings', (3, 5)), ('first - order graph - based parser', (7, 14)), ('2 features', (15, 17)), ('outperforms', (17, 18)), ('all other systems', (18, 21)), ('external resources', (24, 26)), ('third - order TurboParser', (29, 33))]","[['first - order graph - based parser', 'with', '2 features'], ['outperforms', 'not using', 'external resources'], ['all other systems', 'not using', 'external resources'], ['external resources', 'including', 'third - order TurboParser']]","[['external embeddings', 'has', 'first - order graph - based parser'], ['2 features', 'has', 'outperforms'], ['outperforms', 'has', 'all other systems']]","[['Results', 'When not using', 'external embeddings']]",[],dependency_parsing,1,300
452,results,Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,"[('Moving from', (0, 2)), ('to', (8, 9)), ('leads to', (17, 19)), ('for', (23, 24))]","[('simple ( 4 features )', (3, 8)), ('extended ( 11 features ) feature set', (10, 17)), ('some gains in accuracy', (19, 23))]","[['simple ( 4 features )', 'to', 'extended ( 11 features ) feature set'], ['simple ( 4 features )', 'leads to', 'some gains in accuracy'], ['extended ( 11 features ) feature set', 'leads to', 'some gains in accuracy']]",[],"[['Results', 'Moving from', 'simple ( 4 features )']]",[],dependency_parsing,1,302
453,results,Dynamic oracle training yields nice gains for both English and Chinese .,"[('yields', (3, 4)), ('for', (6, 7))]","[('Dynamic oracle training', (0, 3)), ('nice gains', (4, 6))]","[['Dynamic oracle training', 'yields', 'nice gains']]","[['Dynamic oracle training', 'has', 'nice gains']]",[],"[['Results', 'has', 'Dynamic oracle training']]",dependency_parsing,1,311
454,research-problem,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,[],"[('Ensemble of Greedy Dependency Parsers', (2, 7))]",[],[],[],[],dependency_parsing,2,2
455,model,"In 3 , we apply this idea to build a firstorder graph - based ( FOG ) ensemble parser ) that seeks consensus among 20 randomly - initialized stack LSTM parsers , achieving nearly the best - reported performance on the standard Penn Treebank Stanford dependencies task ( 94.51 UAS , 92.70 LAS ) .","[('to build', (7, 9)), ('seeks', (21, 22)), ('among', (23, 24)), ('achieving', (32, 33)), ('on', (39, 40))]","[('firstorder graph - based ( FOG ) ensemble parser', (10, 19)), ('consensus', (22, 23)), ('20 randomly - initialized stack LSTM parsers', (24, 31)), ('nearly the best - reported performance', (33, 39)), ('standard Penn Treebank Stanford dependencies task', (41, 47))]","[['consensus', 'among', '20 randomly - initialized stack LSTM parsers'], ['consensus', 'achieving', 'nearly the best - reported performance'], ['nearly the best - reported performance', 'on', 'standard Penn Treebank Stanford dependencies task']]",[],"[['Model', 'to build', 'firstorder graph - based ( FOG ) ensemble parser']]",[],dependency_parsing,2,13
456,model,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .","[('distilling', (7, 8)), ('into', (10, 11)), ('with', (15, 16)), ('by defining', (18, 20))]","[('ensemble', (9, 10)), ('single FOG parser', (12, 15)), ('discriminative training', (16, 18)), ('new cost function', (21, 24))]","[['ensemble', 'into', 'single FOG parser'], ['single FOG parser', 'with', 'discriminative training'], ['discriminative training', 'by defining', 'new cost function']]",[],"[['Model', 'distilling', 'ensemble']]",[],dependency_parsing,2,18
457,model,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .","[('derive', (5, 6)), ('of', (8, 9)), ('from', (12, 13)), ('use', (21, 22)), ('in', (24, 25))]","[('cost', (7, 8)), ('each possible attachment', (9, 12)), (""ensemble 's division of votes"", (14, 19)), ('cost', (23, 24)), ('discriminative learning', (25, 27))]","[['cost', 'of', 'each possible attachment'], ['each possible attachment', 'from', ""ensemble 's division of votes""], ['cost', 'in', 'discriminative learning']]",[],"[['Model', 'derive', 'cost']]",[],dependency_parsing,2,19
458,model,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .",[],"[('graphbased dependency parsing', (9, 12))]",[],[],[],[],dependency_parsing,2,22
459,experiments,"Our ensembles of greedy , locally normalized parsers perform comparably to the best previously reported , due to , which uses a beam ( width 32 ) for training and decoding .","[('perform', (8, 9)), ('to', (10, 11)), ('for', (27, 28))]","[('Our ensembles of greedy , locally normalized parsers', (0, 8)), ('comparably', (9, 10)), ('training and decoding', (28, 31))]","[['Our ensembles of greedy , locally normalized parsers', 'perform', 'comparably']]",[],[],[],dependency_parsing,2,69
460,experiments,"3 . When the ensemble is confident , cost for its choice ( s ) is lower than it would be under Hamming cost - even when the ensemble is wrong .","[('for', (9, 10)), ('under', (21, 22))]","[('ensemble is', (4, 6)), ('confident', (6, 7)), ('cost', (8, 9)), ('its', (10, 11)), ('choice ( s )', (11, 15)), ('lower', (16, 17)), ('Hamming cost', (22, 24))]","[['cost', 'for', 'its'], ['cost', 'for', 'choice ( s )'], ['lower', 'under', 'Hamming cost']]","[['ensemble is', 'has', 'confident'], ['confident', 'has', 'cost'], ['its', 'has', 'choice ( s )']]",[],[],dependency_parsing,2,114
461,experiments,"Second , we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer .","[('apply', (3, 4)), ('of', (9, 10)), ('to', (11, 12))]","[('per-epoch learning rate decay', (5, 9)), ('0.05', (10, 11)), ('Adam optimizer', (13, 15))]","[['per-epoch learning rate decay', 'of', '0.05'], ['0.05', 'to', 'Adam optimizer']]","[['per-epoch learning rate decay', 'has', '0.05']]",[],[],dependency_parsing,2,150
462,experiments,"While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes , we find that this additional per-epoch decay consistently improves performance across all settings and languages .","[('automatically adjusts', (4, 6)), ('according to', (10, 12)), ('find', (17, 18)), ('consistently improves', (23, 25)), ('across', (26, 27))]","[('Adam optimizer', (2, 4)), ('global learning rate', (7, 10)), ('past gradient magnitudes', (12, 15)), ('additional per-epoch decay', (20, 23)), ('performance', (25, 26)), ('all settings and languages', (27, 31))]","[['Adam optimizer', 'automatically adjusts', 'global learning rate'], ['global learning rate', 'according to', 'past gradient magnitudes'], ['global learning rate', 'find', 'additional per-epoch decay'], ['additional per-epoch decay', 'consistently improves', 'performance'], ['performance', 'across', 'all settings and languages']]",[],[],[],dependency_parsing,2,151
463,hyperparameters,We used the standard splits for all languages .,"[('used', (1, 2)), ('for', (5, 6))]","[('standard splits', (3, 5)), ('all languages', (6, 8))]","[['standard splits', 'for', 'all languages']]",[],"[['Hyperparameters', 'used', 'standard splits']]",[],dependency_parsing,2,155
464,hyperparameters,For German we use the predicted tags provided by the CoNLL 2009 shared task organizers .,"[('For', (0, 1)), ('use', (3, 4)), ('provided by', (7, 9))]","[('German', (1, 2)), ('predicted tags', (5, 7)), ('CoNLL 2009 shared task organizers', (10, 15))]","[['German', 'use', 'predicted tags'], ['predicted tags', 'provided by', 'CoNLL 2009 shared task organizers']]",[],"[['Hyperparameters', 'For', 'German']]",[],dependency_parsing,2,157
465,hyperparameters,"All models were augmented with pretrained structured - skipgram embeddings ; for English we used the Gigaword corpus and 100 dimensions , for Chinese Gigaword and 80 , and for German WMT 2010 monolingual data and 64 .","[('augmented with', (3, 5)), ('used', (14, 15))]","[('pretrained structured - skipgram embeddings', (5, 10)), ('English', (12, 13)), ('Gigaword corpus and 100 dimensions', (16, 21)), ('German WMT 2010 monolingual data', (30, 35))]","[['English', 'used', 'Gigaword corpus and 100 dimensions']]",[],"[['Hyperparameters', 'augmented with', 'pretrained structured - skipgram embeddings']]",[],dependency_parsing,2,158
466,hyperparameters,For the Adam optimizer we use the default settings in the CNN neural network library .,"[('For', (0, 1)), ('use', (5, 6)), ('in', (9, 10))]","[('Adam optimizer', (2, 4)), ('default settings', (7, 9)), ('CNN neural network library', (11, 15))]","[['Adam optimizer', 'use', 'default settings'], ['default settings', 'in', 'CNN neural network library']]",[],"[['Hyperparameters', 'For', 'Adam optimizer']]",[],dependency_parsing,2,161
467,results,"Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .","[('training', (2, 3)), ('with', (6, 7)), ('gives', (9, 10)), ('for', (12, 13))]","[('same model', (4, 6)), ('distillation cost', (7, 9)), ('consistent improvements', (10, 12)), ('all languages', (13, 15))]","[['same model', 'with', 'distillation cost'], ['distillation cost', 'gives', 'consistent improvements'], ['consistent improvements', 'for', 'all languages']]",[],"[['Results', 'training', 'same model']]",[],dependency_parsing,2,178
468,results,"The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS , compared to 93.6 UAS and 91.1 LAS for the model with distillation cost .","[('trained with', (2, 4)), ('achieved', (6, 7)), ('compared to', (13, 15)), ('for', (20, 21))]","[('model', (1, 2)), ('Hamming cost', (4, 6)), ('93.1 UAS and 90.9 LAS', (7, 12)), ('93.6 UAS and 91.1 LAS', (15, 20)), ('model', (22, 23)), ('distillation cost', (24, 26))]","[['model', 'trained with', 'Hamming cost'], ['Hamming cost', 'achieved', '93.1 UAS and 90.9 LAS'], ['93.1 UAS and 90.9 LAS', 'compared to', '93.6 UAS and 91.1 LAS'], ['93.6 UAS and 91.1 LAS', 'for', 'model']]",[],[],"[['Results', 'has', 'model']]",dependency_parsing,2,183
469,research-problem,From POS tagging to dependency parsing for biomedical event extraction,[],[],[],[],[],[],dependency_parsing,3,2
470,code,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.,[],[],[],[],[],[],dependency_parsing,3,12
471,experimental-setup,"For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .","[('For', (0, 1)), ('utilizes', (20, 21)), ('employ', (26, 27))]","[('three BiLSTM - CRF - based models', (2, 9)), ('Stanford - NNdep', (10, 13)), ('jPTDP', (14, 15)), ('Stanford - Biaffine', (16, 19)), ('pre-trained word embeddings', (21, 24)), ('200 dimensional pre-trained word vectors', (27, 32))]","[['Stanford - Biaffine', 'utilizes', 'pre-trained word embeddings'], ['pre-trained word embeddings', 'employ', '200 dimensional pre-trained word vectors']]","[['three BiLSTM - CRF - based models', 'name', 'Stanford - NNdep']]","[['Experimental setup', 'For', 'three BiLSTM - CRF - based models']]",[],dependency_parsing,3,97
472,experimental-setup,"We perform a grid search of hyperparameters to select the number of BiLSTM layers from { 1 , 2 } and the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .","[('perform', (1, 2)), ('of', (5, 6)), ('to select', (7, 9)), ('from', (14, 15)), ('in', (26, 27)), ('from', (29, 30))]","[('grid search', (3, 5)), ('hyperparameters', (6, 7)), ('number of BiLSTM layers', (10, 14)), ('{ 1 , 2 }', (15, 20)), ('number of LSTM units', (22, 26)), ('each layer', (27, 29)), ('{ 100 , 150 , 200 , 250 , 300 }', (30, 41))]","[['grid search', 'of', 'hyperparameters'], ['grid search', 'to select', 'number of LSTM units'], ['hyperparameters', 'to select', 'number of BiLSTM layers'], ['hyperparameters', 'to select', 'number of LSTM units'], ['number of BiLSTM layers', 'from', '{ 1 , 2 }'], ['each layer', 'from', '{ 100 , 150 , 200 , 250 , 300 }'], ['number of LSTM units', 'in', 'each layer'], ['each layer', 'from', '{ 100 , 150 , 200 , 250 , 300 }']]",[],"[['Experimental setup', 'perform', 'grid search']]",[],dependency_parsing,3,101
473,experimental-setup,Early stopping is applied when no performance improvement on the development set is obtained after 10 contiguous epochs .,"[('applied when', (3, 5)), ('on', (8, 9)), ('obtained after', (13, 15))]","[('Early stopping', (0, 2)), ('no performance improvement', (5, 8)), ('development set', (10, 12)), ('10 contiguous epochs', (15, 18))]","[['Early stopping', 'applied when', 'no performance improvement'], ['no performance improvement', 'on', 'development set'], ['no performance improvement', 'obtained after', '10 contiguous epochs'], ['development set', 'obtained after', '10 contiguous epochs']]",[],[],"[['Experimental setup', 'has', 'Early stopping']]",dependency_parsing,3,102
474,experimental-setup,"For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .","[('For', (0, 1)), ('select', (6, 7)), ('from', (10, 11)), ('from', (23, 24)), ('fix', (40, 41)), ('with', (43, 44))]","[('Stanford - NNdep', (1, 4)), ('word CutOff', (8, 10)), ('{ 1 , 2 }', (11, 16)), ('size of the', (18, 21)), ('hidden layer', (21, 23)), ('{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }', (24, 39)), ('other hyperparameters', (41, 43)), ('default values', (45, 47))]","[['Stanford - NNdep', 'select', 'word CutOff'], ['word CutOff', 'from', '{ 1 , 2 }'], ['hidden layer', 'from', '{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }'], ['hidden layer', 'from', '{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }'], ['word CutOff', 'fix', 'other hyperparameters'], ['size of the', 'fix', 'other hyperparameters'], ['other hyperparameters', 'with', 'default values']]","[['size of the', 'has', 'hidden layer']]","[['Experimental setup', 'For', 'Stanford - NNdep']]",[],dependency_parsing,3,103
475,experimental-setup,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .","[('For', (0, 1)), ('use', (4, 5)), ('fix', (11, 12)), ('at', (16, 17))]","[('jPTDP', (1, 2)), ('50 - dimensional character embeddings', (5, 10)), ('initial learning rate', (13, 16)), ('0.0005', (17, 18))]","[['jPTDP', 'use', '50 - dimensional character embeddings'], ['50 - dimensional character embeddings', 'fix', 'initial learning rate'], ['initial learning rate', 'at', '0.0005']]",[],"[['Experimental setup', 'For', 'jPTDP']]",[],dependency_parsing,3,104
476,experimental-setup,"We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .","[('fix', (2, 3)), ('at', (8, 9)), ('select', (11, 12)), ('in', (17, 18)), ('from', (20, 21))]","[('number of BiLSTM layers', (4, 8)), ('2', (9, 10)), ('number of LSTM units', (13, 17)), ('each layer', (18, 20)), ('{ 100 , 150 , 200 , 250 , 300 }', (21, 32))]","[['number of BiLSTM layers', 'at', '2'], ['number of LSTM units', 'in', 'each layer'], ['each layer', 'from', '{ 100 , 150 , 200 , 250 , 300 }']]","[['number of BiLSTM layers', 'has', '2']]","[['Experimental setup', 'fix', 'number of BiLSTM layers']]",[],dependency_parsing,3,105
477,code,https://github.com/tdozat/Parser-v2,[],[],[],[],[],[],dependency_parsing,3,109
478,experiments,Corpus - level accuracy differences of at least 0.17 % in GENIA and 0.26 % in CRAFT between two POS tagging models are significant at p ? 0.05 .,"[('of', (5, 6)), ('in', (10, 11)), ('between', (17, 18))]","[('Corpus - level accuracy differences', (0, 5)), ('at least 0.17 %', (6, 10)), ('GENIA', (11, 12)), ('0.26 %', (13, 15)), ('CRAFT', (16, 17)), ('two POS tagging models', (18, 22))]","[['Corpus - level accuracy differences', 'of', 'at least 0.17 %'], ['Corpus - level accuracy differences', 'of', '0.26 %'], ['at least 0.17 %', 'in', 'GENIA'], ['0.26 %', 'in', 'CRAFT'], ['0.26 %', 'between', 'two POS tagging models'], ['CRAFT', 'between', 'two POS tagging models']]",[],[],[],dependency_parsing,3,113
479,results,POS tagging results,[],[],[],[],[],[],dependency_parsing,3,118
480,results,"In general , we find that the six retrained models produce competitive results .","[('find', (4, 5)), ('produce', (10, 11))]","[('six retrained models', (7, 10)), ('competitive results', (11, 13))]","[['six retrained models', 'produce', 'competitive results']]",[],"[['Results', 'find', 'six retrained models']]",[],dependency_parsing,3,119
481,results,BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .,"[('obtains', (3, 4)), ('of', (5, 6)), ('on', (8, 9)), ('on', (15, 16))]","[('BiLSTM - CRF', (0, 3)), ('accuracies', (4, 5)), ('98.44 %', (6, 8)), ('GE - NIA', (9, 12)), ('97.25 %', (13, 15)), ('CRAFT', (16, 17))]","[['BiLSTM - CRF', 'obtains', 'accuracies'], ['accuracies', 'of', '98.44 %'], ['98.44 %', 'on', 'GE - NIA'], ['98.44 %', 'on', 'CRAFT'], ['97.25 %', 'on', 'CRAFT'], ['97.25 %', 'on', 'CRAFT']]","[['BiLSTM - CRF', 'has', 'accuracies']]",[],"[['Results', 'has', 'BiLSTM - CRF']]",dependency_parsing,3,124
482,results,"Using character - level word embeddings helps to produce about 0.5 % and Trained on the PTB sections 0 - 18 , the accuracies for the GENIA tagger , Stanford tagger , MarMoT , NLP4J - POS , BiLSTM- CRF and BiLSTM - CRF + CNN - char on the benchmark test set of PTB sections 22 - 24 were reported at 97.05 % , 97.23 % , 97.28 % , 97.64 % , 97.45 % and 97.55 % , respectively .","[('Using', (0, 1)), ('helps to produce', (6, 9)), ('Trained on', (13, 15)), ('for', (24, 25)), ('on', (48, 49)), ('of', (53, 54)), ('reported at', (60, 62))]","[('character - level word embeddings', (1, 6)), ('about 0.5 %', (9, 12)), ('accuracies', (23, 24)), ('GENIA tagger', (26, 28)), ('Stanford tagger', (29, 31)), ('MarMoT', (32, 33)), ('NLP4J - POS', (34, 37)), ('BiLSTM- CRF', (38, 40)), ('benchmark test set', (50, 53)), ('97.05 %', (62, 64))]","[['character - level word embeddings', 'helps to produce', 'about 0.5 %'], ['accuracies', 'for', 'GENIA tagger'], ['accuracies', 'for', 'MarMoT'], ['accuracies', 'for', 'NLP4J - POS'], ['accuracies', 'for', 'BiLSTM- CRF']]","[['character - level word embeddings', 'has', 'about 0.5 %']]","[['Results', 'Using', 'character - level word embeddings']]",[],dependency_parsing,3,125
483,results,"Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .","[('Note', (0, 1)), ('for', (2, 3)), ('provided', (14, 15)), ('to', (19, 20))]","[('PTB', (3, 4)), ('CNN - based character - level word embeddings', (5, 13)), ('0.1 % improvement', (16, 19)), ('BiLSTM - CRF', (20, 23))]","[['CNN - based character - level word embeddings', 'provided', '0.1 % improvement'], ['0.1 % improvement', 'to', 'BiLSTM - CRF']]","[['PTB', 'has', 'CNN - based character - level word embeddings']]",[],[],dependency_parsing,3,130
484,results,"On GENIA , among pre-trained models , BLLIP obtains highest results .","[('On', (0, 1)), ('among', (3, 4)), ('obtains', (8, 9))]","[('GENIA', (1, 2)), ('pre-trained models', (4, 6)), ('BLLIP', (7, 8)), ('highest results', (9, 11))]","[['GENIA', 'among', 'pre-trained models'], ['BLLIP', 'obtains', 'highest results']]","[['GENIA', 'has', 'pre-trained models'], ['pre-trained models', 'has', 'BLLIP']]","[['Results', 'On', 'GENIA']]",[],dependency_parsing,3,148
485,results,Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .,"[('Note', (0, 1)), ('result in', (8, 10)), ('irrespective of', (14, 16)), ('of', (18, 19)), ('i.e.', (22, 23))]","[('pre-trained NNdep and Biaffine models', (3, 8)), ('no significant performance differences', (10, 14)), ('source', (17, 18)), ('POS tags', (19, 21))]","[['pre-trained NNdep and Biaffine models', 'result in', 'no significant performance differences'], ['no significant performance differences', 'irrespective of', 'source'], ['source', 'of', 'POS tags']]",[],"[['Results', 'Note', 'pre-trained NNdep and Biaffine models']]",[],dependency_parsing,3,152
486,results,"Regarding the retrained parsing models , on both GENIA and CRAFT , Stanford - Biaffine achieves the","[('Regarding', (0, 1)), ('on', (6, 7)), ('achieves', (15, 16))]","[('retrained parsing models', (2, 5)), ('GENIA and CRAFT', (8, 11)), ('Stanford - Biaffine', (12, 15))]","[['retrained parsing models', 'on', 'GENIA and CRAFT']]","[['retrained parsing models', 'has', 'GENIA and CRAFT']]",[],[],dependency_parsing,3,153
487,results,"As expected , all parsers produce better results for shorter sentences on both corpora ; longer sentences are likely to have longer dependencies which are typically harder to predict precisely .","[('produce', (5, 6)), ('for', (8, 9)), ('on', (11, 12)), ('likely to have', (18, 21))]","[('all parsers', (3, 5)), ('better results', (6, 8)), ('shorter sentences', (9, 11)), ('both corpora', (12, 14)), ('longer sentences', (15, 17)), ('longer dependencies', (21, 23))]","[['all parsers', 'produce', 'better results'], ['better results', 'for', 'shorter sentences'], ['shorter sentences', 'on', 'both corpora'], ['longer sentences', 'likely to have', 'longer dependencies']]","[['all parsers', 'has', 'better results']]",[],"[['Results', 'has', 'all parsers']]",dependency_parsing,3,159
488,results,Impact of parsing on event extraction,"[('on', (3, 4))]","[('Impact of parsing', (0, 3))]",[],[],[],"[['Results', 'has', 'Impact of parsing']]",dependency_parsing,3,237
489,results,"The results for parsers trained with the GENIA treebank ( Rows 1 - 6 , ) are generally higher than http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi","[('for', (2, 3)), ('trained with', (4, 6))]","[('parsers', (3, 4)), ('GENIA treebank', (7, 9))]","[['parsers', 'trained with', 'GENIA treebank']]",[],"[['Results', 'for', 'parsers']]",[],dependency_parsing,3,238
490,results,"Among the four dependency parsers trained on GENIA , Stanford - Biaffine , jPTDP and NLP4J - dep produce similar event extraction scores on the development set , while on the the test set jPTDP and NLP4 Jdep obtain the lowest and highest scores , respectively .","[('Among', (0, 1)), ('trained on', (5, 7)), ('produce', (18, 19)), ('on', (23, 24)), ('on', (29, 30)), ('obtain', (38, 39))]","[('four dependency parsers', (2, 5)), ('GENIA', (7, 8)), ('Stanford - Biaffine', (9, 12)), ('jPTDP', (13, 14)), ('similar event extraction scores', (19, 23)), ('development set', (25, 27)), ('test set', (32, 34)), ('jPTDP and NLP4 Jdep', (34, 38)), ('lowest and highest scores', (40, 44))]","[['four dependency parsers', 'trained on', 'GENIA'], ['four dependency parsers', 'trained on', 'jPTDP'], ['four dependency parsers', 'produce', 'similar event extraction scores'], ['similar event extraction scores', 'on', 'development set'], ['similar event extraction scores', 'on', 'test set'], ['similar event extraction scores', 'on', 'test set'], ['jPTDP and NLP4 Jdep', 'obtain', 'lowest and highest scores']]","[['test set', 'has', 'jPTDP and NLP4 Jdep']]","[['Results', 'Among', 'four dependency parsers']]",[],dependency_parsing,3,244
491,research-problem,Stack - Pointer Networks for Dependency Parsing,[],[],[],[],[],[],dependency_parsing,4,2
492,research-problem,We introduce a novel architecture for dependency parsing : stack - pointer networks ( STACKPTR ) .,[],"[('dependency parsing', (6, 8)), ('stack - pointer networks ( STACKPTR )', (9, 16))]",[],"[['dependency parsing', 'name', 'stack - pointer networks ( STACKPTR )']]",[],[],dependency_parsing,4,4
493,research-problem,"Dependency parsing , which predicts the existence and type of linguistic dependency relations between words , is a first step towards deep language understanding .",[],"[('Dependency parsing', (0, 2))]",[],[],[],[],dependency_parsing,4,11
494,model,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .","[('propose', (5, 6)), ('for', (11, 12))]","[('novel neural network architecture', (7, 11)), ('dependency parsing', (12, 14)), ('stackpointer networks ( STACKPTR )', (15, 20))]","[['novel neural network architecture', 'for', 'dependency parsing']]",[],"[['Model', 'propose', 'novel neural network architecture']]",[],dependency_parsing,4,25
495,model,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .","[('as', (7, 8)), ('equipped with', (13, 15)), ('to maintain', (18, 20)), ('in', (25, 26))]","[('STACKPTR parser', (1, 3)), ('pointer network', (5, 7)), ('backbone', (9, 10)), ('internal stack', (16, 18)), ('order of head words', (21, 25)), ('tree structures', (26, 28))]","[['pointer network', 'as', 'backbone'], ['STACKPTR parser', 'equipped with', 'internal stack'], ['internal stack', 'to maintain', 'order of head words'], ['order of head words', 'in', 'tree structures']]","[['STACKPTR parser', 'has', 'pointer network'], ['pointer network', 'has', 'backbone']]",[],"[['Model', 'has', 'STACKPTR parser']]",dependency_parsing,4,27
496,model,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .","[('performs', (3, 4)), ('in', (5, 6)), ('generates', (21, 22)), ('by assigning', (24, 26)), ('at the top of', (31, 35))]","[('STACKPTR parser', (1, 3)), ('parsing', (4, 5)), ('incremental , topdown , depth - first fashion', (7, 15)), ('child', (27, 28)), ('headword', (30, 31)), ('internal stack', (36, 38))]","[['STACKPTR parser', 'performs', 'parsing'], ['parsing', 'in', 'incremental , topdown , depth - first fashion'], ['headword', 'at the top of', 'internal stack']]",[],[],"[['Model', 'has', 'STACKPTR parser']]",dependency_parsing,4,28
497,model,"This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees , while maintaining a number of parsing steps linear in the sentence length .","[('from', (8, 9)), ('maintaining', (20, 21)), ('in', (27, 28))]","[('capture', (6, 7)), ('information', (7, 8)), ('whole sentence and all the previously derived subtrees', (10, 18)), ('number of parsing steps', (22, 26)), ('linear', (26, 27)), ('sentence length', (29, 31))]","[['information', 'from', 'whole sentence and all the previously derived subtrees'], ['linear', 'in', 'sentence length']]","[['capture', 'has', 'information'], ['number of parsing steps', 'has', 'linear']]",[],[],dependency_parsing,4,29
498,hyperparameters,"For all the parsing models in different languages , we initialize word vectors with pretrained word embeddings .","[('For', (0, 1)), ('in', (5, 6)), ('initialize', (10, 11)), ('with', (13, 14))]","[('all the parsing models', (1, 5)), ('different languages', (6, 8)), ('word vectors', (11, 13)), ('pretrained word embeddings', (14, 17))]","[['all the parsing models', 'in', 'different languages'], ['all the parsing models', 'initialize', 'word vectors'], ['different languages', 'initialize', 'word vectors'], ['word vectors', 'with', 'pretrained word embeddings']]",[],"[['Hyperparameters', 'For', 'all the parsing models']]",[],dependency_parsing,4,128
499,hyperparameters,"For Chinese , Dutch , English , German and Spanish , we use the structured - skipgram embeddings .","[('For', (0, 1)), ('use', (12, 13))]","[('Chinese , Dutch , English , German and Spanish', (1, 10)), ('structured - skipgram embeddings', (14, 18))]","[['Chinese , Dutch , English , German and Spanish', 'use', 'structured - skipgram embeddings']]",[],"[['Hyperparameters', 'For', 'Chinese , Dutch , English , German and Spanish']]",[],dependency_parsing,4,129
500,hyperparameters,For other languages we use Polyglot embeddings .,"[('use', (4, 5))]","[('other languages', (1, 3)), ('Polyglot embeddings', (5, 7))]","[['other languages', 'use', 'Polyglot embeddings']]",[],[],[],dependency_parsing,4,130
501,hyperparameters,Parameter optimization is performed with the Adam optimizer with ? 1 = ? 2 = 0.9 . We choose an initial learning rate of ? 0 = 0.001 .,"[('performed with', (3, 5)), ('with', (8, 9)), ('choose', (18, 19)), ('of', (23, 24))]","[('Parameter optimization', (0, 2)), ('Adam optimizer', (6, 8)), ('? 1 = ? 2 = 0.9', (9, 16)), ('initial learning rate', (20, 23)), ('? 0 = 0.001', (24, 28))]","[['Parameter optimization', 'performed with', 'Adam optimizer'], ['Adam optimizer', 'with', '? 1 = ? 2 = 0.9'], ['Adam optimizer', 'choose', 'initial learning rate'], ['? 1 = ? 2 = 0.9', 'choose', 'initial learning rate'], ['initial learning rate', 'of', '? 0 = 0.001']]",[],[],"[['Hyperparameters', 'has', 'Parameter optimization']]",dependency_parsing,4,132
502,hyperparameters,The learning rate ?,[],"[('learning rate', (1, 3))]",[],[],[],"[['Hyperparameters', 'has', 'learning rate']]",dependency_parsing,4,133
503,hyperparameters,"To reduce the effects of "" gradient exploding "" , we use gradient clipping of 5.0 .","[('To reduce', (0, 2)), ('of', (4, 5)), ('use', (11, 12)), ('of', (14, 15))]","[('effects', (3, 4)), ('"" gradient exploding ""', (5, 9)), ('gradient clipping', (12, 14)), ('5.0', (15, 16))]","[['effects', 'of', '"" gradient exploding ""'], ['effects', 'of', '5.0'], ['gradient clipping', 'of', '5.0'], ['"" gradient exploding ""', 'use', 'gradient clipping'], ['gradient clipping', 'of', '5.0']]","[['effects', 'has', '"" gradient exploding ""']]","[['Hyperparameters', 'To reduce', 'effects']]",[],dependency_parsing,4,136
504,hyperparameters,"To mitigate overfitting , we apply dropout .","[('To mitigate', (0, 2)), ('apply', (5, 6))]","[('overfitting', (2, 3)), ('dropout', (6, 7))]","[['overfitting', 'apply', 'dropout']]",[],"[['Hyperparameters', 'To mitigate', 'overfitting']]",[],dependency_parsing,4,138
505,hyperparameters,"For BLSTM , we use recurrent dropout with a drop rate of 0.33 between hidden states and 0.33 between layers .","[('For', (0, 1)), ('use', (4, 5)), ('with', (7, 8)), ('of', (11, 12)), ('between', (13, 14)), ('between', (18, 19))]","[('BLSTM', (1, 2)), ('recurrent dropout', (5, 7)), ('drop rate', (9, 11)), ('0.33', (12, 13)), ('hidden states', (14, 16)), ('0.33', (17, 18)), ('layers', (19, 20))]","[['BLSTM', 'use', 'recurrent dropout'], ['recurrent dropout', 'with', 'drop rate'], ['drop rate', 'of', '0.33'], ['0.33', 'between', 'hidden states'], ['0.33', 'between', '0.33'], ['0.33', 'between', 'layers']]",[],"[['Hyperparameters', 'For', 'BLSTM']]",[],dependency_parsing,4,139
506,hyperparameters,"Following , we also use embedding dropout with a rate of 0.33 on all word , character , and POS embeddings .","[('use', (4, 5)), ('with', (7, 8)), ('of', (10, 11)), ('on', (12, 13))]","[('embedding dropout', (5, 7)), ('rate', (9, 10)), ('0.33', (11, 12)), ('all word , character , and POS embeddings', (13, 21))]","[['embedding dropout', 'with', 'rate'], ['rate', 'of', '0.33'], ['0.33', 'on', 'all word , character , and POS embeddings']]","[['embedding dropout', 'has', 'rate']]","[['Hyperparameters', 'use', 'embedding dropout']]",[],dependency_parsing,4,140
507,results,"We compare the performance of four variations of our model with different decoder inputs - Org , + gpar , + sib and Full - where the Org model utilizes only the encoder hidden states of head words , while the + gpar and + sib models augments the original one with grandparent and sibling information , respectively .","[('with', (10, 11)), ('where', (25, 26)), ('utilizes', (29, 30)), ('of', (35, 36)), ('augments', (47, 48)), ('with', (51, 52))]","[('different decoder inputs', (11, 14)), ('Org , + gpar , + sib and Full', (15, 24)), ('Org model', (27, 29)), ('encoder hidden states', (32, 35)), ('head words', (36, 38)), ('+ gpar and + sib models', (41, 47)), ('original one', (49, 51)), ('grandparent and sibling information', (52, 56))]","[['original one', 'with', 'grandparent and sibling information'], ['different decoder inputs', 'where', 'Org model'], ['Org model', 'utilizes', 'encoder hidden states'], ['encoder hidden states', 'of', 'head words'], ['+ gpar and + sib models', 'augments', 'original one'], ['original one', 'with', 'grandparent and sibling information']]","[['different decoder inputs', 'name', 'Org , + gpar , + sib and Full']]","[['Results', 'with', 'different decoder inputs']]",[],dependency_parsing,4,162
508,results,"An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .","[('achieves', (8, 9)), ('on', (12, 13)), ('performs', (18, 19)), ('than', (21, 22)), ('on', (24, 25))]","[('Full model', (6, 8)), ('best accuracy', (10, 12)), ('English and Chinese', (13, 16)), ('slightly worse', (19, 21)), ('+ sib', (22, 24)), ('German', (25, 26))]","[['Full model', 'achieves', 'best accuracy'], ['best accuracy', 'on', 'English and Chinese'], ['+ sib', 'on', 'German'], ['slightly worse', 'than', '+ sib'], ['+ sib', 'on', 'German']]","[['Full model', 'has', 'best accuracy']]",[],[],dependency_parsing,4,166
509,results,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .","[('On', (0, 1)), ('on', (9, 10)), ('showing', (13, 14)), ('on', (19, 20))]","[('LCM and UCM', (1, 4)), ('STACKPTR', (5, 6)), ('significantly outperforms', (6, 8)), ('BIAF', (8, 9)), ('all languages', (10, 12)), ('superiority', (15, 16)), ('complete sentence parsing', (20, 23))]","[['BIAF', 'On', 'all languages'], ['significantly outperforms', 'on', 'all languages'], ['BIAF', 'on', 'all languages'], ['all languages', 'showing', 'superiority']]","[['LCM and UCM', 'has', 'STACKPTR'], ['STACKPTR', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'BIAF']]","[['Results', 'On', 'LCM and UCM']]",[],dependency_parsing,4,168
510,results,The results of our parser on RA are slightly worse than BIAF .,"[('of', (2, 3)), ('on', (5, 6)), ('slightly worse than', (8, 11))]","[('results', (1, 2)), ('our parser', (3, 5)), ('RA', (6, 7)), ('BIAF', (11, 12))]","[['results', 'of', 'our parser'], ['our parser', 'on', 'RA'], ['results', 'slightly worse than', 'BIAF'], ['our parser', 'slightly worse than', 'BIAF']]","[['results', 'has', 'our parser']]",[],[],dependency_parsing,4,169
511,results,"Our Full model significantly outperforms all the transition - based parsers on all three languages , and achieves better results than most graph - based parsers .","[('on', (11, 12)), ('achieves', (17, 18)), ('than', (20, 21))]","[('Our Full model', (0, 3)), ('significantly outperforms', (3, 5)), ('all the transition - based parsers', (5, 11)), ('better results', (18, 20)), ('most graph - based parsers', (21, 26))]","[['Our Full model', 'achieves', 'better results'], ['better results', 'than', 'most graph - based parsers']]","[['Our Full model', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all the transition - based parsers']]",[],[],dependency_parsing,4,172
512,results,"re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .","[('of', (1, 2)), ('obtains', (3, 4)), ('than', (6, 7))]","[('re-implementation', (0, 1)), ('BIAF', (2, 3)), ('better performance', (4, 6)), ('original one', (8, 10))]","[['re-implementation', 'of', 'BIAF'], ['re-implementation', 'obtains', 'better performance'], ['BIAF', 'obtains', 'better performance'], ['better performance', 'than', 'original one']]",[],[],"[['Results', 'has', 're-implementation']]",dependency_parsing,4,175
513,results,"Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .","[('achieves', (2, 3)), ('on', (11, 12)), ('on', (16, 17))]","[('Our model', (0, 2)), ('state - of - the - art performance', (3, 11)), ('UAS and LAS', (13, 16)), ('Chinese', (17, 18)), ('best UAS', (20, 22))]","[['Our model', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', 'UAS and LAS'], ['state - of - the - art performance', 'on', 'Chinese'], ['state - of - the - art performance', 'on', 'best UAS'], ['UAS and LAS', 'on', 'Chinese'], ['UAS and LAS', 'on', 'Chinese']]",[],[],"[['Results', 'has', 'Our model']]",dependency_parsing,4,176
514,results,"Consistent with the analysis in , STACKPTR tends to perform better on shorter sentences , which make fewer parsing decisions , significantly reducing the chance of error propagation .","[('tends to perform', (7, 10)), ('on', (11, 12)), ('make', (16, 17)), ('significantly reducing', (21, 23))]","[('STACKPTR', (6, 7)), ('better', (10, 11)), ('shorter sentences', (12, 14)), ('fewer parsing decisions', (17, 20)), ('error propagation', (26, 28))]","[['STACKPTR', 'tends to perform', 'better'], ['better', 'on', 'shorter sentences'], ['shorter sentences', 'make', 'fewer parsing decisions']]",[],[],"[['Results', 'has', 'STACKPTR']]",dependency_parsing,4,186
515,results,CoNLL,[],[],[],[],[],[],dependency_parsing,4,213
516,results,"First , both BIAF and STACKPTR parsers achieve relatively high parsing accuracies on all the 12 languages - all with UAS are higher than 90 % .","[('achieve', (7, 8)), ('on', (12, 13)), ('higher than', (22, 24))]","[('BIAF and STACKPTR parsers', (3, 7)), ('relatively high parsing accuracies', (8, 12)), ('all the 12 languages', (13, 17)), ('UAS', (20, 21)), ('90 %', (24, 26))]","[['BIAF and STACKPTR parsers', 'achieve', 'relatively high parsing accuracies'], ['relatively high parsing accuracies', 'on', 'all the 12 languages']]",[],[],[],dependency_parsing,4,220
517,results,"On nine languages - Catalan , Czech , Dutch , English , French , German , Norwegian , Russian and Spanish - STACKPTR outperforms BIAF for both UAS and LAS .","[('On', (0, 1)), ('outperforms', (23, 24)), ('for both', (25, 27))]","[('nine languages', (1, 3)), ('Catalan', (4, 5)), ('English', (10, 11)), ('STACKPTR', (22, 23)), ('BIAF', (24, 25)), ('UAS and LAS', (27, 30))]","[['nine languages', 'outperforms', 'BIAF'], ['STACKPTR', 'outperforms', 'BIAF'], ['BIAF', 'for both', 'UAS and LAS']]","[['nine languages', 'has', 'Catalan']]","[['Results', 'On', 'nine languages']]",[],dependency_parsing,4,221
518,results,"On Bulgarian , STACKPTR achieves slightly better UAS while LAS is slightly worse than BIAF .","[('On', (0, 1)), ('achieves', (4, 5)), ('slightly worse than', (11, 14))]","[('Bulgarian', (1, 2)), ('STACKPTR', (3, 4)), ('slightly better UAS', (5, 8)), ('LAS', (9, 10)), ('BIAF', (14, 15))]","[['STACKPTR', 'achieves', 'slightly better UAS'], ['LAS', 'slightly worse than', 'BIAF']]","[['Bulgarian', 'has', 'STACKPTR']]","[['Results', 'On', 'Bulgarian']]",[],dependency_parsing,4,222
519,results,"On Italian and Romanian , BIAF obtains marginally better parsing performance than STACKPTR .","[('On', (0, 1)), ('obtains', (6, 7)), ('than', (11, 12))]","[('Italian and Romanian', (1, 4)), ('BIAF', (5, 6)), ('marginally better parsing performance', (7, 11)), ('STACKPTR', (12, 13))]","[['BIAF', 'obtains', 'marginally better parsing performance'], ['marginally better parsing performance', 'than', 'STACKPTR']]","[['Italian and Romanian', 'has', 'BIAF']]","[['Results', 'On', 'Italian and Romanian']]",[],dependency_parsing,4,223
520,research-problem,Structured Training for Neural Network Transition - Based Parsing,[],[],[],[],[],[],dependency_parsing,5,2
521,research-problem,Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention .,[],"[('Syntactic analysis', (0, 2))]",[],[],[],[],dependency_parsing,5,10
522,model,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .","[('In', (0, 1)), ('processed in', (8, 10)), ('choose', (25, 26)), ('defined by', (32, 34))]","[('transition - based parsing', (1, 5)), ('sentences', (6, 7)), ('linear left to right pass', (11, 16))]","[['sentences', 'processed in', 'linear left to right pass']]","[['transition - based parsing', 'has', 'sentences']]","[['Model', 'In', 'transition - based parsing']]",[],dependency_parsing,5,13
523,model,"Furthermore , because the neural network uses a distributed representation , it is able to model lexical , part - of - speech ( POS ) tag , and arc label similarities in a continuous space .","[('able to model', (13, 16)), ('in', (32, 33))]","[('lexical', (16, 17)), ('part - of - speech ( POS ) tag', (18, 27)), ('arc label similarities', (29, 32)), ('continuous space', (34, 36))]","[['arc label similarities', 'in', 'continuous space']]",[],"[['Model', 'able to model', 'lexical']]",[],dependency_parsing,5,22
524,model,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .","[('combine', (5, 6)), ('of', (9, 10)), ('with', (12, 13)), ('enabled by', (16, 18)), ('making', (23, 24))]","[('representational power', (7, 9)), ('neural networks', (10, 12)), ('superior search', (14, 16)), ('structured training and inference', (18, 22)), ('our parser', (24, 26))]","[['representational power', 'of', 'neural networks'], ['neural networks', 'with', 'superior search'], ['superior search', 'enabled by', 'structured training and inference'], ['structured training and inference', 'making', 'our parser']]",[],"[['Model', 'combine', 'representational power']]",[],dependency_parsing,5,24
525,research-problem,"In addition , by incorporating unlabeled data into training , we further improve the accuracy of our model to 94.26 % UAS / 92.41 % LAS ( 93.46 % UAS / 91.49 % LAS for our greedy model ) .","[('incorporating', (4, 5)), ('into', (7, 8)), ('further improve', (11, 13)), ('of', (15, 16)), ('to', (18, 19))]","[('unlabeled data', (5, 7)), ('training', (8, 9)), ('accuracy', (14, 15)), ('our model', (16, 18)), ('94.26 % UAS / 92.41 % LAS', (19, 26))]","[['unlabeled data', 'into', 'training'], ['accuracy', 'of', 'our model'], ['accuracy', 'to', '94.26 % UAS / 92.41 % LAS'], ['our model', 'to', '94.26 % UAS / 92.41 % LAS']]",[],"[['Research problem', 'incorporating', 'unlabeled data']]",[],dependency_parsing,5,26
526,model,"In our approach we start with the basic structure of , but with a deeper architecture and improvements to the optimization procedure .","[('start with', (4, 6)), ('with', (12, 13)), ('to', (18, 19))]","[('basic structure', (7, 9)), ('deeper architecture', (14, 16)), ('improvements', (17, 18)), ('optimization procedure', (20, 22))]","[['basic structure', 'with', 'deeper architecture'], ['improvements', 'to', 'optimization procedure']]","[['improvements', 'has', 'optimization procedure']]","[['Model', 'start with', 'basic structure']]",[],dependency_parsing,5,27
527,model,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .","[('use', (3, 4)), ('from', (6, 7)), ('of', (9, 10)), ('as', (13, 14)), ('in', (16, 17)), ('trained with', (23, 25))]","[('activations', (5, 6)), ('all layers', (7, 9)), ('neural network', (11, 13)), ('representation', (15, 16)), ('structured perceptron model', (18, 21)), ('beam search and early updates', (25, 30))]","[['activations', 'from', 'all layers'], ['all layers', 'of', 'neural network'], ['activations', 'as', 'representation'], ['neural network', 'as', 'representation'], ['representation', 'in', 'structured perceptron model'], ['structured perceptron model', 'trained with', 'beam search and early updates']]",[],"[['Model', 'use', 'activations']]",[],dependency_parsing,5,31
528,model,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .","[('generate', (5, 6)), ('of', (8, 9)), ('by parsing', (14, 16)), ('with', (18, 19)), ('selecting', (23, 24)), ('produced', (32, 33))]","[('large quantities', (6, 8)), ('high - confidence parse trees', (9, 14)), ('unlabeled data', (16, 18)), ('two different parsers', (19, 22)), ('sentences', (26, 27)), ('two parsers', (30, 32)), ('same trees', (34, 36))]","[['large quantities', 'of', 'high - confidence parse trees'], ['high - confidence parse trees', 'by parsing', 'unlabeled data'], ['unlabeled data', 'with', 'two different parsers'], ['high - confidence parse trees', 'selecting', 'sentences'], ['two parsers', 'produced', 'same trees']]",[],"[['Model', 'generate', 'large quantities']]",[],dependency_parsing,5,35
529,model,To this end we generate large quantities of high - confidence parse trees by parsing an unlabeled corpus and selecting only the sentences on which two different parsers produced the same parse trees .,"[('generate', (4, 5)), ('of', (7, 8)), ('by parsing', (13, 15)), ('selecting', (19, 20)), ('on', (23, 24)), ('produced', (28, 29))]","[('large quantities', (5, 7)), ('high - confidence parse trees', (8, 13)), ('unlabeled corpus', (16, 18)), ('sentences', (22, 23)), ('two different parsers', (25, 28)), ('same parse trees', (30, 33))]","[['large quantities', 'of', 'high - confidence parse trees'], ['high - confidence parse trees', 'by parsing', 'unlabeled corpus'], ['large quantities', 'selecting', 'sentences'], ['high - confidence parse trees', 'selecting', 'sentences'], ['sentences', 'on', 'two different parsers'], ['two different parsers', 'produced', 'same parse trees']]",[],"[['Model', 'generate', 'large quantities']]",[],dependency_parsing,5,47
530,model,"This idea comes from tri-training and while applicable to other parsers as well , we show that it benefits neural network parsers more than models with discrete features .","[('comes from', (2, 4)), ('applicable to', (7, 9)), ('show', (15, 16)), ('benefits', (18, 19)), ('more than', (22, 24))]","[('tri-training', (4, 5)), ('neural network parsers', (19, 22)), ('models with', (24, 26)), ('discrete features', (26, 28))]","[['neural network parsers', 'more than', 'models with']]","[['models with', 'has', 'discrete features']]","[['Model', 'comes from', 'tri-training']]",[],dependency_parsing,5,48
531,results,"We use a CRF - based POS tagger to generate 5 fold jack - knifed POS tags on the training set and predicted tags on the dev , test and tune sets ; our tagger gets comparable accuracy to the Stanford POS tagger with 97 . 44 % on the test set .","[('use', (1, 2)), ('to generate', (8, 10)), ('on', (17, 18)), ('on', (24, 25)), ('gets', (35, 36)), ('with', (43, 44)), ('on', (48, 49))]","[('CRF - based POS tagger', (3, 8)), ('5 fold jack - knifed POS tags', (10, 17)), ('training set and predicted tags', (19, 24)), ('dev , test and tune sets', (26, 32)), ('our', (33, 34)), ('comparable accuracy', (36, 38)), ('Stanford POS tagger', (40, 43)), ('97 . 44 %', (44, 48))]","[['CRF - based POS tagger', 'to generate', '5 fold jack - knifed POS tags'], ['5 fold jack - knifed POS tags', 'on', 'training set and predicted tags'], ['training set and predicted tags', 'on', 'dev , test and tune sets'], ['Stanford POS tagger', 'with', '97 . 44 %']]",[],"[['Results', 'use', 'CRF - based POS tagger']]",[],dependency_parsing,5,173
532,baselines,We train on the union of each corpora 's training set and test on each domain separately .,"[('train on', (1, 3)), ('of', (5, 6)), ('test on', (12, 14))]","[('union', (4, 5)), (""each corpora 's training set"", (6, 11)), ('each domain', (14, 16))]","[['union', 'of', ""each corpora 's training set""]]",[],[],[],dependency_parsing,5,177
533,baselines,"We process it with the Berkeley - Parser , a latent variable constituency parser , and a reimplementation of ZPar , a transition - based parser with beam search .","[('process it with', (1, 4)), ('reimplementation', (17, 18)), ('of', (18, 19)), ('with', (26, 27))]","[('Berkeley - Parser', (5, 8)), ('latent variable constituency parser', (10, 14)), ('ZPar', (19, 20)), ('transition - based parser', (22, 26)), ('beam search', (27, 29))]","[['transition - based parser', 'with', 'beam search']]",[],"[['Baselines', 'process it with', 'Berkeley - Parser']]",[],dependency_parsing,5,180
534,hyperparameters,randomly using a Gaussian distribution with variance 10 ?4 .,"[('randomly using', (0, 2)), ('with', (5, 6))]","[('Gaussian distribution', (3, 5)), ('variance 10 ?4', (6, 9))]","[['Gaussian distribution', 'with', 'variance 10 ?4']]",[],"[['Hyperparameters', 'randomly using', 'Gaussian distribution']]",[],dependency_parsing,5,196
535,hyperparameters,"We used fixed initialization with bi = 0.2 , to ensure that most Relu units are activated during the initial rounds of training .","[('used', (1, 2)), ('with', (4, 5)), ('to ensure', (9, 11)), ('during', (17, 18))]","[('fixed initialization', (2, 4)), ('bi = 0.2', (5, 8)), ('most Relu units', (12, 15)), ('activated', (16, 17)), ('initial rounds of training', (19, 23))]","[['fixed initialization', 'with', 'bi = 0.2'], ['fixed initialization', 'to ensure', 'most Relu units'], ['activated', 'during', 'initial rounds of training']]",[],"[['Hyperparameters', 'used', 'fixed initialization']]",[],dependency_parsing,5,197
536,hyperparameters,"For the word embedding matrix E word , we initialized the parameters using pretrained word embeddings .","[('For', (0, 1)), ('initialized', (9, 10)), ('using', (12, 13))]","[('word embedding matrix E word', (2, 7)), ('parameters', (11, 12)), ('pretrained word embeddings', (13, 16))]","[['word embedding matrix E word', 'initialized', 'parameters'], ['parameters', 'using', 'pretrained word embeddings']]",[],"[['Hyperparameters', 'For', 'word embedding matrix E word']]",[],dependency_parsing,5,199
537,hyperparameters,"For words not appearing in the unsupervised data and the special "" NULL "" etc. tokens , we used random initialization .","[('For', (0, 1)), ('not appearing in', (2, 5)), ('used', (18, 19))]","[('words', (1, 2)), ('unsupervised data', (6, 8)), ('random initialization', (19, 21))]","[['words', 'not appearing in', 'unsupervised data']]",[],"[['Hyperparameters', 'For', 'words']]",[],dependency_parsing,5,201
538,hyperparameters,All hyperparameters ( including structure ) were tuned using Section 24 of the WSJ only .,"[('tuned using', (7, 9)), ('of', (11, 12))]","[('Section 24', (9, 11)), ('WSJ', (13, 14))]","[['Section 24', 'of', 'WSJ']]",[],"[['Hyperparameters', 'tuned using', 'Section 24']]",[],dependency_parsing,5,205
539,hyperparameters,"When not tri-training , we used hyperparameters of ? = 0.2 , ? 0 = 0.05 , = 0.9 , early stopping after roughly 16 hours of training time .","[('used', (5, 6)), ('of', (7, 8)), ('after', (22, 23)), ('of', (26, 27))]","[('not', (1, 2)), ('tri-training', (2, 3)), ('hyperparameters', (6, 7)), ('? = 0.2 , ? 0 = 0.05 , = 0.9', (8, 19)), ('early stopping', (20, 22)), ('roughly 16 hours', (23, 26)), ('training time', (27, 29))]","[['tri-training', 'used', 'hyperparameters'], ['hyperparameters', 'of', '? = 0.2 , ? 0 = 0.05 , = 0.9'], ['roughly 16 hours', 'of', 'training time'], ['early stopping', 'after', 'roughly 16 hours'], ['roughly 16 hours', 'of', 'training time']]","[['not', 'has', 'tri-training']]",[],[],dependency_parsing,5,206
540,hyperparameters,"For the Treebank Union setup , we set M 1 = M 2 = 1024 for the standard training set and for the tri-training setup .","[('For', (0, 1)), ('set', (7, 8)), ('for', (15, 16))]","[('Treebank Union setup', (2, 5)), ('M 1 = M 2 = 1024', (8, 15)), ('standard training set', (17, 20)), ('tri-training setup', (23, 25))]","[['M 1 = M 2 = 1024', 'For', 'standard training set'], ['M 1 = M 2 = 1024', 'For', 'tri-training setup'], ['Treebank Union setup', 'set', 'M 1 = M 2 = 1024'], ['M 1 = M 2 = 1024', 'for', 'standard training set'], ['M 1 = M 2 = 1024', 'for', 'tri-training setup']]",[],"[['Hyperparameters', 'For', 'Treebank Union setup']]",[],dependency_parsing,5,209
541,baselines,We compare to the best dependency parsers in the literature .,"[('compare to', (1, 3))]","[('best dependency parsers', (4, 7))]",[],[],"[['Baselines', 'compare to', 'best dependency parsers']]",[],dependency_parsing,5,211
542,results,"On the WSJ and Web tasks , our parser outperforms all dependency parsers in our comparison by a substantial margin .","[('On', (0, 1)), ('outperforms', (9, 10)), ('in', (13, 14)), ('by', (16, 17))]","[('WSJ and Web tasks', (2, 6)), ('our parser', (7, 9)), ('all dependency parsers', (10, 13)), ('our comparison', (14, 16)), ('substantial margin', (18, 20))]","[['our parser', 'outperforms', 'all dependency parsers'], ['all dependency parsers', 'in', 'our comparison'], ['all dependency parsers', 'by', 'substantial margin'], ['our comparison', 'by', 'substantial margin']]","[['WSJ and Web tasks', 'has', 'our parser']]","[['Results', 'On', 'WSJ and Web tasks']]",[],dependency_parsing,5,213
543,results,"The Question ( QTB ) dataset is more sensitive to the smaller beam size we use in order to train the models in a reasonable time ; if we increase to B = 32 at inference time only , our perceptron performance goes up to 92.29 % LAS .","[('more sensitive to', (7, 10)), ('to train', (18, 20)), ('in', (22, 23)), ('increase to', (29, 31)), ('at', (34, 35)), ('goes up to', (42, 45))]","[('Question ( QTB ) dataset', (1, 6)), ('smaller beam size', (11, 14)), ('models', (21, 22)), ('reasonable time', (24, 26)), ('B = 32', (31, 34)), ('inference time only', (35, 38)), ('our perceptron performance', (39, 42)), ('92.29 % LAS', (45, 48))]","[['Question ( QTB ) dataset', 'more sensitive to', 'smaller beam size'], ['smaller beam size', 'to train', 'models'], ['models', 'in', 'reasonable time'], ['B = 32', 'at', 'inference time only'], ['our perceptron performance', 'goes up to', '92.29 % LAS']]","[['inference time only', 'has', 'our perceptron performance']]",[],[],dependency_parsing,5,214
544,results,"Although tritraining did help the baseline on the dev set , test set performance did not improve significantly .","[('did help', (2, 4)), ('on', (6, 7))]","[('tritraining', (1, 2)), ('baseline', (5, 6)), ('dev set', (8, 10)), ('test set performance', (11, 14)), ('did not improve significantly', (14, 18))]","[['tritraining', 'did help', 'baseline'], ['baseline', 'on', 'dev set']]","[['tritraining', 'has', 'baseline'], ['test set performance', 'has', 'did not improve significantly']]",[],"[['Results', 'has', 'tritraining']]",dependency_parsing,5,216
545,results,"As expected , tri-training helps most dramatically to increase accuracy on the Treebank Union setup with diverse domains , yielding 0.4 - 1.0 % absolute LAS improvement gains for our most accurate model .","[('helps', (4, 5)), ('to', (7, 8)), ('on', (10, 11)), ('with', (15, 16)), ('yielding', (19, 20)), ('for', (28, 29))]","[('tri-training', (3, 4)), ('most dramatically', (5, 7)), ('increase', (8, 9)), ('accuracy', (9, 10)), ('Treebank Union setup', (12, 15)), ('diverse domains', (16, 18)), ('0.4 - 1.0 % absolute LAS improvement gains', (20, 28)), ('our most accurate model', (29, 33))]","[['tri-training', 'helps', 'most dramatically'], ['most dramatically', 'to', 'increase'], ['accuracy', 'on', 'Treebank Union setup'], ['Treebank Union setup', 'with', 'diverse domains'], ['accuracy', 'yielding', '0.4 - 1.0 % absolute LAS improvement gains'], ['0.4 - 1.0 % absolute LAS improvement gains', 'for', 'our most accurate model']]","[['tri-training', 'has', 'most dramatically'], ['increase', 'has', 'accuracy']]",[],"[['Results', 'has', 'tri-training']]",dependency_parsing,5,218
546,results,"While adding a second hidden layer results in a large gain on the tune set , there is no gain on the dev set if pre-trained embeddings are not used .","[('adding', (1, 2)), ('results in', (6, 8)), ('on', (11, 12)), ('on', (20, 21)), ('if', (24, 25))]","[('second hidden layer', (3, 6)), ('large gain', (9, 11)), ('tune set', (13, 15)), ('dev set', (22, 24)), ('not used', (28, 30))]","[['second hidden layer', 'results in', 'large gain'], ['large gain', 'on', 'tune set']]",[],"[['Results', 'adding', 'second hidden layer']]",[],dependency_parsing,5,242
547,results,"For our neural network model , training on the output of the BerkeleyParser yields only modest gains , while training on the data where the two parsers agree produces significantly better results .","[('For', (0, 1)), ('training on', (6, 8)), ('of', (10, 11)), ('yields', (13, 14)), ('training on', (19, 21)), ('where', (23, 24)), ('produces', (28, 29))]","[('our neural network model', (1, 5)), ('output', (9, 10)), ('modest gains', (15, 17)), ('data', (22, 23)), ('two parsers agree', (25, 28)), ('significantly better results', (29, 32))]","[['our neural network model', 'training on', 'output'], ['our neural network model', 'training on', 'data'], ['data', 'where', 'two parsers agree'], ['two parsers agree', 'produces', 'significantly better results']]",[],"[['Results', 'For', 'our neural network model']]",[],dependency_parsing,5,273
548,results,"This was especially pronounced for the greedy models : after tri-training , the greedy neural network model surpasses the BerkeleyParser in accuracy .","[('after', (9, 10)), ('surpasses', (17, 18)), ('in', (20, 21))]","[('greedy models', (6, 8)), ('tri-training', (10, 11)), ('greedy neural network model', (13, 17)), ('BerkeleyParser', (19, 20)), ('accuracy', (21, 22))]","[['greedy models', 'after', 'tri-training'], ['greedy neural network model', 'surpasses', 'BerkeleyParser'], ['BerkeleyParser', 'in', 'accuracy']]","[['tri-training', 'has', 'greedy neural network model']]",[],[],dependency_parsing,5,274
549,results,It is also interesting to note that up - training improved results far more than tri-training for the baseline .,"[('improved', (10, 11)), ('far more than', (12, 15)), ('for', (16, 17))]","[('up - training', (7, 10)), ('results', (11, 12)), ('tri-training', (15, 16)), ('baseline', (18, 19))]","[['up - training', 'improved', 'results'], ['results', 'far more than', 'tri-training'], ['tri-training', 'for', 'baseline']]",[],[],[],dependency_parsing,5,275
550,ablation-analysis,"Regardless of tri-training , using the structured perceptron improved error rates on some of the common and difficult labels : ROOT , ccomp , cc , conj , and nsubj all improved by > 1 % .","[('using', (4, 5)), ('improved', (8, 9)), ('on', (11, 12)), ('improved by', (31, 33))]","[('structured perceptron', (6, 8)), ('error rates', (9, 11)), ('some of the common and difficult labels', (12, 19)), ('ROOT', (20, 21)), ('> 1 %', (33, 36))]","[['structured perceptron', 'improved', 'error rates'], ['error rates', 'on', 'some of the common and difficult labels'], ['error rates', 'improved by', '> 1 %']]",[],"[['Ablation analysis', 'using', 'structured perceptron']]",[],dependency_parsing,5,278
551,research-problem,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING,[],[],[],[],[],[],dependency_parsing,6,2
552,model,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .","[('modify', (1, 2)), ('to achieve', (13, 15)), ('build', (19, 20)), ('uses', (26, 27)), ('replace', (31, 32)), ('affine', (40, 41)), ('with', (43, 44)), ('that reduce', (70, 72))]","[('neural graphbased approach', (3, 6)), ('few', (11, 12)), ('competitive performance', (15, 17)), ('network', (21, 22)), ('larger', (24, 25)), ('more regularization', (27, 29)), ('traditional MLP - based attention mechanism', (33, 39)), ('label classifier', (41, 43)), ('biaffine ones', (44, 46)), ('MLP operations', (68, 70)), ('dimensionality', (73, 74))]","[['neural graphbased approach', 'to achieve', 'competitive performance'], ['competitive performance', 'build', 'network'], ['network', 'uses', 'more regularization'], ['label classifier', 'with', 'biaffine ones'], ['MLP operations', 'that reduce', 'dimensionality']]","[['neural graphbased approach', 'has', 'few'], ['network', 'has', 'larger']]","[['Model', 'modify', 'neural graphbased approach']]",[],dependency_parsing,6,13
553,model,The resulting parser maintains most of the simplicity of neural graph - based approaches while approaching the performance of the SOTA transition - based one .,"[('maintains', (3, 4)), ('of', (8, 9)), ('of', (18, 19))]","[('resulting parser', (1, 3)), ('most of the simplicity', (4, 8)), ('neural graph - based approaches', (9, 14)), ('performance', (17, 18)), ('SOTA transition - based one', (20, 25))]","[['resulting parser', 'maintains', 'most of the simplicity'], ['most of the simplicity', 'of', 'neural graph - based approaches'], ['performance', 'of', 'SOTA transition - based one'], ['performance', 'of', 'SOTA transition - based one']]","[['resulting parser', 'has', 'most of the simplicity']]",[],"[['Model', 'has', 'resulting parser']]",dependency_parsing,6,15
554,model,"We call this a deep bilinear attention mechanism , as opposed to shallow bilinear attention , which uses the recurrent states directly .","[('call', (1, 2)), ('as', (9, 10)), ('opposed to', (10, 12)), ('uses', (17, 18))]","[('deep bilinear attention mechanism', (4, 8)), ('shallow bilinear attention', (12, 15)), ('recurrent states', (19, 21))]","[['deep bilinear attention mechanism', 'opposed to', 'shallow bilinear attention'], ['shallow bilinear attention', 'uses', 'recurrent states']]",[],"[['Model', 'call', 'deep bilinear attention mechanism']]",[],dependency_parsing,6,56
555,model,We use 100 - dimensional uncased word vectors 2 and POS tag vectors ; three BiLSTM layers ( 400 dimensions in each direction ) ; and 500 - and 100 - dimensional ReLU MLP layers .,"[('use', (1, 2))]","[('100 - dimensional uncased word vectors', (2, 8)), ('POS tag vectors', (10, 13)), ('three BiLSTM layers', (14, 17)), ('500 - and 100 - dimensional ReLU MLP layers', (26, 35))]",[],[],"[['Model', 'use', '100 - dimensional uncased word vectors']]",[],dependency_parsing,6,60
556,model,"We also apply dropout at every stage of the model : we drop words and tags ( independently ) ; we drop nodes in the LSTM layers ( input and recurrent connections ) , applying the same dropout mask at every recurrent timestep ( cf. the Bayesian dropout of ) ; and we drop nodes in the MLP layers and classifiers , likewise applying the same dropout mask at every timestep .","[('apply', (2, 3)), ('at', (4, 5)), ('of', (7, 8)), ('drop', (21, 22)), ('in', (23, 24)), ('applying', (34, 35)), ('at', (39, 40)), ('in', (55, 56))]","[('dropout', (3, 4)), ('every stage', (5, 7)), ('nodes', (22, 23)), ('LSTM layers (', (25, 28)), ('same dropout mask', (36, 39)), ('nodes', (54, 55)), ('MLP layers and classifiers', (57, 61))]","[['dropout', 'at', 'every stage'], ['nodes', 'in', 'LSTM layers ('], ['nodes', 'applying', 'same dropout mask'], ['nodes', 'in', 'MLP layers and classifiers']]",[],"[['Model', 'apply', 'dropout']]",[],dependency_parsing,6,61
557,model,"We optimize the network with annealed Adam for about 50,000 steps , rounded up to the nearest epoch .","[('optimize', (1, 2)), ('with', (4, 5)), ('for', (7, 8)), ('rounded up to', (12, 15))]","[('network', (3, 4)), ('annealed Adam', (5, 7)), ('about 50,000 steps', (8, 11)), ('nearest epoch', (16, 18))]","[['network', 'with', 'annealed Adam'], ['annealed Adam', 'for', 'about 50,000 steps'], ['about 50,000 steps', 'rounded up to', 'nearest epoch']]",[],"[['Model', 'optimize', 'network']]",[],dependency_parsing,6,62
558,experiments,What we see is that the deep bilinear model outperforms the others with respect to both speed and accuracy .,"[('with respect to', (12, 15))]","[('deep bilinear model', (6, 9)), ('outperforms', (9, 10)), ('others', (11, 12)), ('both speed and accuracy', (15, 19))]","[['outperforms', 'with respect to', 'both speed and accuracy'], ['others', 'with respect to', 'both speed and accuracy']]","[['deep bilinear model', 'has', 'outperforms'], ['outperforms', 'has', 'others']]",[],[],dependency_parsing,6,73
559,experiments,"The model with shallow bilinear arc and label classifiers gets the same unlabeled performance as the deep model with the same settings , but because the label classifier is much larger ( ( 801 c 801 ) as opposed to ( 101 c 101 ) ) , it runs much slower and overfits .","[('with', (2, 3)), ('gets', (9, 10)), ('as', (14, 15)), ('with', (18, 19)), ('runs', (48, 49))]","[('model', (1, 2)), ('shallow bilinear arc and label classifiers', (3, 9)), ('same unlabeled performance', (11, 14)), ('deep model', (16, 18)), ('same settings', (20, 22)), ('much slower', (49, 51)), ('overfits', (52, 53))]","[['model', 'with', 'shallow bilinear arc and label classifiers'], ['shallow bilinear arc and label classifiers', 'gets', 'same unlabeled performance'], ['same unlabeled performance', 'as', 'deep model'], ['deep model', 'with', 'same settings']]",[],[],[],dependency_parsing,6,74
560,experiments,"We find that using three or four layers gets significantly better performance than two layers , and increasing the LSTM sizes from 200 to 300 or 400 dimensions likewise signficantly improves performance .","[('find', (1, 2)), ('using', (3, 4)), ('gets', (8, 9)), ('than', (12, 13)), ('increasing', (17, 18)), ('from', (21, 22)), ('signficantly improves', (29, 31))]","[('three or four layers', (4, 8)), ('significantly better performance', (9, 12)), ('two layers', (13, 15)), ('LSTM sizes', (19, 21)), ('200 to', (22, 24)), ('300 or 400 dimensions', (24, 28)), ('performance', (31, 32))]","[['three or four layers', 'gets', 'significantly better performance'], ['significantly better performance', 'than', 'two layers'], ['three or four layers', 'increasing', 'LSTM sizes'], ['LSTM sizes', 'from', '200 to'], ['LSTM sizes', 'signficantly improves', 'performance'], ['300 or 400 dimensions', 'signficantly improves', 'performance']]","[['three or four layers', 'has', 'significantly better performance'], ['LSTM sizes', 'has', '200 to'], ['200 to', 'has', '300 or 400 dimensions']]",[],[],dependency_parsing,6,83
561,experiments,"We also implemented the coupled input - forget gate LSTM cells ( Cif - LSTM ) suggested by , 6 finding that while the resulting model still slightly underperforms the more popular LSTM cells , the difference between the two is much smaller .","[('implemented', (2, 3))]","[('coupled input - forget gate LSTM cells ( Cif - LSTM )', (4, 16)), ('resulting', (24, 25)), ('more popular LSTM cells', (30, 34)), ('much smaller', (41, 43))]",[],"[['coupled input - forget gate LSTM cells ( Cif - LSTM )', 'has', 'resulting']]",[],[],dependency_parsing,6,87
562,model,"In addition to using relatively extreme dropout in the recurrent and MLP layers mentioned in , we also regularize the input layer .","[('in', (7, 8)), ('regularize', (18, 19))]","[('recurrent and MLP layers', (9, 13)), ('input layer', (20, 22))]",[],[],[],[],dependency_parsing,6,91
563,experiments,"Interestingly , not using any tags at all actually results in better performance than using tags without dropout .","[('not using', (2, 4)), ('results in', (9, 11)), ('than', (13, 14)), ('without', (16, 17))]","[('any tags at all', (4, 8)), ('better performance', (11, 13)), ('using', (14, 15)), ('tags', (15, 16))]","[['any tags at all', 'results in', 'better performance'], ['better performance', 'than', 'using'], ['better performance', 'than', 'tags']]","[['using', 'has', 'tags']]",[],[],dependency_parsing,6,94
564,results,"Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .","[('gets', (2, 3)), ('on', (8, 9)), ('as', (13, 14)), ('gets', (28, 29)), ('on', (32, 33)), ('as well', (36, 38)), ('on', (41, 42))]","[('Our model', (0, 2)), ('nearly the same UAS performance', (3, 8)), ('PTB - SD 3.3.0', (9, 13)), ('current SOTA model', (15, 18)), ('SOTA UAS performance', (29, 32)), ('CTB 5.1', (33, 35)), ('SOTA performance', (39, 41)), ('all CoNLL 09 languages', (42, 46))]","[['Our model', 'gets', 'nearly the same UAS performance'], ['Our model', 'gets', 'SOTA UAS performance'], ['Our model', 'gets', 'SOTA performance'], ['nearly the same UAS performance', 'on', 'PTB - SD 3.3.0'], ['PTB - SD 3.3.0', 'as', 'current SOTA model'], ['Our model', 'gets', 'SOTA UAS performance'], ['SOTA UAS performance', 'on', 'CTB 5.1'], ['SOTA performance', 'on', 'all CoNLL 09 languages']]","[['Our model', 'has', 'nearly the same UAS performance']]",[],"[['Results', 'has', 'Our model']]",dependency_parsing,6,102
565,research-problem,Training with Exploration Improves a Greedy Stack LSTM Parser,[],[],[],[],[],[],dependency_parsing,7,2
566,model,"Coupled with a recursive tree composition function , the feature representation is able to capture information from the entirety of the state , without resorting to locality assumptions that were common in most other transition - based parsers .","[('Coupled with', (0, 2)), ('able to capture', (12, 15)), ('from', (16, 17)), ('without resorting to', (23, 26))]","[('recursive tree composition function', (3, 7)), ('feature representation', (9, 11)), ('information', (15, 16)), ('entirety of the state', (18, 22)), ('locality assumptions', (26, 28))]","[['feature representation', 'able to capture', 'information'], ['information', 'from', 'entirety of the state'], ['information', 'without resorting to', 'locality assumptions']]","[['recursive tree composition function', 'has', 'feature representation']]","[['Model', 'Coupled with', 'recursive tree composition function']]",[],dependency_parsing,7,12
567,model,"The use of a novel stack LSTM data structure allows the parser to maintain a constant time per-state update , and retain an over all linear parsing time .","[('use of', (1, 3)), ('allows', (9, 10)), ('to maintain', (12, 14)), ('retain', (21, 22))]","[('novel stack LSTM data structure', (4, 9)), ('parser', (11, 12)), ('constant time per-state update', (15, 19)), ('over all linear parsing time', (23, 28))]","[['novel stack LSTM data structure', 'allows', 'parser'], ['parser', 'to maintain', 'constant time per-state update'], ['novel stack LSTM data structure', 'retain', 'over all linear parsing time']]",[],"[['Model', 'use of', 'novel stack LSTM data structure']]",[],dependency_parsing,7,13
568,model,"At test time , the parser makes greedy decisions according to the learned model .","[('At', (0, 1)), ('makes', (6, 7)), ('according to', (9, 11))]","[('test time', (1, 3)), ('parser', (5, 6)), ('greedy decisions', (7, 9)), ('learned model', (12, 14))]","[['parser', 'makes', 'greedy decisions'], ['greedy decisions', 'according to', 'learned model']]","[['test time', 'has', 'parser']]","[['Model', 'At', 'test time']]",[],dependency_parsing,7,15
569,model,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .","[('adapt', (5, 6)), ('drawn', (15, 16))]","[('training criterion', (7, 9)), ('parser states', (13, 15)), ('training data', (20, 22)), ('model', (27, 28)), ('learned', (32, 33))]",[],[],"[['Model', 'adapt', 'training criterion']]",[],dependency_parsing,7,17
570,model,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .","[('interpolating', (1, 2)), ('sampled from', (5, 7)), ('sampled from', (11, 13)), ('at', (20, 21))]","[('algorithm states', (3, 5)), ('model', (8, 9)), ('training data', (14, 16)), ('more robust predictions', (17, 20)), ('test time', (21, 23))]","[['algorithm states', 'sampled from', 'model'], ['algorithm states', 'sampled from', 'training data'], ['more robust predictions', 'at', 'test time']]",[],"[['Model', 'interpolating', 'algorithm states']]",[],dependency_parsing,7,19
571,results,The score achieved by the dynamic oracle for English is 93.56 UAS .,"[('achieved by', (2, 4)), ('for', (7, 8)), ('is', (9, 10))]","[('score', (1, 2)), ('dynamic oracle', (5, 7)), ('English', (8, 9)), ('93.56 UAS', (10, 12))]","[['score', 'achieved by', 'dynamic oracle'], ['dynamic oracle', 'for', 'English'], ['dynamic oracle', 'is', '93.56 UAS'], ['English', 'is', '93.56 UAS']]",[],[],[],dependency_parsing,7,78
572,results,"The error - exploring dynamic - oracle training always improves over static oracle training controlling for the transition system , but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle .","[('always', (8, 9)), ('over', (10, 11)), ('for', (15, 16)), ('when trained with', (29, 32))]","[('error - exploring dynamic - oracle training', (1, 8)), ('static oracle training controlling', (11, 15)), ('transition system', (17, 19)), ('arc-hybrid system', (22, 24)), ('slightly under-performs', (24, 26)), ('arc-standard system', (27, 29)), ('static oracle', (32, 34))]","[['static oracle training controlling', 'for', 'transition system'], ['arc-standard system', 'when trained with', 'static oracle']]","[['arc-hybrid system', 'has', 'slightly under-performs'], ['slightly under-performs', 'has', 'arc-standard system']]",[],"[['Results', 'has', 'error - exploring dynamic - oracle training']]",dependency_parsing,7,84
573,hyperparameters,Flattening the sampling distribution ( ? = 0.75 ) is especially beneficial when training with pretrained word embeddings .,[],"[('Flattening', (0, 1)), ('sampling distribution ( ? = 0.75 )', (2, 9)), ('training', (13, 14)), ('pretrained word embeddings', (15, 18))]",[],"[['Flattening', 'has', 'sampling distribution ( ? = 0.75 )']]",[],"[['Hyperparameters', 'has', 'Flattening']]",dependency_parsing,7,85
574,model,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .","[('demonstrate', (4, 5)), ('without', (11, 12)), ('can achieve', (14, 16)), ('than', (20, 21)), ('as', (23, 24)), ('long as', (24, 26)), ('are', (27, 28))]","[('simple feed - forward networks', (6, 11)), ('any recurrence', (12, 14)), ('comparable or better accuracies', (16, 20)), ('LSTMs', (21, 22)), ('globally normalized', (28, 30))]","[['simple feed - forward networks', 'without', 'any recurrence'], ['simple feed - forward networks', 'can achieve', 'comparable or better accuracies'], ['comparable or better accuracies', 'than', 'LSTMs'], ['comparable or better accuracies', 'long as', 'globally normalized'], ['comparable or better accuracies', 'are', 'globally normalized']]",[],"[['Model', 'demonstrate', 'simple feed - forward networks']]",[],dependency_parsing,8,11
575,model,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .","[('use', (3, 4)), ('perform', (8, 9)), ('for', (11, 12)), ('introduce', (16, 17)), ('with', (19, 20)), ('to overcome', (28, 30))]","[('any recurrence', (4, 6)), ('beam search', (9, 11)), ('maintaining', (12, 13)), ('multiple hypotheses', (13, 15)), ('global normalization', (17, 19)), ('conditional random field ( CRF ) objective', (21, 28)), ('label bias problem', (31, 34)), ('locally normalized models', (35, 38))]","[['beam search', 'for', 'maintaining'], ['global normalization', 'with', 'conditional random field ( CRF ) objective'], ['conditional random field ( CRF ) objective', 'to overcome', 'label bias problem']]","[['maintaining', 'has', 'multiple hypotheses'], ['label bias problem', 'has', 'locally normalized models']]","[['Model', 'use', 'any recurrence']]",[],dependency_parsing,8,13
576,model,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,"[('compute', (1, 2)), ('based on', (3, 5)), ('perform', (10, 11)), ('of', (14, 15)), ('based on', (19, 21))]","[('gradients', (2, 3)), ('approximate global normalization', (6, 9)), ('full backpropagation training', (11, 14)), ('all neural network parameters', (15, 19)), ('CRF loss', (22, 24))]","[['gradients', 'based on', 'approximate global normalization'], ['gradients', 'perform', 'full backpropagation training'], ['full backpropagation training', 'of', 'all neural network parameters'], ['all neural network parameters', 'based on', 'CRF loss']]","[['gradients', 'has', 'approximate global normalization']]",[],[],dependency_parsing,8,15
577,model,"As discussed in more detail in Section 5 , we also outperform previous structured training approaches used for neural network transitionbased parsing .","[('used for', (16, 18))]","[('outperform', (11, 12)), ('previous structured training approaches', (12, 16)), ('neural network transitionbased parsing', (18, 22))]","[['previous structured training approaches', 'used for', 'neural network transitionbased parsing']]","[['outperform', 'has', 'previous structured training approaches']]",[],"[['Model', 'has', 'outperform']]",dependency_parsing,8,21
578,model,"We also provide a pre-trained , state - of - the art English dependency parser called "" Parsey McParseface , "" which we tuned for a balance of speed , simplicity , and accuracy .","[('provide', (2, 3)), ('called', (15, 16))]","[('pre-trained , state - of - the art English dependency parser', (4, 15)), ('Parsey McParseface', (17, 19))]","[['pre-trained , state - of - the art English dependency parser', 'called', 'Parsey McParseface']]","[['pre-trained , state - of - the art English dependency parser', 'name', 'Parsey McParseface']]","[['Model', 'provide', 'pre-trained , state - of - the art English dependency parser']]",[],dependency_parsing,8,27
579,experiments,We use stochastic gradient descent on the negative log - likelihood of the data under the model .,"[('use', (1, 2)), ('on', (5, 6)), ('of', (11, 12)), ('under', (14, 15))]","[('stochastic gradient descent', (2, 5)), ('negative log - likelihood', (7, 11)), ('data', (13, 14)), ('model', (16, 17))]","[['stochastic gradient descent', 'on', 'negative log - likelihood'], ['negative log - likelihood', 'of', 'data'], ['negative log - likelihood', 'under', 'model'], ['data', 'under', 'model']]",[],[],[],dependency_parsing,8,72
580,baselines,"We apply our approach to POS tagging , syntactic dependency parsing , and sentence compression .","[('apply', (1, 2)), ('to', (4, 5))]","[('POS tagging', (5, 7)), ('syntactic dependency parsing', (8, 11)), ('sentence compression', (13, 15))]",[],[],[],[],dependency_parsing,8,151
581,ablation-analysis,"While directly optimizing the global model defined by Eq. ( 5 ) works well , we found that training the model in two steps achieves the same precision much faster : we first pretrain the network using the local objective given in Eq. ( 4 ) , and then perform additional training steps using the global objective given in Eq. ( 6 ) .","[('works', (12, 13)), ('in', (21, 22)), ('achieves', (24, 25)), ('pretrain', (33, 34)), ('using', (36, 37)), ('perform', (49, 50)), ('using', (53, 54))]","[('directly optimizing', (1, 3)), ('global model', (4, 6)), ('well', (13, 14)), ('training', (18, 19)), ('model', (20, 21)), ('two steps', (22, 24)), ('same precision', (26, 28)), ('network', (35, 36)), ('local objective', (38, 40)), ('additional training steps', (50, 53)), ('global objective', (55, 57))]","[['global model', 'works', 'well'], ['model', 'in', 'two steps'], ['training', 'achieves', 'same precision'], ['model', 'achieves', 'same precision'], ['network', 'using', 'local objective'], ['global model', 'perform', 'additional training steps'], ['additional training steps', 'using', 'global objective']]","[['directly optimizing', 'has', 'global model'], ['training', 'has', 'model']]",[],"[['Ablation analysis', 'has', 'directly optimizing']]",dependency_parsing,8,152
582,hyperparameters,"Specifically , we use averaged stochastic gradient descent with momentum , and we tune the learning rate , learning rate schedule , momentum , and early stopping time using a separate held - out corpus for each task .","[('use', (3, 4)), ('with', (8, 9)), ('tune', (13, 14)), ('using', (28, 29)), ('for', (35, 36))]","[('averaged stochastic gradient descent', (4, 8)), ('momentum', (9, 10)), ('learning rate', (15, 17)), ('learning rate schedule', (18, 21)), ('momentum', (22, 23)), ('early stopping time', (25, 28)), ('separate held - out corpus', (30, 35)), ('each task', (36, 38))]","[['averaged stochastic gradient descent', 'with', 'momentum'], ['early stopping time', 'using', 'separate held - out corpus'], ['separate held - out corpus', 'for', 'each task']]",[],"[['Hyperparameters', 'use', 'averaged stochastic gradient descent']]",[],dependency_parsing,8,156
583,experiments,"Part of speech ( POS ) tagging is a classic NLP task , where modeling the structure of the output is important for achieving state - of - the - art performance .",[],"[('speech ( POS ) tagging', (2, 7))]",[],[],[],[],dependency_parsing,8,159
584,ablation-analysis,"We extract features from words , POS tags , and dependency labels from a window of tokens centered on the in - put , as well as features from the history of predictions .","[('extract', (1, 2)), ('from', (3, 4)), ('from', (12, 13)), ('centered on', (17, 19)), ('as', (24, 25)), ('from', (28, 29))]","[('features', (2, 3)), ('words', (4, 5)), ('window of tokens', (14, 17)), ('in - put', (20, 23)), ('features', (27, 28)), ('history of predictions', (30, 33))]","[['features', 'from', 'words'], ['features', 'from', 'history of predictions'], ['window of tokens', 'centered on', 'in - put'], ['window of tokens', 'as', 'features'], ['features', 'from', 'history of predictions']]",[],"[['Ablation analysis', 'extract', 'features']]",[],dependency_parsing,8,167
585,hyperparameters,We use a single hidden layer of size 400 .,"[('use', (1, 2)), ('of size', (6, 8))]","[('single hidden layer', (3, 6)), ('400', (8, 9))]","[['single hidden layer', 'of size', '400']]",[],"[['Hyperparameters', 'use', 'single hidden layer']]",[],dependency_parsing,8,168
586,experiments,Our globally normalized model again significantly outperforms the local model .,[],"[('Our globally normalized model', (0, 4)), ('significantly outperforms', (5, 7)), ('local model', (8, 10))]",[],"[['Our globally normalized model', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'local model']]",[],[],dependency_parsing,8,171
587,baselines,"We also compare to the sentence compression system from , a 3 - layer stacked LSTM which uses dependency label information .","[('compare to', (2, 4)), ('from', (8, 9)), ('uses', (17, 18))]","[('sentence compression system', (5, 8)), ('3 - layer stacked LSTM', (11, 16)), ('dependency label information', (18, 21))]","[['sentence compression system', 'from', '3 - layer stacked LSTM'], ['3 - layer stacked LSTM', 'uses', 'dependency label information']]","[['sentence compression system', 'has', '3 - layer stacked LSTM']]","[['Baselines', 'compare to', 'sentence compression system']]",[],dependency_parsing,8,173
588,experiments,"The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings , but our model is roughly 100 faster .","[('perform', (6, 7)), ('on both', (9, 11)), ('is', (24, 25))]","[('LSTM and our global model', (1, 6)), ('on par', (7, 9)), ('automatic evaluation', (12, 14)), ('human ratings', (18, 20)), ('our model', (22, 24)), ('roughly 100 faster', (25, 28))]","[['LSTM and our global model', 'perform', 'on par'], ['on par', 'on both', 'automatic evaluation'], ['our model', 'is', 'roughly 100 faster']]","[['our model', 'has', 'roughly 100 faster']]",[],[],dependency_parsing,8,174
589,experiments,All compressions kept approximately 42 % of the tokens on average and all the models are significantly better than the automatic extractions ( p < 0.05 ) .,"[('kept', (2, 3)), ('of', (6, 7))]","[('All compressions', (0, 2)), ('approximately 42 %', (3, 6)), ('tokens', (8, 9))]","[['All compressions', 'kept', 'approximately 42 %'], ['approximately 42 %', 'of', 'tokens']]","[['All compressions', 'has', 'approximately 42 %']]",[],[],dependency_parsing,8,175
590,baselines,"Inspired by the integrated POS tagging and parsing transition system of , we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack .","[('employ', (13, 14)), ('uses', (19, 20)), ('predicts', (25, 26)), ('of', (29, 30)), ('on', (33, 34)), ('shifted to', (39, 41))]","[('simple transition system', (15, 18)), ('SHIFT action', (22, 24)), ('POS tag', (27, 29)), ('current word', (31, 33)), ('buffer', (35, 36)), ('stack', (42, 43))]","[['simple transition system', 'uses', 'SHIFT action'], ['simple transition system', 'predicts', 'POS tag'], ['POS tag', 'of', 'current word'], ['current word', 'on', 'buffer'], ['POS tag', 'shifted to', 'stack']]",[],[],[],dependency_parsing,8,177
591,hyperparameters,"We extract the following features on a window 3 tokens centered at the current focus token : word , cluster , character n- gram up to length 3 .","[('extract', (1, 2)), ('on', (5, 6)), ('centered at', (10, 12)), ('up to', (24, 26))]","[('window 3 tokens', (7, 10)), ('current focus token', (13, 16)), ('word , cluster', (17, 20)), ('length 3', (26, 28))]","[['window 3 tokens', 'centered at', 'current focus token']]",[],"[['Hyperparameters', 'extract', 'window 3 tokens']]",[],dependency_parsing,8,178
592,results,Our local model already compares favorably against these methods on average .,"[('compares', (4, 5))]","[('Our local model', (0, 3)), ('favorably', (5, 6))]","[['Our local model', 'compares', 'favorably']]",[],[],"[['Results', 'has', 'Our local model']]",dependency_parsing,8,185
593,results,"Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .","[('Using', (0, 1)), ('with', (3, 4)), ('does', (8, 9)), ('with', (13, 14)), ('leads to', (17, 19)), ('in', (23, 24))]","[('beam search', (1, 3)), ('locally normalized model', (5, 8)), ('help', (10, 11)), ('global normalization', (14, 16)), ('7 % reduction', (20, 23)), ('relative error', (24, 26))]","[['beam search', 'with', 'locally normalized model'], ['beam search', 'with', 'global normalization'], ['locally normalized model', 'does', 'help'], ['beam search', 'with', 'global normalization'], ['global normalization', 'leads to', '7 % reduction'], ['7 % reduction', 'in', 'relative error']]","[['beam search', 'has', 'locally normalized model']]","[['Results', 'Using', 'beam search']]",[],dependency_parsing,8,186
594,experiments,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .","[('is', (6, 7)), ('increasing', (10, 11)), ('on', (13, 14)), ('by', (18, 19))]","[('set of character ngrams feature', (1, 6)), ('very important', (7, 9)), ('average accuracy', (11, 13)), (""CoNLL '09 datasets"", (15, 18)), ('about 0.5 % absolute', (19, 23))]","[['set of character ngrams feature', 'is', 'very important'], ['set of character ngrams feature', 'increasing', 'average accuracy'], ['average accuracy', 'on', ""CoNLL '09 datasets""], ['average accuracy', 'by', 'about 0.5 % absolute'], [""CoNLL '09 datasets"", 'by', 'about 0.5 % absolute']]","[['set of character ngrams feature', 'has', 'very important']]",[],[],dependency_parsing,8,187
595,experiments,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .","[('compares', (10, 11)), ('with', (23, 24))]","[('favorably', (11, 12)), ('94.26 % LAS and 92.41 % UAS', (14, 21))]",[],[],[],[],dependency_parsing,8,192
596,experiments,Our results also significantly outperform the LSTM - based approaches of .,[],"[('significantly outperform', (3, 5)), ('LSTM - based approaches', (6, 10))]",[],"[['significantly outperform', 'has', 'LSTM - based approaches']]",[],[],dependency_parsing,8,194
597,research-problem,Bag of Tricks for Efficient Text Classification,[],[],[],[],[],[],document_classification,0,2
598,research-problem,This paper explores a simple and efficient baseline for text classification .,[],"[('text classification', (9, 11))]",[],[],[],[],document_classification,0,4
599,research-problem,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",[],"[('Text classification', (0, 2))]",[],[],[],[],document_classification,0,8
600,experiments,Sentiment analysis,[],[],[],[],[],[],document_classification,0,53
601,tasks,"On this task , adding bigram information improves the performance by 1 - 4 % .","[('adding', (4, 5)), ('improves', (7, 8)), ('by', (10, 11))]","[('bigram information', (5, 7)), ('performance', (9, 10)), ('1 - 4 %', (11, 15))]","[['bigram information', 'improves', 'performance'], ['performance', 'by', '1 - 4 %']]",[],[],[],document_classification,0,61
602,tasks,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","[('slightly better than', (4, 7)), ('a', (16, 17)), ('bit worse than', (17, 20))]","[('our accuracy', (1, 3)), ('char - CNN and char - CRNN', (7, 14)), ('VDCNN', (20, 21))]","[['our accuracy', 'slightly better than', 'char - CNN and char - CRNN'], ['our accuracy', 'bit worse than', 'VDCNN']]",[],[],"[['Tasks', 'has', 'our accuracy']]",document_classification,0,62
603,tasks,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .","[('increase', (4, 5)), ('by using', (8, 10)), ('for example with', (13, 16)), ('performance on', (19, 21)), ('goes up to', (22, 25))]","[('accuracy slightly', (6, 8)), ('more n-grams', (10, 12)), ('trigrams', (16, 17)), ('Sogou', (21, 22)), ('97.1 %', (25, 27))]","[['accuracy slightly', 'by using', 'more n-grams'], ['more n-grams', 'for example with', 'trigrams'], ['trigrams', 'performance on', 'Sogou'], ['Sogou', 'goes up to', '97.1 %']]",[],[],[],document_classification,0,63
604,experiments,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,"[('focus on', (1, 3)), ('according to', (6, 8))]","[('predicting', (3, 4)), ('tags', (5, 6)), ('title and caption', (9, 12))]","[['tags', 'according to', 'title and caption']]","[['predicting', 'has', 'tags']]",[],[],document_classification,0,72
605,experiments,"The vocabulary size is 297,141 and there are 312,116 unique tags .","[('is', (3, 4))]","[('vocabulary size', (1, 3)), ('297,141', (4, 5)), ('312,116 unique tags', (8, 11))]","[['vocabulary size', 'is', '297,141']]","[['vocabulary size', 'has', '297,141']]",[],[],document_classification,0,76
606,baselines,We consider a frequency - based baseline which predicts the most frequent tag .,"[('consider', (1, 2)), ('predicts', (8, 9))]","[('frequency - based baseline', (3, 7)), ('most frequent tag', (10, 13))]","[['frequency - based baseline', 'predicts', 'most frequent tag']]",[],[],[],document_classification,0,79
607,baselines,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .","[('compare with', (2, 4)), ('based on', (24, 26))]","[('Tagspace ( Weston et al. , 2014 )', (4, 12)), ('tag', (16, 17)), ('Wsabie model', (27, 29))]","[['Tagspace ( Weston et al. , 2014 )', 'based on', 'Wsabie model']]","[['Tagspace ( Weston et al. , 2014 )', 'has', 'tag']]",[],[],document_classification,0,80
608,results,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .","[('described using', (5, 7)), ('consider', (10, 11)), ('achieves', (16, 17)), ('is', (20, 21))]","[('Tagspace model', (2, 4)), ('convolutions', (7, 8)), ('linear version', (12, 14)), ('comparable performance', (17, 19)), ('much faster', (21, 23))]","[['Tagspace model', 'described using', 'convolutions'], ['convolutions', 'consider', 'linear version'], ['linear version', 'achieves', 'comparable performance'], ['Tagspace model', 'is', 'much faster']]",[],[],"[['Results', 'has', 'Tagspace model']]",document_classification,0,81
609,tasks,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .","[('achieve', (2, 3)), ('with', (6, 7)), ('adding', (13, 14)), ('gives', (15, 16)), ('in', (20, 21))]","[('Both', (0, 1)), ('similar performance', (4, 6)), ('small hidden layer', (8, 11)), ('bigrams', (14, 15)), ('significant boost', (18, 20)), ('accuracy', (21, 22))]","[['Both', 'achieve', 'similar performance'], ['similar performance', 'with', 'small hidden layer'], ['Both', 'adding', 'bigrams'], ['bigrams', 'gives', 'significant boost'], ['significant boost', 'in', 'accuracy']]",[],[],[],document_classification,0,83
610,tasks,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .","[('At', (0, 1)), ('needs to compute', (5, 8)), ('for', (10, 11)), ('makes it', (15, 17)), ('gives', (24, 25)), ('when', (30, 31))]","[('test time', (1, 3)), ('Tagspace', (4, 5)), ('scores', (9, 10)), ('all the classes', (11, 14)), ('relatively slow', (17, 19)), ('our fast inference', (21, 24)), ('significant speed - up', (26, 30)), ('number of classes is large ( more than 300 K here )', (32, 44))]","[['Tagspace', 'needs to compute', 'scores'], ['scores', 'for', 'all the classes'], ['all the classes', 'makes it', 'relatively slow'], ['our fast inference', 'gives', 'significant speed - up'], ['significant speed - up', 'when', 'number of classes is large ( more than 300 K here )']]","[['test time', 'has', 'Tagspace'], ['our fast inference', 'has', 'significant speed - up']]",[],[],document_classification,0,84
611,research-problem,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION,[],[],[],[],[],[],document_classification,1,2
612,research-problem,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",[],"[('cross - lingual understanding ( XLU )', (3, 10))]",[],[],[],[],document_classification,1,5
613,approach,"In this paper , we propose to jointly tackle both language and domain transfer .",[],"[('jointly', (7, 8)), ('both language and domain transfer', (9, 14))]",[],[],[],[],document_classification,1,35
614,approach,"Using this unlabeled data , we combine the aforementioned cross - lingual methods with recently proposed unsupervised domain adaptation and weak supervision techniques on the task of cross - lingual document classification .",[],"[('cross - lingual document classification', (27, 32))]",[],[],[],[],document_classification,1,37
615,approach,"In particular , we focus on two approaches for domain adaptation .","[('focus on', (4, 6)), ('for', (8, 9))]","[('two approaches', (6, 8)), ('domain adaptation', (9, 11))]","[['two approaches', 'for', 'domain adaptation']]",[],"[['Approach', 'focus on', 'two approaches']]",[],document_classification,1,38
616,approach,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,"[('based on', (4, 6)), ('using', (17, 18))]","[('first method', (1, 3)), ('masked language model ( MLM ) pre-training', (6, 13)), ('unlabeled target language corpora', (18, 22))]","[['first method', 'based on', 'masked language model ( MLM ) pre-training'], ['masked language model ( MLM ) pre-training', 'using', 'unlabeled target language corpora']]",[],[],"[['Approach', 'has', 'first method']]",document_classification,1,39
617,approach,We propose to alleviate this issue by using self - training technique to do the domain adaptation from the source language into the target language .,"[('to do', (12, 14)), ('from', (17, 18)), ('into', (21, 22))]","[('self - training technique', (8, 12)), ('domain adaptation', (15, 17)), ('source language', (19, 21)), ('target language', (23, 25))]","[['self - training technique', 'to do', 'domain adaptation'], ['domain adaptation', 'from', 'source language'], ['source language', 'into', 'target language']]",[],[],[],document_classification,1,46
618,baselines,"In this paper , we adopt the second approach as the basic model , and utilize the XLM model ) as our base model , which has been pre-trained by large - scale parallel and monolingual data from various languages .","[('adopt', (5, 6)), ('as', (9, 10)), ('utilize', (15, 16)), ('as', (20, 21)), ('pre-trained by', (28, 30))]","[('second approach', (7, 9)), ('basic model', (11, 13)), ('XLM model', (17, 19)), ('our base model', (21, 24)), ('large - scale parallel and monolingual data', (30, 37))]","[['second approach', 'as', 'basic model'], ['XLM model', 'as', 'our base model'], ['our base model', 'pre-trained by', 'large - scale parallel and monolingual data']]","[['second approach', 'has', 'basic model']]","[['Baselines', 'adopt', 'second approach']]",[],document_classification,1,79
619,baselines,SEMI - SUPERVISED XLU,[],[],[],[],[],[],document_classification,1,82
620,experiments,Masked Language,[],[],[],[],[],[],document_classification,1,86
621,experiments,"Alleviating the Train - Test Discrepancy of the UDA Method With the UDA algorithm , the classifier is able to learn some prior information on the target domain , however it still suffers from the train - test discrepancy .","[('Alleviating', (0, 1)), ('of', (6, 7)), ('With', (10, 11)), ('able to learn', (18, 21)), ('on', (24, 25)), ('suffers from', (32, 34))]","[('Train - Test Discrepancy', (2, 6)), ('UDA Method', (8, 10)), ('UDA algorithm', (12, 14)), ('target domain', (26, 28)), ('train - test', (35, 38))]","[['Train - Test Discrepancy', 'of', 'UDA Method'], ['UDA Method', 'With', 'UDA algorithm']]","[['Train - Test Discrepancy', 'has', 'UDA Method']]",[],[],document_classification,1,106
622,baselines,"Follwing this process , we obtain a new classifier trained only based on the target domain , which does not suffer from the train - test mismatch problem .","[('obtain', (5, 6)), ('trained only based on', (9, 13))]","[('new classifier', (7, 9)), ('target domain', (14, 16)), ('train - test mismatch problem', (23, 28))]","[['new classifier', 'trained only based on', 'target domain']]",[],[],[],document_classification,1,118
623,experiments,"In the sentiment classification task , because the size of the unlabeled corpus in each target domain is large enough , we fine - tune an XLM with MLM loss for each target domain respectively .","[('fine - tune', (22, 25)), ('with', (27, 28)), ('for', (30, 31))]","[('sentiment classification task', (2, 5)), ('XLM', (26, 27)), ('MLM loss', (28, 30)), ('each target domain', (31, 34))]","[['XLM', 'with', 'MLM loss'], ['MLM loss', 'for', 'each target domain']]","[['sentiment classification task', 'has', 'XLM']]",[],[],document_classification,1,147
624,baselines,Fine-tune ( Ft ) : Fine - tuning the pre-trained model with the source - domain training set .,"[('with', (11, 12))]","[('Fine-tune ( Ft )', (0, 4)), ('Fine - tuning', (5, 8)), ('pre-trained model', (9, 11)), ('source - domain training set', (13, 18))]","[['pre-trained model', 'with', 'source - domain training set']]","[['Fine-tune ( Ft )', 'has', 'Fine - tuning'], ['Fine - tuning', 'has', 'pre-trained model']]",[],"[['Baselines', 'has', 'Fine-tune ( Ft )']]",document_classification,1,152
625,results,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .","[('Looking at', (0, 2)), ('without the help of', (12, 16)), ('from', (18, 19)), ('between', (29, 30)), ('of', (33, 34)), ('when using', (44, 46))]","[('Ft ( XLM ) results', (2, 7)), ('unlabeled data', (16, 18)), ('target domain', (20, 22)), ('substantial gap', (27, 29)), ('model performance', (31, 33)), ('cross -lingual settings', (35, 38)), ('monolingual baselines', (40, 42)), ('state - of - the - art pre-trained cross -lingual representations', (46, 57))]","[['Ft ( XLM ) results', 'without the help of', 'unlabeled data'], ['unlabeled data', 'from', 'target domain'], ['substantial gap', 'between', 'model performance'], ['substantial gap', 'between', 'monolingual baselines'], ['model performance', 'of', 'cross -lingual settings'], ['substantial gap', 'when using', 'state - of - the - art pre-trained cross -lingual representations']]",[],"[['Results', 'Looking at', 'Ft ( XLM ) results']]",[],document_classification,1,171
626,results,"In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .","[('In', (0, 1)), ('where', (6, 7)), ('is', (11, 12)), ('consistently provides', (23, 25)), ('compared with', (27, 29))]","[('sentiment classification task', (2, 5)), ('unlabeled data size', (8, 11)), ('larger', (12, 13)), ('Ft ( XLM ft ) model usnig MLM pre-training', (14, 23)), ('larger improvements', (25, 27)), ('UDA method', (30, 32))]","[['sentiment classification task', 'where', 'unlabeled data size'], ['unlabeled data size', 'is', 'larger'], ['Ft ( XLM ft ) model usnig MLM pre-training', 'consistently provides', 'larger improvements'], ['larger improvements', 'compared with', 'UDA method']]","[['sentiment classification task', 'has', 'unlabeled data size'], ['unlabeled data size', 'has', 'larger']]","[['Results', 'In', 'sentiment classification task']]",[],document_classification,1,173
627,results,The combination of both methods - as in the UDA ( XLM ft ) model - consistently outperforms either method alone .,"[('combination of', (1, 3)), ('as in', (6, 8)), ('consistently outperforms', (16, 18))]","[('UDA ( XLM ft ) model', (9, 15)), ('either method alone', (18, 21))]",[],[],[],[],document_classification,1,176
628,results,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .","[('In', (0, 1)), ('observe', (7, 8)), ('over', (15, 16))]","[('sentiment classification task', (2, 5)), ('self - training technique', (9, 13)), ('consistently improves', (13, 15)), ('teacher model', (17, 19))]","[['sentiment classification task', 'observe', 'self - training technique'], ['consistently improves', 'over', 'teacher model']]","[['sentiment classification task', 'has', 'self - training technique'], ['self - training technique', 'has', 'consistently improves']]","[['Results', 'In', 'sentiment classification task']]",[],document_classification,1,178
629,results,It offers best results in both XLM and XLM ft based classifiers .,"[('offers', (1, 2)), ('in', (4, 5))]","[('best results', (2, 4)), ('both XLM and XLM ft based classifiers', (5, 12))]","[['best results', 'in', 'both XLM and XLM ft based classifiers']]",[],[],[],document_classification,1,179
630,results,"In the MLdoc dataset , self - training also achieves the best results over all , however the gains are less clear .","[('In', (0, 1)), ('achieves', (9, 10))]","[('MLdoc dataset', (2, 4)), ('self - training', (5, 8)), ('best results', (11, 13))]","[['self - training', 'achieves', 'best results']]","[['MLdoc dataset', 'has', 'self - training']]","[['Results', 'In', 'MLdoc dataset']]",[],document_classification,1,181
631,results,"Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .","[('comparing with', (2, 4)), ('able to', (19, 21)), ('by utilizing', (26, 28)), ('in', (30, 31))]","[('best cross - lingual results and monolingual fine - tune baseline', (5, 16)), ('completely close', (21, 23)), ('performance gap', (24, 26)), ('unlabeled data', (28, 30)), ('target language', (32, 34))]","[['best cross - lingual results and monolingual fine - tune baseline', 'able to', 'completely close'], ['performance gap', 'by utilizing', 'unlabeled data'], ['unlabeled data', 'in', 'target language']]","[['best cross - lingual results and monolingual fine - tune baseline', 'has', 'completely close'], ['completely close', 'has', 'performance gap']]","[['Results', 'comparing with', 'best cross - lingual results and monolingual fine - tune baseline']]",[],document_classification,1,183
632,results,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .","[('reaches', (4, 5)), ('improving over', (15, 17)), ('by', (20, 21))]","[('our framework', (2, 4)), ('new state - of - the - art results', (5, 14)), ('vanilla XLM baselines', (17, 20)), ('44 %', (21, 23))]","[['our framework', 'reaches', 'new state - of - the - art results'], ['our framework', 'improving over', 'vanilla XLM baselines'], ['new state - of - the - art results', 'improving over', 'vanilla XLM baselines'], ['vanilla XLM baselines', 'by', '44 %']]",[],[],"[['Results', 'has', 'our framework']]",document_classification,1,184
633,results,The experment results show that it lags behind the ones using unlabeled data from the target domain .,"[('show', (3, 4)), ('lags behind', (6, 8)), ('from', (13, 14))]","[('experment results', (1, 3)), ('ones using unlabeled data', (9, 13)), ('target domain', (15, 17))]","[['experment results', 'lags behind', 'ones using unlabeled data'], ['ones using unlabeled data', 'from', 'target domain']]",[],[],"[['Results', 'has', 'experment results']]",document_classification,1,186
634,results,"In contrast , the performance of the model improves consistently with more labeled data in the monolingual setting .","[('of', (5, 6)), ('with', (10, 11)), ('in', (14, 15))]","[('performance', (4, 5)), ('model', (7, 8)), ('improves', (8, 9)), ('consistently', (9, 10)), ('more labeled data', (11, 14)), ('monolingual setting', (16, 18))]","[['performance', 'of', 'model'], ['improves', 'with', 'more labeled data'], ['consistently', 'with', 'more labeled data'], ['more labeled data', 'in', 'monolingual setting']]","[['performance', 'has', 'model'], ['model', 'has', 'improves'], ['improves', 'has', 'consistently']]",[],[],document_classification,1,194
635,ablation-analysis,"From the results , we conclude that t2t is the best performing approach , as it 's the best matched to the target domain .","[('conclude that', (5, 7)), ('is', (8, 9))]","[('t2t', (7, 8)), ('best performing approach', (10, 13)), ('target domain', (22, 24))]","[['t2t', 'is', 'best performing approach']]","[['t2t', 'has', 'best performing approach']]","[['Ablation analysis', 'conclude that', 't2t']]",[],document_classification,1,213
636,research-problem,Neural Attentive Bag - of - Entities Model for Text Classification,[],[],[],[],[],[],document_classification,10,2
637,research-problem,"As a result , our model achieved state - of - the - art results on all datasets .","[('achieved', (6, 7)), ('on', (15, 16))]","[('our model', (4, 6)), ('state - of - the - art results', (7, 15)), ('all datasets', (16, 18))]","[['our model', 'achieved', 'state - of - the - art results'], ['state - of - the - art results', 'on', 'all datasets']]",[],[],[],document_classification,10,8
638,model,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .","[('proposes', (2, 3)), ('addresses', (21, 22)), ('by modeling', (26, 28)), ('in', (30, 31)), ('using', (34, 35))]","[('Neural Attentive Bagof - Entities ( NABoE ) model', (4, 13)), ('text classification problem', (23, 26)), ('semantics', (29, 30)), ('target documents', (32, 34)), ('entities in the', (35, 38)), ('KB', (38, 39))]","[['text classification problem', 'by modeling', 'semantics'], ['semantics', 'in', 'target documents']]","[['entities in the', 'has', 'KB']]","[['Model', 'proposes', 'Neural Attentive Bagof - Entities ( NABoE ) model']]",[],document_classification,10,21
639,model,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,"[('computed using', (3, 5)), ('that enables', (10, 12)), ('to focus on', (14, 17)), ('of', (20, 21)), ('that are', (23, 25)), ('in', (27, 28))]","[('weights', (1, 2)), ('novel neural attention mechanism', (6, 10)), ('model', (13, 14)), ('small subset', (18, 20)), ('entities', (22, 23)), ('less ambiguous', (25, 27)), ('meaning', (28, 29)), ('document', (34, 35))]","[['weights', 'computed using', 'novel neural attention mechanism'], ['novel neural attention mechanism', 'that enables', 'model'], ['model', 'to focus on', 'small subset'], ['small subset', 'of', 'entities'], ['entities', 'that are', 'less ambiguous'], ['less ambiguous', 'in', 'meaning']]",[],[],"[['Model', 'has', 'weights']]",document_classification,10,23
640,model,"Furthermore , the attention mechanism improves the interpretability of the model because it enables us to inspect the small number of entities that strongly affect the classification decisions .","[('improves', (5, 6)), ('of', (8, 9))]","[('attention mechanism', (3, 5)), ('interpretability', (7, 8)), ('model', (10, 11))]","[['attention mechanism', 'improves', 'interpretability'], ['interpretability', 'of', 'model']]",[],[],"[['Model', 'has', 'attention mechanism']]",document_classification,10,25
641,code,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,[],[],[],[],[],[],document_classification,10,28
642,hyperparameters,We initialized the embeddings of words ( v w ) and entities ( v e ) using pretrained embeddings trained on KB .,"[('initialized', (1, 2)), ('of', (4, 5)), ('using', (16, 17)), ('trained on', (19, 21))]","[('embeddings', (3, 4)), ('words ( v w ) and entities ( v e )', (5, 16)), ('pretrained embeddings', (17, 19)), ('KB', (21, 22))]","[['embeddings', 'of', 'words ( v w ) and entities ( v e )'], ['words ( v w ) and entities ( v e )', 'using', 'pretrained embeddings'], ['pretrained embeddings', 'trained on', 'KB']]",[],"[['Hyperparameters', 'initialized', 'embeddings']]",[],document_classification,10,64
643,baselines,FTS- BRNN,[],[],[],[],[],[],document_classification,10,90
644,baselines,It uses the logistic regression classifier with the features derived by the RNN .,"[('uses', (1, 2)), ('with', (6, 7)), ('derived by', (9, 11))]","[('logistic regression classifier', (3, 6)), ('features', (8, 9)), ('RNN', (12, 13))]","[['logistic regression classifier', 'with', 'features'], ['features', 'derived by', 'RNN']]",[],[],[],document_classification,10,92
645,baselines,"Similar to our previous experiment , we also add SWEM - concat , and the variants of our NABoEentity and NABoE - full models based on Wikifier and TAGME ( see Section 4.2 ) .","[('add', (8, 9)), ('of', (16, 17)), ('based on', (24, 26))]","[('SWEM - concat', (9, 12)), ('variants', (15, 16)), ('our NABoEentity and NABoE - full models', (17, 24)), ('Wikifier and TAGME', (26, 29))]","[['variants', 'of', 'our NABoEentity and NABoE - full models'], ['our NABoEentity and NABoE - full models', 'based on', 'Wikifier and TAGME']]",[],[],[],document_classification,10,94
646,results,"Overall , our models achieved enhanced performance on this task .","[('achieved', (4, 5))]","[('our models', (2, 4)), ('enhanced performance', (5, 7))]","[['our models', 'achieved', 'enhanced performance']]",[],[],"[['Results', 'has', 'our models']]",document_classification,10,97
647,results,"In particular , the NABoE - full model successfully outperformed all the baseline models , and the NABoE-entity model achieved competitive performance and outperformed all the baseline models in the literature category .","[('successfully', (8, 9)), ('achieved', (19, 20)), ('in', (28, 29))]","[('NABoE - full model', (4, 8)), ('all the baseline models', (10, 14)), ('NABoE-entity model', (17, 19)), ('competitive performance', (20, 22)), ('outperformed', (23, 24)), ('all the baseline models', (24, 28)), ('literature category', (30, 32))]","[['NABoE-entity model', 'achieved', 'competitive performance'], ['NABoE-entity model', 'achieved', 'outperformed'], ['outperformed', 'in', 'literature category'], ['all the baseline models', 'in', 'literature category']]","[['outperformed', 'has', 'all the baseline models']]",[],"[['Results', 'has', 'NABoE - full model']]",document_classification,10,98
648,results,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .","[('yielded', (7, 8)), ('on', (12, 13))]","[('our models', (5, 7)), ('enhanced over all performance', (8, 12)), ('both datasets', (13, 15))]","[['our models', 'yielded', 'enhanced over all performance'], ['enhanced over all performance', 'on', 'both datasets']]",[],[],"[['Results', 'has', 'our models']]",document_classification,10,101
649,results,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,"[('in terms of', (9, 12))]","[('NABoE - full model', (1, 5)), ('outperformed', (5, 6)), ('all baseline models', (6, 9)), ('both measures', (12, 14))]","[['outperformed', 'in terms of', 'both measures'], ['all baseline models', 'in terms of', 'both measures']]","[['NABoE - full model', 'has', 'outperformed'], ['outperformed', 'has', 'all baseline models']]",[],"[['Results', 'has', 'NABoE - full model']]",document_classification,10,102
650,results,"Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .","[('in terms of', (10, 13)), ('on', (15, 16)), ('on', (25, 26))]","[('NABoE-entity model', (3, 5)), ('outperformed', (5, 6)), ('all the baseline models', (6, 10)), ('both measures', (13, 15)), ('20NG dataset', (17, 19)), ('F 1 score', (22, 25)), ('R8 dataset', (27, 29))]","[['all the baseline models', 'in terms of', 'both measures'], ['both measures', 'on', '20NG dataset'], ['F 1 score', 'on', 'R8 dataset']]","[['NABoE-entity model', 'has', 'outperformed'], ['outperformed', 'has', 'all the baseline models']]",[],[],document_classification,10,103
651,results,"Moreover , our attention mechanism consistently improved the performance .","[('consistently', (5, 6))]","[('our attention mechanism', (2, 5)), ('performance', (8, 9))]",[],[],[],"[['Results', 'has', 'our attention mechanism']]",document_classification,10,104
652,results,"Further , the models based on the dictionarybased entity detection ( see Section 2.1 ) generally outperformed the models based on the entity linking systems ( i.e. , Wikifier and TAGME ) .","[('based on', (4, 6)), ('generally outperformed', (15, 17)), ('based on', (19, 21)), ('i.e.', (26, 27))]","[('dictionarybased entity detection', (7, 10)), ('models', (18, 19)), ('entity linking systems', (22, 25)), ('Wikifier', (28, 29)), ('TAGME', (30, 31))]","[['dictionarybased entity detection', 'generally outperformed', 'models'], ['models', 'based on', 'entity linking systems'], ['entity linking systems', 'i.e.', 'Wikifier'], ['entity linking systems', 'i.e.', 'TAGME']]",[],"[['Results', 'based on', 'dictionarybased entity detection']]",[],document_classification,10,107
653,results,"Moreover , our attention mechanism consistently improved the performance for Wikifierand TAGME - based models because the attention mechanism enabled the model to focus on entities that were relevant to the document .","[('consistently improved', (5, 7)), ('for', (9, 10))]","[('our attention mechanism', (2, 5)), ('performance', (8, 9)), ('Wikifierand TAGME - based models', (10, 15))]","[['our attention mechanism', 'consistently improved', 'performance'], ['performance', 'for', 'Wikifierand TAGME - based models']]",[],[],"[['Results', 'has', 'our attention mechanism']]",document_classification,10,109
654,experiments,Factoid Question Answering,[],[],[],[],[],[],document_classification,10,131
655,results,"Furthermore , similar to the previous text classification experiment , the attention mechanism and the pretrained embeddings consistently improved the performance .",[],"[('attention mechanism and the pretrained embeddings', (11, 17)), ('consistently improved', (17, 19)), ('performance', (20, 21))]",[],"[['attention mechanism and the pretrained embeddings', 'has', 'consistently improved'], ['consistently improved', 'has', 'performance']]",[],[],document_classification,10,139
656,results,"Moreover , the models based on dictionary - based entity detection outperformed the models based on the entity linking systems .","[('based on', (14, 16))]","[('models based on', (3, 6)), ('dictionary - based entity detection', (6, 11)), ('outperformed', (11, 12)), ('models', (13, 14)), ('entity linking systems', (17, 20))]","[['models', 'based on', 'entity linking systems']]","[['models based on', 'has', 'dictionary - based entity detection'], ['dictionary - based entity detection', 'has', 'outperformed'], ['outperformed', 'has', 'models']]",[],[],document_classification,10,140
657,research-problem,Task - oriented Word Embedding for Text Classification,[],[],[],[],[],[],document_classification,11,2
658,research-problem,Distributed word representation plays a pivotal role in various natural language processing tasks .,[],"[('Distributed word representation', (0, 3))]",[],[],[],[],document_classification,11,4
659,research-problem,AI : a combination of active learning and self learning for named entity recognition on twitter using conditional random fields learning :,"[('combination of', (3, 5)), ('for', (10, 11)), ('using', (16, 17))]","[('AI', (0, 1)), ('active learning and self learning', (5, 10)), ('named entity recognition on twitter', (11, 16)), ('conditional random fields learning', (17, 21))]","[['AI', 'combination of', 'active learning and self learning'], ['active learning and self learning', 'for', 'named entity recognition on twitter'], ['named entity recognition on twitter', 'using', 'conditional random fields learning']]","[['AI', 'name', 'active learning and self learning']]",[],[],document_classification,11,15
660,research-problem,Learning word representation is a fundamental step in various natural language processing tasks .,[],"[('Learning word representation', (0, 3))]",[],[],[],[],document_classification,11,18
661,model,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .","[('propose', (5, 6)), ('denoted as', (14, 16)), ('to solve', (18, 20))]","[('task - oriented word embedding method', (7, 13)), ('ToWE', (16, 17))]","[['task - oriented word embedding method', 'denoted as', 'ToWE']]","[['task - oriented word embedding method', 'name', 'ToWE']]","[['Model', 'propose', 'task - oriented word embedding method']]",[],document_classification,11,30
662,model,"Specifically , we focus on text classification .","[('focus on', (3, 5))]","[('text classification', (5, 7))]",[],[],"[['Model', 'focus on', 'text classification']]",[],document_classification,11,37
663,model,"In the joint learning framework , the contextual information is captured following the context prediction task introduced by .","[('In', (0, 1)), ('captured following', (10, 12))]","[('joint learning framework', (2, 5)), ('contextual information', (7, 9)), ('context prediction task', (13, 16))]","[['contextual information', 'captured following', 'context prediction task']]","[['joint learning framework', 'has', 'contextual information']]","[['Model', 'In', 'joint learning framework']]",[],document_classification,11,39
664,model,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .","[('regularize', (7, 8)), ('of', (10, 11)), ('to have', (14, 16)), ('adjust', (23, 24)), ('of', (26, 27)), ('in', (30, 31))]","[('task information', (3, 5)), ('distribution', (9, 10)), ('clear classification boundary', (17, 20)), ('distribution', (25, 26)), ('other words', (28, 30)), ('embedding space', (32, 34))]","[['task information', 'regularize', 'distribution'], ['task information', 'adjust', 'distribution'], ['distribution', 'of', 'other words'], ['other words', 'in', 'embedding space']]",[],[],[],document_classification,11,40
665,model,We propose a task - oriented word embedding method that is specially designed for text classification .,"[('propose', (1, 2)), ('specially designed for', (11, 14))]","[('task - oriented word embedding method', (3, 9)), ('text classification', (14, 16))]","[['task - oriented word embedding method', 'specially designed for', 'text classification']]",[],"[['Model', 'propose', 'task - oriented word embedding method']]",[],document_classification,11,46
666,model,It introduces the function - aware component and highlights word 's functional attributes in the embedding space by regularizing the distribution of words to have a clear classification boundary .,"[('introduces', (1, 2)), ('highlights', (8, 9)), ('in', (13, 14)), ('by regularizing', (17, 19)), ('to have', (23, 25))]","[('function - aware component', (3, 7)), (""word 's functional attributes"", (9, 13)), ('embedding space', (15, 17)), ('distribution of words', (20, 23)), ('clear classification boundary', (26, 29))]","[[""word 's functional attributes"", 'in', 'embedding space'], [""word 's functional attributes"", 'by regularizing', 'distribution of words'], ['embedding space', 'by regularizing', 'distribution of words'], ['distribution of words', 'to have', 'clear classification boundary']]",[],"[['Model', 'introduces', 'function - aware component']]",[],document_classification,11,47
667,baselines,It represents each document as a bag of words and the weighting scheme is TFIDF .,"[('represents', (1, 2)), ('as', (4, 5)), ('is', (13, 14))]","[('each document', (2, 4)), ('bag of words', (6, 9)), ('weighting scheme', (11, 13)), ('TFIDF', (14, 15))]","[['each document', 'as', 'bag of words'], ['weighting scheme', 'is', 'TFIDF']]","[['weighting scheme', 'has', 'TFIDF']]",[],[],document_classification,11,147
668,baselines,"It comprises two models , i.e. , CBOW which predicts the target word using context information , and the Skip - gram ( denoted as SG ) which predicts each context word using the target word ; ( 3 ) the Glo Ve method is a state - of - the - art matrix factorization method .","[('comprises', (1, 2)), ('predicts', (9, 10)), ('using', (13, 14)), ('predicts', (28, 29)), ('using', (32, 33)), ('is', (44, 45))]","[('two', (2, 3)), ('CBOW', (7, 8)), ('target word', (11, 13)), ('context information', (14, 16)), ('Skip - gram', (19, 22)), ('each context word', (29, 32)), ('target word', (34, 36)), ('Glo Ve method', (41, 44)), ('state - of - the - art matrix factorization method', (46, 56))]","[['CBOW', 'predicts', 'target word'], ['target word', 'using', 'context information'], ['Skip - gram', 'predicts', 'each context word'], ['each context word', 'using', 'target word'], ['Glo Ve method', 'is', 'state - of - the - art matrix factorization method']]","[['two', 'name', 'CBOW'], ['Glo Ve method', 'has', 'state - of - the - art matrix factorization method']]",[],[],document_classification,11,150
669,ablation-analysis,"In this paper , we use the text classification task to evaluate the performance of word embeddings .","[('use', (5, 6)), ('to evaluate', (10, 12)), ('of', (14, 15))]","[('text classification task', (7, 10)), ('performance', (13, 14)), ('word embeddings', (15, 17))]","[['text classification task', 'to evaluate', 'performance'], ['performance', 'of', 'word embeddings']]",[],"[['Ablation analysis', 'use', 'text classification task']]",[],document_classification,11,153
670,experimental-setup,"We regard document embedding as a document feature and trained a linear classifier using Liblinear 7 , since the feature size is large , and Liblinear can quickly train a linear classifier with high dimension features .","[('regard', (1, 2)), ('as', (4, 5)), ('trained', (9, 10)), ('using', (13, 14))]","[('document embedding', (2, 4)), ('document feature', (6, 8)), ('linear classifier', (11, 13)), ('Liblinear', (14, 15))]","[['document embedding', 'as', 'document feature'], ['document embedding', 'trained', 'linear classifier'], ['linear classifier', 'using', 'Liblinear']]",[],"[['Experimental setup', 'regard', 'document embedding']]",[],document_classification,11,155
671,experimental-setup,"We tokenized the corpus with the Stanford Tokenizer 8 and converted it to lowercase , then removed the stop words .","[('tokenized', (1, 2)), ('with', (4, 5)), ('converted it to', (10, 13)), ('removed', (16, 17))]","[('corpus', (3, 4)), ('Stanford Tokenizer', (6, 8)), ('lowercase', (13, 14)), ('stop words', (18, 20))]","[['corpus', 'with', 'Stanford Tokenizer'], ['lowercase', 'removed', 'stop words']]",[],"[['Experimental setup', 'tokenized', 'corpus']]",[],document_classification,11,160
672,experimental-setup,"For a fair comparison , all word embeddings adhere to the following settings : the dimensionality of vectors is 300 , the size of the context window is 5 , the number of negative samples is 25 .","[('adhere', (8, 9)), ('of', (16, 17)), ('is', (18, 19)), ('is', (27, 28)), ('is', (35, 36))]","[('all word embeddings', (5, 8)), ('dimensionality', (15, 16)), ('vectors', (17, 18)), ('300', (19, 20)), ('size', (22, 23)), ('context window', (25, 27)), ('5', (28, 29)), ('number of negative samples', (31, 35)), ('25', (36, 37))]","[['dimensionality', 'of', 'vectors'], ['size', 'of', 'context window'], ['vectors', 'is', '300'], ['context window', 'is', '5'], ['number of negative samples', 'is', '25']]","[['all word embeddings', 'has', 'dimensionality'], ['number of negative samples', 'has', '25']]",[],"[['Experimental setup', 'has', 'all word embeddings']]",document_classification,11,161
673,experiments,Group dataset .,[],"[('Group dataset', (0, 2))]",[],[],[],[],document_classification,11,164
674,experiments,Group dataset .,[],"[('Group dataset', (0, 2))]",[],[],[],[],document_classification,11,168
675,experimental-setup,The recommended N is 150 with the constraint that the total size of S is under 1200 based on practical experience .,"[('is', (3, 4)), ('with', (5, 6))]","[('recommended N', (1, 3)), ('150', (4, 5)), ('constraint', (7, 8)), ('total', (10, 11))]","[['recommended N', 'is', '150'], ['150', 'with', 'constraint']]","[['recommended N', 'has', '150'], ['150', 'has', 'constraint'], ['constraint', 'has', 'total']]",[],"[['Experimental setup', 'has', 'recommended N']]",document_classification,11,170
676,experimental-setup,"were tuned from 0 to 1 , with a step size of 0.1 .","[('tuned from', (1, 3)), ('with', (7, 8)), ('of', (11, 12))]","[('0 to 1', (3, 6)), ('step size', (9, 11)), ('0.1', (12, 13))]","[['0 to 1', 'with', 'step size'], ['step size', 'of', '0.1']]",[],"[['Experimental setup', 'tuned from', '0 to 1']]",[],document_classification,11,175
677,results,"The proposed method based on Skip - gram and CBOW reaches optimal performance when ? = 0.4 and ? = 0.3 , respectively .","[('based on', (3, 5)), ('reaches', (10, 11)), ('when', (13, 14))]","[('Skip - gram and CBOW', (5, 10)), ('optimal performance', (11, 13)), ('? = 0.4 and ? = 0.3', (14, 21))]","[['Skip - gram and CBOW', 'reaches', 'optimal performance'], ['optimal performance', 'when', '? = 0.4 and ? = 0.3']]",[],[],[],document_classification,11,176
678,results,"( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .","[('performs', (5, 6)), ('than', (7, 8)), ('proved to', (14, 16)), ('for', (19, 20))]","[('Our method', (3, 5)), ('better', (6, 7)), ('other methods', (9, 11)), ('highly reliable', (17, 19)), ('text classification task', (21, 24))]","[['Our method', 'performs', 'better'], ['better', 'than', 'other methods'], ['Our method', 'proved to', 'highly reliable'], ['highly reliable', 'for', 'text classification task']]",[],[],"[['Results', 'has', 'Our method']]",document_classification,11,183
679,results,"In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .","[('on', (13, 14))]","[('ToWE - SG method', (4, 8)), ('significantly outperforms', (8, 10)), ('other baselines', (11, 13)), ('20 New s Group', (15, 19)), ('5 Abstract s Group', (20, 24))]","[['significantly outperforms', 'on', '20 New s Group'], ['significantly outperforms', 'on', '5 Abstract s Group'], ['other baselines', 'on', '20 New s Group'], ['other baselines', 'on', '5 Abstract s Group']]","[['ToWE - SG method', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'other baselines']]",[],[],document_classification,11,184
680,results,"( 2 ) The word embedding methods outperform the basic bag - of - words methods in most cases , indicating the superiority of distributed word representation over the one - hot representation .","[('indicating', (20, 21))]","[('word embedding methods', (4, 7)), ('outperform', (7, 8)), ('basic bag - of - words methods', (9, 16))]",[],"[['word embedding methods', 'has', 'outperform'], ['outperform', 'has', 'basic bag - of - words methods']]",[],"[['Results', 'has', 'word embedding methods']]",document_classification,11,186
681,baselines,( 3 ) The Retrofit method is the knowledge - base enhanced word embedding method .,"[('is', (6, 7))]","[('Retrofit method', (4, 6)), ('knowledge - base enhanced word embedding method', (8, 15))]","[['Retrofit method', 'is', 'knowledge - base enhanced word embedding method']]","[['Retrofit method', 'has', 'knowledge - base enhanced word embedding method']]",[],"[['Baselines', 'has', 'Retrofit method']]",document_classification,11,189
682,results,"Our method achieves better performance over Retrofit method , indicating that the task - specific features could be more effective compared with general semantic relations constructed by humans in the knowledge bases .","[('achieves', (2, 3)), ('over', (5, 6))]","[('Our method', (0, 2)), ('better performance', (3, 5)), ('Retrofit method', (6, 8))]","[['Our method', 'achieves', 'better performance'], ['better performance', 'over', 'Retrofit method']]","[['Our method', 'has', 'better performance']]",[],"[['Results', 'has', 'Our method']]",document_classification,11,190
683,results,"( 4 ) In sentence classification , such as the MR and SST datasets , it is obvious that TWE achieves a relatively lower performance .","[('obvious', (17, 18)), ('achieves', (20, 21))]","[('TWE', (19, 20)), ('relatively lower performance', (22, 25))]","[['TWE', 'achieves', 'relatively lower performance']]",[],[],[],document_classification,11,191
684,results,"Our method outperforms the TWE method on both the document - level and sentence - level tasks , which shows the stability and reliability of modeling taskspecific features in real - world applications .","[('on', (6, 7))]","[('outperforms', (2, 3)), ('TWE method', (4, 6)), ('document - level and sentence - level tasks', (9, 17))]","[['TWE method', 'on', 'document - level and sentence - level tasks']]","[['outperforms', 'has', 'TWE method']]",[],"[['Results', 'has', 'outperforms']]",document_classification,11,193
685,research-problem,Graph Convolutional Networks for Text Classification,[],[],[],[],[],[],document_classification,12,2
686,research-problem,Text classification is an important and classical problem in natural language processing .,[],"[('Text classification', (0, 2))]",[],[],[],[],document_classification,12,4
687,research-problem,Text classification is a fundamental problem in natural language processing ( NLP ) .,[],"[('Text classification', (0, 2))]",[],[],[],[],document_classification,12,14
688,model,"In this work , we propose a new graph neural networkbased method for text classification .","[('propose', (5, 6)), ('for', (12, 13))]","[('new graph neural networkbased method', (7, 12)), ('text classification', (13, 15))]","[['new graph neural networkbased method', 'for', 'text classification']]",[],"[['Model', 'propose', 'new graph neural networkbased method']]",[],document_classification,12,22
689,model,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .","[('model', (1, 2)), ('with', (4, 5)), ('captures', (21, 22))]","[('Graph Convolutional Network ( GCN )', (6, 12)), ('high order neighborhoods information', (22, 26))]",[],[],"[['Model', 'model', 'Graph Convolutional Network ( GCN )']]",[],document_classification,12,24
690,model,We then turn text classification problem into anode classification problem .,"[('turn', (2, 3)), ('into', (6, 7))]","[('text classification problem', (3, 6)), ('anode classification problem', (7, 10))]","[['text classification problem', 'into', 'anode classification problem']]",[],"[['Model', 'turn', 'text classification problem']]",[],document_classification,12,26
691,code,Our source code is available at https://github. com/yao8839836/text_gcn .,[],"[('https://github. com/yao8839836/text_gcn', (6, 8))]",[],[],[],[],document_classification,12,28
692,model,We propose a novel graph neural network method for text classification .,[],"[('text classification', (9, 11))]",[],[],[],[],document_classification,12,30
693,research-problem,Deep Learning for Text Classification,[],[],[],[],[],[],document_classification,12,40
694,experiments,Can our model learn predictive word and document embeddings ?,[],"[('Can our model learn', (0, 4))]",[],[],[],[],document_classification,12,113
695,baselines,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,"[('with', (12, 13))]","[('TF - IDF + LR', (0, 5)), ('bag - of - words model', (6, 12)), ('term frequencyinverse document frequency weighting', (13, 18))]","[['bag - of - words model', 'with', 'term frequencyinverse document frequency weighting']]","[['TF - IDF + LR', 'has', 'bag - of - words model']]",[],"[['Baselines', 'has', 'TF - IDF + LR']]",document_classification,12,116
696,baselines,Logistic Regression is used as the classifier .,"[('used as', (3, 5))]","[('Logistic Regression', (0, 2)), ('classifier', (6, 7))]","[['Logistic Regression', 'used as', 'classifier']]",[],[],"[['Baselines', 'has', 'Logistic Regression']]",document_classification,12,117
697,baselines,CNN : Convolutional Neural Network ( Kim 2014 ) .,[],"[('CNN', (0, 1)), ('Convolutional Neural Network', (2, 5))]",[],"[['CNN', 'has', 'Convolutional Neural Network']]",[],"[['Baselines', 'has', 'CNN']]",document_classification,12,118
698,baselines,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,"[('uses', (8, 9)), ('as', (13, 14)), ('of', (16, 17))]","[('LSTM', (0, 1)), ('last hidden state', (10, 13)), ('representation', (15, 16)), ('whole text', (18, 20))]","[['last hidden state', 'as', 'representation'], ['representation', 'of', 'whole text']]",[],[],"[['Baselines', 'has', 'LSTM']]",document_classification,12,120
699,baselines,"Bi- LSTM : a bi-directional LSTM , commonly used in text classification .","[('commonly used', (7, 9))]","[('Bi- LSTM', (0, 2)), ('text classification', (10, 12))]",[],[],[],"[['Baselines', 'has', 'Bi- LSTM']]",document_classification,12,122
700,baselines,We input pre-trained word embeddings to Bi - LSTM .,"[('input', (1, 2)), ('to', (5, 6))]","[('pre-trained word embeddings', (2, 5)), ('Bi - LSTM', (6, 9))]","[['pre-trained word embeddings', 'to', 'Bi - LSTM']]",[],[],[],document_classification,12,123
701,baselines,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .",[],"[('PV - DBOW', (0, 3)), ('paragraph vector model', (5, 8)), ('orders of words in text', (12, 17)), ('ignored', (18, 19))]",[],"[['PV - DBOW', 'has', 'paragraph vector model'], ['paragraph vector model', 'has', 'orders of words in text'], ['orders of words in text', 'has', 'ignored']]",[],"[['Baselines', 'has', 'PV - DBOW']]",document_classification,12,124
702,baselines,We used Logistic Regression as the classifier .,"[('used', (1, 2)), ('as', (4, 5))]","[('Logistic Regression', (2, 4)), ('classifier', (6, 7))]","[['Logistic Regression', 'as', 'classifier']]",[],"[['Baselines', 'used', 'Logistic Regression']]",[],document_classification,12,125
703,baselines,"PV - DM : a paragraph vector model proposed by , which considers the word order .","[('considers', (12, 13))]","[('PV - DM', (0, 3)), ('paragraph vector model', (5, 8)), ('word order', (14, 16))]","[['paragraph vector model', 'considers', 'word order']]","[['PV - DM', 'has', 'paragraph vector model']]",[],"[['Baselines', 'has', 'PV - DM']]",document_classification,12,126
704,baselines,We used Logistic Regression as the classifier .,"[('used', (1, 2)), ('as', (4, 5))]","[('Logistic Regression', (2, 4)), ('classifier', (6, 7))]","[['Logistic Regression', 'as', 'classifier']]",[],"[['Baselines', 'used', 'Logistic Regression']]",[],document_classification,12,127
705,baselines,"PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .","[('firstly learns', (7, 9)), ('based on', (11, 13)), ('containing', (16, 17)), ('averages', (26, 27)), ('as', (29, 30)), ('for', (32, 33))]","[('PTE', (0, 1)), ('predictive text embedding', (2, 5)), ('word embedding', (9, 11)), ('heterogeneous text network', (13, 16)), ('words , documents and labels as', (17, 23)), ('nodes', (23, 24)), ('word embeddings', (27, 29)), ('document embeddings', (30, 32)), ('text classification', (33, 35))]","[['predictive text embedding', 'firstly learns', 'word embedding'], ['word embedding', 'based on', 'heterogeneous text network'], ['heterogeneous text network', 'containing', 'words , documents and labels as'], ['predictive text embedding', 'averages', 'word embeddings'], ['word embeddings', 'as', 'document embeddings'], ['document embeddings', 'for', 'text classification']]","[['PTE', 'has', 'predictive text embedding'], ['words , documents and labels as', 'has', 'nodes']]",[],"[['Baselines', 'has', 'PTE']]",document_classification,12,128
706,baselines,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .","[('treats', (12, 13)), ('as', (21, 22)), ('feeds', (26, 27)), ('into', (29, 30))]","[('fast Text', (0, 2)), ('text classification', (7, 9)), ('average of word / n- grams embeddings', (14, 21)), ('document embeddings', (22, 24)), ('document embeddings', (27, 29)), ('linear classifier', (31, 33))]","[['text classification', 'treats', 'average of word / n- grams embeddings'], ['average of word / n- grams embeddings', 'as', 'document embeddings'], ['document embeddings', 'into', 'linear classifier']]","[['fast Text', 'has', 'text classification']]",[],"[['Baselines', 'has', 'fast Text']]",document_classification,12,129
707,baselines,"SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .","[('employs', (8, 9)), ('operated over', (12, 14))]","[('SWEM', (0, 1)), ('simple word embedding models', (2, 6)), ('simple pooling strategies', (9, 12)), ('word embeddings', (14, 16))]","[['SWEM', 'employs', 'simple pooling strategies'], ['simple word embedding models', 'employs', 'simple pooling strategies'], ['simple pooling strategies', 'operated over', 'word embeddings']]","[['SWEM', 'has', 'simple word embedding models']]",[],"[['Baselines', 'has', 'SWEM']]",document_classification,12,131
708,baselines,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .","[('embeds', (9, 10)), ('in', (14, 15)), ('for', (19, 20))]","[('LEAM', (0, 1)), ('label - embedding attentive models', (2, 7)), ('words and labels', (11, 14)), ('same joint space', (16, 19)), ('text classification', (20, 22))]","[['label - embedding attentive models', 'embeds', 'words and labels'], ['label - embedding attentive models', 'in', 'same joint space'], ['words and labels', 'in', 'same joint space'], ['same joint space', 'for', 'text classification']]","[['LEAM', 'has', 'label - embedding attentive models']]",[],"[['Baselines', 'has', 'LEAM']]",document_classification,12,132
709,baselines,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .","[('operates', (11, 12)), ('over', (13, 14))]","[('Graph - CNN - C', (0, 5)), ('convolutions', (12, 13)), ('word embedding similarity graphs', (14, 18)), ('Chebyshev filter', (30, 32))]","[['convolutions', 'over', 'word embedding similarity graphs']]",[],[],"[['Baselines', 'has', 'Graph - CNN - C']]",document_classification,12,134
710,baselines,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) . ,[],"[('Graph - CNN - S', (0, 5))]",[],[],[],"[['Baselines', 'has', 'Graph - CNN - S']]",document_classification,12,135
711,baselines,Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,"[('using', (15, 16))]","[('Graph - CNN - F', (0, 5)), ('Fourier filter', (16, 18))]","[['Graph - CNN - F', 'using', 'Fourier filter']]",[],[],"[['Baselines', 'has', 'Graph - CNN - F']]",document_classification,12,136
712,hyperparameters,"For Text GCN , we set the embedding size of the first convolution layer as 200 and set the window size as 20 .","[('For', (0, 1)), ('set', (5, 6)), ('of', (9, 10)), ('as', (14, 15)), ('set', (17, 18)), ('as', (21, 22))]","[('Text GCN', (1, 3)), ('embedding size', (7, 9)), ('first convolution layer', (11, 14)), ('200', (15, 16)), ('window size', (19, 21)), ('20', (22, 23))]","[['Text GCN', 'set', 'embedding size'], ['embedding size', 'of', 'first convolution layer'], ['first convolution layer', 'as', '200'], ['embedding size', 'set', 'window size'], ['window size', 'as', '20']]","[['window size', 'has', '20']]","[['Hyperparameters', 'For', 'Text GCN']]",[],document_classification,12,155
713,hyperparameters,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )","[('For', (0, 1)), ('using', (3, 4)), ('used', (9, 10))]","[('pre-trained word embeddings', (4, 7)), ('300 dimensional Glo Ve word embeddings', (10, 16))]","[['pre-trained word embeddings', 'used', '300 dimensional Glo Ve word embeddings']]",[],"[['Hyperparameters', 'For', 'pre-trained word embeddings']]",[],document_classification,12,158
714,results,"Text GCN performs the best and significantly outperforms all baseline models ( p < 0.05 based on student t- test ) on four datasets , which showcases the effectiveness of the proposed method on long text datasets .","[('performs', (2, 3)), ('based on', (15, 17))]","[('Text GCN', (0, 2)), ('best and significantly outperforms', (4, 8)), ('all baseline models ( p < 0.05', (8, 15))]","[['Text GCN', 'performs', 'best and significantly outperforms']]","[['best and significantly outperforms', 'has', 'all baseline models ( p < 0.05']]",[],"[['Results', 'has', 'Text GCN']]",document_classification,12,161
715,results,"For more in - depth performance analysis , we note that TF - IDF + LR performs well on long text datasets like 20 NG and can outperform CNN with randomly initialized word embeddings .","[('note', (9, 10)), ('performs', (16, 17)), ('on', (18, 19)), ('like', (22, 23)), ('with', (29, 30))]","[('TF - IDF + LR', (11, 16)), ('well', (17, 18)), ('long text datasets', (19, 22)), ('20 NG', (23, 25)), ('outperform', (27, 28)), ('CNN', (28, 29)), ('randomly initialized word embeddings', (30, 34))]","[['TF - IDF + LR', 'performs', 'well'], ['well', 'on', 'long text datasets'], ['long text datasets', 'like', '20 NG'], ['CNN', 'with', 'randomly initialized word embeddings']]","[['outperform', 'has', 'CNN']]","[['Results', 'note', 'TF - IDF + LR']]",[],document_classification,12,162
716,results,"When pre-trained Glo Ve word embeddings are provided , CNN performs much better , especially on Ohsumed and 20 NG .","[('When', (0, 1)), ('provided', (7, 8)), ('performs', (10, 11)), ('especially on', (14, 16))]","[('pre-trained Glo Ve word embeddings', (1, 6)), ('CNN', (9, 10)), ('much better', (11, 13)), ('Ohsumed and 20 NG', (16, 20))]","[['pre-trained Glo Ve word embeddings', 'provided', 'CNN'], ['CNN', 'performs', 'much better'], ['much better', 'especially on', 'Ohsumed and 20 NG']]","[['pre-trained Glo Ve word embeddings', 'has', 'CNN']]","[['Results', 'When', 'pre-trained Glo Ve word embeddings']]",[],document_classification,12,163
717,results,"CNN also achieves the best results on short text dataset MR with pre-trained word embeddings , which shows it can 7 http://nlp.stanford.edu/data/glove.6B.zip model consecutive and short - distance semantics well .","[('achieves', (2, 3)), ('on', (6, 7))]","[('CNN', (0, 1)), ('best results', (4, 6)), ('short text dataset MR', (7, 11))]","[['CNN', 'achieves', 'best results'], ['best results', 'on', 'short text dataset MR']]",[],[],"[['Results', 'has', 'CNN']]",document_classification,12,164
718,results,"PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .","[('achieves', (3, 4)), ('to', (6, 7)), ('on', (9, 10))]","[('PV - DBOW', (0, 3)), ('comparable results', (4, 6)), ('strong baselines', (7, 9)), ('20 NG and Ohsumed', (10, 14))]","[['PV - DBOW', 'achieves', 'comparable results'], ['comparable results', 'to', 'strong baselines'], ['strong baselines', 'on', '20 NG and Ohsumed']]","[['PV - DBOW', 'has', 'comparable results']]",[],"[['Results', 'has', 'PV - DBOW']]",document_classification,12,166
719,results,"PV - DM performs worse than PV - DBOW , the only comparable results are on MR , where word orders are more essential .","[('performs', (3, 4)), ('than', (5, 6))]","[('PV - DM', (0, 3)), ('worse', (4, 5)), ('PV - DBOW', (6, 9))]","[['PV - DM', 'performs', 'worse'], ['worse', 'than', 'PV - DBOW']]",[],[],"[['Results', 'has', 'PV - DM']]",document_classification,12,168
720,results,The results of PV - DBOW and PV - DM indicate that unsupervised document embeddings are not very discriminative in text classification .,"[('of', (2, 3)), ('indicate', (10, 11)), ('are', (15, 16)), ('in', (19, 20))]","[('PV - DBOW and PV - DM', (3, 10)), ('unsupervised document embeddings', (12, 15)), ('not very discriminative', (16, 19)), ('text classification', (20, 22))]","[['PV - DBOW and PV - DM', 'indicate', 'unsupervised document embeddings'], ['unsupervised document embeddings', 'are', 'not very discriminative'], ['not very discriminative', 'in', 'text classification']]",[],"[['Results', 'of', 'PV - DBOW and PV - DM']]",[],document_classification,12,169
721,results,PTE and fast Text clearly outperform PV - DBOW and PV - DM because they learn document embeddings in a supervised manner so that label information can be utilized to learn more discriminative embeddings .,"[('clearly', (4, 5))]","[('PTE and fast Text', (0, 4)), ('PV - DBOW and PV - DM', (6, 13))]",[],[],[],"[['Results', 'has', 'PTE and fast Text']]",document_classification,12,170
722,results,"The two recent methods SWEM and LEAM perform quite well , which demonstrates the effectiveness of simple pooling methods and label descriptions / embeddings .","[('perform', (7, 8))]","[('SWEM and LEAM', (4, 7)), ('quite well', (8, 10))]","[['SWEM and LEAM', 'perform', 'quite well']]",[],[],"[['Results', 'has', 'SWEM and LEAM']]",document_classification,12,171
723,results,Graph - CNN models also show competitive performances .,"[('show', (5, 6))]","[('Graph - CNN models', (0, 4)), ('competitive performances', (6, 8))]","[['Graph - CNN models', 'show', 'competitive performances']]","[['Graph - CNN models', 'has', 'competitive performances']]",[],[],document_classification,12,172
724,baselines,"1 ) the text graph can capture both document - word relations and global word - word relations ; 2 ) the GCN model , as a special form of Laplacian smoothing , computes the new features of anode as the weighted average of itself and its second order neighbors .","[('can capture', (5, 7)), ('computes', (33, 34)), ('of', (37, 38)), ('as', (39, 40)), ('of', (43, 44))]","[('text graph', (3, 5)), ('document - word relations', (8, 12)), ('GCN model', (22, 24)), ('Laplacian smoothing', (30, 32)), ('new features', (35, 37)), ('anode', (38, 39)), ('weighted average', (41, 43)), ('itself', (44, 45)), ('second order neighbors', (47, 50))]","[['text graph', 'can capture', 'document - word relations'], ['GCN model', 'computes', 'new features'], ['new features', 'of', 'anode'], ['new features', 'as', 'weighted average'], ['anode', 'as', 'weighted average'], ['anode', 'as', 'second order neighbors'], ['weighted average', 'of', 'itself']]",[],[],"[['Baselines', 'has', 'text graph']]",document_classification,12,175
725,baselines,"The label information of document nodes can be passed to their neighboring word nodes ( words within the documents ) , then relayed to other word nodes and document nodes thatare neighbor to the first step neighboring word nodes .","[('of', (3, 4)), ('passed to', (8, 10)), ('relayed to', (22, 24)), ('thatare neighbor to', (30, 33))]","[('label information', (1, 3)), ('document nodes', (4, 6)), ('neighboring word nodes ( words within', (11, 17)), ('documents )', (18, 20)), ('other word nodes and document nodes', (24, 30)), ('first step neighboring word nodes', (34, 39))]","[['label information', 'of', 'document nodes'], ['document nodes', 'passed to', 'neighboring word nodes ( words within'], ['other word nodes and document nodes', 'thatare neighbor to', 'first step neighboring word nodes']]","[['neighboring word nodes ( words within', 'has', 'documents )']]",[],"[['Baselines', 'has', 'label information']]",document_classification,12,176
726,results,"However , we also observed that Text GCN did not outperform CNN and LSTM - based models on MR .","[('observed', (4, 5)), ('did not', (8, 10)), ('on', (17, 18))]","[('Text GCN', (6, 8)), ('CNN and LSTM - based models', (11, 17)), ('MR', (18, 19))]","[['CNN and LSTM - based models', 'on', 'MR']]",[],"[['Results', 'observed', 'Text GCN']]",[],document_classification,12,178
727,results,We note that Text GCN can achieve higher test accuracy with limited labeled documents .,"[('note', (1, 2)), ('can achieve', (5, 7)), ('with', (10, 11))]","[('Text GCN', (3, 5)), ('higher test accuracy', (7, 10)), ('limited labeled documents', (11, 14))]","[['Text GCN', 'can achieve', 'higher test accuracy'], ['higher test accuracy', 'with', 'limited labeled documents']]",[],"[['Results', 'note', 'Text GCN']]",[],document_classification,12,194
728,research-problem,Deep Pyramid Convolutional Neural Networks for Text Categorization,[],[],[],[],[],[],document_classification,13,2
729,research-problem,This paper proposes a low - complexity word - level deep convolutional neural network ( CNN ) architecture for text categorization that can efficiently represent longrange associations in text .,"[('proposes', (2, 3)), ('for', (18, 19)), ('efficiently represent', (23, 25))]","[('text categorization', (19, 21)), ('longrange associations in text', (25, 29))]",[],[],"[['Research problem', 'proposes', 'text categorization']]",[],document_classification,13,4
730,model,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .","[('call', (1, 2)), ('as', (10, 11)), ('per', (14, 15)), ('in', (18, 19))]","[('deep pyramid CNN ( DPCNN )', (3, 9)), ('computation time', (12, 14)), ('layer', (15, 16)), ('decreases', (16, 17)), ('exponentially', (17, 18)), (""' pyramid shape"", (20, 23))]","[['deep pyramid CNN ( DPCNN )', 'as', 'computation time'], ['computation time', 'per', 'layer'], ['exponentially', 'in', ""' pyramid shape""]]","[['layer', 'has', 'decreases'], ['decreases', 'has', 'exponentially']]","[['Model', 'call', 'deep pyramid CNN ( DPCNN )']]",[],document_classification,13,29
731,model,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .","[('converting', (1, 2)), ('to', (4, 5)), ('alternates', (12, 13)), ('over and', (20, 22)), ('leading to', (25, 27)), ('in which', (30, 32)), ('shrinks in', (42, 44))]","[('discrete text', (2, 4)), ('continuous representation', (5, 7)), ('DPCNN architecture', (9, 11)), ('convolution block and a downsampling layer', (14, 20)), ('1', (23, 24)), ('deep network', (28, 30)), ('internal data size ( as well as per-layer computation )', (32, 42)), ('pyramid shape', (45, 47))]","[['discrete text', 'to', 'continuous representation'], ['DPCNN architecture', 'alternates', 'convolution block and a downsampling layer'], ['convolution block and a downsampling layer', 'over and', '1'], ['1', 'leading to', 'deep network'], ['deep network', 'in which', 'internal data size ( as well as per-layer computation )'], ['internal data size ( as well as per-layer computation )', 'shrinks in', 'pyramid shape']]","[['discrete text', 'has', 'continuous representation']]","[['Model', 'converting', 'discrete text']]",[],document_classification,13,30
732,model,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .","[('performs', (3, 4)), ('generalizes', (9, 10)), ('to', (14, 15)), ('of', (17, 18)), ('covering', (20, 21))]","[('first layer', (1, 3)), ('text region embedding', (4, 7)), ('commonly used word embedding', (10, 14)), ('embedding', (16, 17)), ('text regions', (18, 20)), ('one or more words', (21, 25))]","[['first layer', 'performs', 'text region embedding'], ['text region embedding', 'generalizes', 'commonly used word embedding'], ['commonly used word embedding', 'to', 'embedding'], ['embedding', 'of', 'text regions'], ['text regions', 'covering', 'one or more words']]",[],[],"[['Model', 'has', 'first layer']]",document_classification,13,37
733,model,We use max pooling for all pooling layers .,"[('use', (1, 2)), ('for', (4, 5))]","[('max pooling', (2, 4)), ('all pooling layers', (5, 8))]","[['max pooling', 'for', 'all pooling layers']]",[],"[['Model', 'use', 'max pooling']]",[],document_classification,13,40
734,hyperparameters,"To minimize a log loss with softmax , minibatch SGD with momentum 0.9 was conducted for n epochs ( n was fixed to 50 for AG , 30 for Yelp.f / p and Dbpedia , and 15 for the rest ) while the learning rate was set to ?","[('To minimize', (0, 2)), ('with', (5, 6)), ('with', (10, 11)), ('conducted for', (14, 16)), ('set to', (46, 48))]","[('log loss', (3, 5)), ('softmax', (6, 7)), ('minibatch SGD', (8, 10)), ('momentum 0.9', (11, 13)), ('n epochs', (16, 18)), ('learning rate', (43, 45))]","[['log loss', 'with', 'softmax'], ['minibatch SGD', 'with', 'momentum 0.9'], ['minibatch SGD', 'with', 'momentum 0.9'], ['momentum 0.9', 'conducted for', 'n epochs']]","[['log loss', 'has', 'softmax']]","[['Hyperparameters', 'To minimize', 'log loss']]",[],document_classification,13,125
735,hyperparameters,The minibatch size was fixed to 100 .,"[('fixed to', (4, 6))]","[('minibatch size', (1, 3)), ('100', (6, 7))]","[['minibatch size', 'fixed to', '100']]","[['minibatch size', 'has', '100']]",[],"[['Hyperparameters', 'has', 'minibatch size']]",document_classification,13,129
736,hyperparameters,Regularization was done by weight decay with the parameter 0.0001 and by optional dropout with 0.5 applied to the input to the top layer .,"[('done by', (2, 4)), ('with', (6, 7)), ('by', (11, 12)), ('with', (14, 15)), ('applied to', (16, 18)), ('to', (20, 21))]","[('Regularization', (0, 1)), ('weight decay', (4, 6)), ('parameter 0.0001', (8, 10)), ('optional dropout', (12, 14)), ('0.5', (15, 16)), ('input', (19, 20))]","[['Regularization', 'done by', 'weight decay'], ['weight decay', 'with', 'parameter 0.0001'], ['optional dropout', 'with', '0.5'], ['Regularization', 'by', 'optional dropout'], ['optional dropout', 'with', '0.5'], ['0.5', 'applied to', 'input']]",[],[],"[['Hyperparameters', 'has', 'Regularization']]",document_classification,13,130
737,hyperparameters,"In some cases overfitting was observed , and so we performed early stopping , based on the validation performance , after reducing the learning rate to 0.1 ?.","[('observed', (5, 6)), ('performed', (10, 11)), ('after reducing', (20, 22)), ('to', (25, 26))]","[('overfitting', (3, 4)), ('early stopping', (11, 13)), ('learning rate', (23, 25)), ('0.1', (26, 27))]","[['early stopping', 'after reducing', 'learning rate'], ['learning rate', 'to', '0.1']]",[],[],[],document_classification,13,131
738,hyperparameters,Weights were initialized by the Gaussian distribution with zero mean and standard deviation 0.01 .,"[('initialized by', (2, 4)), ('with', (7, 8))]","[('Weights', (0, 1)), ('Gaussian distribution', (5, 7)), ('zero mean', (8, 10)), ('standard deviation 0.01', (11, 14))]","[['Weights', 'initialized by', 'Gaussian distribution'], ['Gaussian distribution', 'with', 'zero mean']]",[],[],"[['Hyperparameters', 'has', 'Weights']]",document_classification,13,132
739,hyperparameters,"The discrete input to the region embedding layer was fixed to the bow input , and the region size was chosen from { 1 , 3 , 5 } , while fixing output dimensionality to 250 ( same as convolution layers ) .","[('to', (3, 4)), ('fixed to', (9, 11)), ('chosen from', (20, 22)), ('fixing', (31, 32)), ('to', (34, 35))]","[('discrete input', (1, 3)), ('region embedding layer', (5, 8)), ('bow input', (12, 14)), ('region size', (17, 19)), ('{ 1 , 3 , 5 }', (22, 29)), ('output dimensionality', (32, 34)), ('250', (35, 36))]","[['discrete input', 'to', 'region embedding layer'], ['output dimensionality', 'to', '250'], ['region embedding layer', 'fixed to', 'bow input'], ['output dimensionality', 'fixed to', '250'], ['region size', 'chosen from', '{ 1 , 3 , 5 }'], ['region size', 'fixing', 'output dimensionality'], ['output dimensionality', 'to', '250']]","[['output dimensionality', 'has', '250']]",[],"[['Hyperparameters', 'has', 'discrete input']]",document_classification,13,133
740,experiments,The dimensionality of unsupervised embeddings was set to 300 unless otherwise specified .,"[('of', (2, 3)), ('set to', (6, 8))]","[('dimensionality', (1, 2)), ('unsupervised embeddings', (3, 5)), ('300', (8, 9))]","[['dimensionality', 'of', 'unsupervised embeddings'], ['dimensionality', 'set to', '300'], ['unsupervised embeddings', 'set to', '300']]",[],[],[],document_classification,13,148
741,ablation-analysis,"In the results below , the depth of DPCNN was fixed to 15 unless otherwise specified .","[('of', (7, 8)), ('fixed to', (10, 12))]","[('depth', (6, 7)), ('DPCNN', (8, 9)), ('15', (12, 13))]","[['depth', 'of', 'DPCNN'], ['depth', 'fixed to', '15'], ['DPCNN', 'fixed to', '15']]",[],[],"[['Ablation analysis', 'has', 'depth']]",document_classification,13,151
742,results,Making it deeper did not substantially improve or degrade accuracy .,"[('Making', (0, 1)), ('did not', (3, 5))]","[('deeper', (2, 3)), ('substantially improve', (5, 7)), ('accuracy', (9, 10))]","[['deeper', 'did not', 'substantially improve']]","[['deeper', 'has', 'substantially improve']]","[['Results', 'Making', 'deeper']]",[],document_classification,13,152
743,experiments,Large data results,[],[],[],[],[],[],document_classification,13,155
744,results,"On all the five datasets , DPCNN outperforms all of the previous results , which validates the effectiveness of our approach .","[('On', (0, 1)), ('validates', (15, 16))]","[('DPCNN', (6, 7)), ('outperforms', (7, 8)), ('all of the previous results', (8, 13)), ('effectiveness', (17, 18))]","[['all of the previous results', 'validates', 'effectiveness']]","[['DPCNN', 'has', 'outperforms'], ['outperforms', 'has', 'all of the previous results']]","[['Results', 'On', 'DPCNN']]",[],document_classification,13,159
745,experiments,Small data results,[],[],[],[],[],[],document_classification,13,191
746,results,One difference from the large dataset results is that the strength of shallow models stands out .,"[('from', (2, 3)), ('stands', (14, 15))]","[('large dataset results', (4, 7)), ('strength of', (10, 12)), ('shallow models', (12, 14))]",[],"[['large dataset results', 'has', 'strength of'], ['strength of', 'has', 'shallow models']]","[['Results', 'from', 'large dataset results']]",[],document_classification,13,195
747,results,"ShallowCNN ( row 2 ) rivals DPCNN ( row 1 ) , and Zhang et al. 's best linear model ( row 3 ) moved up from the worst performer to the third best performer .","[('rivals', (5, 6)), ('moved up from', (24, 27)), ('to', (30, 31))]","[('ShallowCNN', (0, 1)), ('DPCNN', (6, 7)), ('best linear model', (17, 20)), ('worst performer', (28, 30)), ('third best performer', (32, 35))]","[['ShallowCNN', 'rivals', 'DPCNN'], ['best linear model', 'moved up from', 'worst performer'], ['worst performer', 'to', 'third best performer']]",[],[],"[['Results', 'has', 'ShallowCNN']]",document_classification,13,196
748,results,The error rate improves as the depth increases .,"[('as', (4, 5))]","[('error rate', (1, 3)), ('improves', (3, 4)), ('depth increases', (6, 8))]","[['improves', 'as', 'depth increases']]","[['error rate', 'has', 'improves']]",[],[],document_classification,13,205
749,research-problem,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,[],"[('Supervised and Semi- Supervised Text Categorization', (0, 6))]",[],[],[],[],document_classification,14,2
750,approach,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .","[('In', (0, 1)), ('at', (19, 20)), ('converted to', (23, 25)), ('with', (28, 29)), ('relevant to', (30, 32)), ('loosely term', (39, 41))]","[('convolution layer', (2, 4)), ('small region of data ( e.g. , a', (6, 14)), ('every location', (20, 22)), ('low-dimensional vector', (26, 28)), ('information', (29, 30)), ('task being', (33, 35)), ('preserved', (35, 36)), ('embedding', (42, 43))]","[['every location', 'converted to', 'low-dimensional vector'], ['low-dimensional vector', 'with', 'information'], ['information', 'relevant to', 'task being']]","[['convolution layer', 'has', 'small region of data ( e.g. , a'], ['task being', 'has', 'preserved']]","[['Approach', 'In', 'convolution layer']]",[],document_classification,14,16
751,approach,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .","[('shared among', (4, 6))]","[('embedding function', (1, 3)), ('all the locations', (6, 9)), ('useful features', (12, 14))]","[['embedding function', 'shared among', 'all the locations']]",[],[],"[['Approach', 'has', 'embedding function']]",document_classification,14,17
752,approach,"A document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .","[('represented as', (3, 5)), ('of', (7, 8)), ('converts', (30, 31)), ('of', (33, 34)), ('to', (45, 46)), ('at', (48, 49)), ('aggregates', (61, 62)), ('to', (66, 67)), ('by taking', (70, 72))]","[('document', (1, 2)), ('sequence', (6, 7)), ('convolution layer', (28, 30)), ('small regions', (31, 33)), ('document', (35, 36)), ('low-dimensional vectors', (46, 48)), ('every location ( embedding of text regions )', (49, 57)), ('pooling layer', (59, 61)), ('region embedding results', (63, 66)), ('document vector', (68, 70)), ('componentwise maximum or average', (72, 76)), ('linear model', (87, 89))]","[['document', 'represented as', 'sequence'], ['convolution layer', 'converts', 'small regions'], ['small regions', 'of', 'document'], ['region embedding results', 'to', 'document vector'], ['low-dimensional vectors', 'at', 'every location ( embedding of text regions )'], ['pooling layer', 'aggregates', 'region embedding results'], ['region embedding results', 'to', 'document vector'], ['document vector', 'by taking', 'componentwise maximum or average']]",[],[],"[['Approach', 'has', 'document']]",document_classification,14,19
753,approach,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .","[('build on', (5, 7)), ('of', (10, 11)), ('explore', (18, 19)), ('via', (24, 25)), ('seeking to overcome', (34, 37)), ('in', (41, 42))]","[('general framework', (8, 10)), ('more sophisticated region embedding', (20, 24)), ('Long Short - Term Memory ( LSTM )', (25, 33)), ('shortcomings', (38, 39)), ('supervised and semi-supervised settings', (43, 47))]","[['more sophisticated region embedding', 'via', 'Long Short - Term Memory ( LSTM )'], ['Long Short - Term Memory ( LSTM )', 'seeking to overcome', 'shortcomings'], ['shortcomings', 'in', 'supervised and semi-supervised settings']]","[['general framework', 'name', 'more sophisticated region embedding']]","[['Approach', 'build on', 'general framework']]",[],document_classification,14,31
754,approach,LSTM ) is a recurrent neural network .,"[('is', (2, 3))]","[('LSTM', (0, 1)), ('recurrent neural network', (4, 7))]","[['LSTM', 'is', 'recurrent neural network']]","[['LSTM', 'has', 'recurrent neural network']]",[],"[['Approach', 'has', 'LSTM']]",document_classification,14,32
755,experiments,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .","[('including', (15, 16))]","[('both our LSTM models and one - hot CNN', (2, 11)), ('strongly outperform', (11, 13)), ('other methods', (13, 15)), ('previous LSTM', (16, 18))]","[['other methods', 'including', 'previous LSTM']]","[['both our LSTM models and one - hot CNN', 'has', 'strongly outperform'], ['strongly outperform', 'has', 'other methods']]",[],[],document_classification,14,42
756,experiments,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .","[('obtained', (4, 5)), ('combining', (6, 7)), ('trained on', (22, 24))]","[('best results', (1, 3)), ('two types of region embeddings ( LSTM embed - dings and CNN embeddings )', (8, 22)), ('unlabeled data', (24, 26))]","[['best results', 'combining', 'two types of region embeddings ( LSTM embed - dings and CNN embeddings )'], ['two types of region embeddings ( LSTM embed - dings and CNN embeddings )', 'trained on', 'unlabeled data']]",[],[],[],document_classification,14,43
757,code,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,[],"[('http://riejohnson.com/cnn download.html', (8, 10))]",[],[],[],[],document_classification,14,46
758,research-problem,Supervised LSTM for text categorization,[],[],[],[],[],[],document_classification,14,76
759,research-problem,Pooling : simplifying sub - problems,[],[],[],[],[],[],document_classification,14,93
760,research-problem,Chopping for speeding up training,[],[],[],[],[],[],document_classification,14,102
761,hyperparameters,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .","[('reduced to', (8, 10)), ('of', (16, 17)), ('to reduce', (20, 22)), ('minimized with', (28, 30)), ('applied to', (31, 33)), ('initialized by', (42, 44))]","[('neural network experiments', (2, 5)), ('vocabulary', (6, 7)), ('most frequent 30 K words', (11, 16)), ('training data', (18, 20)), ('computational burden', (22, 24)), ('square loss', (25, 27)), ('dropout', (30, 31)), ('input to', (34, 36)), ('top layer', (37, 39)), ('weights', (40, 41))]","[['vocabulary', 'reduced to', 'most frequent 30 K words'], ['most frequent 30 K words', 'of', 'training data'], ['most frequent 30 K words', 'to reduce', 'computational burden'], ['training data', 'to reduce', 'computational burden'], ['square loss', 'minimized with', 'dropout'], ['dropout', 'applied to', 'input to']]","[['neural network experiments', 'has', 'vocabulary'], ['input to', 'has', 'top layer']]",[],[],document_classification,14,130
762,experiments,"RCV1 ( second - level topics only ) and 20 NG are for topic categorization of Reuters news articles and newsgroup messages , respectively .","[('for', (12, 13))]","[('20 NG', (9, 11)), ('topic categorization of Reuters news articles and newsgroup messages', (13, 22))]","[['20 NG', 'for', 'topic categorization of Reuters news articles and newsgroup messages']]",[],[],[],document_classification,14,133
763,experiments,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,"[('done with', (2, 4)), ('with', (5, 6)), ('with', (11, 12)), ('optionally', (14, 15)), ('for', (16, 17))]","[('Optimization', (0, 1)), ('SGD', (4, 5)), ('mini-batch size 50 or 100', (6, 11)), ('momentum', (12, 13)), ('rmsprop', (15, 16)), ('acceleration', (17, 18))]","[['Optimization', 'done with', 'SGD'], ['SGD', 'with', 'mini-batch size 50 or 100'], ['mini-batch size 50 or 100', 'with', 'momentum'], ['Optimization', 'optionally', 'rmsprop'], ['SGD', 'optionally', 'rmsprop'], ['rmsprop', 'for', 'acceleration']]",[],[],[],document_classification,14,135
764,experiments,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .","[('Comparing', (0, 1)), ('see that', (9, 11)), ('with', (17, 18)), ('outperforms', (25, 26)), ('on', (35, 36))]","[('our one - hot bidirectional LSTM', (11, 17)), ('pooling ( oh - 2 LSTMp )', (18, 25)), ('word - vector LSTM ( wv - LSTM )', (26, 35)), ('all the datasets', (36, 39))]","[['our one - hot bidirectional LSTM', 'with', 'pooling ( oh - 2 LSTMp )'], ['our one - hot bidirectional LSTM', 'outperforms', 'word - vector LSTM ( wv - LSTM )'], ['pooling ( oh - 2 LSTMp )', 'outperforms', 'word - vector LSTM ( wv - LSTM )'], ['word - vector LSTM ( wv - LSTM )', 'on', 'all the datasets']]",[],[],[],document_classification,14,139
765,tasks,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .","[('obtained by', (2, 4)), ('with', (22, 23)), ('on', (26, 27)), ('with', (33, 34))]","[('bow - CNN', (4, 7)), ('region size 20', (23, 26)), ('RCV1', (27, 28)), ('seq -CNN', (30, 32)), ('regular concatenation input', (35, 38)), ('region size', (40, 42))]","[['region size 20', 'on', 'RCV1'], ['seq -CNN', 'with', 'regular concatenation input']]",[],[],[],document_classification,14,142
766,results,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .","[('on', (2, 3))]","[('three out of the four datasets', (3, 9)), ('oh - 2 LSTMp', (10, 14)), ('outperforms', (14, 15)), ('SVM and the CNN', (15, 19))]",[],"[['three out of the four datasets', 'has', 'oh - 2 LSTMp'], ['oh - 2 LSTMp', 'has', 'outperforms'], ['outperforms', 'has', 'SVM and the CNN']]",[],[],document_classification,14,143
767,results,"However , on RCV1 , it underperforms both .","[('on', (2, 3))]","[('RCV1', (3, 4)), ('underperforms', (6, 7))]",[],"[['RCV1', 'has', 'underperforms']]","[['Results', 'on', 'RCV1']]",[],document_classification,14,144
768,results,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .","[('is', (6, 7)), ('than', (9, 10))]","[('RCV1', (2, 3)), ('n-gram SVM', (4, 6)), ('no better', (7, 9)), ('bag - of - word SVM', (10, 16)), ('outperforms', (25, 26)), ('seq-CNN', (26, 27))]","[['n-gram SVM', 'is', 'no better'], ['no better', 'than', 'bag - of - word SVM']]","[['RCV1', 'has', 'n-gram SVM'], ['n-gram SVM', 'has', 'no better'], ['outperforms', 'has', 'seq-CNN']]",[],[],document_classification,14,147
769,results,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .","[('on', (3, 4)), ('in a', (9, 11)), ('at', (14, 15)), ('are', (17, 18)), ('than', (20, 21)), ('in', (22, 23))]","[('RCV1', (4, 5)), ('bags of words', (6, 9)), ('window of 20', (11, 14)), ('every location', (15, 17)), ('more useful', (18, 20)), ('words', (21, 22)), ('strict order', (23, 25))]","[['bags of words', 'in a', 'window of 20'], ['words', 'in a', 'strict order'], ['window of 20', 'at', 'every location'], ['window of 20', 'are', 'more useful'], ['every location', 'are', 'more useful'], ['more useful', 'than', 'words'], ['words', 'in', 'strict order']]","[['RCV1', 'has', 'bags of words'], ['bags of words', 'has', 'window of 20']]","[['Results', 'on', 'RCV1']]",[],document_classification,14,148
770,results,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .","[('does not have', (5, 8)), ('loses to', (16, 18))]","[('LSTM', (2, 3)), ('words', (12, 13)), ('bags', (14, 15)), ('bow - CNN', (18, 21))]","[['LSTM', 'loses to', 'bow - CNN']]",[],[],"[['Results', 'has', 'LSTM']]",document_classification,14,150
771,results,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .","[('of', (5, 6)), ('to embed', (7, 9)), ('appears not to be', (11, 15))]","[('strength', (4, 5)), ('LSTM', (6, 7)), ('larger regions', (9, 11)), ('big contributor', (16, 18))]","[['strength', 'of', 'LSTM'], ['strength', 'to embed', 'larger regions'], ['LSTM', 'to embed', 'larger regions'], ['larger regions', 'appears not to be', 'big contributor']]",[],[],[],document_classification,14,156
772,results,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .","[('works', (6, 7))]","[('one - hot CNN', (2, 6)), ('surprising well', (7, 9))]","[['one - hot CNN', 'works', 'surprising well']]","[['one - hot CNN', 'has', 'surprising well']]",[],"[['Results', 'has', 'one - hot CNN']]",document_classification,14,158
773,results,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .","[('on', (4, 5)), ('is', (6, 7)), ('of', (15, 16)), ('obtained by', (18, 20)), ('of', (24, 25)), ('with', (27, 28))]","[('previous best performance', (1, 4)), ('20NG', (5, 6)), ('15.3', (7, 8)), ('DL15', (16, 17)), ('pre-training wv - LSTM', (20, 24)), ('1024 units', (25, 27)), ('labeled training data', (28, 31))]","[['previous best performance', 'on', '20NG'], ['20NG', 'is', '15.3'], ['pre-training wv - LSTM', 'of', '1024 units'], ['DL15', 'obtained by', 'pre-training wv - LSTM'], ['pre-training wv - LSTM', 'of', '1024 units'], ['1024 units', 'with', 'labeled training data']]",[],[],"[['Results', 'has', 'previous best performance']]",document_classification,14,160
774,results,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .","[('achieved', (5, 6))]","[('Our oh - 2 LSTMp', (0, 5)), ('13.32', (6, 7)), ('2 % better', (10, 13))]","[['Our oh - 2 LSTMp', 'achieved', '13.32']]",[],[],"[['Results', 'has', 'Our oh - 2 LSTMp']]",document_classification,14,161
775,tasks,"The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one - hot CNN , resulting in higher accuracy .","[('to produce', (5, 7)), ('to', (9, 10)), ('of', (14, 15)), ('resulting in', (20, 22))]","[('obtained tv-embeddings', (1, 3)), ('additional input', (7, 9)), ('supervised region embedding', (11, 14)), ('one - hot CNN', (15, 19)), ('higher accuracy', (22, 24))]","[['obtained tv-embeddings', 'to produce', 'additional input'], ['additional input', 'to', 'supervised region embedding'], ['supervised region embedding', 'of', 'one - hot CNN'], ['supervised region embedding', 'resulting in', 'higher accuracy'], ['one - hot CNN', 'resulting in', 'higher accuracy']]",[],[],[],document_classification,14,175
776,results,"Compared with the supervised oh - 2 LSTMp , clear performance improvements were obtained on all the datasets , thus , confirming the effectiveness of our approach .","[('Compared with', (0, 2)), ('obtained on', (13, 15))]","[('supervised oh - 2 LSTMp', (3, 8)), ('clear performance improvements', (9, 12)), ('all the datasets', (15, 18))]","[['clear performance improvements', 'obtained on', 'all the datasets']]","[['supervised oh - 2 LSTMp', 'has', 'clear performance improvements']]","[['Results', 'Compared with', 'supervised oh - 2 LSTMp']]",[],document_classification,14,205
777,results,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .","[('with', (18, 19))]","[('pre-trained wv - LSTM', (2, 6)), ('clearly outperformed', (6, 8)), ('supervised wv - LSTM', (9, 13)), ('underperformed', (15, 16)), ('models', (17, 18)), ('region tv-embeddings', (19, 21))]","[['models', 'with', 'region tv-embeddings']]","[['pre-trained wv - LSTM', 'has', 'clearly outperformed'], ['clearly outperformed', 'has', 'supervised wv - LSTM'], ['underperformed', 'has', 'models']]",[],"[['Results', 'has', 'pre-trained wv - LSTM']]",document_classification,14,210
778,results,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .","[('using', (8, 9)), ('performed', (18, 19))]","[('wv - 2 LSTMp', (4, 8)), ('Google News vectors', (10, 13)), ('relatively poorly', (19, 21))]","[['wv - 2 LSTMp', 'using', 'Google News vectors'], ['Google News vectors', 'performed', 'relatively poorly']]",[],[],[],document_classification,14,216
779,results,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .","[('trained with', (3, 5)), ('observed after', (13, 15)), ('scaled', (16, 17))]","[('word2vec', (1, 2)), ('domain unlabeled data', (6, 9)), ('better results', (10, 12)), ('word vectors', (17, 19))]","[['word2vec', 'trained with', 'domain unlabeled data'], ['better results', 'scaled', 'word vectors']]",[],[],[],document_classification,14,217
780,results,"Still , it underperformed the models with region tv - embeddings ( row # 4 , 5 ) , which used the same domain unlabeled data .","[('used', (20, 21))]","[('underperformed', (3, 4)), ('models', (5, 6)), ('region tv - embeddings', (7, 11)), ('same domain unlabeled data', (22, 26))]","[['region tv - embeddings', 'used', 'same domain unlabeled data']]","[['underperformed', 'has', 'models']]",[],[],document_classification,14,218
781,results,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,"[('rivals', (7, 8)), ('on', (17, 18)), ('on', (24, 25))]","[('LSTM', (1, 2)), ('CNN', (11, 12)), ('IMDB / Elec', (18, 21)), ('underperforms', (22, 23)), ('RCV1', (25, 26))]","[['LSTM', 'rivals', 'CNN'], ['LSTM', 'rivals', 'underperforms'], ['CNN', 'on', 'IMDB / Elec'], ['underperforms', 'on', 'RCV1']]",[],[],"[['Results', 'has', 'LSTM']]",document_classification,14,221
782,results,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .","[('Increasing', (0, 1)), ('of', (3, 4)), ('from', (6, 7)), ('on', (10, 11)), ('obtain', (14, 15))]","[('dimensionality', (2, 3)), ('LSTM tvembeddings', (4, 6)), ('100 to 300', (7, 10)), ('RCV1', (11, 12)), ('8.62', (15, 16))]","[['dimensionality', 'of', 'LSTM tvembeddings'], ['LSTM tvembeddings', 'from', '100 to 300'], ['100 to 300', 'on', 'RCV1']]",[],"[['Results', 'Increasing', 'dimensionality']]",[],document_classification,14,222
783,results,"For example , adding the CNN tv-embeddings to the LSTM of row# 1 , the error rate on IMDB improved from 6.66 to 5.94 , and adding the LSTM tv-embeddings to the CNN of row # 2 , the error rate on RCV1 improved from 7.71 to 7.15 .","[('adding', (3, 4)), ('on', (17, 18)), ('improved from', (19, 21)), ('on', (41, 42)), ('improved from', (43, 45))]","[('CNN tv-embeddings', (5, 7)), ('error rate', (15, 17)), ('IMDB', (18, 19)), ('6.66 to', (21, 23)), ('5.94', (23, 24)), ('error rate', (39, 41)), ('RCV1', (42, 43)), ('7.71', (45, 46)), ('7.15', (47, 48))]","[['error rate', 'on', 'IMDB'], ['error rate', 'on', 'RCV1'], ['error rate', 'improved from', '6.66 to'], ['IMDB', 'improved from', '6.66 to'], ['error rate', 'on', 'RCV1'], ['RCV1', 'improved from', '7.71']]","[['CNN tv-embeddings', 'has', 'error rate'], ['6.66 to', 'has', '5.94']]","[['Results', 'adding', 'CNN tv-embeddings']]",[],document_classification,14,238
784,results,"The results indicate that , as expected , LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined .","[('indicate', (2, 3)), ('when', (19, 20))]","[('LSTM tv-embeddings and CNN tv-embeddings', (8, 13)), ('complement', (13, 14)), ('each other', (14, 16)), ('improve', (17, 18)), ('performance', (18, 19)), ('combined', (20, 21))]","[['performance', 'when', 'combined']]","[['LSTM tv-embeddings and CNN tv-embeddings', 'has', 'complement'], ['complement', 'has', 'each other'], ['improve', 'has', 'performance']]","[['Results', 'indicate', 'LSTM tv-embeddings and CNN tv-embeddings']]",[],document_classification,14,239
785,results,"The best supervised results on IMDB / Elec of JZ15a are in the first row , obtained by integrating a document embedding layer into one - hot CNN .","[('on', (4, 5)), ('obtained by integrating', (16, 19)), ('into', (23, 24))]","[('best supervised results', (1, 4)), ('IMDB / Elec of JZ15a', (5, 10)), ('document embedding layer', (20, 23)), ('one - hot CNN', (24, 28))]","[['best supervised results', 'on', 'IMDB / Elec of JZ15a'], ['best supervised results', 'obtained by integrating', 'document embedding layer'], ['IMDB / Elec of JZ15a', 'obtained by integrating', 'document embedding layer'], ['document embedding layer', 'into', 'one - hot CNN']]",[],[],"[['Results', 'has', 'best supervised results']]",document_classification,14,243
786,results,"As shown in the last row of , our new model further improved it to 5.94 ; also on Elec and RCV1 , our best models exceeded the previous best results .","[('further improved it', (11, 14)), ('to', (14, 15)), ('on', (18, 19)), ('exceeded', (26, 27))]","[('our new model', (8, 11)), ('5.94', (15, 16)), ('Elec and RCV1', (19, 22)), ('our best models', (23, 26)), ('previous best results', (28, 31))]","[['our new model', 'further improved it', '5.94'], ['our new model', 'on', 'Elec and RCV1'], ['our best models', 'exceeded', 'previous best results']]","[['our new model', 'has', '5.94'], ['Elec and RCV1', 'has', 'our best models']]",[],[],document_classification,14,246
787,research-problem,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,[],[],[],[],[],[],document_classification,15,2
788,model,We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself .,"[('extend', (1, 2)), ('to', (7, 8)), ('by applying', (11, 13)), ('to', (14, 15)), ('in', (18, 19)), ('rather than to', (23, 26))]","[('adversarial and virtual adversarial training', (2, 7)), ('text domain', (9, 11)), ('perturbations', (13, 14)), ('word embeddings', (16, 18)), ('recurrent neural network', (20, 23)), ('original input itself', (27, 30))]","[['adversarial and virtual adversarial training', 'to', 'text domain'], ['perturbations', 'to', 'word embeddings'], ['adversarial and virtual adversarial training', 'by applying', 'perturbations'], ['perturbations', 'to', 'word embeddings'], ['word embeddings', 'in', 'recurrent neural network'], ['recurrent neural network', 'rather than to', 'original input itself']]",[],"[['Model', 'extend', 'adversarial and virtual adversarial training']]",[],document_classification,15,6
789,model,"It improves not only robustness to adversarial examples , but also generalization performance for original examples .","[('improves', (1, 2)), ('to', (5, 6)), ('for', (13, 14))]","[('robustness', (4, 5)), ('adversarial examples', (6, 8)), ('generalization performance', (11, 13)), ('original examples', (14, 16))]","[['robustness', 'to', 'adversarial examples'], ['generalization performance', 'for', 'original examples']]",[],"[['Model', 'improves', 'robustness']]",[],document_classification,15,13
790,model,"This is done by regularizing the model so that given an example , the model will produce the same output distribution as it produces on an adversarial perturbation of that example .","[('done by', (2, 4)), ('given', (9, 10)), ('as', (21, 22)), ('produces on', (23, 25)), ('of', (28, 29))]","[('regularizing', (4, 5)), ('model', (6, 7)), ('example', (11, 12)), ('model', (14, 15)), ('same output distribution', (18, 21)), ('adversarial perturbation', (26, 28)), ('example', (30, 31))]","[['model', 'given', 'example'], ['same output distribution', 'produces on', 'adversarial perturbation'], ['adversarial perturbation', 'of', 'example']]","[['regularizing', 'has', 'model']]","[['Model', 'done by', 'regularizing']]",[],document_classification,15,16
791,model,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .","[('define', (18, 19)), ('on', (21, 22)), ('instead of', (25, 27))]","[('perturbation', (20, 21)), ('continuous word embeddings', (22, 25)), ('discrete word inputs', (27, 30))]","[['perturbation', 'on', 'continuous word embeddings'], ['continuous word embeddings', 'instead of', 'discrete word inputs']]",[],"[['Model', 'define', 'perturbation']]",[],document_classification,15,22
792,experimental-setup,All experiments used TensorFlow on GPUs .,"[('used', (2, 3)), ('on', (4, 5))]","[('TensorFlow', (3, 4)), ('GPUs', (5, 6))]","[['TensorFlow', 'on', 'GPUs']]",[],"[['Experimental setup', 'used', 'TensorFlow']]",[],document_classification,15,100
793,code,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text.,[],[],[],[],[],[],document_classification,15,101
794,experimental-setup,We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,"[('applied', (1, 2)), ('with', (4, 5)), ('set to', (6, 8)), ('on', (9, 10)), ('except', (13, 14))]","[('gradient clipping', (2, 4)), ('norm', (5, 6)), ('1.0', (8, 9)), ('all the parameters', (10, 13)), ('word embeddings', (14, 16))]","[['gradient clipping', 'with', 'norm'], ['norm', 'set to', '1.0'], ['1.0', 'on', 'all the parameters'], ['all the parameters', 'except', 'word embeddings']]",[],"[['Experimental setup', 'applied', 'gradient clipping']]",[],document_classification,15,113
795,experimental-setup,"To reduce runtime on GPU , we used truncated backpropagation up to 400 words from each end of the sequence .","[('To reduce', (0, 2)), ('on', (3, 4)), ('used', (7, 8)), ('up to', (10, 12)), ('from', (14, 15))]","[('runtime', (2, 3)), ('GPU', (4, 5)), ('truncated backpropagation', (8, 10)), ('400 words', (12, 14)), ('each end of the', (15, 19)), ('sequence', (19, 20))]","[['runtime', 'on', 'GPU'], ['runtime', 'used', 'truncated backpropagation'], ['GPU', 'used', 'truncated backpropagation'], ['truncated backpropagation', 'up to', '400 words'], ['400 words', 'from', 'each end of the']]","[['each end of the', 'has', 'sequence']]","[['Experimental setup', 'To reduce', 'runtime']]",[],document_classification,15,114
796,experimental-setup,"For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .","[('For', (0, 1)), ('of', (2, 3)), ('applied', (9, 10)), ('on', (11, 12)), ('with', (16, 17))]","[('regularization', (1, 2)), ('recurrent language model', (4, 7)), ('dropout', (10, 11)), ('word embedding layer', (13, 16)), ('0.5 dropout rate', (17, 20))]","[['regularization', 'of', 'recurrent language model'], ['regularization', 'applied', 'dropout'], ['recurrent language model', 'applied', 'dropout'], ['dropout', 'on', 'word embedding layer'], ['word embedding layer', 'with', '0.5 dropout rate']]",[],"[['Experimental setup', 'For', 'regularization']]",[],document_classification,15,115
797,experimental-setup,"For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .","[('For', (0, 1)), ('used', (7, 8)), ('for', (12, 13)), ('used', (24, 25)), ('shared with', (31, 33))]","[('bidirectional LSTM model', (2, 5)), ('512 hidden units LSTM', (8, 12)), ('standard order and reversed order sequences', (15, 21)), ('256 dimensional word embeddings', (25, 29)), ('both of the LSTMs', (33, 37))]","[['512 hidden units LSTM', 'For', 'standard order and reversed order sequences'], ['bidirectional LSTM model', 'used', '512 hidden units LSTM'], ['512 hidden units LSTM', 'for', 'standard order and reversed order sequences'], ['bidirectional LSTM model', 'used', '256 dimensional word embeddings'], ['256 dimensional word embeddings', 'shared with', 'both of the LSTMs']]",[],"[['Experimental setup', 'For', 'bidirectional LSTM model']]",[],document_classification,15,116
798,results,Pretraining with a recurrent language model was very effective on classification performance on all the datasets we tested on and so our results in Section 5 are with this pretraining .,"[('on', (9, 10))]","[('classification performance', (10, 12))]",[],[],"[['Results', 'on', 'classification performance']]",[],document_classification,15,119
799,experimental-setup,"For optimization , we again used the Adam optimizer , with 0.0005 initial learning rate 0.9998 exponential decay .","[('used', (5, 6)), ('with', (10, 11))]","[('optimization', (1, 2)), ('Adam optimizer', (7, 9)), ('0.0005 initial learning rate', (11, 15))]","[['optimization', 'used', 'Adam optimizer'], ['Adam optimizer', 'with', '0.0005 initial learning rate']]",[],[],[],document_classification,15,124
800,experimental-setup,"Batch sizes are 64 on IMDB , Elec , RCV1 , and 128 on DBpedia .","[('are', (2, 3)), ('on', (4, 5)), ('on', (13, 14))]","[('Batch sizes', (0, 2)), ('64', (3, 4)), ('IMDB', (5, 6)), ('Elec', (7, 8)), ('RCV1', (9, 10)), ('128', (12, 13)), ('DBpedia', (14, 15))]","[['Batch sizes', 'are', '64'], ['64', 'on', 'IMDB'], ['64', 'on', 'DBpedia'], ['128', 'on', 'DBpedia'], ['128', 'on', 'DBpedia']]","[['Batch sizes', 'has', '64']]",[],"[['Experimental setup', 'has', 'Batch sizes']]",document_classification,15,125
801,experimental-setup,We again applied gradient clipping with the norm as 1.0 on all the parameters except the word embedding .,"[('applied', (2, 3)), ('with', (5, 6)), ('as', (8, 9)), ('on', (10, 11)), ('except', (14, 15))]","[('gradient clipping', (3, 5)), ('norm', (7, 8)), ('1.0', (9, 10)), ('all the parameters', (11, 14)), ('word embedding', (16, 18))]","[['gradient clipping', 'with', 'norm'], ['norm', 'as', '1.0'], ['1.0', 'on', 'all the parameters'], ['all the parameters', 'except', 'word embedding']]","[['gradient clipping', 'has', 'norm']]","[['Experimental setup', 'applied', 'gradient clipping']]",[],document_classification,15,129
802,results,Every adversarial training method outperformed every random perturbation method .,[],"[('Every adversarial training method', (0, 4)), ('outperformed', (4, 5)), ('every random perturbation method', (5, 9))]",[],"[['Every adversarial training method', 'has', 'outperformed'], ['outperformed', 'has', 'every random perturbation method']]",[],[],document_classification,15,148
803,results,"For the baseline and random perturbation method , the cosine distances were 0.361 and 0.377 , respectively .","[('For', (0, 1)), ('were', (11, 12))]","[('baseline and random perturbation method', (2, 7)), ('cosine distances', (9, 11)), ('0.361 and 0.377', (12, 15))]","[['cosine distances', 'were', '0.361 and 0.377']]","[['baseline and random perturbation method', 'has', 'cosine distances'], ['cosine distances', 'has', '0.361 and 0.377']]","[['Results', 'For', 'baseline and random perturbation method']]",[],document_classification,15,159
804,results,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .","[('on', (9, 10))]","[('our proposed method', (3, 6)), ('improved test performance', (6, 9)), ('baseline method', (11, 13))]","[['improved test performance', 'on', 'baseline method']]","[['our proposed method', 'has', 'improved test performance']]",[],"[['Results', 'has', 'our proposed method']]",document_classification,15,166
805,results,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,"[('further improves', (19, 21)), ('on', (22, 23))]","[('Our unidirectional LSTM model', (0, 4)), ('improves', (4, 5)), ('state of the art method', (7, 12)), ('our method', (13, 15)), ('results', (21, 22)), ('RCV1', (23, 24))]","[['results', 'on', 'RCV1']]","[['Our unidirectional LSTM model', 'has', 'improves'], ['improves', 'has', 'state of the art method']]",[],"[['Results', 'has', 'Our unidirectional LSTM model']]",document_classification,15,167
806,results,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .","[('able to', (3, 5)), ('over', (6, 7)), ('with', (12, 13)), ('achieved', (20, 21)), ('as', (25, 26))]","[('Adversarial training', (0, 2)), ('improve', (5, 6)), ('baseline method', (8, 10)), ('almost the same performance', (21, 25)), ('current state of the art method', (27, 33))]","[['Adversarial training', 'able to', 'improve'], ['improve', 'over', 'baseline method'], ['almost the same performance', 'as', 'current state of the art method']]","[['Adversarial training', 'has', 'improve']]",[],"[['Results', 'has', 'Adversarial training']]",document_classification,15,170
807,results,"We can see that the baseline method has already achieved nearly the current state of the art performance , and our proposed method improves from the baseline method .","[('see that', (2, 4)), ('achieved', (9, 10)), ('improves from', (23, 25))]","[('baseline method', (5, 7)), ('nearly the current state of the art performance', (10, 18)), ('our proposed method', (20, 23)), ('baseline method', (26, 28))]","[['baseline method', 'achieved', 'nearly the current state of the art performance'], ['our proposed method', 'improves from', 'baseline method']]",[],"[['Results', 'see that', 'baseline method']]",[],document_classification,15,180
808,research-problem,A C - LSTM Neural Network for Text Classification,[],[],[],[],[],[],document_classification,16,2
809,research-problem,Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling .,"[('demonstrated', (5, 6)), ('in', (13, 14))]","[('Neural network models', (0, 3)), ('remarkable performance', (11, 13)), ('sentence and document modeling', (14, 18))]","[['remarkable performance', 'in', 'sentence and document modeling']]",[],[],[],document_classification,16,4
810,research-problem,C - LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics .,"[('able to capture', (4, 7)), ('of', (10, 11)), ('as', (12, 13))]","[('C - LSTM', (0, 3)), ('both local features', (7, 10)), ('phrases', (11, 12)), ('global and temporal sentence semantics', (15, 20))]","[['C - LSTM', 'able to capture', 'both local features'], ['C - LSTM', 'able to capture', 'global and temporal sentence semantics'], ['both local features', 'of', 'phrases']]",[],[],[],document_classification,16,8
811,model,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .","[('introduce', (5, 6)), ('for', (10, 11)), ('by combining', (14, 16)), ('to', (19, 20))]","[('C - LSTM', (11, 14)), ('CNN and LSTM', (16, 19)), ('model sentences', (20, 22))]","[['CNN and LSTM', 'to', 'model sentences']]",[],"[['Model', 'introduce', 'C - LSTM']]",[],document_classification,16,29
812,model,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .","[('To benefit from', (0, 3)), ('of', (5, 6)), ('design', (12, 13)), ('by feeding', (23, 25)), ('of', (27, 28)), ('into', (33, 34))]","[('advantages', (4, 5)), ('simple end - to - end , unified architecture', (14, 23)), ('output', (26, 27)), ('one - layer CNN', (29, 33)), ('LSTM', (34, 35))]","[['output', 'of', 'one - layer CNN'], ['simple end - to - end , unified architecture', 'by feeding', 'output'], ['output', 'of', 'one - layer CNN'], ['one - layer CNN', 'into', 'LSTM']]","[['advantages', 'has', 'simple end - to - end , unified architecture']]","[['Model', 'To benefit from', 'advantages']]",[],document_classification,16,30
813,model,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,"[('constructed on top of', (3, 7)), ('from', (11, 12)), ('to learn', (16, 18)), ('of', (22, 23))]","[('CNN', (1, 2)), ('pre-trained word vectors', (8, 11)), ('massive unlabeled text data', (12, 16)), ('higher - level representions', (18, 22)), ('n-grams', (23, 24))]","[['CNN', 'constructed on top of', 'pre-trained word vectors'], ['pre-trained word vectors', 'from', 'massive unlabeled text data'], ['massive unlabeled text data', 'to learn', 'higher - level representions'], ['higher - level representions', 'of', 'n-grams']]",[],[],"[['Model', 'has', 'CNN']]",document_classification,16,31
814,model,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .","[('to learn', (1, 3)), ('from', (5, 6)), ('of', (15, 16)), ('organized as', (18, 20)), ('to serve', (23, 25)), ('of', (28, 29))]","[('sequential correlations', (3, 5)), ('higher - level suqence representations', (6, 11)), ('feature maps', (13, 15)), ('CNN', (16, 17)), ('sequential window features', (20, 23)), ('input', (27, 28)), ('LSTM', (29, 30))]","[['sequential correlations', 'from', 'higher - level suqence representations'], ['feature maps', 'of', 'CNN'], ['feature maps', 'organized as', 'sequential window features'], ['CNN', 'organized as', 'sequential window features'], ['sequential window features', 'to serve', 'input'], ['input', 'of', 'LSTM']]",[],"[['Model', 'to learn', 'sequential correlations']]",[],document_classification,16,32
815,model,"In this way , instead of constructing LSTM directly from the input sentence , we first transform each sentence into successive window ( n- gram ) features to help disentangle factors of variations within sentences .","[('directly from', (8, 10)), ('first transform', (15, 17)), ('into', (19, 20)), ('to help disentangle', (27, 30)), ('of', (31, 32)), ('within', (33, 34))]","[('constructing', (6, 7)), ('LSTM', (7, 8)), ('input sentence', (11, 13)), ('each sentence', (17, 19)), ('successive window ( n- gram ) features', (20, 27)), ('factors', (30, 31)), ('variations', (32, 33)), ('sentences', (34, 35))]","[['LSTM', 'directly from', 'input sentence'], ['LSTM', 'first transform', 'each sentence'], ['each sentence', 'into', 'successive window ( n- gram ) features'], ['successive window ( n- gram ) features', 'to help disentangle', 'factors'], ['factors', 'of', 'variations'], ['variations', 'within', 'sentences']]","[['constructing', 'has', 'LSTM'], ['factors', 'has', 'variations']]",[],[],document_classification,16,33
816,experiments,"We also show that the combination of CNN and LSTM outperforms individual multi - layer CNN models and RNN models , which indicates that LSTM can learn longterm dependencies from sequences of higher - level representations better than the other models .","[('show', (2, 3))]","[('combination of', (5, 7)), ('CNN and LSTM', (7, 10)), ('outperforms', (10, 11)), ('individual multi - layer CNN models and RNN models', (11, 20))]",[],"[['combination of', 'has', 'CNN and LSTM'], ['CNN and LSTM', 'has', 'outperforms'], ['outperforms', 'has', 'individual multi - layer CNN models and RNN models']]",[],[],document_classification,16,37
817,baselines,"We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .","[('implement', (1, 2)), ('based on', (4, 6)), ('supports', (14, 15)), ('of', (21, 22))]","[('Theano', (6, 7)), ('python library', (10, 12)), ('efficient symbolic differentiation', (15, 18)), ('transparent use', (19, 21)), ('GPU', (23, 24))]","[['python library', 'supports', 'efficient symbolic differentiation'], ['python library', 'supports', 'transparent use'], ['transparent use', 'of', 'GPU']]","[['Theano', 'has', 'python library'], ['transparent use', 'has', 'GPU']]","[['Baselines', 'implement', 'Theano']]",[],document_classification,16,142
818,experimental-setup,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .","[('To benefit from', (0, 3)), ('of', (8, 9)), ('train', (13, 14)), ('on', (16, 17))]","[('efficiency of parallel computation', (4, 8)), ('tensors', (10, 11)), ('model', (15, 16)), ('GPU', (18, 19))]","[['efficiency of parallel computation', 'of', 'tensors'], ['model', 'on', 'GPU']]","[['efficiency of parallel computation', 'has', 'tensors']]","[['Experimental setup', 'To benefit from', 'efficiency of parallel computation']]",[],document_classification,16,143
819,experimental-setup,"For text preprocessing , we only convert all characters in the dataset to lowercase .","[('For', (0, 1)), ('convert', (6, 7)), ('in', (9, 10)), ('to', (12, 13))]","[('text preprocessing', (1, 3)), ('all characters', (7, 9)), ('dataset', (11, 12)), ('lowercase', (13, 14))]","[['text preprocessing', 'convert', 'all characters'], ['all characters', 'in', 'dataset'], ['all characters', 'to', 'lowercase'], ['dataset', 'to', 'lowercase']]",[],"[['Experimental setup', 'For', 'text preprocessing']]",[],document_classification,16,144
820,experimental-setup,"For SST , we conduct hyperparameter ( number of filters , filter length in CNN ; memory dimension in LSTM ; dropout rate and which layer to apply , etc. ) tuning on the validation data in the standard split .","[('For', (0, 1)), ('conduct', (4, 5)), ('on', (32, 33)), ('in', (36, 37))]","[('SST', (1, 2)), ('hyperparameter ( number of filters , filter length in CNN ;', (5, 16)), ('memory dimension in LSTM ;', (16, 21)), ('dropout rate and which layer to apply , etc. ) tuning', (21, 32)), ('validation data', (34, 36)), ('standard split', (38, 40))]","[['SST', 'conduct', 'hyperparameter ( number of filters , filter length in CNN ;'], ['dropout rate and which layer to apply , etc. ) tuning', 'on', 'validation data'], ['validation data', 'in', 'standard split']]","[['hyperparameter ( number of filters , filter length in CNN ;', 'has', 'memory dimension in LSTM ;']]","[['Experimental setup', 'For', 'SST']]",[],document_classification,16,145
821,experimental-setup,"For TREC , we holdout 1000 samples from the training dataset for hyperparameter search and train the model using the remaining data .","[('For', (0, 1)), ('holdout', (4, 5)), ('from', (7, 8)), ('for', (11, 12)), ('train', (15, 16)), ('using', (18, 19))]","[('TREC', (1, 2)), ('1000 samples', (5, 7)), ('training dataset', (9, 11)), ('hyperparameter search', (12, 14)), ('model', (17, 18)), ('remaining data', (20, 22))]","[['training dataset', 'For', 'hyperparameter search'], ['TREC', 'holdout', '1000 samples'], ['1000 samples', 'from', 'training dataset'], ['1000 samples', 'for', 'hyperparameter search'], ['training dataset', 'for', 'hyperparameter search'], ['TREC', 'train', 'model'], ['model', 'using', 'remaining data']]",[],"[['Experimental setup', 'For', 'TREC']]",[],document_classification,16,146
822,experimental-setup,"In our final settings , we only use one convolutional layer and one LSTM layer for both tasks .","[('use', (7, 8)), ('for', (15, 16))]","[('one convolutional layer', (8, 11)), ('both tasks', (16, 18))]",[],[],"[['Experimental setup', 'use', 'one convolutional layer']]",[],document_classification,16,147
823,experimental-setup,"For the filter size , we investigated filter lengths of 2 , 3 and 4 in two cases : a ) single convolutional layer with the same filter length , and b ) multiple convolutional layers with different lengths of filters in parallel .","[('For', (0, 1)), ('investigated', (6, 7)), ('of', (9, 10)), ('in', (15, 16)), ('with', (24, 25)), ('with', (36, 37))]","[('filter size', (2, 4)), ('filter lengths', (7, 9)), ('2 , 3 and 4', (10, 15)), ('two cases', (16, 18)), ('single convolutional layer', (21, 24)), ('same filter length', (26, 29)), ('multiple convolutional layers', (33, 36)), ('different lengths of filters in parallel', (37, 43))]","[['filter size', 'investigated', 'filter lengths'], ['filter lengths', 'of', '2 , 3 and 4'], ['2 , 3 and 4', 'in', 'two cases'], ['single convolutional layer', 'with', 'same filter length'], ['multiple convolutional layers', 'with', 'different lengths of filters in parallel']]","[['two cases', 'name', 'single convolutional layer']]","[['Experimental setup', 'For', 'filter size']]",[],document_classification,16,148
824,baselines,Binary is a 2 - classification task .,"[('is', (1, 2))]","[('Binary', (0, 1)), ('2 - classification task', (3, 7))]","[['Binary', 'is', '2 - classification task']]","[['Binary', 'has', '2 - classification task']]",[],"[['Baselines', 'has', 'Binary']]",document_classification,16,153
825,baselines,The third block are methods related to convolutional neural networks .,"[('related to', (5, 7))]","[('methods', (4, 5)), ('convolutional neural networks', (7, 10))]","[['methods', 'related to', 'convolutional neural networks']]",[],[],"[['Baselines', 'has', 'methods']]",document_classification,16,155
826,experimental-setup,The last block is our model .,[],"[('our model', (4, 6))]",[],[],[],"[['Experimental setup', 'has', 'our model']]",document_classification,16,158
827,baselines,features after convolution and the sequence of window representations is fed into LSTM .,"[('of', (6, 7)), ('fed into', (10, 12))]","[('features after convolution and', (0, 4)), ('sequence', (5, 6)), ('window representations', (7, 9)), ('LSTM', (12, 13))]","[['sequence', 'of', 'window representations'], ['window representations', 'fed into', 'LSTM']]","[['features after convolution and', 'has', 'sequence']]",[],"[['Baselines', 'has', 'features after convolution and']]",document_classification,16,159
828,experimental-setup,We also exploit different combinations of different filter lengths .,"[('exploit', (2, 3)), ('of', (5, 6))]","[('different combinations', (3, 5)), ('different filter lengths', (6, 9))]","[['different combinations', 'of', 'different filter lengths']]",[],"[['Experimental setup', 'exploit', 'different combinations']]",[],document_classification,16,162
829,experimental-setup,"According to the experiments , we choose a single convolutional layer with filter length","[('choose', (6, 7)), ('with', (11, 12))]","[('single convolutional layer', (8, 11))]",[],[],"[['Experimental setup', 'choose', 'single convolutional layer']]",[],document_classification,16,164
830,experimental-setup,"3 . For SST , the number of filters of length 3 is set to be 150 and the memory dimension of LSTM is set to be 150 , too .","[('of', (9, 10)), ('set to', (13, 15)), ('of', (21, 22)), ('set to', (24, 26))]","[('SST', (3, 4)), ('number of filters', (6, 9)), ('length 3', (10, 12)), ('150', (16, 17)), ('memory dimension', (19, 21)), ('LSTM', (22, 23)), ('150', (27, 28))]","[['number of filters', 'of', 'length 3'], ['memory dimension', 'of', 'LSTM'], ['number of filters', 'set to', '150'], ['memory dimension', 'of', 'LSTM'], ['memory dimension', 'set to', '150'], ['LSTM', 'set to', '150']]","[['SST', 'has', 'number of filters']]",[],[],document_classification,16,165
831,experimental-setup,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,"[('of', (13, 14))]","[('word vector layer and the LSTM layer', (1, 8)), ('dropped outwith', (9, 11)), ('probability', (12, 13)), ('0.5', (14, 15))]","[['probability', 'of', '0.5']]","[['word vector layer and the LSTM layer', 'has', 'dropped outwith'], ['dropped outwith', 'has', 'probability']]",[],"[['Experimental setup', 'has', 'word vector layer and the LSTM layer']]",document_classification,16,166
832,experimental-setup,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .","[('For', (0, 1)), ('set to', (8, 10)), ('set to', (17, 19))]","[('TREC', (1, 2)), ('number of filters', (4, 7)), ('300', (11, 12)), ('memory dimension', (14, 16)), ('300', (20, 21))]","[['number of filters', 'set to', '300'], ['memory dimension', 'set to', '300']]","[['TREC', 'has', 'number of filters'], ['number of filters', 'has', '300'], ['memory dimension', 'has', '300']]","[['Experimental setup', 'For', 'TREC']]",[],document_classification,16,167
833,experimental-setup,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,"[('of', (13, 14))]","[('word vector layer and the LSTM layer', (1, 8)), ('dropped outwith', (9, 11)), ('probability', (12, 13)), ('0.5', (14, 15))]","[['probability', 'of', '0.5']]","[['word vector layer and the LSTM layer', 'has', 'dropped outwith'], ['dropped outwith', 'has', 'probability']]",[],"[['Experimental setup', 'has', 'word vector layer and the LSTM layer']]",document_classification,16,168
834,experimental-setup,We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,"[('add', (2, 3)), ('with', (5, 6)), ('of', (8, 9)), ('to', (10, 11)), ('in', (13, 14))]","[('L2 regularization', (3, 5)), ('factor', (7, 8)), ('0.001', (9, 10)), ('weights', (12, 13)), ('softmax layer', (15, 17))]","[['L2 regularization', 'with', 'factor'], ['factor', 'of', '0.001'], ['0.001', 'to', 'weights'], ['weights', 'in', 'softmax layer']]","[['L2 regularization', 'has', 'factor']]","[['Experimental setup', 'add', 'L2 regularization']]",[],document_classification,16,169
835,results,"For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .","[('For', (0, 1)), ('achieve', (7, 8)), ('with respect to', (10, 13))]","[('binary classification task', (2, 5)), ('comparable results', (8, 10)), ('state - of - the - art ones', (14, 22))]","[['binary classification task', 'achieve', 'comparable results'], ['comparable results', 'with respect to', 'state - of - the - art ones']]",[],"[['Results', 'For', 'binary classification task']]",[],document_classification,16,185
836,results,( 2 ) Comparing our results against single CNN and LSTM models shows that LSTM does learn long - term dependencies across sequences of higher - level representations better .,"[('Comparing', (3, 4)), ('against', (6, 7)), ('shows', (12, 13)), ('across', (21, 22)), ('of', (23, 24))]","[('single CNN and LSTM models', (7, 12)), ('LSTM', (14, 15)), ('long - term dependencies', (17, 21)), ('sequences', (22, 23)), ('higher - level representations better', (24, 29))]","[['single CNN and LSTM models', 'shows', 'LSTM'], ['long - term dependencies', 'across', 'sequences'], ['sequences', 'of', 'higher - level representations better']]",[],"[['Results', 'Comparing', 'single CNN and LSTM models']]",[],document_classification,16,189
837,results,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .","[('consistently outperforms', (5, 7))]","[('Our result', (3, 5)), ('all published neural baseline models', (7, 12))]","[['Our result', 'consistently outperforms', 'all published neural baseline models']]","[['Our result', 'has', 'all published neural baseline models']]",[],"[['Results', 'has', 'Our result']]",document_classification,16,198
838,results,( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,"[('close to', (6, 8)), ('depends on', (20, 22))]","[('Our result', (3, 5)), ('state - of - the - art SVM', (11, 19)), ('highly engineered features', (22, 25))]","[['Our result', 'close to', 'state - of - the - art SVM'], ['state - of - the - art SVM', 'depends on', 'highly engineered features']]",[],[],"[['Results', 'has', 'Our result']]",document_classification,16,199
839,ablation-analysis,Here we investigate the impact of different filter configurations in the convolutional layer on the model performance .,"[('investigate', (2, 3)), ('of', (5, 6)), ('in', (9, 10)), ('on', (13, 14))]","[('impact', (4, 5)), ('different filter configurations', (6, 9)), ('convolutional layer', (11, 13)), ('model performance', (15, 17))]","[['impact', 'of', 'different filter configurations'], ['different filter configurations', 'in', 'convolutional layer'], ['different filter configurations', 'on', 'model performance'], ['convolutional layer', 'on', 'model performance']]","[['impact', 'has', 'different filter configurations']]","[['Ablation analysis', 'investigate', 'impact']]",[],document_classification,16,203
840,ablation-analysis,"For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .","[('For the case of', (0, 4)), ('shown that', (12, 14)), ('with', (16, 17)), ('performs', (20, 21)), ('that', (22, 23))]","[('multiple convolutional layers', (4, 7)), ('filter configurations', (14, 16)), ('filter length 3', (17, 20)), ('better', (21, 22))]","[['multiple convolutional layers', 'shown that', 'filter configurations'], ['filter configurations', 'with', 'filter length 3'], ['filter configurations', 'performs', 'better']]","[['multiple convolutional layers', 'has', 'filter configurations']]","[['Ablation analysis', 'For the case of', 'multiple convolutional layers']]",[],document_classification,16,211
841,research-problem,Very Deep Convolutional Networks for Text Classification,[],[],[],[],[],[],document_classification,17,2
842,research-problem,We present a new architecture ( VD - CNN ) for text processing which operates directly at the character level and uses only small convolutions and pooling operations .,"[('operates directly at', (14, 17)), ('uses', (21, 22))]","[('text processing', (11, 13)), ('character level', (18, 20)), ('pooling operations', (26, 28))]","[['text processing', 'operates directly at', 'character level']]",[],[],[],document_classification,17,6
843,model,The fundamental idea of is to consider feature extraction and classification as one jointly trained task .,"[('consider', (6, 7)), ('as', (11, 12))]","[('feature extraction and classification', (7, 11))]",[],[],"[['Model', 'consider', 'feature extraction and classification']]",[],document_classification,17,17
844,model,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .","[('of', (10, 11)), ('to approach', (14, 16)), ('using', (19, 20))]","[('deep architectures', (8, 10)), ('many convolutional layers', (11, 14)), ('up to 29 layers', (20, 24))]","[['deep architectures', 'of', 'many convolutional layers']]",[],[],[],document_classification,17,35
845,experiments,The proposed deep convolutional network shows significantly better results than previous ConvNets approach .,"[('shows', (5, 6)), ('than', (9, 10))]","[('proposed deep convolutional network', (1, 5)), ('significantly better results', (6, 9)), ('previous ConvNets approach', (10, 13))]","[['proposed deep convolutional network', 'shows', 'significantly better results'], ['significantly better results', 'than', 'previous ConvNets approach']]","[['proposed deep convolutional network', 'has', 'significantly better results']]",[],[],document_classification,17,43
846,results,Going from depth 9 to 17 and 29 for Amazon Full reduces the error rate by 1 % absolute .,"[('Going from', (0, 2)), ('for', (8, 9)), ('reduces', (11, 12)), ('by', (15, 16))]","[('depth 9 to 17 and 29', (2, 8)), ('Amazon Full', (9, 11)), ('error rate', (13, 15)), ('1 % absolute', (16, 19))]","[['depth 9 to 17 and 29', 'for', 'Amazon Full'], ['depth 9 to 17 and 29', 'reduces', 'error rate'], ['Amazon Full', 'reduces', 'error rate'], ['error rate', 'by', '1 % absolute']]",[],"[['Results', 'Going from', 'depth 9 to 17 and 29']]",[],document_classification,17,138
847,results,"Overall , compared to previous state - of - the - art , our best architecture with depth 29 and max - pooling has a test error of 37.0 compared to 40.43 % .","[('compared to', (2, 4)), ('with', (16, 17)), ('of', (27, 28)), ('compared to', (29, 31))]","[('previous state - of - the - art', (4, 12)), ('our best architecture', (13, 16)), ('depth 29 and max - pooling', (17, 23)), ('test error', (25, 27)), ('37.0', (28, 29)), ('40.43 %', (31, 33))]","[['our best architecture', 'with', 'depth 29 and max - pooling'], ['test error', 'of', '37.0'], ['37.0', 'compared to', '40.43 %']]","[['previous state - of - the - art', 'has', 'our best architecture']]",[],[],document_classification,17,141
848,experiments,Max - pooling performs better than other pooling types .,"[('performs', (3, 4)), ('than', (5, 6))]","[('Max - pooling', (0, 3)), ('better', (4, 5)), ('other pooling types', (6, 9))]","[['Max - pooling', 'performs', 'better'], ['better', 'than', 'other pooling types']]",[],[],[],document_classification,17,144
849,experiments,Our models outperform state - of - the - art Con -vNets .,[],"[('outperform', (2, 3)), ('state - of - the - art Con -vNets', (3, 12))]",[],"[['outperform', 'has', 'state - of - the - art Con -vNets']]",[],[],document_classification,17,148
850,experiments,"When using shortcut connections , we observe improved results when the network has 49 layers : both the training and test errors go down and the network is less prone to underfitting than it was without shortcut connections .","[('using', (1, 2)), ('observe', (6, 7)), ('when', (9, 10)), ('go', (22, 23))]","[('shortcut connections', (2, 4)), ('improved results', (7, 9)), ('network', (11, 12)), ('49 layers', (13, 15)), ('training and test errors', (18, 22)), ('down', (23, 24)), ('underfitting', (31, 32))]","[['shortcut connections', 'observe', 'improved results'], ['improved results', 'when', 'network'], ['training and test errors', 'go', 'down']]","[['shortcut connections', 'has', 'improved results'], ['network', 'has', '49 layers'], ['training and test errors', 'has', 'down']]",[],[],document_classification,17,167
851,baselines,"Following , all processing is done at the character level which is the atomic representation of a sentence , same as pixels for images .","[('done at', (5, 7)), ('which is', (10, 12)), ('of', (15, 16))]","[('all processing', (2, 4)), ('character level', (8, 10)), ('atomic representation', (13, 15)), ('sentence', (17, 18))]","[['all processing', 'done at', 'character level'], ['character level', 'which is', 'atomic representation'], ['atomic representation', 'of', 'sentence']]",[],[],"[['Baselines', 'has', 'all processing']]",document_classification,17,195
852,experimental-setup,The character embedding is of size 16 .,"[('of size', (4, 6))]","[('character embedding', (1, 3)), ('16', (6, 7))]","[['character embedding', 'of size', '16']]","[['character embedding', 'has', '16']]",[],"[['Experimental setup', 'has', 'character embedding']]",document_classification,17,198
853,experimental-setup,"Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .","[('performed with', (2, 4)), ('using', (6, 7)), ('of size', (9, 11))]","[('Training', (0, 1)), ('SGD', (4, 5)), ('mini-batch', (8, 9)), ('128', (11, 12)), ('initial learning rate', (14, 17)), ('0.01', (18, 19)), ('momentum', (20, 21)), ('0.9', (22, 23))]","[['Training', 'performed with', 'SGD'], ['SGD', 'using', 'mini-batch'], ['SGD', 'using', 'momentum'], ['mini-batch', 'of size', '128'], ['initial learning rate', 'of size', '0.01']]","[['initial learning rate', 'has', '0.01'], ['momentum', 'has', '0.9']]",[],"[['Experimental setup', 'has', 'Training']]",document_classification,17,199
854,experimental-setup,We initialize our convolutional layers following .,"[('initialize', (1, 2))]","[('convolutional layers', (3, 5))]",[],[],"[['Experimental setup', 'initialize', 'convolutional layers']]",[],document_classification,17,201
855,experimental-setup,The implementation is done using Torch 7 .,"[('done using', (3, 5))]","[('Torch', (5, 6))]",[],[],"[['Experimental setup', 'done using', 'Torch']]",[],document_classification,17,204
856,experimental-setup,All experiments are performed on a single NVidia K40 GPU .,"[('performed on', (3, 5))]","[('single NVidia K40 GPU', (6, 10))]",[],[],"[['Experimental setup', 'performed on', 'single NVidia K40 GPU']]",[],document_classification,17,205
857,experimental-setup,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .","[('use', (13, 14)), ('without', (17, 18))]","[('temporal batch norm', (14, 17)), ('dropout', (18, 19))]","[['temporal batch norm', 'without', 'dropout']]",[],"[['Experimental setup', 'use', 'temporal batch norm']]",[],document_classification,17,206
858,results,"Our deep architecture works well on big data sets in particular , even for small depths .","[('works', (3, 4)), ('on', (5, 6)), ('for', (13, 14))]","[('Our deep architecture', (0, 3)), ('well', (4, 5)), ('big data sets', (6, 9)), ('small depths', (14, 16))]","[['Our deep architecture', 'works', 'well'], ['well', 'on', 'big data sets'], ['well', 'for', 'small depths'], ['big data sets', 'for', 'small depths']]","[['Our deep architecture', 'has', 'well']]",[],"[['Results', 'has', 'Our deep architecture']]",document_classification,17,210
859,results,"For the smallest depth we use ( 9 convolutional layers ) , we see that our model already performs better than Zhang 's convolutional baselines ( which includes 6 convolutional layers and has a different architecture ) on the biggest data sets :","[('For', (0, 1)), ('see that', (13, 15)), ('performs', (18, 19)), ('than', (20, 21)), ('on', (37, 38))]","[('smallest depth', (2, 4)), ('our model', (15, 17)), ('better', (19, 20)), (""Zhang 's convolutional baselines"", (21, 25)), ('biggest data sets', (39, 42))]","[['our model', 'performs', 'better'], ['better', 'than', ""Zhang 's convolutional baselines""]]","[['smallest depth', 'has', 'our model']]",[],[],document_classification,17,212
860,results,"Yelp Full , Yahoo Answers and Amazon Full and Polarity .",[],"[('Yelp Full', (0, 2)), ('Amazon Full and Polarity', (6, 10))]",[],[],[],[],document_classification,17,213
861,results,The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples . :,"[('in', (4, 5)), ('observed on', (9, 11)), ('which', (17, 18))]","[('most important decrease', (1, 4)), ('classification error', (5, 7)), ('largest data set Amazon Full', (12, 17)), ('more than 3 Million training samples', (19, 25))]","[['most important decrease', 'in', 'classification error'], ['most important decrease', 'observed on', 'largest data set Amazon Full'], ['classification error', 'observed on', 'largest data set Amazon Full']]",[],[],"[['Results', 'has', 'most important decrease']]",document_classification,17,214
862,results,"We also observe that for a small depth , temporal max - pooling works best on all data sets .","[('observe', (2, 3)), ('for', (4, 5)), ('works', (13, 14)), ('on', (15, 16))]","[('small depth', (6, 8)), ('temporal max - pooling', (9, 13)), ('best', (14, 15)), ('all data sets', (16, 19))]","[['temporal max - pooling', 'works', 'best'], ['best', 'on', 'all data sets']]","[['small depth', 'has', 'temporal max - pooling']]","[['Results', 'observe', 'small depth']]",[],document_classification,17,217
863,research-problem,Character - level Convolutional Networks for Text Classification,[],[],[],[],[],[],document_classification,18,2
864,research-problem,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",[],"[('Text classification', (0, 2))]",[],[],[],[],document_classification,18,13
865,model,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .","[('explore treating', (4, 6)), ('as', (7, 8)), ('at', (13, 14)), ('applying', (18, 19))]","[('raw signal', (11, 13)), ('character level', (14, 16)), ('temporal ( one-dimensional ) ConvNets', (19, 24))]","[['raw signal', 'at', 'character level']]",[],"[['Model', 'explore treating', 'raw signal']]",[],document_classification,18,18
866,model,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .","[('is', (2, 3)), ('where', (5, 6)), ('obtained by', (9, 11)), ('to perform', (14, 16))]","[('design', (1, 2)), ('modular', (3, 4)), ('gradients', (7, 8)), ('back - propagation', (11, 14)), ('optimization', (16, 17))]","[['design', 'is', 'modular'], ['design', 'where', 'gradients'], ['modular', 'where', 'gradients'], ['gradients', 'obtained by', 'back - propagation'], ['back - propagation', 'to perform', 'optimization']]","[['design', 'has', 'modular']]",[],"[['Model', 'has', 'design']]",document_classification,18,30
867,experiments,The dimension of the embedding is 300 .,"[('of', (2, 3)), ('is', (5, 6))]","[('dimension', (1, 2)), ('embedding', (4, 5)), ('300', (6, 7))]","[['dimension', 'of', 'embedding'], ['dimension', 'is', '300'], ['embedding', 'is', '300']]",[],[],[],document_classification,18,118
868,experiments,The number of means is 5000 .,"[('is', (4, 5))]","[('number of means', (1, 4)), ('5000', (5, 6))]","[['number of means', 'is', '5000']]","[['number of means', 'has', '5000']]",[],[],document_classification,18,120
869,research-problem,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling,[],"[('Text Classification', (0, 2))]",[],[],[],[],document_classification,19,2
870,model,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .","[('proposes', (5, 6)), ('to capture', (24, 26)), ('on both', (27, 29))]","[('Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )', (6, 24)), ('features', (26, 27)), ('time - step dimension', (30, 34)), ('feature vector dimension', (36, 39))]","[['Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )', 'to capture', 'features'], ['features', 'on both', 'time - step dimension'], ['features', 'on both', 'feature vector dimension']]",[],"[['Model', 'proposes', 'Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )']]",[],document_classification,19,31
871,model,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,"[('utilizes', (2, 3)), ('to transform', (13, 15)), ('into', (17, 18))]","[('Bidirectional Long Short - Term Memory Networks ( BLSTM )', (3, 13)), ('text', (16, 17)), ('vectors', (18, 19))]","[['Bidirectional Long Short - Term Memory Networks ( BLSTM )', 'to transform', 'text'], ['text', 'into', 'vectors']]",[],"[['Model', 'utilizes', 'Bidirectional Long Short - Term Memory Networks ( BLSTM )']]",[],document_classification,19,32
872,model,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,"[('utilized to obtain', (7, 10))]","[('2D max pooling operation', (2, 6)), ('fixed - length vector', (11, 15))]","[['2D max pooling operation', 'utilized to obtain', 'fixed - length vector']]",[],[],[],document_classification,19,33
873,model,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,"[('applies', (3, 4)), ('to capture', (11, 13)), ('to represent', (16, 18))]","[('2D convolution ( BLSTM - 2DCNN )', (4, 11)), ('more meaningful features', (13, 16)), ('input text', (19, 21))]","[['2D convolution ( BLSTM - 2DCNN )', 'to capture', 'more meaningful features'], ['more meaningful features', 'to represent', 'input text']]",[],"[['Model', 'applies', '2D convolution ( BLSTM - 2DCNN )']]",[],document_classification,19,34
874,model,"This paper proposes a combined framework , which utilizes BLSTM to capture long - term sentence dependencies , and extracts features by 2D convolution and 2D max pooling operation for sequence modeling tasks .","[('proposes', (2, 3)), ('utilizes', (8, 9)), ('to capture', (10, 12)), ('extracts', (19, 20)), ('for', (29, 30))]","[('combined framework', (4, 6)), ('BLSTM', (9, 10)), ('long - term sentence dependencies', (12, 17)), ('features', (20, 21)), ('2D convolution and 2D max pooling operation', (22, 29)), ('sequence modeling tasks', (30, 33))]","[['combined framework', 'utilizes', 'BLSTM'], ['BLSTM', 'to capture', 'long - term sentence dependencies'], ['combined framework', 'extracts', 'features'], ['2D convolution and 2D max pooling operation', 'for', 'sequence modeling tasks']]",[],"[['Model', 'proposes', 'combined framework']]",[],document_classification,19,36
875,model,"This work introduces two combined models BLSTM - 2DPooling and BLSTM - 2DCNN , and verifies them on six text classification tasks , including sentiment analysis , question classification , subjectivity classification , and newsgroups classification .","[('introduces', (2, 3)), ('including', (23, 24))]","[('two combined models', (3, 6)), ('BLSTM - 2DPooling', (6, 9)), ('six text classification tasks', (18, 22)), ('sentiment analysis', (24, 26)), ('question classification', (27, 29)), ('subjectivity classification', (30, 32)), ('newsgroups classification', (34, 36))]","[['six text classification tasks', 'including', 'sentiment analysis'], ['six text classification tasks', 'including', 'question classification'], ['six text classification tasks', 'including', 'subjectivity classification'], ['six text classification tasks', 'including', 'newsgroups classification']]","[['two combined models', 'name', 'BLSTM - 2DPooling']]","[['Model', 'introduces', 'two combined models']]",[],document_classification,19,38
876,hyperparameters,"The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .","[('of', (2, 3)), ('is', (5, 6)), ('of', (11, 12)), ('is', (13, 14))]","[('dimension', (1, 2)), ('word embeddings', (3, 5)), ('300', (6, 7)), ('hidden units', (9, 11)), ('LSTM', (12, 13)), ('300', (14, 15))]","[['dimension', 'of', 'word embeddings'], ['hidden units', 'of', 'LSTM'], ['dimension', 'is', '300'], ['word embeddings', 'is', '300'], ['LSTM', 'is', '300'], ['hidden units', 'of', 'LSTM'], ['hidden units', 'is', '300'], ['LSTM', 'is', '300']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",document_classification,19,170
877,hyperparameters,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .","[('use', (1, 2)), ('for', (6, 7))]","[('100 convolutional filters each', (2, 6)), ('window sizes', (7, 9)), ('( 3 , 3 )', (10, 15)), ('2D pooling size', (16, 19)), ('( 2 , 2 )', (20, 25))]","[['100 convolutional filters each', 'for', 'window sizes']]",[],"[['Hyperparameters', 'use', '100 convolutional filters each']]",[],document_classification,19,171
878,hyperparameters,We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,"[('set', (1, 2)), ('as', (5, 6)), ('of', (11, 12)), ('as', (13, 14))]","[('mini-batch size', (3, 5)), ('10', (6, 7)), ('learning rate', (9, 11)), ('AdaDelta', (12, 13)), ('default value 1.0', (15, 18))]","[['mini-batch size', 'as', '10'], ['learning rate', 'of', 'AdaDelta'], ['learning rate', 'as', 'default value 1.0'], ['AdaDelta', 'as', 'default value 1.0']]","[['mini-batch size', 'has', '10']]","[['Hyperparameters', 'set', 'mini-batch size']]",[],document_classification,19,172
879,hyperparameters,"For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .","[('For', (0, 1)), ('employ', (4, 5)), ('with', (7, 8)), ('of', (10, 11)), ('for', (12, 13)), ('use', (31, 32)), ('with', (35, 36)), ('over', (40, 41))]","[('regularization', (1, 2)), ('Dropout operation', (5, 7)), ('dropout rate', (8, 10)), ('0.5', (11, 12)), ('word embeddings', (14, 16)), ('0.2', (17, 18)), ('BLSTM layer', (20, 22)), ('0.4', (23, 24)), ('penultimate layer', (26, 28)), ('l 2 penalty', (32, 35)), ('coefficient 10 ? 5', (36, 40)), ('parameters', (42, 43))]","[['0.5', 'For', 'word embeddings'], ['0.2', 'For', 'BLSTM layer'], ['regularization', 'employ', 'Dropout operation'], ['Dropout operation', 'with', 'dropout rate'], ['dropout rate', 'of', '0.5'], ['0.5', 'for', 'word embeddings'], ['l 2 penalty', 'with', 'coefficient 10 ? 5'], ['coefficient 10 ? 5', 'over', 'parameters']]",[],"[['Hyperparameters', 'For', 'regularization']]",[],document_classification,19,173
880,hyperparameters,These values are chosen via a grid search on the SST - 1 development set .,"[('chosen via', (3, 5)), ('on', (8, 9))]","[('grid search', (6, 8)), ('SST - 1 development set', (10, 15))]","[['grid search', 'on', 'SST - 1 development set']]",[],"[['Hyperparameters', 'chosen via', 'grid search']]",[],document_classification,19,174
881,hyperparameters,"We only tune these hyperparameters , and more finer tuning , such as using different numbers of hidden units of LSTM layer , or using wide convolution , may further improve the performance .","[('tune', (2, 3)), ('such as using', (11, 14)), ('of', (19, 20))]","[('hyperparameters', (4, 5)), ('more finer tuning', (7, 10)), ('different numbers of hidden units', (14, 19)), ('LSTM layer', (20, 22)), ('wide convolution', (25, 27)), ('performance', (32, 33))]","[['more finer tuning', 'such as using', 'different numbers of hidden units'], ['more finer tuning', 'such as using', 'wide convolution'], ['different numbers of hidden units', 'of', 'LSTM layer']]",[],"[['Hyperparameters', 'tune', 'hyperparameters']]",[],document_classification,19,175
882,results,"This work implements four models , BLSTM , BLSTM - Att , BLSTM - 2DPooling , and BLSTM - 2DCNN . presents the performance of the four models and other state - of - the - art models on six classification tasks .","[('implements', (2, 3))]","[('four models', (3, 5)), ('BLSTM', (6, 7)), ('BLSTM - Att', (8, 11)), ('BLSTM - 2DPooling', (12, 15)), ('BLSTM - 2DCNN', (17, 20))]",[],[],"[['Results', 'implements', 'four models']]",[],document_classification,19,178
883,results,The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,"[('achieves', (5, 6)), ('on', (8, 9))]","[('BLSTM - 2DCNN model', (1, 5)), ('excellent performance', (6, 8)), ('4 out of 6 tasks', (9, 14))]","[['BLSTM - 2DCNN model', 'achieves', 'excellent performance'], ['excellent performance', 'on', '4 out of 6 tasks']]",[],[],"[['Results', 'has', 'BLSTM - 2DCNN model']]",document_classification,19,179
884,results,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .","[('achieves', (3, 4)), ('on', (11, 12))]","[('52.4 % and 89.5 % test accuracies', (4, 11)), ('SST - 1 and SST - 2', (12, 19))]","[['52.4 % and 89.5 % test accuracies', 'on', 'SST - 1 and SST - 2']]",[],[],[],document_classification,19,180
885,results,BLSTM - 2DPooling performs worse than the state - of - the - art models .,"[('performs', (3, 4)), ('than', (5, 6))]","[('BLSTM - 2DPooling', (0, 3)), ('worse', (4, 5)), ('state - of - the - art models', (7, 15))]","[['BLSTM - 2DPooling', 'performs', 'worse'], ['worse', 'than', 'state - of - the - art models']]",[],[],"[['Results', 'has', 'BLSTM - 2DPooling']]",document_classification,19,181
886,results,"BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .","[('beats', (3, 4)), ('on', (6, 7))]","[('BLSTM - CNN', (0, 3)), ('all baselines', (4, 6)), ('SST - 1 ,', (7, 11)), ('SST - 2', (11, 14)), ('TREC datasets', (16, 18))]","[['BLSTM - CNN', 'beats', 'all baselines'], ['all baselines', 'on', 'SST - 1 ,'], ['all baselines', 'on', 'TREC datasets']]",[],[],"[['Results', 'has', 'BLSTM - CNN']]",document_classification,19,183
887,results,"As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .","[('As for', (0, 2)), ('gets', (10, 11))]","[('Subj and MR datasets', (2, 6)), ('BLSTM - 2DCNN', (7, 10)), ('second higher accuracies', (12, 15))]","[['BLSTM - 2DCNN', 'gets', 'second higher accuracies']]","[['Subj and MR datasets', 'has', 'BLSTM - 2DCNN'], ['BLSTM - 2DCNN', 'has', 'second higher accuracies']]","[['Results', 'As for', 'Subj and MR datasets']]",[],document_classification,19,184
888,results,"Compared with RCNN , BLSTM - 2DCNN achieves a comparable result .","[('Compared with', (0, 2)), ('achieves', (7, 8))]","[('RCNN', (2, 3)), ('BLSTM - 2DCNN', (4, 7)), ('comparable result', (9, 11))]","[['BLSTM - 2DCNN', 'achieves', 'comparable result']]","[['RCNN', 'has', 'BLSTM - 2DCNN']]","[['Results', 'Compared with', 'RCNN']]",[],document_classification,19,188
889,results,"BLSTM-2DCNN is an extension of BLSTM - 2DPooling , and the results show that BLSTM - 2DCNN can capture more dependencies in text .","[('extension of', (3, 5)), ('show', (12, 13)), ('capture', (18, 19)), ('in', (21, 22))]","[('BLSTM-2DCNN', (0, 1)), ('BLSTM - 2DPooling', (5, 8)), ('BLSTM -', (14, 16)), ('more dependencies', (19, 21)), ('text', (22, 23))]","[['BLSTM-2DCNN', 'extension of', 'BLSTM - 2DPooling'], ['BLSTM -', 'capture', 'more dependencies'], ['more dependencies', 'in', 'text']]",[],[],"[['Results', 'has', 'BLSTM-2DCNN']]",document_classification,19,192
890,results,"Ada Sent utilizes a more complicated model to form a hierarchy of representations , and it outperforms BLSTM - 2DCNN on Subj and MR datasets .","[('utilizes', (2, 3)), ('to form', (7, 9)), ('of', (11, 12)), ('on', (20, 21))]","[('Ada Sent', (0, 2)), ('more complicated model', (4, 7)), ('hierarchy', (10, 11)), ('outperforms', (16, 17)), ('BLSTM - 2DCNN', (17, 20)), ('Subj and MR datasets', (21, 25))]","[['Ada Sent', 'utilizes', 'more complicated model'], ['more complicated model', 'to form', 'hierarchy'], ['BLSTM - 2DCNN', 'on', 'Subj and MR datasets']]","[['hierarchy', 'has', 'outperforms'], ['outperforms', 'has', 'BLSTM - 2DCNN']]",[],"[['Results', 'has', 'Ada Sent']]",document_classification,19,193
891,results,DRNN : Deep recursive neural networks for compositionality in language .,"[('for', (6, 7))]","[('DRNN', (0, 1)), ('Deep recursive neural networks', (2, 6)), ('compositionality in language', (7, 10))]","[['Deep recursive neural networks', 'for', 'compositionality in language']]","[['DRNN', 'has', 'Deep recursive neural networks']]",[],"[['Results', 'has', 'DRNN']]",document_classification,19,198
892,baselines,CNN -nonstatic / MC : Convolutional neural networks for sentence classification .,"[('for', (8, 9))]","[('CNN -nonstatic / MC', (0, 4)), ('Convolutional neural networks', (5, 8)), ('sentence classification', (9, 11))]","[['Convolutional neural networks', 'for', 'sentence classification']]","[['CNN -nonstatic / MC', 'has', 'Convolutional neural networks']]",[],"[['Baselines', 'has', 'CNN -nonstatic / MC']]",document_classification,19,200
893,baselines,"Molding - CNN : Molding CNNs for text : non-linear , non-consecutive convolutions .",[],"[('Molding - CNN', (0, 3)), ('Molding CNNs for', (4, 7)), ('text', (7, 8)), ('non-linear', (9, 10))]",[],"[['Molding - CNN', 'has', 'Molding CNNs for'], ['Molding CNNs for', 'has', 'text']]",[],"[['Baselines', 'has', 'Molding - CNN']]",document_classification,19,202
894,baselines,"MVCNN : Multichannel variable - size convolution for sentence classification ( Yin and Schtze , 2016 ) .","[('for', (7, 8))]","[('MVCNN', (0, 1)), ('Multichannel variable - size convolution', (2, 7)), ('sentence classification', (8, 10))]","[['Multichannel variable - size convolution', 'for', 'sentence classification']]","[['MVCNN', 'has', 'Multichannel variable - size convolution']]",[],"[['Baselines', 'has', 'MVCNN']]",document_classification,19,204
895,baselines,RCNN : Recurrent Convolutional Neural Networks for Text Classification .,[],"[('Text Classification', (7, 9))]",[],[],[],[],document_classification,19,205
896,baselines,S- LSTM : Long short - term memory over recursive structures .,"[('over', (8, 9))]","[('S- LSTM', (0, 2)), ('Long short - term memory', (3, 8)), ('recursive structures', (9, 11))]","[['Long short - term memory', 'over', 'recursive structures']]","[['S- LSTM', 'has', 'Long short - term memory']]",[],"[['Baselines', 'has', 'S- LSTM']]",document_classification,19,206
897,baselines,LSTM / BLSTM / Tree-LSTM :,[],"[('LSTM / BLSTM / Tree-LSTM', (0, 5))]",[],[],[],"[['Baselines', 'has', 'LSTM / BLSTM / Tree-LSTM']]",document_classification,19,207
898,baselines,LSTMN : Long short - term memory - networks for machine reading .,"[('for', (9, 10))]","[('LSTMN', (0, 1)), ('Long short - term memory - networks', (2, 9)), ('machine reading', (10, 12))]","[['Long short - term memory - networks', 'for', 'machine reading']]","[['LSTMN', 'has', 'Long short - term memory - networks']]",[],"[['Baselines', 'has', 'LSTMN']]",document_classification,19,209
899,baselines,C - LSTM : A C - LSTM Neural Network for Text Classification .,[],"[('Text Classification', (11, 13))]",[],[],[],[],document_classification,19,217
900,results,Effect of Sentence Length,[],[],[],[],[],[],document_classification,19,221
901,results,It is found that both BLSTM - 2DPooling and BLSTM - 2DCNN outperform the other two models .,"[('found that', (2, 4))]","[('both BLSTM - 2DPooling and BLSTM - 2DCNN', (4, 12)), ('outperform', (12, 13)), ('other two models', (14, 17))]",[],"[['both BLSTM - 2DPooling and BLSTM - 2DCNN', 'has', 'outperform'], ['outperform', 'has', 'other two models']]","[['Results', 'found that', 'both BLSTM - 2DPooling and BLSTM - 2DCNN']]",[],document_classification,19,222
902,results,"The best accuracy is 52.6 with 2D filter size ( 5 , 5 ) and 2D max pooling size ( 5 , 5 ) , this shows that finer tuning can further improve the performance reported here .","[('is', (3, 4)), ('with', (5, 6))]","[('best accuracy', (1, 3)), ('52.6', (4, 5)), ('2D filter size ( 5 , 5 )', (6, 14)), ('2D max pooling size ( 5 , 5 )', (15, 24))]","[['best accuracy', 'is', '52.6'], ['best accuracy', 'is', '2D max pooling size ( 5 , 5 )'], ['52.6', 'with', '2D filter size ( 5 , 5 )'], ['52.6', 'with', '2D max pooling size ( 5 , 5 )']]","[['best accuracy', 'has', '52.6']]",[],"[['Results', 'has', 'best accuracy']]",document_classification,19,232
903,research-problem,Rethinking Complex Neural Network Architectures for Document Classification,[],[],[],[],[],[],document_classification,2,2
904,research-problem,"Surprisingly , our simple model is able to achieve these results without attention mechanisms .","[('able to achieve', (6, 9)), ('without', (11, 12))]","[('our simple model', (2, 5)), ('results', (10, 11)), ('attention mechanisms', (12, 14))]","[['our simple model', 'able to achieve', 'results'], ['results', 'without', 'attention mechanisms']]","[['our simple model', 'has', 'results']]",[],[],document_classification,2,7
905,model,Our work provides an opensource platform and the foundation for future work in document classification .,[],"[('document classification', (13, 15))]",[],[],[],[],document_classification,2,25
906,experimental-setup,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .","[('performed on', (5, 7)), ('with', (16, 17)), ('as', (19, 20))]","[('Nvidia GTX 1080 and RTX 2080 Ti GPUs', (7, 15)), ('PyTorch 0.4.1', (17, 19)), ('backend framework', (21, 23))]","[['Nvidia GTX 1080 and RTX 2080 Ti GPUs', 'with', 'PyTorch 0.4.1'], ['PyTorch 0.4.1', 'as', 'backend framework']]",[],"[['Experimental setup', 'performed on', 'Nvidia GTX 1080 and RTX 2080 Ti GPUs']]",[],document_classification,2,67
907,experimental-setup,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,"[('use', (1, 2)), ('for computing', (4, 6)), ('implementing', (12, 13))]","[('Scikitlearn 0.19.2', (2, 4)), ('tf - idf vectors', (7, 11)), ('LR and SVMs', (13, 16))]","[['Scikitlearn 0.19.2', 'for computing', 'tf - idf vectors'], ['Scikitlearn 0.19.2', 'implementing', 'LR and SVMs']]",[],"[['Experimental setup', 'use', 'Scikitlearn 0.19.2']]",[],document_classification,2,68
908,experimental-setup,"For HAN , we use a batch size of 32 across all the datasets , with a learning rate of 0.01 for Reuters and 0.001 for the rest .","[('For', (0, 1)), ('use', (4, 5)), ('of', (8, 9)), ('across', (10, 11)), ('with', (15, 16))]","[('HAN', (1, 2)), ('batch size', (6, 8)), ('32', (9, 10)), ('all the datasets', (11, 14)), ('learning rate', (17, 19)), ('0.01', (20, 21)), ('0.001', (24, 25))]","[['HAN', 'use', 'batch size'], ['batch size', 'of', '32'], ['learning rate', 'of', '0.01'], ['32', 'across', 'all the datasets']]","[['batch size', 'has', '32'], ['learning rate', 'has', '0.01']]","[['Experimental setup', 'For', 'HAN']]",[],document_classification,2,82
909,experimental-setup,"To train XML - CNN , we select a dynamic pooling window length of eight , a learning rate of 0.001 , and 128 output channels , with batch sizes of 32 and 64 for single - label and multilabel datasets , respectively .","[('train', (1, 2)), ('select', (7, 8)), ('with', (27, 28)), ('for', (34, 35))]","[('XML - CNN', (2, 5)), ('dynamic pooling window length', (9, 13)), ('eight', (14, 15)), ('learning rate', (17, 19)), ('0.001', (20, 21)), ('128 output channels', (23, 26)), ('batch sizes', (28, 30)), ('32 and 64', (31, 34)), ('single - label and multilabel datasets', (35, 41))]","[['XML - CNN', 'select', 'dynamic pooling window length'], ['XML - CNN', 'select', '128 output channels'], ['128 output channels', 'with', 'batch sizes'], ['32 and 64', 'for', 'single - label and multilabel datasets']]","[['learning rate', 'has', '0.001']]","[['Experimental setup', 'train', 'XML - CNN']]",[],document_classification,2,83
910,experimental-setup,"For KimCNN , we use a batch size of 64 with a learning rate of 0.01 .","[('For', (0, 1)), ('use', (4, 5)), ('of', (8, 9)), ('with', (10, 11)), ('of', (14, 15))]","[('KimCNN', (1, 2)), ('batch size', (6, 8)), ('64', (9, 10)), ('learning rate', (12, 14)), ('0.01', (15, 16))]","[['KimCNN', 'use', 'batch size'], ['batch size', 'of', '64'], ['learning rate', 'of', '0.01'], ['batch size', 'with', 'learning rate'], ['64', 'with', 'learning rate'], ['learning rate', 'of', '0.01']]","[['learning rate', 'has', '0.01']]","[['Experimental setup', 'For', 'KimCNN']]",[],document_classification,2,84
911,experimental-setup,"For LSTM reg and LSTM base , we use the Adam optimizer with a learning rate of 0.01 on Reuters and 0.001 on the rest of the datasets , using batch sizes of 32 and 64 for multi-label and single - label tasks , respectively .","[('For', (0, 1)), ('use', (8, 9)), ('with', (12, 13)), ('of', (16, 17)), ('on', (18, 19)), ('using', (29, 30)), ('of', (32, 33)), ('for', (36, 37))]","[('LSTM reg and LSTM base', (1, 6)), ('Adam optimizer', (10, 12)), ('learning rate', (14, 16)), ('0.01', (17, 18)), ('Reuters', (19, 20)), ('0.001', (21, 22)), ('rest of the datasets', (24, 28)), ('batch sizes', (30, 32)), ('32 and 64', (33, 36)), ('multi-label and single - label tasks', (37, 43))]","[['LSTM reg and LSTM base', 'use', 'Adam optimizer'], ['Adam optimizer', 'with', 'learning rate'], ['Adam optimizer', 'with', '0.001'], ['learning rate', 'of', '0.01'], ['batch sizes', 'of', '32 and 64'], ['0.01', 'on', 'Reuters'], ['0.001', 'using', 'batch sizes'], ['batch sizes', 'of', '32 and 64'], ['32 and 64', 'for', 'multi-label and single - label tasks']]","[['learning rate', 'has', '0.01'], ['batch sizes', 'has', '32 and 64']]","[['Experimental setup', 'For', 'LSTM reg and LSTM base']]",[],document_classification,2,87
912,experimental-setup,"For LSTM reg , we also apply temporal averaging ( TA ) : as shown in , TA reduces both generalization error and stochastic noise in recent parameter estimates from stochastic approximation .","[('apply', (6, 7))]","[('LSTM reg', (1, 3)), ('temporal averaging ( TA )', (7, 12))]","[['LSTM reg', 'apply', 'temporal averaging ( TA )']]",[],[],[],document_classification,2,88
913,experimental-setup,We set the default TA exponential smoothing coefficient of ? EMA to 0.99 .,"[('set', (1, 2)), ('of', (8, 9)), ('to', (11, 12))]","[('default TA exponential smoothing coefficient', (3, 8)), ('? EMA', (9, 11)), ('0.99', (12, 13))]","[['default TA exponential smoothing coefficient', 'of', '? EMA'], ['default TA exponential smoothing coefficient', 'to', '0.99']]",[],"[['Experimental setup', 'set', 'default TA exponential smoothing coefficient']]",[],document_classification,2,89
914,experimental-setup,"We choose 512 hidden units for the Bi - LSTM models , whose max - pooled output is regularized using a dropout rate of 0.5 .","[('choose', (1, 2)), ('for', (5, 6)), ('whose', (12, 13)), ('using', (19, 20)), ('of', (23, 24))]","[('512 hidden units', (2, 5)), ('Bi - LSTM models', (7, 11)), ('max - pooled output', (13, 17)), ('regularized', (18, 19)), ('dropout rate', (21, 23)), ('0.5', (24, 25))]","[['512 hidden units', 'for', 'Bi - LSTM models'], ['Bi - LSTM models', 'whose', 'max - pooled output'], ['regularized', 'using', 'dropout rate'], ['dropout rate', 'of', '0.5']]","[['max - pooled output', 'has', 'regularized']]","[['Experimental setup', 'choose', '512 hidden units']]",[],document_classification,2,90
915,experimental-setup,"We also regularize the input-hidden and hidden - hidden Bi - LSTM connections using embedding dropout and weight dropping , respectively , with dropout rates of 0.1 and 0.2 .","[('regularize', (2, 3)), ('using', (13, 14)), ('with', (22, 23)), ('of', (25, 26))]","[('input-hidden and hidden - hidden Bi - LSTM connections', (4, 13)), ('embedding dropout and weight dropping', (14, 19)), ('dropout rates', (23, 25)), ('0.1 and 0.2', (26, 29))]","[['input-hidden and hidden - hidden Bi - LSTM connections', 'using', 'embedding dropout and weight dropping'], ['embedding dropout and weight dropping', 'with', 'dropout rates'], ['dropout rates', 'of', '0.1 and 0.2']]",[],"[['Experimental setup', 'regularize', 'input-hidden and hidden - hidden Bi - LSTM connections']]",[],document_classification,2,91
916,experimental-setup,"For our optimization objective , we use crossentropy and binary cross - entropy loss for singlelabel and multi-label tasks , respectively .","[('use', (6, 7)), ('for', (14, 15))]","[('optimization objective', (2, 4)), ('crossentropy and binary cross - entropy loss', (7, 14)), ('singlelabel and multi-label tasks', (15, 19))]","[['optimization objective', 'use', 'crossentropy and binary cross - entropy loss'], ['crossentropy and binary cross - entropy loss', 'for', 'singlelabel and multi-label tasks']]",[],[],[],document_classification,2,92
917,experimental-setup,"On all datasets and models , we use 300 - dimensional word vectors pre-trained on Google News .","[('use', (7, 8)), ('pre-trained on', (13, 15))]","[('300 - dimensional word vectors', (8, 13)), ('Google News', (15, 17))]","[['300 - dimensional word vectors', 'pre-trained on', 'Google News']]",[],"[['Experimental setup', 'use', '300 - dimensional word vectors']]",[],document_classification,2,93
918,experimental-setup,"We train all neural models for 30 epochs with five random seeds , reporting the mean validation set scores and their corresponding test set results .","[('train', (1, 2)), ('for', (5, 6)), ('with', (8, 9)), ('reporting', (13, 14))]","[('all neural models', (2, 5)), ('30 epochs', (6, 8)), ('five random seeds', (9, 12)), ('mean', (15, 16))]","[['all neural models', 'for', '30 epochs'], ['30 epochs', 'with', 'five random seeds'], ['five random seeds', 'reporting', 'mean']]",[],"[['Experimental setup', 'train', 'all neural models']]",[],document_classification,2,94
919,results,"We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .","[('see that', (1, 3)), ('achieves', (8, 9)), ('on', (13, 14)), ('establishing', (26, 27)), ('for', (33, 34)), ('on', (39, 40)), ('of', (43, 44))]","[('our simple LSTM reg model', (3, 8)), ('state of the art', (9, 13)), ('Reuters and IMDB', (14, 17)), ('mean scores', (27, 29)), ('87.0 and 52.8', (30, 33)), ('F 1 score', (34, 37)), ('test sets', (41, 43))]","[['our simple LSTM reg model', 'achieves', 'state of the art'], ['state of the art', 'on', 'Reuters and IMDB'], ['state of the art', 'establishing', 'mean scores'], ['87.0 and 52.8', 'for', 'F 1 score']]",[],"[['Results', 'see that', 'our simple LSTM reg model']]",[],document_classification,2,111
920,results,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .","[('observe', (1, 2)), ('upon', (7, 8)), ('of', (10, 11)), ('yields', (31, 32)), ('of', (33, 34)), ('for', (38, 39))]","[('LSTM reg', (3, 5)), ('consistently improves', (5, 7)), ('performance', (9, 10)), ('LSTM base', (11, 13)), ('regularization', (30, 31)), ('increases', (32, 33)), ('1.5 and 0.5 points', (34, 38)), ('F 1 score', (39, 42))]","[['consistently improves', 'upon', 'performance'], ['performance', 'of', 'LSTM base'], ['regularization', 'yields', 'increases'], ['increases', 'of', '1.5 and 0.5 points'], ['1.5 and 0.5 points', 'for', 'F 1 score']]","[['LSTM reg', 'has', 'consistently improves']]","[['Results', 'observe', 'LSTM reg']]",[],document_classification,2,113
921,results,"We also find the accuracy of LSTM reg and our reimplemented version of HAN on Yelp 2014 to be almost two points lower than the copied result of HAN ( rows 6 , 7 , and 10 ) from .","[('find', (2, 3)), ('of', (5, 6)), ('to be', (17, 19)), ('than', (23, 24)), ('of', (27, 28))]","[('accuracy', (4, 5)), ('LSTM reg and our reimplemented version of HAN', (6, 14)), ('almost two points lower', (19, 23)), ('copied result', (25, 27)), ('HAN', (28, 29))]","[['accuracy', 'of', 'LSTM reg and our reimplemented version of HAN'], ['LSTM reg and our reimplemented version of HAN', 'to be', 'almost two points lower'], ['almost two points lower', 'than', 'copied result'], ['copied result', 'of', 'HAN']]",[],[],[],document_classification,2,116
922,results,"On the other hand , both of the models surpass the original result by nearly two points for the IMDB dataset .","[('surpass', (9, 10)), ('by', (13, 14)), ('for', (17, 18))]","[('original result', (11, 13)), ('nearly two points', (14, 17)), ('IMDB dataset', (19, 21))]","[['original result', 'by', 'nearly two points'], ['nearly two points', 'for', 'IMDB dataset']]",[],[],[],document_classification,2,117
923,results,"Interestingly , the non-neural LR and SVM baselines perform remarkably well .","[('perform', (8, 9))]","[('non-neural LR and SVM baselines', (3, 8)), ('remarkably well', (9, 11))]","[['non-neural LR and SVM baselines', 'perform', 'remarkably well']]",[],[],"[['Results', 'has', 'non-neural LR and SVM baselines']]",document_classification,2,119
924,results,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .","[('On', (0, 1)), ('beats', (8, 9)), ('including', (13, 14))]","[('Reuters', (1, 2)), ('SVM', (7, 8)), ('many neural baselines', (9, 12)), ('our non-regularized LSTM base', (14, 18))]","[['SVM', 'beats', 'many neural baselines'], ['many neural baselines', 'including', 'our non-regularized LSTM base']]","[['Reuters', 'has', 'SVM']]",[],[],document_classification,2,120
925,results,"On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .","[('On', (0, 1)), ('losing only to', (13, 16))]","[('AAPD', (1, 2)), ('SVM', (4, 5)), ('ties or beats', (6, 9)), ('other models', (10, 12)), ('SGM', (16, 17))]",[],"[['AAPD', 'has', 'SVM'], ['SVM', 'has', 'ties or beats'], ['ties or beats', 'has', 'other models']]","[['Results', 'On', 'AAPD']]",[],document_classification,2,121
926,research-problem,Practical Text Classification With Large Pre-Trained Language Models,[],"[('Practical Text Classification', (0, 3))]",[],[],[],[],document_classification,20,2
927,research-problem,Multi-emotion sentiment classification is a natural language processing ( NLP ) problem with valuable use cases on realworld data .,[],"[('Multi-emotion sentiment classification', (0, 3))]",[],[],[],[],document_classification,20,4
928,research-problem,"By training an attention - based Transformer network ( Vaswani et al. 2017 ) on 40 GB of text ( Amazon reviews ) ( McAuley et al. 2015 ) and fine - tuning on the training set , our model achieves a 0.69 F1 score on the SemEval Task 1:E - c multidimensional emotion classification problem ( Mohammad et al. 2018 ) , based on the Plutchik wheel of emotions ( Plutchik 1979 ) .","[('training', (1, 2)), ('on', (14, 15)), ('achieves', (40, 41)), ('on', (45, 46))]","[('attention - based Transformer network', (3, 8)), ('40 GB of text', (15, 19)), ('0.69 F1 score', (42, 45)), ('SemEval Task 1:E - c multidimensional emotion classification problem', (47, 56))]","[['attention - based Transformer network', 'on', '40 GB of text'], ['0.69 F1 score', 'on', 'SemEval Task 1:E - c multidimensional emotion classification problem']]",[],"[['Research problem', 'training', 'attention - based Transformer network']]",[],document_classification,20,6
929,model,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .","[('train', (5, 6)), ('on', (12, 13)), ('transfer', (21, 22)), ('to', (24, 25)), ('including', (33, 34)), ('based on', (42, 44))]","[('large 40 GB text dataset', (14, 19)), ('two text classification problems', (25, 29)), ('binary sentiment', (30, 32)), ('multidimensional emotion classification', (39, 42))]",[],"[['two text classification problems', 'name', 'binary sentiment']]","[['Model', 'train', 'large 40 GB text dataset']]",[],document_classification,20,17
930,model,"By training a language model across a large text dataset , we expose our model to many contexts .","[('training', (1, 2)), ('across', (5, 6)), ('expose', (12, 13))]","[('language model', (3, 5)), ('large text dataset', (7, 10))]","[['language model', 'across', 'large text dataset']]",[],"[['Model', 'training', 'language model']]",[],document_classification,20,25
931,baselines,"We also compare our language models to ELMo ) , a contextualized word representation based on a deep bidirectional language model , trained on large text corpus .","[('based on', (14, 16)), ('trained on', (22, 24))]","[('ELMo', (7, 8)), ('contextualized word representation', (11, 14)), ('deep bidirectional language model', (17, 21)), ('large text corpus', (24, 27))]","[['contextualized word representation', 'based on', 'deep bidirectional language model'], ['deep bidirectional language model', 'trained on', 'large text corpus']]","[['ELMo', 'has', 'contextualized word representation']]",[],[],document_classification,20,97
932,results,"We find that the inclusion of easier , more balanced label categories improves performance on harder ones in .","[('find', (1, 2)), ('of', (5, 6)), ('improves', (12, 13)), ('on', (14, 15))]","[('inclusion', (4, 5)), ('easier , more balanced label categories', (6, 12)), ('performance', (13, 14)), ('harder ones', (15, 17))]","[['inclusion', 'of', 'easier , more balanced label categories'], ['easier , more balanced label categories', 'improves', 'performance'], ['performance', 'on', 'harder ones']]",[],"[['Results', 'find', 'inclusion']]",[],document_classification,20,107
933,results,"For both the multihead MLP and the single linear layer instantiating off d , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as t * = 0.5 .","[('For', (0, 1)), ('instantiating off', (10, 12)), ('found that', (15, 17)), ('produced', (19, 20)), ('than using', (23, 25)), ('such as', (29, 31))]","[('multihead MLP and the single linear layer', (3, 10)), ('thresholding predictions', (17, 19)), ('noticeably better results', (20, 23)), ('fixed threshold value', (26, 29)), ('t * = 0.5', (31, 35))]","[['thresholding predictions', 'produced', 'noticeably better results'], ['noticeably better results', 'than using', 'fixed threshold value'], ['fixed threshold value', 'such as', 't * = 0.5']]","[['multihead MLP and the single linear layer', 'has', 'thresholding predictions']]","[['Results', 'For', 'multihead MLP and the single linear layer']]",[],document_classification,20,110
934,results,We find that our models outperform Watson on every emotion category .,"[('find', (1, 2)), ('on', (7, 8))]","[('our models', (3, 5)), ('outperform', (5, 6)), ('Watson', (6, 7)), ('every emotion category', (8, 11))]","[['outperform', 'on', 'every emotion category'], ['Watson', 'on', 'every emotion category']]","[['our models', 'has', 'outperform'], ['outperform', 'has', 'Watson']]","[['Results', 'find', 'our models']]",[],document_classification,20,148
935,results,"We submitted our finetuned Transformer model to the SemEval Task1:E - C challenge , as seen in Table 6 .","[('to', (6, 7))]","[('SemEval Task1:E - C challenge', (8, 13))]",[],[],[],[],document_classification,20,150
936,results,"Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .","[('achieved', (2, 3)), ('among', (8, 9)), ('with', (12, 13)), ('for', (17, 18))]","[('Our model', (0, 2)), ('top macro-averaged F1 score', (4, 8)), ('all submission', (9, 11)), ('competitive', (13, 14)), ('micro -average F1', (19, 22)), ('Jaccard Index accuracy', (24, 27))]","[['Our model', 'achieved', 'top macro-averaged F1 score'], ['top macro-averaged F1 score', 'among', 'all submission'], ['top macro-averaged F1 score', 'with', 'competitive']]",[],[],"[['Results', 'has', 'Our model']]",document_classification,20,152
937,results,We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .,"[('compare', (2, 3)), ('of', (7, 8)), ('find', (18, 19)), ('across', (26, 27))]","[('deep learning architectures', (4, 7)), ('Transformer and m LSTM', (9, 13)), ('Transformer', (21, 22)), ('outperforms', (22, 23)), ('m LSTM', (24, 26)), ('Plutchik categories', (27, 29))]","[['deep learning architectures', 'of', 'Transformer and m LSTM'], ['m LSTM', 'across', 'Plutchik categories']]","[['deep learning architectures', 'has', 'Transformer and m LSTM'], ['Transformer', 'has', 'outperforms'], ['outperforms', 'has', 'm LSTM']]","[['Results', 'compare', 'deep learning architectures']]",[],document_classification,20,154
938,results,"As with the SemEval challenge tweets , the Transformer outperformed the mLSTM .",[],"[('SemEval', (3, 4)), ('Transformer', (8, 9)), ('outperformed', (9, 10)), ('mLSTM', (11, 12))]",[],"[['SemEval', 'has', 'Transformer'], ['Transformer', 'has', 'outperformed'], ['outperformed', 'has', 'mLSTM']]",[],[],document_classification,20,163
939,results,Both models performed significantly better than the Watson API on all categories for which Watson supplies predictions .,"[('performed', (2, 3)), ('than', (5, 6)), ('on', (9, 10)), ('for', (12, 13)), ('supplies', (15, 16))]","[('significantly better', (3, 5)), ('Watson API', (7, 9)), ('all categories', (10, 12)), ('Watson', (14, 15)), ('predictions', (16, 17))]","[['significantly better', 'than', 'Watson API'], ['Watson API', 'on', 'all categories'], ['all categories', 'for', 'Watson'], ['Watson', 'supplies', 'predictions']]","[['Watson', 'has', 'predictions']]","[['Results', 'performed', 'significantly better']]",[],document_classification,20,165
940,results,"Applying the SemEval - trained Transformer directly to our company tweets dataset gets reasonably good results ( 0.338 macro average ) , also validating that our labeling technique is similar to that of SemEval .","[('Applying', (0, 1)), ('directly to', (6, 8)), ('gets', (12, 13)), ('similar to', (29, 31))]","[('SemEval - trained Transformer', (2, 6)), ('our company tweets dataset', (8, 12)), ('reasonably good results', (13, 16))]","[['SemEval - trained Transformer', 'directly to', 'our company tweets dataset'], ['SemEval - trained Transformer', 'gets', 'reasonably good results'], ['our company tweets dataset', 'gets', 'reasonably good results']]",[],"[['Results', 'Applying', 'SemEval - trained Transformer']]",[],document_classification,20,188
941,results,"Looking at rater agreement by dataset , we see that Plutchik category labels contain large rater dis agreement , even among vetted raters who passed the golden set test .","[('Looking at', (0, 2)), ('see that', (8, 10)), ('contain', (13, 14)), ('among', (20, 21)), ('who passed', (23, 25))]","[('rater agreement by dataset', (2, 6)), ('Plutchik category labels', (10, 13)), ('large rater dis agreement', (14, 18)), ('vetted raters', (21, 23)), ('golden set test', (26, 29))]","[['rater agreement by dataset', 'see that', 'Plutchik category labels'], ['Plutchik category labels', 'contain', 'large rater dis agreement'], ['large rater dis agreement', 'among', 'vetted raters'], ['vetted raters', 'who passed', 'golden set test']]","[['rater agreement by dataset', 'has', 'Plutchik category labels'], ['Plutchik category labels', 'has', 'large rater dis agreement']]","[['Results', 'Looking at', 'rater agreement by dataset']]",[],document_classification,20,189
942,research-problem,Squeezed Very Deep Convolutional Neural Networks for Text Classification,[],[],[],[],[],[],document_classification,3,2
943,research-problem,VDCNN accuracy increases with depth .,"[('with', (3, 4))]","[('VDCNN accuracy', (0, 2)), ('increases', (2, 3)), ('depth', (4, 5))]","[['increases', 'with', 'depth']]","[['VDCNN accuracy', 'has', 'increases']]",[],[],document_classification,3,19
944,model,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .","[('propose', (7, 8)), ('requires', (24, 25)), ('compared to', (28, 30))]","[('Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )', (9, 18)), ('text classification model', (20, 23)), ('significantly fewer parameters', (25, 28)), ('stateof - the - art CNNs', (31, 37))]","[['text classification model', 'requires', 'significantly fewer parameters'], ['significantly fewer parameters', 'compared to', 'stateof - the - art CNNs']]","[['Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )', 'has', 'text classification model']]","[['Model', 'propose', 'Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )']]",[],document_classification,3,32
945,model,a) Temporal Depthwise Separable Convolutions ( TD - SCs ) :,[],"[('Temporal Depthwise Separable Convolutions ( TD - SCs )', (1, 10))]",[],[],[],"[['Model', 'has', 'Temporal Depthwise Separable Convolutions ( TD - SCs )']]",document_classification,3,94
946,model,b) Global Average Pooling ( GAP ) :,[],"[('Global Average Pooling ( GAP )', (1, 7))]",[],[],[],"[['Model', 'has', 'Global Average Pooling ( GAP )']]",document_classification,3,124
947,experimental-setup,"The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .","[('performed with', (4, 6)), ('utilizing', (8, 9)), ('with', (14, 15))]","[('training', (1, 2)), ('SGD', (6, 7)), ('size batch of 64', (9, 13)), ('maximum of 100 epochs', (16, 20))]","[['training', 'performed with', 'SGD'], ['SGD', 'utilizing', 'size batch of 64'], ['size batch of 64', 'with', 'maximum of 100 epochs']]",[],[],"[['Experimental setup', 'has', 'training']]",document_classification,3,147
948,experiments,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .","[('use', (1, 2)), ('of', (6, 7))]","[('initial learning rate', (3, 6)), ('0.01', (7, 8)), ('momentum', (10, 11)), ('0.9', (12, 13)), ('weight decay', (15, 17)), ('0.001', (18, 19))]","[['initial learning rate', 'of', '0.01'], ['momentum', 'of', '0.9'], ['weight decay', 'of', '0.001']]",[],[],[],document_classification,3,148
949,experimental-setup,All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,"[('performed on', (4, 6))]","[('NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU', (7, 18))]",[],[],"[['Experimental setup', 'performed on', 'NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU']]",[],document_classification,3,149
950,results,The use of TDSCs promoted a significant reduction in convolutional parameters compared to VDCNN .,"[('use of', (1, 3)), ('promoted', (4, 5)), ('in', (8, 9)), ('compared to', (11, 13))]","[('TDSCs', (3, 4)), ('significant reduction', (6, 8)), ('convolutional parameters', (9, 11)), ('VDCNN', (13, 14))]","[['TDSCs', 'promoted', 'significant reduction'], ['significant reduction', 'in', 'convolutional parameters'], ['significant reduction', 'compared to', 'VDCNN'], ['convolutional parameters', 'compared to', 'VDCNN']]",[],"[['Results', 'use of', 'TDSCs']]",[],document_classification,3,153
951,results,The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks .,"[('obtained by', (3, 5)), ('is', (7, 8))]","[('network reduction', (1, 3)), ('GAP', (6, 7)), ('even more representative', (8, 11))]","[['network reduction', 'obtained by', 'GAP'], ['GAP', 'is', 'even more representative']]","[['network reduction', 'has', 'GAP']]",[],"[['Results', 'has', 'network reduction']]",document_classification,3,158
952,results,"Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .","[('between', (5, 6)), ('varies between', (10, 12))]","[('performance difference', (3, 5)), ('VDCNN and SVDCNN models', (6, 10)), ('0.4 and 1.3 %', (12, 16))]","[['performance difference', 'between', 'VDCNN and SVDCNN models'], ['performance difference', 'varies between', '0.4 and 1.3 %'], ['VDCNN and SVDCNN models', 'varies between', '0.4 and 1.3 %']]",[],[],"[['Results', 'has', 'performance difference']]",document_classification,3,167
953,results,The base property of VDCNN model is preserved on its squeezed model : the performance still increasing up with the depth and b),"[('of', (3, 4)), ('preserved on', (7, 9))]","[('base property', (1, 3)), ('VDCNN model', (4, 6)), ('squeezed model', (10, 12))]","[['base property', 'of', 'VDCNN model'], ['VDCNN model', 'preserved on', 'squeezed model']]",[],[],"[['Results', 'has', 'base property']]",document_classification,3,170
954,results,"The performance evaluated for the most extensive dataset , i.e. , Yelp Review ( 62.30 % ) , still overcomes the accuracy of the Char - CNN model ( 62.05 % ) .","[('evaluated for', (2, 4)), ('i.e.', (9, 10)), ('of', (22, 23))]","[('performance', (1, 2)), ('most extensive dataset', (5, 8)), ('Yelp Review', (11, 13)), ('accuracy', (21, 22)), ('Char - CNN model', (24, 28))]","[['performance', 'evaluated for', 'most extensive dataset'], ['most extensive dataset', 'i.e.', 'Yelp Review'], ['accuracy', 'of', 'Char - CNN model']]",[],[],"[['Results', 'has', 'performance']]",document_classification,3,171
955,research-problem,Joint Embedding of Words and Labels for Text Classification,[],[],[],[],[],[],document_classification,4,2
956,research-problem,Text classification is a fundamental problem in natural language processing ( NLP ) .,[],"[('Text classification', (0, 2))]",[],[],[],[],document_classification,4,15
957,results,All approaches are better than traditional bag - of - words method .,"[('better than', (3, 5))]","[('All approaches', (0, 2)), ('traditional bag - of - words method', (5, 12))]","[['All approaches', 'better than', 'traditional bag - of - words method']]",[],[],"[['Results', 'has', 'All approaches']]",document_classification,4,107
958,results,"Our proposed LEAM outperforms the state - of - the - art methods on two largest datasets , i.e. , Yahoo and DBPedia .","[('on', (13, 14))]","[('outperforms', (3, 4)), ('state - of - the - art methods', (5, 13)), ('two largest datasets', (14, 17)), ('Yahoo and DBPedia', (20, 23))]","[['state - of - the - art methods', 'on', 'two largest datasets']]","[['outperforms', 'has', 'state - of - the - art methods'], ['two largest datasets', 'name', 'Yahoo and DBPedia']]",[],"[['Results', 'has', 'outperforms']]",document_classification,4,108
959,results,LEAM consistently outperforms other methods with different proportion of labeled data .,"[('consistently', (1, 2)), ('with', (5, 6))]","[('LEAM', (0, 1)), ('other methods', (3, 5)), ('different proportion of labeled data', (6, 11))]","[['other methods', 'with', 'different proportion of labeled data']]",[],[],[],document_classification,4,119
960,results,Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model .,"[('use', (2, 3)), ('as', (10, 11)), ('for', (12, 13)), ('in', (18, 19))]","[('300 - dimensional Glo Ve word embeddings', (3, 10)), ('initialization', (11, 12)), ('word embeddings and label embeddings', (13, 18)), ('our model', (19, 21))]","[['300 - dimensional Glo Ve word embeddings', 'as', 'initialization'], ['initialization', 'for', 'word embeddings and label embeddings'], ['word embeddings and label embeddings', 'in', 'our model']]",[],"[['Results', 'use', '300 - dimensional Glo Ve word embeddings']]",[],document_classification,4,199
961,ablation-analysis,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .","[('initialized from', (10, 12)), ('with', (15, 16))]","[('Out - Of - Vocabulary ( OOV ) words', (0, 9)), ('uniform distribution', (13, 15)), ('range [ ? 0.01 , 0.01 ]', (16, 23))]","[['Out - Of - Vocabulary ( OOV ) words', 'initialized from', 'uniform distribution'], ['uniform distribution', 'with', 'range [ ? 0.01 , 0.01 ]']]",[],[],"[['Ablation analysis', 'has', 'Out - Of - Vocabulary ( OOV ) words']]",document_classification,4,200
962,results,"We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .","[('train', (1, 2)), ('with', (6, 7))]","[(""our model 's parameters"", (2, 6)), ('Adam Optimizer', (8, 10)), ('initial learning rate', (20, 23)), ('0.001', (24, 25)), ('minibatch size', (28, 30)), ('100', (31, 32))]","[[""our model 's parameters"", 'with', 'Adam Optimizer'], [""our model 's parameters"", 'with', 'initial learning rate'], [""our model 's parameters"", 'with', 'minibatch size']]","[['initial learning rate', 'has', '0.001'], ['minibatch size', 'has', '100']]","[['Results', 'train', ""our model 's parameters""]]",[],document_classification,4,202
963,ablation-analysis,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .","[('employed on', (3, 5)), ('with', (10, 11))]","[('Dropout regularization', (0, 2)), ('final MLP layer', (6, 9)), ('dropout rate 0.5', (11, 14))]","[['Dropout regularization', 'employed on', 'final MLP layer'], ['Dropout regularization', 'with', 'dropout rate 0.5'], ['final MLP layer', 'with', 'dropout rate 0.5']]",[],[],"[['Ablation analysis', 'has', 'Dropout regularization']]",document_classification,4,203
964,ablation-analysis,The model is implemented using Tensorflow and is trained on GPU Titan X.,"[('implemented using', (3, 5)), ('trained on', (8, 10))]","[('Tensorflow', (5, 6))]",[],[],"[['Ablation analysis', 'implemented using', 'Tensorflow']]",[],document_classification,4,204
965,code,The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,[],"[('https://github.com/guoyinwang/LEAM', (9, 10))]",[],[],[],[],document_classification,4,205
966,experiments,"To demonstrate the practical value of label embeddings , we apply LEAM for a real healthcare scenario : medical code prediction on the Electronic Health Records dataset .","[('apply', (10, 11)), ('for', (12, 13)), ('on', (21, 22))]","[('LEAM', (11, 12)), ('medical code prediction', (18, 21)), ('Electronic Health Records dataset', (23, 27))]","[['medical code prediction', 'on', 'Electronic Health Records dataset']]",[],[],[],document_classification,4,240
967,baselines,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .","[('including', (14, 15))]","[('multi-label classification of clinical text', (8, 13)), ('Condensed Memory Networks ( C - MemNN )', (15, 23)), ('Attentive LSTM', (24, 26)), ('Convolutional Attention ( CAML )', (27, 32))]","[['multi-label classification of clinical text', 'including', 'Condensed Memory Networks ( C - MemNN )'], ['multi-label classification of clinical text', 'including', 'Attentive LSTM'], ['multi-label classification of clinical text', 'including', 'Convolutional Attention ( CAML )']]",[],[],[],document_classification,4,250
968,results,"LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .","[('provides', (1, 2)), ('than', (13, 14)), ('except', (16, 17))]","[('LEAM', (0, 1)), ('best AUC score', (3, 6)), ('better F1 and P@5 values', (8, 13)), ('all methods', (14, 16)), ('CNN', (17, 18))]","[['LEAM', 'provides', 'best AUC score'], ['better F1 and P@5 values', 'than', 'all methods'], ['all methods', 'except', 'CNN']]","[['LEAM', 'has', 'best AUC score']]",[],"[['Results', 'has', 'LEAM']]",document_classification,4,256
969,results,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .","[('consistently', (1, 2)), ('performs', (15, 16)), ('than', (17, 18))]","[('CNN', (0, 1)), ('basic Bi - GRU architecture', (4, 9)), ('logistic regression baseline', (12, 15)), ('worse', (16, 17)), ('all deep learning architectures', (18, 22))]","[['logistic regression baseline', 'performs', 'worse'], ['worse', 'than', 'all deep learning architectures']]",[],[],[],document_classification,4,257
970,research-problem,HDLTex : Hierarchical Deep Learning for Text Classification,[],[],[],[],[],[],document_classification,5,2
971,research-problem,"Increasingly large document collections require improved information processing methods for searching , retrieving , and organizing text .",[],"[('searching , retrieving , and organizing text', (10, 17))]",[],[],[],[],document_classification,5,4
972,research-problem,Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased .,[],[],[],[],[],[],document_classification,5,6
973,research-problem,"Much of the recent work on automatic document classification has involved supervised learning techniques such as classification trees , nave Bayes , support vector machines ( SVM ) , neural nets , and ensemble methods .",[],"[('automatic document classification', (6, 9))]",[],[],[],[],document_classification,5,15
974,model,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,"[('call', (12, 13))]","[('hierarchical', (7, 8)), ('Hierarchical Deep Learning for Text classification ( HDLTex )', (13, 22))]",[],[],[],[],document_classification,5,24
975,model,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,"[('combines', (1, 2)), ('to allow', (5, 7)), ('by', (13, 14))]","[('HDLTex', (0, 1)), ('deep learning architectures', (2, 5)), ('over all and specialized learning', (8, 13)), ('level of the document hierarchy', (14, 19))]","[['HDLTex', 'combines', 'deep learning architectures'], ['deep learning architectures', 'to allow', 'over all and specialized learning'], ['over all and specialized learning', 'by', 'level of the document hierarchy']]","[['HDLTex', 'name', 'deep learning architectures']]",[],"[['Model', 'has', 'HDLTex']]",document_classification,5,25
976,research-problem,Researchers have studied and developed a variety of methods for document classification .,[],"[('document classification', (10, 12))]",[],[],[],[],document_classification,5,30
977,model,This paper uses newer methods of machine learning for document classification taken from deep learning .,"[('uses', (2, 3)), ('for', (8, 9)), ('taken from', (11, 13))]","[('document classification', (9, 11)), ('deep learning', (13, 15))]","[['document classification', 'taken from', 'deep learning']]",[],"[['Model', 'uses', 'document classification']]",[],document_classification,5,41
978,model,This paper describes the use of deep learning approaches to create a hierarchical document classification approach .,"[('to create', (9, 11))]","[('deep learning approaches', (6, 9)), ('hierarchical document classification approach', (12, 16))]","[['deep learning approaches', 'to create', 'hierarchical document classification approach']]",[],[],[],document_classification,5,60
979,model,This paper compares fifteen methods for performing document classification .,[],"[('document classification', (7, 9))]",[],[],[],[],document_classification,5,66
980,research-problem,Multi - Class SVM : Text classification using string kernels within SVMs has been successful in many research projects .,[],"[('Multi - Class SVM', (0, 4)), ('Text classification', (5, 7))]",[],"[['Multi - Class SVM', 'has', 'Text classification']]",[],[],document_classification,5,74
981,model,Stacking Support Vector Machines ( SVM ) :,[],"[('Stacking Support Vector Machines ( SVM )', (0, 7))]",[],[],[],"[['Model', 'has', 'Stacking Support Vector Machines ( SVM )']]",document_classification,5,84
982,experiments,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .","[('done on', (3, 5)), ('with', (14, 15)), ('were', (25, 26))]","[('processing', (1, 2)), ('Xeon E5 ? 2640 ( 2.6 GHz )', (6, 14)), ('32 cores', (15, 17)), ('64GB memory', (18, 20)), ('GPU cards', (23, 25)), ('N vidia Quadro K620', (26, 30)), ('N vidia Tesla K20c', (31, 35))]","[['processing', 'done on', 'Xeon E5 ? 2640 ( 2.6 GHz )'], ['processing', 'done on', 'GPU cards'], ['processing', 'done on', 'N vidia Tesla K20c'], ['Xeon E5 ? 2640 ( 2.6 GHz )', 'with', '32 cores'], ['Xeon E5 ? 2640 ( 2.6 GHz )', 'with', '64GB memory'], ['GPU cards', 'were', 'N vidia Quadro K620'], ['GPU cards', 'were', 'N vidia Tesla K20c']]","[['processing', 'has', 'Xeon E5 ? 2640 ( 2.6 GHz )'], ['GPU cards', 'has', 'N vidia Quadro K620']]",[],[],document_classification,5,232
983,results,CNN performs secondbest for three data sets .,"[('performs', (1, 2)), ('for', (3, 4))]","[('CNN', (0, 1)), ('secondbest', (2, 3)), ('three data sets', (4, 7))]","[['CNN', 'performs', 'secondbest'], ['secondbest', 'for', 'three data sets']]",[],[],"[['Results', 'has', 'CNN']]",document_classification,5,238
984,results,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,"[('is', (4, 5))]","[('SVM with term weighting', (0, 4)), ('third', (5, 6))]","[['SVM with term weighting', 'is', 'third']]","[['SVM with term weighting', 'has', 'third']]",[],"[['Results', 'has', 'SVM with term weighting']]",document_classification,5,239
985,results,"For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .","[('For', (0, 1)), ('obtained by', (12, 14)), ('for', (17, 18)), ('for', (25, 26))]","[('data set W OS ? 11967', (1, 7)), ('best accuracy', (9, 11)), ('combination RNN', (15, 17)), ('first level of classification', (19, 23)), ('DNN', (24, 25)), ('second level', (27, 29))]","[['combination RNN', 'For', 'first level of classification'], ['DNN', 'For', 'second level'], ['best accuracy', 'obtained by', 'combination RNN'], ['combination RNN', 'for', 'first level of classification'], ['combination RNN', 'for', 'DNN'], ['DNN', 'for', 'second level'], ['DNN', 'for', 'second level']]","[['data set W OS ? 11967', 'has', 'best accuracy']]","[['Results', 'For', 'data set W OS ? 11967']]",[],document_classification,5,244
986,results,This is significantly better than all of the others except for the combination of CNN and DNN .,"[('than', (4, 5)), ('except for', (9, 11))]","[('significantly better', (2, 4)), ('all of the others', (5, 9)), ('combination of CNN and DNN', (12, 17))]","[['significantly better', 'than', 'all of the others'], ['all of the others', 'except for', 'combination of CNN and DNN']]","[['significantly better', 'has', 'all of the others']]",[],[],document_classification,5,246
987,results,For data set W OS ? 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2 .,"[('achieved by', (12, 14)), ('for', (15, 16))]","[('data set W OS ? 46985', (1, 7)), ('best scores', (8, 10)), ('RNN', (14, 15)), ('RNN', (22, 23))]","[['best scores', 'achieved by', 'RNN']]","[['data set W OS ? 46985', 'has', 'best scores']]",[],[],document_classification,5,247
988,results,"Document classification is an important problem to address , given the growing size of scientific literature and other document sets .",[],"[('Document classification', (0, 2))]",[],[],[],"[['Results', 'has', 'Document classification']]",document_classification,5,254
989,baselines,"This paper introduces a new approach to hierarchical document classification , HDLTex , that combines multiple deep learning approaches to produce hierarchical classifications .","[('combines', (14, 15))]","[('hierarchical document classification', (7, 10)), ('HDLTex', (11, 12))]",[],[],[],[],document_classification,5,256
990,results,Testing on a data set of documents obtained from the Web of Science shows that combinations of RNN at the higher level and DNN or CNN at the lower level produced accuracies consistently higher than those obtainable by conventional approaches using nave Bayes or SVM .,"[('Testing on', (0, 2)), ('shows', (13, 14)), ('of', (16, 17)), ('at', (18, 19)), ('at', (26, 27)), ('produced', (30, 31)), ('than', (34, 35)), ('obtainable by', (36, 38)), ('using', (40, 41))]","[('combinations', (15, 16)), ('RNN', (17, 18)), ('higher level', (20, 22)), ('DNN or CNN', (23, 26)), ('lower level', (28, 30)), ('accuracies', (31, 32)), ('consistently higher', (32, 34)), ('conventional approaches', (38, 40)), ('nave Bayes or SVM', (41, 45))]","[['combinations', 'of', 'RNN'], ['combinations', 'of', 'DNN or CNN'], ['RNN', 'at', 'higher level'], ['RNN', 'at', 'lower level'], ['DNN or CNN', 'at', 'lower level'], ['combinations', 'at', 'lower level'], ['RNN', 'at', 'lower level'], ['DNN or CNN', 'at', 'lower level'], ['combinations', 'produced', 'accuracies'], ['lower level', 'produced', 'accuracies'], ['conventional approaches', 'using', 'nave Bayes or SVM']]","[['accuracies', 'has', 'consistently higher']]","[['Results', 'Testing on', 'combinations']]",[],document_classification,5,257
991,research-problem,Explicit Interaction Model towards Text Classification,[],[],[],[],[],[],document_classification,6,2
992,research-problem,Text classification is one of the fundamental tasks in natural language processing .,[],"[('Text classification', (0, 2))]",[],[],[],[],document_classification,6,4
993,model,"Thereafter , a fullyconnected ( FC ) layer at the topmost of the network is appended to make the final decision .","[('at', (8, 9)), ('appended', (15, 16)), ('to make', (16, 18))]","[('fullyconnected ( FC ) layer', (3, 8)), ('topmost of the', (10, 13)), ('network', (13, 14)), ('final decision', (19, 21))]","[['fullyconnected ( FC ) layer', 'at', 'topmost of the']]","[['topmost of the', 'has', 'network']]",[],"[['Model', 'has', 'fullyconnected ( FC ) layer']]",document_classification,6,25
994,model,"Mathematically , it interprets the parameter matrix of the FC layer as a set of class representations ( each column is associated with a class ) .","[('interprets', (3, 4)), ('of', (7, 8)), ('as', (11, 12))]","[('parameter matrix', (5, 7)), ('FC layer', (9, 11)), ('set', (13, 14)), ('class representations', (15, 17))]","[['parameter matrix', 'of', 'FC layer'], ['set', 'of', 'class representations'], ['parameter matrix', 'as', 'set']]",[],"[['Model', 'interprets', 'parameter matrix']]",[],document_classification,6,29
995,model,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .","[('introduce', (7, 8)), ('capable of incorporating', (21, 24)), ('for', (30, 31))]","[('interaction mechanism', (9, 11)), ('word - level matching signals', (25, 30)), ('text classification', (31, 33))]","[['interaction mechanism', 'capable of incorporating', 'word - level matching signals'], ['word - level matching signals', 'for', 'text classification']]",[],"[['Model', 'introduce', 'interaction mechanism']]",[],document_classification,6,31
996,model,"From the word - level representation , it computes an interaction matrix , in which each entry is the matching score between a word and a class ( dot -product between their representations ) , illustrating the word - level matching signals .","[('From', (0, 1)), ('computes', (8, 9)), ('in which', (13, 15)), ('is', (17, 18)), ('between', (21, 22)), ('between', (30, 31)), ('illustrating', (35, 36))]","[('word - level representation', (2, 6)), ('interaction matrix', (10, 12)), ('each entry', (15, 17)), ('matching score', (19, 21)), ('word and a class ( dot -product', (23, 30)), ('word - level matching signals', (37, 42))]","[['word - level representation', 'computes', 'interaction matrix'], ['interaction matrix', 'in which', 'each entry'], ['each entry', 'is', 'matching score'], ['matching score', 'between', 'word and a class ( dot -product'], ['matching score', 'illustrating', 'word - level matching signals'], ['word and a class ( dot -product', 'illustrating', 'word - level matching signals']]",[],"[['Model', 'From', 'word - level representation']]",[],document_classification,6,33
997,model,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .","[('Based upon', (0, 2)), ('devise', (7, 8)), ('as', (14, 15))]","[('EXplicit interAction Model ( dubbed', (9, 14))]",[],[],"[['Model', 'Based upon', 'EXplicit interAction Model ( dubbed']]",[],document_classification,6,35
998,model,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .","[('consists of', (5, 7))]","[('three main components', (7, 10)), ('word - level encoder', (11, 15)), ('interaction layer', (16, 18)), ('aggregation layer', (20, 22))]",[],"[['three main components', 'name', 'word - level encoder']]","[['Model', 'consists of', 'three main components']]",[],document_classification,6,36
999,model,The word - level encoder projects the textual contents into the word - level representations .,"[('projects', (5, 6)), ('into', (9, 10))]","[('word - level encoder', (1, 5)), ('textual contents', (7, 9)), ('word - level representations', (11, 15))]","[['word - level encoder', 'projects', 'textual contents'], ['textual contents', 'into', 'word - level representations']]",[],[],"[['Model', 'has', 'word - level encoder']]",document_classification,6,37
1000,model,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .","[('calculates', (5, 6)), ('between', (9, 10)), ('constructs', (17, 18))]","[('interaction layer', (3, 5)), ('matching scores', (7, 9)), ('words and classes', (11, 14)), ('interaction matrix', (19, 21))]","[['interaction layer', 'calculates', 'matching scores'], ['matching scores', 'between', 'words and classes'], ['words and classes', 'constructs', 'interaction matrix']]",[],[],"[['Model', 'has', 'interaction layer']]",document_classification,6,38
1001,model,"In summary , the contributions of this work are threefold : We present a novel framework , EXAM , which leverages the interaction mechanism to explicitly compute the wordlevel interaction signals for the text classification .","[('present', (12, 13)), ('leverages', (20, 21)), ('to explicitly compute', (24, 27)), ('for', (31, 32))]","[('threefold', (9, 10)), ('novel framework', (14, 16)), ('EXAM', (17, 18)), ('interaction mechanism', (22, 24)), ('wordlevel interaction signals', (28, 31)), ('text classification', (33, 35))]","[['threefold', 'present', 'novel framework'], ['novel framework', 'leverages', 'interaction mechanism'], ['EXAM', 'leverages', 'interaction mechanism'], ['interaction mechanism', 'to explicitly compute', 'wordlevel interaction signals'], ['wordlevel interaction signals', 'for', 'text classification']]","[['threefold', 'name', 'novel framework'], ['novel framework', 'name', 'EXAM']]",[],[],document_classification,6,42
1002,experimental-setup,"For the multi -class task , we chose region embedding as the Encoder in EXAM .","[('For', (0, 1)), ('chose', (7, 8)), ('as', (10, 11))]","[('multi -class task', (2, 5)), ('region embedding', (8, 10)), ('Encoder in EXAM', (12, 15))]","[['multi -class task', 'chose', 'region embedding'], ['region embedding', 'as', 'Encoder in EXAM']]",[],"[['Experimental setup', 'For', 'multi -class task']]",[],document_classification,6,141
1003,experimental-setup,The region size is 7 and embedding size is 128 .,"[('is', (3, 4)), ('is', (8, 9))]","[('region size', (1, 3)), ('7', (4, 5)), ('embedding size', (6, 8)), ('128', (9, 10))]","[['region size', 'is', '7'], ['embedding size', 'is', '128'], ['embedding size', 'is', '128']]","[['region size', 'has', '7'], ['embedding size', 'has', '128']]",[],"[['Experimental setup', 'has', 'region size']]",document_classification,6,142
1004,experimental-setup,We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,"[('used', (1, 2)), ('as', (9, 10)), ('with', (12, 13)), ('set to', (23, 25))]","[('adam ( Kingma and Ba 2014 )', (2, 9)), ('optimizer', (11, 12)), ('initial learning rate 0.0001', (14, 18)), ('batch size', (20, 22)), ('16', (25, 26))]","[['adam ( Kingma and Ba 2014 )', 'as', 'optimizer'], ['adam ( Kingma and Ba 2014 )', 'with', 'initial learning rate 0.0001'], ['optimizer', 'with', 'initial learning rate 0.0001'], ['batch size', 'set to', '16']]","[['batch size', 'has', '16']]","[['Experimental setup', 'used', 'adam ( Kingma and Ba 2014 )']]",[],document_classification,6,143
1005,experimental-setup,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .","[('set', (7, 8)), ('of', (10, 11)), ('as', (14, 15))]","[('aggregation MLP', (3, 5)), ('size', (9, 10)), ('hidden layer', (12, 14)), ('2 times interaction feature length', (15, 20))]","[['aggregation MLP', 'set', 'size'], ['size', 'of', 'hidden layer'], ['size', 'as', '2 times interaction feature length'], ['hidden layer', 'as', '2 times interaction feature length']]",[],[],[],document_classification,6,144
1006,experimental-setup,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,"[('implemented and trained by', (3, 7)), ('with', (13, 14))]","[('MXNet ( Chen et', (7, 11)), ('single NVIDIA TITAN Xp', (15, 19))]",[],[],"[['Experimental setup', 'implemented and trained by', 'MXNet ( Chen et']]",[],document_classification,6,145
1007,baselines,1 ) models based on feature engineering ;,"[('based on', (3, 5))]","[('feature engineering', (5, 7))]",[],[],"[['Baselines', 'based on', 'feature engineering']]",[],document_classification,6,149
1008,baselines,"2 ) Char - based deep models , and 3 ) Word - based deep models .",[],"[('Char - based deep models', (2, 7)), ('Word - based deep models', (11, 16))]",[],[],[],[],document_classification,6,150
1009,results,Models based on feature engineering get the worst results on all the five datasets compared to the other methods .,"[('based on', (1, 3)), ('get', (5, 6)), ('on', (9, 10)), ('compared to', (14, 16))]","[('Models', (0, 1)), ('feature engineering', (3, 5)), ('worst results', (7, 9)), ('all the five datasets', (10, 14)), ('other methods', (17, 19))]","[['Models', 'based on', 'feature engineering'], ['Models', 'get', 'worst results'], ['feature engineering', 'get', 'worst results'], ['worst results', 'on', 'all the five datasets'], ['worst results', 'compared to', 'other methods'], ['all the five datasets', 'compared to', 'other methods']]",[],[],"[['Results', 'has', 'Models']]",document_classification,6,154
1010,results,Char - based models get the highest over all scores on the two Amazon datasets .,"[('get', (4, 5)), ('on', (10, 11))]","[('Char - based models', (0, 4)), ('highest over all scores', (6, 10)), ('two Amazon datasets', (12, 15))]","[['Char - based models', 'get', 'highest over all scores'], ['highest over all scores', 'on', 'two Amazon datasets']]","[['Char - based models', 'has', 'highest over all scores']]",[],"[['Results', 'has', 'Char - based models']]",document_classification,6,156
1011,results,"For the three char - based baselines , VDCNN gets the best performance on almost all the datasets because it has 29 convolutional layers allowing the model to learn more combinations of characters .","[('For', (0, 1)), ('gets', (9, 10)), ('on', (13, 14))]","[('three char - based baselines', (2, 7)), ('VDCNN', (8, 9)), ('best performance', (11, 13)), ('almost all the datasets', (14, 18))]","[['three char - based baselines', 'gets', 'best performance'], ['VDCNN', 'gets', 'best performance'], ['best performance', 'on', 'almost all the datasets']]","[['three char - based baselines', 'has', 'VDCNN'], ['VDCNN', 'has', 'best performance']]","[['Results', 'For', 'three char - based baselines']]",[],document_classification,6,159
1012,results,Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .,"[('exceed', (4, 5)), ('on', (8, 9)), ('lose on', (12, 14))]","[('Word - based baselines', (0, 4)), ('other variants', (6, 8)), ('three datasets', (9, 11)), ('two Amazon datasets', (15, 18))]","[['Word - based baselines', 'exceed', 'other variants'], ['other variants', 'on', 'three datasets'], ['Word - based baselines', 'lose on', 'two Amazon datasets']]","[['Word - based baselines', 'has', 'other variants']]",[],[],document_classification,6,160
1013,results,"For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .","[('performs', (8, 9))]","[('W.C Region Emb', (5, 8)), ('best', (10, 11))]","[['W.C Region Emb', 'performs', 'best']]","[['W.C Region Emb', 'has', 'best']]",[],"[['Results', 'has', 'W.C Region Emb']]",document_classification,6,162
1014,results,"It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .","[('achieves', (7, 8)), ('over', (11, 12))]","[('EXAM', (6, 7)), ('best performance', (9, 11)), ('three datasets', (13, 15)), ('AG', (16, 17)), ('Yah. A.', (18, 20)), ('DBP', (21, 22))]","[['EXAM', 'achieves', 'best performance'], ['best performance', 'over', 'three datasets']]","[['three datasets', 'name', 'AG']]",[],[],document_classification,6,163
1015,results,"For the Yah.A. , EXAM improves the best performance by 1.1 % .","[('For', (0, 1)), ('improves', (5, 6)), ('by', (9, 10))]","[('Yah.A.', (2, 3)), ('EXAM', (4, 5)), ('best performance', (7, 9)), ('1.1 %', (10, 12))]","[['EXAM', 'improves', 'best performance'], ['best performance', 'by', '1.1 %']]","[['Yah.A.', 'has', 'EXAM']]","[['Results', 'For', 'Yah.A.']]",[],document_classification,6,164
1016,results,"Additionally , as a word - based model , EXAM beats all the word - based baselines on the other two Amazon datasets with a performance gain of 1.0 % on the Amazon Full , because our EXAM considers more fine - grained interaction features between classes and words , which is quite helpful in this task .","[('as', (2, 3)), ('beats', (10, 11)), ('on', (17, 18)), ('with', (23, 24)), ('of', (27, 28)), ('on', (30, 31))]","[('word - based model', (4, 8)), ('EXAM', (9, 10)), ('all the word - based baselines', (11, 17)), ('other two Amazon datasets', (19, 23)), ('performance gain', (25, 27)), ('1.0 %', (28, 30)), ('Amazon Full', (32, 34))]","[['EXAM', 'beats', 'all the word - based baselines'], ['all the word - based baselines', 'on', 'other two Amazon datasets'], ['1.0 %', 'on', 'Amazon Full'], ['other two Amazon datasets', 'with', 'performance gain'], ['performance gain', 'of', '1.0 %'], ['1.0 %', 'on', 'Amazon Full']]","[['word - based model', 'has', 'EXAM']]","[['Results', 'as', 'word - based model']]",[],document_classification,6,165
1017,baselines,We built a model called EXAM Encoder to preserve only the Encoder component with a max pooling layer and FC layer to derive the final probabilities .,"[('built', (1, 2)), ('called', (4, 5)), ('to preserve', (7, 9)), ('with', (13, 14)), ('to derive', (21, 23))]","[('EXAM Encoder', (5, 7)), ('only the Encoder component', (9, 13)), ('max pooling layer and FC layer', (15, 21)), ('final probabilities', (24, 26))]","[['EXAM Encoder', 'to preserve', 'only the Encoder component'], ['only the Encoder component', 'with', 'max pooling layer and FC layer'], ['only the Encoder component', 'to derive', 'final probabilities'], ['max pooling layer and FC layer', 'to derive', 'final probabilities']]",[],[],[],document_classification,6,168
1018,experimental-setup,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .","[('used', (1, 2)), ('trained by', (4, 6)), ('to initialize', (7, 9)), ('is', (17, 18))]","[('matrix', (3, 4)), ('word2vec', (6, 7)), ('embedding layer', (10, 12)), ('embedding size', (15, 17)), ('256', (18, 19))]","[['matrix', 'trained by', 'word2vec'], ['word2vec', 'to initialize', 'embedding layer'], ['embedding size', 'is', '256']]","[['embedding size', 'has', '256']]","[['Experimental setup', 'used', 'matrix']]",[],document_classification,6,195
1019,experimental-setup,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .","[('adopted', (1, 2)), ('as', (3, 4))]","[('GRU', (2, 3)), ('Encoder', (5, 6)), ('1,024', (12, 13))]","[['GRU', 'as', 'Encoder']]",[],"[['Experimental setup', 'adopted', 'GRU']]",[],document_classification,6,196
1020,experimental-setup,The accumulated MLP has 60 hidden units .,[],"[('accumulated MLP', (1, 3)), ('60 hidden units', (4, 7))]",[],"[['accumulated MLP', 'has', '60 hidden units']]",[],"[['Experimental setup', 'has', 'accumulated MLP']]",document_classification,6,197
1021,experimental-setup,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,"[('applied', (1, 2)), ('to optimize', (3, 5)), ('on', (6, 7)), ('with', (11, 12)), ('is', (22, 23))]","[('Adam', (2, 3)), ('models', (5, 6)), ('one NVIDIA TITAN Xp', (7, 11)), ('batch size of 1000', (13, 17)), ('initial learning rate', (19, 22)), ('0.001', (23, 24))]","[['Adam', 'to optimize', 'models'], ['models', 'on', 'one NVIDIA TITAN Xp'], ['one NVIDIA TITAN Xp', 'with', 'batch size of 1000'], ['initial learning rate', 'is', '0.001']]","[['initial learning rate', 'has', '0.001']]","[['Experimental setup', 'applied', 'Adam']]",[],document_classification,6,198
1022,experimental-setup,The validation set is applied for early - stopping to avoid overfitting .,"[('applied for', (4, 6)), ('to avoid', (9, 11))]","[('validation set', (1, 3)), ('early - stopping', (6, 9)), ('overfitting', (11, 12))]","[['validation set', 'applied for', 'early - stopping'], ['early - stopping', 'to avoid', 'overfitting']]",[],[],"[['Experimental setup', 'has', 'validation set']]",document_classification,6,199
1023,experimental-setup,All hyperparameters are chosen empirically .,"[('chosen', (3, 4))]","[('hyperparameters', (1, 2)), ('empirically', (4, 5))]","[['hyperparameters', 'chosen', 'empirically']]","[['hyperparameters', 'has', 'empirically']]",[],"[['Experimental setup', 'has', 'hyperparameters']]",document_classification,6,200
1024,results,Word - based models are better than char - based models in Kanshan - Cup dataset .,"[('better than', (5, 7)), ('in', (11, 12))]","[('Word - based models', (0, 4)), ('char - based models', (7, 11)), ('Kanshan - Cup dataset', (12, 16))]","[['Word - based models', 'better than', 'char - based models'], ['char - based models', 'in', 'Kanshan - Cup dataset']]",[],[],"[['Results', 'has', 'Word - based models']]",document_classification,6,210
1025,results,"For word - based baseline models , all the baselines have similar performance which corroborates the conclusion in FastText ) that simple network is on par with deep learning classifiers in text classification .","[('For', (0, 1)), ('have', (10, 11)), ('on par with', (24, 27)), ('in', (30, 31))]","[('word - based baseline models', (1, 6)), ('all the baselines', (7, 10)), ('similar performance', (11, 13)), ('simple network', (21, 23)), ('deep learning classifiers', (27, 30)), ('text classification', (31, 33))]","[['all the baselines', 'have', 'similar performance'], ['simple network', 'on par with', 'deep learning classifiers'], ['deep learning classifiers', 'in', 'text classification']]","[['word - based baseline models', 'has', 'all the baselines'], ['all the baselines', 'has', 'similar performance']]","[['Results', 'For', 'word - based baseline models']]",[],document_classification,6,212
1026,results,Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .,"[('achieve', (2, 3)), ('over', (12, 13))]","[('Our models', (0, 2)), ('state - of - the - art performance', (4, 12)), ('two different datasets', (13, 16))]","[['Our models', 'achieve', 'state - of - the - art performance'], ['state - of - the - art performance', 'over', 'two different datasets']]",[],[],"[['Results', 'has', 'Our models']]",document_classification,6,213
1027,research-problem,A Corpus for Multilingual Document Classification in Eight Languages,[],"[('Multilingual Document Classification', (3, 6))]",[],[],[],[],document_classification,7,2
1028,research-problem,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .,[],"[('Cross - lingual document classification', (0, 5))]",[],[],[],[],document_classification,7,4
1029,research-problem,There is a large body of research on approaches for document classification .,[],"[('document classification', (10, 12))]",[],[],[],[],document_classification,7,17
1030,model,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .","[('extend', (1, 2)), ('use', (5, 6)), ('in', (8, 9)), ('to define', (14, 16)), ('for', (23, 24)), ('namely', (29, 30))]","[('previous works', (2, 4)), ('data', (7, 8)), ('Reuters Corpus Volume 2', (10, 14)), ('new cross - lingual document classification tasks', (16, 23)), ('eight very different languages', (24, 28)), ('English', (30, 31)), ('French', (32, 33)), ('Spanish', (34, 35)), ('Italian', (36, 37)), ('German', (38, 39)), ('Russian', (40, 41)), ('Chinese', (42, 43)), ('Japanese', (44, 45))]","[['data', 'in', 'Reuters Corpus Volume 2'], ['Reuters Corpus Volume 2', 'to define', 'new cross - lingual document classification tasks'], ['new cross - lingual document classification tasks', 'for', 'eight very different languages'], ['eight very different languages', 'namely', 'English'], ['eight very different languages', 'namely', 'French'], ['eight very different languages', 'namely', 'Spanish'], ['eight very different languages', 'namely', 'Italian'], ['eight very different languages', 'namely', 'German'], ['eight very different languages', 'namely', 'Russian'], ['eight very different languages', 'namely', 'Chinese'], ['eight very different languages', 'namely', 'Japanese']]",[],"[['Model', 'extend', 'previous works']]",[],document_classification,7,31
1031,results,Since the initial work by many alternative approaches to cross -lingual document classification have been developed .,[],"[('cross -lingual document classification', (9, 13))]",[],[],[],[],document_classification,7,73
1032,baselines,"In this paper , we propose initial strong baselines which represent two complementary directions of research : one based on the aggregation of multilingual word embeddings , and another one , which directly learns multilingual sentence representations .","[('propose', (5, 6)), ('represent', (10, 11)), ('of', (14, 15)), ('based on', (18, 20)), ('of', (22, 23))]","[('initial strong baselines', (6, 9)), ('two complementary directions', (11, 14)), ('research', (15, 16)), ('aggregation', (21, 22)), ('multilingual word embeddings', (23, 26)), ('multilingual sentence representations', (34, 37))]","[['initial strong baselines', 'represent', 'two complementary directions'], ['two complementary directions', 'of', 'research'], ['aggregation', 'of', 'multilingual word embeddings']]","[['two complementary directions', 'has', 'research']]","[['Baselines', 'propose', 'initial strong baselines']]",[],document_classification,7,76
1033,results,"We will name this case "" zero - shot cross - lingual document classification "" .",[],"[('zero - shot cross - lingual document classification', (6, 14))]",[],[],[],[],document_classification,7,82
1034,results,This type of cross - lingual document classification needs a very strong multilingual representation since no knowledge on the target language was used during the development of the classifier .,[],"[('cross - lingual document classification', (3, 8))]",[],[],[],"[['Results', 'has', 'cross - lingual document classification']]",document_classification,7,86
1035,baselines,"We train a simple one - layer convolutional neural network ( CNN ) on top of the word embeddings , which has shown to perform well on text classification tasks regardless of training data size .","[('train', (1, 2)), ('on top of', (13, 16)), ('shown to', (22, 24))]","[('simple one - layer convolutional neural network ( CNN )', (3, 13)), ('word embeddings', (17, 19)), ('perform', (24, 25)), ('well', (25, 26))]","[['simple one - layer convolutional neural network ( CNN )', 'on top of', 'word embeddings'], ['word embeddings', 'shown to', 'perform']]","[['perform', 'has', 'well']]","[['Baselines', 'train', 'simple one - layer convolutional neural network ( CNN )']]",[],document_classification,7,100
1036,ablation-analysis,"Specifically , convolutional filters are applied to windows of word embeddings , with a max - over - time pooling on top of them .","[('applied to', (5, 7)), ('of', (8, 9)), ('with', (12, 13)), ('on top of', (20, 23))]","[('convolutional filters', (2, 4)), ('windows', (7, 8)), ('word embeddings', (9, 11)), ('max - over - time pooling', (14, 20))]","[['convolutional filters', 'applied to', 'windows'], ['windows', 'of', 'word embeddings'], ['windows', 'with', 'max - over - time pooling']]",[],[],"[['Ablation analysis', 'has', 'convolutional filters']]",document_classification,7,101
1037,hyperparameters,"Hyper- parameters such as convolutional output dimension , window sizes are done by grid search over the Dev set of the same language as the train set .","[('such as', (2, 4)), ('done by', (11, 13)), ('over', (15, 16)), ('as', (23, 24))]","[('Hyper- parameters', (0, 2)), ('convolutional output dimension', (4, 7)), ('grid search', (13, 15)), ('Dev set of the same language', (17, 23)), ('train set', (25, 27))]","[['Hyper- parameters', 'such as', 'convolutional output dimension'], ['grid search', 'over', 'Dev set of the same language'], ['Dev set of the same language', 'as', 'train set']]",[],[],"[['Hyperparameters', 'has', 'Hyper- parameters']]",document_classification,7,103
1038,results,A second direction of research is to directly learn multilingual sentence representations .,[],"[('multilingual sentence representations', (9, 12))]",[],[],[],[],document_classification,7,105
1039,baselines,"In this paper , we evaluate a recently proposed technique to learn joint multilingual sentence representations .",[],"[('joint multilingual sentence representations', (12, 16))]",[],[],[],[],document_classification,7,106
1040,results,"We have developed two versions of the system : one trained on the Europarl corpus to cover the languages English , German , French , Spanish and Italian , and another one trained on the United Nations corpus which allows to learn a joint sentence embedding for English , French , Spanish , Russian and Chinese .","[('developed', (2, 3)), ('of', (5, 6)), ('trained on', (10, 12)), ('to cover', (15, 17)), ('trained on', (32, 34)), ('allows to learn', (39, 42)), ('for', (46, 47))]","[('two versions', (3, 5)), ('Europarl corpus', (13, 15)), ('languages', (18, 19)), ('English', (19, 20)), ('United Nations corpus', (35, 38)), ('joint sentence embedding', (43, 46)), ('English', (47, 48))]","[['Europarl corpus', 'to cover', 'languages'], ['United Nations corpus', 'allows to learn', 'joint sentence embedding'], ['joint sentence embedding', 'for', 'English']]","[['languages', 'has', 'English']]","[['Results', 'developed', 'two versions']]",[],document_classification,7,111
1041,ablation-analysis,We use a one hidden - layer MLP as classifier .,"[('use', (1, 2)), ('as', (8, 9))]","[('one hidden - layer MLP', (3, 8)), ('classifier', (9, 10))]","[['one hidden - layer MLP', 'as', 'classifier']]",[],"[['Ablation analysis', 'use', 'one hidden - layer MLP']]",[],document_classification,7,112
1042,results,"For comparison , we have evaluated its performance on the original subset of RCV2 as used in previous publications on cross - lingual document classification : we are able to outperform the current state - of - the - art in three out of six transfer directions .","[('evaluated', (5, 6)), ('on', (8, 9)), ('able to', (28, 30)), ('in', (40, 41))]","[('performance', (7, 8)), ('original subset of RCV2', (10, 14)), ('cross - lingual document classification', (20, 25)), ('outperform', (30, 31)), ('three out of six transfer directions', (41, 47))]","[['performance', 'on', 'original subset of RCV2'], ['cross - lingual document classification', 'able to', 'outperform']]",[],"[['Results', 'evaluated', 'performance']]",[],document_classification,7,113
1043,results,Zero - shot cross - lingual document classification,[],[],[],[],[],[],document_classification,7,114
1044,results,The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,"[('based on', (2, 4)), ('perform', (7, 8)), ('on', (10, 11))]","[('classifiers', (1, 2)), ('MultiCCA embeddings', (5, 7)), ('very well', (8, 10)), ('development corpus', (12, 14)), ('accuracies', (15, 16)), ('close or exceeding 90 %', (16, 21))]","[['classifiers', 'based on', 'MultiCCA embeddings'], ['classifiers', 'perform', 'very well'], ['MultiCCA embeddings', 'perform', 'very well'], ['very well', 'on', 'development corpus']]","[['accuracies', 'has', 'close or exceeding 90 %']]",[],"[['Results', 'has', 'classifiers']]",document_classification,7,116
1045,results,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .","[('trained on', (2, 4)), ('achieves', (6, 7)), ('when transfered to', (9, 12)), ('scores', (17, 18)), ('for', (19, 20))]","[('system', (1, 2)), ('English', (4, 5)), ('excellent results', (7, 9)), ('different languages', (13, 15)), ('best', (18, 19)), ('three out of seven languages ( DE , IT and ZH )', (20, 32))]","[['system', 'trained on', 'English'], ['system', 'achieves', 'excellent results'], ['English', 'achieves', 'excellent results'], ['excellent results', 'when transfered to', 'different languages'], ['different languages', 'scores', 'best'], ['best', 'for', 'three out of seven languages ( DE , IT and ZH )']]",[],[],"[['Results', 'has', 'system']]",document_classification,7,117
1046,results,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,[],[],[],[],[],[],document_classification,7,120
1047,results,Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,[],"[('Crosslingual transfer between very different languages', (0, 6))]",[],[],[],[],document_classification,7,123
1048,results,Targeted cross - lingual document classification,[],[],[],[],[],[],document_classification,7,124
1049,research-problem,Disconnected Recurrent Neural Networks for Text Categorization,[],[],[],[],[],[],document_classification,8,2
1050,research-problem,Recurrent neural network ( RNN ) has achieved remarkable performance in text categorization .,[],"[('text categorization', (11, 13))]",[],[],[],[],document_classification,8,4
1051,research-problem,"Text categorization is a fundamental and traditional task in natural language processing ( NLP ) , which can be applied in various applications such as sentiment analysis , question classification and topic classification .",[],"[('Text categorization', (0, 2))]",[],[],[],[],document_classification,8,11
1052,model,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .","[('incorporate', (5, 6)), ('into', (7, 8)), ('propose', (10, 11)), ('named', (14, 15))]","[('positioninvariance', (6, 7)), ('RNN', (8, 9)), ('Disconnected Recurrent Neural Network ( DRNN )', (15, 22))]","[['positioninvariance', 'into', 'RNN']]",[],"[['Model', 'incorporate', 'positioninvariance']]",[],document_classification,8,23
1053,model,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .","[('To maintain', (0, 2)), ('utilize', (8, 9)), ('to extract', (11, 13))]","[('position - invariance', (3, 6)), ('max pooling', (9, 11)), ('important information', (14, 16))]","[['position - invariance', 'utilize', 'max pooling'], ['max pooling', 'to extract', 'important information']]",[],"[['Model', 'To maintain', 'position - invariance']]",[],document_classification,8,27
1054,model,We also find that there is a trade - off between position - invariance and long - term dependencies in the DRNN .,"[('between', (10, 11)), ('in', (19, 20))]","[('trade - off', (7, 10)), ('position - invariance and long - term dependencies', (11, 19)), ('DRNN', (21, 22))]","[['trade - off', 'between', 'position - invariance and long - term dependencies'], ['position - invariance and long - term dependencies', 'in', 'DRNN']]",[],[],[],document_classification,8,31
1055,model,1 . We propose a novel model to incorporate position - variance into RNN .,"[('propose', (3, 4)), ('to incorporate', (7, 9)), ('into', (12, 13))]","[('novel model', (5, 7)), ('position - variance', (9, 12)), ('RNN', (13, 14))]","[['novel model', 'to incorporate', 'position - variance'], ['position - variance', 'into', 'RNN']]",[],"[['Model', 'propose', 'novel model']]",[],document_classification,8,39
1056,model,"Based on this , we propose an empirical method to find the optimal window size .","[('propose', (5, 6)), ('to find', (9, 11))]","[('empirical method', (7, 9)), ('optimal window size', (12, 15))]","[['empirical method', 'to find', 'optimal window size']]",[],"[['Model', 'propose', 'empirical method']]",[],document_classification,8,43
1057,experimental-setup,We tokenize all the corpus with NLTK 's tokenizer .,"[('tokenize', (1, 2)), ('with', (5, 6))]","[('all the corpus', (2, 5)), (""NLTK 's tokenizer"", (6, 9))]","[['all the corpus', 'with', ""NLTK 's tokenizer""]]",[],"[['Experimental setup', 'tokenize', 'all the corpus']]",[],document_classification,8,152
1058,experimental-setup,"We utilize the 300D Glo Ve 840B vectors ( Pennington et al. , 2014 ) as our pre-trained word embeddings .","[('utilize', (1, 2)), ('as', (15, 16))]","[('300D Glo Ve 840B vectors', (3, 8)), ('pre-trained word embeddings', (17, 20))]","[['300D Glo Ve 840B vectors', 'as', 'pre-trained word embeddings']]",[],"[['Experimental setup', 'utilize', '300D Glo Ve 840B vectors']]",[],document_classification,8,156
1059,experimental-setup,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .","[('use', (1, 2)), ('to optimize', (8, 10))]","[('Adadelta ( Zeiler , 2012 )', (2, 8)), ('all the trainable parameters', (10, 14))]","[['Adadelta ( Zeiler , 2012 )', 'to optimize', 'all the trainable parameters']]",[],"[['Experimental setup', 'use', 'Adadelta ( Zeiler , 2012 )']]",[],document_classification,8,159
1060,experimental-setup,The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,"[('of', (2, 3)), ('set as', (5, 7)), ('suggest', (11, 12)), ('is', (20, 21))]","[('hyperparameter', (1, 2)), ('Adadelta', (3, 4)), ('Zeiler ( 2012 )', (7, 11)), ('1 e ? 6', (14, 18)), ('0.95', (21, 22))]","[['hyperparameter', 'of', 'Adadelta'], ['Adadelta', 'set as', 'Zeiler ( 2012 )'], ['Zeiler ( 2012 )', 'suggest', '1 e ? 6']]",[],[],"[['Experimental setup', 'has', 'hyperparameter']]",document_classification,8,160
1061,experimental-setup,"To avoid the gradient explosion problem , we apply gradient norm clipping .","[('To avoid', (0, 2)), ('apply', (8, 9))]","[('gradient explosion problem', (3, 6)), ('gradient norm clipping', (9, 12))]","[['gradient explosion problem', 'apply', 'gradient norm clipping']]",[],"[['Experimental setup', 'To avoid', 'gradient explosion problem']]",[],document_classification,8,161
1062,experimental-setup,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,"[('set to', (4, 6)), ('of', (11, 12)), ('shows', (16, 17)), ('in', (27, 28))]","[('batch size', (1, 3)), ('128', (6, 7)), ('all the dimensions', (8, 11)), ('input vectors and hidden', (12, 16)), ('our proposed model', (18, 21)), ('significantly outperforms', (21, 23)), ('all the other models', (23, 27)), ('7 datasets', (28, 30))]","[['batch size', 'set to', '128'], ['all the dimensions', 'of', 'input vectors and hidden'], ['input vectors and hidden', 'shows', 'our proposed model'], ['all the other models', 'in', '7 datasets']]","[['batch size', 'has', '128'], ['our proposed model', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all the other models']]",[],"[['Experimental setup', 'has', 'batch size']]",document_classification,8,162
1063,results,Fast - Text and region embedding methods achieve comparable performance with other CNN and RNN based models .,"[('achieve', (7, 8)), ('with', (10, 11))]","[('Fast - Text and region embedding methods', (0, 7)), ('comparable performance', (8, 10)), ('other CNN and RNN based models', (11, 17))]","[['Fast - Text and region embedding methods', 'achieve', 'comparable performance'], ['comparable performance', 'with', 'other CNN and RNN based models']]",[],[],"[['Results', 'has', 'Fast - Text and region embedding methods']]",document_classification,8,167
1064,baselines,The D - LSTM is a discriminative LSTM model .,"[('is', (4, 5))]","[('D - LSTM', (1, 4)), ('discriminative LSTM model', (6, 9))]","[['D - LSTM', 'is', 'discriminative LSTM model']]","[['D - LSTM', 'has', 'discriminative LSTM model']]",[],"[['Baselines', 'has', 'D - LSTM']]",document_classification,8,170
1065,baselines,Hierarchical attention network ( HAN ) is a hierarchical GRU model with attentive pooling .,"[('is', (6, 7)), ('with', (11, 12))]","[('Hierarchical attention network ( HAN )', (0, 6)), ('hierarchical GRU model', (8, 11)), ('attentive pooling', (12, 14))]","[['Hierarchical attention network ( HAN )', 'is', 'hierarchical GRU model'], ['hierarchical GRU model', 'with', 'attentive pooling']]","[['Hierarchical attention network ( HAN )', 'has', 'hierarchical GRU model']]",[],"[['Baselines', 'has', 'Hierarchical attention network ( HAN )']]",document_classification,8,171
1066,results,We can see that very deep CNN ( VDCNN ) performs well in large datasets .,"[('see', (2, 3)), ('performs', (10, 11)), ('in', (12, 13))]","[('very deep CNN ( VDCNN )', (4, 10)), ('well', (11, 12)), ('large datasets', (13, 15))]","[['very deep CNN ( VDCNN )', 'performs', 'well'], ['well', 'in', 'large datasets']]",[],"[['Results', 'see', 'very deep CNN ( VDCNN )']]",[],document_classification,8,172
1067,results,"By contrast , our proposed model can achieve : DGRU compared with CNN better performance in these datasets by simply setting a large window size .","[('achieve', (7, 8)), ('compared with', (10, 12)), ('by', (18, 19))]","[('CNN better performance', (12, 15)), ('large window size', (22, 25))]",[],[],[],[],document_classification,8,174
1068,baselines,Char-CRNN in the fourth block is a model which combines positioninvariance of CNN and long - term dependencies of RNN .,"[('combines', (9, 10)), ('of', (11, 12)), ('of', (18, 19))]","[('Char-CRNN', (0, 1)), ('positioninvariance', (10, 11)), ('CNN', (12, 13)), ('long - term dependencies', (14, 18)), ('RNN', (19, 20))]","[['positioninvariance', 'of', 'CNN'], ['positioninvariance', 'of', 'long - term dependencies'], ['long - term dependencies', 'of', 'RNN'], ['long - term dependencies', 'of', 'RNN']]",[],[],"[['Baselines', 'has', 'Char-CRNN']]",document_classification,8,175
1069,results,shows that our model achieves 10 - 50 % relative error reduction compared with char - CRNN in these datasets .,"[('shows', (0, 1)), ('achieves', (4, 5)), ('compared with', (12, 14))]","[('our model', (2, 4)), ('10 - 50 % relative error reduction', (5, 12)), ('char - CRNN', (14, 17))]","[['our model', 'achieves', '10 - 50 % relative error reduction'], ['10 - 50 % relative error reduction', 'compared with', 'char - CRNN']]",[],"[['Results', 'shows', 'our model']]",[],document_classification,8,180
1070,results,Comparison with RNN and CNN,[],[],[],[],[],[],document_classification,8,181
1071,results,shows that DRNN performs far better than CNN .,"[('shows', (0, 1)), ('performs', (3, 4)), ('than', (6, 7))]","[('DRNN', (2, 3)), ('far better', (4, 6)), ('CNN', (7, 8))]","[['DRNN', 'performs', 'far better'], ['far better', 'than', 'CNN']]",[],"[['Results', 'shows', 'DRNN']]",[],document_classification,8,186
1072,results,Our model DRNN achieves much better performance than GRU and LSTM .,"[('achieves', (3, 4)), ('than', (7, 8))]","[('Our model DRNN', (0, 3)), ('much better performance', (4, 7)), ('GRU and LSTM', (8, 11))]","[['Our model DRNN', 'achieves', 'much better performance'], ['much better performance', 'than', 'GRU and LSTM']]",[],[],"[['Results', 'has', 'Our model DRNN']]",document_classification,8,193
1073,results,We find that the disconnected naive RNN performs just a little worse than disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU ) when the window size is lower than 5 .,"[('find that', (1, 3)), ('performs', (7, 8)), ('than', (12, 13)), ('when', (24, 25)), ('is', (28, 29))]","[('disconnected naive RNN', (4, 7)), ('disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU )', (13, 24)), ('window size', (26, 28)), ('lower than 5', (29, 32))]","[['disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU )', 'when', 'window size'], ['window size', 'is', 'lower than 5']]","[['window size', 'has', 'lower than 5']]","[['Results', 'find that', 'disconnected naive RNN']]",[],document_classification,8,222
1074,results,"DGRU achieves the best performance when the window size is 15 , while the best window size for DLSTM is 5 .","[('achieves', (1, 2)), ('when', (5, 6)), ('is', (9, 10)), ('for', (17, 18)), ('is', (19, 20))]","[('DGRU', (0, 1)), ('best performance', (3, 5)), ('window size', (7, 9)), ('15', (10, 11)), ('best window size', (14, 17)), ('DLSTM', (18, 19)), ('5', (20, 21))]","[['DGRU', 'achieves', 'best performance'], ['best performance', 'when', 'window size'], ['window size', 'is', '15'], ['DLSTM', 'is', '5'], ['best window size', 'for', 'DLSTM'], ['DLSTM', 'is', '5']]","[['DGRU', 'has', 'best performance'], ['window size', 'has', '15']]",[],"[['Results', 'has', 'DGRU']]",document_classification,8,226
1075,results,We still conduct the experiments on AG dataset .,"[('on', (5, 6))]","[('AG dataset', (6, 8))]",[],[],"[['Results', 'on', 'AG dataset']]",[],document_classification,8,233
1076,results,"From ( b ) , we can see that the DRNN model with max pooling performs better than the others .","[('see that', (7, 9)), ('with', (12, 13)), ('performs', (15, 16)), ('than', (17, 18))]","[('DRNN model', (10, 12)), ('max pooling', (13, 15)), ('better', (16, 17)), ('others', (19, 20))]","[['DRNN model', 'with', 'max pooling'], ['DRNN model', 'performs', 'better'], ['max pooling', 'performs', 'better'], ['better', 'than', 'others']]",[],"[['Results', 'see that', 'DRNN model']]",[],document_classification,8,235
1077,results,We find attentive pooling is not significantly affected by window sizes .,"[('find', (1, 2)), ('not', (5, 6))]","[('attentive pooling', (2, 4)), ('window sizes', (9, 11))]",[],[],"[['Results', 'find', 'attentive pooling']]",[],document_classification,8,237
1078,results,Window size analysis,[],[],[],[],[],[],document_classification,8,239
1079,research-problem,Investigating Capsule Networks with Dynamic Routing for Text Classification,[],[],[],[],[],[],document_classification,9,2
1080,code,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,[],"[('https : //github.com/andyweizhao/capsule_text_ classification', (7, 11))]",[],[],[],[],document_classification,9,10
1081,research-problem,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,[],"[('Modeling articles or sentences computationally', (0, 5))]",[],[],[],[],document_classification,9,12
1082,model,It then hierarchically builds such pattern extraction pipelines at multiple levels .,"[('hierarchically builds', (2, 4)), ('at', (8, 9))]","[('pattern extraction pipelines', (5, 8)), ('multiple levels', (9, 11))]","[['pattern extraction pipelines', 'at', 'multiple levels']]",[],"[['Model', 'hierarchically builds', 'pattern extraction pipelines']]",[],document_classification,9,25
1083,hyperparameters,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .","[('use', (5, 6)), ('to initialize', (11, 13))]","[('300 - dimensional word2vec vectors', (6, 11)), ('embedding vectors', (13, 15))]","[['300 - dimensional word2vec vectors', 'to initialize', 'embedding vectors']]",[],"[['Hyperparameters', 'use', '300 - dimensional word2vec vectors']]",[],document_classification,9,139
1084,hyperparameters,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,"[('conduct', (1, 2)), ('with', (3, 4)), ('for', (6, 7))]","[('mini-batch', (2, 3)), ('size 50', (4, 6)), (""AG 's news"", (7, 10)), ('size 25', (11, 13)), ('other datasets', (14, 16))]","[['mini-batch', 'with', 'size 50'], ['size 50', 'for', ""AG 's news""]]",[],"[['Hyperparameters', 'conduct', 'mini-batch']]",[],document_classification,9,140
1085,hyperparameters,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,"[('use', (1, 2)), ('with', (5, 6)), ('to train', (11, 13))]","[('Adam optimization algorithm', (2, 5)), ('1e - 3 learning rate', (6, 11)), ('model', (14, 15))]","[['Adam optimization algorithm', 'with', '1e - 3 learning rate'], ['1e - 3 learning rate', 'to train', 'model']]",[],"[['Hyperparameters', 'use', 'Adam optimization algorithm']]",[],document_classification,9,141
1086,hyperparameters,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,"[('use', (1, 2)), ('of', (4, 5)), ('for', (6, 7)), ('optimizes', (11, 12)), ('converges to', (16, 18))]","[('3 iteration', (2, 4)), ('routing', (5, 6)), ('all datasets', (7, 9)), ('loss', (13, 14)), ('lower loss', (19, 21))]","[['3 iteration', 'of', 'routing'], ['routing', 'for', 'all datasets'], ['3 iteration', 'converges to', 'lower loss']]",[],"[['Hyperparameters', 'use', '3 iteration']]",[],document_classification,9,142
1087,baselines,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .","[('evaluate', (5, 6)), ('including', (15, 16))]","[('several strong baseline methods', (11, 15)), ('LSTM / Bi - LSTM', (17, 22)), ('tree - structured LSTM ( Tree - LSTM )', (23, 32)), ('LSTM regularized by linguistic knowledge ( LR - LSTM )', (33, 43)), ('CNNrand / CNN - static / CNN - non-static', (44, 53)), ('very deep convolutional network ( VD - CNN )', (59, 68)), ('character - level convolutional network ( CL - CNN )', (70, 80))]","[['several strong baseline methods', 'including', 'LSTM / Bi - LSTM'], ['several strong baseline methods', 'including', 'tree - structured LSTM ( Tree - LSTM )'], ['several strong baseline methods', 'including', 'LSTM regularized by linguistic knowledge ( LR - LSTM )'], ['several strong baseline methods', 'including', 'very deep convolutional network ( VD - CNN )'], ['several strong baseline methods', 'including', 'character - level convolutional network ( CL - CNN )']]","[['several strong baseline methods', 'name', 'LSTM / Bi - LSTM']]","[['Baselines', 'evaluate', 'several strong baseline methods']]",[],document_classification,9,144
1088,results,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .","[('observe', (5, 6)), ('achieve', (10, 11)), ('on', (13, 14)), ('verifies', (21, 22))]","[('capsule networks', (8, 10)), ('best results', (11, 13)), ('4 out of 6 benchmarks', (14, 19)), ('effectiveness', (23, 24))]","[['capsule networks', 'achieve', 'best results'], ['best results', 'on', '4 out of 6 benchmarks'], ['best results', 'verifies', 'effectiveness'], ['4 out of 6 benchmarks', 'verifies', 'effectiveness']]",[],"[['Results', 'observe', 'capsule networks']]",[],document_classification,9,149
1089,research-problem,Deep Joint Entity Disambiguation with Local Neural Attention,[],"[('Deep Joint Entity Disambiguation', (0, 4))]",[],[],[],[],entity_linking,0,2
1090,research-problem,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .,[],"[('Entity disambiguation ( ED )', (0, 5))]",[],[],[],[],entity_linking,0,9
1091,research-problem,"In recent years , many text and language understanding tasks have been advanced by neural network architectures .",[],[],[],[],[],[],entity_linking,0,15
1092,experimental-setup,All models are implemented in the Torch framework .,"[('implemented in', (3, 5))]","[('Torch framework', (6, 8))]",[],[],"[['Experimental setup', 'implemented in', 'Torch framework']]",[],entity_linking,0,179
1093,tasks,We use Adagrad with a learning rate of 0.3 .,"[('use', (1, 2)), ('with', (3, 4)), ('of', (7, 8))]","[('Adagrad', (2, 3)), ('learning rate', (5, 7)), ('0.3', (8, 9))]","[['Adagrad', 'with', 'learning rate'], ['learning rate', 'of', '0.3']]",[],[],[],entity_linking,0,186
1094,tasks,"We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .","[('choose', (1, 2))]","[('embedding size d', (2, 5)), ('300', (6, 7)), ('pre-trained ( fixed ) Word2 Vec word vectors', (8, 16))]",[],[],[],[],entity_linking,0,187
1095,tasks,"Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .","[('trained on', (7, 9)), ('validated on', (17, 19)), ('tested on', (23, 25))]","[('AIDA - train ( multiple epochs )', (9, 16)), ('AIDA - A', (19, 22))]","[['AIDA - train ( multiple epochs )', 'validated on', 'AIDA - A']]",[],[],[],entity_linking,0,199
1096,tasks,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .","[('use', (1, 2)), ('with', (3, 4)), ('of', (6, 7)), ('until', (10, 11)), ('exceeds', (13, 14)), ('setting it to', (18, 21))]","[('Adam', (2, 3)), ('learning rate', (4, 6)), ('1e - 4', (7, 10)), ('validation accuracy', (11, 13)), ('90 %', (14, 16)), ('1e - 5', (21, 24))]","[['Adam', 'with', 'learning rate'], ['learning rate', 'of', '1e - 4'], ['1e - 4', 'until', 'validation accuracy'], ['validation accuracy', 'exceeds', '90 %'], ['validation accuracy', 'setting it to', '1e - 5'], ['90 %', 'setting it to', '1e - 5']]",[],[],[],entity_linking,0,200
1097,experimental-setup,Variable size mini-batches consisting of all mentions in a document are used during training .,"[('consisting of', (3, 5)), ('used during', (11, 13))]","[('Variable size mini-batches', (0, 3)), ('training', (13, 14))]",[],[],[],"[['Experimental setup', 'has', 'Variable size mini-batches']]",entity_linking,0,201
1098,tasks,"Hyper- parameters of the best validated global model are : ? = 0.01 , K = 100 , R = 25 , S = 7 , ? = 0.5 , T = 10 .","[('of', (2, 3)), ('are', (8, 9))]","[('Hyper- parameters', (0, 2)), ('best validated global model', (4, 8)), ('? = 0.01', (10, 13))]","[['Hyper- parameters', 'of', 'best validated global model'], ['best validated global model', 'are', '? = 0.01']]",[],[],"[['Tasks', 'has', 'Hyper- parameters']]",entity_linking,0,203
1099,tasks,"For the local model , R = 50 was best .","[('For', (0, 1)), ('was', (8, 9))]","[('local model', (2, 4)), ('R = 50', (5, 8)), ('best', (9, 10))]","[['R = 50', 'was', 'best']]","[['local model', 'has', 'R = 50'], ['R = 50', 'has', 'best']]",[],[],entity_linking,0,204
1100,tasks,"To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .","[('use', (4, 5)), ('after', (19, 20))]","[('early stopping', (5, 7)), ('validation accuracy', (14, 16)), ('does not increase', (16, 19)), ('500 epochs', (20, 22))]","[['does not increase', 'after', '500 epochs']]","[['validation accuracy', 'has', 'does not increase']]",[],[],entity_linking,0,206
1101,experimental-setup,"By using diagonal matrices A , B , C , we keep the number of parameters very low ( approx. 1.2 K parameters ) .","[('using', (1, 2)), ('keep', (11, 12)), ('approx.', (19, 20))]","[('diagonal matrices A , B , C', (2, 9)), ('number of parameters', (13, 16)), ('very low', (16, 18))]","[['diagonal matrices A , B , C', 'keep', 'number of parameters']]","[['number of parameters', 'has', 'very low']]","[['Experimental setup', 'using', 'diagonal matrices A , B , C']]",[],entity_linking,0,208
1102,tasks,"We also experimented with diagonal plus low - rank matrices , but encountered quality degradation .","[('experimented with', (2, 4)), ('encountered', (12, 13))]","[('diagonal plus low - rank matrices', (4, 10)), ('quality degradation', (13, 15))]",[],[],[],[],entity_linking,0,210
1103,results,Our method outperforms the well established Wikipedia link measure and the method of using less information ( only word - entity statistics ) .,"[('of using', (12, 14))]","[('outperforms', (2, 3)), ('well established Wikipedia link measure', (4, 9)), ('less information ( only word - entity statistics', (14, 22))]","[['outperforms', 'of using', 'less information ( only word - entity statistics']]","[['outperforms', 'has', 'well established Wikipedia link measure']]",[],"[['Results', 'has', 'outperforms']]",entity_linking,0,213
1104,results,"We emphasize that our global ED model outperforms Huang 's ED model , likely due to the power of our local and joint neural network architectures .","[('emphasize', (1, 2)), ('of', (18, 19))]","[('our global ED model', (3, 7)), ('outperforms', (7, 8)), (""Huang 's ED model"", (8, 12)), ('power', (17, 18)), ('our local and joint neural network architectures', (19, 26))]","[['power', 'of', 'our local and joint neural network architectures']]","[['our global ED model', 'has', 'outperforms'], ['outperforms', 'has', ""Huang 's ED model""]]","[['Results', 'emphasize', 'our global ED model']]",[],entity_linking,0,218
1105,research-problem,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation,[],[],[],[],[],[],entity_linking,1,2
1106,research-problem,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .","[('propose', (5, 6)), ('for', (15, 16))]","[('named entity disambiguation ( NED )', (16, 22))]",[],[],"[['Research problem', 'propose', 'named entity disambiguation ( NED )']]",[],entity_linking,1,5
1107,research-problem,Named entity disambiguation ( NED ) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base ( KB ) .,[],"[('Named entity disambiguation ( NED )', (0, 6))]",[],[],[],[],entity_linking,1,12
1108,model,"In this paper , we describe a new contextualized embedding model for words and entities for NED .","[('for', (15, 16))]","[('words', (12, 13)), ('NED', (16, 17))]",[],[],[],[],entity_linking,1,18
1109,model,"Following , the proposed model is based on the bidirectional transformer encoder .","[('based on', (6, 8))]","[('bidirectional transformer encoder', (9, 12))]",[],[],"[['Model', 'based on', 'bidirectional transformer encoder']]",[],entity_linking,1,19
1110,model,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .","[('takes', (1, 2)), ('in', (8, 9)), ('produces', (14, 15)), ('for', (18, 19))]","[('sequence of words and entities', (3, 8)), ('input text', (10, 12)), ('contextualized embedding', (16, 18)), ('each word and entity', (19, 23))]","[['sequence of words and entities', 'in', 'input text'], ['contextualized embedding', 'for', 'each word and entity']]",[],"[['Model', 'takes', 'sequence of words and entities']]",[],entity_linking,1,20
1111,model,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .","[('propose', (5, 6)), ('based on', (25, 27)), ('in', (31, 32))]","[('masked entity prediction', (6, 9)), ('randomly masked entities', (22, 25)), ('words and non-masked entities', (27, 31)), ('input text', (33, 35))]","[['randomly masked entities', 'based on', 'words and non-masked entities'], ['words and non-masked entities', 'in', 'input text']]",[],"[['Model', 'propose', 'masked entity prediction']]",[],entity_linking,1,21
1112,model,The NED model addresses the task by capturing word - based and entity - based contextual information using the trained contextualized embeddings .,"[('addresses', (3, 4)), ('by capturing', (6, 8)), ('using', (17, 18))]","[('NED model', (1, 3)), ('task', (5, 6)), ('word - based and entity - based contextual information', (8, 17)), ('trained contextualized embeddings', (19, 22))]","[['NED model', 'addresses', 'task'], ['task', 'by capturing', 'word - based and entity - based contextual information'], ['word - based and entity - based contextual information', 'using', 'trained contextualized embeddings']]","[['NED model', 'has', 'task']]",[],"[['Model', 'has', 'NED model']]",entity_linking,1,24
1113,experiments,"We also set the feed - forward / filter size to 4096 , the dropout probability applied to all layers was 0.1 , and the maximum word length in an input sequence was set to 512 .","[('set', (2, 3)), ('to', (10, 11)), ('applied to', (16, 18)), ('was', (20, 21)), ('in', (28, 29)), ('set to', (33, 35))]","[('feed - forward / filter size', (4, 10)), ('4096', (11, 12)), ('dropout probability', (14, 16)), ('all layers', (18, 20)), ('0.1', (21, 22)), ('maximum word length', (25, 28)), ('input sequence', (30, 32)), ('512', (35, 36))]","[['feed - forward / filter size', 'to', '4096'], ['dropout probability', 'applied to', 'all layers'], ['dropout probability', 'was', '0.1'], ['all layers', 'was', '0.1'], ['maximum word length', 'in', 'input sequence'], ['maximum word length', 'set to', '512']]","[['feed - forward / filter size', 'has', '4096']]",[],[],entity_linking,1,63
1114,hyperparameters,"Other parameters , namely the parameters in the MEP and the embeddings for entities , were initialized randomly .","[('namely', (3, 4)), ('in', (6, 7)), ('for', (12, 13))]","[('Other parameters', (0, 2)), ('parameters', (5, 6)), ('MEP and the embeddings', (8, 12)), ('entities', (13, 14)), ('initialized randomly', (16, 18))]","[['Other parameters', 'namely', 'parameters'], ['parameters', 'in', 'MEP and the embeddings'], ['MEP and the embeddings', 'for', 'entities']]",[],[],"[['Hyperparameters', 'has', 'Other parameters']]",entity_linking,1,66
1115,experiments,"We used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .","[('used', (1, 2)), ('with', (5, 6)), ('of', (9, 10)), ('of', (27, 28))]","[('Adam optimizer', (3, 5)), ('learning rate', (7, 9)), ('2 e - 5', (10, 14)), ('L2 weight decay', (24, 27)), ('0.01', (28, 29))]","[['Adam optimizer', 'with', 'learning rate'], ['Adam optimizer', 'with', 'L2 weight decay'], ['learning rate', 'of', '2 e - 5'], ['L2 weight decay', 'of', '0.01']]","[['Adam optimizer', 'has', 'learning rate'], ['L2 weight decay', 'has', '0.01']]",[],[],entity_linking,1,73
1116,hyperparameters,The batch size was set to 252 .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('252', (6, 7))]","[['batch size', 'set to', '252']]","[['batch size', 'has', '252']]",[],"[['Hyperparameters', 'has', 'batch size']]",entity_linking,1,74
1117,experiments,"We set the batch size to 32 , and used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .","[('set', (1, 2)), ('to', (5, 6)), ('used', (9, 10)), ('with', (13, 14)), ('of', (17, 18)), ('of', (35, 36))]","[('batch size', (3, 5)), ('32', (6, 7)), ('Adam optimizer', (11, 13)), ('learning rate', (15, 17)), ('2 e - 5', (18, 22)), ('?1 = 0.9 , ?2 = 0.999', (23, 30)), ('L2 weight decay', (32, 35)), ('0.01', (36, 37))]","[['batch size', 'to', '32'], ['Adam optimizer', 'with', 'learning rate'], ['Adam optimizer', 'with', '?1 = 0.9 , ?2 = 0.999'], ['Adam optimizer', 'with', 'L2 weight decay'], ['learning rate', 'of', '2 e - 5'], ['learning rate', 'of', '2 e - 5'], ['L2 weight decay', 'of', '0.01']]","[['batch size', 'has', '32'], ['Adam optimizer', 'has', 'learning rate'], ['learning rate', 'has', '2 e - 5'], ['L2 weight decay', 'has', '0.01']]",[],[],entity_linking,1,106
1118,results,"As shown , our models outperformed all previously proposed models .",[],"[('outperformed', (5, 6))]",[],[],[],[],entity_linking,1,113
1119,results,"Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .","[('using', (2, 3)), ('boosted', (6, 7)), ('by', (9, 10))]","[('pseudo entity annotations', (3, 6)), ('accuracy', (8, 9)), ('0.3 %', (10, 12))]","[['pseudo entity annotations', 'boosted', 'accuracy'], ['accuracy', 'by', '0.3 %']]",[],"[['Results', 'using', 'pseudo entity annotations']]",[],entity_linking,1,114
1120,research-problem,Deep contextualized word representations,[],[],[],[],[],[],entity_linking,10,2
1121,model,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .","[('introduce', (5, 6)), ('in', (36, 37)), ('across', (40, 41))]","[('deep contextualized word representation', (10, 14)), ('existing models', (25, 27)), ('significantly improves', (29, 31)), ('state of the art', (32, 36))]",[],"[['significantly improves', 'has', 'state of the art']]","[['Model', 'introduce', 'deep contextualized word representation']]",[],entity_linking,10,12
1122,model,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"[('differ from', (2, 4)), ('in', (8, 9)), ('assigned', (13, 14)), ('of', (20, 21))]","[('traditional word type embeddings', (4, 8)), ('each token', (10, 12)), ('representation', (15, 16)), ('function', (19, 20))]","[['traditional word type embeddings', 'in', 'each token'], ['each token', 'assigned', 'representation']]",[],"[['Model', 'differ from', 'traditional word type embeddings']]",[],entity_linking,10,13
1123,model,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,"[('use', (1, 2)), ('derived from', (3, 5)), ('trained with', (10, 12)), ('on', (22, 23))]","[('vectors', (2, 3)), ('bidirectional LSTM', (6, 8)), ('coupled lan - guage model ( LM ) objective', (13, 22)), ('large text corpus', (24, 27))]","[['vectors', 'derived from', 'bidirectional LSTM'], ['bidirectional LSTM', 'trained with', 'coupled lan - guage model ( LM ) objective'], ['coupled lan - guage model ( LM ) objective', 'on', 'large text corpus']]",[],"[['Model', 'use', 'vectors']]",[],entity_linking,10,14
1124,model,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","[('call', (5, 6))]","[('ELMo ( Embeddings from Language Models ) representations', (7, 15))]",[],[],"[['Model', 'call', 'ELMo ( Embeddings from Language Models ) representations']]",[],entity_linking,10,15
1125,model,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .","[('are', (11, 12)), ('function of', (21, 23)), ('of', (28, 29))]","[('ELMo representations', (9, 11)), ('deep', (12, 13)), ('all of the internal layers', (23, 28)), ('biLM', (30, 31))]","[['ELMo representations', 'are', 'deep'], ['ELMo representations', 'function of', 'all of the internal layers'], ['all of the internal layers', 'of', 'biLM']]","[['ELMo representations', 'has', 'deep']]",[],"[['Model', 'has', 'ELMo representations']]",entity_linking,10,16
1126,model,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .","[('learn', (4, 5)), ('stacked above', (11, 13)), ('for', (16, 17)), ('markedly', (22, 23)), ('over', (25, 26))]","[('linear combination of the vectors', (6, 11)), ('each input word', (13, 16)), ('each end task', (17, 20)), ('performance', (24, 25)), ('using the top LSTM layer', (27, 32))]","[['linear combination of the vectors', 'stacked above', 'each input word'], ['each input word', 'for', 'each end task'], ['performance', 'over', 'using the top LSTM layer']]",[],"[['Model', 'learn', 'linear combination of the vectors']]",[],entity_linking,10,17
1127,model,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision that are most useful for each end task .","[('Simultaneously exposing', (0, 2)), ('is', (6, 7)), ('allowing', (10, 11)), ('for', (23, 24))]","[('all of these signals', (2, 6)), ('highly beneficial', (7, 9)), ('learned models', (12, 14)), ('types of semi-supervision', (16, 19)), ('most useful', (21, 23)), ('each end task', (24, 27))]","[['all of these signals', 'is', 'highly beneficial'], ['all of these signals', 'allowing', 'learned models'], ['highly beneficial', 'allowing', 'learned models'], ['most useful', 'for', 'each end task']]","[['all of these signals', 'has', 'highly beneficial']]","[['Model', 'Simultaneously exposing', 'all of these signals']]",[],entity_linking,10,20
1128,model,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .","[('benefits from', (3, 5)), ('through the use of', (7, 11)), ('seamlessly incorporate', (16, 18)), ('into', (20, 21)), ('without explicitly', (23, 25))]","[('subword units', (5, 7)), ('character convolutions', (11, 13)), ('multi-sense information', (18, 20)), ('downstream tasks', (21, 23)), ('predefined sense classes', (28, 31))]","[['subword units', 'through the use of', 'character convolutions'], ['multi-sense information', 'into', 'downstream tasks']]",[],"[['Model', 'benefits from', 'subword units']]",[],entity_linking,10,31
1129,model,context2vec uses a bidirectional Long Short Term Memory ( LSTM ; to encode the context around a pivot word .,"[('uses', (1, 2)), ('to encode', (11, 13)), ('around', (15, 16))]","[('context2vec', (0, 1)), ('bidirectional Long Short Term Memory ( LSTM ;', (3, 11)), ('context', (14, 15)), ('pivot word', (17, 19))]","[['context2vec', 'uses', 'bidirectional Long Short Term Memory ( LSTM ;'], ['bidirectional Long Short Term Memory ( LSTM ;', 'to encode', 'context'], ['context', 'around', 'pivot word']]","[['context2vec', 'name', 'bidirectional Long Short Term Memory ( LSTM ;']]",[],"[['Model', 'has', 'context2vec']]",entity_linking,10,33
1130,model,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .","[('show', (1, 2)), ('induced by', (7, 9)), ('of', (14, 15)), ('that mix', (31, 33))]","[('similar signals', (3, 5)), ('modified language model objective', (10, 14)), ('our ELMo representations', (15, 18)), ('downstream tasks', (29, 31))]","[['similar signals', 'induced by', 'modified language model objective'], ['modified language model objective', 'of', 'our ELMo representations']]",[],"[['Model', 'show', 'similar signals']]",[],entity_linking,10,43
1131,model,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .","[('with', (7, 8)), ('fix', (12, 13)), ('add', (16, 17)), ('allowing us to leverage', (22, 26)), ('for cases', (33, 35)), ('dictates', (40, 41))]","[('biLM', (6, 7)), ('unlabeled data', (8, 10)), ('weights', (14, 15)), ('additional taskspecific model capacity', (17, 21)), ('large , rich and universal biLM representations', (26, 33)), ('downstream training data size', (36, 40)), ('smaller supervised model', (42, 45))]","[['biLM', 'with', 'unlabeled data'], ['biLM', 'fix', 'weights'], ['additional taskspecific model capacity', 'allowing us to leverage', 'large , rich and universal biLM representations'], ['large , rich and universal biLM representations', 'for cases', 'downstream training data size'], ['downstream training data size', 'dictates', 'smaller supervised model']]",[],[],[],entity_linking,10,45
1132,experiments,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .","[('adding', (6, 7)), ('establishes', (8, 9)), ('with', (20, 21)), ('ranging from', (24, 26)), ('over', (30, 31))]","[('ELMo', (7, 8)), ('relative error reductions', (21, 24)), ('6 - 20 %', (26, 30)), ('strong base models', (31, 34))]","[['relative error reductions', 'ranging from', '6 - 20 %'], ['6 - 20 %', 'over', 'strong base models']]",[],[],[],entity_linking,10,99
1133,experiments,"After adding ELMo to the baseline model , test set F 1 improved by 4.7 % from 81.1 % to 85.8 % , a 24.9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1.4 % .","[('adding', (1, 2)), ('to', (3, 4)), ('improved by', (12, 14)), ('from', (16, 17)), ('to', (19, 20)), ('improving', (34, 35)), ('by', (46, 47))]","[('ELMo', (2, 3)), ('baseline model', (5, 7)), ('test set F 1', (8, 12)), ('4.7 %', (14, 16)), ('81.1 %', (17, 19)), ('85.8 %', (20, 22)), ('24.9 % relative error reduction', (24, 29)), ('overall single model state - of - the - art', (36, 46)), ('1.4 %', (47, 49))]","[['ELMo', 'to', 'baseline model'], ['81.1 %', 'to', '85.8 %'], ['test set F 1', 'improved by', '4.7 %'], ['4.7 %', 'from', '81.1 %'], ['81.1 %', 'to', '85.8 %'], ['test set F 1', 'improving', 'overall single model state - of - the - art'], ['overall single model state - of - the - art', 'by', '1.4 %']]","[['ELMo', 'has', 'baseline model'], ['baseline model', 'has', 'test set F 1']]",[],[],entity_linking,10,107
1134,experiments,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .","[('adding', (2, 3)), ('to', (4, 5)), ('improves', (8, 9)), ('by', (10, 11)), ('across', (16, 17))]","[('ELMo', (3, 4)), ('ESIM model', (6, 8)), ('accuracy', (9, 10)), ('average of 0.7 %', (12, 16)), ('five random seeds', (17, 20))]","[['ELMo', 'to', 'ESIM model'], ['ELMo', 'improves', 'accuracy'], ['ESIM model', 'improves', 'accuracy'], ['accuracy', 'by', 'average of 0.7 %'], ['average of 0.7 %', 'across', 'five random seeds']]",[],[],[],entity_linking,10,116
1135,experiments,"As shown in , our ELMo enhanced biLSTM - CRF achieves 92. 22 % F 1 averaged over five runs .","[('achieves', (10, 11)), ('averaged over', (16, 18))]","[('our', (4, 5)), ('ELMo enhanced biLSTM - CRF', (5, 10)), ('92. 22 % F 1', (11, 16)), ('five runs', (18, 20))]","[['ELMo enhanced biLSTM - CRF', 'achieves', '92. 22 % F 1'], ['92. 22 % F 1', 'averaged over', 'five runs']]","[['our', 'has', 'ELMo enhanced biLSTM - CRF']]",[],[],entity_linking,10,128
1136,experiments,"As shown in Sec. 5.1 , using all layers instead of just the last layer improves performance across multiple tasks .","[('using', (6, 7)), ('instead of', (9, 11)), ('improves', (15, 16)), ('across', (17, 18))]","[('all layers', (7, 9)), ('last layer', (13, 15)), ('performance', (16, 17)), ('multiple tasks', (18, 20))]","[['all layers', 'instead of', 'last layer'], ['all layers', 'improves', 'performance'], ['last layer', 'improves', 'performance'], ['performance', 'across', 'multiple tasks']]",[],[],[],entity_linking,10,131
1137,experiments,Sentiment analysis,[],[],[],[],[],[],entity_linking,10,132
1138,experiments,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to = 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( = 1 vs. = 0.001 ) .","[('Averaging', (0, 1)), ('instead of using', (4, 7)), ('improves', (11, 12)), ('another', (14, 15)), ('allowing', (30, 31)), ('to learn', (34, 36)), ('improves', (39, 40)), ('another', (42, 43))]","[('all biLM layers', (1, 4)), ('last layer', (9, 11)), ('F 1', (12, 14)), ('0.3 %', (15, 17)), ('task model', (32, 34)), ('individual layer weights', (36, 39)), ('F 1', (40, 42)), ('0.2 %', (43, 45))]","[['all biLM layers', 'instead of using', 'last layer'], ['last layer', 'improves', 'F 1'], ['F 1', 'another', '0.3 %'], ['all biLM layers', 'allowing', 'task model'], ['task model', 'to learn', 'individual layer weights'], ['individual layer weights', 'improves', 'F 1'], ['F 1', 'another', '0.2 %']]","[['F 1', 'has', '0.3 %']]",[],[],entity_linking,10,148
1139,experiments,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves overall results for some tasks .","[('including', (5, 6)), ('at', (7, 8)), ('of', (10, 11)), ('in', (13, 14)), ('improves', (18, 19)), ('for', (21, 22))]","[('ELMo', (6, 7)), ('output', (9, 10)), ('biRNN', (12, 13)), ('task - specific architectures', (14, 18)), ('overall results', (19, 21)), ('some tasks', (22, 24))]","[['ELMo', 'at', 'output'], ['output', 'of', 'biRNN'], ['biRNN', 'in', 'task - specific architectures'], ['task - specific architectures', 'improves', 'overall results'], ['overall results', 'for', 'some tasks']]","[['ELMo', 'has', 'output']]",[],[],entity_linking,10,155
1140,experiments,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .","[('including', (4, 5)), ('for', (13, 14)), ('improves over', (17, 19)), ('for', (25, 26)), ('is', (36, 37)), ('included at', (41, 43))]","[('ELMo', (5, 6)), ('input and output layers', (9, 13)), ('SNLI and SQuAD', (14, 17)), ('input layer', (21, 23)), ('SRL', (26, 27)), ('highest', (37, 38)), ('input layer', (45, 47))]","[['input and output layers', 'for', 'SNLI and SQuAD'], ['SNLI and SQuAD', 'improves over', 'input layer'], ['input and output layers', 'for', 'SRL']]","[['ELMo', 'has', 'input and output layers'], ['SRL', 'has', 'highest']]",[],[],entity_linking,10,156
1141,results,the task - specific context representations are likely more important than those from the biLM .,"[('likely more', (7, 9))]","[('task - specific context representations', (1, 6)), ('biLM', (14, 15))]",[],[],[],[],entity_linking,10,161
1142,experiments,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .","[('improves', (1, 2)), ('over', (4, 5))]","[('ELMo', (0, 1)), ('task performance', (2, 4)), ('word vectors alone', (5, 8))]","[['ELMo', 'improves', 'task performance'], ['task performance', 'over', 'word vectors alone']]","[['ELMo', 'has', 'task performance']]",[],[],entity_linking,10,164
1143,experiments,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .","[('have', (7, 8)), ('of', (10, 11)), ('better at', (14, 16)), ('then', (17, 18))]","[('biLM top layer rep-resentations', (3, 7)), ('F 1', (8, 10)), ('69.0', (11, 12)), ('WSD', (16, 17)), ('first layer', (19, 21))]","[['biLM top layer rep-resentations', 'have', 'F 1'], ['F 1', 'of', '69.0'], ['biLM top layer rep-resentations', 'better at', 'WSD'], ['WSD', 'then', 'first layer']]","[['biLM top layer rep-resentations', 'has', 'F 1'], ['F 1', 'has', '69.0']]",[],[],entity_linking,10,178
1144,experiments,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .","[('using', (6, 7)), ('higher than', (12, 14)), ('consistent with', (18, 20)), ('in', (26, 27))]","[('accuracies', (5, 6)), ('first biLM layer', (8, 11)), ('top layer', (15, 17)), ('deep biL - STMs', (22, 26)), ('multi-task training and MT', (27, 31))]","[['accuracies', 'using', 'first biLM layer'], ['first biLM layer', 'higher than', 'top layer'], ['deep biL - STMs', 'in', 'multi-task training and MT']]",[],[],[],entity_linking,10,185
1145,results,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .","[('with', (8, 9)), ('of', (11, 12)), ('as', (21, 22)), ('with', (25, 26)), ('of', (28, 29))]","[('SRL', (2, 3)), ('ELMo model', (6, 8)), ('1 %', (9, 11)), ('training set', (13, 15)), ('baseline model', (23, 25)), ('10 %', (26, 28))]","[['ELMo model', 'with', '1 %'], ['baseline model', 'with', '10 %'], ['1 %', 'of', 'training set'], ['baseline model', 'with', '10 %']]","[['SRL', 'has', 'ELMo model']]",[],[],entity_linking,10,195
1146,ablation-analysis,"The output layer weights are relatively balanced , with a slight preference for the lower layers .","[('are', (4, 5)), ('with', (8, 9)), ('for', (12, 13))]","[('output layer weights', (1, 4)), ('relatively balanced', (5, 7)), ('slight preference', (10, 12)), ('lower layers', (14, 16))]","[['output layer weights', 'are', 'relatively balanced'], ['relatively balanced', 'with', 'slight preference'], ['slight preference', 'for', 'lower layers']]","[['output layer weights', 'has', 'relatively balanced']]",[],"[['Ablation analysis', 'has', 'output layer weights']]",entity_linking,10,199
1147,results,"Replacing the Glo Ve vectors with the biLM character layer gives a slight improvement for all tasks ( e.g. from 80.8 to 81.4 F 1 for SQuAD ) , but overall the improvements are small compared to the full ELMo model .","[('Replacing', (0, 1)), ('with', (5, 6)), ('gives', (10, 11)), ('for', (14, 15)), ('are', (33, 34)), ('compared to', (35, 37))]","[('Glo Ve vectors', (2, 5)), ('biLM character layer', (7, 10)), ('slight improvement', (12, 14)), ('all tasks', (15, 17)), ('improvements', (32, 33)), ('small', (34, 35)), ('full ELMo model', (38, 41))]","[['Glo Ve vectors', 'with', 'biLM character layer'], ['biLM character layer', 'gives', 'slight improvement'], ['slight improvement', 'for', 'all tasks'], ['improvements', 'are', 'small'], ['small', 'compared to', 'full ELMo model']]","[['improvements', 'has', 'small']]","[['Results', 'Replacing', 'Glo Ve vectors']]",[],entity_linking,10,205
1148,results,"From this , we conclude that most of the gains in the downstream tasks are due to the contextual information and not the sub-word information .","[('conclude', (4, 5)), ('in', (10, 11)), ('due to', (15, 17)), ('not', (21, 22))]","[('most of the gains', (6, 10)), ('downstream tasks', (12, 14)), ('contextual information', (18, 20)), ('sub-word information', (23, 25))]","[['most of the gains', 'in', 'downstream tasks'], ['most of the gains', 'due to', 'contextual information'], ['most of the gains', 'not', 'sub-word information']]",[],"[['Results', 'conclude', 'most of the gains']]",[],entity_linking,10,206
1149,tasks,"As shown in the two right hand columns of , adding Glo Ve to models with ELMo generally provides a marginal improvement over ELMo only models ( e.g. 0.2 % F 1 improvement for SRL from 84.5 to 84.7 ) .","[('adding', (10, 11)), ('to', (13, 14)), ('with', (15, 16)), ('provides', (18, 19)), ('over', (22, 23))]","[('Glo Ve', (11, 13)), ('models', (14, 15)), ('ELMo', (16, 17)), ('marginal improvement', (20, 22)), ('ELMo only models', (23, 26))]","[['Glo Ve', 'to', 'models'], ['models', 'with', 'ELMo'], ['models', 'provides', 'marginal improvement'], ['ELMo', 'provides', 'marginal improvement'], ['marginal improvement', 'over', 'ELMo only models']]",[],[],[],entity_linking,10,211
1150,research-problem,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,[],[],[],[],[],[],entity_linking,11,2
1151,research-problem,"In this article , we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation , by exploiting the semantic relationships between senses such as synonymy , hypernymy and hyponymy , in order to compress the sense vocabulary of Princeton WordNet , and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database .","[('tackle', (5, 6)), ('for', (17, 18)), ('of', (20, 21)), ('by exploiting', (25, 27)), ('between', (30, 31)), ('such as', (32, 34)), ('of', (47, 48)), ('reduce', (53, 54)), ('of', (68, 69))]","[('issue', (7, 8)), ('limited quantity of manually sense annotated corpora', (10, 17)), ('task', (19, 20)), ('word sense disambiguation', (21, 24)), ('semantic relationships', (28, 30)), ('senses', (31, 32)), ('synonymy', (34, 35)), ('sense vocabulary', (45, 47)), ('Princeton WordNet', (48, 50)), ('number of different sense tags', (55, 60)), ('all words', (66, 68)), ('lexical database', (70, 72))]","[['limited quantity of manually sense annotated corpora', 'for', 'task'], ['task', 'of', 'word sense disambiguation'], ['word sense disambiguation', 'by exploiting', 'semantic relationships'], ['semantic relationships', 'between', 'senses'], ['senses', 'such as', 'synonymy'], ['sense vocabulary', 'of', 'Princeton WordNet'], ['semantic relationships', 'reduce', 'number of different sense tags'], ['all words', 'of', 'lexical database']]","[['issue', 'has', 'limited quantity of manually sense annotated corpora']]","[['Research problem', 'tackle', 'issue']]",[],entity_linking,11,4
1152,research-problem,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .","[('present', (7, 8)), ('relies on', (12, 14)), ('to achieve', (20, 22)), ('that', (23, 24)), ('on', (31, 32))]","[('WSD system', (9, 11)), ('pre-trained BERT word vectors', (14, 18)), ('results', (22, 23)), ('significantly outperforms', (24, 26)), ('state of the art', (27, 31)), ('all WSD evaluation tasks', (32, 36))]","[['WSD system', 'relies on', 'pre-trained BERT word vectors'], ['pre-trained BERT word vectors', 'to achieve', 'results'], ['results', 'that', 'significantly outperforms'], ['state of the art', 'on', 'all WSD evaluation tasks']]","[['results', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'state of the art']]","[['Research problem', 'present', 'WSD system']]",[],entity_linking,11,6
1153,research-problem,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,11,8
1154,model,"Therefore , we propose two different methods for building this subset and we call them sense vocabulary compression methods .","[('propose', (3, 4)), ('for', (7, 8)), ('call them', (13, 15))]","[('two different methods', (4, 7)), ('building', (8, 9)), ('subset', (10, 11)), ('sense vocabulary compression methods', (15, 19))]","[['two different methods', 'for', 'building'], ['two different methods', 'call them', 'sense vocabulary compression methods']]","[['building', 'has', 'subset']]","[['Model', 'propose', 'two different methods']]",[],entity_linking,11,24
1155,research-problem,"Current state of the art supervised WSD systems such as , , and are all confronted to the following issues :",[],"[('supervised WSD', (5, 7))]",[],[],[],[],entity_linking,11,65
1156,model,"In order to overcome all these issues , we propose a method for grouping together multiple sense tags that refer in fact to the same concept .","[('propose', (9, 10)), ('for grouping together', (12, 15)), ('refer in fact to', (19, 23))]","[('method', (11, 12)), ('multiple sense tags', (15, 18)), ('same concept', (24, 26))]","[['method', 'for grouping together', 'multiple sense tags'], ['multiple sense tags', 'refer in fact to', 'same concept']]",[],"[['Model', 'propose', 'method']]",[],entity_linking,11,71
1157,research-problem,From Senses to Synsets : A Vocabulary Compression Based on Synonymy,[],"[('Vocabulary Compression', (6, 8))]",[],[],[],[],entity_linking,11,73
1158,experimental-setup,"For BERT , we used the model named "" bert - largecased "" of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .","[('For', (0, 1)), ('used', (4, 5)), ('of', (13, 14)), ('consists of', (20, 22)), ('of', (23, 24)), ('trained on', (27, 29))]","[('BERT', (1, 2)), ('model named', (6, 8)), ('"" bert - largecased ""', (8, 13)), ('PyTorch implementation', (15, 17)), ('vectors', (22, 23)), ('dimension 1024', (24, 26)), ('Book s Corpus', (29, 32)), ('English Wikipedia', (33, 35))]","[['BERT', 'used', 'model named'], ['"" bert - largecased ""', 'of', 'PyTorch implementation'], ['vectors', 'of', 'dimension 1024'], ['PyTorch implementation', 'consists of', 'vectors'], ['vectors', 'of', 'dimension 1024'], ['vectors', 'trained on', 'Book s Corpus'], ['vectors', 'trained on', 'English Wikipedia']]","[['BERT', 'has', 'model named'], ['model named', 'has', '"" bert - largecased ""']]","[['Experimental setup', 'For', 'BERT']]",[],entity_linking,11,132
1159,experimental-setup,"For the Transformer encoder layers , we used the same parameters as the "" base "" model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .","[('For', (0, 1)), ('used', (7, 8)), ('as', (11, 12)), ('with', (23, 24))]","[('Transformer encoder layers', (2, 5)), ('same parameters', (9, 11)), ('"" base "" model', (13, 17)), ('6 layers', (21, 23)), ('8 attention heads', (24, 27)), ('hidden size', (29, 31)), ('2048', (32, 33)), ('dropout', (36, 37)), ('0.1', (38, 39))]","[['Transformer encoder layers', 'used', 'same parameters'], ['same parameters', 'as', '"" base "" model'], ['6 layers', 'with', '8 attention heads'], ['6 layers', 'with', 'dropout']]","[['Transformer encoder layers', 'has', 'same parameters'], ['same parameters', 'has', '"" base "" model']]","[['Experimental setup', 'For', 'Transformer encoder layers']]",[],entity_linking,11,134
1160,experiments,We performed every training for 20 epochs .,"[('performed', (1, 2)), ('for', (4, 5))]","[('every training', (2, 4)), ('20 epochs', (5, 7))]","[['every training', 'for', '20 epochs']]",[],[],[],entity_linking,11,140
1161,baselines,"3 . A "" all relations "" system which applies our second vocabulary compression through all relations on the training corpus .","[('applies', (9, 10)), ('through', (14, 15)), ('on', (17, 18))]","[('"" all relations "" system', (3, 8)), ('our second vocabulary compression', (10, 14)), ('all relations', (15, 17)), ('training corpus', (19, 21))]","[['"" all relations "" system', 'applies', 'our second vocabulary compression'], ['our second vocabulary compression', 'through', 'all relations'], ['all relations', 'on', 'training corpus']]",[],[],"[['Baselines', 'has', '"" all relations "" system']]",entity_linking,11,147
1162,experiments,"We trained with mini-batches of 100 sentences , truncated to 80 words , and we used Adam with a learning rate of 0.0001 as the optimization method .","[('trained with', (1, 3)), ('of', (4, 5)), ('truncated to', (8, 10)), ('used', (15, 16)), ('with', (17, 18)), ('of', (21, 22)), ('as', (23, 24))]","[('mini-batches', (3, 4)), ('100 sentences', (5, 7)), ('80 words', (10, 12)), ('Adam', (16, 17)), ('learning rate', (19, 21)), ('0.0001', (22, 23)), ('optimization method', (25, 27))]","[['mini-batches', 'of', '100 sentences'], ['learning rate', 'of', '0.0001'], ['100 sentences', 'truncated to', '80 words'], ['Adam', 'with', 'learning rate'], ['learning rate', 'of', '0.0001'], ['0.0001', 'as', 'optimization method']]","[['mini-batches', 'has', '100 sentences'], ['learning rate', 'has', '0.0001']]",[],[],entity_linking,11,148
1163,experimental-setup,All models have been trained on one Nvidia 's Titan X GPU .,"[('trained on', (4, 6))]","[(""one Nvidia 's Titan X GPU"", (6, 12))]",[],[],"[['Experimental setup', 'trained on', ""one Nvidia 's Titan X GPU""]]",[],entity_linking,11,149
1164,results,"In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .","[('use', (12, 13)), ('through', (17, 18)), ('obtain', (23, 24)), ('that are', (25, 27)), ('to', (29, 30))]","[('our systems', (9, 11)), ('sense vocabulary compression', (14, 17)), ('hypernyms or', (18, 20)), ('through all relations', (20, 23)), ('scores', (24, 25)), ('overall equivalent', (27, 29)), ('systems that do not use it', (31, 37))]","[['our systems', 'use', 'sense vocabulary compression'], ['sense vocabulary compression', 'through', 'hypernyms or'], ['sense vocabulary compression', 'through', 'through all relations'], ['through all relations', 'obtain', 'scores'], ['scores', 'that are', 'overall equivalent'], ['overall equivalent', 'to', 'systems that do not use it']]","[['hypernyms or', 'has', 'through all relations']]",[],[],entity_linking,11,164
1165,results,"In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .","[('thanks to', (7, 9)), ('added to', (14, 16)), ('use of', (21, 23)), ('as', (24, 25)), ('on', (36, 37))]","[('Princeton WordNet Gloss Corpus', (10, 14)), ('training data', (17, 19)), ('BERT', (23, 24)), ('input embeddings', (25, 27)), ('outperform', (29, 30)), ('systematically', (30, 31)), ('state of the art', (32, 36)), ('every task', (37, 39))]","[['Princeton WordNet Gloss Corpus', 'added to', 'training data'], ['Princeton WordNet Gloss Corpus', 'use of', 'BERT'], ['BERT', 'as', 'input embeddings'], ['state of the art', 'on', 'every task']]","[['outperform', 'has', 'systematically'], ['systematically', 'has', 'state of the art']]","[['Results', 'thanks to', 'Princeton WordNet Gloss Corpus']]",[],entity_linking,11,169
1166,ablation-analysis,"As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .","[('the use of', (16, 19)), ('as', (20, 21)), ('have', (24, 25)), ('on', (28, 29)), ('lead to', (32, 34))]","[('additional training corpus ( WNGC )', (7, 13)), ('BERT', (19, 20)), ('input embeddings', (21, 23)), ('major impact', (26, 28)), ('our results', (29, 31)), ('scores above the state of the art', (34, 41))]","[['BERT', 'as', 'input embeddings'], ['input embeddings', 'have', 'major impact'], ['major impact', 'on', 'our results']]",[],[],[],entity_linking,11,180
1167,ablation-analysis,"Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .","[('Using', (0, 1)), ('instead of', (2, 4)), ('improves', (8, 9)), ('by', (12, 13)), ('adding', (23, 24)), ('to', (26, 27)), ('improves', (30, 31)), ('by', (32, 33))]","[('BERT', (1, 2)), ('ELMo or Glo Ve', (4, 8)), ('score', (11, 12)), ('approximately 3 and 5 points', (13, 18)), ('WNGC', (25, 26)), ('training data', (28, 30)), ('approximately 2 points', (33, 36))]","[['BERT', 'instead of', 'ELMo or Glo Ve'], ['BERT', 'improves', 'score'], ['ELMo or Glo Ve', 'improves', 'score'], ['score', 'by', 'approximately 3 and 5 points'], ['BERT', 'adding', 'WNGC'], ['ELMo or Glo Ve', 'adding', 'WNGC'], ['WNGC', 'to', 'training data']]",[],"[['Ablation analysis', 'Using', 'BERT']]",[],entity_linking,11,181
1168,ablation-analysis,"Finally , through the scores obtained by invidual models ( without ensemble ) , we can observe on the standard deviations that the vocabulary compression method through hypernyms never impact significantly the final score .","[('through', (2, 3)), ('obtained by', (5, 7)), ('observe on', (16, 18)), ('through', (26, 27)), ('never', (28, 29))]","[('scores', (4, 5)), ('invidual models ( without ensemble )', (7, 13)), ('standard deviations', (19, 21)), ('vocabulary compression method', (23, 26)), ('hypernyms', (27, 28)), ('significantly', (30, 31)), ('final score', (32, 34))]","[['vocabulary compression method', 'through', 'hypernyms'], ['scores', 'obtained by', 'invidual models ( without ensemble )'], ['scores', 'observe on', 'standard deviations'], ['vocabulary compression method', 'through', 'hypernyms']]","[['standard deviations', 'has', 'vocabulary compression method'], ['significantly', 'has', 'final score']]","[['Ablation analysis', 'through', 'scores']]",[],entity_linking,11,185
1169,ablation-analysis,"However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .","[('through', (5, 6)), ('seems to', (8, 10)), ('in', (14, 15))]","[('compression method', (3, 5)), ('all relations', (6, 8)), ('negatively impact', (10, 12)), ('results', (13, 14)), ('some cases', (15, 17)), ('ELMo or GloVe', (20, 23))]","[['compression method', 'through', 'all relations'], ['all relations', 'seems to', 'negatively impact'], ['results', 'in', 'some cases']]","[['negatively impact', 'has', 'results']]",[],[],entity_linking,11,186
1170,research-problem,Incorporating Glosses into Neural Word Sense Disambiguation,[],[],[],[],[],[],entity_linking,12,2
1171,research-problem,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,12,4
1172,research-problem,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .","[('models', (1, 2)), ('between', (5, 6)), ('in', (11, 12)), ('of', (22, 23))]","[('GAS', (0, 1)), ('semantic relationship', (3, 5)), ('context and the gloss', (7, 11)), ('improved memory network framework', (13, 17)), ('previous supervised methods and knowledge - based methods', (24, 32))]","[['GAS', 'models', 'semantic relationship'], ['semantic relationship', 'between', 'context and the gloss'], ['context and the gloss', 'in', 'improved memory network framework']]",[],[],[],entity_linking,12,9
1173,research-problem,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,12,13
1174,model,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","[('propose', (5, 6)), ('variant of', (21, 23))]","[('novel model GAS', (7, 10)), ('gloss - augmented WSD neural network', (12, 18)), ('memory network', (24, 26))]","[['gloss - augmented WSD neural network', 'variant of', 'memory network']]","[['novel model GAS', 'has', 'gloss - augmented WSD neural network']]","[['Model', 'propose', 'novel model GAS']]",[],entity_linking,12,30
1175,model,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"[('jointly encodes', (1, 3)), ('of', (7, 8)), ('models', (12, 13)), ('between', (16, 17)), ('in', (21, 22))]","[('GAS', (0, 1)), ('context and glosses', (4, 7)), ('target word', (9, 11)), ('semantic relationship', (14, 16)), ('context and glosses', (18, 21)), ('memory module', (23, 25))]","[['GAS', 'jointly encodes', 'context and glosses'], ['context and glosses', 'of', 'target word'], ['GAS', 'models', 'semantic relationship'], ['semantic relationship', 'between', 'context and glosses'], ['context and glosses', 'in', 'memory module']]",[],[],"[['Model', 'has', 'GAS']]",entity_linking,12,31
1176,model,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .","[('to measure', (2, 4)), ('between', (7, 8)), ('employ', (15, 16)), ('within', (19, 20)), ('as', (22, 23)), ('adopt', (27, 28))]","[('inner relationship', (5, 7)), ('glosses and context', (8, 11)), ('more accurately', (11, 13)), ('multiple passes operation', (16, 19)), ('memory', (21, 22)), ('re-reading process', (24, 26)), ('two memory updating mechanisms', (28, 32))]","[['inner relationship', 'between', 'glosses and context'], ['inner relationship', 'employ', 'multiple passes operation'], ['multiple passes operation', 'within', 'memory'], ['memory', 'as', 're-reading process'], ['multiple passes operation', 'adopt', 'two memory updating mechanisms']]","[['glosses and context', 'has', 'more accurately']]","[['Model', 'to measure', 'inner relationship']]",[],entity_linking,12,32
1177,model,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .","[('to model', (2, 4)), ('of', (6, 7)), ('propose', (12, 13)), ('in', (20, 21))]","[('semantic relationship', (4, 6)), ('context', (7, 8)), ('glossaugmented neural network ( GAS )', (14, 20)), ('improved memory network paradigm', (22, 26))]","[['semantic relationship', 'of', 'context'], ['semantic relationship', 'propose', 'glossaugmented neural network ( GAS )'], ['glossaugmented neural network ( GAS )', 'in', 'improved memory network paradigm']]","[['context', 'name', 'glossaugmented neural network ( GAS )']]","[['Model', 'to model', 'semantic relationship']]",[],entity_linking,12,36
1178,model,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,"[('extend', (1, 2)), ('in', (5, 6)), ('to', (7, 8)), ('to mirror', (13, 15)), ('in', (20, 21))]","[('gloss module', (3, 5)), ('GAS', (6, 7)), ('hierarchical framework', (9, 11)), ('hierarchies of word senses', (16, 20)), ('WordNet', (21, 22))]","[['gloss module', 'in', 'GAS'], ['hierarchies of word senses', 'in', 'WordNet'], ['gloss module', 'to', 'hierarchical framework'], ['hierarchical framework', 'to mirror', 'hierarchies of word senses'], ['hierarchies of word senses', 'in', 'WordNet']]","[['gloss module', 'name', 'GAS']]","[['Model', 'extend', 'gloss module']]",[],entity_linking,12,38
1179,hyperparameters,"We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .","[('use', (1, 2)), ('with', (5, 6)), ('keep', (11, 12)), ('during', (14, 15))]","[('pre-trained word embeddings', (2, 5)), ('300 dimensions', (6, 8)), ('fixed', (13, 14)), ('training process', (16, 18))]","[['pre-trained word embeddings', 'with', '300 dimensions'], ['fixed', 'during', 'training process']]",[],"[['Hyperparameters', 'use', 'pre-trained word embeddings']]",[],entity_linking,12,195
1180,hyperparameters,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","[('employ', (1, 2)), ('in both', (5, 7))]","[('256 hidden units', (2, 5)), ('gloss module', (8, 10)), ('context module', (12, 14))]","[['256 hidden units', 'in both', 'gloss module'], ['256 hidden units', 'in both', 'context module']]",[],"[['Hyperparameters', 'employ', '256 hidden units']]",[],entity_linking,12,196
1181,hyperparameters,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","[('used for', (3, 5)), ('in', (6, 7)), ('with', (12, 13)), ('used for', (21, 23))]","[('Orthogonal initialization', (0, 2)), ('weights', (5, 6)), ('LSTM', (7, 8)), ('random uniform initialization', (9, 12)), ('range [ - 0.1 , 0.1 ]', (13, 20)), ('others', (23, 24))]","[['Orthogonal initialization', 'used for', 'weights'], ['weights', 'in', 'LSTM'], ['random uniform initialization', 'with', 'range [ - 0.1 , 0.1 ]'], ['Orthogonal initialization', 'used for', 'others'], ['random uniform initialization', 'used for', 'others'], ['range [ - 0.1 , 0.1 ]', 'used for', 'others']]",[],[],"[['Hyperparameters', 'has', 'Orthogonal initialization']]",entity_linking,12,197
1182,hyperparameters,We assign gloss expansion depth K the value of 4 .,"[('assign', (1, 2)), ('of', (8, 9))]","[('gloss expansion depth K', (2, 6)), ('value', (7, 8)), ('4', (9, 10))]","[['value', 'of', '4']]","[['gloss expansion depth K', 'has', 'value']]","[['Hyperparameters', 'assign', 'gloss expansion depth K']]",[],entity_linking,12,198
1183,experiments,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","[('experiment with', (2, 4)), ('from', (12, 13)), ('in', (16, 17)), ('finding', (20, 21)), ('performs', (27, 28))]","[('number of passes | T M |', (5, 12)), ('our framework', (17, 19)), ('best', (28, 29))]",[],[],[],[],entity_linking,12,199
1184,hyperparameters,We use Adam optimizer in the training process with 0.001 initial learning rate .,"[('use', (1, 2)), ('in', (4, 5)), ('with', (8, 9))]","[('Adam optimizer', (2, 4)), ('training process', (6, 8)), ('0.001 initial learning rate', (9, 13))]","[['Adam optimizer', 'in', 'training process'], ['Adam optimizer', 'with', '0.001 initial learning rate'], ['training process', 'with', '0.001 initial learning rate']]",[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],entity_linking,12,200
1185,hyperparameters,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .","[('to avoid', (2, 4)), ('use', (7, 8)), ('set', (11, 12)), ('to', (14, 15))]","[('overfitting', (4, 5)), ('dropout regularization', (8, 10)), ('drop rate', (12, 14)), ('0.5', (15, 16))]","[['overfitting', 'use', 'dropout regularization'], ['drop rate', 'to', '0.5']]","[['drop rate', 'has', '0.5']]","[['Hyperparameters', 'to avoid', 'overfitting']]",[],entity_linking,12,201
1186,experiments,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,"[('exploits', (2, 3)), ('from', (7, 8)), ('builds', (10, 11)), ('for', (17, 18))]","[('Babelfy', (0, 1)), ('semantic network structure', (4, 7)), ('BabelNet', (8, 9)), ('unified graph - based architecture', (12, 17)), ('WSD and Entity Linking', (18, 22))]","[['Babelfy', 'exploits', 'semantic network structure'], ['semantic network structure', 'from', 'BabelNet'], ['Babelfy', 'builds', 'unified graph - based architecture'], ['unified graph - based architecture', 'for', 'WSD and Entity Linking']]","[['Babelfy', 'has', 'semantic network structure']]",[],[],entity_linking,12,212
1187,baselines,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,"[('selects', (3, 4)), ('as', (5, 6)), ('makes use of', (10, 13)), ('as', (15, 16))]","[('IMS +emb', (0, 2)), ('IMS', (4, 5)), ('underlying framework', (7, 9)), ('word embeddings', (13, 15)), ('features', (16, 17))]","[['IMS +emb', 'selects', 'IMS'], ['IMS', 'as', 'underlying framework'], ['IMS +emb', 'makes use of', 'word embeddings'], ['word embeddings', 'as', 'features']]","[['IMS +emb', 'has', 'IMS']]",[],"[['Baselines', 'has', 'IMS +emb']]",entity_linking,12,216
1188,baselines,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,"[('leverages', (3, 4)), ('which shares', (8, 10)), ('among', (12, 13))]","[('Bi- LSTM', (0, 2)), ('model parameters', (10, 12)), ('all words', (13, 15))]","[['model parameters', 'among', 'all words']]",[],[],"[['Baselines', 'has', 'Bi- LSTM']]",entity_linking,12,219
1189,results,English all - words results,[],[],[],[],[],[],entity_linking,12,225
1190,results,"In this section , we show the performance of our proposed model in the English all - words task .","[('in', (12, 13))]","[('performance', (7, 8)), ('English all - words task', (14, 19))]",[],[],[],[],entity_linking,12,226
1191,results,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,"[('achieves', (4, 5)), ('on', (12, 13))]","[('GAS and GAS ext', (0, 4)), ('state - of - theart performance', (6, 12)), ('concatenation of all test datasets', (14, 19))]","[['GAS and GAS ext', 'achieves', 'state - of - theart performance'], ['state - of - theart performance', 'on', 'concatenation of all test datasets']]",[],[],"[['Results', 'has', 'GAS and GAS ext']]",entity_linking,12,230
1192,results,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .","[('find that', (18, 20)), ('with', (22, 23)), ('achieves', (27, 28)), ('on', (32, 33))]","[('GAS ext', (20, 22)), ('concatenation memory updating strategy', (23, 27)), ('best results 70.6', (29, 32)), ('concatenation of the four test datasets', (34, 40))]","[['GAS ext', 'with', 'concatenation memory updating strategy'], ['concatenation memory updating strategy', 'achieves', 'best results 70.6'], ['best results 70.6', 'on', 'concatenation of the four test datasets']]",[],"[['Results', 'find that', 'GAS ext']]",[],entity_linking,12,231
1193,results,". fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .","[('find that', (6, 8)), ('on', (18, 19))]","[('our best model', (8, 11)), ('outperforms', (11, 12)), ('previous best neural network models', (13, 18)), ('every individual test set', (19, 23))]","[['previous best neural network models', 'on', 'every individual test set']]","[['our best model', 'has', 'outperforms'], ['outperforms', 'has', 'previous best neural network models']]","[['Results', 'find that', 'our best model']]",[],entity_linking,12,235
1194,results,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .","[('on', (11, 12))]","[('best model', (3, 5)), ('IMS + emb', (8, 11)), ('SE3 , SE13 and SE15 test sets', (13, 20))]","[['IMS + emb', 'on', 'SE3 , SE13 and SE15 test sets']]",[],[],[],entity_linking,12,237
1195,results,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,"[('Incorporating', (0, 1)), ('into', (2, 3)), ('extending', (11, 12))]","[('glosses', (1, 2)), ('neural WSD', (3, 5)), ('greatly improve', (6, 8)), ('performance', (9, 10)), ('original gloss', (13, 15)), ('results', (19, 20))]","[['glosses', 'into', 'neural WSD'], ['glosses', 'extending', 'original gloss']]","[['greatly improve', 'has', 'performance']]","[['Results', 'Incorporating', 'glosses']]",[],entity_linking,12,238
1196,results,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .","[('Compared with', (0, 2)), ('greatly improves', (16, 18)), ('by', (21, 22)), ('with the help of', (27, 31))]","[('Bi - LSTM baseline', (3, 7)), ('labeled data', (10, 12)), ('our proposed model', (13, 16)), ('WSD task', (19, 21)), ('2.2 % F1 - score', (22, 27)), ('gloss knowledge', (31, 33))]","[['our proposed model', 'greatly improves', 'WSD task'], ['WSD task', 'by', '2.2 % F1 - score'], ['2.2 % F1 - score', 'with the help of', 'gloss knowledge']]",[],"[['Results', 'Compared with', 'Bi - LSTM baseline']]",[],entity_linking,12,239
1197,results,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .","[('compared with', (2, 4)), ('which only uses', (6, 9)), ('as', (11, 12)), ('can further improve', (18, 21)), ('with the help of', (23, 27)), ('through', (30, 31))]","[('GAS', (5, 6)), ('original gloss', (9, 11)), ('background knowledge', (13, 15)), ('GAS ext', (16, 18)), ('performance', (22, 23)), ('extended glosses', (28, 30)), ('semantic relations', (32, 34))]","[['GAS', 'which only uses', 'original gloss'], ['original gloss', 'as', 'background knowledge'], ['GAS ext', 'can further improve', 'performance'], ['performance', 'with the help of', 'extended glosses'], ['extended glosses', 'through', 'semantic relations']]",[],"[['Results', 'compared with', 'GAS']]",[],entity_linking,12,240
1198,results,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .","[('shows', (1, 2)), ('performs', (6, 7)), ('than', (8, 9))]","[('multiple passes operation', (3, 6)), ('better', (7, 8)), ('one pass', (9, 11))]","[['multiple passes operation', 'performs', 'better'], ['better', 'than', 'one pass']]",[],"[['Results', 'shows', 'multiple passes operation']]",[],entity_linking,12,252
1199,research-problem,Word Sense Disambiguation using a Bidirectional LSTM,[],"[('Word Sense Disambiguation', (0, 3))]",[],[],[],[],entity_linking,13,2
1200,research-problem,"In this paper we present a clean , yet effective , model for word sense disambiguation .",[],"[('word sense disambiguation', (13, 16))]",[],[],[],[],entity_linking,13,4
1201,model,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .","[('modeling', (10, 11)), ('surrounding', (15, 16)), ('represent', (36, 37)), ('using', (39, 40))]","[('sequence of words', (12, 15)), ('target word', (17, 19)), ('words', (38, 39)), ('real valued vector representation', (40, 44))]","[['sequence of words', 'surrounding', 'target word'], ['words', 'using', 'real valued vector representation']]",[],"[['Model', 'modeling', 'sequence of words']]",[],entity_linking,13,23
1202,experimental-setup,"The source code , implemented using TensorFlow , has been released as open source 1 .","[('implemented using', (4, 6)), ('released as', (10, 12))]","[('source code', (1, 3)), ('TensorFlow', (6, 7)), ('open source 1', (12, 15))]","[['source code', 'implemented using', 'TensorFlow'], ['source code', 'released as', 'open source 1'], ['TensorFlow', 'released as', 'open source 1']]","[['source code', 'has', 'TensorFlow']]",[],[],entity_linking,13,83
1203,experimental-setup,The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,"[('initialized using', (3, 5)), ('of', (7, 8)), ('trained on', (14, 16))]","[('embeddings', (1, 2)), ('set', (6, 7)), ('freely available 2 Glo Ve vectors', (8, 14)), ('Wikipedia and Gigaword', (16, 19))]","[['embeddings', 'initialized using', 'set'], ['set', 'of', 'freely available 2 Glo Ve vectors'], ['freely available 2 Glo Ve vectors', 'trained on', 'Wikipedia and Gigaword']]",[],[],"[['Experimental setup', 'has', 'embeddings']]",entity_linking,13,87
1204,experimental-setup,"Words not included in this set are initialized from N ( 0 , 0.1 ) .","[('initialized from', (7, 9))]","[('Words', (0, 1)), ('N ( 0 , 0.1 )', (9, 15))]","[['Words', 'initialized from', 'N ( 0 , 0.1 )']]",[],[],"[['Experimental setup', 'has', 'Words']]",entity_linking,13,88
1205,results,htsa 3 by was the winner of the SE3 lexical sample task with a F 1 score of 72.9 .,"[('of', (6, 7)), ('with', (12, 13)), ('of', (17, 18))]","[('htsa', (0, 1)), ('winner', (5, 6)), ('SE3 lexical sample task', (8, 12)), ('F 1 score', (14, 17)), ('72.9', (18, 19))]","[['winner', 'of', 'SE3 lexical sample task'], ['F 1 score', 'of', '72.9'], ['winner', 'with', 'F 1 score'], ['SE3 lexical sample task', 'with', 'F 1 score'], ['F 1 score', 'of', '72.9']]","[['htsa', 'has', 'winner']]",[],[],entity_linking,13,101
1206,results,Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,"[('achieves', (3, 4)), ('on', (7, 8)), ('tied with', (11, 13))]","[('Our proposed model', (0, 3)), ('top score', (5, 7)), ('SE2', (8, 9)), ('IMS + adapted CW', (13, 17)), ('SE3', (18, 19))]","[['Our proposed model', 'achieves', 'top score'], ['top score', 'on', 'SE2'], ['IMS + adapted CW', 'on', 'SE3'], ['Our proposed model', 'tied with', 'IMS + adapted CW']]","[['Our proposed model', 'has', 'top score']]",[],"[['Results', 'has', 'Our proposed model']]",entity_linking,13,106
1207,results,"Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .","[('see', (3, 4)), ('consistently', (6, 7)), ('on', (10, 11))]","[('dropword', (5, 6)), ('results', (9, 10)), ('both', (11, 12)), ('SE2 and SE3', (12, 15))]","[['results', 'on', 'both'], ['results', 'on', 'SE2 and SE3']]","[['both', 'has', 'SE2 and SE3']]","[['Results', 'see', 'dropword']]",[],entity_linking,13,107
1208,results,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .","[('Randomizing', (0, 1)), ('of', (3, 4)), ('yields', (7, 8)), ('are', (25, 26))]","[('input words', (5, 7)), ('substantially worse result', (9, 12)), ('order of the words', (21, 25)), ('significant', (26, 27))]","[['input words', 'yields', 'substantially worse result'], ['order of the words', 'are', 'significant']]","[['order of the words', 'has', 'significant']]","[['Results', 'Randomizing', 'input words']]",[],entity_linking,13,108
1209,research-problem,Knowledge - based Word Sense Disambiguation using Topic Models,[],"[('Knowledge -', (0, 2))]",[],[],[],[],entity_linking,14,2
1210,research-problem,"In this paper , we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context .","[('leverage', (5, 6)), ('of', (8, 9)), ('to design', (11, 13)), ('scales', (17, 18)), ('with', (19, 20)), ('in', (24, 25))]","[('formalism', (7, 8)), ('topic model', (9, 11)), ('WSD system', (14, 16)), ('linearly', (18, 19)), ('number of words', (21, 24)), ('context', (26, 27))]","[['formalism', 'of', 'topic model'], ['topic model', 'to design', 'WSD system'], ['WSD system', 'scales', 'linearly'], ['linearly', 'with', 'number of words'], ['number of words', 'in', 'context']]","[['formalism', 'has', 'topic model']]","[['Research problem', 'leverage', 'formalism']]",[],entity_linking,14,6
1211,research-problem,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,14,12
1212,research-problem,"WSD , being AI - complete ( Navigli 2009 ) , is still an open problem after over two decades of research .",[],"[('WSD', (0, 1))]",[],[],[],[],entity_linking,14,14
1213,model,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .","[('propose', (5, 6)), ('for', (13, 14)), ('utilizes', (22, 23)), ('as', (26, 27)), ('for', (29, 30))]","[('novel knowledge - based WSD algorithm', (7, 13)), ('all - word WSD task', (15, 20)), ('whole document', (24, 26)), ('context', (28, 29)), ('word', (31, 32)), ('current sentence', (37, 39))]","[['novel knowledge - based WSD algorithm', 'for', 'all - word WSD task'], ['all - word WSD task', 'utilizes', 'whole document'], ['whole document', 'as', 'context'], ['context', 'for', 'word']]",[],"[['Model', 'propose', 'novel knowledge - based WSD algorithm']]",[],entity_linking,14,22
1214,model,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .","[('leverage', (11, 12)), ('of', (14, 15)), ('especially', (18, 19))]","[('WSD', (8, 9)), ('formalism', (13, 14)), ('topic models', (15, 17)), ('Latent Dirichlet Allocation ( LDA )', (19, 25))]","[['formalism', 'of', 'topic models'], ['topic models', 'especially', 'Latent Dirichlet Allocation ( LDA )']]",[],[],[],entity_linking,14,23
1215,model,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,"[('use', (1, 2)), ('prior for', (4, 6)), ('over', (9, 10)), ('to model', (11, 13)), ('within', (17, 18))]","[('non-uniform', (3, 4)), ('synset distribution', (7, 9)), ('words', (10, 11)), ('frequency of words', (14, 17)), ('synset', (19, 20))]","[['non-uniform', 'prior for', 'synset distribution'], ['synset distribution', 'over', 'words'], ['words', 'to model', 'frequency of words'], ['frequency of words', 'within', 'synset']]",[],"[['Model', 'use', 'non-uniform']]",[],entity_linking,14,25
1216,model,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .","[('model', (4, 5)), ('between', (7, 8)), ('by using', (9, 11)), ('prior for drawing', (13, 16)), ('of', (19, 20))]","[('relationships', (6, 7)), ('synsets', (8, 9)), ('logisticnormal', (12, 13)), ('synset proportions', (17, 19)), ('document', (21, 22))]","[['relationships', 'between', 'synsets'], ['synsets', 'by using', 'logisticnormal'], ['logisticnormal', 'prior for drawing', 'synset proportions'], ['synset proportions', 'of', 'document']]",[],"[['Model', 'model', 'relationships']]",[],entity_linking,14,26
1217,results,"The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .","[('denoted by', (4, 6)), ('referring', (12, 13)), ('using', (15, 16)), ('by', (30, 31)), ('by achieving', (39, 41))]","[('WSD', (14, 15)), ('outperforms', (19, 20)), ('state - of - the - art WSD system', (21, 30)), ('significant margin', (32, 34)), ('pvalue < 0.01', (35, 38)), ('overall F1 - score', (42, 46)), ('66.9', (47, 48))]","[['state - of - the - art WSD system', 'by', 'significant margin'], ['state - of - the - art WSD system', 'by achieving', 'overall F1 - score']]","[['outperforms', 'has', 'state - of - the - art WSD system'], ['significant margin', 'has', 'pvalue < 0.01']]","[['Results', 'denoted by', 'WSD']]",[],entity_linking,14,169
1218,results,"We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .","[('observe', (2, 3)), ('of', (6, 7)), ('than', (14, 15))]","[('performance', (5, 6)), ('proposed model', (8, 10)), ('not much worse', (11, 14)), ('best supervised system', (16, 19)), ('Melamud16 ( 69.4 )', (20, 24))]","[['performance', 'of', 'proposed model'], ['not much worse', 'than', 'best supervised system']]","[['performance', 'has', 'proposed model'], ['best supervised system', 'name', 'Melamud16 ( 69.4 )']]",[],[],entity_linking,14,170
1219,results,The proposed system outperforms all previous knowledgebased systems overall parts of speech .,[],"[('proposed system', (1, 3)), ('outperforms', (3, 4)), ('all previous knowledgebased systems overall parts of speech', (4, 12))]",[],"[['proposed system', 'has', 'outperforms'], ['outperforms', 'has', 'all previous knowledgebased systems overall parts of speech']]",[],"[['Results', 'has', 'proposed system']]",entity_linking,14,172
1220,research-problem,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,[],"[('Question Answering Data', (8, 11))]",[],[],[],[],entity_linking,15,2
1221,research-problem,"Our approach outperforms the previous state - of - the - art system on this data , resulting in an average 8 % improvement of the final score .","[('resulting in', (17, 19)), ('of', (24, 25))]","[('Our approach', (0, 2)), ('outperforms', (2, 3)), ('previous state - of - the - art system', (4, 13)), ('average 8 % improvement', (20, 24)), ('final score', (26, 28))]","[['outperforms', 'resulting in', 'average 8 % improvement'], ['previous state - of - the - art system', 'resulting in', 'average 8 % improvement'], ['average 8 % improvement', 'of', 'final score']]","[['Our approach', 'has', 'outperforms'], ['outperforms', 'has', 'previous state - of - the - art system']]",[],[],entity_linking,15,7
1222,research-problem,Knowledge base question answering ( QA ) requires a precise modeling of the question semantics through the entities and relations available in the knowledge base ( KB ) in order to retrieve the correct answer .,[],"[('Knowledge base question answering ( QA )', (0, 7))]",[],[],[],[],entity_linking,15,10
1223,model,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .","[('perform', (16, 17)), ('in', (24, 25)), ('that makes', (29, 31))]","[('entity mention detection and entity disambiguation jointly', (17, 24)), ('single neural model', (26, 29)), ('whole process', (32, 34)), ('end - to - end differentiable', (34, 40))]","[['entity mention detection and entity disambiguation jointly', 'in', 'single neural model'], ['single neural model', 'that makes', 'whole process']]","[['whole process', 'has', 'end - to - end differentiable']]","[['Model', 'perform', 'entity mention detection and entity disambiguation jointly']]",[],entity_linking,15,27
1224,model,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .","[('To overcome', (0, 2)), ('in', (4, 5)), ('automatically learn', (9, 11)), ('over', (12, 13)), ('of', (17, 18))]","[('noise', (3, 4)), ('data', (6, 7)), ('features', (11, 12)), ('set of contexts', (14, 17)), ('different granularity levels', (18, 21))]","[['noise', 'in', 'data'], ['noise', 'automatically learn', 'features'], ['features', 'over', 'set of contexts'], ['set of contexts', 'of', 'different granularity levels']]","[['noise', 'has', 'data']]","[['Model', 'To overcome', 'noise']]",[],entity_linking,15,29
1225,model,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .","[('extract', (3, 4)), ('from', (5, 6)), ('of', (10, 11)), ('extracted for', (20, 22)), ('produced based on', (31, 34)), ('surrounding', (36, 37)), ('in', (40, 41))]","[('features', (4, 5)), ('knowledge base context', (7, 10)), ('candidate entity', (12, 14)), ('character - level features', (15, 19)), ('entity label', (23, 25)), ('higher - level features', (26, 30)), ('entities', (35, 36)), ('candidate entity', (38, 40)), ('knowledge graph', (42, 44))]","[['features', 'from', 'knowledge base context'], ['knowledge base context', 'of', 'candidate entity'], ['character - level features', 'extracted for', 'entity label'], ['higher - level features', 'produced based on', 'entities'], ['entities', 'surrounding', 'candidate entity'], ['candidate entity', 'in', 'knowledge graph']]",[],"[['Model', 'extract', 'features']]",[],entity_linking,15,32
1226,research-problem,"In the recent years , EL on Twitter data has emerged as a branch of entity linking research .",[],"[('EL on Twitter data', (5, 9))]",[],[],[],[],entity_linking,15,47
1227,results,We compile two new datasets for entity linking on questions that we derive from publicly available question answering data : WebQSP and GraphQuestions .,"[('derive from', (12, 14))]","[('entity linking on questions', (6, 10)), ('publicly available question answering data', (14, 19)), ('WebQSP', (20, 21)), ('GraphQuestions', (22, 23))]","[['entity linking on questions', 'derive from', 'publicly available question answering data']]","[['publicly available question answering data', 'name', 'WebQSP']]",[],[],entity_linking,15,178
1228,baselines,We also include a heuristics baseline that ranks candidate entities according to their frequency in Wikipedia .,"[('include', (2, 3)), ('ranks', (7, 8)), ('according to', (10, 12)), ('in', (14, 15))]","[('heuristics baseline', (4, 6)), ('candidate entities', (8, 10)), ('Wikipedia', (15, 16))]","[['heuristics baseline', 'ranks', 'candidate entities']]",[],"[['Baselines', 'include', 'heuristics baseline']]",[],entity_linking,15,216
1229,results,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,"[('shows', (3, 4)), ('better than', (11, 13)), ('by', (17, 18))]","[('VCG model', (1, 3)), ('overall F- score result', (5, 9)), ('DBPedia Spotlight baseline', (14, 17)), ('wide margin', (19, 21))]","[['VCG model', 'shows', 'overall F- score result'], ['overall F- score result', 'better than', 'DBPedia Spotlight baseline'], ['DBPedia Spotlight baseline', 'by', 'wide margin']]","[['VCG model', 'has', 'overall F- score result']]",[],"[['Results', 'has', 'VCG model']]",entity_linking,15,238
1230,results,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,"[('achieves', (7, 8)), ('manages to keep', (17, 20))]","[('our model', (5, 7)), ('higher precision values', (8, 11)), ('other approaches', (14, 16)), ('satisfactory level of recall', (21, 25))]","[['our model', 'achieves', 'higher precision values'], ['our model', 'manages to keep', 'satisfactory level of recall']]",[],[],"[['Results', 'has', 'our model']]",entity_linking,15,239
1231,research-problem,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,[],[],[],[],[],[],entity_linking,16,2
1232,research-problem,"Word Sense Disambiguation ( WSD ) is an important problem in Natural Language Processing ( NLP ) , both in its own right and as a steppingstone to other advanced tasks in the NLP pipeline , applications such as machine translation and question answering .",[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,16,11
1233,approach,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .","[('develop', (5, 6)), ('that leverages', (10, 12))]","[('our supervised WSD model', (6, 10)), ('Bidirectional Long Short - Term Memory ( BLSTM ) network', (13, 23))]","[['our supervised WSD model', 'that leverages', 'Bidirectional Long Short - Term Memory ( BLSTM ) network']]",[],"[['Approach', 'develop', 'our supervised WSD model']]",[],entity_linking,16,16
1234,approach,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .","[('works with', (2, 4)), ('learned during', (15, 17)), ('employs', (21, 22)), ('learned through', (33, 35)), ('called', (40, 41)), ('for', (49, 50))]","[('neural sense vectors ( i.e. sense embeddings )', (4, 12)), ('model training', (17, 19)), ('neural word vectors ( i.e. word embeddings )', (22, 30)), ('unsupervised deep learning approach', (36, 40)), ('GloVe ( Global Vectors for word representation )', (41, 49)), ('context words', (51, 53))]","[['neural sense vectors ( i.e. sense embeddings )', 'learned during', 'model training'], ['neural word vectors ( i.e. word embeddings )', 'learned through', 'unsupervised deep learning approach'], ['unsupervised deep learning approach', 'called', 'GloVe ( Global Vectors for word representation )'], ['GloVe ( Global Vectors for word representation )', 'for', 'context words']]",[],"[['Approach', 'works with', 'neural sense vectors ( i.e. sense embeddings )']]",[],entity_linking,16,17
1235,hyperparameters,This results in a vocabulary size of | V | = 29044 .,"[('results in', (1, 3)), ('of', (6, 7))]","[('vocabulary size', (4, 6)), ('| V | = 29044', (7, 12))]","[['vocabulary size', 'of', '| V | = 29044']]",[],"[['Hyperparameters', 'results in', 'vocabulary size']]",[],entity_linking,16,106
1236,results,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .","[('show', (1, 2)), ('sits among', (5, 7))]","[('5 top - performing algorithms', (8, 13))]",[],[],"[['Results', 'show', '5 top - performing algorithms']]",[],entity_linking,16,114
1237,baselines,IMS + adapted CW is another WSD model that considers deep neural networks and also uses pre-trained word embeddings as inputs .,"[('considers', (9, 10)), ('uses', (15, 16)), ('as', (19, 20))]","[('IMS + adapted CW', (0, 4)), ('WSD', (6, 7)), ('deep neural networks', (10, 13)), ('pre-trained word embeddings', (16, 19)), ('inputs', (20, 21))]","[['WSD', 'considers', 'deep neural networks'], ['IMS + adapted CW', 'uses', 'pre-trained word embeddings'], ['pre-trained word embeddings', 'as', 'inputs']]","[['IMS + adapted CW', 'has', 'WSD']]",[],"[['Baselines', 'has', 'IMS + adapted CW']]",entity_linking,16,119
1238,results,htsa 3 was the winner of the SensEval - 3 lexical sample .,"[('of', (5, 6))]","[('htsa 3', (0, 2)), ('winner', (4, 5)), ('SensEval - 3 lexical sample', (7, 12))]","[['winner', 'of', 'SensEval - 3 lexical sample']]","[['htsa 3', 'has', 'winner']]",[],"[['Results', 'has', 'htsa 3']]",entity_linking,16,122
1239,baselines,"IRST - Kernels utilizes kernel methods for pattern abstraction , paradigmatic and syntagmatic information and unsupervised term proximity on British National Corpus ( BNC ) , in SVM classifiers .","[('utilizes', (3, 4)), ('for', (6, 7)), ('on', (18, 19)), ('in', (26, 27))]","[('IRST', (0, 1)), ('kernel methods', (4, 6)), ('pattern abstraction', (7, 9)), ('paradigmatic and syntagmatic information', (10, 14)), ('unsupervised term proximity', (15, 18)), ('British National Corpus ( BNC )', (19, 25)), ('SVM classifiers', (27, 29))]","[['kernel methods', 'for', 'pattern abstraction'], ['kernel methods', 'for', 'unsupervised term proximity'], ['unsupervised term proximity', 'on', 'British National Corpus ( BNC )'], ['unsupervised term proximity', 'in', 'SVM classifiers'], ['British National Corpus ( BNC )', 'in', 'SVM classifiers']]",[],[],"[['Baselines', 'has', 'IRST']]",entity_linking,16,124
1240,research-problem,Neural Sequence Learning Models for Word Sense Disambiguation,[],[],[],[],[],[],entity_linking,2,2
1241,research-problem,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .",[],"[('Word Sense Disambiguation , WSD', (16, 21))]",[],[],[],[],entity_linking,2,9
1242,model,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .",[],"[('supervised WSD', (7, 9)), ('sequence labeling', (55, 57))]",[],[],[],[],entity_linking,2,18
1243,model,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .","[('design , analyze and compare', (6, 11)), ('of', (15, 16)), ('ranging from', (19, 21)), ('to', (29, 30))]","[('various neural architectures', (12, 15)), ('different complexities', (16, 18)), ('single bidirectional Long Short - Term Memory', (22, 29)), ('sequence - tosequence approach', (31, 35))]","[['various neural architectures', 'of', 'different complexities'], ['different complexities', 'ranging from', 'single bidirectional Long Short - Term Memory'], ['single bidirectional Long Short - Term Memory', 'to', 'sequence - tosequence approach']]",[],"[['Model', 'design , analyze and compare', 'various neural architectures']]",[],entity_linking,2,20
1244,hyperparameters,LSTM Tagger,[],[],[],[],[],[],entity_linking,2,70
1245,results,"6 Finally , we carried out an experiment on multilingual WSD using the Italian , German , French and Spanish data of SE13 .","[('carried out', (4, 6)), ('on', (8, 9)), ('using', (11, 12)), ('of', (21, 22))]","[('experiment', (7, 8)), ('multilingual WSD', (9, 11)), ('Italian , German , French and Spanish data', (13, 21)), ('SE13', (22, 23))]","[['experiment', 'on', 'multilingual WSD'], ['multilingual WSD', 'using', 'Italian , German , French and Spanish data'], ['Italian , German , French and Spanish data', 'of', 'SE13']]",[],"[['Results', 'carried out', 'experiment']]",[],entity_linking,2,141
1246,hyperparameters,"All models were trained fora fixed number of epochs E = 40 using Adadelta ( Zeiler , 2012 ) with learning rate 1.0 and batch size 32 .","[('trained fora', (3, 5)), ('using', (12, 13)), ('with', (19, 20))]","[('fixed number of epochs E = 40', (5, 12)), ('Adadelta', (13, 14)), ('learning rate', (20, 22)), ('1.0', (22, 23)), ('batch size', (24, 26)), ('32', (26, 27))]","[['fixed number of epochs E = 40', 'using', 'Adadelta'], ['Adadelta', 'with', 'learning rate'], ['Adadelta', 'with', 'batch size']]","[['learning rate', 'has', '1.0'], ['batch size', 'has', '32']]","[['Hyperparameters', 'trained fora', 'fixed number of epochs E = 40']]",[],entity_linking,2,151
1247,results,"11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .","[('achieved', (7, 8)), ('to', (31, 32)), ('in', (36, 37)), ('performing', (40, 41)), ('with', (43, 44)), ('tuned over', (46, 48))]","[('both BLSTM and Seq2Seq', (3, 7)), ('results', (8, 9)), ('state - of - the - art', (12, 19)), ('best supervised system', (33, 36)), ('each benchmark', (37, 39)), ('on par', (41, 43)), ('word experts', (44, 46)), ('explicitly engineered features', (48, 51))]","[['both BLSTM and Seq2Seq', 'achieved', 'results'], ['best supervised system', 'in', 'each benchmark'], ['results', 'performing', 'on par'], ['on par', 'with', 'word experts'], ['word experts', 'tuned over', 'explicitly engineered features']]",[],[],"[['Results', 'has', 'both BLSTM and Seq2Seq']]",entity_linking,2,161
1248,results,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .","[('tended consistently to', (5, 8)), ('suggesting', (13, 14)), ('for', (29, 30))]","[('BLSTM models', (3, 5)), ('outperform', (8, 9)), ('Seq2Seq counterparts', (10, 12)), ('encoder - decoder architecture', (16, 20)), ('suboptimal', (28, 29)), ('WSD', (30, 31))]","[['BLSTM models', 'tended consistently to', 'outperform'], ['Seq2Seq counterparts', 'suggesting', 'encoder - decoder architecture'], ['suboptimal', 'for', 'WSD']]","[['BLSTM models', 'has', 'outperform'], ['outperform', 'has', 'Seq2Seq counterparts']]",[],[],entity_linking,2,162
1249,results,"Furthermore , introducing LEX ( cf. Section 4 ) as auxiliary task was generally helpful ; on the other hand , POS did not seem to help , corroborating previous findings ( Alonso .","[('introducing', (2, 3)), ('as', (9, 10)), ('did not', (22, 24))]","[('LEX', (3, 4)), ('auxiliary task', (10, 12)), ('generally helpful', (13, 15)), ('POS', (21, 22)), ('help', (26, 27))]","[['LEX', 'as', 'auxiliary task'], ['POS', 'did not', 'help']]","[['LEX', 'has', 'auxiliary task']]","[['Results', 'introducing', 'LEX']]",[],entity_linking,2,163
1250,results,English All - words WSD,[],[],[],[],[],[],entity_linking,2,164
1251,results,"It is worth noting that RNN - based architectures outperformed classical supervised approaches when dealing with verbs , which are shown to be highly ambiguous .","[('worth', (2, 3)), ('when dealing with', (13, 16)), ('shown to be', (20, 23))]","[('RNN - based architectures', (5, 9)), ('outperformed', (9, 10)), ('classical supervised approaches', (10, 13)), ('verbs', (16, 17)), ('highly ambiguous', (23, 25))]","[['classical supervised approaches', 'when dealing with', 'verbs'], ['verbs', 'shown to be', 'highly ambiguous']]","[['RNN - based architectures', 'has', 'outperformed'], ['outperformed', 'has', 'classical supervised approaches']]","[['Results', 'worth', 'RNN - based architectures']]",[],entity_linking,2,169
1252,results,Multilingual All - words WSD,[],[],[],[],[],[],entity_linking,2,172
1253,results,"F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .","[('show', (4, 5)), ('trained only on', (13, 16)), ('consistently outperformed', (19, 21)), ('achieved', (25, 26)), ('with', (30, 31))]","[('F - score figures', (0, 4)), ('bilingual and multilingual models', (6, 10)), ('English data', (16, 18)), ('MFS baseline', (22, 24)), ('results', (26, 27)), ('competitive', (29, 30)), ('best participating systems', (32, 35))]","[['F - score figures', 'show', 'bilingual and multilingual models'], ['bilingual and multilingual models', 'trained only on', 'English data'], ['bilingual and multilingual models', 'consistently outperformed', 'MFS baseline'], ['bilingual and multilingual models', 'achieved', 'results'], ['competitive', 'with', 'best participating systems']]",[],[],"[['Results', 'has', 'F - score figures']]",entity_linking,2,179
1254,results,"We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .","[('note', (2, 3)), ('when', (18, 19)), ('from', (20, 21))]","[('overall F- score performance', (5, 9)), ('did not change substantially ( and slightly improved )', (9, 18)), ('moving', (19, 20)), ('bilingual to multilingual models', (21, 25))]","[['did not change substantially ( and slightly improved )', 'when', 'moving'], ['moving', 'from', 'bilingual to multilingual models']]","[['overall F- score performance', 'has', 'did not change substantially ( and slightly improved )']]","[['Results', 'note', 'overall F- score performance']]",[],entity_linking,2,180
1255,research-problem,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings,[],"[('Interpretable Word Sense Disambiguation', (6, 10))]",[],[],[],[],entity_linking,3,2
1256,research-problem,Word Sense Disambiguation ( WSD ) is the task to identify the correct sense of the usage of a word from a ( usually ) fixed inventory of sense identifiers .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,3,13
1257,results,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,"[('beats', (10, 11)), ('of', (16, 17))]","[('Simple k NN with ELMo as well', (0, 7)), ('state of the art', (12, 16)), ('lexical sample task', (18, 21))]","[['state of the art', 'of', 'lexical sample task']]",[],[],[],entity_linking,3,137
1258,code,We are using Version 2.1 : https://github.com/getalp/,[],[],[],[],[],[],entity_linking,3,143
1259,results,UFSAC 4 BERT performed best in experiment one .,"[('performed', (3, 4))]","[('UFSAC 4 BERT', (0, 3)), ('best', (4, 5))]","[['UFSAC 4 BERT', 'performed', 'best']]",[],[],"[['Results', 'has', 'UFSAC 4 BERT']]",entity_linking,3,144
1260,results,shows that including the POS restriction increases the F 1 scores for S7 - T7 and S7 - T17 .,"[('including', (2, 3)), ('increases', (6, 7)), ('for', (11, 12))]","[('POS restriction', (4, 6)), ('F 1 scores', (8, 11)), ('S7 - T7 and S7 - T17', (12, 19))]","[['POS restriction', 'increases', 'F 1 scores'], ['F 1 scores', 'for', 'S7 - T7 and S7 - T17']]",[],"[['Results', 'including', 'POS restriction']]",[],entity_linking,3,187
1261,research-problem,Learning Distributed Representations of Texts and Entities from Knowledge Base,[],"[('Learning Distributed Representations of Texts and Entities', (0, 7))]",[],[],[],[],entity_linking,4,2
1262,code,Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base ( KB ) such as Wikipedia and Freebase .,[],"[('learning distributed representations of entities', (4, 9)), ('https://github.com/studio-ousia/ntee base ( KB )', (13, 18))]",[],[],[],[],entity_linking,4,22
1263,model,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .","[('propose', (4, 5)), ('to jointly learn', (18, 21)), ('of', (23, 24))]","[('Neural Text - Entity Encoder ( NTEE )', (5, 13)), ('neural network model', (15, 18)), ('distributed representations', (21, 23)), ('texts ( i.e. , sentences and paragraphs )', (24, 32)), ('KB entities', (33, 35))]","[['neural network model', 'to jointly learn', 'distributed representations'], ['distributed representations', 'of', 'texts ( i.e. , sentences and paragraphs )'], ['distributed representations', 'of', 'KB entities']]","[['Neural Text - Entity Encoder ( NTEE )', 'has', 'neural network model']]","[['Model', 'propose', 'Neural Text - Entity Encoder ( NTEE )']]",[],entity_linking,4,26
1264,model,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .","[('For', (0, 1)), ('in', (3, 4)), ('aims to', (9, 11)), ('places', (17, 18)), ('close to', (24, 26)), ('in', (28, 29))]","[('every text', (1, 3)), ('KB', (5, 6)), ('our model', (7, 9)), ('predict', (11, 12)), ('relevant entities', (13, 15)), ('text and the relevant entities', (19, 24)), ('each other', (26, 28)), ('continuous vector space', (30, 33))]","[['every text', 'in', 'KB'], ['each other', 'in', 'continuous vector space'], ['our model', 'aims to', 'predict'], ['our model', 'places', 'text and the relevant entities'], ['text and the relevant entities', 'close to', 'each other'], ['each other', 'in', 'continuous vector space']]","[['every text', 'has', 'KB'], ['predict', 'has', 'relevant entities']]","[['Model', 'For', 'every text']]",[],entity_linking,4,27
1265,research-problem,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .","[('shows', (3, 4)), ('can be', (6, 8)), ('using', (10, 11)), ('of', (14, 15))]","[('ESA', (2, 3)), ('text', (5, 6)), ('accurately represented', (8, 10)), ('small set', (12, 14))]","[['ESA', 'shows', 'text'], ['text', 'can be', 'accurately represented'], ['accurately represented', 'using', 'small set']]",[],[],[],entity_linking,4,31
1266,model,"In both tasks , we adopt a simple multi -layer perceptron ( MLP ) classifier with the learned representations as features .","[('adopt', (5, 6)), ('with', (15, 16))]","[('simple multi -layer perceptron ( MLP ) classifier', (7, 15)), ('learned representations', (17, 19))]","[['simple multi -layer perceptron ( MLP ) classifier', 'with', 'learned representations']]",[],"[['Model', 'adopt', 'simple multi -layer perceptron ( MLP ) classifier']]",[],entity_linking,4,40
1267,model,"Our work differs from these works because we aim to map texts ( i.e. , sentences and paragraphs ) and entities into the same vector space .",[],[],[],[],[],[],entity_linking,4,44
1268,model,We train the model using a large amount of entity annotations extracted directly from Wikipedia .,"[('train', (1, 2)), ('using', (4, 5)), ('extracted directly from', (11, 14))]","[('model', (3, 4)), ('large amount of entity annotations', (6, 11)), ('Wikipedia', (14, 15))]","[['model', 'using', 'large amount of entity annotations'], ['large amount of entity annotations', 'extracted directly from', 'Wikipedia']]",[],"[['Model', 'train', 'model']]",[],entity_linking,4,47
1269,baselines,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,"[('based on', (4, 6)), ('augmented with', (9, 11)), ('with', (14, 15))]","[('BOW - DT', (0, 3)), ('BOW baseline', (7, 9)), ('feature set', (12, 14)), ('dependency relation indicators', (15, 18))]","[['BOW - DT', 'based on', 'BOW baseline'], ['BOW baseline', 'augmented with', 'feature set'], ['feature set', 'with', 'dependency relation indicators']]",[],[],"[['Baselines', 'has', 'BOW - DT']]",entity_linking,4,126
1270,baselines,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,"[('based on', (4, 6)), ('to derive', (10, 12)), ('of', (15, 16))]","[('QANTA', (0, 1)), ('recursive neural network', (7, 10)), ('distributed representations', (13, 15)), ('questions', (16, 17))]","[['QANTA', 'based on', 'recursive neural network'], ['recursive neural network', 'to derive', 'distributed representations'], ['distributed representations', 'of', 'questions']]",[],[],"[['Baselines', 'has', 'QANTA']]",entity_linking,4,127
1271,baselines,The method also uses the LR classifier with the derived representations as features .,"[('uses', (3, 4)), ('with', (7, 8)), ('as', (11, 12))]","[('LR classifier', (5, 7)), ('derived representations', (9, 11)), ('features', (12, 13))]","[['LR classifier', 'with', 'derived representations'], ['derived representations', 'as', 'features']]",[],[],[],entity_linking,4,128
1272,baselines,FTS - BRNN is based on the bidirectional recurrent neural network ( RNN ) with gated recurrent units ( GRU ) .,"[('based on', (4, 6)), ('with', (14, 15))]","[('FTS - BRNN', (0, 3)), ('bidirectional recurrent neural network ( RNN )', (7, 14)), ('gated recurrent units ( GRU )', (15, 21))]","[['FTS - BRNN', 'based on', 'bidirectional recurrent neural network ( RNN )'], ['bidirectional recurrent neural network ( RNN )', 'with', 'gated recurrent units ( GRU )']]",[],[],"[['Baselines', 'has', 'FTS - BRNN']]",entity_linking,4,129
1273,results,The experimental results show that our NTEE model achieved the best performance compared to the other proposed models and all the baseline methods on both the history and the literature datasets . :,"[('show', (3, 4)), ('achieved', (8, 9)), ('compared to', (12, 14)), ('on', (23, 24))]","[('our NTEE model', (5, 8)), ('best performance', (10, 12)), ('other proposed models', (15, 18)), ('all the baseline methods', (19, 23)), ('history and the literature datasets', (26, 31))]","[['our NTEE model', 'achieved', 'best performance'], ['best performance', 'compared to', 'other proposed models'], ['best performance', 'compared to', 'all the baseline methods'], ['all the baseline methods', 'on', 'history and the literature datasets']]",[],"[['Results', 'show', 'our NTEE model']]",[],entity_linking,4,135
1274,results,"In particular , despite the simplicity of the neural network architecture of our method compared to the state - of - the - art methods ( i.e. , QANTA and FTS - BRNN ) , our method clearly outperformed these methods .","[('of', (11, 12)), ('compared to', (14, 16)), ('i.e.', (26, 27))]","[('our method', (12, 14)), ('state - of - the - art methods', (17, 25)), ('QANTA and FTS - BRNN )', (28, 34)), ('our method', (35, 37)), ('clearly outperformed', (37, 39)), ('methods', (40, 41))]","[['our method', 'compared to', 'state - of - the - art methods'], ['state - of - the - art methods', 'i.e.', 'QANTA and FTS - BRNN )']]","[['state - of - the - art methods', 'name', 'QANTA and FTS - BRNN )'], ['our method', 'has', 'clearly outperformed'], ['clearly outperformed', 'has', 'methods']]",[],[],entity_linking,4,137
1275,research-problem,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,[],"[('Neural Word Sense Disambiguation', (8, 12))]",[],[],[],[],entity_linking,5,2
1276,research-problem,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,5,10
1277,research-problem,"Various approaches have been proposed to achieve WSD , and they are generally ordered by the type and the quantity of resources they use :","[('ordered by', (13, 15))]","[('Various approaches', (0, 2)), ('WSD', (7, 8))]",[],[],[],[],entity_linking,5,11
1278,model,"Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing , but in this work , the idea is to solve this issue by taking advantage of one of the multiple semantic relationships between senses included in WordNet : the hypernymy and hyponymy relationships .","[('by creating', (7, 9)), ('through', (20, 21)), ('between', (45, 46)), ('included in', (47, 49))]","[('new sense annotated corpora', (9, 13)), ('automatically', (15, 16)), ('crowdsourcing', (21, 22)), ('one', (39, 40)), ('multiple semantic relationships', (42, 45)), ('senses', (46, 47)), ('WordNet', (49, 50)), ('hypernymy and hyponymy relationships', (52, 56))]","[['multiple semantic relationships', 'between', 'senses'], ['senses', 'included in', 'WordNet']]","[['one', 'has', 'multiple semantic relationships']]","[['Model', 'by creating', 'new sense annotated corpora']]",[],entity_linking,5,19
1279,code,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,[],[],[],[],[],[],entity_linking,5,31
1280,experiments,Language Model Based WSD,[],[],[],[],[],[],entity_linking,5,34
1281,experiments,We performed every training for 20 epochs .,"[('performed', (1, 2)), ('for', (4, 5))]","[('every training', (2, 4)), ('20 epochs', (5, 7))]","[['every training', 'for', '20 epochs']]",[],[],[],entity_linking,5,149
1282,experiments,"We trained with mini-batches of 100 sentences , truncated to 80 words , and padded with zero vectors from the end , and we used Adam ( Kingma and Ba , 2014 ) , with the same default parameters described in their article as the optimization method , except for the learning rate that we set to 0.0001 ( 10 times smaller than the default value ) .","[('trained with', (1, 3)), ('of', (4, 5)), ('truncated to', (8, 10)), ('padded with', (14, 16)), ('used', (24, 25)), ('except for', (48, 50)), ('set to', (55, 57))]","[('mini-batches', (3, 4)), ('100 sentences', (5, 7)), ('80 words', (10, 12)), ('zero vectors', (16, 18)), ('Adam', (25, 26)), ('learning rate', (51, 53))]","[['mini-batches', 'of', '100 sentences'], ['100 sentences', 'truncated to', '80 words']]","[['mini-batches', 'has', '100 sentences']]",[],[],entity_linking,5,154
1283,experimental-setup,All models have been trained on Nvidia 's Titan X GPUs .,"[('trained on', (4, 6))]","[(""Nvidia 's Titan X GPUs"", (6, 11))]",[],[],"[['Experimental setup', 'trained on', ""Nvidia 's Titan X GPUs""]]",[],entity_linking,5,155
1284,results,"Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .","[('of', (11, 12)), ('obtained by', (13, 15)), ('using', (17, 18)), ('is', (24, 25)), ('regarding', (29, 30))]","[('difference', (10, 11)), ('scores', (12, 13)), ('our system', (15, 17)), ('sense vocabulary reduction or', (19, 23)), ('overall', (25, 26)), ('not significant', (26, 28)), ('"" ALL "" column', (31, 35))]","[['difference', 'of', 'scores'], ['scores', 'obtained by', 'our system'], ['scores', 'using', 'sense vocabulary reduction or'], ['our system', 'using', 'sense vocabulary reduction or'], ['difference', 'is', 'overall'], ['scores', 'is', 'overall'], ['sense vocabulary reduction or', 'is', 'overall'], ['not significant', 'regarding', '"" ALL "" column']]","[['overall', 'has', 'not significant']]",[],[],entity_linking,5,185
1285,results,"In comparison with the other works , our systems trained on the SemCor alone expose results comparable with the best system of , which is trained on the same corpus and augmented with a semi-supervised method .","[('trained on', (9, 11)), ('expose', (14, 15)), ('comparable with', (16, 18)), ('trained on', (25, 27)), ('augmented with', (31, 33))]","[('our systems', (7, 9)), ('SemCor alone', (12, 14)), ('results', (15, 16)), ('best system', (19, 21)), ('same corpus', (28, 30)), ('semi-supervised method', (34, 36))]","[['our systems', 'trained on', 'SemCor alone'], ['SemCor alone', 'expose', 'results'], ['results', 'comparable with', 'best system'], ['best system', 'trained on', 'same corpus']]","[['results', 'has', 'best system']]",[],"[['Results', 'has', 'our systems']]",entity_linking,5,189
1286,results,"When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .","[('add', (2, 3)), ('to', (7, 8)), ('obtain', (14, 15)), ('on', (21, 22)), ('except on', (24, 26))]","[('WordNet Gloss Tagged', (4, 7)), ('training data', (9, 11)), ('systematically', (15, 16)), ('state of the art results', (16, 21)), ('all tasks', (22, 24)), ('SensEval', (26, 27))]","[['WordNet Gloss Tagged', 'to', 'training data'], ['training data', 'obtain', 'systematically'], ['state of the art results', 'on', 'all tasks'], ['all tasks', 'except on', 'SensEval']]","[['WordNet Gloss Tagged', 'has', 'training data'], ['systematically', 'has', 'state of the art results']]","[['Results', 'add', 'WordNet Gloss Tagged']]",[],entity_linking,5,190
1287,results,"Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .","[('does not', (7, 9)), ('on', (15, 16)), ('in', (20, 21)), ('is', (31, 32)), ('as', (35, 36))]","[('sense reduction method', (4, 7)), ('consistently improves or decreases', (9, 13)), ('score', (14, 15)), ('every task', (16, 18)), ('overall', (21, 22)), ('result', (30, 31)), ('roughly the same', (32, 35))]","[['sense reduction method', 'does not', 'consistently improves or decreases'], ['score', 'on', 'every task'], ['sense reduction method', 'in', 'overall'], ['result', 'is', 'roughly the same']]","[['sense reduction method', 'has', 'consistently improves or decreases'], ['consistently improves or decreases', 'has', 'score'], ['overall', 'has', 'result'], ['result', 'has', 'roughly the same']]",[],"[['Results', 'has', 'sense reduction method']]",entity_linking,5,191
1288,results,"As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .","[('is', (6, 7)), ('in', (11, 12)), ('improves', (15, 16))]","[('ensembling', (5, 6)), ('very efficient method', (8, 11)), ('WSD', (12, 13)), ('systematically', (16, 17)), ('all our results', (17, 20))]","[['ensembling', 'is', 'very efficient method'], ['very efficient method', 'in', 'WSD'], ['very efficient method', 'improves', 'systematically']]","[['ensembling', 'has', 'very efficient method'], ['systematically', 'has', 'all our results']]",[],"[['Results', 'has', 'ensembling']]",entity_linking,5,193
1289,results,"Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .","[('with', (2, 3)), ('are', (7, 8)), ('when applying', (10, 12))]","[('ensembles', (3, 4)), ('scores', (6, 7)), ('significantly higher', (8, 10)), ('vocabulary reduction algorithm', (13, 16))]","[['scores', 'are', 'significantly higher'], ['significantly higher', 'when applying', 'vocabulary reduction algorithm']]","[['ensembles', 'has', 'scores'], ['scores', 'has', 'significantly higher']]","[['Results', 'with', 'ensembles']]",[],entity_linking,5,194
1290,research-problem,Incorporating Glosses into Neural Word Sense Disambiguation,[],[],[],[],[],[],entity_linking,6,2
1291,research-problem,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,6,4
1292,research-problem,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .","[('models', (1, 2)), ('between', (5, 6)), ('in', (11, 12)), ('of', (22, 23))]","[('GAS', (0, 1)), ('semantic relationship', (3, 5)), ('context and the gloss', (7, 11)), ('improved memory network framework', (13, 17)), ('previous supervised methods and knowledge - based methods', (24, 32))]","[['GAS', 'models', 'semantic relationship'], ['semantic relationship', 'between', 'context and the gloss'], ['context and the gloss', 'in', 'improved memory network framework']]",[],[],[],entity_linking,6,9
1293,research-problem,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,6,13
1294,model,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","[('propose', (5, 6)), ('variant of', (21, 23))]","[('novel model GAS', (7, 10)), ('gloss - augmented WSD neural network', (12, 18)), ('memory network', (24, 26))]","[['gloss - augmented WSD neural network', 'variant of', 'memory network']]","[['novel model GAS', 'has', 'gloss - augmented WSD neural network']]","[['Model', 'propose', 'novel model GAS']]",[],entity_linking,6,31
1295,model,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"[('jointly encodes', (1, 3)), ('of', (7, 8)), ('models', (12, 13)), ('between', (16, 17)), ('in', (21, 22))]","[('GAS', (0, 1)), ('context and glosses', (4, 7)), ('target word', (9, 11)), ('semantic relationship', (14, 16)), ('context and glosses', (18, 21)), ('memory module', (23, 25))]","[['GAS', 'jointly encodes', 'context and glosses'], ['context and glosses', 'of', 'target word'], ['GAS', 'models', 'semantic relationship'], ['semantic relationship', 'between', 'context and glosses'], ['context and glosses', 'in', 'memory module']]",[],[],"[['Model', 'has', 'GAS']]",entity_linking,6,32
1296,model,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .","[('to measure', (2, 4)), ('between', (7, 8)), ('employ', (15, 16)), ('within', (19, 20)), ('as', (22, 23)), ('adopt', (27, 28))]","[('inner relationship', (5, 7)), ('glosses and context', (8, 11)), ('more accurately', (11, 13)), ('multiple passes operation', (16, 19)), ('memory', (21, 22)), ('re-reading process', (24, 26)), ('two memory updating mechanisms', (28, 32))]","[['inner relationship', 'between', 'glosses and context'], ['inner relationship', 'employ', 'multiple passes operation'], ['multiple passes operation', 'within', 'memory'], ['memory', 'as', 're-reading process'], ['multiple passes operation', 'adopt', 'two memory updating mechanisms']]","[['glosses and context', 'has', 'more accurately']]","[['Model', 'to measure', 'inner relationship']]",[],entity_linking,6,33
1297,model,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .","[('to model', (2, 4)), ('of', (6, 7)), ('propose', (12, 13)), ('in', (20, 21))]","[('semantic relationship', (4, 6)), ('context', (7, 8)), ('glossaugmented neural network ( GAS )', (14, 20)), ('improved memory network paradigm', (22, 26))]","[['semantic relationship', 'of', 'context'], ['semantic relationship', 'propose', 'glossaugmented neural network ( GAS )'], ['glossaugmented neural network ( GAS )', 'in', 'improved memory network paradigm']]","[['context', 'name', 'glossaugmented neural network ( GAS )']]","[['Model', 'to model', 'semantic relationship']]",[],entity_linking,6,37
1298,model,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,"[('extend', (1, 2)), ('in', (5, 6)), ('to', (7, 8)), ('to mirror', (13, 15)), ('in', (20, 21))]","[('gloss module', (3, 5)), ('GAS', (6, 7)), ('hierarchical framework', (9, 11)), ('hierarchies of word senses', (16, 20)), ('WordNet', (21, 22))]","[['gloss module', 'in', 'GAS'], ['hierarchies of word senses', 'in', 'WordNet'], ['gloss module', 'to', 'hierarchical framework'], ['hierarchical framework', 'to mirror', 'hierarchies of word senses'], ['hierarchies of word senses', 'in', 'WordNet']]","[['gloss module', 'name', 'GAS']]","[['Model', 'extend', 'gloss module']]",[],entity_linking,6,39
1299,hyperparameters,"We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .","[('use', (1, 2)), ('with', (5, 6)), ('keep', (11, 12)), ('during', (14, 15))]","[('pre-trained word embeddings', (2, 5)), ('300 dimensions', (6, 8)), ('fixed', (13, 14)), ('training process', (16, 18))]","[['pre-trained word embeddings', 'with', '300 dimensions'], ['pre-trained word embeddings', 'keep', 'fixed'], ['fixed', 'during', 'training process']]",[],"[['Hyperparameters', 'use', 'pre-trained word embeddings']]",[],entity_linking,6,196
1300,hyperparameters,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","[('employ', (1, 2)), ('in both', (5, 7))]","[('256 hidden units', (2, 5)), ('gloss module', (8, 10)), ('context module', (12, 14))]","[['256 hidden units', 'in both', 'gloss module'], ['256 hidden units', 'in both', 'context module']]",[],"[['Hyperparameters', 'employ', '256 hidden units']]",[],entity_linking,6,197
1301,hyperparameters,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","[('used for', (3, 5)), ('in', (6, 7)), ('with', (12, 13)), ('used for', (21, 23))]","[('Orthogonal initialization', (0, 2)), ('weights', (5, 6)), ('LSTM', (7, 8)), ('random uniform initialization', (9, 12)), ('range [ - 0.1 , 0.1 ]', (13, 20)), ('others', (23, 24))]","[['Orthogonal initialization', 'used for', 'weights'], ['weights', 'in', 'LSTM'], ['random uniform initialization', 'with', 'range [ - 0.1 , 0.1 ]'], ['Orthogonal initialization', 'used for', 'others'], ['random uniform initialization', 'used for', 'others'], ['range [ - 0.1 , 0.1 ]', 'used for', 'others']]",[],[],"[['Hyperparameters', 'has', 'Orthogonal initialization']]",entity_linking,6,198
1302,hyperparameters,We assign gloss expansion depth K the value of 4 .,"[('assign', (1, 2)), ('of', (8, 9))]","[('gloss expansion depth K', (2, 6)), ('value', (7, 8)), ('4', (9, 10))]","[['value', 'of', '4']]","[['gloss expansion depth K', 'has', 'value']]","[['Hyperparameters', 'assign', 'gloss expansion depth K']]",[],entity_linking,6,199
1303,experiments,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","[('experiment with', (2, 4)), ('from', (12, 13)), ('in', (16, 17)), ('finding', (20, 21)), ('performs', (27, 28))]","[('number of passes | T M |', (5, 12)), ('our framework', (17, 19)), ('best', (28, 29))]",[],[],[],[],entity_linking,6,200
1304,hyperparameters,We use Adam optimizer in the training process with 0.001 initial learning rate .,"[('use', (1, 2)), ('in', (4, 5)), ('with', (8, 9))]","[('Adam optimizer', (2, 4)), ('training process', (6, 8)), ('0.001 initial learning rate', (9, 13))]","[['Adam optimizer', 'in', 'training process'], ['Adam optimizer', 'with', '0.001 initial learning rate'], ['training process', 'with', '0.001 initial learning rate']]",[],"[['Hyperparameters', 'use', 'Adam optimizer']]",[],entity_linking,6,201
1305,hyperparameters,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .","[('to avoid', (2, 4)), ('use', (7, 8)), ('set', (11, 12)), ('to', (14, 15))]","[('overfitting', (4, 5)), ('dropout regularization', (8, 10)), ('drop rate', (12, 14)), ('0.5', (15, 16))]","[['overfitting', 'use', 'dropout regularization'], ['drop rate', 'to', '0.5']]","[['drop rate', 'has', '0.5']]","[['Hyperparameters', 'to avoid', 'overfitting']]",[],entity_linking,6,202
1306,experiments,This work shows that glosses are important to WSD and enriching gloss information via its semantic relations can help to WSD .,"[('shows', (2, 3)), ('important to', (6, 8)), ('enriching', (10, 11)), ('via', (13, 14)), ('help to', (18, 20))]","[('glosses', (4, 5)), ('WSD', (8, 9)), ('gloss information', (11, 13)), ('semantic relations', (15, 17)), ('WSD', (20, 21))]","[['glosses', 'important to', 'WSD'], ['glosses', 'enriching', 'gloss information'], ['gloss information', 'via', 'semantic relations'], ['gloss information', 'help to', 'WSD']]",[],[],[],entity_linking,6,214
1307,experiments,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,"[('exploits', (2, 3)), ('from', (7, 8)), ('builds', (10, 11)), ('for', (17, 18))]","[('Babelfy', (0, 1)), ('semantic network structure', (4, 7)), ('BabelNet', (8, 9)), ('unified graph - based architecture', (12, 17)), ('WSD and Entity Linking', (18, 22))]","[['Babelfy', 'exploits', 'semantic network structure'], ['semantic network structure', 'from', 'BabelNet'], ['Babelfy', 'builds', 'unified graph - based architecture'], ['unified graph - based architecture', 'for', 'WSD and Entity Linking']]","[['Babelfy', 'has', 'semantic network structure']]",[],[],entity_linking,6,215
1308,baselines,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,"[('selects', (3, 4)), ('as', (5, 6)), ('makes use of', (10, 13)), ('as', (15, 16))]","[('IMS +emb', (0, 2)), ('IMS', (4, 5)), ('underlying framework', (7, 9)), ('word embeddings', (13, 15)), ('features', (16, 17))]","[['IMS +emb', 'selects', 'IMS'], ['IMS', 'as', 'underlying framework'], ['IMS +emb', 'makes use of', 'word embeddings'], ['word embeddings', 'as', 'features']]","[['IMS +emb', 'has', 'IMS']]",[],"[['Baselines', 'has', 'IMS +emb']]",entity_linking,6,219
1309,baselines,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,"[('leverages', (3, 4)), ('which shares', (8, 10)), ('among', (12, 13))]","[('Bi- LSTM', (0, 2)), ('model parameters', (10, 12)), ('all words', (13, 15))]","[['model parameters', 'among', 'all words']]",[],[],"[['Baselines', 'has', 'Bi- LSTM']]",entity_linking,6,222
1310,results,English all - words results,[],[],[],[],[],[],entity_linking,6,228
1311,results,"In this section , we show the performance of our proposed model in the English all - words task .","[('in', (12, 13))]","[('performance', (7, 8)), ('English all - words task', (14, 19))]",[],[],[],[],entity_linking,6,229
1312,results,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,"[('achieves', (4, 5)), ('on', (12, 13))]","[('GAS and GAS ext', (0, 4)), ('state - of - theart performance', (6, 12)), ('concatenation of all test datasets', (14, 19))]","[['GAS and GAS ext', 'achieves', 'state - of - theart performance'], ['state - of - theart performance', 'on', 'concatenation of all test datasets']]",[],[],"[['Results', 'has', 'GAS and GAS ext']]",entity_linking,6,233
1313,results,". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .","[('performs', (2, 3)), ('on', (4, 5)), ('with', (17, 18)), ('achieves', (22, 23)), ('on', (27, 28))]","[('best', (3, 4)), ('all the test sets', (5, 9)), ('GAS ext', (15, 17)), ('concatenation memory updating strategy', (18, 22)), ('best results', (24, 26)), ('70.6', (26, 27)), ('concatenation of', (29, 31))]","[['best', 'on', 'all the test sets'], ['best results', 'on', 'concatenation of'], ['70.6', 'on', 'concatenation of'], ['GAS ext', 'with', 'concatenation memory updating strategy'], ['concatenation memory updating strategy', 'achieves', 'best results'], ['70.6', 'on', 'concatenation of']]","[['best results', 'has', '70.6']]","[['Results', 'performs', 'best']]",[],entity_linking,6,237
1314,results,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .","[('Compared with', (0, 2)), ('find that', (15, 17)), ('outperforms', (20, 21)), ('on', (27, 28))]","[('our best model', (17, 20)), ('previous best neural network models', (22, 27)), ('every individual test set', (28, 32))]","[['our best model', 'outperforms', 'previous best neural network models'], ['previous best neural network models', 'on', 'every individual test set']]",[],"[['Results', 'Compared with', 'our best model']]",[],entity_linking,6,238
1315,results,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .","[('on', (11, 12))]","[('best model', (3, 5)), ('IMS + emb', (8, 11)), ('SE3 , SE13 and SE15 test sets', (13, 20))]","[['IMS + emb', 'on', 'SE3 , SE13 and SE15 test sets']]",[],[],[],entity_linking,6,240
1316,results,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,"[('Incorporating', (0, 1)), ('into', (2, 3)), ('extending', (11, 12))]","[('glosses', (1, 2)), ('neural WSD', (3, 5)), ('greatly improve', (6, 8)), ('performance', (9, 10)), ('original gloss', (13, 15)), ('results', (19, 20))]","[['glosses', 'into', 'neural WSD'], ['glosses', 'extending', 'original gloss']]","[['greatly improve', 'has', 'performance']]","[['Results', 'Incorporating', 'glosses']]",[],entity_linking,6,241
1317,results,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .","[('Compared with', (0, 2)), ('greatly improves', (16, 18)), ('by', (21, 22)), ('with the help of', (27, 31))]","[('Bi - LSTM baseline', (3, 7)), ('labeled data', (10, 12)), ('our proposed model', (13, 16)), ('WSD task', (19, 21)), ('2.2 % F1 - score', (22, 27)), ('gloss knowledge', (31, 33))]","[['our proposed model', 'greatly improves', 'WSD task'], ['WSD task', 'by', '2.2 % F1 - score'], ['2.2 % F1 - score', 'with the help of', 'gloss knowledge']]",[],"[['Results', 'Compared with', 'Bi - LSTM baseline']]",[],entity_linking,6,242
1318,results,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .","[('compared with', (2, 4)), ('which only uses', (6, 9)), ('as', (11, 12)), ('can further improve', (18, 21)), ('with the help of', (23, 27)), ('through', (30, 31))]","[('GAS', (5, 6)), ('original gloss', (9, 11)), ('background knowledge', (13, 15)), ('GAS ext', (16, 18)), ('performance', (22, 23)), ('extended glosses', (28, 30)), ('semantic relations', (32, 34))]","[['GAS', 'which only uses', 'original gloss'], ['original gloss', 'as', 'background knowledge'], ['GAS ext', 'can further improve', 'performance'], ['performance', 'with the help of', 'extended glosses'], ['extended glosses', 'through', 'semantic relations']]",[],"[['Results', 'compared with', 'GAS']]",[],entity_linking,6,243
1319,research-problem,Semi-supervised Word Sense Disambiguation with Neural Models,[],"[('Semi-supervised Word Sense Disambiguation', (0, 4))]",[],[],[],[],entity_linking,7,2
1320,research-problem,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,[],"[('text - word sense disambiguation ( WSD )', (7, 15))]",[],[],[],[],entity_linking,7,4
1321,research-problem,Word sense disambiguation ( WSD ) is a long - standing problem in natural language processing ( NLP ) with broad applications .,[],"[('Word sense disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,7,13
1322,model,"In this paper , we describe two novel WSD algorithms .","[('describe', (5, 6))]","[('two novel WSD algorithms', (6, 10))]",[],[],"[['Model', 'describe', 'two novel WSD algorithms']]",[],entity_linking,7,20
1323,model,The first is based on a Long Short Term Memory ( LSTM ) ) .,"[('based on', (3, 5))]","[('Long Short Term Memory ( LSTM ) )', (6, 14))]",[],[],"[['Model', 'based on', 'Long Short Term Memory ( LSTM ) )']]",[],entity_linking,7,21
1324,model,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,"[('present', (2, 3)), ('to label', (10, 12)), ('based on', (14, 16)), ('to', (18, 19))]","[('semi-supervised algorithm', (4, 6)), ('label propagation', (8, 10)), ('unlabeled sentences', (12, 14)), ('similarity', (17, 18)), ('labeled ones', (19, 21))]","[['label propagation', 'to label', 'unlabeled sentences'], ['unlabeled sentences', 'based on', 'similarity'], ['similarity', 'to', 'labeled ones']]",[],"[['Model', 'present', 'semi-supervised algorithm']]",[],entity_linking,7,23
1325,experiments,Sem Eval Tasks,[],[],[],[],[],[],entity_linking,7,130
1326,results,Our proposed algorithms achieve the highest all - words F 1 scores except for Sem - Eval 2013 .,"[('achieve', (3, 4)), ('except for', (12, 14))]","[('Our proposed algorithms', (0, 3)), ('highest all - words F 1 scores', (5, 12)), ('Sem - Eval 2013', (14, 18))]","[['Our proposed algorithms', 'achieve', 'highest all - words F 1 scores'], ['highest all - words F 1 scores', 'except for', 'Sem - Eval 2013']]",[],[],"[['Results', 'has', 'Our proposed algorithms']]",entity_linking,7,136
1327,results,"Our self - trained word embeddings have similar performance to the pre-trained embeddings , as shown in .","[('have', (6, 7)), ('to', (9, 10))]","[('Our self - trained word embeddings', (0, 6)), ('similar performance', (7, 9)), ('pre-trained embeddings', (11, 13))]","[['Our self - trained word embeddings', 'have', 'similar performance'], ['similar performance', 'to', 'pre-trained embeddings']]","[['Our self - trained word embeddings', 'has', 'similar performance']]",[],"[['Results', 'has', 'Our self - trained word embeddings']]",entity_linking,7,141
1328,hyperparameters,The learning rate is 0.1 .,"[('is', (3, 4))]","[('learning rate', (1, 3)), ('0.1', (4, 5))]","[['learning rate', 'is', '0.1']]","[['learning rate', 'has', '0.1']]",[],"[['Hyperparameters', 'has', 'learning rate']]",entity_linking,7,145
1329,experiments,"We experimented with other learning rates , and observed no significant performance difference after the training converges .","[('experimented with', (1, 3)), ('observed', (8, 9)), ('after', (13, 14))]","[('other learning rates', (3, 6)), ('no significant performance difference', (9, 13)), ('training converges', (15, 17))]","[['no significant performance difference', 'after', 'training converges']]",[],[],[],entity_linking,7,146
1330,results,Word2 Vec vectors Vs. LSTM,[],[],[],[],[],[],entity_linking,7,148
1331,results,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,"[('shows', (0, 1))]","[('LSTM classifier', (3, 5)), ('outperforms', (5, 6)), ('Word2 Vec classifier', (7, 10))]",[],"[['LSTM classifier', 'has', 'outperforms'], ['outperforms', 'has', 'Word2 Vec classifier']]","[['Results', 'shows', 'LSTM classifier']]",[],entity_linking,7,151
1332,results,Sem Cor Vs. OMSTI,[],[],[],[],[],[],entity_linking,7,152
1333,results,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .","[('trained with', (10, 12)), ('performs', (13, 14)), ('than', (15, 16)), ('trained with', (17, 19))]","[('LSTM classifier', (8, 10)), ('OMSTI', (12, 13)), ('worse', (14, 15)), ('SemCor', (19, 20))]","[['LSTM classifier', 'trained with', 'OMSTI'], ['LSTM classifier', 'performs', 'worse'], ['OMSTI', 'performs', 'worse'], ['worse', 'trained with', 'SemCor']]",[],[],[],entity_linking,7,153
1334,tasks,The algorithm performs similarly on the different data sets .,"[('performs', (2, 3)), ('on', (4, 5))]","[('algorithm', (1, 2)), ('similarly', (3, 4)), ('different data sets', (6, 9))]","[['algorithm', 'performs', 'similarly'], ['similarly', 'on', 'different data sets']]",[],[],"[['Tasks', 'has', 'algorithm']]",entity_linking,7,159
1335,baselines,"Word 2 Vec : a nearest - neighbor classifier with Word2 Vec word embedding , which has similar performance to cutting - edge algorithms studied in on SemEval tasks .","[('with', (9, 10))]","[('Word 2 Vec', (0, 3)), ('nearest', (5, 6)), ('Word2 Vec word embedding', (10, 14))]",[],"[['Word 2 Vec', 'has', 'nearest']]",[],"[['Baselines', 'has', 'Word 2 Vec']]",entity_linking,7,182
1336,results,"Across all part of speech tags and datasets , F1 scores increase after adding more training data .","[('Across', (0, 1)), ('after', (12, 13))]","[('all part of speech tags and datasets', (1, 8)), ('F1 scores', (9, 11)), ('increase', (11, 12)), ('adding', (13, 14)), ('more training data', (14, 17))]","[['F1 scores', 'after', 'adding'], ['increase', 'after', 'adding']]","[['all part of speech tags and datasets', 'has', 'F1 scores'], ['F1 scores', 'has', 'increase'], ['adding', 'has', 'more training data']]","[['Results', 'Across', 'all part of speech tags and datasets']]",[],entity_linking,7,189
1337,results,The SemCor ( or MASC ) trained classifier is on a par with the NOAD trained classifier on F1 score .,"[('with', (12, 13)), ('on', (17, 18))]","[('SemCor ( or MASC ) trained classifier', (1, 8)), ('on a par', (9, 12)), ('NOAD trained classifier', (14, 17)), ('F1 score', (18, 20))]","[['on a par', 'with', 'NOAD trained classifier'], ['NOAD trained classifier', 'on', 'F1 score']]","[['SemCor ( or MASC ) trained classifier', 'has', 'on a par']]",[],"[['Results', 'has', 'SemCor ( or MASC ) trained classifier']]",entity_linking,7,191
1338,research-problem,LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS FROM TEXT,[],"[('LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS', (0, 8))]",[],[],[],[],entity_linking,8,2
1339,research-problem,Work done as a Google AI Resident,[],[],[],[],[],[],entity_linking,8,3
1340,model,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .","[('trained to match', (21, 24))]","[('RELIC', (2, 3))]",[],[],[],"[['Model', 'has', 'RELIC']]",entity_linking,8,21
1341,model,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .","[('apply', (1, 2)), ('to', (3, 4))]","[('RELIC', (2, 3)), ('entity typing', (4, 6)), ('entity', (22, 23)), ('trivia question answering', (38, 41))]","[['RELIC', 'to', 'entity typing']]",[],"[['Model', 'apply', 'RELIC']]",[],entity_linking,8,22
1342,research-problem,RELIC accurately captures categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies .,"[('accurately captures', (1, 3)), ('encoded by', (5, 7)), ('in', (9, 10))]","[('RELIC', (0, 1)), ('categorical information', (3, 5)), ('human experts', (7, 9)), ('Freebase and Wikipedia category hierarchies', (11, 16))]","[['RELIC', 'accurately captures', 'categorical information'], ['categorical information', 'encoded by', 'human experts'], ['human experts', 'in', 'Freebase and Wikipedia category hierarchies']]",[],[],[],entity_linking,8,24
1343,model,We also show that given just a few exemplar entities of a given category such as Scottish footballers we can use RELIC to recover the remaining entities of that category with good precision .,"[('given', (4, 5)), ('of', (10, 11)), ('such as', (14, 16)), ('use', (20, 21)), ('to recover', (22, 24)), ('of', (27, 28)), ('with', (30, 31))]","[('just', (5, 6)), ('given category', (12, 14)), ('Scottish footballers', (16, 18)), ('RELIC', (21, 22)), ('remaining entities', (25, 27)), ('good precision', (31, 33))]","[['given category', 'such as', 'Scottish footballers'], ['given category', 'use', 'RELIC'], ['RELIC', 'to recover', 'remaining entities'], ['remaining entities', 'with', 'good precision']]",[],"[['Model', 'given', 'just']]",[],entity_linking,8,26
1344,research-problem,Using RELIC for entity linking can match state - of - the - art approaches that make use of non-local and non-linguistic information about entities .,"[('for', (2, 3)), ('can match', (5, 7)), ('that make use of', (15, 19))]","[('RELIC', (1, 2)), ('entity linking', (3, 5)), ('state - of - the - art approaches', (7, 15)), ('non-local and', (19, 21))]","[['RELIC', 'for', 'entity linking'], ['RELIC', 'can match', 'state - of - the - art approaches'], ['entity linking', 'can match', 'state - of - the - art approaches'], ['state - of - the - art approaches', 'that make use of', 'non-local and']]",[],[],[],entity_linking,8,27
1345,experiments,"RELIC learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned , and not the surface form of the mention itself .","[('learns', (1, 2)), ('of', (4, 5)), ('if', (7, 8)), ('trained to match', (10, 13)), ('not', (23, 24))]","[('RELIC', (0, 1)), ('better representations', (2, 4)), ('entity properties', (5, 7)), ('contexts', (15, 16)), ('entities', (18, 19)), ('surface form', (25, 27))]","[['RELIC', 'learns', 'better representations'], ['better representations', 'of', 'entity properties'], ['better representations', 'trained to match', 'contexts'], ['entity properties', 'trained to match', 'contexts']]",[],[],[],entity_linking,8,30
1346,experiments,"We also apply RELIC 's context encoder and entity embeddings to the task of end - to - end trivia question answering , and we show that this approach can capture more than half of the answers identified by the best existing reading comprehension systems .","[('apply', (2, 3)), ('to', (10, 11)), ('of', (13, 14))]","[(""RELIC 's context encoder and entity embeddings"", (3, 10)), ('task', (12, 13)), ('end - to - end trivia question answering', (14, 22))]","[[""RELIC 's context encoder and entity embeddings"", 'to', 'task'], ['task', 'of', 'end - to - end trivia question answering']]",[],[],[],entity_linking,8,112
1347,results,"RELIC outperforms prior work , even with only 5 % of the training data .","[('even with', (5, 7)), ('of', (10, 11))]","[('RELIC', (0, 1)), ('outperforms', (1, 2)), ('prior work', (2, 4)), ('only 5 %', (7, 10)), ('training data', (12, 14))]","[['outperforms', 'even with', 'only 5 %'], ['prior work', 'even with', 'only 5 %'], ['only 5 %', 'of', 'training data']]","[['RELIC', 'has', 'outperforms'], ['outperforms', 'has', 'prior work']]",[],[],entity_linking,8,141
1348,results,We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,"[('observe', (1, 2)), ('taken by', (10, 12)), ('taken by', (19, 21))]","[('retrieve - then - read approach', (4, 10)), ('ORQA', (12, 13)), ('outperforms', (13, 14)), ('direct answer retrieval approach', (15, 19)), ('RELIC', (21, 22))]","[['retrieve - then - read approach', 'taken by', 'ORQA'], ['direct answer retrieval approach', 'taken by', 'RELIC']]","[['ORQA', 'has', 'outperforms'], ['outperforms', 'has', 'direct answer retrieval approach']]","[['Results', 'observe', 'retrieve - then - read approach']]",[],entity_linking,8,196
1349,results,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .","[('by', (11, 12))]","[('RELIC', (5, 6)), ('outperforms', (6, 7)), ('reading comprehension baseline', (8, 11)), ('20 points', (12, 14))]","[['reading comprehension baseline', 'by', '20 points']]","[['RELIC', 'has', 'outperforms'], ['outperforms', 'has', 'reading comprehension baseline']]",[],[],entity_linking,8,198
1350,research-problem,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,[],[],[],[],[],[],entity_linking,9,2
1351,research-problem,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",[],"[('Named Entity Disambiguation ( NED )', (0, 6))]",[],[],[],[],entity_linking,9,4
1352,research-problem,"By combining contexts based on the proposed embedding with standard NED features , we achieved state - of - theart accuracy of 93.1 % on the standard CoNLL dataset and 85.2 % on the TAC 2010 dataset .","[('combining', (1, 2)), ('based on', (3, 5)), ('with', (8, 9)), ('achieved', (14, 15)), ('of', (21, 22)), ('on', (24, 25)), ('on', (32, 33))]","[('contexts', (2, 3)), ('proposed embedding', (6, 8)), ('standard NED features', (9, 12)), ('state - of - theart accuracy', (15, 21)), ('93.1 %', (22, 24)), ('standard CoNLL dataset', (26, 29)), ('85.2 %', (30, 32)), ('TAC 2010 dataset', (34, 37))]","[['contexts', 'based on', 'proposed embedding'], ['proposed embedding', 'with', 'standard NED features'], ['contexts', 'achieved', 'state - of - theart accuracy'], ['proposed embedding', 'achieved', 'state - of - theart accuracy'], ['state - of - theart accuracy', 'of', '93.1 %'], ['93.1 %', 'on', 'standard CoNLL dataset'], ['93.1 %', 'on', '85.2 %'], ['93.1 %', 'on', 'TAC 2010 dataset'], ['85.2 %', 'on', 'TAC 2010 dataset'], ['85.2 %', 'on', 'TAC 2010 dataset']]","[['contexts', 'has', 'proposed embedding']]","[['Research problem', 'combining', 'contexts']]",[],entity_linking,9,9
1353,research-problem,"Named Entity Disambiguation ( NED ) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",[],"[('Named Entity Disambiguation ( NED )', (0, 6))]",[],[],[],[],entity_linking,9,11
1354,model,The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low - dimensional vector space .,"[('designed to capture', (3, 6)), ('of', (9, 10)), ('when', (11, 12)), ('placed near', (15, 17)), ('in', (19, 20))]","[('semantic similarity', (7, 9)), ('words', (10, 11)), ('similar words', (12, 14)), ('one another', (17, 19)), ('relatively low - dimensional vector space', (21, 27))]","[['semantic similarity', 'of', 'words'], ['words', 'when', 'similar words'], ['similar words', 'placed near', 'one another'], ['one another', 'in', 'relatively low - dimensional vector space']]",[],[],[],entity_linking,9,20
1355,model,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .","[('propose', (5, 6)), ('to construct', (8, 10)), ('into', (19, 20))]","[('method', (7, 8)), ('novel embedding', (11, 13)), ('words and entities', (16, 19)), ('same continuous vector space', (21, 25))]","[['method', 'to construct', 'novel embedding'], ['words and entities', 'into', 'same continuous vector space']]",[],"[['Model', 'propose', 'method']]",[],entity_linking,9,21
1356,model,Our model consists of the following three models based on the skip - gram model :,"[('consists of', (2, 4)), ('based on', (8, 10))]","[('skip - gram model', (11, 15))]",[],[],"[['Model', 'consists of', 'skip - gram model']]",[],entity_linking,9,26
1357,model,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .","[('given', (14, 15)), ('in', (18, 19)), ('given', (34, 35)), ('in', (38, 39)), ('of', (42, 43)), ('using', (63, 64))]","[('conventional skip - gram model', (3, 8)), ('neighboring words', (12, 14)), ('target word', (16, 18)), ('text corpora', (19, 21)), ('KB graph model', (25, 28)), ('neighboring entities', (32, 34)), ('target entity', (36, 38)), ('link graph', (40, 42)), ('KB', (44, 45)), ('anchor context model', (50, 53)), ('anchors', (64, 65)), ('KB', (71, 72))]","[['neighboring words', 'given', 'target word'], ['neighboring entities', 'given', 'target entity'], ['target word', 'in', 'text corpora'], ['neighboring entities', 'given', 'target entity'], ['target entity', 'in', 'link graph'], ['link graph', 'of', 'KB']]",[],[],"[['Model', 'has', 'conventional skip - gram model']]",entity_linking,9,27
1358,model,"By jointly optimizing these models , our method simultaneously learns the embedding of words and entities .","[('jointly optimizing', (1, 3)), ('simultaneously learns', (8, 10)), ('of', (12, 13))]","[('our method', (6, 8)), ('embedding', (11, 12)), ('words and entities', (13, 16))]","[['our method', 'simultaneously learns', 'embedding'], ['embedding', 'of', 'words and entities']]",[],"[['Model', 'jointly optimizing', 'our method']]",[],entity_linking,9,28
1359,model,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .","[('develop', (8, 9)), ('computes', (14, 15)), ('using', (17, 18))]","[('straightforward NED method', (10, 13)), ('two contexts', (15, 17)), ('proposed embedding', (19, 21)), ('textual context similarity', (22, 25)), ('coherence', (27, 28))]","[['straightforward NED method', 'computes', 'two contexts'], ['two contexts', 'using', 'proposed embedding']]","[['proposed embedding', 'name', 'textual context similarity']]","[['Model', 'develop', 'straightforward NED method']]",[],entity_linking,9,29
1360,model,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .","[('combines', (3, 4)), ('with', (6, 7)), ('using', (16, 17))]","[('Our', (0, 1)), ('NED method', (1, 3)), ('contexts', (5, 6)), ('several standard features', (7, 10)), ('prior probability', (13, 15)), ('supervised machine learning', (17, 20))]","[['Our', 'combines', 'contexts'], ['NED method', 'combines', 'contexts'], ['contexts', 'with', 'several standard features'], ['several standard features', 'using', 'supervised machine learning']]","[['Our', 'has', 'NED method'], ['NED method', 'has', 'contexts'], ['several standard features', 'name', 'prior probability']]",[],"[['Model', 'has', 'Our']]",entity_linking,9,32
1361,research-problem,Skip- gram Model for Word Similarity,[],[],[],[],[],[],entity_linking,9,40
1362,hyperparameters,We use stochastic gradient descent ( SGD ) for the optimization .,"[('use', (1, 2)), ('for', (8, 9))]","[('stochastic gradient descent ( SGD )', (2, 8)), ('optimization', (10, 11))]","[['stochastic gradient descent ( SGD )', 'for', 'optimization']]",[],"[['Hyperparameters', 'use', 'stochastic gradient descent ( SGD )']]",[],entity_linking,9,99
1363,experiments,"Following , we also used learning rate ? = 0.025 which linearly decreased with the iterations of the Wikipedia dump .","[('used', (4, 5)), ('of', (16, 17))]","[('learning rate ? = 0.025', (5, 10)), ('linearly decreased', (11, 13)), ('iterations', (15, 16)), ('Wikipedia dump', (18, 20))]","[['iterations', 'of', 'Wikipedia dump']]","[['learning rate ? = 0.025', 'has', 'linearly decreased']]",[],[],entity_linking,9,164
1364,results,"Surprisingly , we attained results comparable with those of some state - of - the - art methods on the both datasets by only using base features .","[('attained', (3, 4)), ('comparable with', (5, 7)), ('by', (22, 23))]","[('results', (4, 5)), ('some state - of - the - art methods', (9, 18)), ('base features', (25, 27))]","[['results', 'comparable with', 'some state - of - the - art methods']]",[],"[['Results', 'attained', 'results']]",[],entity_linking,9,225
1365,results,Adding string similarity features slightly further improved performance .,"[('Adding', (0, 1))]","[('string similarity features', (1, 4)), ('slightly further improved performance', (4, 8))]",[],"[['string similarity features', 'has', 'slightly further improved performance']]","[['Results', 'Adding', 'string similarity features']]",[],entity_linking,9,226
1366,results,Our method outperformed some state - of - the - art methods without using coherence .,"[('without using', (12, 14))]","[('Our method', (0, 2)), ('outperformed', (2, 3)), ('some state - of - the - art methods', (3, 12)), ('coherence', (14, 15))]","[['some state - of - the - art methods', 'without using', 'coherence']]","[['Our method', 'has', 'outperformed'], ['outperformed', 'has', 'some state - of - the - art methods']]",[],[],entity_linking,9,228
1367,research-problem,"3D Face Morphable Models "" In - the - Wild """,[],"[('3D Face Morphable Models', (0, 4))]",[],[],[],[],face_alignment,0,2
1368,research-problem,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,[],"[('3 D facial shape estimation from single images', (0, 8))]",[],[],[],[],face_alignment,0,18
1369,model,The method requires the construction of a 3 DMM which is a statistical model of facial texture and shape in a space where there are explicit correspondences .,"[('requires', (2, 3)), ('of', (5, 6)), ('is', (10, 11)), ('of', (14, 15)), ('in', (19, 20)), ('where there are', (22, 25))]","[('construction', (4, 5)), ('3 DMM', (7, 9)), ('statistical model', (12, 14)), ('facial texture and shape', (15, 19)), ('space', (21, 22)), ('explicit correspondences', (25, 27))]","[['construction', 'of', '3 DMM'], ['statistical model', 'of', 'facial texture and shape'], ['3 DMM', 'is', 'statistical model'], ['statistical model', 'of', 'facial texture and shape'], ['facial texture and shape', 'in', 'space'], ['space', 'where there are', 'explicit correspondences']]","[['construction', 'has', '3 DMM']]","[['Model', 'requires', 'construction']]",[],face_alignment,0,21
1370,research-problem,"3 D facial shape recovery from a single image under "" inthe - wild "" conditions is still an open and challenging problem in computer vision mainly due to the fact that :",[],"[('3 D facial shape recovery from a single image', (0, 9))]",[],[],[],[],face_alignment,0,28
1371,research-problem,The general problem of extracting the 3D facial shape from a single image is an ill - posed problem which is notoriously difficult to be solved without the use of any statistical priors for the shape and texture of faces .,[],"[('extracting the 3D facial shape', (4, 9)), ('single image', (11, 13))]",[],[],[],[],face_alignment,0,29
1372,research-problem,"Learning statistical priors of the 3D facial shape and texture for "" in - the - wild "" images is currently very difficult by using modern acquisition devices .",[],"[('Learning statistical priors of', (0, 4)), ('3D facial shape and texture for "" in - the - wild "" images', (5, 19))]",[],[],[],[],face_alignment,0,33
1373,model,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .","[('from', (10, 11)), ('with', (25, 26))]","[('learning', (5, 6)), ('statistical texture model', (7, 10)), ('"" in - the -wild "" facial images', (11, 19)), ('in', (22, 23)), ('full correspondence', (23, 25)), ('statistical shape', (27, 29)), ('identity and expression variations', (33, 37))]","[['statistical texture model', 'from', '"" in - the -wild "" facial images'], ['full correspondence', 'with', 'statistical shape']]","[['learning', 'has', 'statistical texture model'], ['in', 'has', 'full correspondence']]",[],[],face_alignment,0,41
1374,model,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .","[('for', (30, 31))]","[('featurebased texture models', (27, 30)), ('3 DMMs', (31, 33))]","[['featurebased texture models', 'for', '3 DMMs']]",[],[],[],face_alignment,0,42
1375,model,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .","[('show', (1, 2)), ('of using', (5, 7)), ('is', (19, 20)), ('gets', (24, 25))]","[('advantage', (4, 5)), ('"" in - the -wild "" feature - based texture model', (8, 19)), ('fitting strategy', (22, 24)), ('simplified', (25, 26))]","[['advantage', 'of using', '"" in - the -wild "" feature - based texture model'], ['"" in - the -wild "" feature - based texture model', 'is', 'fitting strategy'], ['fitting strategy', 'gets', 'simplified']]","[['"" in - the -wild "" feature - based texture model', 'has', 'fitting strategy'], ['fitting strategy', 'has', 'simplified']]","[['Model', 'show', 'advantage']]",[],face_alignment,0,43
1376,model,"By capitalising on the recent advancements in fitting statistical deformable models , we propose a novel and fast algorithm for fitting "" in - the -wild "" 3 DMMs .","[('propose', (13, 14)), ('for fitting', (19, 21))]","[('fitting statistical', (7, 9)), ('novel and fast algorithm', (15, 19)), ('"" in - the -wild "" 3 DMMs', (21, 29))]","[['novel and fast algorithm', 'for fitting', '"" in - the -wild "" 3 DMMs']]",[],[],[],face_alignment,0,44
1377,hyperparameters,Gauss - Newton Optimization,[],[],[],[],[],[],face_alignment,0,151
1378,hyperparameters,"To train our model , which we label as ITW , we use a variant of the Basel Face Model ( BFM ) that we trained to contain both identities drawn from the original BFM model along with expressions provided by .","[('train', (1, 2)), ('label as', (7, 9)), ('use', (12, 13)), ('of', (15, 16)), ('trained to contain', (25, 28)), ('drawn from', (30, 32))]","[('our model', (2, 4)), ('ITW', (9, 10)), ('variant', (14, 15)), ('both identities', (28, 30)), ('original BFM model', (33, 36))]","[['our model', 'label as', 'ITW'], ['our model', 'use', 'variant'], ['both identities', 'drawn from', 'original BFM model']]",[],"[['Hyperparameters', 'train', 'our model']]",[],face_alignment,0,206
1379,experiments,3D Shape Recovery,[],[],[],[],[],[],face_alignment,0,210
1380,experiments,"Herein , we evaluate our "" in - the -wild "" 3 DMM ( ITW ) in terms of 3D shape estimation accuracy against two popular state - of - the - art alternative 3 DMM formulations .","[('evaluate', (3, 4)), ('in terms of', (16, 19)), ('against', (23, 24))]","[('"" in - the -wild "" 3 DMM ( ITW )', (5, 16)), ('3D shape estimation accuracy', (19, 23)), ('two popular state - of - the - art alternative 3 DMM formulations', (24, 37))]","[['"" in - the -wild "" 3 DMM ( ITW )', 'in terms of', '3D shape estimation accuracy'], ['3D shape estimation accuracy', 'against', 'two popular state - of - the - art alternative 3 DMM formulations']]",[],[],[],face_alignment,0,211
1381,baselines,The first one is a classic 3 DMM with the original Basel laboratory texture model and full lighting equation which we term Classic .,"[('with', (8, 9)), ('term', (21, 22))]","[('classic 3 DMM', (5, 8)), ('original Basel laboratory texture model', (10, 15)), ('full lighting equation', (16, 19)), ('Classic', (22, 23))]","[['classic 3 DMM', 'with', 'original Basel laboratory texture model'], ['classic 3 DMM', 'with', 'full lighting equation'], ['full lighting equation', 'term', 'Classic']]",[],[],"[['Baselines', 'has', 'classic 3 DMM']]",face_alignment,0,212
1382,baselines,The second is the texture - less linear model proposed in which we refer to as Linear .,"[('refer to', (13, 15))]","[('texture - less linear model', (4, 9)), ('Linear', (16, 17))]",[],[],[],"[['Baselines', 'has', 'texture - less linear model']]",face_alignment,0,213
1383,hyperparameters,"The mean mesh of each model under testis landmarked with the same 49 point markup used in the dataset , and is registered against the ground truth mesh by performing a Procrustes alignment using the sparse annotations followed by Non-Rigid Iterative Closest Point ( N - ICP ) to iteratively deform the two surfaces until they are brought into correspondence .","[('of', (3, 4)), ('under', (6, 7)), ('with', (9, 10)), ('registered against', (22, 24)), ('by performing', (28, 30)), ('using', (33, 34)), ('followed by', (37, 39)), ('to iteratively deform', (48, 51)), ('until', (54, 55)), ('brought into', (57, 59))]","[('mean mesh', (1, 3)), ('each model', (4, 6)), ('testis landmarked', (7, 9)), ('same 49 point markup', (11, 15)), ('ground truth mesh', (25, 28)), ('Procrustes alignment', (31, 33)), ('sparse annotations', (35, 37)), ('Non-Rigid Iterative Closest Point ( N - ICP )', (39, 48)), ('two surfaces', (52, 54)), ('correspondence', (59, 60))]","[['mean mesh', 'of', 'each model'], ['each model', 'under', 'testis landmarked'], ['testis landmarked', 'with', 'same 49 point markup'], ['ground truth mesh', 'by performing', 'Procrustes alignment'], ['Procrustes alignment', 'using', 'sparse annotations'], ['sparse annotations', 'followed by', 'Non-Rigid Iterative Closest Point ( N - ICP )'], ['Non-Rigid Iterative Closest Point ( N - ICP )', 'to iteratively deform', 'two surfaces']]",[],[],"[['Hyperparameters', 'has', 'mean mesh']]",face_alignment,0,216
1384,results,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .","[('does', (6, 7)), ('most able to recover', (14, 18)), ('due to', (21, 23)), ('for', (27, 28))]","[('texture - free Linear model', (1, 6)), ('better', (7, 8)), ('ITW model', (11, 13)), ('facial shapes', (19, 21)), ('ideal feature basis', (24, 27)), ('"" in - the -wild "" conditions', (29, 36))]","[['texture - free Linear model', 'does', 'better'], ['ITW model', 'most able to recover', 'facial shapes'], ['facial shapes', 'due to', 'ideal feature basis'], ['ideal feature basis', 'for', '"" in - the -wild "" conditions']]","[['texture - free Linear model', 'has', 'better'], ['better', 'has', 'ITW model']]",[],"[['Results', 'has', 'texture - free Linear model']]",face_alignment,0,223
1385,experiments,ITW slightly outperforms IMM even though both IMM and PS - NL use all four available images of each subject .,[],"[('ITW', (0, 1)), ('slightly outperforms', (1, 3)), ('IMM', (3, 4))]",[],"[['ITW', 'has', 'slightly outperforms'], ['slightly outperforms', 'has', 'IMM']]",[],[],face_alignment,0,231
1386,research-problem,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,[],"[('Large Pose 3D Face Reconstruction', (0, 5))]",[],[],[],[],face_alignment,1,2
1387,research-problem,"Our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry ( including the non-visible parts of the face ) bypassing the construction ( during training ) and fitting ( during testing ) of a 3D Morphable Model .","[('works with', (2, 4)), ('does not require', (12, 15)), ('works for', (24, 26)), ('including', (45, 46)), ('bypassing', (53, 54)), ('of', (66, 67))]","[('Our CNN', (0, 2)), ('just', (4, 5)), ('accurate', (15, 16)), ('arbitrary facial poses and expressions', (26, 31)), ('reconstruct', (37, 38)), ('whole 3 D facial geometry', (39, 44)), ('non-visible parts of', (47, 50)), ('construction', (55, 56)), ('fitting', (61, 62)), ('3D Morphable Model', (68, 71))]","[['Our CNN', 'works with', 'just'], ['Our CNN', 'does not require', 'accurate'], ['whole 3 D facial geometry', 'including', 'non-visible parts of'], ['whole 3 D facial geometry', 'bypassing', 'construction'], ['fitting', 'of', '3D Morphable Model']]","[['reconstruct', 'has', 'whole 3 D facial geometry']]",[],[],face_alignment,1,9
1388,research-problem,3 D face reconstruction is the problem of recovering the 3D facial geometry from 2D images .,[],"[('3 D face reconstruction', (0, 4))]",[],[],[],[],face_alignment,1,14
1389,research-problem,This work is on 3D face reconstruction using only a single image .,"[('on', (3, 4)), ('using', (7, 8))]","[('3D face reconstruction', (4, 7))]",[],[],"[['Research problem', 'on', '3D face reconstruction']]",[],face_alignment,1,17
1390,model,"In this paper , we propose to approach it , for the first time to the best of our knowledge , by directly learning a mapping from pixels to 3D coordinates using a Convolutional Neural Network ( CNN ) .","[('by directly learning', (21, 24)), ('from', (26, 27)), ('using', (31, 32))]","[('mapping', (25, 26)), ('pixels to 3D coordinates', (27, 31)), ('Convolutional Neural Network ( CNN )', (33, 39))]","[['mapping', 'from', 'pixels to 3D coordinates'], ['pixels to 3D coordinates', 'using', 'Convolutional Neural Network ( CNN )']]",[],"[['Model', 'by directly learning', 'mapping']]",[],face_alignment,1,19
1391,model,"Besides its simplicity , our approach works with totally unconstrained images downloaded from the web , including facial images of arbitrary poses , facial expressions and occlusions , as shown in .","[('works with', (6, 8)), ('downloaded from', (11, 13)), ('including', (16, 17)), ('of', (19, 20))]","[('totally unconstrained images', (8, 11)), ('web', (14, 15)), ('facial images', (17, 19)), ('arbitrary poses', (20, 22)), ('facial expressions', (23, 25)), ('occlusions', (26, 27))]","[['totally unconstrained images', 'downloaded from', 'web'], ['totally unconstrained images', 'including', 'facial images'], ['facial images', 'of', 'arbitrary poses'], ['facial images', 'of', 'occlusions']]",[],"[['Model', 'works with', 'totally unconstrained images']]",[],face_alignment,1,20
1392,hyperparameters,"Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .","[('trained', (5, 6)), ('using', (11, 12)), ('with', (13, 14)), ('of', (18, 19)), ('lowered after', (25, 27)), ('to', (29, 30))]","[('Each of our architectures', (0, 4)), ('end - to - end', (6, 11)), ('RMSProp', (12, 13)), ('initial learning rate', (15, 18)), ('10 ? 4', (19, 22)), ('40 epochs', (27, 29)), ('10 ?5', (30, 32))]","[['Each of our architectures', 'trained', 'end - to - end'], ['Each of our architectures', 'using', 'RMSProp'], ['end - to - end', 'using', 'RMSProp'], ['RMSProp', 'with', 'initial learning rate'], ['initial learning rate', 'of', '10 ? 4'], ['initial learning rate', 'lowered after', '40 epochs'], ['40 epochs', 'to', '10 ?5']]",[],[],"[['Hyperparameters', 'has', 'Each of our architectures']]",face_alignment,1,147
1393,hyperparameters,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .","[('During', (0, 1)), ('applied to', (6, 8)), ('applied', (25, 26))]","[('training', (1, 2)), ('random augmentation', (3, 5)), ('each input sample ( face image )', (8, 15)), ('in - plane rotation r ? [ ?45 , ... , 45 ]', (26, 39)), ('translation t z', (40, 43))]","[['random augmentation', 'applied to', 'each input sample ( face image )']]","[['training', 'has', 'random augmentation']]","[['Hyperparameters', 'During', 'training']]",[],face_alignment,1,148
1394,results,"3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .","[('on', (3, 4))]","[('3DDFA and EOS', (0, 3))]",[],[],[],[],face_alignment,1,160
1395,results,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .","[('perform', (4, 5)), ('across', (6, 7)), ('of', (10, 11))]","[('All VRNs', (2, 4)), ('well', (5, 6)), ('whole spectrum', (8, 10)), ('facial poses , expressions and occlusions', (11, 17))]","[['All VRNs', 'perform', 'well'], ['well', 'across', 'whole spectrum'], ['whole spectrum', 'of', 'facial poses , expressions and occlusions']]",[],[],"[['Results', 'has', 'All VRNs']]",face_alignment,1,161
1396,results,"Also , there are no significant performance discrepancies across different datasets ( ALFW2000 - 3D seems to be slightly more difficult ) .","[('there are', (2, 4)), ('across', (8, 9))]","[('no', (4, 5)), ('significant performance discrepancies', (5, 8)), ('different datasets', (9, 11))]","[['significant performance discrepancies', 'across', 'different datasets']]","[['no', 'has', 'significant performance discrepancies']]","[['Results', 'there are', 'no']]",[],face_alignment,1,162
1397,baselines,VRN - Guided uses another stacked hourglass network for landmark localization .,"[('uses', (3, 4)), ('for', (8, 9))]","[('VRN - Guided', (0, 3)), ('another stacked hourglass network', (4, 8)), ('landmark localization', (9, 11))]","[['VRN - Guided', 'uses', 'another stacked hourglass network'], ['another stacked hourglass network', 'for', 'landmark localization']]",[],[],"[['Baselines', 'has', 'VRN - Guided']]",face_alignment,1,164
1398,results,"4 . VRN - Multitask does not always perform particularly better than the plain VRN ( in fact on BU - 4 DFE it performs worse ) , not justifying the increase of network complexity .","[('does not always', (5, 8)), ('than', (11, 12))]","[('VRN - Multitask', (2, 5)), ('particularly better', (9, 11)), ('plain VRN', (13, 15))]","[['particularly better', 'than', 'plain VRN']]",[],[],"[['Results', 'has', 'VRN - Multitask']]",face_alignment,1,165
1399,ablation-analysis,"For all experiments reported , we used the best performing VRN - Guided .","[('used', (6, 7))]","[('best performing', (8, 10)), ('VRN - Guided', (10, 13))]",[],"[['best performing', 'has', 'VRN - Guided']]","[['Ablation analysis', 'used', 'best performing']]",[],face_alignment,1,199
1400,ablation-analysis,"The performance of the 3D reconstruction dropped by a negligible amount , suggesting that as long as the Gaussians are of a sensible size , guidance will always help .","[('of', (2, 3))]","[('performance', (1, 2)), ('3D reconstruction', (4, 6)), ('dropped', (6, 7)), ('negligible amount', (9, 11))]","[['performance', 'of', '3D reconstruction']]","[['3D reconstruction', 'has', 'dropped']]",[],"[['Ablation analysis', 'has', 'performance']]",face_alignment,1,211
1401,research-problem,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,[],"[('Robust Facial Landmark Localisation', (3, 7))]",[],[],[],[],face_alignment,10,2
1402,research-problem,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .",[],"[('Facial landmark localisation', (0, 3)), ('face alignment', (5, 7))]",[],[],[],[],face_alignment,10,14
1403,research-problem,"The existing challenge is to achieve robust and accurate landmark localisation of unconstrained faces that are impacted by a variety of appearance variations , e.g. in pose , expression , illumination , image blurring and occlusion .","[('of', (11, 12)), ('impacted by', (16, 18)), ('e.g. in', (24, 26))]","[('robust and accurate landmark localisation', (6, 11)), ('unconstrained faces', (12, 14)), ('variety of', (19, 21)), ('appearance variations', (21, 23)), ('pose', (26, 27)), ('expression', (28, 29)), ('illumination', (30, 31)), ('image blurring', (32, 34)), ('occlusion', (35, 36))]","[['robust and accurate landmark localisation', 'of', 'unconstrained faces'], ['unconstrained faces', 'impacted by', 'variety of'], ['appearance variations', 'e.g. in', 'pose'], ['appearance variations', 'e.g. in', 'expression'], ['appearance variations', 'e.g. in', 'illumination'], ['appearance variations', 'e.g. in', 'image blurring'], ['appearance variations', 'e.g. in', 'occlusion']]","[['variety of', 'has', 'appearance variations']]",[],[],face_alignment,10,17
1404,model,"To further address the issue , we propose a new loss function , namely Wing loss ( ) , for robust facial landmark localisation .","[('propose', (7, 8)), ('namely', (13, 14)), ('for', (19, 20))]","[('new loss function', (9, 12)), ('Wing loss ( )', (14, 18)), ('robust facial landmark localisation', (20, 24))]","[['new loss function', 'namely', 'Wing loss ( )'], ['new loss function', 'for', 'robust facial landmark localisation'], ['Wing loss ( )', 'for', 'robust facial landmark localisation']]","[['new loss function', 'name', 'Wing loss ( )']]","[['Model', 'propose', 'new loss function']]",[],face_alignment,10,30
1405,model,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .","[('namely', (5, 6)), ('designed to improve', (12, 15)), ('for', (21, 22))]","[('novel loss function', (1, 4)), ('Wing loss', (7, 9)), ('deep neural network training capability', (16, 21)), ('small and medium range errors', (22, 27))]","[['novel loss function', 'namely', 'Wing loss'], ['novel loss function', 'designed to improve', 'deep neural network training capability'], ['deep neural network training capability', 'for', 'small and medium range errors']]","[['novel loss function', 'has', 'Wing loss']]",[],[],face_alignment,10,34
1406,experimental-setup,"In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .","[('used', (5, 6))]","[('Matlab 2017a', (6, 8))]",[],[],"[['Experimental setup', 'used', 'Matlab 2017a']]",[],face_alignment,10,209
1407,experimental-setup,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .","[('of', (4, 5)), ('conducted on', (8, 10)), ('running', (12, 13)), ('with', (15, 16))]","[('server', (11, 12)), ('Ubuntu 16.04', (13, 15)), ('2 Intel Xeon E5-2667 v4 CPU', (16, 22)), ('256 GB RAM', (23, 26)), ('4 NVIDIA GeForce GTX Titan X ( Pascal ) cards', (27, 37))]","[['server', 'running', 'Ubuntu 16.04'], ['server', 'running', '4 NVIDIA GeForce GTX Titan X ( Pascal ) cards'], ['Ubuntu 16.04', 'with', '2 Intel Xeon E5-2667 v4 CPU'], ['Ubuntu 16.04', 'with', '4 NVIDIA GeForce GTX Titan X ( Pascal ) cards']]",[],"[['Experimental setup', 'of', 'server']]",[],face_alignment,10,210
1408,experimental-setup,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .","[('set', (1, 2)), ('to', (5, 6)), ('for', (19, 20))]","[('weight decay', (3, 5)), ('5 10 ? 4', (6, 10)), ('momentum', (11, 12)), ('0.9', (13, 14)), ('batch size', (15, 17)), ('8', (18, 19)), ('network training', (20, 22))]","[['weight decay', 'to', '5 10 ? 4'], ['momentum', 'to', '0.9'], ['batch size', 'to', '8'], ['8', 'for', 'network training']]","[['weight decay', 'has', '5 10 ? 4'], ['momentum', 'has', '0.9'], ['batch size', 'has', '8']]","[['Experimental setup', 'set', 'weight decay']]",[],face_alignment,10,212
1409,experimental-setup,Each model was trained for 120 k iterations .,"[('trained for', (3, 5))]","[('120 k iterations', (5, 8))]",[],[],"[['Experimental setup', 'trained for', '120 k iterations']]",[],face_alignment,10,213
1410,experimental-setup,"The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .","[('used for', (5, 7)), ('with', (13, 14)), ('used to', (19, 21))]","[('standard ReLu function', (1, 4)), ('nonlinear activation', (7, 9)), ('Max pooling', (11, 13)), ('stride of 2', (15, 18)), ('downsize', (21, 22)), ('feature maps', (22, 24))]","[['standard ReLu function', 'used for', 'nonlinear activation'], ['Max pooling', 'with', 'stride of 2'], ['Max pooling', 'used to', 'downsize'], ['stride of 2', 'used to', 'downsize']]","[['downsize', 'has', 'feature maps']]",[],"[['Experimental setup', 'has', 'standard ReLu function']]",face_alignment,10,215
1411,experimental-setup,"For the convolutional layer , we used 3 3 kernels with the stride of 1 .","[('For', (0, 1)), ('used', (6, 7)), ('with', (10, 11))]","[('convolutional layer', (2, 4)), ('3 3 kernels', (7, 10)), ('stride of 1', (12, 15))]","[['convolutional layer', 'used', '3 3 kernels'], ['3 3 kernels', 'with', 'stride of 1']]",[],"[['Experimental setup', 'For', 'convolutional layer']]",[],face_alignment,10,216
1412,experimental-setup,"For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .","[('set to', (12, 14)), ('for', (15, 16))]","[('PDB strategy', (3, 5)), ('number of bins K', (7, 11)), ('17', (14, 15)), ('AFLW', (16, 17)), ('9', (18, 19)), ('300W', (20, 21))]","[['number of bins K', 'set to', '17'], ['17', 'for', 'AFLW'], ['17', 'for', '9'], ['9', 'for', '300W']]","[['PDB strategy', 'has', 'number of bins K'], ['number of bins K', 'has', '17']]",[],[],face_alignment,10,218
1413,experimental-setup,"For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .","[('For', (0, 1)), ('is', (9, 10)), ('reduced', (15, 16)), ('from', (19, 20)), ('to', (24, 25)), ('for', (28, 29))]","[('CNN - 6', (1, 4)), ('input image size', (6, 9)), ('64 64 3', (10, 13)), ('learning rate', (17, 19)), ('3 10 ? 6', (20, 24)), ('3 10 ?8', (25, 28)), ('L2 loss', (30, 32)), ('3 10 ?5', (35, 38)), ('other loss functions', (45, 48))]","[['input image size', 'is', '64 64 3'], ['input image size', 'reduced', 'learning rate'], ['learning rate', 'from', '3 10 ? 6'], ['learning rate', 'from', '3 10 ?5'], ['3 10 ? 6', 'to', '3 10 ?8'], ['3 10 ?8', 'for', 'L2 loss']]","[['CNN - 6', 'has', 'input image size'], ['input image size', 'has', '64 64 3'], ['learning rate', 'has', '3 10 ? 6']]","[['Experimental setup', 'For', 'CNN - 6']]",[],face_alignment,10,219
1414,experimental-setup,"The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .","[('of', (2, 3)), ('set tow', (7, 9)), ('For', (15, 16)), ('is', (24, 25)), ('reduced', (30, 31)), ('from', (34, 35)), ('to', (39, 40)), ('for', (43, 44)), ('from', (49, 50))]","[('parameters', (1, 2)), ('Wing loss', (4, 6)), ('CNN - 7', (16, 19)), ('input image size', (21, 24)), ('128 128', (25, 27)), ('learning rate', (32, 34)), ('1 10 ? 6', (35, 39)), ('1 10 ?8', (40, 43)), ('L2 loss', (45, 47)), ('1 10 ? 5 to 1 10 ? 7', (50, 59)), ('other loss functions', (61, 64))]","[['parameters', 'of', 'Wing loss'], ['1 10 ? 5 to 1 10 ? 7', 'For', 'other loss functions'], ['input image size', 'is', '128 128'], ['learning rate', 'from', '1 10 ? 6'], ['1 10 ? 6', 'to', '1 10 ?8'], ['1 10 ?8', 'for', 'L2 loss'], ['learning rate', 'from', '1 10 ? 5 to 1 10 ? 7']]","[['parameters', 'has', 'Wing loss'], ['CNN - 7', 'has', 'input image size'], ['input image size', 'has', '128 128'], ['learning rate', 'has', '1 10 ? 6']]",[],"[['Experimental setup', 'has', 'parameters']]",face_alignment,10,220
1415,experimental-setup,The parameters of the Wing loss were set tow = 15 and = 3 .,"[('of', (2, 3)), ('set tow', (7, 9))]","[('parameters', (1, 2)), ('Wing loss', (4, 6))]","[['parameters', 'of', 'Wing loss']]",[],[],"[['Experimental setup', 'has', 'parameters']]",face_alignment,10,221
1416,experimental-setup,"To perform data augmentation , we randomly rotated each training image between [ ? 30 , 30 ] degrees for CNN - 6 and between [ ? 10 , 10 ] degrees for CNN - 7 .","[('perform', (1, 2)), ('randomly rotated', (6, 8)), ('between', (11, 12)), ('for', (19, 20)), ('between', (24, 25)), ('for', (32, 33))]","[('data augmentation', (2, 4)), ('each training image', (8, 11)), ('[ ? 30 , 30 ] degrees', (12, 19)), ('CNN - 6', (20, 23)), ('[ ? 10 , 10 ] degrees', (25, 32)), ('CNN - 7', (33, 36))]","[['data augmentation', 'randomly rotated', 'each training image'], ['data augmentation', 'between', 'each training image'], ['each training image', 'between', '[ ? 30 , 30 ] degrees'], ['each training image', 'between', '[ ? 10 , 10 ] degrees'], ['[ ? 30 , 30 ] degrees', 'for', 'CNN - 6'], ['each training image', 'between', '[ ? 30 , 30 ] degrees'], ['each training image', 'between', '[ ? 10 , 10 ] degrees'], ['[ ? 10 , 10 ] degrees', 'for', 'CNN - 7']]",[],"[['Experimental setup', 'perform', 'data augmentation']]",[],face_alignment,10,222
1417,experimental-setup,"In addition , we randomly flipped each training image with the probability of 50 % .","[('randomly flipped', (4, 6)), ('with', (9, 10))]","[('each training image', (6, 9)), ('probability of', (11, 13)), ('50 %', (13, 15))]","[['each training image', 'with', 'probability of']]","[['probability of', 'has', '50 %']]","[['Experimental setup', 'randomly flipped', 'each training image']]",[],face_alignment,10,223
1418,experimental-setup,"For bounding box perturbation , we applied random translations to the upper-left and bottom - right corners of the face bounding box within 5 % of the bounding .","[('For', (0, 1)), ('applied', (6, 7)), ('to', (9, 10)), ('of', (17, 18)), ('within', (22, 23)), ('of', (25, 26))]","[('bounding box perturbation', (1, 4)), ('random translations', (7, 9)), ('upper-left and bottom - right corners', (11, 17)), ('face bounding box', (19, 22)), ('5 %', (23, 25)), ('bounding', (27, 28))]","[['bounding box perturbation', 'applied', 'random translations'], ['random translations', 'to', 'upper-left and bottom - right corners'], ['upper-left and bottom - right corners', 'of', 'face bounding box'], ['face bounding box', 'within', '5 %'], ['upper-left and bottom - right corners', 'of', 'face bounding box'], ['5 %', 'of', 'bounding']]",[],"[['Experimental setup', 'For', 'bounding box perturbation']]",[],face_alignment,10,224
1419,experimental-setup,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .","[('randomly injected', (3, 5)), ('to', (11, 12)), ('with', (15, 16))]","[('Gaussian blur (? = 1 )', (5, 11)), ('each training image', (12, 15)), ('probability', (17, 18)), ('50 %', (19, 21))]","[['Gaussian blur (? = 1 )', 'to', 'each training image'], ['each training image', 'with', 'probability']]","[['probability', 'has', '50 %']]","[['Experimental setup', 'randomly injected', 'Gaussian blur (? = 1 )']]",[],face_alignment,10,227
1420,results,Comparison with state of the art 7.2.1 AFLW,[],"[('Comparison with state of the art', (0, 6))]",[],[],[],"[['Results', 'has', 'Comparison with state of the art']]",face_alignment,10,234
1421,experimental-setup,"In our experiments , we used our two - stage facial landmark localisation framework by stacking the CNN - 6 and CNN - 7 networks ( denoted by CNN - 6 / 7 ) , as introduced in Section 6 .","[('used', (5, 6)), ('by stacking', (14, 16))]","[('our two - stage facial landmark localisation framework', (6, 14)), ('CNN - 6 and CNN - 7 networks', (17, 25))]","[['our two - stage facial landmark localisation framework', 'by stacking', 'CNN - 6 and CNN - 7 networks']]",[],"[['Experimental setup', 'used', 'our two - stage facial landmark localisation framework']]",[],face_alignment,10,240
1422,experimental-setup,"In addition , the proposed Pose - based Data Balancing ( PDB ) strategy was adopted , as presented in Section 5 .",[],"[('proposed Pose - based Data Balancing ( PDB ) strategy', (4, 14))]",[],[],[],[],face_alignment,10,241
1423,results,"As shown in , our CNN - 6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function ( magenta solid line ) .","[('trained with', (16, 18))]","[('our CNN - 6/7 network', (4, 9)), ('outperforms', (9, 10)), ('all the other approaches', (10, 14)), ('commonly used L2 loss function ( magenta solid line )', (19, 29))]","[['all the other approaches', 'trained with', 'commonly used L2 loss function ( magenta solid line )']]","[['our CNN - 6/7 network', 'has', 'outperforms'], ['outperforms', 'has', 'all the other approaches']]",[],"[['Results', 'has', 'our CNN - 6/7 network']]",face_alignment,10,246
1424,experiments,"Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .","[('switching', (4, 5)), ('from', (8, 9)), ('of', (18, 19))]","[('loss function', (6, 8)), ('L2 to L1', (9, 12)), ('smooth L1', (13, 15)), ('performance', (17, 18)), ('our method', (19, 21)), ('improved', (23, 24)), ('significantly', (24, 25)), ('red solid and black dashed lines', (26, 32))]","[['loss function', 'from', 'L2 to L1'], ['performance', 'of', 'our method']]","[['improved', 'has', 'significantly'], ['significantly', 'has', 'red solid and black dashed lines']]",[],[],face_alignment,10,248
1425,results,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .","[('use of', (3, 5)), ('further improves', (11, 13))]","[('our newly proposed Wing loss function', (5, 11)), ('accuracy', (14, 15))]","[['our newly proposed Wing loss function', 'further improves', 'accuracy']]",[],[],[],face_alignment,10,249
1426,experimental-setup,The face images involved in 300W have been semi-automatically annotated by 68 facial landmarks .,"[('involved in', (3, 5))]","[('face images', (1, 3)), ('300W', (5, 6)), ('semi-automatically', (8, 9)), ('68 facial landmarks', (11, 14))]","[['face images', 'involved in', '300W']]",[],[],"[['Experimental setup', 'has', 'face images']]",face_alignment,10,253
1427,experimental-setup,The final size of the test set is 689 .,"[('of', (3, 4)), ('is', (7, 8))]","[('final size', (1, 3)), ('test set', (5, 7)), ('689', (8, 9))]","[['final size', 'of', 'test set'], ['final size', 'is', '689'], ['test set', 'is', '689']]",[],[],"[['Experimental setup', 'has', 'final size']]",face_alignment,10,257
1428,results,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .","[('with', (11, 12)), ('on', (32, 33))]","[('our two - stage landmark localisation framework', (4, 11)), ('PDB strategy', (13, 15)), ('Wing loss function', (19, 22)), ('outperforms', (22, 23)), ('all the other stateof - the - art algorithms', (23, 32)), ('300 W dataset inaccuracy', (34, 38))]","[['our two - stage landmark localisation framework', 'with', 'PDB strategy'], ['all the other stateof - the - art algorithms', 'on', '300 W dataset inaccuracy']]","[['our two - stage landmark localisation framework', 'has', 'PDB strategy'], ['Wing loss function', 'has', 'outperforms'], ['outperforms', 'has', 'all the other stateof - the - art algorithms']]",[],"[['Results', 'has', 'our two - stage landmark localisation framework']]",face_alignment,10,262
1429,results,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,"[('reduced by', (4, 6)), ('compared to', (10, 12)), ('reported by', (16, 18))]","[('error', (1, 2)), ('almost 20 %', (6, 9)), ('current best result', (13, 16)), ('RAR algorithm', (19, 21))]","[['error', 'reduced by', 'almost 20 %'], ['almost 20 %', 'compared to', 'current best result'], ['current best result', 'reported by', 'RAR algorithm']]",[],[],"[['Results', 'has', 'error']]",face_alignment,10,263
1430,experiments,Run time and network architectures,[],[],[],[],[],[],face_alignment,10,264
1431,experiments,"Facial landmark localisation has been widely used in many real - time practical applications , hence the speed together with accuracy of an algorithm is crucial for the deployment of the algorithm in commercial use cases .",[],"[('Facial landmark localisation', (0, 3))]",[],[],[],[],face_alignment,10,265
1432,experimental-setup,The input for ResNet is a 224 224 3 colour image .,"[('input for', (1, 3)), ('is', (4, 5))]","[('ResNet', (3, 4)), ('224 224 3 colour image', (6, 11))]","[['ResNet', 'is', '224 224 3 colour image']]",[],"[['Experimental setup', 'input for', 'ResNet']]",[],face_alignment,10,269
1433,results,"For both AFLW and 300 W , by replacing the CNN - 6/7 network with ResNet - 50 , the performance has been further improved by around 10 % , as shown in .","[('For', (0, 1)), ('replacing', (8, 9)), ('with', (14, 15)), ('by', (25, 26))]","[('AFLW and 300 W', (2, 6)), ('CNN - 6/7 network', (10, 14)), ('ResNet - 50', (15, 18)), ('performance', (20, 21)), ('further improved', (23, 25)), ('around 10 %', (26, 29))]","[['AFLW and 300 W', 'replacing', 'CNN - 6/7 network'], ['CNN - 6/7 network', 'with', 'ResNet - 50'], ['further improved', 'by', 'around 10 %']]","[['ResNet - 50', 'has', 'performance'], ['performance', 'has', 'further improved']]","[['Results', 'For', 'AFLW and 300 W']]",[],face_alignment,10,271
1434,experiments,The speed of TR - DRN is 83 fps on an NVIDIA GeForce GTX Titan X card .,"[('of', (2, 3)), ('is', (6, 7)), ('on', (9, 10))]","[('speed', (1, 2)), ('TR - DRN', (3, 6)), ('83 fps', (7, 9)), ('NVIDIA GeForce GTX Titan X card', (11, 17))]","[['speed', 'of', 'TR - DRN'], ['TR - DRN', 'is', '83 fps'], ['83 fps', 'on', 'NVIDIA GeForce GTX Titan X card']]",[],[],[],face_alignment,10,280
1435,results,"It should be noted that our CNN - 6 / 7 still outperforms the state - of - the - art approaches by a significant margin while running at 170 fps on a GPU card , as shown in .","[('noted', (3, 4)), ('by', (22, 23)), ('on', (31, 32))]","[('our CNN - 6 / 7', (5, 11)), ('outperforms', (12, 13)), ('state - of - the - art approaches', (14, 22)), ('significant margin', (24, 26)), ('running', (27, 28)), ('170 fps', (29, 31)), ('GPU card', (33, 35))]","[['state - of - the - art approaches', 'by', 'significant margin'], ['170 fps', 'on', 'GPU card']]","[['our CNN - 6 / 7', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art approaches'], ['running', 'has', '170 fps']]",[],[],face_alignment,10,282
1436,research-problem,Unsupervised Training for 3D Morphable Model Regression,[],[],[],[],[],[],face_alignment,11,2
1437,research-problem,"Finding the coordinates of a person in this space from a single image of that person is a common task for applications such as 3D avatar creation , facial animation transfer , and video editing ( e.g. ) .",[],"[('Finding the coordinates', (0, 3))]",[],[],[],[],face_alignment,11,10
1438,model,We map features from a facial recognition network into identity parameters for the Basel 2017 Morphable Face Model .,"[('map', (1, 2)), ('from', (3, 4)), ('into', (8, 9))]","[('facial recognition network', (5, 8)), ('identity parameters', (9, 11)), ('Basel 2017 Morphable Face Model', (13, 18))]","[['facial recognition network', 'into', 'identity parameters']]",[],"[['Model', 'map', 'facial recognition network']]",[],face_alignment,11,18
1439,model,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,"[('removes', (11, 12)), ('reliance on', (21, 23)), ('to reproduce', (25, 27))]","[('training', (6, 7)), ('regression network', (8, 10)), ('supervised training data', (16, 19)), ('inverse rendering', (23, 25)), ('image pixels', (27, 29))]","[['regression network', 'reliance on', 'inverse rendering'], ['inverse rendering', 'to reproduce', 'image pixels']]","[['training', 'has', 'regression network']]",[],[],face_alignment,11,20
1440,model,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .","[('learns to', (4, 6)), ('based on', (9, 11)), ('produced by', (15, 17)), ('such as', (21, 23))]","[('minimize', (6, 7)), ('loss', (8, 9)), ('facial identity features', (12, 15)), ('face recognition network', (18, 21)), ('VGG - Face', (23, 26)), (""Google 's FaceNet"", (27, 30))]","[['loss', 'based on', 'facial identity features'], ['facial identity features', 'produced by', 'face recognition network'], ['face recognition network', 'such as', 'VGG - Face'], ['face recognition network', 'such as', ""Google 's FaceNet""]]","[['minimize', 'has', 'loss']]","[['Model', 'learns to', 'minimize']]",[],face_alignment,11,21
1441,model,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,"[('apply', (5, 6)), ('that matches', (8, 10)), ('between', (13, 14)), ('of', (21, 22))]","[('loss', (7, 8)), ('identity features', (11, 13)), ('input photograph', (15, 17)), ('synthetic rendering', (19, 21)), ('predicted face', (23, 25))]","[['loss', 'that matches', 'identity features'], ['identity features', 'between', 'input photograph'], ['identity features', 'between', 'synthetic rendering'], ['synthetic rendering', 'of', 'predicted face']]",[],"[['Model', 'apply', 'loss']]",[],face_alignment,11,23
1442,model,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .","[('alleviate', (1, 2)), ('by', (5, 6)), ('applying', (6, 7)), ('to match', (15, 17)), ('of', (19, 20)), ('to', (23, 24)), ('of', (26, 27)), ('to ensure', (34, 36)), ('can correctly reinterpret', (39, 42)), ('that combines', (51, 53)), ('from', (54, 55)), ('of', (59, 60))]","[('fooling problem', (3, 5)), ('three novel losses', (7, 10)), ('batch distribution loss', (12, 15)), ('statistics', (18, 19)), ('each training batch', (20, 23)), ('statistics', (25, 26)), ('morphable model', (28, 30)), ('loopback loss', (32, 34)), ('regression network', (37, 39)), ('own output', (43, 45)), ('multi-view identity loss', (48, 51)), ('features', (53, 54)), ('multiple , independent views', (55, 59)), ('predicted shape', (61, 63))]","[['fooling problem', 'applying', 'three novel losses'], ['batch distribution loss', 'to match', 'statistics'], ['statistics', 'of', 'each training batch'], ['each training batch', 'to', 'statistics'], ['statistics', 'of', 'morphable model'], ['multiple , independent views', 'of', 'predicted shape'], ['loopback loss', 'to ensure', 'regression network'], ['regression network', 'can correctly reinterpret', 'own output'], ['multi-view identity loss', 'that combines', 'features'], ['features', 'from', 'multiple , independent views'], ['multiple , independent views', 'of', 'predicted shape']]","[['three novel losses', 'name', 'batch distribution loss']]","[['Model', 'alleviate', 'fooling problem']]",[],face_alignment,11,26
1443,model,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .","[('train', (5, 6)), ('using', (13, 14))]","[('3D shape and texture regression network', (7, 13)), ('only a face recognition network', (14, 19)), ('morphable face model', (21, 24)), ('dataset of unlabeled face images', (27, 32))]","[['3D shape and texture regression network', 'using', 'only a face recognition network'], ['3D shape and texture regression network', 'using', 'morphable face model'], ['3D shape and texture regression network', 'using', 'dataset of unlabeled face images']]",[],"[['Model', 'train', '3D shape and texture regression network']]",[],face_alignment,11,27
1444,hyperparameters,We use the Phong reflection model for shading .,"[('use', (1, 2)), ('for', (6, 7))]","[('Phong reflection model', (3, 6)), ('shading', (7, 8))]","[['Phong reflection model', 'for', 'shading']]",[],"[['Hyperparameters', 'use', 'Phong reflection model']]",[],face_alignment,11,105
1445,results,Identity prediction can be further enhanced by using multiple poses for each face .,"[('by using', (6, 8)), ('for', (10, 11))]","[('Identity prediction', (0, 2)), ('further enhanced', (4, 6)), ('multiple poses', (8, 10)), ('each face', (11, 13))]","[['further enhanced', 'by using', 'multiple poses'], ['multiple poses', 'for', 'each face']]","[['Identity prediction', 'has', 'further enhanced']]",[],"[['Results', 'has', 'Identity prediction']]",face_alignment,11,138
1446,hyperparameters,Batch Distribution,[],[],[],[],[],[],face_alignment,11,142
1447,results,"Our method shows improved likeness and color fidelity over competing methods , especially in the shape of the eyes , eyebrows , and nose .","[('shows', (2, 3)), ('over', (8, 9)), ('especially in the shape of', (12, 17))]","[('improved likeness and color fidelity', (3, 8)), ('competing methods', (9, 11)), ('eyes', (18, 19))]","[['improved likeness and color fidelity', 'over', 'competing methods'], ['competing methods', 'especially in the shape of', 'eyes']]",[],"[['Results', 'shows', 'improved likeness and color fidelity']]",[],face_alignment,11,171
1448,results,Neutral Pose Reconstruction on MICC,[],[],[],[],[],[],face_alignment,11,184
1449,results,We quantitatively evaluate the ground - truth accuracy of our models on the MICC Florence 3D Faces dataset ( MICC ) in .,"[('on', (11, 12))]","[('MICC Florence 3D Faces dataset ( MICC )', (13, 21))]",[],[],"[['Results', 'on', 'MICC Florence 3D Faces dataset ( MICC )']]",[],face_alignment,11,185
1450,experiments,We instead average our encoder embeddings before making a single reconstruction .,"[('average', (2, 3)), ('before making', (6, 8))]","[('single reconstruction', (9, 11))]",[],[],[],[],face_alignment,11,190
1451,results,"Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .","[('indicate', (2, 3)), ('improved', (6, 7)), ('to', (9, 10)), ('by', (13, 14)), ('with less', (30, 32))]","[('absolute error', (7, 9)), ('ground truth', (11, 13)), ('20 - 25 %', (14, 18)), ('more consistent', (23, 25))]","[['absolute error', 'to', 'ground truth'], ['ground truth', 'by', '20 - 25 %']]",[],"[['Results', 'indicate', 'absolute error']]",[],face_alignment,11,194
1452,results,"We are also more stable across changing environments , with similar results for all three test sets .","[('across', (5, 6))]","[('more stable', (3, 5)), ('changing environments', (6, 8))]","[['more stable', 'across', 'changing environments']]",[],[],[],face_alignment,11,195
1453,results,Face Recognition Results,[],[],[],[],[],[],face_alignment,11,196
1454,results,Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .,"[('achieves', (2, 3)), ('between', (6, 7)), ('of', (10, 11)), ('on', (12, 13))]","[('Our method', (0, 2)), ('average similarity', (4, 6)), ('rendering and photo', (7, 10)), ('0.403', (11, 12)), ('MoFA test', (13, 15))]","[['Our method', 'achieves', 'average similarity'], ['average similarity', 'between', 'rendering and photo'], ['rendering and photo', 'of', '0.403'], ['0.403', 'on', 'MoFA test']]",[],[],"[['Results', 'has', 'Our method']]",face_alignment,11,202
1455,results,"Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .","[('closer to', (5, 7)), ('than', (12, 13)), ('closer to', (27, 29))]","[(""Our method 's results"", (0, 4)), ('same - person distribution', (8, 12)), ('differentperson distribution', (14, 16)), ('all cases', (17, 19)), ('other methods results', (22, 25)), ('different - person distribution', (30, 34))]","[[""Our method 's results"", 'closer to', 'same - person distribution'], ['same - person distribution', 'than', 'differentperson distribution'], ['other methods results', 'closer to', 'different - person distribution']]",[],[],"[['Results', 'has', ""Our method 's results""]]",face_alignment,11,205
1456,results,"Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .","[('between', (4, 5)), ('is', (15, 16)), ('with', (19, 20))]","[('distance', (3, 4)), ('GT distribution and the same - person LFW distribution', (6, 15)), ('very low', (16, 18)), ('almost the same mean ( 0.51 vs 0.50 )', (20, 29))]","[['distance', 'between', 'GT distribution and the same - person LFW distribution'], ['GT distribution and the same - person LFW distribution', 'is', 'very low'], ['very low', 'with', 'almost the same mean ( 0.51 vs 0.50 )']]",[],[],[],face_alignment,11,211
1457,research-problem,Facial features smoothly degrade as the necessary information is no longer present in the input image .,"[('as', (4, 5))]","[('Facial features', (0, 2)), ('smoothly degrade', (2, 4)), ('necessary information', (6, 8)), ('no longer present', (9, 12)), ('input image', (14, 16))]","[['smoothly degrade', 'as', 'necessary information']]","[['Facial features', 'has', 'smoothly degrade']]",[],[],face_alignment,11,251
1458,research-problem,Face alignment is a classic problem in the computer vision field .,[],"[('Face alignment', (0, 2))]",[],[],[],[],face_alignment,12,4
1459,research-problem,"In this paper , for the first time , we aim at providing a very dense 3D alignment for largepose face images .","[('for', (18, 19))]","[('very dense 3D alignment', (14, 18)), ('largepose face images', (19, 22))]","[['very dense 3D alignment', 'for', 'largepose face images']]",[],[],[],face_alignment,12,6
1460,approach,"Moreover , DeFA should offer dense correspondence not only between two face images , but also between the face image and the canonical 3 D face model .","[('offer', (4, 5)), ('between', (9, 10)), ('between', (16, 17))]","[('DeFA', (2, 3)), ('dense correspondence', (5, 7)), ('two face images', (10, 13)), ('face image and', (18, 21)), ('canonical 3 D face model', (22, 27))]","[['DeFA', 'offer', 'dense correspondence'], ['dense correspondence', 'between', 'two face images'], ['dense correspondence', 'between', 'face image and'], ['dense correspondence', 'between', 'canonical 3 D face model']]","[['DeFA', 'has', 'dense correspondence']]",[],"[['Approach', 'has', 'DeFA']]",face_alignment,12,21
1461,approach,"In this work , we choose to develop the idea of fitting a dense 3 D face model to an image , where the model with thousands of vertexes makes it possible for face alignment to go very "" dense "" .","[('develop', (7, 8)), ('of', (10, 11)), ('to', (18, 19)), ('where', (22, 23)), ('makes', (29, 30)), ('to go', (35, 37))]","[('idea', (9, 10)), ('fitting', (11, 12)), ('image', (20, 21)), ('model with thousands of vertexes', (24, 29)), ('face alignment', (33, 35)), ('very "" dense ""', (37, 41))]","[['idea', 'of', 'fitting'], ['image', 'where', 'model with thousands of vertexes'], ['face alignment', 'to go', 'very "" dense ""']]","[['idea', 'has', 'fitting'], ['fitting', 'has', 'image']]","[['Approach', 'develop', 'idea']]",[],face_alignment,12,24
1462,approach,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .","[('learn', (9, 10)), ('to fit', (12, 14)), ('to', (19, 20))]","[('CNN', (11, 12)), ('3 D face model', (15, 19)), ('face image', (21, 23))]","[['CNN', 'to fit', '3 D face model'], ['3 D face model', 'to', 'face image']]",[],"[['Approach', 'learn', 'CNN']]",[],face_alignment,12,38
1463,approach,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .","[('of', (4, 5)), ('propose', (10, 11)), ('employ', (12, 13))]","[('first challenge', (2, 4)), ('limited landmark labeling', (5, 8)), ('additional constraints', (13, 15))]","[['first challenge', 'of', 'limited landmark labeling'], ['limited landmark labeling', 'employ', 'additional constraints']]",[],[],[],face_alignment,12,41
1464,approach,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .","[('include', (1, 2)), ('where', (4, 5)), ('of', (7, 8)), ('detected on', (28, 30)), ('of', (33, 34)), ('map to', (38, 40)), ('on', (43, 44))]","[('contour constraint', (2, 4)), ('contour', (6, 7)), ('predicted shape', (9, 11)), ('detected', (14, 15)), ('2 D face boundary', (15, 19)), ('SIFT constraint', (21, 23)), ('SIFT key points', (25, 28)), ('two face images', (30, 33)), ('same individual', (35, 37)), ('same vertexes', (41, 43)), ('3D face model', (45, 48))]","[['contour constraint', 'where', 'contour'], ['contour constraint', 'where', 'SIFT constraint'], ['SIFT constraint', 'where', 'SIFT key points'], ['contour', 'of', 'predicted shape'], ['two face images', 'of', 'same individual'], ['SIFT key points', 'detected on', 'two face images'], ['two face images', 'of', 'same individual'], ['SIFT key points', 'map to', 'same vertexes'], ['same vertexes', 'on', '3D face model']]","[['contour constraint', 'has', 'contour'], ['detected', 'has', '2 D face boundary']]","[['Approach', 'include', 'contour constraint']]",[],face_alignment,12,42
1465,approach,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .","[('integrated into', (3, 5)), ('as', (8, 9)), ('where', (14, 15)), ('results in', (22, 24)), ('for', (27, 28))]","[('Both constraints', (0, 2)), ('CNN training', (6, 8)), ('additional loss function terms', (9, 13)), ('end - to - end training', (16, 22)), ('enhanced CNN', (25, 27)), ('3 D face model fitting', (28, 33))]","[['Both constraints', 'integrated into', 'CNN training'], ['CNN training', 'as', 'additional loss function terms'], ['additional loss function terms', 'where', 'end - to - end training'], ['end - to - end training', 'results in', 'enhanced CNN'], ['enhanced CNN', 'for', '3 D face model fitting']]",[],[],"[['Approach', 'has', 'Both constraints']]",face_alignment,12,43
1466,approach,"Generally , our main contributions can be summarized as : 1 . We identify and define anew problem of dense face alignment , which seeks alignment of face - region pixels beyond the sparse set of landmarks .","[('identify and define', (13, 16)), ('of', (18, 19)), ('which seeks', (23, 25)), ('of', (26, 27)), ('beyond', (31, 32))]","[('dense face alignment', (19, 22)), ('alignment', (25, 26)), ('face - region pixels', (27, 31)), ('sparse set of landmarks', (33, 37))]","[['alignment', 'of', 'face - region pixels'], ['dense face alignment', 'which seeks', 'alignment'], ['alignment', 'of', 'face - region pixels'], ['face - region pixels', 'beyond', 'sparse set of landmarks']]",[],"[['Approach', 'identify and define', 'dense face alignment']]",[],face_alignment,12,46
1467,approach,"To achieve dense face alignment , we develop a novel 3 D face model fitting algorithm that adopts multiple constraints and leverages multiple datasets .","[('To achieve', (0, 2)), ('develop', (7, 8)), ('adopts', (17, 18)), ('leverages', (21, 22))]","[('dense face alignment', (2, 5)), ('novel 3 D face model fitting algorithm', (9, 16)), ('multiple constraints', (18, 20)), ('multiple datasets', (22, 24))]","[['dense face alignment', 'develop', 'novel 3 D face model fitting algorithm'], ['novel 3 D face model fitting algorithm', 'adopts', 'multiple constraints'], ['novel 3 D face model fitting algorithm', 'leverages', 'multiple datasets']]",[],"[['Approach', 'To achieve', 'dense face alignment']]",[],face_alignment,12,48
1468,hyperparameters,"At stage 1 , we use 300W - LP to train our DeFA network with parameter constraint ( PL ) .","[('use', (5, 6)), ('to train', (9, 11)), ('with', (14, 15))]","[('300W - LP', (6, 9)), ('our DeFA network', (11, 14)), ('parameter constraint ( PL )', (15, 20))]","[['300W - LP', 'to train', 'our DeFA network'], ['our DeFA network', 'with', 'parameter constraint ( PL )']]",[],"[['Hyperparameters', 'use', '300W - LP']]",[],face_alignment,12,196
1469,hyperparameters,"At stage 2 , we additionally include samples from the Caltech10K , and COFW to continue the training of our network with the additional landmark fitting constraint ( LFC ) .","[('include', (6, 7)), ('from', (8, 9)), ('to continue', (14, 16)), ('of', (18, 19)), ('with', (21, 22))]","[('samples', (7, 8)), ('training', (17, 18)), ('our network', (19, 21)), ('additional landmark fitting constraint ( LFC )', (23, 30))]","[['training', 'of', 'our network'], ['our network', 'with', 'additional landmark fitting constraint ( LFC )']]",[],"[['Hyperparameters', 'include', 'samples']]",[],face_alignment,12,197
1470,baselines,"At stage 3 , we fine - tune the model with SPC and CFC constraints .","[('fine - tune', (5, 8)), ('with', (10, 11))]","[('model', (9, 10)), ('SPC and CFC constraints', (11, 15))]","[['model', 'with', 'SPC and CFC constraints']]",[],[],[],face_alignment,12,198
1471,experiments,"For large - pose face alignment , we fine - tune the model with AFLW - LFPA training set .","[('For', (0, 1)), ('fine - tune', (8, 11)), ('with', (13, 14))]","[('large - pose face alignment', (1, 6)), ('model', (12, 13)), ('AFLW - LFPA training set', (14, 19))]","[['large - pose face alignment', 'fine - tune', 'model'], ['model', 'with', 'AFLW - LFPA training set']]",[],[],[],face_alignment,12,199
1472,hyperparameters,"For near - frontal face alignment , we fine - tune the model with 300 W training set .","[('For', (0, 1)), ('fine - tune', (8, 11)), ('with', (13, 14))]","[('near - frontal face alignment', (1, 6)), ('model', (12, 13)), ('300 W training set', (14, 18))]","[['near - frontal face alignment', 'fine - tune', 'model'], ['model', 'with', '300 W training set']]",[],"[['Hyperparameters', 'For', 'near - frontal face alignment']]",[],face_alignment,12,200
1473,hyperparameters,"All samples at the third stage are augmented 20 times with up to 20 random in - plain rotation and 15 % random noise on the center , width , and length of the initial bounding box .","[('at', (2, 3)), ('augmented', (7, 8)), ('with', (10, 11)), ('on', (24, 25))]","[('samples', (1, 2)), ('third stage', (4, 6)), ('20 times', (8, 10)), ('up to 20 random in - plain rotation', (11, 19)), ('15 % random noise', (20, 24)), ('center , width , and length of', (26, 33))]","[['samples', 'at', 'third stage'], ['samples', 'augmented', '20 times'], ['third stage', 'augmented', '20 times'], ['third stage', 'augmented', '15 % random noise'], ['20 times', 'with', 'up to 20 random in - plain rotation'], ['20 times', 'with', '15 % random noise'], ['15 % random noise', 'on', 'center , width , and length of']]",[],[],"[['Hyperparameters', 'has', 'samples']]",face_alignment,12,201
1474,experiments,"To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .","[('use', (6, 7)), ('for', (14, 15))]","[('network', (3, 4)), ('20 , 10 , and 10 epochs', (7, 14)), ('stage 1 to 3', (15, 19))]","[['network', 'use', '20 , 10 , and 10 epochs'], ['20 , 10 , and 10 epochs', 'for', 'stage 1 to 3']]",[],[],[],face_alignment,12,203
1475,hyperparameters,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .","[('set', (1, 2)), ('as', (7, 8)), ('reduce', (14, 15)), ('by', (18, 19)), ('when', (23, 24)), ('approaches', (27, 28))]","[('initial global learning rate', (3, 7)), ('1 e ? 3', (8, 12)), ('learning rate', (16, 18)), ('factor of 10', (20, 23)), ('training error', (25, 27)), ('plateau', (29, 30))]","[['initial global learning rate', 'as', '1 e ? 3'], ['learning rate', 'by', 'factor of 10'], ['factor of 10', 'when', 'training error'], ['training error', 'approaches', 'plateau']]","[['initial global learning rate', 'has', '1 e ? 3']]","[['Hyperparameters', 'set', 'initial global learning rate']]",[],face_alignment,12,204
1476,experiments,"The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .","[('is', (3, 4)), ('is', (8, 9)), ('for', (15, 16)), ('is', (18, 19))]","[('minibatch size', (1, 3)), ('32', (4, 5)), ('weight decay', (6, 8)), ('0.005', (9, 10)), ('leak factor', (13, 15)), ('Leaky ReLU', (16, 18)), ('0.01', (19, 20))]","[['minibatch size', 'is', '32'], ['Leaky ReLU', 'is', '0.01'], ['weight decay', 'is', '0.005'], ['leak factor', 'for', 'Leaky ReLU'], ['Leaky ReLU', 'is', '0.01']]","[['minibatch size', 'has', '32'], ['weight decay', 'has', '0.005']]",[],[],face_alignment,12,205
1477,hyperparameters,"In stage 2 , the regularization weights ?",[],"[('regularization weights', (5, 7))]",[],[],[],"[['Hyperparameters', 'has', 'regularization weights']]",face_alignment,12,206
1478,hyperparameters,"lm for LFC is 5 ; In stage 3 , the regularization weights ? lm , ? s , ?","[('for', (1, 2)), ('is', (3, 4))]","[('lm', (0, 1)), ('LFC', (2, 3)), ('5', (4, 5))]","[['lm', 'for', 'LFC'], ['LFC', 'is', '5']]",[],[],"[['Hyperparameters', 'has', 'lm']]",face_alignment,12,208
1479,hyperparameters,"c for LFC , SPC and CFC are set as 5 , 1 and 1 , respectively .","[('set as', (8, 10))]","[('LFC', (2, 3)), ('SPC and CFC', (4, 7)), ('5 , 1 and 1', (10, 15))]","[['SPC and CFC', 'set as', '5 , 1 and 1']]","[['LFC', 'has', 'SPC and CFC']]",[],"[['Hyperparameters', 'has', 'LFC']]",face_alignment,12,209
1480,experiments,"For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .","[('For', (0, 1)), ('with', (11, 12)), ('of', (15, 16))]","[('AFLW - LFPA', (1, 4)), ('our method', (5, 7)), ('outperforms', (7, 8)), ('best methods', (9, 11)), ('large margin', (13, 15)), ('17.8 % improvement', (16, 19))]","[['outperforms', 'with', 'large margin'], ['best methods', 'with', 'large margin'], ['large margin', 'of', '17.8 % improvement']]","[['AFLW - LFPA', 'has', 'our method'], ['our method', 'has', 'outperforms'], ['outperforms', 'has', 'best methods']]",[],[],face_alignment,12,218
1481,experiments,"For AFLW2000 - 3D , our method also shows a large improvement .","[('For', (0, 1)), ('shows', (8, 9))]","[('AFLW2000 - 3D', (1, 4)), ('our method', (5, 7)), ('large improvement', (10, 12))]","[['our method', 'shows', 'large improvement']]","[['AFLW2000 - 3D', 'has', 'our method'], ['our method', 'has', 'large improvement']]",[],[],face_alignment,12,219
1482,experiments,"Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .","[('for', (2, 3)), ('with', (4, 5)), ('in', (7, 8)), ('improves', (16, 17)), ('by', (19, 20))]","[('images', (3, 4)), ('yaw angle', (5, 7)), ('[ 60 , 90 ]', (8, 13)), ('our method', (14, 16)), ('performance', (18, 19)), ('28 % ( from 7.93 to 5.68 )', (20, 28))]","[['images', 'with', 'yaw angle'], ['yaw angle', 'in', '[ 60 , 90 ]'], ['yaw angle', 'in', 'our method'], ['our method', 'improves', 'performance'], ['performance', 'by', '28 % ( from 7.93 to 5.68 )']]",[],[],[],face_alignment,12,220
1483,experiments,"For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .","[('For', (0, 1))]","[('IJB - A dataset', (2, 6))]",[],[],[],[],face_alignment,12,221
1484,experiments,The consistently superior performance of our DeFA indicates that we have advanced the state of the art in large - pose face alignment .,"[('of', (4, 5)), ('indicates', (7, 8)), ('in', (17, 18))]","[('consistently superior performance', (1, 4)), ('our DeFA', (5, 7)), ('advanced', (11, 12)), ('state of the art', (13, 17)), ('large - pose face alignment', (18, 23))]","[['consistently superior performance', 'of', 'our DeFA'], ['consistently superior performance', 'indicates', 'advanced'], ['our DeFA', 'indicates', 'advanced'], ['state of the art', 'in', 'large - pose face alignment']]","[['advanced', 'has', 'state of the art']]",[],[],face_alignment,12,223
1485,results,"Even though the proposed method can handle largepose alignment , to show its performance on the near- frontal datasets , we evaluate our method on the 300W dataset .","[('evaluate', (21, 22)), ('on', (24, 25))]","[('300W dataset', (26, 28))]",[],[],[],[],face_alignment,12,225
1486,baselines,"4 . To find the corresponding landmarks on the cheek , we apply the landmark marching algorithm to move contour landmarks from self - occluded location to the silhouette .","[('To find', (2, 4)), ('on', (7, 8)), ('apply', (12, 13)), ('to move', (17, 19)), ('from', (21, 22)), ('to', (26, 27))]","[('corresponding landmarks', (5, 7)), ('cheek', (9, 10)), ('landmark marching algorithm', (14, 17)), ('contour landmarks', (19, 21)), ('self - occluded location', (22, 26)), ('silhouette', (28, 29))]","[['corresponding landmarks', 'on', 'cheek'], ['corresponding landmarks', 'apply', 'landmark marching algorithm'], ['landmark marching algorithm', 'to move', 'contour landmarks'], ['contour landmarks', 'from', 'self - occluded location'], ['self - occluded location', 'to', 'silhouette']]",[],"[['Baselines', 'To find', 'corresponding landmarks']]",[],face_alignment,12,227
1487,results,Our method is the second best method on the challenging set .,"[('is', (2, 3)), ('on', (7, 8))]","[('Our method', (0, 2)), ('second best method', (4, 7)), ('challenging set', (9, 11))]","[['Our method', 'is', 'second best method'], ['second best method', 'on', 'challenging set']]","[['Our method', 'has', 'second best method']]",[],"[['Results', 'has', 'Our method']]",face_alignment,12,228
1488,results,"In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .","[('of', (5, 6)), ('comparable to', (9, 11)), ('that', (13, 14))]","[('performance', (4, 5)), ('our method', (6, 8)), ('other methods', (11, 13)), ('near - frontal datasets', (17, 21))]","[['performance', 'of', 'our method'], ['performance', 'comparable to', 'other methods'], ['our method', 'comparable to', 'other methods']]","[['performance', 'has', 'our method']]",[],"[['Results', 'has', 'performance']]",face_alignment,12,229
1489,results,"It is a strong testimony of our model in that DeFA , without further finetuning , outperforms both 3DDFA and its fine tuned version with SDM .","[('of', (5, 6)), ('without', (12, 13)), ('with', (24, 25))]","[('strong testimony', (3, 5)), ('DeFA', (10, 11)), ('outperforms', (16, 17)), ('3DDFA', (18, 19)), ('its fine tuned version', (20, 24)), ('SDM', (25, 26))]","[['its fine tuned version', 'with', 'SDM']]","[['strong testimony', 'has', 'DeFA'], ['DeFA', 'has', 'outperforms'], ['outperforms', 'has', '3DDFA']]",[],[],face_alignment,12,232
1490,ablation-analysis,The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,"[('of', (2, 3)), ('on', (5, 6)), ('by adding', (12, 14))]","[('accuracy', (1, 2)), ('our method', (3, 5)), ('AFLW2000 - 3D', (7, 10)), ('consistently improves', (10, 12)), ('more datasets', (14, 16))]","[['accuracy', 'of', 'our method'], ['accuracy', 'on', 'AFLW2000 - 3D'], ['our method', 'on', 'AFLW2000 - 3D'], ['consistently improves', 'by adding', 'more datasets']]",[],[],"[['Ablation analysis', 'has', 'accuracy']]",face_alignment,12,238
1491,ablation-analysis,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .","[('For', (0, 1)), ('achieves', (9, 10)), ('by utilizing', (17, 19)), ('in', (21, 22)), ('over', (28, 29))]","[('AFLW - PIFA dataset', (2, 6)), ('our method', (7, 9)), ('9.5 % and 20 % relative improvement', (10, 17)), ('datasets', (20, 21)), ('stage 2 and stage 3', (23, 28)), ('first stage', (30, 32))]","[['our method', 'achieves', '9.5 % and 20 % relative improvement'], ['9.5 % and 20 % relative improvement', 'by utilizing', 'datasets'], ['datasets', 'in', 'stage 2 and stage 3'], ['stage 2 and stage 3', 'over', 'first stage']]","[['AFLW - PIFA dataset', 'has', 'our method']]","[['Ablation analysis', 'For', 'AFLW - PIFA dataset']]",[],face_alignment,12,239
1492,ablation-analysis,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .","[('including', (1, 2)), ('from', (4, 5)), ('can have', (13, 15)), ('achieve', (20, 21)), ('of', (22, 23))]","[('datasets', (3, 4)), ('both the second and third stages', (5, 11)), ('26 % relative improvement', (15, 19)), ('NME', (21, 22)), ('3.86 %', (23, 25))]","[['datasets', 'from', 'both the second and third stages'], ['both the second and third stages', 'can have', '26 % relative improvement'], ['26 % relative improvement', 'achieve', 'NME'], ['NME', 'of', '3.86 %']]","[['datasets', 'has', 'both the second and third stages']]","[['Ablation analysis', 'including', 'datasets']]",[],face_alignment,12,240
1493,ablation-analysis,5 shows that the effectiveness of CFC and SPC is more than LFC .,"[('shows', (1, 2)), ('of', (5, 6)), ('more than', (10, 12))]","[('effectiveness', (4, 5)), ('CFC and SPC', (6, 9)), ('LFC', (12, 13))]","[['effectiveness', 'of', 'CFC and SPC'], ['CFC and SPC', 'more than', 'LFC']]",[],"[['Ablation analysis', 'shows', 'effectiveness']]",[],face_alignment,12,242
1494,ablation-analysis,Comparing LFC + SPC and LFC + CFC performances shows that the CFC is more helpful than the SPC .,"[('Comparing', (0, 1)), ('shows', (9, 10)), ('than', (16, 17))]","[('LFC + SPC and LFC + CFC performances', (1, 9)), ('CFC', (12, 13)), ('more helpful', (14, 16)), ('SPC', (18, 19))]","[['LFC + SPC and LFC + CFC performances', 'shows', 'CFC'], ['more helpful', 'than', 'SPC']]","[['LFC + SPC and LFC + CFC performances', 'has', 'CFC'], ['CFC', 'has', 'more helpful']]","[['Ablation analysis', 'Comparing', 'LFC + SPC and LFC + CFC performances']]",[],face_alignment,12,248
1495,ablation-analysis,Using all constraints achieves the best performance .,"[('Using', (0, 1)), ('achieves', (3, 4))]","[('all constraints', (1, 3)), ('best performance', (5, 7))]","[['all constraints', 'achieves', 'best performance']]","[['all constraints', 'has', 'best performance']]","[['Ablation analysis', 'Using', 'all constraints']]",[],face_alignment,12,250
1496,ablation-analysis,This result shows that for the images with NME - lp between 5 % and 15 % the SPC is helpful .,"[('shows', (2, 3)), ('for', (4, 5)), ('with', (7, 8)), ('between', (11, 12)), ('is', (19, 20))]","[('images', (6, 7)), ('NME - lp', (8, 11)), ('5 % and 15 %', (12, 17)), ('SPC', (18, 19)), ('helpful', (20, 21))]","[['images', 'with', 'NME - lp'], ['NME - lp', 'between', '5 % and 15 %'], ['5 % and 15 %', 'is', 'helpful'], ['SPC', 'is', 'helpful']]","[['5 % and 15 %', 'has', 'SPC'], ['SPC', 'has', 'helpful']]","[['Ablation analysis', 'shows', 'images']]",[],face_alignment,12,254
1497,ablation-analysis,"As shown in , SPC utilizes SIFT points to cover the whole 3D shape and the points in the highly textured areas are substantially used .","[('utilizes', (5, 6)), ('to cover', (8, 10)), ('in', (17, 18)), ('are', (22, 23))]","[('SPC', (4, 5)), ('SIFT points', (6, 8)), ('whole 3D shape', (11, 14)), ('points', (16, 17)), ('highly textured areas', (19, 22)), ('substantially used', (23, 25))]","[['SPC', 'utilizes', 'SIFT points'], ['SIFT points', 'to cover', 'whole 3D shape'], ['points', 'in', 'highly textured areas'], ['highly textured areas', 'are', 'substantially used']]","[['SPC', 'has', 'SIFT points']]",[],"[['Ablation analysis', 'has', 'SPC']]",face_alignment,12,262
1498,research-problem,Nonlinear 3D Face Morphable Model,[],[],[],[],[],[],face_alignment,13,2
1499,research-problem,"As a classic statistical model of 3D facial shape and texture , 3D Morphable Model ( 3 DMM ) is widely used in facial analysis , e.g. , model fitting , image synthesis .","[('of', (5, 6)), ('widely used in', (20, 23)), ('e.g.', (26, 27))]","[('3D facial', (6, 8)), ('3D Morphable Model ( 3 DMM )', (12, 19)), ('facial analysis', (23, 25)), ('model fitting', (28, 30)), ('image synthesis', (31, 33))]","[['3D Morphable Model ( 3 DMM )', 'widely used in', 'facial analysis'], ['3D Morphable Model ( 3 DMM )', 'e.g.', 'model fitting'], ['facial analysis', 'e.g.', 'model fitting'], ['facial analysis', 'e.g.', 'image synthesis']]",[],"[['Research problem', 'of', '3D facial']]",[],face_alignment,13,4
1500,model,The entire network is end - to - end trainable with only weak supervision .,"[('is', (3, 4)), ('with', (10, 11))]","[('entire network', (1, 3)), ('end - to - end trainable', (4, 10)), ('only weak supervision', (11, 14))]","[['entire network', 'is', 'end - to - end trainable'], ['end - to - end trainable', 'with', 'only weak supervision']]","[['entire network', 'has', 'end - to - end trainable']]",[],"[['Model', 'has', 'entire network']]",face_alignment,13,11
1501,model,"The morphable model framework provides two key benefits : first , a point - to - point correspondence between the reconstruction and all other models , enabling morphing , and second , modeling underlying transformations between types of faces ( male to female , neutral to smile , etc . ) .","[('provides', (4, 5)), ('between', (18, 19)), ('enabling', (26, 27)), ('between', (35, 36))]","[('morphable model framework', (1, 4)), ('two', (5, 6)), ('point - to - point correspondence', (12, 18)), ('reconstruction and all other models', (20, 25)), ('modeling', (32, 33)), ('underlying transformations', (33, 35)), ('types of faces ( male to female , neutral to smile , etc . )', (36, 51))]","[['morphable model framework', 'provides', 'two'], ['point - to - point correspondence', 'between', 'reconstruction and all other models'], ['underlying transformations', 'between', 'types of faces ( male to female , neutral to smile , etc . )']]","[['two', 'has', 'point - to - point correspondence'], ['modeling', 'has', 'underlying transformations']]",[],"[['Model', 'has', 'morphable model framework']]",face_alignment,13,16
1502,research-problem,"3 DMM has been widely applied in numerous areas , such as computer vision , graphics , human behavioral analysis and craniofacial surgery .",[],"[('3 DMM', (0, 2))]",[],[],[],[],face_alignment,13,17
1503,model,We propose a nonlinear 3 DMM to model shape / texture via deep neural networks ( DNNs ) .,"[('propose', (1, 2)), ('to model', (6, 8)), ('via', (11, 12))]","[('nonlinear 3 DMM', (3, 6)), ('shape / texture', (8, 11)), ('deep neural networks ( DNNs )', (12, 18))]","[['nonlinear 3 DMM', 'to model', 'shape / texture'], ['shape / texture', 'via', 'deep neural networks ( DNNs )']]",[],"[['Model', 'propose', 'nonlinear 3 DMM']]",[],face_alignment,13,19
1504,model,"It can be trained from in - the - wild face images without 3 D scans , and also better reconstructs the original images due to the inherent nonlinearity .","[('trained from', (3, 5)), ('without', (12, 13)), ('better', (19, 20)), ('due to', (24, 26))]","[('in - the - wild face images', (5, 12)), ('3 D scans', (13, 16)), ('original images', (22, 24)), ('inherent nonlinearity', (27, 29))]","[['in - the - wild face images', 'without', '3 D scans'], ['original images', 'due to', 'inherent nonlinearity']]",[],[],[],face_alignment,13,20
1505,research-problem,"To model highly variable 3 D face shapes , a large amount of high - quality 3 D face scans is required .","[('To model', (0, 2))]","[('highly variable 3 D face shapes', (2, 8)), ('large amount of high - quality 3 D face scans', (10, 20))]",[],"[['highly variable 3 D face shapes', 'has', 'large amount of high - quality 3 D face scans']]","[['Research problem', 'To model', 'highly variable 3 D face shapes']]",[],face_alignment,13,22
1506,model,"Hence , it is fragile to large variances in the face identity .","[('is', (3, 4)), ('to', (5, 6)), ('in', (8, 9))]","[('fragile', (4, 5)), ('large variances', (6, 8)), ('face identity', (10, 12))]","[['fragile', 'to', 'large variances'], ['large variances', 'in', 'face identity']]","[['fragile', 'has', 'large variances']]",[],[],face_alignment,13,26
1507,model,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .","[('utilize', (3, 4)), ('instead of', (8, 10)), ('as', (14, 15))]","[('two network decoders', (4, 7)), ('two PCA spaces', (10, 13)), ('shape and texture model components', (16, 21))]","[['two network decoders', 'instead of', 'two PCA spaces'], ['two PCA spaces', 'as', 'shape and texture model components']]",[],"[['Model', 'utilize', 'two network decoders']]",[],face_alignment,13,45
1508,model,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .","[('of', (3, 4)), ('design', (8, 9)), ('for', (11, 12)), ('for', (24, 25)), ('for', (33, 34))]","[('each component', (4, 6)), ('different networks', (9, 11)), ('shape and texture', (12, 15)), ('multi - layer perceptron ( MLP )', (17, 24)), ('shape', (25, 26)), ('convolutional neural network ( CNN )', (27, 33)), ('texture', (34, 35))]","[['each component', 'design', 'different networks'], ['different networks', 'for', 'shape and texture'], ['multi - layer perceptron ( MLP )', 'for', 'shape'], ['multi - layer perceptron ( MLP )', 'for', 'texture'], ['convolutional neural network ( CNN )', 'for', 'texture'], ['multi - layer perceptron ( MLP )', 'for', 'shape'], ['convolutional neural network ( CNN )', 'for', 'texture']]",[],"[['Model', 'of', 'each component']]",[],face_alignment,13,46
1509,model,These two decoders are essentially the nonlinear 3 DMM .,"[('are', (3, 4))]","[('nonlinear 3 DMM', (6, 9))]",[],[],[],[],face_alignment,13,48
1510,model,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .","[('learn', (3, 4)), ('to', (7, 8)), ('formulated as', (15, 17))]","[('fitting algorithm', (5, 7)), ('our nonlinear 3 DMM', (8, 12)), ('CNN encoder', (18, 20))]","[['fitting algorithm', 'to', 'our nonlinear 3 DMM'], ['our nonlinear 3 DMM', 'formulated as', 'CNN encoder']]","[['fitting algorithm', 'has', 'our nonlinear 3 DMM']]","[['Model', 'learn', 'fitting algorithm']]",[],face_alignment,13,49
1511,model,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .","[('takes', (2, 3)), ('as', (8, 9)), ('generates', (11, 12)), ('estimate', (22, 23))]","[('encoder', (1, 2)), ('2 D face image', (4, 8)), ('input', (9, 10)), ('shape and texture parameters', (13, 17)), ('two', (20, 21)), ('3D face', (24, 26))]","[['encoder', 'takes', '2 D face image'], ['2 D face image', 'as', 'input'], ['encoder', 'generates', 'shape and texture parameters']]",[],[],"[['Model', 'has', 'encoder']]",face_alignment,13,50
1512,model,"Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .","[('design', (3, 4)), ('to generate', (8, 10)), ('by fusing', (13, 15)), ('estimated by', (26, 28))]","[('differentiable rendering layer', (5, 8)), ('reconstructed face', (11, 13)), ('3D face , texture', (16, 20)), ('encoder', (29, 30))]","[['differentiable rendering layer', 'to generate', 'reconstructed face'], ['reconstructed face', 'by fusing', '3D face , texture']]",[],"[['Model', 'design', 'differentiable rendering layer']]",[],face_alignment,13,52
1513,model,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .","[('constructed where', (9, 11)), ('learnt jointly', (17, 19)), ('to minimize', (19, 21)), ('between', (23, 24))]","[('endto - end learning scheme', (3, 8)), ('encoder and two decoders', (12, 16)), ('difference', (22, 23)), ('reconstructed face and the input face', (25, 31))]","[['endto - end learning scheme', 'constructed where', 'encoder and two decoders'], ['encoder and two decoders', 'to minimize', 'difference'], ['difference', 'between', 'reconstructed face and the input face']]",[],[],"[['Model', 'has', 'endto - end learning scheme']]",face_alignment,13,53
1514,model,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,"[('Jointly learning', (0, 2)), ('without relying on', (21, 24))]","[('3 DMM and the model fitting encoder', (3, 10)), ('large collection of unconstrained 2D images', (15, 21)), ('3D scans', (24, 26))]","[['large collection of unconstrained 2D images', 'without relying on', '3D scans']]",[],"[['Model', 'Jointly learning', '3 DMM and the model fitting encoder']]",[],face_alignment,13,54
1515,model,1 ) We learn a nonlinear 3 DMM model that has greater representation power than its traditional linear counterpart .,"[('learn', (3, 4)), ('than', (14, 15))]","[('nonlinear 3 DMM model', (5, 9)), ('greater representation power', (11, 14)), ('traditional linear counterpart', (16, 19))]","[['greater representation power', 'than', 'traditional linear counterpart']]","[['nonlinear 3 DMM model', 'has', 'greater representation power']]","[['Model', 'learn', 'nonlinear 3 DMM model']]",[],face_alignment,13,58
1516,model,"2 ) We jointly learn the model and the model fitting algorithm via weak supervision , by leveraging a large collection of 2D images without 3D scans .","[('jointly learn', (3, 5)), ('via', (12, 13)), ('by leveraging', (16, 18)), ('without', (24, 25))]","[('model and the model fitting algorithm', (6, 12)), ('weak supervision', (13, 15)), ('large collection of 2D images', (19, 24)), ('3D scans', (25, 27))]","[['model and the model fitting algorithm', 'via', 'weak supervision'], ['weak supervision', 'by leveraging', 'large collection of 2D images'], ['large collection of 2D images', 'without', '3D scans']]",[],"[['Model', 'jointly learn', 'model and the model fitting algorithm']]",[],face_alignment,13,59
1517,model,The novel rendering layer enables the end - to - end training .,"[('enables', (4, 5))]","[('novel rendering layer', (1, 4)), ('end - to - end training', (6, 12))]","[['novel rendering layer', 'enables', 'end - to - end training']]",[],[],"[['Model', 'has', 'novel rendering layer']]",face_alignment,13,60
1518,results,"Using facial mesh triangle definition by Basel Face Model ( BFM ) , we train our 3 DMM using 300W - LP dataset .","[('Using', (0, 1)), ('by', (5, 6)), ('train', (14, 15)), ('using', (18, 19))]","[('facial mesh triangle definition', (1, 5)), ('Basel Face Model ( BFM )', (6, 12)), ('3 DMM', (16, 18)), ('300W - LP dataset', (19, 23))]","[['3 DMM', 'Using', '300W - LP dataset'], ['facial mesh triangle definition', 'by', 'Basel Face Model ( BFM )'], ['facial mesh triangle definition', 'train', '3 DMM'], ['3 DMM', 'using', '300W - LP dataset']]","[['facial mesh triangle definition', 'has', 'Basel Face Model ( BFM )']]","[['Results', 'Using', 'facial mesh triangle definition']]",[],face_alignment,13,207
1519,ablation-analysis,"The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.","[('optimized using', (3, 5)), ('with', (7, 8)), ('of', (12, 13)), ('when minimizing', (14, 16)), ('when minimizing', (21, 23))]","[('Adam optimizer', (5, 7)), ('initial learning rate', (9, 12)), ('0.001', (13, 14)), ('L 0', (16, 18)), ('0.0002', (20, 21))]","[['Adam optimizer', 'with', 'initial learning rate'], ['Adam optimizer', 'with', '0.0002'], ['initial learning rate', 'of', '0.001'], ['initial learning rate', 'of', '0.0002'], ['0.001', 'when minimizing', 'L 0']]",[],"[['Ablation analysis', 'optimized using', 'Adam optimizer']]",[],face_alignment,13,208
1520,ablation-analysis,"We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .","[('set to make', (29, 32)), ('to have', (33, 35))]","[('Q', (6, 7)), ('losses', (32, 33)), ('similar magnitudes', (35, 37))]","[['losses', 'to have', 'similar magnitudes']]","[['losses', 'has', 'similar magnitudes']]",[],[],face_alignment,13,209
1521,results,Expressiveness,[],[],[],[],[],[],face_alignment,13,210
1522,results,"Our nonlinear model has a significantly smaller reconstruction error than the linear model , 0.0196 vs. 0.0241 ( Tab. 3 ) .","[('than', (9, 10))]","[('Our nonlinear model', (0, 3)), ('significantly smaller reconstruction error', (5, 9)), ('linear model', (11, 13)), ('0.0196', (14, 15))]","[['significantly smaller reconstruction error', 'than', 'linear model']]","[['Our nonlinear model', 'has', 'significantly smaller reconstruction error']]",[],"[['Results', 'has', 'Our nonlinear model']]",face_alignment,13,247
1523,experiments,visualizes our 3 DMM fitting results on CelebA dataset .,"[('visualizes', (0, 1)), ('on', (6, 7))]","[('3 DMM fitting results', (2, 6)), ('CelebA dataset', (7, 9))]","[['3 DMM fitting results', 'on', 'CelebA dataset']]",[],[],[],face_alignment,13,253
1524,experiments,We can recover personal facial characteristic in both shape and texture .,"[('recover', (2, 3)), ('in', (6, 7))]","[('personal facial characteristic', (3, 6)), ('both shape and texture', (7, 11))]","[['personal facial characteristic', 'in', 'both shape and texture']]",[],[],[],face_alignment,13,255
1525,experiments,Face alignment is a critical step for any facial analysis task such as face recognition .,[],"[('Face alignment', (0, 2))]",[],[],[],[],face_alignment,13,258
1526,experiments,We obtain a low error that is comparable to optimization - based methods .,"[('obtain', (1, 2)), ('comparable to', (7, 9))]","[('low error', (3, 5)), ('optimization - based methods', (9, 13))]","[['low error', 'comparable to', 'optimization - based methods']]",[],[],[],face_alignment,13,265
1527,research-problem,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses,[],"[('Facial Alignment', (5, 7))]",[],[],[],[],face_alignment,14,2
1528,model,The non-visible regions of the face are determined by the estimated camera center and the estimated 3D shape .,"[('of', (3, 4)), ('determined by', (7, 9))]","[('non-visible regions', (1, 3)), ('face', (5, 6)), ('estimated camera center', (10, 13)), ('estimated 3D shape', (15, 18))]","[['non-visible regions', 'of', 'face'], ['non-visible regions', 'determined by', 'estimated camera center'], ['non-visible regions', 'determined by', 'estimated 3D shape']]",[],[],"[['Model', 'has', 'non-visible regions']]",face_alignment,14,20
1529,model,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .","[('observe', (9, 10)), ('can', (16, 17)), ('deformed to', (25, 27)), ('at', (30, 31)), ('compared to', (36, 38))]","[('fairly accurate 3 D models', (11, 16)), ('simple mean shape', (22, 25)), ('input image', (28, 30)), ('relatively low computational cost', (32, 36)), ('other approaches', (38, 40))]","[['simple mean shape', 'deformed to', 'input image'], ['input image', 'at', 'relatively low computational cost'], ['relatively low computational cost', 'compared to', 'other approaches']]",[],"[['Model', 'observe', 'fairly accurate 3 D models']]",[],face_alignment,14,33
1530,model,Landmark locations can be directly predicted by a regression from a learned feature space .,"[('directly predicted by', (4, 7)), ('from', (9, 10))]","[('Landmark locations', (0, 2)), ('regression', (8, 9)), ('learned feature space', (11, 14))]","[['Landmark locations', 'directly predicted by', 'regression'], ['regression', 'from', 'learned feature space']]",[],[],"[['Model', 'has', 'Landmark locations']]",face_alignment,14,49
1531,model,The objective function in GSDM is divided into multiple regions of similar gradient directions .,"[('in', (3, 4)), ('divided into', (6, 8)), ('of', (10, 11))]","[('objective function', (1, 3)), ('GSDM', (4, 5)), ('multiple regions', (8, 10)), ('similar gradient directions', (11, 14))]","[['objective function', 'in', 'GSDM'], ['objective function', 'divided into', 'multiple regions'], ['multiple regions', 'of', 'similar gradient directions']]",[],[],"[['Model', 'has', 'objective function']]",face_alignment,14,51
1532,model,It then constructs a separate cascaded shape regressor for each region .,"[('constructs', (2, 3)), ('for', (8, 9))]","[('separate cascaded shape regressor', (4, 8)), ('each region', (9, 11))]","[['separate cascaded shape regressor', 'for', 'each region']]",[],"[['Model', 'constructs', 'separate cascaded shape regressor']]",[],face_alignment,14,52
1533,baselines,3DDFA fits a dense 3 D face model to the image via CNN and DDN proposes a novel cascaded framework incorporating geometric constraints for localizing landmarks in faces and other non-rigid objects .,"[('fits', (1, 2)), ('to', (8, 9)), ('via', (11, 12)), ('proposes', (15, 16)), ('incorporating', (20, 21)), ('for localizing', (23, 25)), ('in', (26, 27))]","[('3DDFA', (0, 1)), ('dense 3 D face model', (3, 8)), ('image', (10, 11)), ('CNN', (12, 13)), ('novel cascaded framework', (17, 20)), ('geometric constraints', (21, 23)), ('landmarks', (25, 26)), ('faces and other non-rigid objects', (27, 32))]","[['3DDFA', 'fits', 'dense 3 D face model'], ['dense 3 D face model', 'to', 'image'], ['novel cascaded framework', 'incorporating', 'geometric constraints'], ['geometric constraints', 'for localizing', 'landmarks'], ['landmarks', 'in', 'faces and other non-rigid objects']]","[['3DDFA', 'has', 'dense 3 D face model']]",[],"[['Baselines', 'has', '3DDFA']]",face_alignment,14,57
1534,research-problem,Nonlinear statistical model approaches are impractical in real - time applications .,[],"[('Nonlinear statistical model approaches', (0, 4))]",[],[],[],[],face_alignment,14,60
1535,research-problem,CNNs for 3D Object Modeling,[],[],[],[],[],[],face_alignment,14,63
1536,hyperparameters,Our network is implemented in the Caffe framework .,"[('implemented in', (3, 5))]","[('Our network', (0, 2)), ('Caffe framework', (6, 8))]","[['Our network', 'implemented in', 'Caffe framework']]",[],[],"[['Hyperparameters', 'has', 'Our network']]",face_alignment,14,203
1537,ablation-analysis,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .","[('created', (4, 5)), ('consisting of', (5, 7))]","[('new layer', (1, 3)), ('3D TPS transformation module', (8, 12)), ('camera projection module', (14, 17)), ('bilinear sampler module', (19, 22))]","[['new layer', 'created', 'bilinear sampler module'], ['new layer', 'consisting of', '3D TPS transformation module'], ['new layer', 'consisting of', 'bilinear sampler module']]",[],[],"[['Ablation analysis', 'has', 'new layer']]",face_alignment,14,204
1538,hyperparameters,All modules are differentiable so that the whole network can be trained end - to - end .,"[('differentiable', (3, 4)), ('can be trained', (9, 12))]","[('whole network', (7, 9)), ('end - to - end', (12, 17))]","[['whole network', 'can be trained', 'end - to - end']]",[],"[['Hyperparameters', 'differentiable', 'whole network']]",[],face_alignment,14,205
1539,hyperparameters,"We adopt two architectures , AlexNet and VGG - 16 , as the pre-trained models for our shared feature extraction networks in , i.e. we use the convolution layers from the pre-trained models to initialize ours .","[('adopt', (1, 2)), ('as', (11, 12)), ('for', (15, 16)), ('use', (25, 26)), ('from', (29, 30)), ('to initialize', (33, 35))]","[('two architectures', (2, 4)), ('AlexNet and VGG - 16', (5, 10)), ('shared feature extraction networks', (17, 21)), ('convolution layers', (27, 29))]","[['shared feature extraction networks', 'use', 'convolution layers']]","[['two architectures', 'name', 'AlexNet and VGG - 16']]","[['Hyperparameters', 'adopt', 'two architectures']]",[],face_alignment,14,206
1540,hyperparameters,"Since these networks already extract informative low - level features and we do not want to lose this information , we freeze some of the earlier convolution layers and finetune the rest .","[('extract', (4, 5)), ('freeze', (21, 22)), ('finetune', (29, 30))]","[('some of', (22, 24)), ('earlier convolution layers', (25, 28)), ('rest', (31, 32))]",[],"[['some of', 'has', 'earlier convolution layers']]","[['Hyperparameters', 'extract', 'some of']]",[],face_alignment,14,207
1541,hyperparameters,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .","[('For', (0, 1)), ('freeze', (6, 7)), ('for', (11, 12)), ('are', (22, 23))]","[('AlexNet architecture', (2, 4)), ('first layer', (8, 10)), ('VGG - 16 architecture', (13, 17)), ('first 4 layers', (19, 22)), ('frozen', (23, 24))]","[['AlexNet architecture', 'freeze', 'first layer'], ['first 4 layers', 'are', 'frozen']]","[['VGG - 16 architecture', 'has', 'first 4 layers'], ['first 4 layers', 'has', 'frozen']]","[['Hyperparameters', 'For', 'AlexNet architecture']]",[],face_alignment,14,208
1542,hyperparameters,The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,"[('implemented by', (5, 7)), ('on top of', (10, 13))]","[('2D landmark regression', (1, 4)), ('attaching', (7, 8)), ('additional layers', (8, 10)), ('last convolution layer', (14, 17))]","[['2D landmark regression', 'implemented by', 'attaching'], ['additional layers', 'on top of', 'last convolution layer']]","[['attaching', 'has', 'additional layers']]",[],"[['Hyperparameters', 'has', '2D landmark regression']]",face_alignment,14,209
1543,hyperparameters,"With N landmarks to regress , we need NFC layers to compute the offsets for each individual landmark .","[('With', (0, 1)), ('need', (7, 8)), ('to compute', (10, 12)), ('for', (14, 15))]","[('N landmarks to regress', (1, 5)), ('NFC layers', (8, 10)), ('offsets', (13, 14)), ('each individual landmark', (15, 18))]","[['N landmarks to regress', 'need', 'NFC layers'], ['NFC layers', 'to compute', 'offsets'], ['offsets', 'for', 'each individual landmark']]",[],"[['Hyperparameters', 'With', 'N landmarks to regress']]",[],face_alignment,14,210
1544,hyperparameters,"While it 's possible to setup N individual FC layers , here we implement this by adding one Scaling layer followed by a Reduction layer and Bias layer .","[('followed by', (20, 22))]","[('one Scaling layer', (17, 20)), ('Reduction layer and Bias layer', (23, 28))]","[['one Scaling layer', 'followed by', 'Reduction layer and Bias layer']]",[],[],[],face_alignment,14,211
1545,hyperparameters,During training only the new layers are updated and all previous layers are frozen .,"[('During', (0, 1)), ('are', (6, 7))]","[('training', (1, 2)), ('new layers', (4, 6)), ('updated', (7, 8)), ('all previous layers', (9, 12)), ('frozen', (13, 14))]","[['new layers', 'are', 'updated'], ['all previous layers', 'are', 'frozen']]","[['training', 'has', 'new layers']]","[['Hyperparameters', 'During', 'training']]",[],face_alignment,14,212
1546,experiments,Training on 300W - LP,[],"[('Training on', (0, 2))]",[],[],[],[],face_alignment,14,213
1547,experiments,"For the AlexNet architecture , we train for 100,000 iterations with a batch size of 50 .","[('For', (0, 1)), ('train for', (6, 8)), ('with', (10, 11))]","[('AlexNet architecture', (2, 4)), ('100,000 iterations', (8, 10)), ('batch size of 50', (12, 16))]","[['AlexNet architecture', 'train for', '100,000 iterations'], ['100,000 iterations', 'with', 'batch size of 50']]",[],[],[],face_alignment,14,216
1548,hyperparameters,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 50,000 iterations .","[('set to', (5, 7)), ('drops by', (9, 11)), ('after', (15, 16))]","[('initial learning rate', (1, 4)), ('0.001', (7, 8)), ('factor of 2', (12, 15)), ('50,000 iterations', (16, 18))]","[['initial learning rate', 'set to', '0.001'], ['initial learning rate', 'drops by', 'factor of 2'], ['factor of 2', 'after', '50,000 iterations']]","[['initial learning rate', 'has', '0.001']]",[],"[['Hyperparameters', 'has', 'initial learning rate']]",face_alignment,14,217
1549,hyperparameters,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 40,000 iterations .","[('training', (1, 2)), ('is', (10, 11)), ('drops by', (13, 15)), ('every', (19, 20))]","[('landmark regression', (3, 5)), ('initial learning rate', (7, 10)), ('0.01', (11, 12)), ('factor of 10', (16, 19)), ('40,000 iterations', (20, 22))]","[['initial learning rate', 'is', '0.01'], ['initial learning rate', 'drops by', 'factor of 10'], ['factor of 10', 'every', '40,000 iterations']]","[['landmark regression', 'has', 'initial learning rate'], ['initial learning rate', 'has', '0.01']]","[['Hyperparameters', 'training', 'landmark regression']]",[],face_alignment,14,218
1550,experiments,"For the VGG - 16 architecture , we train for 200,000 iterations with a batch size of 25 .","[('For', (0, 1)), ('train for', (8, 10)), ('with', (12, 13)), ('of', (16, 17))]","[('VGG - 16 architecture', (2, 6)), ('200,000 iterations', (10, 12)), ('batch size', (14, 16)), ('25', (17, 18))]","[['VGG - 16 architecture', 'train for', '200,000 iterations'], ['200,000 iterations', 'with', 'batch size'], ['batch size', 'of', '25']]","[['batch size', 'has', '25']]",[],[],face_alignment,14,219
1551,hyperparameters,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 100,000 iterations .","[('set to', (5, 7)), ('drops by', (9, 11)), ('after', (15, 16))]","[('initial learning rate', (1, 4)), ('0.001', (7, 8)), ('factor of 2', (12, 15)), ('100,000 iterations', (16, 18))]","[['initial learning rate', 'set to', '0.001'], ['initial learning rate', 'drops by', 'factor of 2'], ['factor of 2', 'after', '100,000 iterations']]","[['initial learning rate', 'has', '0.001']]",[],"[['Hyperparameters', 'has', 'initial learning rate']]",face_alignment,14,220
1552,hyperparameters,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 70,000 iterations .","[('is', (10, 11)), ('drops by', (13, 15)), ('every', (19, 20))]","[('landmark regression', (3, 5)), ('initial learning rate', (7, 10)), ('0.01', (11, 12)), ('factor of 10', (16, 19)), ('70,000 iterations', (20, 22))]","[['initial learning rate', 'is', '0.01'], ['initial learning rate', 'drops by', 'factor of 10'], ['factor of 10', 'every', '70,000 iterations']]","[['landmark regression', 'has', 'initial learning rate'], ['initial learning rate', 'has', '0.01']]",[],"[['Hyperparameters', 'has', 'landmark regression']]",face_alignment,14,221
1553,hyperparameters,The momentum for all experiments is set to 0.9 .,"[('for', (2, 3)), ('set to', (6, 8))]","[('momentum', (1, 2)), ('all experiments', (3, 5)), ('0.9', (8, 9))]","[['momentum', 'for', 'all experiments'], ['momentum', 'set to', '0.9'], ['all experiments', 'set to', '0.9']]",[],[],"[['Hyperparameters', 'has', 'momentum']]",face_alignment,14,222
1554,ablation-analysis,The VGG - 16 model outperforms the AlexNet model in all three pose ranges on the AFLW detected set as shown in .,"[('in', (9, 10)), ('on', (14, 15))]","[('VGG - 16 model', (1, 5)), ('outperforms', (5, 6)), ('AlexNet model', (7, 9)), ('all three pose ranges', (10, 14)), ('AFLW detected set', (16, 19))]","[['AlexNet model', 'in', 'all three pose ranges'], ['all three pose ranges', 'on', 'AFLW detected set']]","[['VGG - 16 model', 'has', 'outperforms'], ['outperforms', 'has', 'AlexNet model']]",[],"[['Ablation analysis', 'has', 'VGG - 16 model']]",face_alignment,14,229
1555,ablation-analysis,shows that the landmark regression step greatly helps to improve the accuracy .,"[('shows', (0, 1)), ('to improve', (8, 10))]","[('landmark regression step', (3, 6)), ('greatly helps', (6, 8)), ('accuracy', (11, 12))]","[['greatly helps', 'to improve', 'accuracy']]","[['landmark regression step', 'has', 'greatly helps']]","[['Ablation analysis', 'shows', 'landmark regression step']]",[],face_alignment,14,233
1556,baselines,"AFLW : Since the CMS - RCNN approach may only detect the easier to landmark faces , we use the provided bounding box anytime the face is not detected by the detector .","[('use', (18, 19)), ('anytime', (23, 24)), ('not detected by', (27, 30))]","[('AFLW', (0, 1)), ('provided bounding box', (20, 23)), ('face', (25, 26)), ('detector', (31, 32))]","[['provided bounding box', 'anytime', 'face'], ['face', 'not detected by', 'detector']]","[['provided bounding box', 'has', 'face']]",[],"[['Baselines', 'has', 'AFLW']]",face_alignment,14,235
1557,baselines,"We compare against baseline methods used by on the same dataset , namely Cascaded Deformable Shape Models ( CDM ) , Robust Cascaded Pose Regression ( RCPR ) , Explicit Shape Regression ( ESR ) , SDM and 3DDFA .","[('compare', (1, 2)), ('namely', (12, 13))]","[('Cascaded Deformable Shape Models ( CDM )', (13, 20)), ('Robust Cascaded Pose Regression ( RCPR )', (21, 28)), ('Explicit Shape Regression ( ESR )', (29, 35)), ('SDM', (36, 37)), ('3DDFA', (38, 39))]",[],[],"[['Baselines', 'compare', 'Cascaded Deformable Shape Models ( CDM )']]",[],face_alignment,14,238
1558,baselines,All methods except for CDM were retrained on the 300W - LP dataset .,"[('except for', (2, 4)), ('retrained on', (6, 8))]","[('methods', (1, 2)), ('CDM', (4, 5)), ('300W - LP dataset', (9, 13))]","[['methods', 'except for', 'CDM'], ['methods', 'retrained on', '300W - LP dataset']]",[],[],"[['Baselines', 'has', 'methods']]",face_alignment,14,239
1559,results,"clearly shows that our model using the VGG - 16 architecture has achieved better accuracy in all pose ranges , especially the ( 60 , 90 ] category , and has achieved a smaller standard deviation in the error .","[('shows', (1, 2)), ('using', (5, 6)), ('achieved', (12, 13)), ('in', (15, 16)), ('especially', (20, 21)), ('achieved', (31, 32)), ('in', (36, 37))]","[('our model', (3, 5)), ('VGG - 16 architecture', (7, 11)), ('better accuracy', (13, 15)), ('all pose ranges', (16, 19)), ('( 60 , 90 ] category', (22, 28)), ('smaller standard deviation', (33, 36)), ('error', (38, 39))]","[['our model', 'using', 'VGG - 16 architecture'], ['our model', 'achieved', 'better accuracy'], ['VGG - 16 architecture', 'achieved', 'better accuracy'], ['better accuracy', 'in', 'all pose ranges'], ['better accuracy', 'especially', '( 60 , 90 ] category'], ['all pose ranges', 'especially', '( 60 , 90 ] category'], ['VGG - 16 architecture', 'achieved', 'smaller standard deviation'], ['better accuracy', 'achieved', 'smaller standard deviation'], ['smaller standard deviation', 'in', 'error']]",[],"[['Results', 'shows', 'our model']]",[],face_alignment,14,241
1560,results,"This means that not only are the landmarks more accurate , they are more consistent than the other methods ..","[('than', (15, 16))]","[('landmarks', (7, 8)), ('more accurate', (8, 10)), ('more consistent', (13, 15)), ('other methods', (17, 19))]","[['more consistent', 'than', 'other methods']]","[['landmarks', 'has', 'more accurate']]",[],[],face_alignment,14,242
1561,baselines,AFLW2000 - 3D :,[],"[('AFLW2000 - 3D', (0, 3))]",[],[],[],"[['Baselines', 'has', 'AFLW2000 - 3D']]",face_alignment,14,247
1562,results,"Here we see that though 3DDFA + SDM performs well , the VGG - 16 architecture of our model still performs best in both the [ 0 , 30 ] and ( 60 , 90 ] ranges .","[('performs', (8, 9)), ('of', (16, 17)), ('performs', (20, 21)), ('in both', (22, 24))]","[('3DDFA + SDM', (5, 8)), ('well', (9, 10)), ('VGG - 16 architecture', (12, 16)), ('our model', (17, 19)), ('best', (21, 22))]","[['3DDFA + SDM', 'performs', 'well'], ['VGG - 16 architecture', 'of', 'our model']]",[],[],[],face_alignment,14,251
1563,results,"While the VGG - 16 model is only second best in the ( 30 , 60 ] range by a small amount , the improvement in ( 60 , 90 ] means that , once again , our method generates more accurate and more consistent landmarks , even in a 3D sense .","[('is', (6, 7)), ('in', (10, 11)), ('by', (18, 19)), ('generates', (39, 40))]","[('VGG - 16 model', (2, 6)), ('second best', (8, 10)), ('( 30 , 60 ] range', (12, 18)), ('small amount', (20, 22)), ('our method', (37, 39)), ('more accurate and', (40, 43))]","[['VGG - 16 model', 'is', 'second best'], ['second best', 'in', '( 30 , 60 ] range'], ['second best', 'by', 'small amount'], ['( 30 , 60 ] range', 'by', 'small amount'], ['our method', 'generates', 'more accurate and']]","[['VGG - 16 model', 'has', 'second best']]",[],[],face_alignment,14,252
1564,results,Running Speed,[],[],[],[],[],[],face_alignment,14,254
1565,results,The models are evaluated on a 3.40 GHz Intel Core i7-6700 CPU and an NVIDIA GeForce GTX TITAN X GPU .,"[('evaluated on', (3, 5))]","[('3.40 GHz Intel Core i7-6700 CPU', (6, 12)), ('NVIDIA GeForce GTX TITAN X GPU', (14, 20))]",[],[],"[['Results', 'evaluated on', '3.40 GHz Intel Core i7-6700 CPU']]",[],face_alignment,14,257
1566,research-problem,Face Alignment Across Large Poses : A 3D Solution,[],"[('Face Alignment Across', (0, 3))]",[],[],[],[],face_alignment,15,2
1567,model,The latter extracts features around key points and regresses it to the ground truth landmarks .,"[('features around', (3, 5)), ('regresses it to', (8, 11))]","[('key points', (5, 7)), ('ground truth landmarks', (12, 15))]",[],[],[],[],face_alignment,15,18
1568,model,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .","[('rather than', (11, 13)), ('to', (18, 19))]","[('3D dense face model', (7, 11)), ('sparse landmark shape model', (14, 18)), ('image', (20, 21))]","[['3D dense face model', 'rather than', 'sparse landmark shape model'], ['sparse landmark shape model', 'to', 'image']]",[],[],[],face_alignment,15,45
1569,research-problem,We call this method 3D Dense Face Alignment ( 3DDFA ) .,[],"[('3D Dense Face Alignment ( 3DDFA )', (4, 11))]",[],[],[],[],face_alignment,15,47
1570,model,"To resolve the fitting process in 3 DDFA , we propose a cascaded convolutional neutral network ( CNN ) based regression method .","[('To resolve', (0, 2)), ('in', (5, 6)), ('propose', (10, 11))]","[('fitting process', (3, 5)), ('3 DDFA', (6, 8)), ('cascaded convolutional neutral network ( CNN ) based regression method', (12, 22))]","[['fitting process', 'in', '3 DDFA'], ['3 DDFA', 'propose', 'cascaded convolutional neutral network ( CNN ) based regression method']]","[['fitting process', 'has', '3 DDFA']]","[['Model', 'To resolve', 'fitting process']]",[],face_alignment,15,50
1571,model,"In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .","[('adopt', (5, 6)), ('to fit', (7, 9)), ('with', (13, 14)), ('namely', (19, 20))]","[('CNN', (6, 7)), ('3D face model', (10, 13)), ('Projected Normalized Coordinate Code ( PNCC )', (20, 27))]","[['CNN', 'to fit', '3D face model']]",[],"[['Model', 'adopt', 'CNN']]",[],face_alignment,15,52
1572,model,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .","[('proposed as', (10, 12))]","[('Weighted Parameter Distance Cost ( WPDC )', (2, 9)), ('cost function', (13, 15))]","[['Weighted Parameter Distance Cost ( WPDC )', 'proposed as', 'cost function']]","[['Weighted Parameter Distance Cost ( WPDC )', 'has', 'cost function']]",[],"[['Model', 'has', 'Weighted Parameter Distance Cost ( WPDC )']]",face_alignment,15,53
1573,model,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,"[('further propose', (1, 3)), ('to synthesize', (7, 9)), ('across', (14, 15))]","[('face profiling algorithm', (4, 7)), ('60 k + training samples', (9, 14)), ('large poses', (15, 17))]","[['face profiling algorithm', 'to synthesize', '60 k + training samples'], ['60 k + training samples', 'across', 'large poses']]",[],"[['Model', 'further propose', 'face profiling algorithm']]",[],face_alignment,15,57
1574,code,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/.",[],[],[],[],[],[],face_alignment,15,59
1575,baselines,"In this paper , we test the performance of 3DDFA on three different tasks , including the large - pose face alignment on AFLW , 3 D face alignment on AFLW2000 - 3D and mediumpose face alignment on 300W .","[('test', (5, 6)), ('of', (8, 9)), ('on', (10, 11)), ('including', (15, 16)), ('on', (22, 23)), ('on', (29, 30)), ('on', (37, 38))]","[('performance', (7, 8)), ('3DDFA', (9, 10)), ('three', (11, 12)), ('large - pose face alignment', (17, 22)), ('AFLW', (23, 24)), ('3 D face alignment', (25, 29)), ('AFLW2000 - 3D', (30, 33)), ('mediumpose face alignment', (34, 37)), ('300W', (38, 39))]","[['performance', 'of', '3DDFA'], ['3DDFA', 'on', 'three'], ['large - pose face alignment', 'on', 'AFLW'], ['mediumpose face alignment', 'on', '300W'], ['large - pose face alignment', 'on', 'AFLW'], ['large - pose face alignment', 'on', 'AFLW2000 - 3D'], ['large - pose face alignment', 'on', 'mediumpose face alignment'], ['large - pose face alignment', 'on', '300W'], ['3 D face alignment', 'on', 'AFLW2000 - 3D'], ['mediumpose face alignment', 'on', '300W']]","[['performance', 'has', '3DDFA']]","[['Baselines', 'test', 'performance']]",[],face_alignment,15,239
1576,baselines,Large Pose Face Alignment in AFLW Protocol :,[],"[('Large Pose Face Alignment in', (0, 5)), ('AFLW Protocol', (5, 7))]",[],"[['Large Pose Face Alignment in', 'has', 'AFLW Protocol']]",[],"[['Baselines', 'has', 'Large Pose Face Alignment in']]",face_alignment,15,240
1577,hyperparameters,The bounding boxes provided by AFLW are used for initialization ( which are not the ground truth ) .,"[('provided by', (3, 5)), ('used for', (7, 9))]","[('bounding boxes', (1, 3)), ('AFLW', (5, 6)), ('initialization', (9, 10))]","[['bounding boxes', 'provided by', 'AFLW']]",[],[],"[['Hyperparameters', 'has', 'bounding boxes']]",face_alignment,15,242
1578,hyperparameters,"During training , for 2D methods we use the projected 3D landmarks as the ground truth and for 3DDFA we directly regress the 3 DMM parameters .","[('During', (0, 1)), ('for', (3, 4)), ('use', (7, 8)), ('as', (12, 13)), ('for', (17, 18)), ('directly regress', (20, 22))]","[('training', (1, 2)), ('2D methods', (4, 6)), ('projected 3D landmarks', (9, 12)), ('ground truth', (14, 16)), ('3DDFA', (18, 19)), ('3 DMM parameters', (23, 26))]","[['training', 'for', '2D methods'], ['2D methods', 'use', 'projected 3D landmarks'], ['projected 3D landmarks', 'as', 'ground truth'], ['projected 3D landmarks', 'for', '3DDFA'], ['3DDFA', 'directly regress', '3 DMM parameters']]","[['training', 'has', '2D methods']]","[['Hyperparameters', 'During', 'training']]",[],face_alignment,15,243
1579,baselines,CDM is the first one claimed to perform pose - free face alignment .,[],"[('CDM', (0, 1)), ('pose - free face alignment', (8, 13))]",[],[],[],"[['Baselines', 'has', 'CDM']]",face_alignment,15,250
1580,baselines,RCPR is a occlusion - robust method with the potential to deal with selfocclusion and we train it with landmark visibility labels computed by .,"[('is', (1, 2)), ('with', (7, 8)), ('to deal with', (10, 13)), ('train it with', (16, 19))]","[('RCPR', (0, 1)), ('occlusion - robust method', (3, 7)), ('potential', (9, 10)), ('selfocclusion', (13, 14)), ('landmark visibility labels', (19, 22))]","[['RCPR', 'is', 'occlusion - robust method'], ['occlusion - robust method', 'with', 'potential'], ['potential', 'to deal with', 'selfocclusion'], ['RCPR', 'train it with', 'landmark visibility labels']]","[['RCPR', 'has', 'occlusion - robust method'], ['occlusion - robust method', 'has', 'potential']]",[],"[['Baselines', 'has', 'RCPR']]",face_alignment,15,251
1581,results,"Firstly , the results indicate that all the methods benefits substantially from face profiling when dealing with large poses .","[('indicate', (4, 5)), ('benefits', (9, 10)), ('from', (11, 12)), ('when dealing with', (14, 17))]","[('all the methods', (6, 9)), ('substantially', (10, 11)), ('face profiling', (12, 14)), ('large poses', (17, 19))]","[['all the methods', 'benefits', 'substantially'], ['substantially', 'from', 'face profiling'], ['face profiling', 'when dealing with', 'large poses']]",[],"[['Results', 'indicate', 'all the methods']]",[],face_alignment,15,258
1582,results,"The improvements in [ 60 , 90 ] are 44.06 % for RCPR , 40.36 % for ESR and 42.10 % for SDM .","[('in', (2, 3)), ('are', (8, 9)), ('for', (11, 12))]","[('improvements', (1, 2)), ('[ 60 , 90 ]', (3, 8)), ('44.06 %', (9, 11)), ('RCPR', (12, 13)), ('40.36 %', (14, 16)), ('ESR', (17, 18)), ('42.10 %', (19, 21)), ('SDM', (22, 23))]","[['improvements', 'in', '[ 60 , 90 ]'], ['improvements', 'are', '44.06 %'], ['[ 60 , 90 ]', 'are', '44.06 %'], ['44.06 %', 'for', 'RCPR'], ['44.06 %', 'for', '42.10 %'], ['40.36 %', 'for', 'ESR']]",[],[],[],face_alignment,15,259
1583,results,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .","[('reaches', (3, 4)), ('above', (9, 10)), ('especially beyond', (14, 16))]","[('3DDFA', (2, 3)), ('state of the art', (5, 9)), ('all the 2D methods', (10, 14)), ('medium poses', (16, 18))]","[['3DDFA', 'reaches', 'state of the art'], ['state of the art', 'above', 'all the 2D methods'], ['all the 2D methods', 'especially beyond', 'medium poses']]","[['3DDFA', 'has', 'state of the art'], ['state of the art', 'has', 'all the 2D methods']]",[],"[['Results', 'has', '3DDFA']]",face_alignment,15,262
1584,results,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,"[('of', (4, 5)), ('demonstrates', (7, 8)), ('to pose', (10, 12))]","[('minimum standard deviation', (1, 4)), ('3DDFA', (5, 6)), ('robustness', (9, 10)), ('variations', (12, 13))]","[['minimum standard deviation', 'of', '3DDFA'], ['minimum standard deviation', 'demonstrates', 'robustness'], ['3DDFA', 'demonstrates', 'robustness'], ['robustness', 'to pose', 'variations']]",[],[],"[['Results', 'has', 'minimum standard deviation']]",face_alignment,15,263
1585,baselines,3D Face Alignment in AFLW2000-3D,[],"[('3D Face Alignment', (0, 3))]",[],[],[],"[['Baselines', 'has', '3D Face Alignment']]",face_alignment,15,265
1586,results,"Compared with the results in AFLW , we can seethe defect of barely evaluating visible landmarks .","[('Compared with', (0, 2)), ('in', (4, 5)), ('seethe', (9, 10)), ('of', (11, 12))]","[('results', (3, 4)), ('AFLW', (5, 6)), ('defect', (10, 11)), ('barely evaluating visible landmarks', (12, 16))]","[['results', 'in', 'AFLW'], ['results', 'seethe', 'defect'], ['AFLW', 'seethe', 'defect'], ['defect', 'of', 'barely evaluating visible landmarks']]","[['results', 'has', 'AFLW'], ['AFLW', 'has', 'defect'], ['defect', 'has', 'barely evaluating visible landmarks']]","[['Results', 'Compared with', 'results']]",[],face_alignment,15,269
1587,results,"For all the methods , despite with ground truth bounding boxes the performance in [ 60 , 90 ] and the standard deviation are obviously reduced when considering all the landmarks .","[('despite with', (5, 7)), ('in', (13, 14)), ('when considering', (26, 28))]","[('ground truth bounding boxes', (7, 11)), ('performance', (12, 13)), ('[ 60 , 90 ]', (14, 19)), ('standard deviation', (21, 23)), ('reduced', (25, 26)), ('all the landmarks', (28, 31))]","[['ground truth bounding boxes', 'in', 'performance'], ['performance', 'in', '[ 60 , 90 ]'], ['performance', 'in', 'standard deviation'], ['reduced', 'when considering', 'all the landmarks']]","[['ground truth bounding boxes', 'has', 'performance']]","[['Results', 'despite with', 'ground truth bounding boxes']]",[],face_alignment,15,270
1588,research-problem,Deep Multi- Center Learning for Face Alignment,[],[],[],[],[],[],face_alignment,16,2
1589,code,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,[],"[('https://github.com/ZhiwenShao/MCNet-Extension', (8, 9))]",[],[],[],[],face_alignment,16,11
1590,model,"However , it needs extra labels of facial attributes for training samples , which limits its universality .","[('needs', (3, 4)), ('of', (6, 7)), ('for', (9, 10)), ('limits', (14, 15))]","[('extra labels', (4, 6)), ('facial attributes', (7, 9)), ('training', (10, 11)), ('samples', (11, 12)), ('universality', (16, 17))]","[['extra labels', 'of', 'facial attributes'], ['facial attributes', 'for', 'training'], ['samples', 'limits', 'universality']]","[['training', 'has', 'samples']]","[['Model', 'needs', 'extra labels']]",[],face_alignment,16,27
1591,model,It is observed that the nose can be localized roughly with the locations of eyes and mouth .,"[('observed that', (2, 4)), ('can be', (6, 8)), ('with', (10, 11))]","[('nose', (5, 6)), ('localized', (8, 9)), ('roughly', (9, 10)), ('locations of eyes and mouth', (12, 17))]","[['nose', 'can be', 'localized'], ['localized', 'with', 'locations of eyes and mouth'], ['roughly', 'with', 'locations of eyes and mouth']]","[['nose', 'has', 'localized'], ['localized', 'has', 'roughly']]","[['Model', 'observed that', 'nose']]",[],face_alignment,16,32
1592,model,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .","[('propose', (6, 7)), ('named', (12, 13)), ('to exploit', (20, 22)), ('among', (25, 26))]","[('novel deep learning framework', (8, 12)), ('Multi - Center Learning ( MCL )', (13, 20)), ('strong correlations', (23, 25)), ('landmarks', (26, 27))]","[['novel deep learning framework', 'named', 'Multi - Center Learning ( MCL )'], ['novel deep learning framework', 'to exploit', 'strong correlations'], ['strong correlations', 'among', 'landmarks']]","[['novel deep learning framework', 'name', 'Multi - Center Learning ( MCL )']]","[['Model', 'propose', 'novel deep learning framework']]",[],face_alignment,16,35
1593,model,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .","[('uses', (5, 6)), ('to predict', (10, 12)), ('emphasizes on', (22, 24)), ('of', (26, 27))]","[('our', (3, 4)), ('multiple shape prediction layers', (6, 10)), ('locations of landmarks', (13, 16)), ('each shape prediction layer', (18, 22)), ('detection', (25, 26)), ('certain cluster of landmarks', (28, 32))]","[['our', 'uses', 'multiple shape prediction layers'], ['multiple shape prediction layers', 'to predict', 'locations of landmarks'], ['each shape prediction layer', 'emphasizes on', 'detection'], ['detection', 'of', 'certain cluster of landmarks']]",[],[],"[['Model', 'has', 'our']]",face_alignment,16,36
1594,model,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .","[('weighting', (1, 2)), ('of', (4, 5)), ('is', (19, 20))]","[('loss', (3, 4)), ('each landmark', (5, 7)), ('challenging landmarks', (8, 10)), ('focused', (11, 12)), ('firstly', (12, 13)), ('each cluster of landmarks', (15, 19)), ('further optimized', (20, 22))]","[['loss', 'of', 'each landmark'], ['each cluster of landmarks', 'is', 'further optimized']]","[['focused', 'has', 'firstly']]","[['Model', 'weighting', 'loss']]",[],face_alignment,16,37
1595,model,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .","[('to decrease', (2, 4)), ('propose', (9, 10)), ('to integrate', (14, 16)), ('into', (20, 21))]","[('model complexity', (5, 7)), ('model assembling method', (11, 14)), ('multiple shape prediction layers', (16, 20)), ('one shape prediction layer', (21, 25))]","[['model complexity', 'propose', 'model assembling method'], ['model assembling method', 'to integrate', 'multiple shape prediction layers'], ['multiple shape prediction layers', 'into', 'one shape prediction layer']]","[['model complexity', 'has', 'model assembling method']]","[['Model', 'to decrease', 'model complexity']]",[],face_alignment,16,38
1596,model,The entire framework reinforces the learning process of each landmark with a low model complexity .,"[('reinforces', (3, 4)), ('of', (7, 8)), ('with', (10, 11))]","[('learning process', (5, 7)), ('each landmark', (8, 10)), ('low model complexity', (12, 15))]","[['learning process', 'of', 'each landmark'], ['each landmark', 'with', 'low model complexity']]",[],"[['Model', 'reinforces', 'learning process']]",[],face_alignment,16,39
1597,model,We propose a novel multi-center learning framework for exploiting the strong correlations among landmarks .,"[('among', (12, 13))]","[('strong correlations', (10, 12)), ('landmarks', (13, 14))]","[['strong correlations', 'among', 'landmarks']]",[],[],[],face_alignment,16,41
1598,model,We propose a model assembling method which ensures a low model complexity .,"[('propose', (1, 2)), ('ensures', (7, 8))]","[('model assembling method', (3, 6)), ('low model complexity', (9, 12))]","[['model assembling method', 'ensures', 'low model complexity']]",[],"[['Model', 'propose', 'model assembling method']]",[],face_alignment,16,42
1599,research-problem,C. Face Alignment via Deep Learning,[],"[('C. Face Alignment', (0, 3))]",[],[],[],[],face_alignment,16,87
1600,hyperparameters,"We enhance the diversity of raw training data on account of their limited variation patterns , using five steps : rotation , uniform scaling , translation , horizontal flip , and JPEG compression .","[('enhance', (1, 2)), ('of', (4, 5)), ('on account of', (8, 11)), ('using', (16, 17))]","[('diversity', (3, 4)), ('raw training data', (5, 8)), ('limited variation patterns', (12, 15)), ('five steps', (17, 19)), ('rotation', (20, 21)), ('uniform scaling', (22, 24)), ('translation', (25, 26)), ('horizontal flip', (27, 29)), ('JPEG compression', (31, 33))]","[['diversity', 'of', 'raw training data'], ['diversity', 'on account of', 'limited variation patterns'], ['raw training data', 'on account of', 'limited variation patterns'], ['limited variation patterns', 'using', 'five steps']]","[['five steps', 'name', 'rotation']]","[['Hyperparameters', 'enhance', 'diversity']]",[],face_alignment,16,208
1601,hyperparameters,We train our MCL using an open source deep learning framework Caffe .,"[('train', (1, 2)), ('using', (4, 5))]","[('our MCL', (2, 4)), ('open source deep learning framework Caffe', (6, 12))]","[['our MCL', 'using', 'open source deep learning framework Caffe']]",[],"[['Hyperparameters', 'train', 'our MCL']]",[],face_alignment,16,213
1602,hyperparameters,"The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .","[('of', (4, 5)), ('are', (10, 11)), ('of', (23, 24)), ('are', (31, 32))]","[('maximum learning iterations', (1, 4)), ('pre-training', (5, 6)), ('each finetuning step', (7, 10)), ('1810 4 and 610 4', (11, 16)), ('initial learning rates', (20, 23)), ('pre-training', (24, 25)), ('each fine - tuning step', (26, 31)), ('0.02 and 0.001', (32, 35))]","[['maximum learning iterations', 'of', 'pre-training'], ['initial learning rates', 'of', 'pre-training'], ['each finetuning step', 'are', '1810 4 and 610 4'], ['initial learning rates', 'of', 'pre-training'], ['each fine - tuning step', 'are', '0.02 and 0.001']]",[],[],"[['Hyperparameters', 'has', 'maximum learning iterations']]",face_alignment,16,217
1603,baselines,"FLD + PDE performs facial landmark detection , pose and deformation estimation simultaneously , in which the training data of pose and deformation estimation are used .","[('performs', (3, 4)), ('in', (14, 15)), ('of', (19, 20))]","[('FLD + PDE', (0, 3)), ('facial landmark detection', (4, 7)), ('pose and deformation estimation', (8, 12)), ('training data', (17, 19)), ('pose and deformation estimation', (20, 24))]","[['FLD + PDE', 'performs', 'facial landmark detection'], ['pose and deformation estimation', 'in', 'training data'], ['training data', 'of', 'pose and deformation estimation']]",[],[],"[['Baselines', 'has', 'FLD + PDE']]",face_alignment,16,234
1604,results,"Our method MCL outperforms most of the state - of - the - art methods , especially on AFLW dataset where a relative error reduction of 3.93 % is achieved compared to RecNet .","[('especially on', (16, 18))]","[('Our method MCL', (0, 3)), ('outperforms', (3, 4)), ('most of the state - of - the - art methods', (4, 15)), ('AFLW dataset', (18, 20))]","[['most of the state - of - the - art methods', 'especially on', 'AFLW dataset']]","[['Our method MCL', 'has', 'outperforms'], ['outperforms', 'has', 'most of the state - of - the - art methods']]",[],"[['Results', 'has', 'Our method MCL']]",face_alignment,16,237
1605,ablation-analysis,1 ) Global Average Pooling vs. Full Connection :,[],"[('Global Average Pooling vs. Full Connection', (2, 8))]",[],[],[],"[['Ablation analysis', 'has', 'Global Average Pooling vs. Full Connection']]",face_alignment,16,256
1606,ablation-analysis,2 ) Robustness of Weighting :,[],"[('Robustness of Weighting', (2, 5))]",[],[],[],"[['Ablation analysis', 'has', 'Robustness of Weighting']]",face_alignment,16,266
1607,ablation-analysis,"When ? is 0.4 , WM can still achieves good performance .","[('When', (0, 1)), ('is', (2, 3))]","[('?', (1, 2)), ('0.4', (3, 4)), ('WM', (5, 6)), ('good performance', (9, 11))]","[['?', 'is', '0.4']]","[['?', 'has', '0.4'], ['0.4', 'has', 'WM']]","[['Ablation analysis', 'When', '?']]",[],face_alignment,16,273
1608,ablation-analysis,"Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .","[('Compared to', (0, 2)), ('reduce', (14, 15)), ('of', (18, 19))]","[('WM', (2, 3)), ('left eye model and the right eye model', (5, 13)), ('alignment errors', (16, 18)), ('corresponding clusters', (20, 22))]","[['left eye model and the right eye model', 'reduce', 'alignment errors'], ['alignment errors', 'of', 'corresponding clusters']]","[['WM', 'has', 'left eye model and the right eye model']]","[['Ablation analysis', 'Compared to', 'WM']]",[],face_alignment,16,279
1609,ablation-analysis,"Taking the left eye model as an example , it additionally reduces the errors of landmarks of right eye , mouth , and chin , which is due to the correlations among different facial parts .","[('Taking', (0, 1)), ('of', (14, 15)), ('of', (16, 17))]","[('left eye model', (2, 5)), ('errors', (13, 14)), ('landmarks', (15, 16)), ('right eye , mouth , and chin', (17, 24))]","[['errors', 'of', 'landmarks'], ['landmarks', 'of', 'right eye , mouth , and chin'], ['errors', 'of', 'landmarks'], ['landmarks', 'of', 'right eye , mouth , and chin']]",[],"[['Ablation analysis', 'Taking', 'left eye model']]",[],face_alignment,16,282
1610,ablation-analysis,"Moreover , for the right eye cluster , the right eye model improves the accuracy more significantly than the left eye model .","[('for', (2, 3)), ('improves', (12, 13)), ('than', (17, 18))]","[('right eye cluster', (4, 7)), ('right eye model', (9, 12)), ('accuracy', (14, 15)), ('more significantly', (15, 17)), ('left eye model', (19, 22))]","[['right eye model', 'improves', 'accuracy'], ['accuracy', 'than', 'left eye model'], ['more significantly', 'than', 'left eye model']]","[['right eye cluster', 'has', 'right eye model'], ['accuracy', 'has', 'more significantly']]","[['Ablation analysis', 'for', 'right eye cluster']]",[],face_alignment,16,283
1611,ablation-analysis,"Note that Simplified AM has already acquired good results , which verifies the effectiveness of the multicenter fine - tuning stage .","[('acquired', (6, 7))]","[('Simplified AM', (2, 4)), ('good results', (7, 9))]","[['Simplified AM', 'acquired', 'good results']]","[['Simplified AM', 'has', 'good results']]",[],"[['Ablation analysis', 'has', 'Simplified AM']]",face_alignment,16,287
1612,ablation-analysis,It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG .,"[('seen that', (3, 5)), ('improves', (8, 9)), ('on', (10, 11)), ('fails to search', (13, 16)), ('on', (19, 20))]","[('Weighting Simplified AM', (5, 8)), ('COFW', (11, 12)), ('better solution', (17, 19)), ('IBUG', (20, 21))]","[['better solution', 'on', 'IBUG'], ['Weighting Simplified AM', 'fails to search', 'better solution'], ['better solution', 'on', 'IBUG']]",[],"[['Ablation analysis', 'seen that', 'Weighting Simplified AM']]",[],face_alignment,16,290
1613,research-problem,Aggregation via Separation : Boosting Facial Landmark Detector with Semi-Supervised,[],"[('Aggregation via Separation', (0, 3)), ('Boosting Facial Landmark Detector', (4, 8))]",[],"[['Aggregation via Separation', 'has', 'Boosting Facial Landmark Detector']]",[],[],face_alignment,17,2
1614,research-problem,Style Translation,[],[],[],[],[],[],face_alignment,17,3
1615,research-problem,"With these augmented synthetic samples , our semi-supervised model surprisingly outperforms the fully - supervised one by a large margin .","[('With', (0, 1)), ('by', (16, 17))]","[('our semi-supervised model', (6, 9)), ('surprisingly outperforms', (9, 11)), ('fully - supervised one', (12, 16)), ('large margin', (18, 20))]","[['fully - supervised one', 'by', 'large margin']]","[['our semi-supervised model', 'has', 'surprisingly outperforms'], ['surprisingly outperforms', 'has', 'fully - supervised one']]","[['Research problem', 'With', 'our semi-supervised model']]",[],face_alignment,17,8
1616,code,The code is made publicly available at https://github.com/thesouthfrog/stylealign.,[],[],[],[],[],[],face_alignment,17,11
1617,research-problem,"Facial landmark detection is a fundamentally important step in many face applications , such as face recognition , 3 D face reconstruction , face tracking and face editing .",[],"[('Facial landmark detection', (0, 3))]",[],[],[],[],face_alignment,17,13
1618,model,"We instead utilize style transfer and disentangled representation learning to tackle the face alignment problem , since style transfer aims at altering style while preserving content .","[('utilize', (2, 3)), ('to tackle', (9, 11))]","[('style transfer and disentangled representation learning', (3, 9)), ('face alignment problem', (12, 15))]","[['style transfer and disentangled representation learning', 'to tackle', 'face alignment problem']]",[],"[['Model', 'utilize', 'style transfer and disentangled representation learning']]",[],face_alignment,17,24
1619,model,"Our idea is based on the purpose of facial landmark detection , which is to regress "" facial content "" - the principal component of facial geometry - by filtering unconstrained "" styles "" .","[('by filtering', (28, 30))]","[('facial landmark detection', (8, 11)), ('unconstrained "" styles ""', (30, 34))]",[],[],[],[],face_alignment,17,26
1620,model,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .","[('propose', (5, 6)), ('to', (9, 10)), ('for', (12, 13)), ('without using', (16, 18))]","[('new framework', (7, 9)), ('augment training', (10, 12)), ('facial landmark detection', (13, 16)), ('extra knowledge', (18, 20))]","[['new framework', 'to', 'augment training'], ['augment training', 'for', 'facial landmark detection'], ['facial landmark detection', 'without using', 'extra knowledge']]",[],"[['Model', 'propose', 'new framework']]",[],face_alignment,17,30
1621,model,"Instead of directly generating images , we first map face images into the space of structure and style .","[('first map', (7, 9)), ('into', (11, 12))]","[('face images', (9, 11)), ('space of structure and style', (13, 18))]","[['face images', 'into', 'space of structure and style']]",[],"[['Model', 'first map', 'face images']]",[],face_alignment,17,31
1622,model,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .","[('To guarantee', (0, 2)), ('design', (10, 11)), ('in which', (19, 21)), ('incorporated for', (33, 35)), ('of', (37, 38))]","[('disentanglement', (3, 4)), ('conditional variational auto - encoder model', (12, 18)), ('Kullback - Leiber ( KL ) divergence loss and skip connections', (21, 32)), ('compact representation', (35, 37)), ('style and structure', (38, 41))]","[['disentanglement', 'design', 'conditional variational auto - encoder model'], ['conditional variational auto - encoder model', 'in which', 'Kullback - Leiber ( KL ) divergence loss and skip connections'], ['Kullback - Leiber ( KL ) divergence loss and skip connections', 'incorporated for', 'compact representation'], ['compact representation', 'of', 'style and structure']]",[],"[['Model', 'To guarantee', 'disentanglement']]",[],face_alignment,17,32
1623,model,"By factoring these features , we perform visual style translation between existing facial geometry .","[('perform', (6, 7)), ('between', (10, 11))]","[('visual style translation', (7, 10)), ('existing facial geometry', (11, 14))]","[['visual style translation', 'between', 'existing facial geometry']]",[],"[['Model', 'perform', 'visual style translation']]",[],face_alignment,17,33
1624,model,A novel semi-supervised framework based on conditional variational auto - encoder is built upon this new perspective .,"[('based on', (4, 6))]","[('novel semi-supervised framework', (1, 4)), ('conditional variational auto - encoder', (6, 11))]","[['novel semi-supervised framework', 'based on', 'conditional variational auto - encoder']]",[],[],[],face_alignment,17,40
1625,model,"By disentangling style and structure , our model generates style - augmented images via style translation , further boosting facial landmark detection .","[('generates', (8, 9)), ('via', (13, 14)), ('further boosting', (17, 19))]","[('style and structure', (2, 5)), ('our model', (6, 8)), ('style - augmented images', (9, 13)), ('style translation', (14, 16)), ('facial landmark detection', (19, 22))]","[['our model', 'generates', 'style - augmented images'], ['style - augmented images', 'via', 'style translation'], ['style - augmented images', 'further boosting', 'facial landmark detection'], ['style translation', 'further boosting', 'facial landmark detection']]","[['style and structure', 'has', 'our model']]",[],[],face_alignment,17,41
1626,experiments,The Res - 18 baseline receives strong enhancement using synthetic images .,"[('receives', (5, 6)), ('using', (8, 9))]","[('Res - 18 baseline', (1, 5)), ('strong enhancement', (6, 8)), ('synthetic images', (9, 11))]","[['Res - 18 baseline', 'receives', 'strong enhancement'], ['strong enhancement', 'using', 'synthetic images']]","[['Res - 18 baseline', 'has', 'strong enhancement']]",[],[],face_alignment,17,168
1627,experiments,"By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .","[('utilizing', (1, 2)), ('achieves', (8, 9)), ('under', (12, 13)), ('by', (26, 27))]","[('stronger baseline', (3, 5)), ('our model', (6, 8)), ('4.39 % NME', (9, 12)), ('style - augmented training', (13, 17)), ('outperforms', (18, 19)), ('state - of the - art entries', (19, 26)), ('large margin', (28, 30))]","[['stronger baseline', 'achieves', '4.39 % NME'], ['our model', 'achieves', '4.39 % NME'], ['4.39 % NME', 'under', 'style - augmented training'], ['state - of the - art entries', 'by', 'large margin']]","[['stronger baseline', 'has', 'our model'], ['outperforms', 'has', 'state - of the - art entries']]",[],[],face_alignment,17,171
1628,experiments,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .","[('for', (3, 4)), ('brings', (11, 12)), ('to', (15, 16)), ('to', (23, 24)), ('from', (25, 26)), ('to', (29, 30))]","[('strong baselines', (5, 7)), ('our method', (8, 10)), ('15.9 % improvement', (12, 15)), ('SAN model', (16, 18)), ('9 % boost', (20, 23)), ('LAB', (24, 25)), ('5.27 % NME', (26, 29)), ('4.76 %', (30, 32))]","[['our method', 'brings', '15.9 % improvement'], ['our method', 'brings', '9 % boost'], ['15.9 % improvement', 'to', 'SAN model'], ['9 % boost', 'to', 'LAB'], ['5.27 % NME', 'to', '4.76 %'], ['9 % boost', 'to', 'LAB'], ['9 % boost', 'from', '5.27 % NME'], ['LAB', 'from', '5.27 % NME'], ['5.27 % NME', 'to', '4.76 %']]","[['strong baselines', 'has', 'our method']]",[],[],face_alignment,17,172
1629,ablation-analysis,"It shows when the data is limited , our separation component tends to capture weak style information , such as color and lighting .","[('shows when', (1, 3)), ('is', (5, 6)), ('tends to capture', (11, 14)), ('such as', (18, 20))]","[('data', (4, 5)), ('our separation component', (8, 11)), ('weak style information', (14, 17)), ('color and lighting', (20, 23))]","[['our separation component', 'tends to capture', 'weak style information'], ['weak style information', 'such as', 'color and lighting']]","[['data', 'has', 'our separation component']]","[['Ablation analysis', 'shows when', 'data']]",[],face_alignment,17,215
1630,research-problem,Deep Alignment Network : A convolutional neural network for robust face alignment,"[('for', (8, 9))]","[('Deep Alignment Network', (0, 3)), ('convolutional neural network', (5, 8))]",[],"[['Deep Alignment Network', 'has', 'convolutional neural network']]",[],[],face_alignment,18,2
1631,research-problem,"Face alignment is an important component of many computer vision applications , such as face verification , facial emotion recognition , humancomputer interaction and facial motion capture .",[],"[('Face alignment', (0, 2))]",[],[],[],[],face_alignment,18,13
1632,model,The features are then used to iteratively refine the estimates of landmark locations .,"[('used to', (4, 6)), ('of', (10, 11))]","[('features', (1, 2)), ('iteratively refine', (6, 8)), ('estimates', (9, 10)), ('landmark locations', (11, 13))]","[['features', 'used to', 'iteratively refine'], ['estimates', 'of', 'landmark locations']]","[['iteratively refine', 'has', 'estimates']]",[],"[['Model', 'has', 'features']]",face_alignment,18,16
1633,model,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .","[('proposing', (10, 11)), ('dub', (18, 19))]","[('novel face alignment method', (12, 16)), ('Deep Alignment Network ( DAN )', (19, 25))]","[['novel face alignment method', 'dub', 'Deep Alignment Network ( DAN )']]","[['novel face alignment method', 'name', 'Deep Alignment Network ( DAN )']]","[['Model', 'proposing', 'novel face alignment method']]",[],face_alignment,18,19
1634,model,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .","[('based on', (2, 4)), ('where', (8, 9)), ('refines', (11, 12)), ('estimated at', (15, 17)), ('iteratively improving', (21, 23))]","[('multistage neural network', (5, 8)), ('each stage', (9, 11)), ('landmark positions', (13, 15)), ('previous stage', (18, 20)), ('landmark locations', (24, 26))]","[['multistage neural network', 'where', 'each stage'], ['each stage', 'refines', 'landmark positions'], ['landmark positions', 'estimated at', 'previous stage'], ['landmark positions', 'iteratively improving', 'landmark locations']]","[['multistage neural network', 'has', 'each stage']]","[['Model', 'based on', 'multistage neural network']]",[],face_alignment,18,20
1635,model,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .","[('To make use of', (0, 4)), ('during', (8, 9)), ('of', (11, 12)), ('input at', (17, 19)), ('of', (30, 31))]","[('entire face image', (5, 8)), ('process', (10, 11)), ('face alignment', (12, 14)), ('each stage', (19, 21)), ('landmark heatmap', (22, 24)), ('key element', (28, 30))]","[['entire face image', 'during', 'process'], ['process', 'of', 'face alignment']]","[['each stage', 'has', 'landmark heatmap']]","[['Model', 'To make use of', 'entire face image']]",[],face_alignment,18,22
1636,model,The convolutional neural network can use the heatmaps to infer the current estimates of landmark locations in the image and thus refine them .,"[('use', (5, 6)), ('to infer', (8, 10)), ('of', (13, 14)), ('in', (16, 17))]","[('convolutional neural network', (1, 4)), ('heatmaps', (7, 8)), ('current estimates', (11, 13)), ('landmark locations', (14, 16)), ('image', (18, 19))]","[['convolutional neural network', 'use', 'heatmaps'], ['heatmaps', 'to infer', 'current estimates'], ['current estimates', 'of', 'landmark locations'], ['landmark locations', 'in', 'image']]",[],[],"[['Model', 'has', 'convolutional neural network']]",face_alignment,18,24
1637,model,We introduce landmark heatmaps which transfer the information about current landmark location estimates between the stages of our method .,"[('introduce', (1, 2)), ('transfer', (5, 6)), ('about', (8, 9)), ('between', (13, 14))]","[('landmark heatmaps', (2, 4)), ('information', (7, 8)), ('current landmark location estimates', (9, 13)), ('stages of our method', (15, 19))]","[['landmark heatmaps', 'transfer', 'information'], ['information', 'about', 'current landmark location estimates'], ['current landmark location estimates', 'between', 'stages of our method']]",[],"[['Model', 'introduce', 'landmark heatmaps']]",[],face_alignment,18,29
1638,model,"This improvement allows our method to make use of the entire image of a face , instead of local patches , and avoid falling into local minima .","[('allows', (2, 3)), ('to make use of', (5, 9)), ('of', (12, 13)), ('instead of', (16, 18)), ('avoid falling into', (22, 25))]","[('our method', (3, 5)), ('entire image', (10, 12)), ('face', (14, 15)), ('local patches', (18, 20)), ('local minima', (25, 27))]","[['our method', 'to make use of', 'entire image'], ['entire image', 'of', 'face'], ['face', 'instead of', 'local patches'], ['our method', 'avoid falling into', 'local minima']]",[],"[['Model', 'allows', 'our method']]",[],face_alignment,18,30
1639,baselines,"We train two models , DAN which is trained on the training subset of the 300W competition data and DAN - Menpo which is trained on both the above mentioned dataset and the Menpo challenge training set .","[('train', (1, 2)), ('trained on', (8, 10)), ('of', (13, 14)), ('trained on', (24, 26))]","[('two models', (2, 4)), ('DAN', (5, 6)), ('training subset', (11, 13)), ('300W competition data', (15, 18)), ('DAN - Menpo', (19, 22)), ('Menpo challenge training set', (33, 37))]","[['DAN', 'trained on', 'training subset'], ['training subset', 'of', '300W competition data'], ['two models', 'trained on', 'DAN - Menpo'], ['DAN - Menpo', 'trained on', 'Menpo challenge training set']]","[['two models', 'name', 'DAN']]","[['Baselines', 'train', 'two models']]",[],face_alignment,18,191
1640,experimental-setup,"Data augmentation is performed by mirroring around the Y axis as well as random translation , rotation and scaling , all sampled from normal distributions .","[('performed by', (3, 5)), ('around', (6, 7)), ('as well as', (10, 13)), ('sampled from', (21, 23))]","[('Data augmentation', (0, 2)), ('mirroring', (5, 6)), ('Y axis', (8, 10)), ('random translation', (13, 15)), ('rotation and', (16, 18)), ('scaling', (18, 19)), ('normal distributions', (23, 25))]","[['Data augmentation', 'performed by', 'mirroring'], ['Data augmentation', 'performed by', 'random translation'], ['Data augmentation', 'performed by', 'scaling'], ['mirroring', 'around', 'Y axis'], ['mirroring', 'as well as', 'random translation'], ['mirroring', 'as well as', 'scaling'], ['Y axis', 'as well as', 'random translation'], ['random translation', 'sampled from', 'normal distributions'], ['scaling', 'sampled from', 'normal distributions']]","[['rotation and', 'has', 'scaling']]",[],"[['Experimental setup', 'has', 'Data augmentation']]",face_alignment,18,192
1641,baselines,Both models ( DAN and DAN - Menpo ) consist of two stages .,"[('consist of', (9, 11))]","[('two stages', (11, 13))]",[],[],"[['Baselines', 'consist of', 'two stages']]",[],face_alignment,18,194
1642,experimental-setup,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,"[('performed using', (2, 4))]","[('Training', (0, 1)), ('Theano 0.9.0', (4, 6)), ('Lasagne 0.2', (7, 9))]","[['Training', 'performed using', 'Theano 0.9.0'], ['Training', 'performed using', 'Lasagne 0.2']]",[],[],"[['Experimental setup', 'has', 'Training']]",face_alignment,18,195
1643,experimental-setup,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,"[('For', (0, 1)), ('use', (3, 4)), ('with', (7, 8))]","[('optimization', (1, 2)), ('Adam stochastic optimization', (4, 7)), ('initial step size', (9, 12)), ('0.001', (13, 14)), ('mini batch size', (15, 18)), ('64', (19, 20))]","[['optimization', 'use', 'Adam stochastic optimization'], ['Adam stochastic optimization', 'with', 'initial step size'], ['Adam stochastic optimization', 'with', 'mini batch size']]","[['mini batch size', 'has', '64']]","[['Experimental setup', 'For', 'optimization']]",[],face_alignment,18,196
1644,experimental-setup,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,"[('runs at', (3, 5)), ('for', (7, 8)), ('processed in', (9, 11)), ('for', (16, 17)), ('processed', (18, 19)), ('on', (20, 21))]","[('Python implementation', (1, 3)), ('73 fps', (5, 7)), ('images', (8, 9)), ('45 fps', (14, 16)), ('images', (17, 18)), ('sequentially', (19, 20)), ('GeForce GTX 1070 GPU', (22, 26))]","[['Python implementation', 'runs at', '73 fps'], ['Python implementation', 'runs at', '45 fps'], ['73 fps', 'for', 'images'], ['45 fps', 'for', 'images'], ['45 fps', 'for', 'images'], ['45 fps', 'processed', 'sequentially'], ['images', 'processed', 'sequentially'], ['sequentially', 'on', 'GeForce GTX 1070 GPU']]",[],[],"[['Experimental setup', 'has', 'Python implementation']]",face_alignment,18,198
1645,experimental-setup,For each test set we initialize our method using the face detector bounding boxes provided with the datasets .,"[('For', (0, 1)), ('initialize', (5, 6)), ('using', (8, 9)), ('provided with', (14, 16))]","[('each test set', (1, 4)), ('our method', (6, 8)), ('face detector bounding boxes', (10, 14)), ('datasets', (17, 18))]","[['each test set', 'initialize', 'our method'], ['our method', 'using', 'face detector bounding boxes'], ['face detector bounding boxes', 'provided with', 'datasets']]",[],"[['Experimental setup', 'For', 'each test set']]",[],face_alignment,18,206
1646,results,Results on the Menpo challenge test set,"[('on', (1, 2))]",[],[],[],[],[],face_alignment,18,214
1647,experimental-setup,The first step performs face alignment using a square initialization bounding box placed in the middle of the image with a size set to a percentage of image height .,"[('performs', (3, 4)), ('using', (6, 7)), ('placed in the', (12, 15)), ('with', (19, 20)), ('set to', (22, 24))]","[('first', (1, 2)), ('face alignment', (4, 6)), ('square initialization bounding box', (8, 12)), ('middle', (15, 16)), ('image', (18, 19)), ('size', (21, 22)), ('percentage of image height', (25, 29))]","[['first', 'performs', 'face alignment'], ['face alignment', 'using', 'square initialization bounding box'], ['square initialization bounding box', 'placed in the', 'middle'], ['image', 'with', 'size'], ['size', 'set to', 'percentage of image height']]",[],[],"[['Experimental setup', 'has', 'first']]",face_alignment,18,218
1648,experimental-setup,The chosen bounding box size was 46 % of the image height .,"[('chosen', (1, 2)), ('was', (5, 6)), ('of', (8, 9))]","[('bounding box size', (2, 5)), ('46 %', (6, 8)), ('image height', (10, 12))]","[['bounding box size', 'was', '46 %'], ['46 %', 'of', 'image height']]","[['bounding box size', 'has', '46 %']]","[['Experimental setup', 'chosen', 'bounding box size']]",[],face_alignment,18,224
1649,experimental-setup,For the AUC and the failure rate we have chosen a threshold of 0.03 of the bounding box diagonal as it is approximately equivalent to 0.08 of the interocular distance used in the previous chapter .,"[('For', (0, 1)), ('chosen', (9, 10)), ('of', (12, 13)), ('of', (14, 15))]","[('AUC and the failure rate', (2, 7)), ('threshold', (11, 12)), ('0.03', (13, 14)), ('bounding box diagonal', (16, 19))]","[['AUC and the failure rate', 'chosen', 'threshold'], ['threshold', 'of', '0.03'], ['0.03', 'of', 'bounding box diagonal'], ['0.03', 'of', 'bounding box diagonal']]","[['AUC and the failure rate', 'has', 'threshold']]","[['Experimental setup', 'For', 'AUC and the failure rate']]",[],face_alignment,18,227
1650,results,The addition of the second stage increases the AUC 0.08 by 20 % while the mean error and failure rate are reduced by 14 % and 56 % respectively .,"[('addition of', (1, 3)), ('increases', (6, 7)), ('by', (10, 11)), ('reduced by', (21, 23))]","[('second stage', (4, 6)), ('AUC', (8, 9)), ('0.08', (9, 10)), ('20 %', (11, 13)), ('mean error and failure rate', (15, 20)), ('14 % and 56 %', (23, 28))]","[['second stage', 'increases', 'AUC'], ['0.08', 'by', '20 %'], ['mean error and failure rate', 'reduced by', '14 % and 56 %']]","[['AUC', 'has', '0.08']]","[['Results', 'addition of', 'second stage']]",[],face_alignment,18,234
1651,results,The addition of a third stage does not bring significant benefit in any of the metrics .,"[('addition of', (1, 3)), ('does not bring', (6, 9)), ('in', (11, 12))]","[('third stage', (4, 6)), ('significant benefit', (9, 11)), ('any of', (12, 14))]","[['third stage', 'does not bring', 'significant benefit'], ['significant benefit', 'in', 'any of']]",[],[],[],face_alignment,18,235
1652,research-problem,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild,[],"[('Face Alignment In', (6, 9))]",[],[],[],[],face_alignment,2,2
1653,research-problem,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .",[],"[('Face Alignment', (0, 2))]",[],[],[],[],face_alignment,2,4
1654,research-problem,"In this paper , we introduce DeCaFA , an end - to - end deep convolutional cascade architecture for face alignment .","[('introduce', (5, 6)), ('for', (18, 19))]","[('DeCaFA', (6, 7)), ('end - to - end deep convolutional cascade architecture', (9, 18)), ('face alignment', (19, 21))]","[['end - to - end deep convolutional cascade architecture', 'for', 'face alignment']]","[['DeCaFA', 'has', 'end - to - end deep convolutional cascade architecture']]","[['Research problem', 'introduce', 'DeCaFA']]",[],face_alignment,2,6
1655,research-problem,"We show experimentally that DeCaFA significantly outperforms existing approaches on 300W , CelebA and WFLW databases .","[('show', (1, 2)), ('on', (9, 10))]","[('DeCaFA', (4, 5)), ('significantly outperforms', (5, 7)), ('existing approaches', (7, 9)), ('300W , CelebA and WFLW databases', (10, 16))]","[['significantly outperforms', 'on', '300W , CelebA and WFLW databases'], ['existing approaches', 'on', '300W , CelebA and WFLW databases']]","[['DeCaFA', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'existing approaches']]","[['Research problem', 'show', 'DeCaFA']]",[],face_alignment,2,10
1656,model,"This allows to robustly learn rigid transformations , such as translation and rotation , in the first cascade stages , while learning non-rigid deformation ( e.g. due to facial expression or non-planar rotation ) later on .","[('such as', (8, 10)), ('in', (14, 15)), ('due to', (26, 28))]","[('robustly learn', (3, 5)), ('rigid transformations', (5, 7)), ('translation and rotation', (10, 13)), ('first cascade stages', (16, 19)), ('learning', (21, 22)), ('non-rigid deformation', (22, 24)), ('facial expression', (28, 30)), ('non-planar rotation', (31, 33))]","[['rigid transformations', 'such as', 'translation and rotation'], ['rigid transformations', 'in', 'first cascade stages'], ['translation and rotation', 'in', 'first cascade stages'], ['non-rigid deformation', 'due to', 'facial expression'], ['non-rigid deformation', 'due to', 'non-planar rotation']]","[['robustly learn', 'has', 'rigid transformations'], ['learning', 'has', 'non-rigid deformation']]",[],[],face_alignment,2,17
1657,model,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .","[('introduce', (5, 6))]","[('Deep convolutional Cascade for Face Alignment ( DeCaFA )', (7, 16))]",[],[],"[['Model', 'introduce', 'Deep convolutional Cascade for Face Alignment ( DeCaFA )']]",[],face_alignment,2,22
1658,model,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .","[('composed of', (2, 4)), ('each produce', (7, 9)), ('relatively to', (15, 17))]","[('DeCaFA', (0, 1)), ('several stages', (4, 6)), ('landmark - wise attention maps', (9, 14)), ('heterogeneous annotation markups', (17, 20))]","[['DeCaFA', 'composed of', 'several stages'], ['several stages', 'each produce', 'landmark - wise attention maps'], ['landmark - wise attention maps', 'relatively to', 'heterogeneous annotation markups']]",[],[],"[['Model', 'has', 'DeCaFA']]",face_alignment,2,23
1659,model,"We introduce a fully - convolutional Deep Cascade for Face Alignment ( DeCaFA ) that unifies cascaded regression and end - to - end deep approaches , by using landmark - wise attention maps fused to extract local information around a current landmark estimate .","[('introduce', (1, 2)), ('unifies', (15, 16)), ('by using', (27, 29)), ('fused to extract', (34, 37)), ('around', (39, 40))]","[('fully - convolutional Deep Cascade for', (3, 9)), ('Face Alignment ( DeCaFA )', (9, 14)), ('cascaded regression and', (16, 19)), ('end - to - end deep approaches', (19, 26)), ('landmark - wise attention maps', (29, 34)), ('local information', (37, 39)), ('current landmark estimate', (41, 44))]","[['fully - convolutional Deep Cascade for', 'unifies', 'cascaded regression and'], ['fully - convolutional Deep Cascade for', 'unifies', 'end - to - end deep approaches'], ['Face Alignment ( DeCaFA )', 'unifies', 'cascaded regression and'], ['Face Alignment ( DeCaFA )', 'unifies', 'end - to - end deep approaches'], ['end - to - end deep approaches', 'by using', 'landmark - wise attention maps'], ['landmark - wise attention maps', 'fused to extract', 'local information'], ['local information', 'around', 'current landmark estimate']]","[['fully - convolutional Deep Cascade for', 'name', 'Face Alignment ( DeCaFA )'], ['cascaded regression and', 'has', 'end - to - end deep approaches']]","[['Model', 'introduce', 'fully - convolutional Deep Cascade for']]",[],face_alignment,2,27
1660,model,"We show that intermediate supervision with increasing weights helps DeCaFA to learn coarse attention maps in its early stages , that are refined in the later stages .","[('show', (1, 2)), ('with', (5, 6)), ('helps', (8, 9)), ('to learn', (10, 12)), ('in', (15, 16))]","[('intermediate supervision', (3, 5)), ('increasing weights', (6, 8)), ('DeCaFA', (9, 10)), ('coarse attention maps', (12, 15)), ('early stages', (17, 19))]","[['intermediate supervision', 'with', 'increasing weights'], ['increasing weights', 'helps', 'DeCaFA'], ['DeCaFA', 'to learn', 'coarse attention maps'], ['coarse attention maps', 'in', 'early stages']]","[['intermediate supervision', 'has', 'increasing weights']]","[['Model', 'show', 'intermediate supervision']]",[],face_alignment,2,28
1661,model,"Through chaining multiple transfer layers , DeCaFA integrates heterogeneous data annotated with different numbers of landmarks and model the intrinsic relationship between these tasks .","[('Through chaining', (0, 2)), ('integrates', (7, 8)), ('annotated with', (10, 12)), ('model', (17, 18))]","[('multiple transfer layers', (2, 5)), ('DeCaFA', (6, 7)), ('heterogeneous data', (8, 10)), ('different numbers of landmarks', (12, 16)), ('intrinsic relationship', (19, 21)), ('tasks', (23, 24))]","[['DeCaFA', 'integrates', 'heterogeneous data'], ['heterogeneous data', 'annotated with', 'different numbers of landmarks'], ['heterogeneous data', 'model', 'intrinsic relationship']]","[['multiple transfer layers', 'has', 'DeCaFA']]","[['Model', 'Through chaining', 'multiple transfer layers']]",[],face_alignment,2,29
1662,hyperparameters,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .","[('use', (8, 9)), ('each contains', (14, 16)), ('with', (21, 22)), ('for', (34, 35))]","[('DeCaFA models', (1, 3)), ('1 to 4 stages', (9, 13)), ('12 3 3 convolutional layers', (16, 21)), ('64 ? 64 ? 128 ? 128 ? 256 ? 256 channels', (22, 34)), ('downsampling portion', (36, 38)), ('upsampling portion', (45, 47))]","[['DeCaFA models', 'use', '1 to 4 stages'], ['1 to 4 stages', 'each contains', '12 3 3 convolutional layers'], ['12 3 3 convolutional layers', 'with', '64 ? 64 ? 128 ? 128 ? 256 ? 256 channels'], ['64 ? 64 ? 128 ? 128 ? 256 ? 256 channels', 'for', 'downsampling portion']]",[],[],"[['Hyperparameters', 'has', 'DeCaFA models']]",face_alignment,2,130
1663,hyperparameters,Each convolution is followed by a batch normalization layer with ReLU activation .,"[('followed by', (3, 5)), ('with', (9, 10))]","[('Each convolution', (0, 2)), ('batch normalization layer', (6, 9)), ('ReLU activation', (10, 12))]","[['Each convolution', 'followed by', 'batch normalization layer'], ['batch normalization layer', 'with', 'ReLU activation']]",[],[],"[['Hyperparameters', 'has', 'Each convolution']]",face_alignment,2,132
1664,hyperparameters,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,"[('to generate', (2, 4)), ('do not use', (8, 11)), ('bilinear', (14, 15)), ('followed with', (17, 19))]","[('smooth feature maps', (4, 7)), ('transposed convolution', (11, 13)), ('image upsampling', (15, 17)), ('3 3 convolutional layers', (19, 23))]","[['smooth feature maps', 'do not use', 'transposed convolution'], ['smooth feature maps', 'bilinear', 'image upsampling'], ['image upsampling', 'followed with', '3 3 convolutional layers']]",[],"[['Hyperparameters', 'to generate', 'smooth feature maps']]",[],face_alignment,2,133
1665,hyperparameters,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,"[('trained using', (4, 6)), ('with', (8, 9)), ('with', (15, 16)), ('annealing with', (21, 23))]","[('whole architecture', (1, 3)), ('ADAM optimizer', (6, 8)), ('5e ? 4 learning rate', (10, 15)), ('momentum 0.9', (16, 18)), ('learning rate', (19, 21)), ('power 0.9', (23, 25))]","[['whole architecture', 'trained using', 'ADAM optimizer'], ['ADAM optimizer', 'with', '5e ? 4 learning rate'], ['ADAM optimizer', 'with', 'learning rate'], ['5e ? 4 learning rate', 'with', 'momentum 0.9'], ['5e ? 4 learning rate', 'with', 'momentum 0.9'], ['5e ? 4 learning rate', 'with', 'learning rate'], ['learning rate', 'annealing with', 'power 0.9']]",[],[],"[['Hyperparameters', 'has', 'whole architecture']]",face_alignment,2,134
1666,experiments,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .","[('apply', (1, 2)), ('with', (4, 5)), ('for', (8, 9))]","[('400000 updates', (2, 4)), ('batch size 8', (5, 8)), ('each database', (9, 11))]","[['400000 updates', 'with', 'batch size 8'], ['batch size 8', 'for', 'each database']]",[],[],[],face_alignment,2,135
1667,ablation-analysis,"The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .","[('as', (4, 5)), ('add', (6, 7)), ('after', (12, 13)), ('on', (15, 16))]","[('accuracy', (1, 2)), ('steadily increases', (2, 4)), ('more stages', (7, 9)), ('saturates', (11, 12)), ('third', (14, 15)), ('LFPW and HELEN', (16, 19))]","[['accuracy', 'add', 'more stages'], ['steadily increases', 'add', 'more stages'], ['saturates', 'after', 'third'], ['third', 'on', 'LFPW and HELEN']]","[['accuracy', 'has', 'steadily increases'], ['saturates', 'has', 'third']]",[],"[['Ablation analysis', 'has', 'accuracy']]",face_alignment,2,153
1668,ablation-analysis,"On IBUG , this difference is more conspicuous , thus there is for improvement by stacking more cascade stages .","[('On', (0, 1)), ('is', (5, 6)), ('by', (14, 15))]","[('IBUG', (1, 2)), ('difference', (4, 5)), ('more conspicuous', (6, 8)), ('improvement', (13, 14)), ('stacking', (15, 16)), ('more cascade stages', (16, 19))]","[['difference', 'is', 'more conspicuous'], ['improvement', 'by', 'stacking']]","[['IBUG', 'has', 'difference'], ['difference', 'has', 'more conspicuous'], ['stacking', 'has', 'more cascade stages']]","[['Ablation analysis', 'On', 'IBUG']]",[],face_alignment,2,154
1669,ablation-analysis,"Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .","[('significantly helps', (7, 9))]","[('Coarsely annotated data ( 5 landmarks )', (0, 7)), ('fine - grained landmark localization', (10, 15))]","[['Coarsely annotated data ( 5 landmarks )', 'significantly helps', 'fine - grained landmark localization']]",[],[],"[['Ablation analysis', 'has', 'Coarsely annotated data ( 5 landmarks )']]",face_alignment,2,156
1670,ablation-analysis,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .","[('reinjecting', (2, 3)), ('significantly', (18, 19)), ('on', (22, 23)), ('such as', (25, 27))]","[('whole input image', (4, 7)), ('accuracy', (21, 22)), ('challenging data', (23, 25)), ('300 W - challenging', (27, 31)), ('WFLW - pose', (32, 35))]","[['accuracy', 'on', 'challenging data'], ['challenging data', 'such as', '300 W - challenging'], ['challenging data', 'such as', 'WFLW - pose']]","[['whole input image', 'name', 'accuracy']]","[['Ablation analysis', 'reinjecting', 'whole input image']]",[],face_alignment,2,160
1671,ablation-analysis,F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,"[('using', (15, 16)), ('rivals', (20, 21))]","[('local + global information', (16, 20)), ('basic deep approach', (22, 25))]","[['local + global information', 'rivals', 'basic deep approach']]",[],[],[],face_alignment,2,161
1672,ablation-analysis,"Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .","[('is', (14, 15)), ('by', (17, 18))]","[('F 5 - Equation fusion', (2, 7)), ('local and global cues', (10, 14)), ('best', (16, 17)), ('significant margin', (19, 21))]","[['local and global cues', 'is', 'best'], ['best', 'by', 'significant margin']]",[],[],"[['Ablation analysis', 'has', 'F 5 - Equation fusion']]",face_alignment,2,162
1673,ablation-analysis,shows a comparison between DeCaFA and recent state - of - the - art approaches on 300W database .,"[('on', (15, 16))]","[('300W database', (16, 18))]",[],[],"[['Ablation analysis', 'on', '300W database']]",[],face_alignment,2,165
1674,ablation-analysis,"Our approach performs better than most existing approaches on the common subset , and performs very close to its best contenders on the challenging subset .","[('performs', (2, 3)), ('than', (4, 5)), ('on', (8, 9)), ('performs', (14, 15)), ('to', (17, 18)), ('on', (21, 22))]","[('Our approach', (0, 2)), ('better', (3, 4)), ('most existing approaches', (5, 8)), ('common subset', (10, 12)), ('very close', (15, 17)), ('its', (18, 19)), ('best contenders', (19, 21)), ('challenging subset', (23, 25))]","[['Our approach', 'performs', 'better'], ['better', 'than', 'most existing approaches'], ['most existing approaches', 'on', 'common subset'], ['best contenders', 'on', 'challenging subset'], ['Our approach', 'performs', 'very close'], ['very close', 'to', 'its'], ['very close', 'to', 'best contenders'], ['best contenders', 'on', 'challenging subset']]","[['its', 'has', 'best contenders']]",[],"[['Ablation analysis', 'has', 'Our approach']]",face_alignment,2,166
1675,ablation-analysis,"Note that DeCaFA trained only on 300 W trainset has a ME of 3.69 % and is already very competitive with recent approaches , thanks to its end - to - end cascade architecture .","[('trained only on', (3, 6)), ('of', (12, 13)), ('thanks to', (24, 26))]","[('DeCaFA', (2, 3)), ('300 W trainset', (6, 9)), ('ME', (11, 12)), ('3.69 %', (13, 15)), ('very competitive', (18, 20)), ('end - to - end cascade architecture', (27, 34))]","[['DeCaFA', 'trained only on', '300 W trainset'], ['ME', 'of', '3.69 %']]",[],[],"[['Ablation analysis', 'has', 'DeCaFA']]",face_alignment,2,167
1676,ablation-analysis,"DeCaFA is competitive with the best approaches , LAB and DAN - MENPO as well as JMFA - MENPO , which also use external data .","[('with', (3, 4)), ('as well as', (13, 16)), ('use', (22, 23))]","[('DeCaFA', (0, 1)), ('competitive', (2, 3)), ('best approaches', (5, 7)), ('LAB and DAN - MENPO', (8, 13)), ('JMFA - MENPO', (16, 19)), ('external data', (23, 25))]","[['competitive', 'with', 'best approaches'], ['LAB and DAN - MENPO', 'as well as', 'JMFA - MENPO'], ['JMFA - MENPO', 'use', 'external data']]","[['DeCaFA', 'has', 'competitive']]",[],"[['Ablation analysis', 'has', 'DeCaFA']]",face_alignment,2,168
1677,ablation-analysis,DeCaFA performs better than LAB and Wing by a significant margin on every subset .,"[('performs', (1, 2)), ('than', (3, 4)), ('by', (7, 8)), ('on', (11, 12))]","[('DeCaFA', (0, 1)), ('better', (2, 3)), ('LAB and Wing', (4, 7)), ('significant margin', (9, 11)), ('every subset', (12, 14))]","[['DeCaFA', 'performs', 'better'], ['better', 'than', 'LAB and Wing'], ['DeCaFA', 'by', 'significant margin'], ['LAB and Wing', 'by', 'significant margin'], ['significant margin', 'on', 'every subset']]",[],[],"[['Ablation analysis', 'has', 'DeCaFA']]",face_alignment,2,172
1678,ablation-analysis,"Also , note that DeCaFA trained solely on WFLW already as a ME of 5.01 on the whole test set , which is still better that these two methods .","[('note', (2, 3)), ('trained solely on', (5, 8)), ('as', (10, 11)), ('of', (13, 14)), ('on', (15, 16))]","[('DeCaFA', (4, 5)), ('WFLW', (8, 9)), ('ME', (12, 13)), ('5.01', (14, 15)), ('whole test set', (17, 20)), ('still better', (23, 25))]","[['DeCaFA', 'trained solely on', 'WFLW'], ['WFLW', 'as', 'ME'], ['ME', 'of', '5.01'], ['5.01', 'on', 'whole test set']]",[],"[['Ablation analysis', 'note', 'DeCaFA']]",[],face_alignment,2,173
1679,ablation-analysis,"Method Mean error ( % ) SDM 4.35 CFSS 3,95 DSRN 3.08 AAN 2.99 DeCaFA 2.10 approach is the best by a significant margin .","[('is', (17, 18)), ('by', (20, 21))]","[('Method Mean error ( % )', (0, 6)), ('SDM', (6, 7)), ('best', (19, 20)), ('significant margin', (22, 24))]","[['best', 'by', 'significant margin']]","[['Method Mean error ( % )', 'has', 'SDM']]",[],"[['Ablation analysis', 'has', 'Method Mean error ( % )']]",face_alignment,2,179
1680,ablation-analysis,"Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .","[('sets', (3, 4)), ('on', (13, 14)), ('with', (17, 18))]","[('new state - of - the - art', (5, 13)), ('three databases', (15, 17)), ('several evaluation metrics', (18, 21))]","[['new state - of - the - art', 'on', 'three databases'], ['three databases', 'with', 'several evaluation metrics']]",[],[],[],face_alignment,2,181
1681,ablation-analysis,"A allows to substantially improve the landmark localization on both datasets , most notably when the number of training images is very low .","[('on', (8, 9)), ('is', (20, 21))]","[('A', (0, 1)), ('substantially improve', (3, 5)), ('landmark localization', (6, 8)), ('both datasets', (9, 11)), ('number of training images', (16, 20)), ('very low', (21, 23))]","[['landmark localization', 'on', 'both datasets'], ['number of training images', 'is', 'very low']]","[['A', 'has', 'substantially improve'], ['substantially improve', 'has', 'landmark localization'], ['number of training images', 'has', 'very low']]",[],"[['Ablation analysis', 'has', 'A']]",face_alignment,2,188
1682,ablation-analysis,"DeCaFA trained with 15 % of 300 W trainset and 6 % of WFLW trainset is on par with SAN on 300W ( , see ) , and is substantially better than DVLN on WFLW .","[('trained with', (1, 3)), ('with', (18, 19)), ('on', (20, 21)), ('than', (31, 32)), ('on', (33, 34))]","[('DeCaFA', (0, 1)), ('15 % of 300 W trainset', (3, 9)), ('6 % of WFLW trainset', (10, 15)), ('on par', (16, 18)), ('SAN', (19, 20)), ('300W', (21, 22)), ('substantially better', (29, 31)), ('DVLN', (32, 33)), ('WFLW', (34, 35))]","[['DeCaFA', 'trained with', '15 % of 300 W trainset'], ['on par', 'with', 'SAN'], ['SAN', 'on', '300W'], ['substantially better', 'than', 'DVLN'], ['DVLN', 'on', 'WFLW']]",[],[],"[['Ablation analysis', 'has', 'DeCaFA']]",face_alignment,2,190
1683,results,"Also notice that the predicted landmarks are close to the corresponding ground truth , even in the presence of rotations and occlusions ( WFLW ) or facial expressions ( CelebA ) .","[('notice', (1, 2)), ('close to', (7, 9)), ('in the presence of', (15, 19))]","[('predicted landmarks', (4, 6)), ('corresponding ground truth', (10, 13)), ('rotations and occlusions ( WFLW )', (19, 25)), ('facial expressions ( CelebA )', (26, 31))]","[['predicted landmarks', 'close to', 'corresponding ground truth'], ['predicted landmarks', 'in the presence of', 'facial expressions ( CelebA )'], ['corresponding ground truth', 'in the presence of', 'rotations and occlusions ( WFLW )']]","[['predicted landmarks', 'has', 'corresponding ground truth']]","[['Results', 'notice', 'predicted landmarks']]",[],face_alignment,2,195
1684,research-problem,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,[],"[('Robust Face Alignment', (4, 7))]",[],[],[],[],face_alignment,3,2
1685,code,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.,[],[],[],[],[],[],face_alignment,3,13
1686,research-problem,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .","[('known', (4, 5))]","[('Face alignment', (0, 2)), ('facial landmark localization', (6, 9))]",[],[],[],[],face_alignment,3,15
1687,research-problem,"Face alignment plays an essential role in many face related applications such as face recognition , face frontalization and 3D face reconstruction .",[],"[('Face alignment', (0, 2))]",[],[],[],[],face_alignment,3,16
1688,experiments,"As a result of i ) and ii ) , models trained with the MSE loss tend to predict a blurry and dilated heatmap with low intensity on foreground pixels compared to the ground truth ( .","[('trained with', (11, 13)), ('tend to predict', (16, 19)), ('with', (24, 25)), ('on', (27, 28)), ('compared to', (30, 32))]","[('models', (10, 11)), ('MSE loss', (14, 16)), ('blurry and dilated heatmap', (20, 24)), ('low intensity', (25, 27)), ('foreground pixels', (28, 30)), ('ground truth', (33, 35))]","[['models', 'trained with', 'MSE loss'], ['MSE loss', 'tend to predict', 'blurry and dilated heatmap'], ['blurry and dilated heatmap', 'with', 'low intensity'], ['low intensity', 'on', 'foreground pixels'], ['low intensity', 'compared to', 'ground truth']]",[],[],[],face_alignment,3,26
1689,model,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .","[('propose', (2, 3)), ('name it', (8, 10)), ('able to', (18, 20)), ('of', (24, 25))]","[('new loss function', (4, 7)), ('Adaptive Wing loss', (10, 13)), ('significantly improve', (20, 22)), ('quality', (23, 24)), ('heatmap regression results', (25, 28))]","[['Adaptive Wing loss', 'able to', 'significantly improve'], ['quality', 'of', 'heatmap regression results']]","[['significantly improve', 'has', 'quality']]","[['Model', 'propose', 'new loss function']]",[],face_alignment,3,30
1690,model,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .","[('Due to', (0, 2)), ('of', (5, 6)), ('in', (9, 10)), ('such as', (19, 21)), ('for', (41, 42))]","[('translation invariance', (3, 5)), ('convolution operation', (7, 9)), ('bottom - up and top - down CNN structures', (10, 19)), ('stacked Hourglass ( HG )', (21, 26)), ('network', (28, 29)), ('coordinate information', (33, 35)), ('facial landmark localization', (42, 45))]","[['translation invariance', 'of', 'convolution operation'], ['convolution operation', 'in', 'bottom - up and top - down CNN structures'], ['bottom - up and top - down CNN structures', 'such as', 'stacked Hourglass ( HG )']]","[['bottom - up and top - down CNN structures', 'name', 'stacked Hourglass ( HG )']]","[['Model', 'Due to', 'translation invariance']]",[],face_alignment,3,31
1691,model,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .","[('Inspired by', (0, 2)), ('encode into', (14, 16)), ('only', (25, 26)), ('on', (26, 27)), ('predicted from', (28, 30)), ('into', (34, 35))]","[('Coord - Conv layer', (3, 7)), ('our model', (16, 18)), ('full coordinate information', (19, 22)), ('information', (24, 25)), ('boundaries', (27, 28)), ('previous HG module', (31, 34)), ('our model', (35, 37))]","[['Coord - Conv layer', 'encode into', 'our model'], ['Coord - Conv layer', 'encode into', 'information'], ['information', 'on', 'boundaries'], ['boundaries', 'predicted from', 'previous HG module'], ['boundaries', 'into', 'our model']]","[['our model', 'has', 'full coordinate information']]","[['Model', 'Inspired by', 'Coord - Conv layer']]",[],face_alignment,3,32
1692,model,The encoded coordinate information further improves the performance of our approach .,"[('further improves', (4, 6)), ('of', (8, 9))]","[('encoded coordinate information', (1, 4)), ('performance', (7, 8)), ('our approach', (9, 11))]","[['encoded coordinate information', 'further improves', 'performance'], ['performance', 'of', 'our approach']]","[['encoded coordinate information', 'has', 'performance']]",[],"[['Model', 'has', 'encoded coordinate information']]",face_alignment,3,33
1693,model,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .","[('To encode', (0, 2)), ('add', (7, 8)), ('of', (10, 11)), ('by concatenating', (13, 15)), ('into', (19, 20)), ('jointly trained with', (26, 29))]","[('boundary coordinates', (2, 4)), ('sub-task', (9, 10)), ('boundary prediction', (11, 13)), ('additional boundary channel', (16, 19)), ('ground truth heatmap', (21, 24)), ('other channels', (29, 31))]","[['boundary coordinates', 'add', 'sub-task'], ['sub-task', 'of', 'boundary prediction'], ['boundary prediction', 'by concatenating', 'additional boundary channel'], ['additional boundary channel', 'into', 'ground truth heatmap'], ['ground truth heatmap', 'jointly trained with', 'other channels']]",[],"[['Model', 'To encode', 'boundary coordinates']]",[],face_alignment,3,34
1694,model,With proposed Weighted Loss Map it is also able to focus on foreground pixels and difficult background pixels during training .,"[('With', (0, 1)), ('during', (18, 19))]","[('proposed Weighted Loss Map', (1, 5)), ('focus', (10, 11)), ('foreground pixels', (12, 14)), ('difficult background pixels', (15, 18)), ('training', (19, 20))]","[['difficult background pixels', 'during', 'training']]",[],"[['Model', 'With', 'proposed Weighted Loss Map']]",[],face_alignment,3,38
1695,model,"Encode coordinate information , including coordinates on boundary , into the face alignment algorithm using CoordConv .","[('Encode', (0, 1)), ('including', (4, 5)), ('on', (6, 7)), ('into', (9, 10)), ('using', (14, 15))]","[('coordinate information', (1, 3)), ('coordinates', (5, 6)), ('boundary', (7, 8)), ('face alignment algorithm', (11, 14)), ('CoordConv', (15, 16))]","[['coordinate information', 'including', 'coordinates'], ['coordinates', 'on', 'boundary'], ['coordinate information', 'into', 'face alignment algorithm'], ['coordinate information', 'using', 'CoordConv'], ['face alignment algorithm', 'using', 'CoordConv']]","[['coordinate information', 'has', 'coordinates']]","[['Model', 'Encode', 'coordinate information']]",[],face_alignment,3,39
1696,experiments,"The reduced influence of correct estimations helps the network to stay converged , instead of oscillating like the L1 and the Wing loss .","[('of', (3, 4)), ('helps', (6, 7)), ('to stay', (9, 11)), ('instead of', (13, 15)), ('like', (16, 17))]","[('reduced influence', (1, 3)), ('correct estimations', (4, 6)), ('network', (8, 9)), ('converged', (11, 12)), ('oscillating', (15, 16)), ('L1 and the Wing loss', (18, 23))]","[['reduced influence', 'of', 'correct estimations'], ['reduced influence', 'of', 'oscillating'], ['reduced influence', 'helps', 'network'], ['correct estimations', 'helps', 'network'], ['network', 'to stay', 'converged'], ['converged', 'instead of', 'oscillating'], ['oscillating', 'like', 'L1 and the Wing loss']]","[['oscillating', 'has', 'L1 and the Wing loss']]",[],[],face_alignment,3,128
1697,ablation-analysis,"Difficult background pixels should also be focused on since these pixels are relatively difficult to regress , accurately regressing them could help narrow down the area of foreground pixels to improve localization accuracy . :","[('accurately', (17, 18)), ('to improve', (29, 31))]","[('Difficult background pixels', (0, 3)), ('focused', (6, 7)), ('area of', (25, 27)), ('foreground pixels', (27, 29)), ('localization accuracy', (31, 33))]",[],"[['Difficult background pixels', 'has', 'focused'], ['area of', 'has', 'foreground pixels']]",[],"[['Ablation analysis', 'has', 'Difficult background pixels']]",face_alignment,3,190
1698,hyperparameters,"For the WFLW dataset , the provided bounding boxes are not very accurate , to ensure all landmarks are preserved from cropping , we enlarge the bounding boxes by 10 % on both dimensions .","[('For', (0, 1)), ('are', (9, 10)), ('to ensure', (14, 16)), ('preserved from', (19, 21)), ('enlarge', (24, 25)), ('by', (28, 29)), ('on', (31, 32))]","[('WFLW dataset', (2, 4)), ('not very accurate', (10, 13)), ('all landmarks', (16, 18)), ('cropping', (21, 22)), ('bounding boxes', (26, 28)), ('10 %', (29, 31)), ('both dimensions', (32, 34))]","[['not very accurate', 'to ensure', 'all landmarks'], ['all landmarks', 'preserved from', 'cropping'], ['all landmarks', 'enlarge', 'bounding boxes'], ['bounding boxes', 'by', '10 %'], ['10 %', 'on', 'both dimensions']]","[['WFLW dataset', 'has', 'not very accurate']]","[['Hyperparameters', 'For', 'WFLW dataset']]",[],face_alignment,3,227
1699,hyperparameters,"The input of the network is 256 256 , the output of each stacked HG is 64 64 .","[('input of', (1, 3)), ('is', (5, 6)), ('output of', (10, 12)), ('is', (15, 16))]","[('network', (4, 5)), ('256 256', (6, 8)), ('each stacked HG', (12, 15)), ('64 64', (16, 18))]","[['network', 'is', '256 256'], ['each stacked HG', 'is', '64 64'], ['network', 'output of', 'each stacked HG'], ['256 256', 'output of', 'each stacked HG'], ['each stacked HG', 'is', '64 64']]",[],[],[],face_alignment,3,229
1700,hyperparameters,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .","[('During', (0, 1)), ('use', (4, 5)), ('with', (8, 9)), ('of', (13, 14))]","[('training', (1, 2)), ('RM - SProp', (5, 8)), ('initial learning rate', (10, 13)), ('1 10 ?4', (14, 17))]","[['training', 'use', 'RM - SProp'], ['RM - SProp', 'with', 'initial learning rate'], ['initial learning rate', 'of', '1 10 ?4']]",[],"[['Hyperparameters', 'During', 'training']]",[],face_alignment,3,231
1701,hyperparameters,We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,"[('set', (1, 2)), ('to be', (4, 6)), ('to be', (15, 17))]","[('momentum', (3, 4)), ('0', (6, 7)), ('weight decay', (13, 15)), ('1 10 ?5', (17, 20))]","[['momentum', 'to be', '0'], ['weight decay', 'to be', '1 10 ?5']]","[['momentum', 'has', '0'], ['weight decay', 'has', '1 10 ?5']]","[['Hyperparameters', 'set', 'momentum']]",[],face_alignment,3,232
1702,hyperparameters,"We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .","[('train for', (1, 3)), ('reduced to', (11, 13)), ('after', (21, 22))]","[('240 epoches', (3, 5)), ('learning rate', (8, 10)), ('1 10 ?5 and 1 10 ? 6', (13, 21)), ('80 and 160 epoches', (22, 26))]","[['learning rate', 'reduced to', '1 10 ?5 and 1 10 ? 6'], ['1 10 ?5 and 1 10 ? 6', 'after', '80 and 160 epoches']]","[['learning rate', 'has', '1 10 ?5 and 1 10 ? 6']]","[['Hyperparameters', 'train for', '240 epoches']]",[],face_alignment,3,233
1703,hyperparameters,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .","[('performed with', (3, 5))]","[('Data augmentation', (0, 2)), ('random rotation ( 50 )', (5, 10)), ('translation ( 25 px )', (11, 16)), ('flipping ( 50 % )', (17, 22)), ('rescaling ( 15 % )', (24, 29))]","[['Data augmentation', 'performed with', 'random rotation ( 50 )'], ['Data augmentation', 'performed with', 'translation ( 25 px )'], ['Data augmentation', 'performed with', 'flipping ( 50 % )'], ['Data augmentation', 'performed with', 'rescaling ( 15 % )']]",[],[],"[['Hyperparameters', 'has', 'Data augmentation']]",face_alignment,3,234
1704,hyperparameters,"Random Gaussian blur , noise and occlusion are also used .",[],"[('Random Gaussian blur', (0, 3)), ('noise and occlusion', (4, 7))]",[],[],[],[],face_alignment,3,235
1705,results,"14 . Our approach outperforms previous state - of - the - art by a significant margin , especially on the failure rate .","[('by', (13, 14)), ('especially on', (18, 20))]","[('Our approach', (2, 4)), ('outperforms', (4, 5)), ('previous state - of - the - art', (5, 13)), ('significant margin', (15, 17)), ('failure rate', (21, 23))]","[['outperforms', 'by', 'significant margin'], ['previous state - of - the - art', 'by', 'significant margin'], ['outperforms', 'especially on', 'failure rate'], ['previous state - of - the - art', 'especially on', 'failure rate'], ['significant margin', 'especially on', 'failure rate']]","[['Our approach', 'has', 'outperforms'], ['outperforms', 'has', 'previous state - of - the - art']]",[],"[['Results', 'has', 'Our approach']]",face_alignment,3,240
1706,experiments,We are able to reduce the failure rate measured at 10 % NME from 3.73 % to 0.99 % .,"[('measured at', (8, 10)), ('from', (13, 14)), ('to', (16, 17))]","[('reduce', (4, 5)), ('failure rate', (6, 8)), ('10 % NME', (10, 13)), ('3.73 %', (14, 16)), ('0.99 %', (17, 19))]","[['failure rate', 'measured at', '10 % NME'], ['10 % NME', 'from', '3.73 %'], ['3.73 %', 'to', '0.99 %']]","[['reduce', 'has', 'failure rate']]",[],[],face_alignment,3,241
1707,results,Our performance on the COFW shows the robustness of our approach against faces with large pose and heavy occlusion .,"[('on', (2, 3)), ('shows', (5, 6)), ('of', (8, 9)), ('against', (11, 12)), ('with', (13, 14))]","[('COFW', (4, 5)), ('robustness', (7, 8)), ('our approach', (9, 11)), ('faces', (12, 13)), ('large pose and heavy occlusion', (14, 19))]","[['COFW', 'shows', 'robustness'], ['robustness', 'of', 'our approach'], ['our approach', 'against', 'faces'], ['faces', 'with', 'large pose and heavy occlusion']]",[],"[['Results', 'on', 'COFW']]",[],face_alignment,3,243
1708,results,"Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .","[('able to achieve', (3, 6)), ('on', (15, 16))]","[('Our', (0, 1)), ('state - of - the - art performance', (7, 15)), ('300W testing dataset', (17, 20))]","[['Our', 'able to achieve', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', '300W testing dataset']]",[],[],"[['Results', 'has', 'Our']]",face_alignment,3,245
1709,results,"For the challenge subset ( iBug dataset ) , we are able to outperform","[('For', (0, 1)), ('able to', (11, 13))]","[('challenge subset ( iBug dataset )', (2, 8))]",[],[],"[['Results', 'For', 'challenge subset ( iBug dataset )']]",[],face_alignment,3,246
1710,results,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .","[('achieves', (3, 4)), ('on', (7, 8)), ('than', (18, 19))]","[('Our method', (0, 2)), ('best results', (5, 7)), ('WFLW dataset', (9, 11)), ('significantly more difficult', (15, 18)), ('COFW and 300W', (19, 22))]","[['Our method', 'achieves', 'best results'], ['best results', 'on', 'WFLW dataset'], ['significantly more difficult', 'than', 'COFW and 300W']]",[],[],"[['Results', 'has', 'Our method']]",face_alignment,3,251
1711,results,On every subset we outperform the previous state - of - the - art ap - :,"[('On', (0, 1))]","[('every subset', (1, 3)), ('outperform', (4, 5)), ('previous state - of - the - art', (6, 14))]",[],"[['every subset', 'has', 'outperform'], ['outperform', 'has', 'previous state - of - the - art']]","[['Results', 'On', 'every subset']]",[],face_alignment,3,252
1712,results,We are also able to reduce the failure rate and increase the AUC dramatically and hence improving the overall localization quality significantly .,[],"[('reduce', (5, 6)), ('failure rate', (7, 9)), ('increase', (10, 11)), ('AUC', (12, 13)), ('dramatically', (13, 14)), ('improving', (16, 17)), ('overall localization quality', (18, 21)), ('significantly', (21, 22))]",[],"[['reduce', 'has', 'failure rate'], ['increase', 'has', 'AUC'], ['AUC', 'has', 'dramatically'], ['improving', 'has', 'overall localization quality'], ['overall localization quality', 'has', 'significantly']]",[],[],face_alignment,3,255
1713,results,"All in all , our approach fails on only 2.84 % of all images , more than a two times improvement compared with 7.6 .","[('on', (7, 8)), ('of', (11, 12))]","[('our approach', (4, 6)), ('fails', (6, 7)), ('only 2.84 %', (8, 11)), ('all images', (12, 14))]","[['fails', 'on', 'only 2.84 %'], ['only 2.84 %', 'of', 'all images']]","[['our approach', 'has', 'fails']]",[],[],face_alignment,3,256
1714,results,Note the baseline model ( model trained with MSE ) underperforms the state - of - theart .,"[('Note', (0, 1))]","[('baseline model ( model trained', (2, 7)), ('MSE )', (8, 10)), ('underperforms', (10, 11)), ('state - of - theart', (12, 17))]",[],"[['MSE )', 'has', 'underperforms'], ['underperforms', 'has', 'state - of - theart']]","[['Results', 'Note', 'baseline model ( model trained']]",[],face_alignment,3,273
1715,results,"To compare with a naive weight mask without focus on hard negative pixels , we introduced a baseline weight map W M base =? W + 1 , where W = 10 . The major contribution comes from Adaptive Wing loss , which improves the benchmark by 0.74 % .","[('compare with', (1, 3)), ('without focus on', (7, 10)), ('introduced', (15, 16)), ('where', (28, 29)), ('improves', (43, 44)), ('by', (46, 47))]","[('naive weight mask', (4, 7)), ('hard negative pixels', (10, 13)), ('baseline weight map W M base =? W + 1', (17, 27)), ('Adaptive Wing loss', (38, 41)), ('benchmark', (45, 46)), ('0.74 %', (47, 49))]","[['naive weight mask', 'without focus on', 'hard negative pixels'], ['naive weight mask', 'introduced', 'baseline weight map W M base =? W + 1'], ['Adaptive Wing loss', 'improves', 'benchmark'], ['benchmark', 'by', '0.74 %']]",[],[],[],face_alignment,3,274
1716,results,"All other modules contributed incrementally to the localization performance , our Weighted Loss Map improves 0.25 % , boundary prediction and coordinates encoding are able to contribute another 0.09 % .","[('contributed', (3, 4)), ('improves', (14, 15)), ('able to contribute', (24, 27))]","[('All other modules', (0, 3)), ('incrementally', (4, 5)), ('localization performance', (7, 9)), ('our Weighted Loss Map', (10, 14)), ('0.25 %', (15, 17)), ('boundary prediction and coordinates encoding', (18, 23)), ('another', (27, 28)), ('0.09 %', (28, 30))]","[['All other modules', 'contributed', 'incrementally'], ['our Weighted Loss Map', 'improves', '0.25 %'], ['boundary prediction and coordinates encoding', 'able to contribute', 'another'], ['boundary prediction and coordinates encoding', 'able to contribute', '0.09 %']]","[['another', 'has', '0.09 %']]",[],[],face_alignment,3,275
1717,results,"Our proposed Adaptive Wing loss significantly boosts performance compared with MSE , which proves the general applicability of the proposed Adaptive Wing loss on more heatmap regression tasks .","[('compared with', (8, 10))]","[('Our proposed Adaptive Wing loss', (0, 5)), ('significantly boosts', (5, 7)), ('performance', (7, 8)), ('MSE', (10, 11))]","[['significantly boosts', 'compared with', 'MSE'], ['performance', 'compared with', 'MSE']]","[['Our proposed Adaptive Wing loss', 'has', 'significantly boosts'], ['significantly boosts', 'has', 'performance']]",[],"[['Results', 'has', 'Our proposed Adaptive Wing loss']]",face_alignment,3,290
1718,results,"Even with only one HG block , our approach still outperforms previous state - of - the - arts in all datasets except the common subset and the full dataset of 300W .","[('in', (19, 20)), ('except', (22, 23))]","[('our approach', (7, 9)), ('outperforms', (10, 11)), ('previous state - of - the - arts', (11, 19)), ('all datasets', (20, 22)), ('common subset', (24, 26)), ('full dataset of', (28, 31)), ('300W', (31, 32))]","[['previous state - of - the - arts', 'in', 'all datasets'], ['all datasets', 'except', 'common subset'], ['all datasets', 'except', 'full dataset of']]","[['our approach', 'has', 'outperforms'], ['outperforms', 'has', 'previous state - of - the - arts'], ['full dataset of', 'has', '300W']]",[],"[['Results', 'has', 'our approach']]",face_alignment,3,331
1719,experiments,Runtime is evaluated on Nvidia GTX 1080 Ti graphics card with batch size of 1 .,"[('evaluated on', (2, 4)), ('with', (10, 11))]","[('Runtime', (0, 1)), ('Nvidia GTX 1080 Ti graphics card', (4, 10)), ('batch size of 1', (11, 15))]","[['Runtime', 'evaluated on', 'Nvidia GTX 1080 Ti graphics card'], ['Nvidia GTX 1080 Ti graphics card', 'with', 'batch size of 1']]",[],[],[],face_alignment,3,338
1720,research-problem,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network,[],"[('Facial Landmarks Detection', (0, 3))]",[],[],[],[],face_alignment,4,2
1721,research-problem,"It aims to detect the facial landmarks such as eyes , nose and mouth , namely predicting the location parameters of landmarks .","[('aims to detect', (1, 4)), ('such as', (7, 9)), ('namely', (15, 16)), ('of', (20, 21))]","[('facial landmarks', (5, 7)), ('eyes , nose and mouth', (9, 14)), ('location parameters', (18, 20)), ('landmarks', (21, 22))]","[['facial landmarks', 'such as', 'eyes , nose and mouth'], ['location parameters', 'of', 'landmarks']]",[],"[['Research problem', 'aims to detect', 'facial landmarks']]",[],face_alignment,4,15
1722,research-problem,Researchers usually regard this task as atypical non -linear least squares problem .,[],[],[],[],[],[],face_alignment,4,16
1723,model,( b ) Self - Iterative Regression . :,[],"[('Self - Iterative Regression', (3, 7))]",[],[],[],"[['Model', 'has', 'Self - Iterative Regression']]",face_alignment,4,22
1724,model,"To predict the landmarks ' location parameters , the CR based methods require multiple regressors , while SIR just need one regressor and updates parameters iteratively .","[('To predict', (0, 2)), ('require', (12, 13)), ('need', (19, 20)), ('updates', (23, 24))]","[(""landmarks ' location parameters"", (3, 7)), ('CR based methods', (9, 12)), ('multiple regressors', (13, 15)), ('SIR', (17, 18)), ('one regressor', (20, 22)), ('parameters', (24, 25))]","[['CR based methods', 'require', 'multiple regressors'], ['SIR', 'need', 'one regressor'], ['SIR', 'updates', 'parameters']]","[[""landmarks ' location parameters"", 'has', 'CR based methods']]","[['Model', 'To predict', ""landmarks ' location parameters""]]",[],face_alignment,4,24
1725,model,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .","[('develop', (5, 6))]","[('Self - Iterative Regression ( SIR ) framework', (7, 15))]",[],[],"[['Model', 'develop', 'Self - Iterative Regression ( SIR ) framework']]",[],face_alignment,4,31
1726,model,"The training data is obtained by random sampling in the parameter space , and in the test - ing process , parameters are updated iteratively by calling the same regressor , which is dubbed Self - Iterative Regression .","[('obtained by', (4, 6)), ('in', (8, 9)), ('in', (14, 15)), ('updated', (23, 24)), ('by calling', (25, 27)), ('which is', (31, 33))]","[('training data', (1, 3)), ('random sampling', (6, 8)), ('parameter space', (10, 12)), ('test - ing process', (16, 20)), ('parameters', (21, 22)), ('iteratively', (24, 25)), ('same regressor', (28, 30)), ('dubbed Self - Iterative Regression', (33, 38))]","[['training data', 'obtained by', 'random sampling'], ['random sampling', 'in', 'parameter space'], ['random sampling', 'in', 'test - ing process'], ['training data', 'in', 'test - ing process'], ['parameters', 'updated', 'iteratively'], ['iteratively', 'by calling', 'same regressor'], ['same regressor', 'which is', 'dubbed Self - Iterative Regression']]","[['training data', 'has', 'random sampling'], ['test - ing process', 'has', 'parameters']]",[],"[['Model', 'has', 'training data']]",face_alignment,4,33
1727,model,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .","[('to obtain', (2, 4)), ('proposed', (9, 10)), ('focuses on', (20, 22))]","[('discriminative landmarks features', (4, 7)), ('Landmarks - Attention Network ( LAN )', (11, 18)), ('appearance around', (23, 25)), ('landmarks', (25, 26))]","[['discriminative landmarks features', 'proposed', 'Landmarks - Attention Network ( LAN )'], ['Landmarks - Attention Network ( LAN )', 'focuses on', 'appearance around']]","[['discriminative landmarks features', 'name', 'Landmarks - Attention Network ( LAN )'], ['appearance around', 'has', 'landmarks']]","[['Model', 'to obtain', 'discriminative landmarks features']]",[],face_alignment,4,36
1728,model,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .","[('concurrently extracts', (2, 4)), ('obtains', (10, 11)), ('of', (20, 21))]","[(""local landmarks ' features"", (4, 8)), ('holistic increment', (12, 14)), ('significantly reduces', (16, 18)), ('final feature', (22, 24))]",[],"[['holistic increment', 'has', 'significantly reduces'], ['significantly reduces', 'has', 'final feature']]","[['Model', 'concurrently extracts', ""local landmarks ' features""]]",[],face_alignment,4,37
1729,model,"2 . The Landmarks - Attention Network ( LAN ) is developed to independently learn discriminative features around each landmarks , which significantly reduces the dimension of feature layer and the number of model parameters .","[('developed to independently', (11, 14)), ('around', (17, 18))]","[('Landmarks - Attention Network ( LAN )', (3, 10)), ('discriminative features', (15, 17)), ('each landmarks', (18, 20))]","[['discriminative features', 'around', 'each landmarks']]",[],[],"[['Model', 'has', 'Landmarks - Attention Network ( LAN )']]",face_alignment,4,40
1730,experiments,"As illustrated in , SIR is more robust than CR because the former can cover more training space and is n't affected by the optimization path .","[('than', (8, 9))]","[('SIR', (4, 5)), ('more robust', (6, 8)), ('CR', (9, 10))]","[['more robust', 'than', 'CR']]","[['SIR', 'has', 'more robust'], ['more robust', 'has', 'CR']]",[],[],face_alignment,4,138
1731,experiments,"Once one regressor predicts the false direction , the final result is prone to drift away ; ( b ) SIR Descent Direction Map : the training space of SIR includes distribution from coarse stages to fine stages and all descent directions are pointed to ground truth .","[('predicts', (3, 4)), ('prone to', (12, 14)), ('pointed to', (43, 45))]","[('one regressor', (1, 3)), ('false direction', (5, 7)), ('final result', (9, 11)), ('drift away', (14, 16)), ('SIR Descent Direction Map', (20, 24))]","[['one regressor', 'predicts', 'false direction'], ['final result', 'prone to', 'drift away']]","[['one regressor', 'has', 'false direction']]",[],[],face_alignment,4,141
1732,results,The NME results shows that SIR performs comparatively with RAR ) and outperform other existing methods .,"[('shows', (3, 4)), ('performs', (6, 7)), ('with', (8, 9))]","[('NME results', (1, 3)), ('SIR', (5, 6)), ('comparatively', (7, 8)), ('RAR', (9, 10)), ('outperform', (12, 13)), ('other existing methods', (13, 16))]","[['NME results', 'shows', 'SIR'], ['NME results', 'shows', 'outperform'], ['SIR', 'performs', 'comparatively'], ['SIR', 'performs', 'outperform'], ['comparatively', 'with', 'RAR']]","[['NME results', 'has', 'SIR'], ['outperform', 'has', 'other existing methods']]",[],"[['Results', 'has', 'NME results']]",face_alignment,4,175
1733,results,Comparison with Cascaded Regression,[],[],[],[],[],[],face_alignment,4,182
1734,results,"Different from them , our method obtains state - of - the - art performance by iterative call the same regressor rather than adding anymore regressors .","[('obtains', (6, 7)), ('by iterative', (15, 17)), ('rather than adding', (21, 24))]","[('our method', (4, 6)), ('state - of - the - art performance', (7, 15)), ('same regressor', (19, 21)), ('anymore regressors', (24, 26))]","[['our method', 'obtains', 'state - of - the - art performance'], ['same regressor', 'rather than adding', 'anymore regressors']]",[],[],"[['Results', 'has', 'our method']]",face_alignment,4,184
1735,research-problem,Look at Boundary : A Boundary - Aware Face Alignment Algorithm,[],[],[],[],[],[],face_alignment,5,2
1736,research-problem,We present a novel boundary - aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation .,"[('present', (1, 2)), ('by utilising', (10, 12)), ('as', (14, 15)), ('of', (18, 19)), ('to help', (22, 24))]","[('novel', (3, 4)), ('boundary - aware face alignment algorithm', (4, 10)), ('boundary lines', (12, 14)), ('geometric structure', (16, 18)), ('human face', (20, 22)), ('facial landmark localisation', (24, 27))]","[['boundary - aware face alignment algorithm', 'by utilising', 'boundary lines'], ['boundary lines', 'as', 'geometric structure'], ['geometric structure', 'of', 'human face']]","[['novel', 'has', 'boundary - aware face alignment algorithm'], ['boundary lines', 'has', 'geometric structure']]","[['Research problem', 'present', 'novel']]",[],face_alignment,5,4
1737,research-problem,"By utilising boundary information of 300 - W dataset , our method achieves 3.92 % mean error with 0.39 % failure rate on COFW dataset , and 1.25 % mean error on AFLW - Full dataset .","[('utilising', (1, 2)), ('of', (4, 5)), ('achieves', (12, 13)), ('with', (17, 18)), ('on', (22, 23)), ('on', (31, 32))]","[('boundary information', (2, 4)), ('300 - W dataset', (5, 9)), ('our method', (10, 12)), ('3.92 % mean error', (13, 17)), ('0.39 % failure rate', (18, 22)), ('COFW dataset', (23, 25)), ('1.25 % mean error', (27, 31)), ('AFLW - Full dataset', (32, 36))]","[['boundary information', 'of', '300 - W dataset'], ['our method', 'achieves', '3.92 % mean error'], ['3.92 % mean error', 'with', '0.39 % failure rate'], ['3.92 % mean error', 'with', '1.25 % mean error'], ['0.39 % failure rate', 'on', 'COFW dataset'], ['1.25 % mean error', 'on', 'AFLW - Full dataset']]","[['boundary information', 'has', '300 - W dataset']]","[['Research problem', 'utilising', 'boundary information']]",[],face_alignment,5,12
1738,dataset,"In this work , we represent facial structure using 13 boundary lines .","[('represent', (5, 6)), ('using', (8, 9))]","[('facial structure', (6, 8)), ('13 boundary lines', (9, 12))]","[['facial structure', 'using', '13 boundary lines']]",[],"[['Dataset', 'represent', 'facial structure']]",[],face_alignment,5,30
1739,model,"Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets , which will not suffer from inconsistency of the annotation schemes .","[('interpolated from', (6, 8)), ('across', (14, 15)), ('of', (24, 25))]","[('Each facial boundary line', (0, 4)), ('sufficient number of facial landmarks', (9, 14)), ('multiple datasets', (15, 17)), ('inconsistency', (23, 24)), ('annotation schemes', (26, 28))]","[['Each facial boundary line', 'interpolated from', 'sufficient number of facial landmarks'], ['sufficient number of facial landmarks', 'across', 'multiple datasets'], ['inconsistency', 'of', 'annotation schemes']]",[],[],"[['Model', 'has', 'Each facial boundary line']]",face_alignment,5,31
1740,model,Our boundary - aware face alignment algorithm contains two stages .,"[('contains', (7, 8))]","[('boundary - aware face alignment algorithm', (1, 7)), ('two stages', (8, 10))]","[['boundary - aware face alignment algorithm', 'contains', 'two stages']]","[['boundary - aware face alignment algorithm', 'has', 'two stages']]",[],"[['Model', 'has', 'boundary - aware face alignment algorithm']]",face_alignment,5,32
1741,model,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,"[('estimate', (2, 3)), ('with the help of', (10, 14))]","[('facial boundary heatmaps', (3, 6)), ('regress', (8, 9)), ('landmarks', (9, 10)), ('boundary heatmaps', (14, 16))]","[['landmarks', 'with the help of', 'boundary heatmaps']]","[['regress', 'has', 'landmarks']]","[['Model', 'estimate', 'facial boundary heatmaps']]",[],face_alignment,5,33
1742,model,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .","[('introduce', (11, 12)), ('by using', (15, 17))]","[('facial', (5, 6)), ('adversarial learning ideas', (12, 15)), ('landmark - based boundary effectiveness discriminator', (18, 24))]","[['adversarial learning ideas', 'by using', 'landmark - based boundary effectiveness discriminator']]",[],[],[],face_alignment,5,35
1743,model,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .","[('can be', (12, 14)), ('in', (16, 17))]","[('boundary heatmap estimator', (1, 4)), ('landmark regressor', (5, 7)), ('boundary effectiveness discriminator', (9, 12)), ('jointly', (14, 15)), ('end - to - end manner', (18, 24))]","[['boundary effectiveness discriminator', 'can be', 'jointly']]",[],[],"[['Model', 'has', 'boundary heatmap estimator']]",face_alignment,5,37
1744,model,"After generating facial boundary heatmaps , the next step is deriving facial landmarks using boundaries .","[('generating', (1, 2)), ('deriving', (10, 11)), ('using', (13, 14))]","[('facial boundary heatmaps', (2, 5)), ('next', (7, 8)), ('facial landmarks', (11, 13)), ('boundaries', (14, 15))]","[['next', 'deriving', 'facial landmarks'], ['facial landmarks', 'using', 'boundaries']]","[['facial boundary heatmaps', 'has', 'next']]","[['Model', 'generating', 'facial boundary heatmaps']]",[],face_alignment,5,39
1745,model,"To fully utilise the structure information , we apply boundary heatmaps at multiple stages in the landmark regression network .","[('To fully utilise', (0, 3)), ('apply', (8, 9)), ('at', (11, 12)), ('in', (14, 15))]","[('structure information', (4, 6)), ('boundary heatmaps', (9, 11)), ('multiple stages', (12, 14)), ('landmark regression network', (16, 19))]","[['structure information', 'apply', 'boundary heatmaps'], ['boundary heatmaps', 'at', 'multiple stages'], ['multiple stages', 'in', 'landmark regression network']]",[],"[['Model', 'To fully utilise', 'structure information']]",[],face_alignment,5,43
1746,dataset,Each image is annotated with 98 landmarks and 6 attributes .,"[('annotated with', (3, 5))]","[('Each image', (0, 2)), ('98 landmarks', (5, 7)), ('6 attributes', (8, 10))]","[['Each image', 'annotated with', '98 landmarks'], ['Each image', 'annotated with', '6 attributes']]",[],[],"[['Dataset', 'has', 'Each image']]",face_alignment,5,50
1747,tasks,AFLW dataset : AFLW contains 24386 in - the - wild faces with large head pose up to 120 for yaw and 90 for pitch and roll .,"[('contains', (4, 5)), ('with', (12, 13)), ('up to', (16, 18)), ('for', (19, 20))]","[('AFLW dataset', (0, 2)), ('24386 in - the - wild faces', (5, 12)), ('large head pose', (13, 16)), ('120', (18, 19)), ('yaw', (20, 21)), ('90', (22, 23)), ('pitch and roll', (24, 27))]","[['AFLW dataset', 'contains', '24386 in - the - wild faces'], ['24386 in - the - wild faces', 'with', 'large head pose'], ['24386 in - the - wild faces', 'with', '90'], ['large head pose', 'up to', '120'], ['large head pose', 'up to', '90'], ['120', 'for', 'yaw']]",[],[],"[['Tasks', 'has', 'AFLW dataset']]",face_alignment,5,207
1748,experimental-setup,All our models are trained with Caffe [ 24 ] on 4 Titan X GPUs .,"[('trained with', (4, 6)), ('on', (10, 11))]","[('Caffe [ 24 ]', (6, 10)), ('4 Titan X GPUs', (11, 15))]","[['Caffe [ 24 ]', 'on', '4 Titan X GPUs']]",[],"[['Experimental setup', 'trained with', 'Caffe [ 24 ]']]",[],face_alignment,5,224
1749,results,Our method performs best among all of the state - of - the - art methods .,"[('performs', (2, 3)), ('among', (4, 5))]","[('Our method', (0, 2)), ('best', (3, 4)), ('all of the state - of - the - art methods', (5, 16))]","[['Our method', 'performs', 'best'], ['best', 'among', 'all of the state - of - the - art methods']]",[],[],"[['Results', 'has', 'Our method']]",face_alignment,5,231
1750,results,shows the CED curves of our method against state - of - the - art methods on the COFW - 68 dataset .,"[('shows', (0, 1)), ('of', (4, 5)), ('against', (7, 8)), ('on', (16, 17))]","[('CED curves', (2, 4)), ('our', (5, 6)), ('state - of - the - art methods', (8, 16)), ('COFW - 68 dataset', (18, 22))]","[['CED curves', 'of', 'our'], ['CED curves', 'against', 'state - of - the - art methods'], ['our', 'against', 'state - of - the - art methods'], ['state - of - the - art methods', 'on', 'COFW - 68 dataset']]","[['CED curves', 'has', 'our']]","[['Results', 'shows', 'CED curves']]",[],face_alignment,5,240
1751,results,Our model outperforms previous results with a large margin .,"[('with', (5, 6))]","[('Our model', (0, 2)), ('outperforms', (2, 3)), ('previous results', (3, 5)), ('large margin', (7, 9))]","[['outperforms', 'with', 'large margin'], ['previous results', 'with', 'large margin']]","[['Our model', 'has', 'outperforms'], ['outperforms', 'has', 'previous results']]",[],[],face_alignment,5,241
1752,results,We achieve 4.62 % mean error with 2.17 % failure rate .,"[('achieve', (1, 2)), ('with', (6, 7))]","[('4.62 % mean error', (2, 6)), ('2.17 % failure rate', (7, 11))]","[['4.62 % mean error', 'with', '2.17 % failure rate']]",[],[],[],face_alignment,5,242
1753,results,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .","[('is', (3, 4)), ('by', (6, 7))]","[('failure rate', (1, 3)), ('significantly reduced', (4, 6)), ('3.75 %', (7, 9))]","[['failure rate', 'is', 'significantly reduced'], ['significantly reduced', 'by', '3.75 %']]","[['failure rate', 'has', 'significantly reduced']]",[],"[['Results', 'has', 'failure rate']]",face_alignment,5,243
1754,results,There is a clear boost between our method without and with using boundary information .,"[('between', (5, 6)), ('without and with using', (8, 12))]","[('clear boost', (3, 5)), ('our method', (6, 8)), ('boundary information', (12, 14))]","[['clear boost', 'between', 'our method'], ['clear boost', 'without and with using', 'boundary information'], ['our method', 'without and with using', 'boundary information']]",[],[],"[['Results', 'has', 'clear boost']]",face_alignment,5,247
1755,results,"Moreover , our method uses boundary information achieves 29 % , 32 % and 29 % relative performance improve- ment over the baseline method ( "" LAB without boundary "" ) on COFW - 29 , AFLW - Full and AFLW - Frontal respectively .","[('uses', (4, 5)), ('achieves', (7, 8)), ('over', (20, 21)), ('on', (31, 32))]","[('our method', (2, 4)), ('boundary information', (5, 7)), ('29 % , 32 % and 29 % relative performance improve- ment', (8, 20)), ('baseline method ( "" LAB without boundary', (22, 29)), ('COFW - 29', (32, 35)), ('AFLW - Full', (36, 39)), ('AFLW - Frontal', (40, 43))]","[['our method', 'uses', 'boundary information'], ['boundary information', 'achieves', '29 % , 32 % and 29 % relative performance improve- ment'], ['29 % , 32 % and 29 % relative performance improve- ment', 'over', 'baseline method ( "" LAB without boundary'], ['29 % , 32 % and 29 % relative performance improve- ment', 'over', 'AFLW - Frontal']]",[],[],"[['Results', 'has', 'our method']]",face_alignment,5,249
1756,ablation-analysis,"Our framework consists of several pivotal components , i.e. , boundary information fusion , message passing and adversarial learning .","[('consists of', (2, 4)), ('i.e.', (8, 9))]","[('Our', (0, 1)), ('several pivotal components', (4, 7)), ('boundary information fusion', (10, 13)), ('message passing', (14, 16)), ('adversarial learning', (17, 19))]","[['Our', 'consists of', 'several pivotal components'], ['several pivotal components', 'i.e.', 'boundary information fusion'], ['several pivotal components', 'i.e.', 'message passing'], ['several pivotal components', 'i.e.', 'adversarial learning']]","[['several pivotal components', 'name', 'boundary information fusion']]",[],"[['Ablation analysis', 'has', 'Our']]",face_alignment,5,253
1757,ablation-analysis,"As indicated in , our final model that fuses boundary information in all four levels improves mean error from 7.12 % to 6.13 % .","[('fuses', (8, 9)), ('in', (11, 12)), ('improves', (15, 16)), ('from', (18, 19)), ('to', (21, 22))]","[('our final model', (4, 7)), ('boundary information', (9, 11)), ('all four levels', (12, 15)), ('mean error', (16, 18)), ('7.12 %', (19, 21)), ('6.13 %', (22, 24))]","[['our final model', 'fuses', 'boundary information'], ['boundary information', 'in', 'all four levels'], ['boundary information', 'improves', 'mean error'], ['all four levels', 'improves', 'mean error'], ['mean error', 'from', '7.12 %'], ['7.12 %', 'to', '6.13 %']]",[],[],"[['Ablation analysis', 'has', 'our final model']]",face_alignment,5,263
1758,model,"In this paper , we present a novel use of facial boundary to derive facial landmarks .","[('present', (5, 6)), ('of', (9, 10)), ('to derive', (12, 14))]","[('novel use', (7, 9)), ('facial boundary', (10, 12)), ('facial landmarks', (14, 16))]","[['novel use', 'of', 'facial boundary'], ['facial boundary', 'to derive', 'facial landmarks']]",[],"[['Model', 'present', 'novel use']]",[],face_alignment,5,281
1759,model,The runtime of our algorithm is 60 ms on TITAN X GPU .,"[('of', (2, 3)), ('is', (5, 6)), ('on', (8, 9))]","[('runtime', (1, 2)), ('our algorithm', (3, 5)), ('60 ms', (6, 8)), ('TITAN X GPU', (9, 12))]","[['runtime', 'of', 'our algorithm'], ['runtime', 'is', '60 ms'], ['our algorithm', 'is', '60 ms'], ['60 ms', 'on', 'TITAN X GPU']]",[],[],"[['Model', 'has', 'runtime']]",face_alignment,5,285
1760,research-problem,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees,[],"[('Face Alignment', (0, 2))]",[],[],[],[],face_alignment,6,2
1761,research-problem,"In this paper we present 3DDE , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ensembles of regression trees .","[('present', (4, 5)), ('based on', (14, 16)), ('of', (23, 24)), ('of', (25, 26))]","[('3DDE', (5, 6)), ('robust', (8, 9)), ('coarse - to - fine cascade', (17, 23)), ('ensembles', (24, 25)), ('regression trees', (26, 28))]","[['coarse - to - fine cascade', 'of', 'ensembles'], ['ensembles', 'of', 'regression trees']]","[['3DDE', 'has', 'robust']]","[['Research problem', 'present', '3DDE']]",[],face_alignment,6,6
1762,research-problem,"In the experiments performed , 3 DDE improves the state - of - the - art in 300W , COFW , AFLW and WFLW data sets .","[('improves', (7, 8)), ('in', (16, 17))]","[('3 DDE', (5, 7)), ('state - of - the - art', (9, 16)), ('300W , COFW , AFLW and WFLW data sets', (17, 26))]","[['3 DDE', 'improves', 'state - of - the - art'], ['state - of - the - art', 'in', '300W , COFW , AFLW and WFLW data sets']]",[],[],[],face_alignment,6,11
1763,model,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .","[('present', (4, 5)), ('based on', (24, 26)), ('of', (33, 34))]","[('3 DDE ( 3D Deeply - initialized Ensemble ) regressor', (6, 16)), ('robust and efficient face alignment algorithm', (18, 24)), ('coarse - to - fine cascade', (27, 33))]","[['robust and efficient face alignment algorithm', 'based on', 'coarse - to - fine cascade']]","[['3 DDE ( 3D Deeply - initialized Ensemble ) regressor', 'has', 'robust and efficient face alignment algorithm']]","[['Model', 'present', '3 DDE ( 3D Deeply - initialized Ensemble ) regressor']]",[],face_alignment,6,23
1764,model,"It is a hybrid approach that inherits good properties of ERT , such as the ability to impose a face shape prior , and the robustness of deep models .","[('is', (1, 2)), ('inherits', (6, 7)), ('of', (9, 10)), ('such as', (12, 14))]","[('hybrid approach', (3, 5)), ('good properties', (7, 9)), ('ERT', (10, 11)), ('ability', (15, 16)), ('face shape', (19, 21)), ('robustness of', (25, 27)), ('deep models', (27, 29))]","[['hybrid approach', 'inherits', 'good properties'], ['good properties', 'of', 'ERT'], ['good properties', 'such as', 'ability'], ['ERT', 'such as', 'ability']]","[['robustness of', 'has', 'deep models']]","[['Model', 'is', 'hybrid approach']]",[],face_alignment,6,24
1765,model,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,"[('initialized by', (2, 4)), ('to', (11, 12)), ('produced by', (15, 17))]","[('robustly fitting', (4, 6)), ('3 D face model', (7, 11)), ('probability maps', (13, 15)), ('CNN', (18, 19))]","[['3 D face model', 'to', 'probability maps'], ['probability maps', 'produced by', 'CNN']]","[['robustly fitting', 'has', '3 D face model']]","[['Model', 'initialized by', 'robustly fitting']]",[],face_alignment,6,25
1766,model,"With this initialization we tackle one of the main drawbacks of ERT , namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations .","[('tackle', (4, 5)), ('of', (10, 11)), ('in the presence of', (20, 24))]","[('ERT', (11, 12)), ('difficulty', (15, 16)), ('initializing', (17, 18)), ('regressor', (19, 20)), ('occlusions and large face rotations', (24, 29))]","[['regressor', 'in the presence of', 'occlusions and large face rotations']]","[['initializing', 'has', 'regressor']]","[['Model', 'tackle', 'ERT']]",[],face_alignment,6,26
1767,model,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .","[('imposes', (8, 9)), ('on', (13, 14)), ('addressing', (17, 18)), ('of', (20, 21)), ('when', (23, 24)), ('are', (29, 30))]","[('ERT', (6, 7)), ('prior face shape', (10, 13)), ('solution', (15, 16)), ('shortcomings', (19, 20)), ('deep models', (21, 23)), ('occlusions and ambiguous face configurations', (24, 29)), ('present', (30, 31))]","[['ERT', 'imposes', 'prior face shape'], ['prior face shape', 'on', 'solution'], ['prior face shape', 'addressing', 'shortcomings'], ['shortcomings', 'of', 'deep models'], ['deep models', 'when', 'occlusions and ambiguous face configurations'], ['occlusions and ambiguous face configurations', 'are', 'present']]",[],[],"[['Model', 'has', 'ERT']]",face_alignment,6,27
1768,model,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .","[('tackles', (9, 10)), ('of', (13, 14)), ('of', (23, 24)), ('using', (25, 26))]","[('coarse - to - fine structure', (3, 9)), ('combinatorial explosion', (11, 13)), ('parts deformation', (14, 16)), ('key limitation', (21, 23)), ('approaches', (24, 25)), ('shape constraints', (26, 28))]","[['coarse - to - fine structure', 'tackles', 'combinatorial explosion'], ['combinatorial explosion', 'of', 'parts deformation'], ['key limitation', 'of', 'approaches'], ['key limitation', 'of', 'approaches'], ['approaches', 'using', 'shape constraints']]",[],[],"[['Model', 'has', 'coarse - to - fine structure']]",face_alignment,6,28
1769,model,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,"[('improve', (2, 3)), ('by using', (5, 7)), ('in the presence of', (16, 20))]","[('initialization', (4, 5)), ('RANSAC - like procedure', (8, 12)), ('robustness', (15, 16)), ('occlusions', (20, 21))]","[['initialization', 'by using', 'RANSAC - like procedure'], ['robustness', 'in the presence of', 'occlusions']]",[],"[['Model', 'improve', 'initialization']]",[],face_alignment,6,31
1770,experimental-setup,"For each data set , we train from scratch the CNN selecting the model parameters with lowest validation error .","[('For', (0, 1)), ('train from', (6, 8)), ('selecting', (11, 12)), ('with', (15, 16))]","[('each data set', (1, 4)), ('CNN', (10, 11)), ('model parameters', (13, 15)), ('lowest validation error', (16, 19))]","[['CNN', 'selecting', 'model parameters'], ['model parameters', 'with', 'lowest validation error']]",[],"[['Experimental setup', 'For', 'each data set']]",[],face_alignment,6,207
1771,experimental-setup,We crop faces using the ground truth bounding boxes annotations enlarged by 30 % .,"[('crop', (1, 2)), ('using', (3, 4)), ('enlarged by', (10, 12))]","[('faces', (2, 3)), ('ground truth bounding boxes annotations', (5, 10)), ('30 %', (12, 14))]","[['faces', 'using', 'ground truth bounding boxes annotations'], ['ground truth bounding boxes annotations', 'enlarged by', '30 %']]",[],"[['Experimental setup', 'crop', 'faces']]",[],face_alignment,6,208
1772,experimental-setup,"We generate different training samples in each epoch by applying random in plane rotations between 45 , scale changes by 15 % and translations by 5 % of bounding box size , randomly mirroring images horizontally and generating random rectangular occlusions .","[('generate', (1, 2)), ('in', (5, 6)), ('by applying', (8, 10)), ('in', (11, 12)), ('between', (14, 15)), ('by', (19, 20)), ('by', (24, 25)), ('of', (27, 28)), ('randomly mirroring', (32, 34)), ('generating', (37, 38))]","[('different training samples', (2, 5)), ('each epoch', (6, 8)), ('random', (10, 11)), ('plane rotations', (12, 14)), ('45', (15, 16)), ('scale changes', (17, 19)), ('15 %', (20, 22)), ('translations', (23, 24)), ('5 %', (25, 27)), ('bounding box size', (28, 31)), ('images', (34, 35)), ('horizontally', (35, 36)), ('random rectangular occlusions', (38, 41))]","[['different training samples', 'in', 'each epoch'], ['random', 'in', 'plane rotations'], ['different training samples', 'by applying', 'random'], ['each epoch', 'by applying', 'random'], ['each epoch', 'by applying', 'translations'], ['random', 'in', 'plane rotations'], ['plane rotations', 'between', '45'], ['scale changes', 'by', '15 %'], ['translations', 'by', '5 %'], ['5 %', 'of', 'bounding box size'], ['translations', 'randomly mirroring', 'images'], ['different training samples', 'generating', 'random rectangular occlusions'], ['random', 'generating', 'random rectangular occlusions'], ['scale changes', 'generating', 'random rectangular occlusions'], ['translations', 'generating', 'random rectangular occlusions']]","[['images', 'has', 'horizontally']]","[['Experimental setup', 'generate', 'different training samples']]",[],face_alignment,6,209
1773,experimental-setup,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .","[('use', (1, 2)), ('with', (5, 6))]","[('Adam stochastic optimization', (2, 5)), ('? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters', (6, 22))]","[['Adam stochastic optimization', 'with', '? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters']]",[],"[['Experimental setup', 'use', 'Adam stochastic optimization']]",[],face_alignment,6,210
1774,experimental-setup,We train until convergence with an initial learning rate ? = 0.001 .,"[('train', (1, 2)), ('until', (2, 3)), ('with', (4, 5))]","[('convergence', (3, 4)), ('initial learning rate ? = 0.001', (6, 12))]","[['convergence', 'with', 'initial learning rate ? = 0.001']]",[],"[['Experimental setup', 'train', 'convergence']]",[],face_alignment,6,211
1775,experimental-setup,"When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .","[('levels', (3, 4)), ('multiply', (10, 11)), ('by', (14, 15)), ('=', (16, 17))]","[('validation error', (1, 3)), ('out', (4, 5)), ('10 epochs', (6, 8)), ('learning rate', (12, 14)), ('decay', (15, 16)), ('0.05', (17, 18))]","[['validation error', 'levels', 'out'], ['validation error', 'multiply', 'learning rate'], ['10 epochs', 'multiply', 'learning rate'], ['learning rate', 'by', 'decay'], ['decay', '=', '0.05']]","[['validation error', 'has', 'out'], ['decay', 'has', '0.05']]",[],"[['Experimental setup', 'has', 'validation error']]",face_alignment,6,212
1776,experimental-setup,In the CNN the cropped input face is reduced from 160160 to 11 pixels gradually dividing by half their size across B = 8 branches applying astride 2 convolution with kernel size 22 1 .,"[('In', (0, 1)), ('reduced from', (8, 10)), ('to', (11, 12)), ('gradually dividing by', (14, 17)), ('across', (20, 21)), ('applying', (25, 26)), ('with', (29, 30))]","[('CNN', (2, 3)), ('cropped input face', (4, 7)), ('160160', (10, 11)), ('half their size', (17, 20)), ('B = 8 branches', (21, 25)), ('astride 2 convolution', (26, 29)), ('kernel', (30, 31))]","[['cropped input face', 'reduced from', '160160'], ['half their size', 'across', 'B = 8 branches'], ['B = 8 branches', 'applying', 'astride 2 convolution'], ['astride 2 convolution', 'with', 'kernel']]","[['CNN', 'has', 'cropped input face']]","[['Experimental setup', 'In', 'CNN']]",[],face_alignment,6,213
1777,experimental-setup,We apply batch normalization after each convolution .,"[('apply', (1, 2)), ('after', (4, 5))]","[('batch normalization', (2, 4)), ('each convolution', (5, 7))]","[['batch normalization', 'after', 'each convolution']]",[],"[['Experimental setup', 'apply', 'batch normalization']]",[],face_alignment,6,214
1778,experimental-setup,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .","[('apply', (1, 2)), ('with', (5, 6)), ('to', (9, 10)), ('to stabilize', (14, 16))]","[('Gaussian filter', (3, 5)), ('? = 33', (6, 9)), ('output probability maps', (11, 14)), ('initialization', (17, 18))]","[['Gaussian filter', 'with', '? = 33'], ['Gaussian filter', 'to', 'output probability maps'], ['? = 33', 'to', 'output probability maps'], ['output probability maps', 'to stabilize', 'initialization']]",[],"[['Experimental setup', 'apply', 'Gaussian filter']]",[],face_alignment,6,216
1779,experimental-setup,The depth of trees is set to 4 .,"[('set to', (5, 7))]","[('depth of', (1, 3)), ('trees', (3, 4)), ('4', (7, 8))]","[['trees', 'set to', '4']]","[['depth of', 'has', 'trees']]",[],"[['Experimental setup', 'has', 'depth of']]",face_alignment,6,219
1780,experimental-setup,"The number of tests to choose the best split parameters , ? , is set to 200 .","[('to choose', (4, 6)), ('set to', (14, 16))]","[('number of tests', (1, 4)), ('best split parameters', (7, 10)), ('200', (16, 17))]","[['number of tests', 'to choose', 'best split parameters'], ['best split parameters', 'set to', '200']]",[],[],"[['Experimental setup', 'has', 'number of tests']]",face_alignment,6,220
1781,experimental-setup,We resize each image to set the face size to 160160 pixels .,"[('resize', (1, 2)), ('to set', (4, 6)), ('to', (9, 10))]","[('each image', (2, 4)), ('face size', (7, 9)), ('160160 pixels', (10, 12))]","[['each image', 'to set', 'face size'], ['face size', 'to set', '160160 pixels'], ['face size', 'to', '160160 pixels']]",[],"[['Experimental setup', 'resize', 'each image']]",[],face_alignment,6,221
1782,experimental-setup,"For feature extraction , the FREAK pattern diameter is reduced gradually in each stage ( i.e. , in the last stages the pixel pairs for each feature are closer ) .","[('For', (0, 1)), ('reduced', (9, 10)), ('in', (11, 12))]","[('feature extraction', (1, 3)), ('FREAK pattern diameter', (5, 8)), ('gradually', (10, 11)), ('each stage', (12, 14))]","[['FREAK pattern diameter', 'reduced', 'gradually'], ['gradually', 'in', 'each stage']]","[['feature extraction', 'has', 'FREAK pattern diameter'], ['FREAK pattern diameter', 'has', 'gradually']]","[['Experimental setup', 'For', 'feature extraction']]",[],face_alignment,6,222
1783,experimental-setup,We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,"[('generate', (1, 2)), ('in', (6, 7)), ('of', (12, 13))]","[('Z = 25 initializations', (2, 6)), ('robust soft POSIT scheme', (8, 12)), ('g 0', (13, 15))]","[['Z = 25 initializations', 'in', 'robust soft POSIT scheme'], ['robust soft POSIT scheme', 'of', 'g 0']]",[],"[['Experimental setup', 'generate', 'Z = 25 initializations']]",[],face_alignment,6,223
1784,experimental-setup,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,"[('To avoid', (0, 2)), ('use', (4, 5)), ('in', (17, 18))]","[('overfitting', (2, 3)), ('shrinkage factor ? = 0.1', (6, 11)), ('subsampling factor ? = 0.5', (12, 17)), ('ERT', (19, 20))]","[['overfitting', 'use', 'shrinkage factor ? = 0.1'], ['overfitting', 'use', 'subsampling factor ? = 0.5'], ['subsampling factor ? = 0.5', 'in', 'ERT']]",[],"[['Experimental setup', 'To avoid', 'overfitting']]",[],face_alignment,6,225
1785,experimental-setup,"Training the CNN and the coarse - to - fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20 GHz ( 210 cores / 20 threads , 128 GB of RAM ) with a batch size of 32 images .","[('takes', (13, 14)), ('using', (16, 17)), ('at', (36, 37)), ('with', (51, 52))]","[('Training', (0, 1)), ('CNN and the coarse - to - fine ensemble of trees', (2, 13)), ('48 hours', (14, 16)), ('NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU', (18, 28)), ('dual Intel Xeon Silver 4114 CPU', (30, 36)), ('2.20 GHz', (37, 39)), ('batch size', (53, 55)), ('32', (56, 57))]","[['CNN and the coarse - to - fine ensemble of trees', 'takes', '48 hours'], ['Training', 'using', 'dual Intel Xeon Silver 4114 CPU'], ['CNN and the coarse - to - fine ensemble of trees', 'using', 'dual Intel Xeon Silver 4114 CPU'], ['48 hours', 'using', 'NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU'], ['48 hours', 'using', 'dual Intel Xeon Silver 4114 CPU'], ['dual Intel Xeon Silver 4114 CPU', 'at', '2.20 GHz'], ['2.20 GHz', 'with', 'batch size']]","[['Training', 'has', 'CNN and the coarse - to - fine ensemble of trees']]",[],"[['Experimental setup', 'has', 'Training']]",face_alignment,6,227
1786,baselines,"The selected algorithms are representative of the three main families of solutions : a ) ensembles of regression trees ( c GPRT , RCPR , ERT ) , b) CNN - based approaches ( LAB , DAN , RCN ) and c ) mixed approaches with deep nets and ensembles of regression trees ( 3DDE , DCFE ) .","[('representative of', (4, 6)), ('with', (45, 46))]","[('three main families of solutions', (7, 12)), ('ensembles of regression trees', (15, 19)), ('CNN - based approaches (', (29, 34)), ('LAB , DAN , RCN', (34, 39)), ('mixed approaches', (43, 45)), ('deep nets and ensembles of regression trees', (46, 53)), ('3DDE', (54, 55))]","[['mixed approaches', 'with', 'deep nets and ensembles of regression trees']]","[['three main families of solutions', 'name', 'ensembles of regression trees'], ['ensembles of regression trees', 'name', 'CNN - based approaches ('], ['CNN - based approaches (', 'name', 'LAB , DAN , RCN']]","[['Baselines', 'representative of', 'three main families of solutions']]",[],face_alignment,6,236
1787,results,"Overall , 3 DDE is better than any other providing a public implementation in the literature .","[('is', (4, 5)), ('than', (6, 7)), ('providing', (9, 10))]","[('3 DDE', (2, 4)), ('better', (5, 6)), ('any', (7, 8)), ('public implementation', (11, 13))]","[['3 DDE', 'is', 'better'], ['better', 'than', 'any'], ['better', 'providing', 'public implementation'], ['any', 'providing', 'public implementation']]","[['3 DDE', 'has', 'better']]",[],[],face_alignment,6,237
1788,results,"In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .","[('able to', (4, 6)), ('improve by', (6, 8)), ('as', (14, 15))]","[('large margin', (9, 11)), ('other ERT methods', (11, 14)), ('RCPR', (15, 16)), ('ERT or c GPRT', (17, 21))]","[['other ERT methods', 'as', 'RCPR'], ['other ERT methods', 'as', 'ERT or c GPRT']]","[['large margin', 'has', 'other ERT methods'], ['other ERT methods', 'name', 'RCPR']]","[['Results', 'able to', 'large margin']]",[],face_alignment,6,239
1789,results,"We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .","[('without', (5, 6))]","[('outperform', (2, 3)), ('RCN', (3, 4))]",[],"[['outperform', 'has', 'RCN']]",[],[],face_alignment,6,240
1790,results,Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition ( see ) and in the full subset of the 300W public test set ( see ) .,"[('obtains', (2, 3)), ('in', (7, 8)), ('of', (13, 14)), ('of', (25, 26))]","[('Our approach', (0, 2)), ('best overall performance', (4, 7)), ('indoor and outdoor subsets', (9, 13)), ('private competition', (15, 17)), ('full subset', (23, 25)), ('300W public test set', (27, 31))]","[['Our approach', 'obtains', 'best overall performance'], ['best overall performance', 'in', 'indoor and outdoor subsets'], ['best overall performance', 'in', 'full subset'], ['indoor and outdoor subsets', 'of', 'private competition'], ['full subset', 'of', '300W public test set'], ['full subset', 'of', '300W public test set']]",[],[],"[['Results', 'has', 'Our approach']]",face_alignment,6,247
1791,results,"In the challenging subset of the 300W public competition , SHN gets better results than 3DDE .","[('In', (0, 1)), ('of', (4, 5)), ('gets', (11, 12)), ('than', (14, 15))]","[('challenging subset', (2, 4)), ('300W public competition', (6, 9)), ('SHN', (10, 11)), ('better results', (12, 14)), ('3DDE', (15, 16))]","[['challenging subset', 'of', '300W public competition'], ['SHN', 'gets', 'better results'], ['better results', 'than', '3DDE']]","[['challenging subset', 'has', '300W public competition'], ['300W public competition', 'has', 'SHN']]","[['Results', 'In', 'challenging subset']]",[],face_alignment,6,249
1792,results,"3 DDE obtains the best results , NME 5.11 , establishing anew state - of - the - art .","[('obtains', (2, 3)), ('establishing', (10, 11))]","[('DDE', (1, 2)), ('best results', (4, 6)), ('NME 5.11', (7, 9))]","[['DDE', 'obtains', 'best results']]","[['DDE', 'has', 'best results'], ['best results', 'has', 'NME 5.11']]",[],[],face_alignment,6,255
1793,results,"In terms of landmark visibility estimation , we have obtained better precision with an overall better recall than the best previous approach , DCFE .","[('In terms of', (0, 3)), ('obtained', (9, 10)), ('with', (12, 13)), ('than', (17, 18))]","[('landmark visibility estimation', (3, 6)), ('better precision', (10, 12)), ('overall better recall', (14, 17)), ('best previous approach', (19, 22)), ('DCFE', (23, 24))]","[['landmark visibility estimation', 'obtained', 'better precision'], ['better precision', 'with', 'overall better recall'], ['overall better recall', 'than', 'best previous approach']]","[['best previous approach', 'name', 'DCFE']]","[['Results', 'In terms of', 'landmark visibility estimation']]",[],face_alignment,6,266
1794,experiments,"Again , the regularization together with the new initialization contributes to improve DCFE .","[('together with', (4, 6)), ('contributes to', (9, 11))]","[('regularization', (3, 4)), ('new initialization', (7, 9)), ('improve DCFE', (11, 13))]","[['regularization', 'together with', 'new initialization'], ['regularization', 'contributes to', 'improve DCFE'], ['new initialization', 'contributes to', 'improve DCFE']]",[],[],[],face_alignment,6,267
1795,results,"Although the results in are not strictly comparable , because each paper uses its own train and test subsets , we get an NME of 2.06 with the full 21 landmarks set .","[('get', (21, 22)), ('of', (24, 25)), ('with', (26, 27))]","[('NME', (23, 24)), ('2.06', (25, 26)), ('full 21 landmarks set', (28, 32))]","[['NME', 'of', '2.06'], ['2.06', 'with', 'full 21 landmarks set']]",[],[],[],face_alignment,6,270
1796,results,3DDE outperforms its competitors in all the WFLW subsets by a large margin .,"[('in', (4, 5)), ('by', (9, 10))]","[('3DDE', (0, 1)), ('outperforms', (1, 2)), ('its competitors', (2, 4)), ('all the WFLW subsets', (5, 9)), ('large margin', (11, 13))]","[['its competitors', 'in', 'all the WFLW subsets'], ['outperforms', 'by', 'large margin'], ['its competitors', 'by', 'large margin'], ['all the WFLW subsets', 'by', 'large margin']]","[['3DDE', 'has', 'outperforms'], ['outperforms', 'has', 'its competitors']]",[],[],face_alignment,6,277
1797,ablation-analysis,"3DDE is based on three key ideas : 3D initialization , a cascaded ERT regressor operating on probabilistic CNN features and a coarse - to - fine scheme .","[('based on', (2, 4)), ('operating on', (15, 17))]","[('3DDE', (0, 1)), ('3D initialization', (8, 10)), ('cascaded ERT regressor', (12, 15)), ('probabilistic CNN features', (17, 20)), ('coarse - to - fine scheme', (22, 28))]","[['cascaded ERT regressor', 'operating on', 'probabilistic CNN features'], ['cascaded ERT regressor', 'operating on', 'coarse - to - fine scheme']]",[],[],"[['Ablation analysis', 'has', '3DDE']]",face_alignment,6,286
1798,ablation-analysis,In we show the results obtained by different configurations of our framework when evaluated on WFLW .,"[('show', (2, 3)), ('obtained by', (5, 7)), ('of', (9, 10)), ('evaluated on', (13, 15))]","[('different configurations', (7, 9)), ('our framework', (10, 12)), ('WFLW', (15, 16))]","[['different configurations', 'of', 'our framework'], ['different configurations', 'evaluated on', 'WFLW'], ['our framework', 'evaluated on', 'WFLW']]",[],"[['Ablation analysis', 'show', 'different configurations']]",[],face_alignment,6,288
1799,ablation-analysis,"When combined with the cascaded ERT , the 3D initialization is key to achieve top overall performance , see CNN + MS + DE vs CNN + 3D + DE in the full subset .","[('combined with', (1, 3)), ('key to achieve', (11, 14)), ('see', (18, 19))]","[('cascaded ERT', (4, 6)), ('3D initialization', (8, 10)), ('top overall performance', (14, 17)), ('CNN + MS + DE vs CNN + 3D + DE', (19, 30))]","[['3D initialization', 'key to achieve', 'top overall performance'], ['top overall performance', 'see', 'CNN + MS + DE vs CNN + 3D + DE']]","[['cascaded ERT', 'has', '3D initialization']]","[['Ablation analysis', 'combined with', 'cascaded ERT']]",[],face_alignment,6,296
1800,ablation-analysis,"So , it provides the largest improvement in the pose subset .","[('provides', (3, 4)), ('in', (7, 8))]","[('largest improvement', (5, 7)), ('pose subset', (9, 11))]","[['largest improvement', 'in', 'pose subset']]",[],"[['Ablation analysis', 'provides', 'largest improvement']]",[],face_alignment,6,300
1801,ablation-analysis,The use of CNN probability maps improves the NME in the full data set in about 20 % ( see CNN+ 3D + SE vs CNN + 3D + DE ) .,"[('use of', (1, 3)), ('improves', (6, 7)), ('in', (9, 10)), ('in', (14, 15))]","[('CNN probability maps', (3, 6)), ('NME', (8, 9)), ('full data set', (11, 14)), ('about 20 %', (15, 18))]","[['CNN probability maps', 'improves', 'NME'], ['NME', 'in', 'full data set'], ['full data set', 'in', 'about 20 %'], ['NME', 'in', 'about 20 %'], ['full data set', 'in', 'about 20 %']]",[],"[['Ablation analysis', 'use of', 'CNN probability maps']]",[],face_alignment,6,301
1802,ablation-analysis,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .","[('in', (7, 8)), ('provides', (11, 12)), ('in', (15, 16)), ('with', (19, 20))]","[('coarse - to - fine strategy', (1, 7)), ('our cascaded ERT', (8, 11)), ('significative local improvements', (12, 15)), ('difficult cases', (16, 18)), ('rare facial part combinations', (20, 24))]","[['coarse - to - fine strategy', 'in', 'our cascaded ERT'], ['significative local improvements', 'in', 'difficult cases'], ['coarse - to - fine strategy', 'provides', 'significative local improvements'], ['our cascaded ERT', 'provides', 'significative local improvements'], ['significative local improvements', 'in', 'difficult cases'], ['significative local improvements', 'with', 'rare facial part combinations']]",[],[],"[['Ablation analysis', 'has', 'coarse - to - fine strategy']]",face_alignment,6,303
1803,results,"The smallest database , COFW , has the worst cross - dataset results .",[],"[('smallest database', (1, 3)), ('COFW', (4, 5)), ('worst cross - dataset results', (8, 13))]",[],"[['smallest database', 'has', 'COFW'], ['COFW', 'has', 'worst cross - dataset results']]",[],"[['Results', 'has', 'smallest database']]",face_alignment,6,322
1804,results,"On the other hand , the data set with greatest diversity , WFLW , has the best results .","[('with', (8, 9))]","[('data set', (6, 8)), ('greatest diversity', (9, 11)), ('best results', (16, 18))]","[['data set', 'with', 'greatest diversity']]","[['greatest diversity', 'has', 'best results']]",[],[],face_alignment,6,323
1805,results,"Moreover , the model All , trained with the training sets of all data bases , is able to improve , in all cross - dataset experiments , the models trained in a single data set .","[('trained with', (6, 8)), ('able to', (17, 19)), ('in', (21, 22)), ('trained in', (30, 32))]","[('model All', (3, 5)), ('training sets of all data bases', (9, 15)), ('improve', (19, 20)), ('all cross - dataset experiments', (22, 27)), ('models', (29, 30)), ('single data set', (33, 36))]","[['model All', 'trained with', 'training sets of all data bases'], ['model All', 'able to', 'improve'], ['training sets of all data bases', 'able to', 'improve'], ['improve', 'in', 'all cross - dataset experiments'], ['models', 'trained in', 'single data set']]","[['improve', 'has', 'all cross - dataset experiments']]",[],"[['Results', 'has', 'model All']]",face_alignment,6,324
1806,results,"The landmarks with highest NME are those related to the ears , the bottom of the mouth and the chin .","[('with', (2, 3)), ('are', (5, 6)), ('related to', (7, 9))]","[('landmarks', (1, 2)), ('highest NME', (3, 5)), ('ears', (10, 11)), ('bottom of the mouth', (13, 17)), ('chin', (19, 20))]","[['landmarks', 'with', 'highest NME'], ['highest NME', 'related to', 'ears']]","[['landmarks', 'has', 'highest NME']]",[],"[['Results', 'has', 'landmarks']]",face_alignment,6,329
1807,research-problem,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,[],"[('Joint 3D Face Reconstruction and Dense Alignment', (0, 7))]",[],[],[],[],face_alignment,7,2
1808,code,Code is available at https://github.com/YadiraF/PRNet.,[],[],[],[],[],[],face_alignment,7,10
1809,research-problem,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .,[],"[('3 D face reconstruction and face alignment', (0, 7))]",[],[],[],[],face_alignment,7,12
1810,model,"trains a complex network to regress 68 facial landmarks with 2D coordinates from a single image , but needs an extra network to estimate the depth value .","[('trains', (0, 1)), ('to regress', (4, 6)), ('with', (9, 10)), ('from', (12, 13))]","[('complex network', (2, 4)), ('68 facial landmarks', (6, 9)), ('2D coordinates', (10, 12)), ('single image', (14, 16))]","[['complex network', 'to regress', '68 facial landmarks'], ['68 facial landmarks', 'with', '2D coordinates'], ['2D coordinates', 'from', 'single image']]",[],"[['Model', 'trains', 'complex network']]",[],face_alignment,7,21
1811,model,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .","[('propose', (5, 6)), ('called', (13, 14)), ('to jointly predict', (21, 24)), ('reconstruct', (27, 28))]","[('end - to - end method', (7, 13)), ('Position map Regression Network ( PRN )', (14, 21)), ('dense alignment', (24, 26)), ('3 D face shape', (28, 32))]","[['end - to - end method', 'called', 'Position map Regression Network ( PRN )'], ['Position map Regression Network ( PRN )', 'to jointly predict', 'dense alignment'], ['Position map Regression Network ( PRN )', 'reconstruct', '3 D face shape']]","[['end - to - end method', 'name', 'Position map Regression Network ( PRN )']]","[['Model', 'propose', 'end - to - end method']]",[],face_alignment,7,27
1812,model,"Meanwhile , our method is straightforward with a very light - weighted model which provides the result in one pass with 9.8 ms .","[('is', (4, 5)), ('with', (6, 7)), ('provides', (14, 15)), ('in', (17, 18)), ('with', (20, 21))]","[('our method', (2, 4)), ('straightforward', (5, 6)), ('very light - weighted model', (8, 13)), ('result', (16, 17)), ('one pass', (18, 20)), ('9.8 ms', (21, 23))]","[['our method', 'is', 'straightforward'], ['our method', 'with', 'very light - weighted model'], ['straightforward', 'with', 'very light - weighted model'], ['one pass', 'with', '9.8 ms'], ['very light - weighted model', 'provides', 'result'], ['result', 'in', 'one pass'], ['one pass', 'with', '9.8 ms']]","[['our method', 'has', 'straightforward']]",[],"[['Model', 'has', 'our method']]",face_alignment,7,29
1813,model,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .","[('design', (3, 4)), ('which is', (9, 11)), ('recording', (14, 15)), ('of', (18, 19)), ('keeping', (30, 31)), ('at', (34, 35))]","[('UV position map', (5, 8)), ('2D image', (12, 14)), ('3D coordinates', (16, 18)), ('complete facial point cloud', (20, 24)), ('semantic meaning', (32, 34)), ('each UV place', (35, 38))]","[['UV position map', 'which is', '2D image'], ['2D image', 'recording', '3D coordinates'], ['3D coordinates', 'of', 'complete facial point cloud'], ['UV position map', 'keeping', 'semantic meaning'], ['semantic meaning', 'at', 'each UV place']]","[['UV position map', 'has', '2D image']]","[['Model', 'design', 'UV position map']]",[],face_alignment,7,31
1814,model,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,"[('train', (2, 3)), ('with', (9, 10)), ('focuses more on', (14, 17)), ('to regress', (19, 21)), ('from', (25, 26))]","[('simple encoder - decoder network', (4, 9)), ('weighted loss', (11, 13)), ('discriminative region', (17, 19)), ('UV position map', (22, 25)), ('single 2 D facial image', (27, 32))]","[['simple encoder - decoder network', 'with', 'weighted loss'], ['weighted loss', 'focuses more on', 'discriminative region'], ['discriminative region', 'to regress', 'UV position map'], ['UV position map', 'from', 'single 2 D facial image']]",[],"[['Model', 'train', 'simple encoder - decoder network']]",[],face_alignment,7,32
1815,experiments,"Figure1 shows our method is robust to poses , illuminations and occlusions .","[('shows', (1, 2)), ('to', (6, 7))]","[('our method', (2, 4)), ('robust', (5, 6)), ('poses , illuminations and occlusions', (7, 12))]","[['robust', 'to', 'poses , illuminations and occlusions']]","[['our method', 'has', 'robust']]",[],[],face_alignment,7,33
1816,model,"- To directly regress the 3D facial structure and dense alignment , we develop a novel representation called UV position map , which records the position information of 3 D face and provides dense correspondence to the semantic meaning of each point on UV space .","[('To directly regress', (1, 4)), ('develop', (13, 14)), ('called', (17, 18)), ('records', (23, 24)), ('of', (27, 28)), ('provides', (32, 33)), ('to', (35, 36)), ('on', (42, 43))]","[('3D', (5, 6)), ('novel representation', (15, 17)), ('UV position map', (18, 21)), ('position information', (25, 27)), ('3 D face', (28, 31)), ('dense correspondence', (33, 35)), ('semantic meaning', (37, 39)), ('each point', (40, 42)), ('UV space', (43, 45))]","[['3D', 'develop', 'novel representation'], ['novel representation', 'called', 'UV position map'], ['novel representation', 'records', 'position information'], ['UV position map', 'records', 'position information'], ['position information', 'of', '3 D face'], ['novel representation', 'provides', 'dense correspondence'], ['dense correspondence', 'to', 'semantic meaning'], ['each point', 'on', 'UV space']]","[['novel representation', 'name', 'UV position map']]","[['Model', 'To directly regress', '3D']]",[],face_alignment,7,36
1817,model,"- For training , we proposed a weight mask which assigns different weight to each point on position map and compute a weighted loss .","[('proposed', (5, 6)), ('which', (9, 10)), ('assigns', (10, 11)), ('to', (13, 14)), ('on', (16, 17)), ('compute', (20, 21))]","[('training', (2, 3)), ('weight mask', (7, 9)), ('different weight', (11, 13)), ('each point', (14, 16)), ('position map', (17, 19)), ('weighted loss', (22, 24))]","[['training', 'proposed', 'weight mask'], ['weight mask', 'assigns', 'different weight'], ['different weight', 'to', 'each point'], ['each point', 'on', 'position map'], ['weight mask', 'compute', 'weighted loss']]","[['training', 'has', 'weight mask']]",[],[],face_alignment,7,37
1818,model,- We finally provide a light - weighted framework that runs at over 100 FPS to directly obtain 3 D face reconstruction and alignment result from a single 2 D facial image .,"[('provide', (3, 4)), ('runs at', (10, 12)), ('to directly obtain', (15, 18)), ('from', (25, 26))]","[('light - weighted framework', (5, 9)), ('over 100 FPS', (12, 15)), ('3 D face reconstruction and alignment result', (18, 25)), ('single 2 D facial image', (27, 32))]","[['light - weighted framework', 'runs at', 'over 100 FPS'], ['over 100 FPS', 'to directly obtain', '3 D face reconstruction and alignment result'], ['3 D face reconstruction and alignment result', 'from', 'single 2 D facial image']]",[],"[['Model', 'provide', 'light - weighted framework']]",[],face_alignment,7,39
1819,experimental-setup,"Like , we also augment our training data by scaling color channels with scale range from 0.6 to 1.4 .","[('augment', (4, 5)), ('by scaling', (8, 10)), ('with', (12, 13)), ('from', (15, 16)), ('to', (17, 18))]","[('training data', (6, 8)), ('color channels', (10, 12)), ('scale range', (13, 15)), ('0.6', (16, 17))]","[['training data', 'by scaling', 'color channels'], ['color channels', 'with', 'scale range'], ['scale range', 'from', '0.6']]",[],"[['Experimental setup', 'augment', 'training data']]",[],face_alignment,7,142
1820,experimental-setup,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .","[('For', (0, 1)), ('use', (4, 5)), ('with', (7, 8)), ('begins at', (11, 13)), ('after', (17, 18))]","[('optimization', (1, 2)), ('Adam optimizer', (5, 7)), ('learning rate', (9, 11)), ('0.0001', (13, 14)), ('decays', (15, 16)), ('half', (16, 17)), ('each 5 epochs', (18, 21))]","[['optimization', 'use', 'Adam optimizer'], ['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'begins at', '0.0001'], ['half', 'after', 'each 5 epochs']]","[['decays', 'has', 'half']]","[['Experimental setup', 'For', 'optimization']]",[],face_alignment,7,146
1821,experimental-setup,The batch size is set as 16 .,"[('set as', (4, 6))]","[('batch size', (1, 3)), ('16', (6, 7))]","[['batch size', 'set as', '16']]","[['batch size', 'has', '16']]",[],"[['Experimental setup', 'has', 'batch size']]",face_alignment,7,147
1822,experimental-setup,All of our training codes are implemented with TensorFlow .,"[('implemented with', (6, 8))]","[('TensorFlow', (8, 9))]",[],[],"[['Experimental setup', 'implemented with', 'TensorFlow']]",[],face_alignment,7,148
1823,ablation-analysis,Network trained without using weight mask has worst performance compared with other two settings .,"[('compared with', (9, 11))]","[('Network trained', (0, 2)), ('weight mask', (4, 6)), ('worst performance', (7, 9)), ('other two settings', (11, 14))]","[['worst performance', 'compared with', 'other two settings']]",[],[],"[['Ablation analysis', 'has', 'Network trained']]",face_alignment,7,207
1824,ablation-analysis,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .","[('adding', (1, 2)), ('to', (3, 4)), ('such as', (6, 8)), ('shows', (19, 20)), ('on', (22, 23)), ('over', (26, 27))]","[('weights', (2, 3)), ('specific regions', (4, 6)), ('68 facial landmarks or', (8, 12)), ('central face region', (12, 15)), ('weight ratio', (16, 18)), ('considerable improvement', (20, 22)), ('68 points datasets', (23, 26)), ('weight ratio 2', (27, 30))]","[['weights', 'to', 'specific regions'], ['specific regions', 'such as', '68 facial landmarks or'], ['specific regions', 'such as', 'central face region'], ['specific regions', 'such as', 'weight ratio'], ['weight ratio', 'shows', 'considerable improvement'], ['considerable improvement', 'on', '68 points datasets'], ['68 points datasets', 'over', 'weight ratio 2']]","[['weights', 'has', 'specific regions'], ['weight ratio', 'has', 'considerable improvement']]","[['Ablation analysis', 'adding', 'weights']]",[],face_alignment,7,208
1825,research-problem,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning,[],"[('Joint 3D Face Reconstruction and Dense Face Alignment', (0, 8))]",[],[],[],[],face_alignment,8,2
1826,research-problem,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,[],"[('3 D face reconstruction from a single 2D image', (0, 9))]",[],[],[],[],face_alignment,8,9
1827,model,"Specifically , taking the sparse 2 D facial landmarks as additional information , 2 DSAL introduces four novel self - supervision schemes that view the 2D landmark and 3D landmark prediction as a self - mapping process , including the 2D and 3D landmark self - prediction consistency , cycle - consistency over the 2D landmark prediction and self - critic over the predicted 3 DMM coefficients based on landmark predictions .","[('taking', (2, 3)), ('introduces', (15, 16)), ('that view', (22, 24)), ('as', (31, 32)), ('including', (38, 39)), ('over', (52, 53)), ('over', (61, 62)), ('based on', (67, 69))]","[('sparse 2 D facial landmarks', (4, 9)), ('2', (13, 14)), ('four novel self - supervision schemes', (16, 22)), ('2D landmark and 3D landmark prediction', (25, 31)), ('self - mapping process', (33, 37)), ('2D and 3D landmark self - prediction consistency', (40, 48)), ('cycle - consistency', (49, 52)), ('2D landmark prediction', (54, 57)), ('self - critic', (58, 61)), ('predicted 3 DMM coefficients', (63, 67)), ('landmark predictions', (69, 71))]","[['2', 'introduces', 'four novel self - supervision schemes'], ['four novel self - supervision schemes', 'that view', '2D landmark and 3D landmark prediction'], ['2D landmark and 3D landmark prediction', 'as', 'self - mapping process'], ['self - mapping process', 'including', '2D and 3D landmark self - prediction consistency'], ['cycle - consistency', 'over', '2D landmark prediction'], ['self - critic', 'over', 'predicted 3 DMM coefficients'], ['predicted 3 DMM coefficients', 'based on', 'landmark predictions']]","[['sparse 2 D facial landmarks', 'has', '2']]","[['Model', 'taking', 'sparse 2 D facial landmarks']]",[],face_alignment,8,13
1828,research-problem,"Using these four self - supervision schemes , the 2DASL method significantly relieves demands on the the conventional paired 2D - to - 3D annotations and gives much higher - quality 3 D face models without requiring any additional 3D annotations .","[('on', (14, 15)), ('gives', (26, 27)), ('without requiring', (35, 37))]","[('2DASL method', (9, 11)), ('significantly relieves', (11, 13)), ('demands', (13, 14)), ('conventional paired 2D - to - 3D annotations', (17, 25)), ('much higher - quality 3 D face models', (27, 35)), ('additional 3D annotations', (38, 41))]","[['demands', 'on', 'conventional paired 2D - to - 3D annotations'], ['2DASL method', 'gives', 'much higher - quality 3 D face models'], ['much higher - quality 3 D face models', 'without requiring', 'additional 3D annotations']]","[['2DASL method', 'has', 'significantly relieves'], ['significantly relieves', 'has', 'demands']]",[],[],face_alignment,8,14
1829,research-problem,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,[],"[('3 D face reconstruction from a single 2D image', (0, 9))]",[],[],[],[],face_alignment,8,23
1830,research-problem,3 D face reconstruction is an important task in the field of computer vision and graphics .,[],"[('3 D face reconstruction', (0, 4))]",[],[],[],[],face_alignment,8,31
1831,model,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .","[('to overcome', (2, 4)), ('propose', (16, 17)), ('leverages', (22, 23)), ('to effectively supervise and facilitate', (33, 38))]","[('intrinsic limitation', (5, 7)), ('novel learning method', (18, 21)), ('2D "" in - the - wild "" face images', (23, 33)), ('3D face model learning', (39, 43))]","[['novel learning method', 'leverages', '2D "" in - the - wild "" face images'], ['2D "" in - the - wild "" face images', 'to effectively supervise and facilitate', '3D face model learning']]","[['intrinsic limitation', 'has', 'novel learning method']]","[['Model', 'to overcome', 'intrinsic limitation']]",[],face_alignment,8,41
1832,model,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,"[('design', (1, 2)), ('able to train', (11, 14)), ('with', (19, 20)), ('from', (22, 23))]","[('novel self - supervised learning method', (3, 9)), ('3 D face model', (15, 19)), ('weak supervision', (20, 22)), ('2D images', (23, 25))]","[['novel self - supervised learning method', 'able to train', '3 D face model'], ['3 D face model', 'with', 'weak supervision'], ['weak supervision', 'from', '2D images']]",[],"[['Model', 'design', 'novel self - supervised learning method']]",[],face_alignment,8,45
1833,model,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .","[('exploits', (6, 7)), ('over', (10, 11)), ('taking', (18, 19)), ('as', (23, 24)), ('that have', (43, 45)), ('with', (47, 48))]","[('our proposed method', (2, 5)), ('cycle - consistency', (7, 10)), ('2D landmark predictions', (12, 15)), ('recovered 2D landmarks', (20, 23)), ('2D', (33, 34)), ('small difference', (45, 47)), ('annotated ones', (49, 51))]","[['our proposed method', 'exploits', 'cycle - consistency'], ['cycle - consistency', 'over', '2D landmark predictions'], ['2D landmark predictions', 'taking', 'recovered 2D landmarks'], ['small difference', 'with', 'annotated ones']]",[],[],"[['Model', 'has', 'our proposed method']]",face_alignment,8,49
1834,model,"To facilitate the overall learning procedure , our method also exploits self - critic learning .","[('To facilitate', (0, 2)), ('exploits', (10, 11))]","[('overall learning procedure', (3, 6)), ('our', (7, 8)), ('self - critic learning', (11, 15))]","[['our', 'exploits', 'self - critic learning']]","[['overall learning procedure', 'has', 'our']]","[['Model', 'To facilitate', 'overall learning procedure']]",[],face_alignment,8,51
1835,model,"It takes as input both the latent representation and 3 DMM coefficients of an face image and learns a critic model to evaluate the intrinsic consistency between the predicted 3 DMM coefficients and the corresponding face image , offering another supervision for 3 D face model learning .","[('takes as input', (1, 4)), ('of', (12, 13)), ('learns', (17, 18)), ('to evaluate', (21, 23)), ('between', (26, 27)), ('offering', (38, 39)), ('for', (41, 42))]","[('both', (4, 5)), ('latent representation and 3 DMM coefficients', (6, 12)), ('face image', (14, 16)), ('critic model', (19, 21)), ('intrinsic consistency', (24, 26)), ('predicted 3 DMM coefficients and the corresponding face image', (28, 37)), ('another supervision', (39, 41)), ('3 D face model learning', (42, 47))]","[['latent representation and 3 DMM coefficients', 'of', 'face image'], ['critic model', 'to evaluate', 'intrinsic consistency'], ['intrinsic consistency', 'between', 'predicted 3 DMM coefficients and the corresponding face image'], ['intrinsic consistency', 'offering', 'another supervision'], ['predicted 3 DMM coefficients and the corresponding face image', 'offering', 'another supervision'], ['another supervision', 'for', '3 D face model learning']]","[['both', 'has', 'latent representation and 3 DMM coefficients']]","[['Model', 'takes as input', 'both']]",[],face_alignment,8,52
1836,model,"We propose anew scheme that aims to fully utilize the abundant "" in - the - wild "" 2 D face images to assist 3 D face model learning .","[('aims to', (5, 7)), ('to assist', (22, 24))]","[('fully utilize', (7, 9)), ('abundant "" in - the - wild "" 2 D face images', (10, 22)), ('3 D face model learning', (24, 29))]","[['abundant "" in - the - wild "" 2 D face images', 'to assist', '3 D face model learning']]","[['fully utilize', 'has', 'abundant "" in - the - wild "" 2 D face images']]","[['Model', 'aims to', 'fully utilize']]",[],face_alignment,8,57
1837,model,We introduce anew method that is able to train 3 D face models with 2 D face images by self - supervised learning .,"[('able to train', (6, 9)), ('with', (13, 14)), ('by', (18, 19))]","[('3 D face models', (9, 13)), ('2 D face images', (14, 18)), ('self - supervised learning', (19, 23))]","[['3 D face models', 'with', '2 D face images'], ['2 D face images', 'by', 'self - supervised learning']]",[],"[['Model', 'able to train', '3 D face models']]",[],face_alignment,8,59
1838,model,"We develop anew self - critic learning based approach which could effectively improve the 3D face model learning procedure and give a better model , even though the 2D landmark annotations are noisy .","[('develop', (1, 2)), ('could', (10, 11))]","[('self - critic learning based approach', (3, 9)), ('effectively improve', (11, 13)), ('3D face model learning procedure', (14, 19)), ('better model', (22, 24))]","[['self - critic learning based approach', 'could', 'effectively improve']]","[['self - critic learning based approach', 'has', 'effectively improve'], ['effectively improve', 'has', '3D face model learning procedure']]","[['Model', 'develop', 'self - critic learning based approach']]",[],face_alignment,8,61
1839,research-problem,2D assisted self - supervised learning,[],[],[],[],[],[],face_alignment,8,154
1840,experiments,"Since the contour landmarks of a 2 D face are inaccurate to represent the corresponding points of 3 D face , we discard them and sample 18 landmarks from the 68 2D facial landmarks .","[('of', (4, 5)), ('discard', (22, 23)), ('sample', (25, 26)), ('from', (28, 29))]","[('18 landmarks', (26, 28)), ('68 2D facial landmarks', (30, 34))]","[['18 landmarks', 'from', '68 2D facial landmarks']]",[],[],[],face_alignment,8,170
1841,experimental-setup,Our proposed 2 DASL is implemented with Pytorch .,"[('implemented with', (5, 7))]","[('Pytorch', (7, 8))]",[],[],"[['Experimental setup', 'implemented with', 'Pytorch']]",[],face_alignment,8,205
1842,experimental-setup,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .","[('use', (1, 2)), ('for', (4, 5)), ('with', (8, 9)), ('beginning at', (12, 14)), ('uses', (23, 24)), ('as', (26, 27)), ('with', (28, 29))]","[('SGD optimizer', (2, 4)), ('CNN regressor', (6, 8)), ('learning rate', (10, 12)), ('5 10 ?5', (14, 17)), ('decays', (18, 19)), ('Adam', (25, 26)), ('optimizer', (27, 28)), ('fixed learning rate 1 10 ?4', (30, 36))]","[['SGD optimizer', 'for', 'CNN regressor'], ['CNN regressor', 'with', 'learning rate'], ['optimizer', 'with', 'fixed learning rate 1 10 ?4'], ['learning rate', 'beginning at', '5 10 ?5'], ['Adam', 'as', 'optimizer'], ['Adam', 'with', 'fixed learning rate 1 10 ?4'], ['optimizer', 'with', 'fixed learning rate 1 10 ?4']]",[],"[['Experimental setup', 'use', 'SGD optimizer']]",[],face_alignment,8,206
1843,experimental-setup,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .","[('fine - tune', (6, 9)), ('using', (11, 12))]","[('our model', (9, 11)), ('Vertex Distance Cost', (13, 16))]","[['our model', 'using', 'Vertex Distance Cost']]",[],"[['Experimental setup', 'fine - tune', 'our model']]",[],face_alignment,8,211
1844,model,The 2D facial landmarks of all the face images are detected by an advanced 2 D facial landmarks detector .,"[('of', (4, 5)), ('detected by', (10, 12))]","[('2D facial landmarks', (1, 4)), ('all the face images', (5, 9)), ('advanced 2 D facial landmarks detector', (13, 19))]","[['2D facial landmarks', 'of', 'all the face images'], ['2D facial landmarks', 'detected by', 'advanced 2 D facial landmarks detector'], ['all the face images', 'detected by', 'advanced 2 D facial landmarks detector']]",[],[],"[['Model', 'has', '2D facial landmarks']]",face_alignment,8,215
1845,experimental-setup,Each face is annotated with its corresponding 3 DMM coefficients and the 68 3D facial landmarks .,"[('annotated with', (3, 5))]","[('Each face', (0, 2)), ('corresponding 3 DMM coefficients', (6, 10)), ('68 3D facial landmarks', (12, 16))]","[['Each face', 'annotated with', 'corresponding 3 DMM coefficients'], ['Each face', 'annotated with', '68 3D facial landmarks']]",[],[],"[['Experimental setup', 'has', 'Each face']]",face_alignment,8,218
1846,experimental-setup,Each image is annotated with 34 facial landmarks .,"[('annotated with', (3, 5))]","[('Each image', (0, 2)), ('34 facial landmarks', (5, 8))]","[['Each image', 'annotated with', '34 facial landmarks']]",[],[],"[['Experimental setup', 'has', 'Each image']]",face_alignment,8,223
1847,experiments,"As can be seen , our results are more accurate than the ground truth in some cases .","[('are', (7, 8)), ('than', (10, 11)), ('in', (14, 15))]","[('our results', (5, 7)), ('more accurate', (8, 10)), ('ground truth', (12, 14)), ('some cases', (15, 17))]","[['our results', 'are', 'more accurate'], ['more accurate', 'than', 'ground truth'], ['ground truth', 'in', 'some cases']]","[['our results', 'has', 'more accurate']]",[],[],face_alignment,8,230
1848,experiments,"The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .","[('see', (9, 10)), ('achieves', (13, 14)), ('on the evaluation of', (20, 24)), ('among', (30, 31))]","[('our 2 DASL', (10, 13)), ('lowest NME ( % )', (15, 20)), ('2 D and 3D coordinates', (25, 30))]","[['our 2 DASL', 'achieves', 'lowest NME ( % )'], ['lowest NME ( % )', 'on the evaluation of', '2 D and 3D coordinates']]",[],[],[],face_alignment,8,239
1849,experiments,"For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .","[('For', (0, 1)), ('by', (16, 17)), ('on both', (20, 22))]","[('3 DMM - based methods', (1, 6)), ('3 DDFA and DeFA', (7, 11)), ('our method', (12, 14)), ('outperforms', (14, 15)), ('large margin', (18, 20)), ('68 spare landmarks', (23, 26)), ('dense coordinates', (28, 30))]","[['outperforms', 'by', 'large margin'], ['large margin', 'on both', '68 spare landmarks'], ['large margin', 'on both', 'dense coordinates']]","[['3 DMM - based methods', 'name', '3 DDFA and DeFA'], ['our method', 'has', 'outperforms'], ['outperforms', 'has', 'large margin']]",[],[],face_alignment,8,240
1850,experiments,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .","[('performs', (3, 4)), ('than', (5, 6)), ('reducing', (8, 9)), ('by', (10, 11)), ('on', (14, 15))]","[('Our 2DASL', (0, 2)), ('better', (4, 5)), ('PRNet', (6, 7)), ('NME', (9, 10)), ('0.09 and 0.08', (11, 14)), ('AFLW2000 - 3D', (15, 18)), ('AFLW - LFPA', (19, 22))]","[['Our 2DASL', 'performs', 'better'], ['better', 'than', 'PRNet'], ['Our 2DASL', 'reducing', 'NME'], ['NME', 'by', '0.09 and 0.08'], ['0.09 and 0.08', 'on', 'AFLW2000 - 3D'], ['0.09 and 0.08', 'on', 'AFLW - LFPA']]",[],[],[],face_alignment,8,246
1851,experiments,"Following , we first employ the Iterative Closest Points ( ICP ) algorithm to find the corresponding nearest points between the reconstructed 3 D face and the ground truth point cloud .","[('employ', (4, 5)), ('to find', (13, 15)), ('between', (19, 20))]","[('Iterative Closest Points ( ICP ) algorithm', (6, 13)), ('corresponding nearest points', (16, 19)), ('reconstructed 3 D face', (21, 25)), ('ground truth point cloud', (27, 31))]","[['Iterative Closest Points ( ICP ) algorithm', 'to find', 'corresponding nearest points'], ['corresponding nearest points', 'between', 'reconstructed 3 D face'], ['corresponding nearest points', 'between', 'ground truth point cloud']]",[],[],[],face_alignment,8,252
1852,experiments,"As can be seen , the 3D reconstruction results of 2 DASL outperforms 3 DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .","[('of', (9, 10)), ('outperforms', (12, 13)), ('by', (15, 16)), ('for', (20, 21))]","[('3D reconstruction results', (6, 9)), ('2 DASL', (10, 12)), ('3 DDFA', (13, 15)), ('0.39 , and 2.29', (16, 20)), ('DeFA', (21, 22))]","[['3D reconstruction results', 'of', '2 DASL'], ['2 DASL', 'outperforms', '3 DDFA'], ['3 DDFA', 'by', '0.39 , and 2.29'], ['0.39 , and 2.29', 'for', 'DeFA']]",[],[],[],face_alignment,8,255
1853,experiments,"As can be seen , the reconstructed shape of our 2 DASL are more smooth , however , both PRNet and VRN - Guided introduce some artifacts into the reconstructed results , which makes the reconstructed faces look unnaturally .","[('of', (8, 9)), ('are', (12, 13)), ('introduce', (24, 25)), ('into', (27, 28)), ('makes', (33, 34)), ('look', (37, 38))]","[('reconstructed shape', (6, 8)), ('our 2 DASL', (9, 12)), ('more smooth', (13, 15)), ('PRNet and VRN - Guided', (19, 24)), ('some artifacts', (25, 27)), ('reconstructed results', (29, 31)), ('reconstructed faces', (35, 37)), ('unnaturally', (38, 39))]","[['reconstructed shape', 'of', 'our 2 DASL'], ['our 2 DASL', 'are', 'more smooth'], ['PRNet and VRN - Guided', 'introduce', 'some artifacts'], ['some artifacts', 'into', 'reconstructed results'], ['some artifacts', 'makes', 'reconstructed faces'], ['reconstructed results', 'makes', 'reconstructed faces'], ['reconstructed faces', 'look', 'unnaturally']]","[['reconstructed faces', 'has', 'unnaturally']]",[],[],face_alignment,8,257
1854,baselines,"( 2 ) 2DASL ( cyc ) , which takes as input the combination of RGB face images and the corresponding 2D FLMs with self - supervison , however without self - critic supervision ; ( 3 ) 2DASL ( sc ) , which takes as input the RGB face images only using self - critic learning .","[('takes as input', (9, 12)), ('with', (23, 24)), ('takes as input', (44, 47)), ('using', (52, 53))]","[('2DASL ( cyc )', (3, 7)), ('combination', (13, 14)), ('RGB face images', (15, 18)), ('self - supervison', (24, 27)), ('2DASL ( sc )', (38, 42)), ('RGB face images', (48, 51)), ('self - critic learning', (53, 57))]","[['2DASL ( cyc )', 'takes as input', 'combination'], ['2DASL ( sc )', 'takes as input', 'RGB face images'], ['RGB face images', 'using', 'self - critic learning']]",[],[],"[['Baselines', 'has', '2DASL ( cyc )']]",face_alignment,8,260
1855,baselines,"( 4 ) 2DASL ( cyc+sc ) , which contains both self - supervision and self - critic supervision .","[('contains', (9, 10))]","[('2DASL ( cyc+sc )', (3, 7)), ('self - supervision', (11, 14)), ('self - critic supervision', (15, 19))]","[['2DASL ( cyc+sc )', 'contains', 'self - supervision'], ['2DASL ( cyc+sc )', 'contains', 'self - critic supervision']]",[],[],[],face_alignment,8,261
1856,ablation-analysis,"2 . Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages , respectively .","[('Adding', (2, 3)), ('to', (4, 5)), ('of', (7, 8)), ('reduces', (11, 12)), ('by', (14, 15)), ('to', (16, 17)), ('on', (18, 19))]","[('weights', (3, 4)), ('central', (5, 6)), ('points', (6, 7)), ('facial landmarks', (9, 11)), ('NME', (13, 14)), ('0.09', (15, 16)), ('0.23', (17, 18)), ('two stages', (20, 22))]","[['weights', 'to', 'central'], ['0.09', 'to', '0.23'], ['points', 'of', 'facial landmarks'], ['facial landmarks', 'reduces', 'NME'], ['NME', 'by', '0.09'], ['0.09', 'to', '0.23'], ['0.23', 'on', 'two stages']]","[['central', 'has', 'points']]","[['Ablation analysis', 'Adding', 'weights']]",[],face_alignment,8,265
1857,ablation-analysis,"If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .","[('If', (0, 1)), ('is', (6, 7)), ('increases', (12, 13)), ('by', (13, 14)), ('for', (15, 16))]","[('self - critic learning', (2, 6)), ('not used', (7, 9)), ('NME', (11, 12)), ('0.04/0.18', (14, 15)), ('with / without weight mask', (16, 21))]","[['self - critic learning', 'is', 'not used'], ['0.04/0.18', 'for', 'with / without weight mask']]","[['self - critic learning', 'has', 'not used'], ['not used', 'has', 'NME']]","[['Ablation analysis', 'If', 'self - critic learning']]",[],face_alignment,8,267
1858,ablation-analysis,The best result is achieved when both these two modules are used .,"[('achieved when', (4, 6)), ('used', (11, 12))]","[('best result', (1, 3)), ('both these two modules', (6, 10))]","[['best result', 'achieved when', 'both these two modules']]",[],[],"[['Ablation analysis', 'has', 'best result']]",face_alignment,8,269
1859,research-problem,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection,[],[],[],[],[],[],face_alignment,9,2
1860,model,"In addition , to recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) to correct outliers by considering the global face shape as a constraint .","[('due to', (9, 11)), ('propose', (17, 18)), ('to correct', (26, 28)), ('by considering', (29, 31)), ('as', (35, 36))]","[('recover', (4, 5)), ('unconfidently predicted landmarks', (6, 9)), ('occlusion and low quality', (11, 15)), ('global heatmap correction unit ( GHCU )', (19, 26)), ('outliers', (28, 29)), ('global face shape', (32, 35)), ('constraint', (37, 38))]","[['unconfidently predicted landmarks', 'due to', 'occlusion and low quality'], ['occlusion and low quality', 'propose', 'global heatmap correction unit ( GHCU )'], ['global heatmap correction unit ( GHCU )', 'to correct', 'outliers'], ['outliers', 'by considering', 'global face shape'], ['global face shape', 'as', 'constraint']]","[['recover', 'has', 'unconfidently predicted landmarks']]",[],[],face_alignment,9,12
1861,model,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .","[('propose', (9, 10)), ('maintains', (19, 20)), ('recovers', (26, 27)), ('caused by', (31, 33)), ('such as', (35, 37))]","[('global heatmap correction unit ( GHCU )', (11, 18)), ('global face shape constraint', (21, 25)), ('unconfidently predicted landmarks', (28, 31)), ('challenging factors', (33, 35)), ('occlusions', (37, 38)), ('low resolution of', (39, 42)), ('images', (42, 43))]","[['global heatmap correction unit ( GHCU )', 'maintains', 'global face shape constraint'], ['global heatmap correction unit ( GHCU )', 'recovers', 'unconfidently predicted landmarks'], ['unconfidently predicted landmarks', 'caused by', 'challenging factors'], ['challenging factors', 'such as', 'occlusions']]","[['low resolution of', 'has', 'images']]","[['Model', 'propose', 'global heatmap correction unit ( GHCU )']]",[],face_alignment,9,37
1862,experiments,"1 . The GHCU implicitly learns the whole face shape constraint from the training data and always gives facialshape landmarks , as shown in .","[('implicitly learns', (4, 6)), ('from', (11, 12)), ('always gives', (16, 18))]","[('GHCU', (3, 4)), ('whole face shape constraint', (7, 11)), ('training data', (13, 15)), ('facialshape landmarks', (18, 20))]","[['GHCU', 'implicitly learns', 'whole face shape constraint'], ['whole face shape constraint', 'from', 'training data'], ['GHCU', 'always gives', 'facialshape landmarks']]",[],[],[],face_alignment,9,181
1863,experimental-setup,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .","[('To', (0, 1)), ('perform', (1, 2)), ('randomly sample', (6, 8)), ('from', (17, 18))]","[('data augmentation', (2, 4)), ('angle of rotation and the bounding box scale', (9, 17)), ('Gaussian distribution', (18, 20))]","[['data augmentation', 'randomly sample', 'angle of rotation and the bounding box scale'], ['angle of rotation and the bounding box scale', 'from', 'Gaussian distribution']]",[],"[['Experimental setup', 'To', 'data augmentation']]",[],face_alignment,9,201
1864,experimental-setup,We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,"[('use', (1, 2)), ('as', (9, 10)), ('trained by', (14, 16))]","[('four - stage stacked hourglass network', (3, 9)), ('our backbone', (10, 12)), ('optimizer RMSprop', (17, 19))]","[['four - stage stacked hourglass network', 'as', 'our backbone'], ['our backbone', 'trained by', 'optimizer RMSprop']]",[],"[['Experimental setup', 'use', 'four - stage stacked hourglass network']]",[],face_alignment,9,202
1865,ablation-analysis,"As described in Section 4 , our algorithm comprises two parts : network training and real groundtruth searching , which are alternatively optimized .","[('comprises', (8, 9)), ('are', (20, 21))]","[('our algorithm', (6, 8)), ('two parts', (9, 11)), ('network training', (12, 14)), ('real groundtruth searching', (15, 18)), ('alternatively optimized', (21, 23))]","[['our algorithm', 'comprises', 'two parts'], ['real groundtruth searching', 'are', 'alternatively optimized']]","[['two parts', 'name', 'network training']]",[],"[['Ablation analysis', 'has', 'our algorithm']]",face_alignment,9,203
1866,ablation-analysis,"Specifically , at each epoch , we first search the real ground - trut ?","[('at', (2, 3)), ('first search', (7, 9))]","[('each epoch', (3, 5))]",[],[],"[['Ablation analysis', 'at', 'each epoch']]",[],face_alignment,9,204
1867,experimental-setup,"When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .","[('with', (6, 7)), ('is', (14, 15)), ('after', (26, 27))]","[('training', (1, 2)), ('roughly converged model', (3, 6)), ('human annotations', (7, 9)), ('initial learning rate', (11, 14)), ('2.5 10 ?4', (15, 18)), ('decayed', (20, 21)), ('2.5 10 ? 6', (22, 26)), ('120 epochs', (27, 29))]","[['roughly converged model', 'with', 'human annotations'], ['initial learning rate', 'is', '2.5 10 ?4'], ['2.5 10 ?4', 'is', 'decayed'], ['2.5 10 ? 6', 'after', '120 epochs']]","[['training', 'has', 'roughly converged model'], ['initial learning rate', 'has', '2.5 10 ?4'], ['decayed', 'has', '2.5 10 ? 6']]",[],"[['Experimental setup', 'has', 'training']]",face_alignment,9,207
1868,experimental-setup,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .","[('of', (8, 9)), ('is', (19, 20)), ('divided by', (26, 28)), ('at', (33, 34))]","[('training', (1, 2)), ('Semantic Alignment', (3, 5)), ('initial learning rate', (16, 19)), ('2.5 10 ? 6', (20, 24)), ('5 , 2 and 2', (28, 33)), ('epoch 30 , 60 and 90', (34, 40))]","[['initial learning rate', 'is', '2.5 10 ? 6'], ['initial learning rate', 'divided by', '5 , 2 and 2'], ['5 , 2 and 2', 'at', 'epoch 30 , 60 and 90']]","[['training', 'has', 'Semantic Alignment'], ['initial learning rate', 'has', '2.5 10 ? 6']]",[],"[['Experimental setup', 'has', 'training']]",face_alignment,9,208
1869,experimental-setup,We set batch size to 10 for network training .,"[('set', (1, 2)), ('to', (4, 5)), ('for', (6, 7))]","[('batch size', (2, 4)), ('10', (5, 6)), ('network training', (7, 9))]","[['batch size', 'to', '10'], ['10', 'for', 'network training']]","[['batch size', 'has', '10']]","[['Experimental setup', 'set', 'batch size']]",[],face_alignment,9,211
1870,experimental-setup,1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .,"[('trained with', (6, 8)), ('on', (12, 13))]","[('PyTorch [ 18 ]', (8, 12)), ('2 Titan X GPUs', (13, 17))]","[['PyTorch [ 18 ]', 'on', '2 Titan X GPUs']]",[],"[['Experimental setup', 'trained with', 'PyTorch [ 18 ]']]",[],face_alignment,9,213
1871,baselines,"2 ) uses the hourglass architecture with human annotations , which is actually the traditional landmark detector training .","[('uses', (2, 3)), ('with', (6, 7)), ('is', (11, 12))]","[('hourglass architecture', (4, 6)), ('human annotations', (7, 9))]","[['hourglass architecture', 'with', 'human annotations']]",[],[],[],face_alignment,9,218
1872,results,"2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .","[('see', (4, 5)), ('with', (7, 8)), ('greatly outperform', (16, 18)), ('in terms of', (29, 32)), ('on', (33, 34)), ('showing', (37, 38))]","[('HGs', (6, 7)), ('our Semantic Alignment ( HGs + SA )', (8, 16)), ('hourglass ( HGs )', (18, 22)), ('4.37 % vs 5.04 %', (24, 29)), ('NME', (32, 33)), ('Full set', (34, 36))]","[['HGs', 'with', 'our Semantic Alignment ( HGs + SA )'], ['our Semantic Alignment ( HGs + SA )', 'greatly outperform', 'hourglass ( HGs )'], ['4.37 % vs 5.04 %', 'in terms of', 'NME'], ['NME', 'on', 'Full set']]",[],"[['Results', 'see', 'HGs']]",[],face_alignment,9,220
1873,results,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .","[('adding', (1, 2)), ('see', (6, 7))]","[('GHCU', (2, 3)), ('HGs + SA + GHCU', (8, 13)), ('slightly outperforms', (13, 15)), ('HGs + SA', (16, 19))]","[['GHCU', 'see', 'HGs + SA + GHCU']]","[['GHCU', 'has', 'HGs + SA + GHCU'], ['HGs + SA + GHCU', 'has', 'slightly outperforms'], ['slightly outperforms', 'has', 'HGs + SA']]","[['Results', 'adding', 'GHCU']]",[],face_alignment,9,221
1874,results,"Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .","[('normalize', (3, 4)), ('by training', (10, 12)), ('conduct', (17, 18)), ('achieve', (30, 31)), ('on', (36, 37))]","[('in - plane - rotation', (5, 10)), ('preprocessing network', (13, 15)), ('normalization ( HGs + SA + GHCU + Norm )', (19, 29)), ('state of the art performance', (31, 36)), ('Challenge set and Full set', (37, 42)), ('6.38 % and 4.02 %', (43, 48))]","[['in - plane - rotation', 'by training', 'preprocessing network'], ['preprocessing network', 'conduct', 'normalization ( HGs + SA + GHCU + Norm )'], ['state of the art performance', 'on', 'Challenge set and Full set']]",[],[],[],face_alignment,9,223
1875,results,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .","[('on', (3, 4))]","[('Challenge set', (4, 6)), ('significantly outperform', (8, 10)), ('state of the art method', (11, 16)), ('6.38 % ( HGs + SA +GHCU + Norm )', (17, 27)), ('6.98 % ( LAB )', (28, 33))]",[],"[['Challenge set', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'state of the art method'], ['state of the art method', 'has', '6.38 % ( HGs + SA +GHCU + Norm )']]","[['Results', 'on', 'Challenge set']]",[],face_alignment,9,224
1876,results,It is also observed that HGs + SA + GHCU works better than HGs + SA .,"[('observed', (3, 4)), ('works', (10, 11)), ('than', (12, 13))]","[('HGs + SA + GHCU', (5, 10)), ('better', (11, 12)), ('HGs + SA', (13, 16))]","[['HGs + SA + GHCU', 'works', 'better'], ['better', 'than', 'HGs + SA']]","[['HGs + SA + GHCU', 'has', 'better']]","[['Results', 'observed', 'HGs + SA + GHCU']]",[],face_alignment,9,233
1877,results,The subset Category 3 is the most challenging one .,"[('is', (4, 5))]","[('subset Category 3', (1, 4)), ('most challenging one', (6, 9))]","[['subset Category 3', 'is', 'most challenging one']]","[['subset Category 3', 'has', 'most challenging one']]",[],[],face_alignment,9,236
1878,results,"4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .","[('see that', (4, 6))]","[('HGs + SA', (6, 9)), ('greatly outperforms', (9, 11)), ('HGs', (11, 12))]",[],"[['HGs + SA', 'has', 'greatly outperforms'], ['greatly outperforms', 'has', 'HGs']]","[['Results', 'see that', 'HGs + SA']]",[],face_alignment,9,238
1879,results,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .","[('compared with', (2, 4)), ('reduce', (13, 14)), ('by', (20, 21)), ('on', (23, 24))]","[('HGs + SA , HGs + SA + GHCU', (4, 13)), ('error rate ( RMSE )', (15, 20)), ('18 %', (21, 23)), ('Category 3 test set', (24, 28))]","[['HGs + SA , HGs + SA + GHCU', 'reduce', 'error rate ( RMSE )'], ['error rate ( RMSE )', 'by', '18 %'], ['18 %', 'on', 'Category 3 test set']]",[],"[['Results', 'compared with', 'HGs + SA , HGs + SA + GHCU']]",[],face_alignment,9,239
1880,results,Comparison with state of the art on AFLW dataset .,"[('on', (6, 7))]","[('AFLW dataset', (7, 9))]",[],[],"[['Results', 'on', 'AFLW dataset']]",[],face_alignment,9,240
1881,baselines,"GHCU considers the global face shape as constraint , being robust to such challenging factors .","[('considers', (1, 2)), ('as', (6, 7))]","[('GHCU', (0, 1)), ('global face shape', (3, 6)), ('constraint', (7, 8))]","[['GHCU', 'considers', 'global face shape'], ['global face shape', 'as', 'constraint']]","[['global face shape', 'has', 'constraint']]",[],"[['Baselines', 'has', 'GHCU']]",face_alignment,9,243
1882,results,"As shown in Tab. 8 , our CNN based GHCU outperforms PCA based method in terms of both accuracy and efficiency .","[('outperforms', (10, 11)), ('in terms of', (14, 17))]","[('our CNN based GHCU', (6, 10)), ('PCA based method', (11, 14)), ('both accuracy and efficiency', (17, 21))]","[['our CNN based GHCU', 'outperforms', 'PCA based method'], ['PCA based method', 'in terms of', 'both accuracy and efficiency']]",[],[],"[['Results', 'has', 'our CNN based GHCU']]",face_alignment,9,285
1883,ablation-analysis,"9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .","[('on', (9, 10))]","[('Semantic alignment', (2, 4)), ('consistently improve', (5, 7)), ('performance', (8, 9)), ('all subset sets', (10, 13))]","[['performance', 'on', 'all subset sets']]","[['Semantic alignment', 'has', 'consistently improve'], ['consistently improve', 'has', 'performance']]",[],"[['Ablation analysis', 'has', 'Semantic alignment']]",face_alignment,9,290
1884,ablation-analysis,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .","[('is', (1, 2)), ('on', (4, 5))]","[('GHCU', (0, 1)), ('more effective', (2, 4)), ('challenge data set', (6, 9)), ('8.15 % vs 9.91 %', (14, 19))]","[['GHCU', 'is', 'more effective'], ['more effective', 'on', 'challenge data set']]","[['GHCU', 'has', 'more effective']]",[],"[['Ablation analysis', 'has', 'GHCU']]",face_alignment,9,291
1885,research-problem,Accurate Face Detection for High Performance,[],"[('Face Detection', (1, 3))]",[],[],[],[],face_detection,0,2
1886,research-problem,Face detection has witnessed significant progress due to the advances of deep convolutional neural networks ( CNNs ) .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,0,4
1887,research-problem,"Face detection is a tremendously important field in computer vision needed for face recognition , sentiment analysis , video surveillance , and many other fields .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,0,11
1888,model,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .","[('to perform', (14, 16)), ('as', (18, 19))]","[('popular one - stage RetinaNet method', (8, 14)), ('face detection', (16, 18)), ('our baseline model', (19, 22))]","[['popular one - stage RetinaNet method', 'to perform', 'face detection'], ['face detection', 'as', 'our baseline model']]",[],[],[],face_detection,0,23
1889,model,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,"[('applied on', (5, 7)), ('to develop', (9, 11)), ('namely', (16, 17))]","[('high performance face detector', (12, 16)), ('AInnoFace', (17, 18))]","[['high performance face detector', 'namely', 'AInnoFace']]","[['high performance face detector', 'name', 'AInnoFace']]","[['Model', 'applied on', 'high performance face detector']]",[],face_detection,0,24
1890,model,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,"[('Employing', (3, 4)), ('for', (11, 12)), ('Applying', (17, 18)), ('for', (27, 28)), ('Revisiting', (33, 34)), ('based on', (37, 39)), ('for', (44, 45)), ('Utilizing', (50, 51)), ('for', (56, 57)), ('Using', (63, 64)), ('for', (68, 69))]","[('two - step classification and regression', (5, 11)), ('detection', (12, 13)), ('Intersection over Union ( IoU ) loss function', (19, 27)), ('regression', (28, 29)), ('data augmentation', (35, 37)), ('data - anchor - sampling', (39, 44)), ('training', (45, 46)), ('max - out operation', (52, 56)), ('robuster classification', (57, 59)), ('multi-scale testing strategy', (65, 68)), ('inference', (69, 70))]","[['two - step classification and regression', 'for', 'detection'], ['Intersection over Union ( IoU ) loss function', 'for', 'regression'], ['multi-scale testing strategy', 'for', 'inference'], ['Intersection over Union ( IoU ) loss function', 'for', 'regression'], ['data augmentation', 'based on', 'data - anchor - sampling'], ['data - anchor - sampling', 'for', 'training'], ['max - out operation', 'for', 'robuster classification'], ['multi-scale testing strategy', 'for', 'inference']]",[],"[['Model', 'Employing', 'two - step classification and regression']]",[],face_detection,0,25
1891,baselines,The focal loss is the reshaping of cross entropy loss such that it down - weights the loss assigned to well - classified examples .,"[('is', (3, 4)), ('of', (6, 7)), ('such that', (10, 12)), ('down - weights', (13, 16)), ('assigned to', (18, 20))]","[('focal loss', (1, 3)), ('reshaping', (5, 6)), ('cross entropy loss', (7, 10)), ('loss', (17, 18)), ('well - classified examples', (20, 24))]","[['focal loss', 'is', 'reshaping'], ['reshaping', 'of', 'cross entropy loss'], ['cross entropy loss', 'down - weights', 'loss'], ['loss', 'assigned to', 'well - classified examples']]","[['focal loss', 'has', 'reshaping']]",[],"[['Baselines', 'has', 'focal loss']]",face_detection,0,74
1892,baselines,STR performs is two - step regression on three high level detection layers to adjust anchors and provide better initialization for the subsequent regressor .,"[('performs', (1, 2)), ('on', (7, 8)), ('to adjust', (13, 15)), ('provide', (17, 18)), ('for', (20, 21))]","[('STR', (0, 1)), ('two - step regression', (3, 7)), ('three high level detection layers', (8, 13)), ('anchors', (15, 16)), ('better initialization', (18, 20)), ('subsequent regressor', (22, 24))]","[['STR', 'performs', 'two - step regression'], ['two - step regression', 'on', 'three high level detection layers'], ['two - step regression', 'to adjust', 'anchors'], ['three high level detection layers', 'to adjust', 'anchors'], ['STR', 'provide', 'better initialization'], ['two - step regression', 'provide', 'better initialization'], ['three high level detection layers', 'provide', 'better initialization'], ['better initialization', 'for', 'subsequent regressor']]","[['STR', 'has', 'two - step regression']]",[],"[['Baselines', 'has', 'STR']]",face_detection,0,93
1893,experimental-setup,The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset .,"[('in', (3, 4)), ('initialized by', (9, 11)), ('on', (14, 15))]","[('backbone network', (1, 3)), ('proposed AInnoFace detector', (5, 8)), ('pretrained model', (12, 14)), ('ImageNet dataset', (16, 18))]","[['backbone network', 'in', 'proposed AInnoFace detector'], ['proposed AInnoFace detector', 'initialized by', 'pretrained model'], ['pretrained model', 'on', 'ImageNet dataset']]","[['backbone network', 'has', 'proposed AInnoFace detector']]",[],"[['Experimental setup', 'has', 'backbone network']]",face_detection,0,134
1894,experimental-setup,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .","[('use', (1, 2)), ('to randomly initialize', (7, 10)), ('in', (12, 13))]","[('"" xavier "" method', (3, 7)), ('parameters', (11, 12)), ('newly added convolutional layers', (14, 18))]","[['"" xavier "" method', 'to randomly initialize', 'parameters'], ['parameters', 'in', 'newly added convolutional layers']]",[],"[['Experimental setup', 'use', '"" xavier "" method']]",[],face_detection,0,135
1895,experimental-setup,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .","[('with', (16, 17))]","[('stochastic gradient descent ( SGD ) algorithm', (1, 8)), ('model', (15, 16)), ('0.9 momentum', (17, 19)), ('0.0001 weight decay', (20, 23)), ('batch size 32', (24, 27))]","[['model', 'with', '0.9 momentum'], ['model', 'with', 'batch size 32']]",[],[],"[['Experimental setup', 'has', 'stochastic gradient descent ( SGD ) algorithm']]",face_detection,0,136
1896,experimental-setup,The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,"[('applied to', (4, 6)), ('from', (12, 13)), ('to', (14, 15)), ('at', (16, 17))]","[('warmup strategy', (1, 3)), ('gradually ramp up', (6, 9)), ('learning rate', (10, 12)), ('0.0003125', (13, 14)), ('0.01', (15, 16)), ('first 5 epochs', (18, 21))]","[['warmup strategy', 'applied to', 'gradually ramp up'], ['learning rate', 'from', '0.0003125'], ['0.0003125', 'to', '0.01'], ['0.01', 'at', 'first 5 epochs']]","[['gradually ramp up', 'has', 'learning rate']]",[],"[['Experimental setup', 'has', 'warmup strategy']]",face_detection,0,137
1897,experiments,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .","[('switches to', (4, 6)), ('dividing by', (14, 16)), ('at', (17, 18)), ('ending at', (23, 25))]","[('regular learning rate schedule', (7, 11)), ('10', (16, 17)), ('100 and 120 epochs', (18, 22)), ('130 epochs', (25, 27))]","[['regular learning rate schedule', 'dividing by', '10'], ['10', 'at', '100 and 120 epochs'], ['regular learning rate schedule', 'ending at', '130 epochs']]",[],[],[],face_detection,0,138
1898,experimental-setup,The full training and testing codes are built on the PyTorch library .,"[('built on', (7, 9))]","[('full training and testing codes', (1, 6)), ('PyTorch library', (10, 12))]","[['full training and testing codes', 'built on', 'PyTorch library']]",[],[],"[['Experimental setup', 'has', 'full training and testing codes']]",face_detection,0,139
1899,results,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .","[('sets', (7, 8)), ('based on', (18, 20)), ('across', (23, 24)), ('on', (27, 28)), ('i.e.', (34, 35)), ('for', (53, 54)), ('for', (75, 76))]","[('our face detector', (4, 7)), ('some new state - of - the - art results', (8, 18)), ('AP score', (21, 23)), ('three subsets', (25, 27)), ('validation and testing subsets', (29, 33)), ('97.0 % ( Easy )', (36, 41)), ('96.1 % ( Medium )', (42, 47)), ('91.8 % ( Hard )', (48, 53)), ('validation subset', (54, 56)), ('96.5 %', (58, 60)), ('95.7 % ( Medium )', (64, 69)), ('91.2 % ( Hard )', (70, 75)), ('testing subset', (76, 78))]","[['our face detector', 'sets', 'some new state - of - the - art results'], ['some new state - of - the - art results', 'based on', 'AP score'], ['AP score', 'across', 'three subsets'], ['three subsets', 'on', 'validation and testing subsets'], ['validation and testing subsets', 'i.e.', '97.0 % ( Easy )'], ['validation and testing subsets', 'i.e.', '96.1 % ( Medium )'], ['91.8 % ( Hard )', 'for', 'validation subset'], ['91.2 % ( Hard )', 'for', 'testing subset']]","[['our face detector', 'has', 'some new state - of - the - art results']]",[],"[['Results', 'has', 'our face detector']]",face_detection,0,141
1900,research-problem,EXTD : Extremely Tiny Face Detector via Iterative Filter Reuse,[],"[('EXTD', (0, 1)), ('Extremely Tiny Face Detector', (2, 6))]",[],"[['EXTD', 'has', 'Extremely Tiny Face Detector']]",[],[],face_detection,1,2
1901,research-problem,"In this paper , we propose a new multi-scale face detector having an extremely tiny number of parameters ( EXTD ) , less than 0.1 million , as well as achieving comparable performance to deep heavy detectors .","[('propose', (5, 6)), ('having', (11, 12)), ('less than', (22, 24)), ('achieving', (30, 31)), ('to', (33, 34))]","[('new multi-scale face detector', (7, 11)), ('extremely tiny number of parameters ( EXTD )', (13, 21)), ('0.1 million', (24, 26)), ('comparable performance', (31, 33)), ('deep heavy detectors', (34, 37))]","[['new multi-scale face detector', 'having', 'extremely tiny number of parameters ( EXTD )'], ['extremely tiny number of parameters ( EXTD )', 'less than', '0.1 million'], ['new multi-scale face detector', 'achieving', 'comparable performance'], ['comparable performance', 'to', 'deep heavy detectors']]","[['new multi-scale face detector', 'has', 'extremely tiny number of parameters ( EXTD )']]","[['Research problem', 'propose', 'new multi-scale face detector']]",[],face_detection,1,4
1902,model,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .","[('propose', (5, 6))]","[('extremely', (12, 13))]",[],[],"[['Model', 'propose', 'extremely']]",[],face_detection,1,28
1903,model,"The main discovery is that we can share the network in generating each feature - map , as shown in .","[('share', (7, 8)), ('in generating', (10, 12))]","[('network', (9, 10)), ('each feature - map', (12, 16))]","[['network', 'in generating', 'each feature - map']]",[],"[['Model', 'share', 'network']]",[],face_detection,1,29
1904,model,"We note that our model does not require any extra layer commonly defined as in , and is trained from scratch .","[('note', (1, 2)), ('does not require', (5, 8)), ('trained from', (18, 20))]","[('our model', (3, 5)), ('any extra layer', (8, 11)), ('scratch', (20, 21))]","[['our model', 'does not require', 'any extra layer'], ['our model', 'trained from', 'scratch']]",[],"[['Model', 'note', 'our model']]",[],face_detection,1,37
1905,model,"We propose an iterative network sharing model for multi-stage face detection which can significantly reduce the parameter size , as well as provide abundant object semantic information to the lower stage feature maps .","[('propose', (1, 2)), ('for', (7, 8)), ('can', (12, 13)), ('to', (27, 28))]","[('multi-stage face detection', (8, 11)), ('significantly reduce', (13, 15)), ('parameter size', (16, 18)), ('abundant object semantic information', (23, 27)), ('lower stage feature maps', (29, 33))]","[['multi-stage face detection', 'can', 'significantly reduce'], ['abundant object semantic information', 'to', 'lower stage feature maps']]","[['significantly reduce', 'has', 'parameter size']]","[['Model', 'propose', 'multi-stage face detection']]",[],face_detection,1,40
1906,experimental-setup,"Using the hard negative mining technique , we balance the ratio of positive and negative samples N neg / N pos to 3 and the balancing parameter ?","[('Using', (0, 1)), ('balance', (8, 9)), ('of', (11, 12)), ('to', (21, 22))]","[('hard negative mining technique', (2, 6)), ('ratio', (10, 11)), ('positive and negative samples N neg / N pos', (12, 21)), ('3', (22, 23)), ('balancing parameter', (25, 27))]","[['hard negative mining technique', 'balance', 'ratio'], ['hard negative mining technique', 'balance', 'balancing parameter'], ['ratio', 'of', 'positive and negative samples N neg / N pos'], ['positive and negative samples N neg / N pos', 'to', '3']]",[],"[['Experimental setup', 'Using', 'hard negative mining technique']]",[],face_detection,1,135
1907,experimental-setup,is set to 4 .,"[('set to', (1, 3))]","[('4', (3, 4))]",[],[],[],[],face_detection,1,136
1908,experimental-setup,The proposed method is implemented with PyTorch and NAVER Smart Machine Learning ( NSML ) system .,"[('implemented with', (4, 6))]","[('PyTorch and NAVER Smart Machine Learning ( NSML ) system', (6, 16))]",[],[],"[['Experimental setup', 'implemented with', 'PyTorch and NAVER Smart Machine Learning ( NSML ) system']]",[],face_detection,1,140
1909,code,Code will be available at https ://github.com/clovaai.,[],[],[],[],[],[],face_detection,1,142
1910,experimental-setup,"For the model , we designed three variations which have a different number of parameters , lighter one having 0.063 M parameters with 32 channels for each feature maps , intermediate one having 0.1 M parameters with 48 channels , and the heavier one with 64 channels and 0.16M parameters when designed as FPN .","[('designed', (5, 6)), ('have', (9, 10)), ('having', (18, 19)), ('with', (22, 23)), ('for', (25, 26)), ('with', (36, 37))]","[('three variations', (6, 8)), ('different number of parameters', (11, 15)), ('lighter one', (16, 18)), ('0.063 M parameters', (19, 22)), ('32 channels', (23, 25)), ('each feature maps', (26, 29)), ('intermediate one', (30, 32)), ('0.1 M parameters', (33, 36)), ('48 channels', (37, 39)), ('heavier one', (42, 44)), ('64 channels and 0.16M parameters', (45, 50)), ('FPN', (53, 54))]","[['three variations', 'have', 'different number of parameters'], ['lighter one', 'having', '0.063 M parameters'], ['0.063 M parameters', 'with', '32 channels'], ['32 channels', 'for', 'each feature maps'], ['0.1 M parameters', 'with', '48 channels']]","[['three variations', 'has', 'different number of parameters'], ['lighter one', 'has', '0.063 M parameters'], ['intermediate one', 'has', '0.1 M parameters']]","[['Experimental setup', 'designed', 'three variations']]",[],face_detection,1,159
1911,experimental-setup,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .","[('of', (3, 4)), ('set to', (9, 11)), ('identical to', (15, 17)), ('of', (21, 22))]","[('negative slope', (1, 3)), ('Leaky - ReLU', (5, 8)), ('0.25', (11, 12)), ('initial negative slope', (18, 21)), ('PReLU', (23, 24))]","[['negative slope', 'of', 'Leaky - ReLU'], ['initial negative slope', 'of', 'PReLU'], ['Leaky - ReLU', 'set to', '0.25'], ['0.25', 'identical to', 'initial negative slope'], ['initial negative slope', 'of', 'PReLU']]",[],[],"[['Experimental setup', 'has', 'negative slope']]",face_detection,1,163
1912,experiments,"For a fair comparison , all the inference processes of the models are implemented by PyTorch 1.0 .","[('of', (9, 10)), ('implemented by', (13, 15))]","[('all the inference processes', (5, 9)), ('models', (11, 12)), ('PyTorch 1.0', (15, 17))]","[['all the inference processes', 'of', 'models'], ['all the inference processes', 'implemented by', 'PyTorch 1.0']]",[],[],[],face_detection,1,176
1913,experiments,Comparison to the Existing Methods :,[],"[('Comparison to the Existing Methods', (0, 5))]",[],[],[],[],face_detection,1,177
1914,experiments,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .","[('compared to', (1, 3)), ('such as', (6, 8)), ('achieved', (24, 25))]","[('SOTA face detectors', (3, 6)), ('Pyra - midBox and DSFD', (8, 13)), ('our best model EXTD - FPN - 64 - PReLU', (14, 24)), ('lower results', (25, 27))]","[['SOTA face detectors', 'such as', 'Pyra - midBox and DSFD'], ['our best model EXTD - FPN - 64 - PReLU', 'achieved', 'lower results']]","[['SOTA face detectors', 'name', 'Pyra - midBox and DSFD']]",[],[],face_detection,1,179
1915,experiments,The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,"[('between', (2, 3)), ('on', (8, 9)), ('was', (13, 14))]","[('margin', (1, 2)), ('PyramidBox and the proposed model', (3, 8)), ('WIDER FACE hard case', (9, 13)), ('3.4 %', (14, 16))]","[['margin', 'between', 'PyramidBox and the proposed model'], ['PyramidBox and the proposed model', 'on', 'WIDER FACE hard case'], ['WIDER FACE hard case', 'was', '3.4 %']]",[],[],[],face_detection,1,180
1916,experiments,"The m AP gap to DSFD , which is tremendously heavier , is about 5.0 % , but it would be safe to suggest that the proposed method offers more decent trade - off in that DSFD uses about 2860 times more parameters than the proposed method .","[('to', (4, 5)), ('is about', (12, 14)), ('uses about', (37, 39)), ('than', (43, 44))]","[('m AP gap', (1, 4)), ('DSFD', (5, 6)), ('tremendously heavier', (9, 11)), ('5.0 %', (14, 16)), ('2860 times more parameters', (39, 43)), ('proposed method', (45, 47))]","[['m AP gap', 'to', 'DSFD'], ['2860 times more parameters', 'than', 'proposed method']]",[],[],[],face_detection,1,182
1917,experiments,"When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .","[('comes to', (2, 4)), ('got', (11, 12)), ('than', (15, 16))]","[('our SSD - based variations', (4, 9)), ('lower mAP results', (12, 15)), ('FPN - based variants', (16, 20))]","[['our SSD - based variations', 'got', 'lower mAP results'], ['lower mAP results', 'than', 'FPN - based variants']]","[['our SSD - based variations', 'has', 'lower mAP results']]",[],[],face_detection,1,186
1918,experiments,"However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .","[('compared with', (3, 5)), ('trained with', (8, 10)), ('achieved', (21, 22))]","[('S3FD version', (6, 8)), ('Mo - bile FaceNet backbone network', (10, 16)), ('proposed SSD variants', (18, 21)), ('comparable or better detection performance', (22, 27))]","[['S3FD version', 'trained with', 'Mo - bile FaceNet backbone network'], ['proposed SSD variants', 'achieved', 'comparable or better detection performance']]",[],[],[],face_detection,1,187
1919,experiments,Detection performance regarding the Face Scale :,"[('regarding', (2, 3))]","[('Detection performance', (0, 2)), ('Face Scale', (4, 6))]","[['Detection performance', 'regarding', 'Face Scale']]",[],[],[],face_detection,1,194
1920,experiments,"From the table , we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases .","[('see', (6, 7)), ('achieved', (10, 11)), ('in', (13, 14)), ('than', (18, 19))]","[('our method', (8, 10)), ('higher performance', (11, 13)), ('WIDER FACE hard dataset', (14, 18)), ('other cases', (19, 21))]","[['our method', 'achieved', 'higher performance'], ['higher performance', 'in', 'WIDER FACE hard dataset'], ['higher performance', 'than', 'other cases'], ['WIDER FACE hard dataset', 'than', 'other cases']]",[],[],[],face_detection,1,196
1921,experiments,"First , for all the different channel width , FPN based architecture achieved better detection performance compared to SSD based architecture , especially for detecting small faces .","[('for', (2, 3)), ('achieved', (12, 13)), ('compared to', (16, 18)), ('especially for detecting', (22, 25))]","[('all the different channel width', (3, 8)), ('FPN based architecture', (9, 12)), ('better detection performance', (13, 16)), ('SSD based architecture', (18, 21)), ('small faces', (25, 27))]","[['FPN based architecture', 'achieved', 'better detection performance'], ['better detection performance', 'compared to', 'SSD based architecture'], ['better detection performance', 'especially for detecting', 'small faces'], ['SSD based architecture', 'especially for detecting', 'small faces']]","[['all the different channel width', 'has', 'FPN based architecture']]",[],[],face_detection,1,204
1922,experiments,"As the channel width increased by 32 to 64 , we can see that the detection accuracy significantly enhanced for all the cases ; Easy , Medium , and Hard .","[('increased by', (4, 6)), ('see that', (12, 14)), ('for', (19, 20))]","[('channel width', (2, 4)), ('32 to 64', (6, 9)), ('detection accuracy', (15, 17)), ('significantly enhanced', (17, 19)), ('all the cases', (20, 23)), ('Easy , Medium , and Hard', (24, 30))]","[['channel width', 'increased by', '32 to 64'], ['channel width', 'see that', 'detection accuracy'], ['significantly enhanced', 'for', 'all the cases']]","[['detection accuracy', 'has', 'significantly enhanced']]",[],[],face_detection,1,209
1923,experiments,"In all the cases including FPN based and SSD based structures , PReLU was the most effective choice when it comes to mAP , but the gap between Leaky - ReLU was not that significant for the FPN variants .","[('including', (4, 5)), ('was', (13, 14)), ('when it comes to', (18, 22))]","[('FPN', (5, 6)), ('PReLU', (12, 13)), ('most effective choice', (15, 18)), ('mAP', (22, 23)), ('gap', (26, 27)), ('Leaky - ReLU', (28, 31)), ('not that significant', (32, 35))]","[['PReLU', 'was', 'most effective choice'], ['most effective choice', 'when it comes to', 'mAP']]",[],[],[],face_detection,1,213
1924,experiments,"When tested with SSD based architecture , PReLU outperformed Leaky - ReLU with larger margin than those using FPN structure .","[('tested with', (1, 3)), ('with', (12, 13)), ('than those using', (15, 18))]","[('SSD based architecture', (3, 6)), ('PReLU', (7, 8)), ('outperformed', (8, 9)), ('Leaky - ReLU', (9, 12)), ('larger margin', (13, 15)), ('FPN structure', (18, 20))]","[['Leaky - ReLU', 'with', 'larger margin'], ['larger margin', 'than those using', 'FPN structure']]","[['SSD based architecture', 'has', 'PReLU'], ['PReLU', 'has', 'outperformed'], ['outperformed', 'has', 'Leaky - ReLU']]",[],[],face_detection,1,214
1925,experiments,It is worth noting that ReLU occurred notable performance decreases especially when the channel width was small for both SSD and FPN cases .,"[('worth noting', (2, 4)), ('occurred', (6, 7)), ('for', (17, 18))]","[('ReLU', (5, 6)), ('notable performance decreases', (7, 10)), ('channel', (13, 14)), ('small', (16, 17)), ('SSD and FPN cases', (19, 23))]","[['ReLU', 'occurred', 'notable performance decreases'], ['small', 'for', 'SSD and FPN cases']]","[['ReLU', 'has', 'notable performance decreases'], ['notable performance decreases', 'has', 'channel']]",[],[],face_detection,1,215
1926,experiments,"When the channel width was set to 32 , m AP for all the three cases were lower than 10 % to 20 % compared to those using other activation functions .","[('set to', (5, 7)), ('for', (11, 12)), ('lower than', (17, 19)), ('compared to', (24, 26)), ('using', (27, 28))]","[('channel width', (2, 4)), ('32', (7, 8)), ('m AP', (9, 11)), ('all the three cases', (12, 16)), ('10 % to 20 %', (19, 24)), ('other activation functions', (28, 31))]","[['channel width', 'set to', '32'], ['m AP', 'for', 'all the three cases'], ['all the three cases', 'lower than', '10 % to 20 %']]","[['channel width', 'has', '32']]",[],[],face_detection,1,216
1927,experimental-setup,"For training the proposed architecture , a stochastic gradient descent optimizer ( SGD ) with learning rate 1e ? 3 , with 0.9 momentum , 0.0005 weight decay , and batch size 16 is used .","[('For', (0, 1)), ('with', (14, 15)), ('with', (21, 22))]","[('stochastic gradient descent optimizer ( SGD )', (7, 14)), ('learning rate', (15, 17)), ('1e ? 3', (17, 20)), ('0.9 momentum', (22, 24)), ('0.0005 weight decay', (25, 28)), ('batch size 16', (30, 33))]","[['stochastic gradient descent optimizer ( SGD )', 'with', 'learning rate'], ['stochastic gradient descent optimizer ( SGD )', 'with', '0.9 momentum'], ['learning rate', 'with', '0.9 momentum'], ['1e ? 3', 'with', '0.9 momentum']]","[['learning rate', 'has', '1e ? 3']]","[['Experimental setup', 'For', 'stochastic gradient descent optimizer ( SGD )']]",[],face_detection,1,226
1928,experimental-setup,"The maximum iteration number is basically set to 240K , and we drop the learning rate to 1e ? 4 and 1e ? 5 at 120 K and 180K iterations .","[('set to', (6, 8)), ('drop', (12, 13)), ('to', (16, 17)), ('at', (24, 25))]","[('maximum iteration number', (1, 4)), ('240K', (8, 9)), ('learning rate', (14, 16)), ('1e ? 4 and 1e ? 5', (17, 24)), ('120 K', (25, 27)), ('180K iterations', (28, 30))]","[['maximum iteration number', 'set to', '240K'], ['learning rate', 'set to', '1e ? 4 and 1e ? 5'], ['maximum iteration number', 'drop', 'learning rate'], ['learning rate', 'to', '1e ? 4 and 1e ? 5'], ['learning rate', 'to', '180K iterations'], ['1e ? 4 and 1e ? 5', 'at', '120 K']]","[['maximum iteration number', 'has', '240K'], ['learning rate', 'has', '1e ? 4 and 1e ? 5']]",[],"[['Experimental setup', 'has', 'maximum iteration number']]",face_detection,1,228
1929,research-problem,DSFD : Dual Shot Face Detector,[],"[('DSFD', (0, 1))]",[],[],[],[],face_detection,10,2
1930,research-problem,"Face detection is a fundamental step for various facial applications , like face alignment , parsing , recognition , and verification .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,10,9
1931,model,The first one is mainly based on the Region Proposal Network ( RPN ) adopted in Faster RCNN and employs two stage detection schemes .,"[('mainly based on', (4, 7)), ('adopted in', (14, 16)), ('employs', (19, 20))]","[('Region Proposal Network ( RPN )', (8, 14)), ('Faster RCNN', (16, 18)), ('two stage detection schemes', (20, 24))]","[['Region Proposal Network ( RPN )', 'adopted in', 'Faster RCNN'], ['Region Proposal Network ( RPN )', 'employs', 'two stage detection schemes']]",[],"[['Model', 'mainly based on', 'Region Proposal Network ( RPN )']]",[],face_detection,10,13
1932,model,RPN is trained end - to - end and generates highquality region proposals which are further refined by Fast R - CNN detector .,"[('trained', (2, 3)), ('generates', (9, 10))]","[('RPN', (0, 1)), ('end - to - end', (3, 8)), ('highquality region proposals', (10, 13)), ('Fast R - CNN detector', (18, 23))]","[['RPN', 'trained', 'end - to - end'], ['RPN', 'generates', 'highquality region proposals']]",[],[],"[['Model', 'has', 'RPN']]",face_detection,10,14
1933,research-problem,"The other one is Single Shot Detector ( SSD ) based one - stage methods , which get rid of RPN , and directly predict the bounding boxes and confidence .","[('get rid of', (17, 20)), ('directly predict', (23, 25))]","[('Single Shot Detector ( SSD ) based one - stage methods', (4, 15)), ('RPN', (20, 21)), ('bounding boxes', (26, 28))]","[['Single Shot Detector ( SSD ) based one - stage methods', 'get rid of', 'RPN'], ['Single Shot Detector ( SSD ) based one - stage methods', 'directly predict', 'bounding boxes']]",[],[],[],face_detection,10,15
1934,hyperparameters,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,"[('initialized by', (4, 6)), ('on', (11, 12))]","[('backbone networks', (1, 3)), ('pretrained VGG / ResNet', (7, 11)), ('Image Net', (12, 14))]","[['backbone networks', 'initialized by', 'pretrained VGG / ResNet'], ['pretrained VGG / ResNet', 'on', 'Image Net']]",[],[],"[['Hyperparameters', 'has', 'backbone networks']]",face_detection,10,127
1935,hyperparameters,All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,"[('initialized by', (8, 10))]","[(""newly added convolution layers ' parameters"", (1, 7)), (""' xavier ' method"", (11, 15))]","[[""newly added convolution layers ' parameters"", 'initialized by', ""' xavier ' method""]]",[],[],"[['Hyperparameters', 'has', ""newly added convolution layers ' parameters""]]",face_detection,10,128
1936,hyperparameters,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .","[('use', (1, 2)), ('with', (3, 4)), ('to fine - tune', (10, 14))]","[('SGD', (2, 3)), ('0.9 momentum', (4, 6)), ('0.0005 weight decay', (7, 10)), ('our DSFD model', (14, 17))]","[['SGD', 'with', '0.9 momentum'], ['0.0005 weight decay', 'to fine - tune', 'our DSFD model']]",[],"[['Hyperparameters', 'use', 'SGD']]",[],face_detection,10,129
1937,hyperparameters,The batch size is set to 16 .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('16', (6, 7))]","[['batch size', 'set to', '16']]","[['batch size', 'has', '16']]",[],"[['Hyperparameters', 'has', 'batch size']]",face_detection,10,130
1938,hyperparameters,"The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .","[('set to', (4, 6)), ('for', (8, 9)), ('for', (27, 28))]","[('learning rate', (1, 3)), ('10 ?3', (6, 8)), ('first 40 k steps', (10, 14)), ('decay', (17, 18)), ('10 ? 4 and 10 ? 5', (20, 27)), ('two 10 k steps', (28, 32))]","[['learning rate', 'set to', '10 ?3'], ['10 ?3', 'for', 'first 40 k steps'], ['10 ? 4 and 10 ? 5', 'for', 'two 10 k steps'], ['10 ? 4 and 10 ? 5', 'for', 'two 10 k steps']]","[['learning rate', 'has', '10 ?3'], ['decay', 'has', '10 ? 4 and 10 ? 5']]",[],"[['Hyperparameters', 'has', 'learning rate']]",face_detection,10,131
1939,hyperparameters,Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,"[('applied with', (3, 5)), ('of', (7, 8)), ('to produce', (9, 11))]","[('Non-maximum suppression', (0, 2)), ('jaccard overlap', (5, 7)), ('0.3', (8, 9)), ('top 750 high confident bounding boxes per image', (11, 19))]","[['Non-maximum suppression', 'applied with', 'jaccard overlap'], ['jaccard overlap', 'of', '0.3'], ['0.3', 'to produce', 'top 750 high confident bounding boxes per image']]",[],[],"[['Hyperparameters', 'has', 'Non-maximum suppression']]",face_detection,10,133
1940,code,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,[],"[('https://github.com/TencentYoutuResearch/FaceDetection-DSFD', (8, 9))]",[],[],[],[],face_detection,10,135
1941,results,Analysis on DSFD,[],[],[],[],[],[],face_detection,10,138
1942,experiments,"Finally , we can improve our DSFD to 96.6 % , 95.7 % , 90.4 % with ResNet 152 as the backbone . Besides , shows that our improved anchor matching strategy greatly increases the number of ground truth faces that are closed to the anchor , which can reduce the contradiction between the discrete anchor scales and continuous face scales .","[('improve', (4, 5)), ('to', (7, 8)), ('with', (16, 17)), ('greatly increases', (32, 34))]","[('our DSFD', (5, 7)), ('96.6 % , 95.7 % , 90.4 %', (8, 16)), ('ResNet 152', (17, 19)), ('our improved anchor matching strategy', (27, 32)), ('number of', (35, 37)), ('anchor', (45, 46))]","[['our DSFD', 'to', '96.6 % , 95.7 % , 90.4 %'], ['96.6 % , 95.7 % , 90.4 %', 'with', 'ResNet 152'], ['our improved anchor matching strategy', 'greatly increases', 'number of']]","[['our DSFD', 'has', '96.6 % , 95.7 % , 90.4 %']]",[],[],face_detection,10,154
1943,results,Comparison with RFB,[],[],[],[],[],[],face_detection,10,156
1944,experiments,"2 ) Auxiliary loss based on progressive anchor is used to train all 12 different scale detection feature maps , and it improves the performance on easy , medium and hard faces simultaneously .","[('based on', (4, 6)), ('improves', (22, 23)), ('on', (25, 26))]","[('Auxiliary loss', (2, 4)), ('progressive anchor', (6, 8)), ('all 12 different scale detection feature maps', (12, 19)), ('performance', (24, 25)), ('easy , medium and hard faces simultaneously', (26, 33))]","[['Auxiliary loss', 'based on', 'progressive anchor'], ['Auxiliary loss', 'improves', 'performance'], ['performance', 'on', 'easy , medium and hard faces simultaneously']]",[],[],[],face_detection,10,164
1945,results,"3 ) Our improved anchor matching provides better initial anchors and ground - truth faces to regress anchor from faces , which achieves the improvements of 0.3 % , 0.1 % , 0.3 % on three settings , respectively .","[('provides', (6, 7)), ('to', (15, 16)), ('from', (18, 19)), ('achieves', (22, 23)), ('of', (25, 26))]","[('Our improved anchor matching', (2, 6)), ('better initial anchors and ground - truth faces', (7, 15)), ('regress anchor', (16, 18)), ('faces', (19, 20)), ('improvements', (24, 25)), ('0.3 %', (26, 28))]","[['Our improved anchor matching', 'provides', 'better initial anchors and ground - truth faces'], ['better initial anchors and ground - truth faces', 'to', 'regress anchor'], ['regress anchor', 'from', 'faces'], ['regress anchor', 'achieves', 'improvements'], ['improvements', 'of', '0.3 %']]",[],[],"[['Results', 'has', 'Our improved anchor matching']]",face_detection,10,165
1946,results,"Additionally , when we enlarge the training batch size ( i.e. , Large BS ) , the result in hard setting can get 91.2 % AP .","[('enlarge', (4, 5)), ('i.e.', (10, 11)), ('result in', (17, 19)), ('can get', (21, 23))]","[('training batch size', (6, 9)), ('hard setting', (19, 21)), ('91.2 % AP', (23, 26))]","[['training batch size', 'result in', 'hard setting'], ['training batch size', 'can get', '91.2 % AP'], ['hard setting', 'can get', '91.2 % AP']]","[['training batch size', 'has', 'hard setting']]",[],[],face_detection,10,166
1947,results,"From , DSFD with SE - ResNeXt101 324d got 95.7 % , 94.8 % , 88.9 % , on easy , medium and hard settings respectively , which indicates that more complexity model and higher Top - 1 I ma - geNet classification accuracy may not benefit face detection AP .","[('with', (3, 4)), ('got', (8, 9)), ('on', (18, 19))]","[('DSFD', (2, 3)), ('SE - ResNeXt101 324d', (4, 8)), ('95.7 % , 94.8 % , 88.9 %', (9, 17)), ('easy , medium and hard settings', (19, 25))]","[['DSFD', 'with', 'SE - ResNeXt101 324d'], ['DSFD', 'got', '95.7 % , 94.8 % , 88.9 %'], ['SE - ResNeXt101 324d', 'got', '95.7 % , 94.8 % , 88.9 %'], ['95.7 % , 94.8 % , 88.9 %', 'on', 'easy , medium and hard settings']]",[],[],[],face_detection,10,170
1948,results,Our DSFD enjoys high inference speed benefited from simply using the second shot detection results .,"[('enjoys', (2, 3)), ('benefited from', (6, 8))]","[('Our', (0, 1)), ('high inference speed', (3, 6)), ('second shot detection results', (11, 15))]","[['Our', 'enjoys', 'high inference speed']]",[],[],"[['Results', 'has', 'Our']]",face_detection,10,172
1949,results,"As shown in , our DSFD achieves the best performance among all of the state - of - the - art face detectors based on the average precision ( AP ) across the three subsets , i.e. , 96.6 % ( Easy ) , 95.7 % ( Medium ) and 90.4 % ( Hard ) on validation set , and 96.0 % ( Easy ) , 95.3 % ( Medium ) and 90.0 % ( Hard ) on test set .","[('achieves', (6, 7)), ('among', (10, 11)), ('based on', (23, 25)), ('across', (31, 32)), ('i.e.', (36, 37)), ('on', (55, 56)), ('on', (77, 78))]","[('our DSFD', (4, 6)), ('best performance', (8, 10)), ('all of the state - of - the - art face detectors', (11, 23)), ('average precision ( AP )', (26, 31)), ('three subsets', (33, 35)), ('96.6 % ( Easy )', (38, 43)), ('95.7 % ( Medium )', (44, 49)), ('90.4 % ( Hard )', (50, 55)), ('validation set', (56, 58)), ('96.0 % ( Easy )', (60, 65)), ('95.3 % ( Medium )', (66, 71)), ('90.0 % ( Hard )', (72, 77)), ('test set', (78, 80))]","[['our DSFD', 'achieves', 'best performance'], ['best performance', 'among', 'all of the state - of - the - art face detectors'], ['best performance', 'among', '90.0 % ( Hard )'], ['all of the state - of - the - art face detectors', 'based on', 'average precision ( AP )'], ['average precision ( AP )', 'across', 'three subsets'], ['average precision ( AP )', 'i.e.', '96.6 % ( Easy )'], ['three subsets', 'i.e.', '96.6 % ( Easy )'], ['90.4 % ( Hard )', 'on', 'validation set'], ['90.0 % ( Hard )', 'on', 'test set']]","[['our DSFD', 'has', 'best performance']]",[],"[['Results', 'has', 'our DSFD']]",face_detection,10,182
1950,experiments,"Since WIDER FACE has bounding box annotation while faces in FDDB are represented by ellipses , we learn a post - hoc ellipses regressor to transform the final prediction results .","[('while', (7, 8)), ('in', (9, 10)), ('represented by', (12, 14)), ('learn', (17, 18)), ('to transform', (24, 26))]","[('bounding box annotation', (4, 7)), ('faces', (8, 9)), ('FDDB', (10, 11)), ('ellipses', (14, 15)), ('post - hoc ellipses regressor', (19, 24)), ('final prediction results', (27, 30))]","[['bounding box annotation', 'while', 'faces'], ['faces', 'in', 'FDDB'], ['faces', 'represented by', 'ellipses'], ['ellipses', 'learn', 'post - hoc ellipses regressor'], ['post - hoc ellipses regressor', 'to transform', 'final prediction results']]",[],[],[],face_detection,10,186
1951,experiments,"As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .","[('achieves', (6, 7)), ('on', (15, 16)), ('i.e.', (23, 24)), ('when', (29, 30)), ('equals to', (35, 37))]","[('our DSFD', (4, 6)), ('state - of - the - art performance', (7, 15)), ('99.1 % and 86.2 %', (24, 29)), ('number of false positives', (31, 35)), ('1 , 000', (37, 40))]","[['our DSFD', 'achieves', 'state - of - the - art performance'], ['99.1 % and 86.2 %', 'when', 'number of false positives'], ['number of false positives', 'equals to', '1 , 000']]",[],[],[],face_detection,10,187
1952,experiments,"After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .","[('adding', (1, 2)), ('to', (4, 5)), ('of', (12, 13)), ('can be', (15, 17))]","[('additional annotations', (2, 4)), ('unlabeled faces', (6, 8)), ('false positives', (10, 12)), ('our model', (13, 15)), ('further reduced', (17, 19)), ('outperform', (20, 21)), ('all other methods', (21, 24))]","[['additional annotations', 'to', 'unlabeled faces'], ['false positives', 'of', 'our model'], ['false positives', 'can be', 'further reduced'], ['false positives', 'can be', 'outperform'], ['our model', 'can be', 'further reduced'], ['our model', 'can be', 'outperform']]","[['additional annotations', 'has', 'unlabeled faces'], ['outperform', 'has', 'all other methods']]",[],[],face_detection,10,188
1953,research-problem,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,[],[],[],[],[],[],face_detection,11,2
1954,model,These complementary scale - specific detectors are combined to produce a strong multi-scale object detector .,"[('combined to produce', (7, 10))]","[('complementary scale - specific detectors', (1, 6)), ('strong multi-scale object detector', (11, 15))]","[['complementary scale - specific detectors', 'combined to produce', 'strong multi-scale object detector']]",[],[],"[['Model', 'has', 'complementary scale - specific detectors']]",face_detection,11,7
1955,model,"The R - CNN samples object proposals at multiple scales , using a preliminary attention stage , and then warps these proposals to the size ( e.g. 224224 ) supported by the CNN .","[('samples', (4, 5)), ('at', (7, 8)), ('using', (11, 12)), ('warps', (19, 20)), ('to', (22, 23)), ('supported by', (29, 31))]","[('R - CNN', (1, 4)), ('object proposals', (5, 7)), ('multiple scales', (8, 10)), ('preliminary attention stage', (13, 16)), ('proposals', (21, 22)), ('size ( e.g. 224224 )', (24, 29)), ('CNN', (32, 33))]","[['R - CNN', 'samples', 'object proposals'], ['object proposals', 'at', 'multiple scales'], ['R - CNN', 'warps', 'proposals'], ['proposals', 'to', 'size ( e.g. 224224 )'], ['size ( e.g. 224224 )', 'supported by', 'CNN']]",[],[],"[['Model', 'has', 'R - CNN']]",face_detection,11,16
1956,model,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .","[('proposes', (2, 3)), ('denoted', (9, 10)), ('for', (20, 21))]","[('unified multi-scale deep CNN', (4, 8)), ('multi -scale CNN ( MS - CNN )', (11, 19)), ('fast object detection', (21, 24))]","[['unified multi-scale deep CNN', 'denoted', 'multi -scale CNN ( MS - CNN )'], ['unified multi-scale deep CNN', 'for', 'fast object detection'], ['multi -scale CNN ( MS - CNN )', 'for', 'fast object detection']]","[['unified multi-scale deep CNN', 'name', 'multi -scale CNN ( MS - CNN )']]","[['Model', 'proposes', 'unified multi-scale deep CNN']]",[],face_detection,11,29
1957,model,Both of them are learned end - to - end and share computations .,"[('learned', (4, 5))]","[('end - to - end', (5, 10)), ('share computations', (11, 13))]",[],[],[],[],face_detection,11,31
1958,model,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,"[('at', (3, 4)), ('combined to form', (8, 11))]","[('complimentary detectors', (1, 3)), ('different output layers', (4, 7)), ('strong multi-scale detector', (12, 15))]","[['complimentary detectors', 'at', 'different output layers'], ['complimentary detectors', 'combined to form', 'strong multi-scale detector'], ['different output layers', 'combined to form', 'strong multi-scale detector']]",[],[],"[['Model', 'has', 'complimentary detectors']]",face_detection,11,35
1959,model,"This is shown to produce accurate object proposals on detection benchmarks with large variation of scale , such as KITTI , achieving a recall of over 95 % for only 100 proposals .","[('shown', (2, 3)), ('produce', (4, 5)), ('on', (8, 9)), ('with', (11, 12)), ('such as', (17, 19)), ('achieving', (21, 22)), ('of', (24, 25)), ('for', (28, 29))]","[('accurate', (5, 6)), ('object proposals', (6, 8)), ('detection benchmarks', (9, 11)), ('large variation of scale', (12, 16)), ('KITTI', (19, 20)), ('recall', (23, 24)), ('over 95 %', (25, 28)), ('only 100 proposals', (29, 32))]","[['object proposals', 'on', 'detection benchmarks'], ['detection benchmarks', 'with', 'large variation of scale'], ['large variation of scale', 'such as', 'KITTI'], ['large variation of scale', 'achieving', 'recall'], ['recall', 'of', 'over 95 %'], ['over 95 %', 'for', 'only 100 proposals']]","[['accurate', 'has', 'object proposals']]",[],[],face_detection,11,36
1960,model,A second contribution of this work is the use of feature upsampling as an alternative to input upsampling .,"[('use of', (8, 10)), ('as', (12, 13))]","[('feature upsampling', (10, 12)), ('input upsampling', (16, 18))]",[],[],"[['Model', 'use of', 'feature upsampling']]",[],face_detection,11,37
1961,experimental-setup,"Learning is initialized with the model generated by the first learning stage of the proposal network , described in Section 3.4 .","[('initialized with', (2, 4)), ('generated by', (6, 8)), ('of', (12, 13))]","[('Learning', (0, 1)), ('model', (5, 6)), ('first learning stage', (9, 12)), ('proposal network', (14, 16))]","[['Learning', 'initialized with', 'model'], ['model', 'generated by', 'first learning stage'], ['first learning stage', 'of', 'proposal network']]",[],[],"[['Experimental setup', 'has', 'Learning']]",face_detection,11,142
1962,experimental-setup,"The learning rate is set to 0.0005 , and reduced by a factor of 10 times after every 10,000 iterations .","[('set to', (4, 6)), ('reduced by', (9, 11)), ('after', (16, 17))]","[('learning rate', (1, 3)), ('0.0005', (6, 7)), ('factor of 10 times', (12, 16)), ('every 10,000 iterations', (17, 20))]","[['learning rate', 'set to', '0.0005'], ['learning rate', 'reduced by', 'factor of 10 times'], ['factor of 10 times', 'after', 'every 10,000 iterations']]","[['learning rate', 'has', '0.0005']]",[],"[['Experimental setup', 'has', 'learning rate']]",face_detection,11,143
1963,experimental-setup,"Learning stops after 25,000 iterations .","[('after', (2, 3))]","[('Learning', (0, 1)), ('stops', (1, 2)), ('25,000 iterations', (3, 5))]","[['stops', 'after', '25,000 iterations']]","[['Learning', 'has', 'stops']]",[],"[['Experimental setup', 'has', 'Learning']]",face_detection,11,144
1964,experimental-setup,The joint optimization of ( 6 ) is solved by back - propagation throughout the unified network .,"[('solved by', (8, 10)), ('throughout', (13, 14))]","[('joint optimization', (1, 3)), ('back - propagation', (10, 13)), ('unified network', (15, 17))]","[['joint optimization', 'solved by', 'back - propagation'], ['back - propagation', 'throughout', 'unified network']]",[],[],"[['Experimental setup', 'has', 'joint optimization']]",face_detection,11,145
1965,experimental-setup,"1 . Following , the parameters of layers "" conv 1 - 1 "" to "" conv2 - 2 "" are fixed during learning , for faster training .","[('of', (6, 7)), ('to', (14, 15)), ('fixed during', (21, 23)), ('for', (25, 26))]","[('parameters', (5, 6)), ('layers "" conv 1 - 1 ""', (7, 14)), ('learning', (23, 24)), ('faster training', (26, 28))]","[['parameters', 'of', 'layers "" conv 1 - 1 ""'], ['layers "" conv 1 - 1 ""', 'fixed during', 'learning'], ['learning', 'for', 'faster training']]",[],[],"[['Experimental setup', 'has', 'parameters']]",face_detection,11,147
1966,experiments,"Simply forwarding object patches , at the original scale , through the CNN impairs performance ( especially for small ones ) , since the pre-trained CNN models have a natural scale ( e.g. 224224 ) .","[('Simply forwarding', (0, 2)), ('at', (5, 6)), ('through', (10, 11))]","[('object patches', (2, 4)), ('original scale', (7, 9)), ('CNN impairs performance', (12, 15))]","[['object patches', 'at', 'original scale'], ['object patches', 'through', 'CNN impairs performance'], ['original scale', 'through', 'CNN impairs performance']]","[['object patches', 'has', 'original scale']]",[],[],face_detection,11,165
1967,experiments,Context Embedding,[],[],[],[],[],[],face_detection,11,182
1968,baselines,"Following , a model was trained for car detection and another for pedestrian / cyclist detection .","[('trained for', (5, 7)), ('another for', (10, 12))]","[('car detection', (7, 9)), ('pedestrian / cyclist detection', (12, 16))]",[],[],[],[],face_detection,11,199
1969,code,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn.","[('implemented in', (3, 5)), ('within', (7, 8))]","[('C ++', (5, 7)), ('Caffe toolbox', (9, 11))]","[['C ++', 'within', 'Caffe toolbox']]",[],"[['Code', 'implemented in', 'C ++']]",[],face_detection,11,202
1970,experimental-setup,An NVIDIA Titan GPU was used for CNN computations .,"[('used for', (5, 7))]","[('NVIDIA Titan GPU', (1, 4)), ('CNN computations', (7, 9))]","[['NVIDIA Titan GPU', 'used for', 'CNN computations']]",[],[],"[['Experimental setup', 'has', 'NVIDIA Titan GPU']]",face_detection,11,204
1971,results,"As expected , each layer has highest accuracy for the objects that match its scale .","[('for', (8, 9)), ('that match', (11, 13))]","[('each layer', (3, 5)), ('highest accuracy', (6, 8)), ('objects', (10, 11)), ('its scale', (13, 15))]","[['highest accuracy', 'for', 'objects'], ['highest accuracy', 'that match', 'its scale'], ['objects', 'that match', 'its scale']]","[['each layer', 'has', 'highest accuracy']]",[],"[['Results', 'has', 'each layer']]",face_detection,11,210
1972,results,"While the individual recall across scales is low , the combination of all detectors achieves high recall for all object scales .","[('across', (4, 5)), ('is', (6, 7)), ('combination', (10, 11)), ('achieves', (14, 15)), ('for', (17, 18))]","[('individual recall', (2, 4)), ('scales', (5, 6)), ('low', (7, 8)), ('all detectors', (12, 14)), ('high recall', (15, 17)), ('all object scales', (18, 21))]","[['individual recall', 'across', 'scales'], ['individual recall', 'is', 'low'], ['scales', 'is', 'low'], ['individual recall', 'combination', 'all detectors'], ['low', 'combination', 'all detectors'], ['all detectors', 'achieves', 'high recall'], ['high recall', 'for', 'all object scales']]",[],[],[],face_detection,11,211
1973,results,The effect of input size shows that the proposal network is fairly robust to the size of input images for cars and pedestrians .,"[('shows', (5, 6)), ('is', (10, 11)), ('to', (13, 14)), ('for', (19, 20))]","[('effect of input size', (1, 5)), ('proposal network', (8, 10)), ('fairly robust', (11, 13)), ('size of input images', (15, 19)), ('cars and pedestrians', (20, 23))]","[['effect of input size', 'shows', 'proposal network'], ['proposal network', 'is', 'fairly robust'], ['fairly robust', 'to', 'size of input images'], ['size of input images', 'for', 'cars and pedestrians']]","[['effect of input size', 'has', 'proposal network'], ['proposal network', 'has', 'fairly robust']]",[],"[['Results', 'has', 'effect of input size']]",face_detection,11,212
1974,results,"shows that , for the MS - CNN , detection can substantially benefit proposal generation , especially for pedestrians .","[('for', (3, 4)), ('especially for', (16, 18))]","[('MS - CNN', (5, 8)), ('detection', (9, 10)), ('substantially benefit', (11, 13)), ('proposal generation', (13, 15)), ('pedestrians', (18, 19))]","[['proposal generation', 'especially for', 'pedestrians']]","[['MS - CNN', 'has', 'detection'], ['detection', 'has', 'substantially benefit'], ['substantially benefit', 'has', 'proposal generation']]","[['Results', 'for', 'MS - CNN']]",[],face_detection,11,217
1975,results,"Comparison with the state - of - the - art compares the proposal generation network to BING , Selective Search , EdgeBoxes , MCG , 3 DOP and RPN .","[('Comparison', (0, 1)), ('compares', (10, 11)), ('to', (15, 16))]","[('state - of - the - art', (3, 10)), ('proposal generation network', (12, 15)), ('BING', (16, 17)), ('Selective Search', (18, 20)), ('EdgeBoxes', (21, 22)), ('MCG', (23, 24)), ('3 DOP', (25, 27)), ('RPN', (28, 29))]","[['state - of - the - art', 'compares', 'proposal generation network'], ['proposal generation network', 'to', 'BING'], ['proposal generation network', 'to', 'RPN']]","[['state - of - the - art', 'has', 'proposal generation network']]","[['Results', 'Comparison', 'state - of - the - art']]",[],face_detection,11,218
1976,results,The top row of the figure shows that the MS - CNN achieves a recall about 98 % with only 100 proposals .,"[('shows', (6, 7)), ('achieves', (12, 13)), ('with', (18, 19))]","[('MS - CNN', (9, 12)), ('recall', (14, 15)), ('about 98 %', (15, 18)), ('only 100 proposals', (19, 22))]","[['MS - CNN', 'achieves', 'recall'], ['recall', 'with', 'only 100 proposals'], ['about 98 %', 'with', 'only 100 proposals']]","[['recall', 'has', 'about 98 %']]",[],[],face_detection,11,219
1977,results,"For pedestrian , bootstrapping and mixture are close , but random is much worse .","[('For', (0, 1)), ('are', (6, 7)), ('is', (11, 12))]","[('pedestrian', (1, 2)), ('bootstrapping and', (3, 5)), ('mixture', (5, 6)), ('close', (7, 8)), ('random', (10, 11)), ('much worse', (12, 14))]","[['mixture', 'are', 'close'], ['random', 'is', 'much worse']]","[['pedestrian', 'has', 'bootstrapping and'], ['bootstrapping and', 'has', 'mixture'], ['random', 'has', 'much worse']]","[['Results', 'For', 'pedestrian']]",[],face_detection,11,241
1978,results,"As shown in , the deconvoltion layer helps inmost cases .","[('helps', (7, 8))]","[('deconvoltion layer', (5, 7)), ('inmost cases', (8, 10))]","[['deconvoltion layer', 'helps', 'inmost cases']]","[['deconvoltion layer', 'has', 'inmost cases']]",[],"[['Results', 'has', 'deconvoltion layer']]",face_detection,11,248
1979,research-problem,"In WSMA - Seg , multimodal annotations are proposed to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .","[('In', (0, 1)), ('proposed to achieve', (8, 11)), ('using', (16, 17)), ('develop', (24, 25)), ('to trace', (31, 33)), ('of', (34, 35))]","[('WSMA - Seg', (1, 4)), ('multimodal annotations', (5, 7)), ('instance - aware segmentation', (12, 16)), ('weakly supervised bounding boxes', (17, 21)), ('run-data - based following algorithm', (26, 31)), ('contours', (33, 34)), ('objects', (35, 36))]","[['multimodal annotations', 'proposed to achieve', 'instance - aware segmentation'], ['instance - aware segmentation', 'using', 'weakly supervised bounding boxes'], ['run-data - based following algorithm', 'to trace', 'contours'], ['contours', 'of', 'objects']]","[['WSMA - Seg', 'has', 'multimodal annotations']]","[['Research problem', 'In', 'WSMA - Seg']]",[],face_detection,12,8
1980,research-problem,"In addition , we propose a multi-scale pooling segmentation ( MSP - Seg ) as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg. Experimental results on multiple datasets show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors .","[('propose', (4, 5)), ('as', (14, 15)), ('of', (19, 20)), ('to achieve', (23, 25)), ('to enhance', (30, 32)), ('of', (35, 36)), ('show', (44, 45))]","[('multi-scale pooling segmentation ( MSP - Seg )', (6, 14)), ('underlying segmentation model', (16, 19)), ('WSMA - Seg', (20, 23)), ('more accurate segmentation', (26, 29)), ('detection accuracy', (33, 35)), ('WSMA - Seg.', (36, 39)), ('outperforms', (52, 53)), ('state - of - the - art detectors', (54, 62))]","[['multi-scale pooling segmentation ( MSP - Seg )', 'as', 'underlying segmentation model'], ['underlying segmentation model', 'of', 'WSMA - Seg'], ['detection accuracy', 'of', 'WSMA - Seg.'], ['underlying segmentation model', 'to achieve', 'more accurate segmentation'], ['multi-scale pooling segmentation ( MSP - Seg )', 'to enhance', 'detection accuracy'], ['underlying segmentation model', 'to enhance', 'detection accuracy'], ['detection accuracy', 'of', 'WSMA - Seg.']]","[['multi-scale pooling segmentation ( MSP - Seg )', 'has', 'underlying segmentation model'], ['WSMA - Seg.', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art detectors']]","[['Research problem', 'propose', 'multi-scale pooling segmentation ( MSP - Seg )']]",[],face_detection,12,9
1981,research-problem,Object detection in images is one of the most widely explored tasks in computer vision .,[],"[('Object detection in images', (0, 4))]",[],[],[],[],face_detection,12,11
1982,approach,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .","[('propose', (9, 10)), ('which uses', (23, 25)), ('to achieve', (27, 29)), ('without', (35, 36))]","[('weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', (11, 22)), ('segmentation models', (25, 27)), ('accurate and robust', (30, 33)), ('object detection', (33, 35)), ('NMS', (36, 37))]","[['weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', 'which uses', 'segmentation models'], ['segmentation models', 'to achieve', 'accurate and robust'], ['object detection', 'without', 'NMS']]","[['accurate and robust', 'has', 'object detection']]","[['Approach', 'propose', 'weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach']]",[],face_detection,12,19
1983,approach,"It consists of two phases , namely , a training and a testing phase .","[('consists of', (1, 3)), ('namely', (6, 7))]","[('two phases', (3, 5)), ('training', (9, 10)), ('testing phase', (12, 14))]","[['two phases', 'namely', 'training'], ['two phases', 'namely', 'testing phase']]","[['two phases', 'name', 'training']]","[['Approach', 'consists of', 'two phases']]",[],face_detection,12,20
1984,approach,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .","[('In', (0, 1)), ('first converts', (8, 10)), ('in', (15, 16)), ('to', (18, 19)), ('like', (22, 23)), ('called', (25, 26)), ('trained using', (35, 37)), ('as', (39, 40)), ('to learn', (41, 43)), ('for', (45, 46))]","[('training phase', (2, 4)), ('weakly supervised bounding box annotations', (10, 15)), ('detection tasks', (16, 18)), ('multi-channel segmentation', (19, 21)), ('masks', (23, 24)), ('multimodal annotations', (26, 28)), ('segmentation model', (32, 34)), ('multimodal annotations', (37, 39)), ('labels', (40, 41)), ('multimodal heatmaps', (43, 45)), ('training images', (47, 49))]","[['weakly supervised bounding box annotations', 'In', 'detection tasks'], ['weakly supervised bounding box annotations', 'in', 'detection tasks'], ['detection tasks', 'to', 'multi-channel segmentation'], ['multi-channel segmentation', 'like', 'masks'], ['masks', 'called', 'multimodal annotations'], ['segmentation model', 'trained using', 'multimodal annotations'], ['multimodal annotations', 'as', 'labels'], ['labels', 'to learn', 'multimodal heatmaps'], ['multimodal heatmaps', 'for', 'training images']]",[],"[['Approach', 'In', 'training phase']]",[],face_detection,12,21
1985,approach,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .","[('In', (0, 1)), ('of', (8, 9)), ('converted into', (14, 16)), ('based on', (22, 24)), ('conducted to generate', (38, 41)), ('for', (42, 43)), ('using', (44, 45)), ('created as', (56, 58)), ('of', (60, 61))]","[('testing phase', (2, 4)), ('resulting heatmaps', (6, 8)), ('given test image', (10, 13)), ('instance - aware segmentation map', (17, 22)), ('pixel - level logic operation', (25, 30)), ('contour tracing operation', (34, 37)), ('contours', (41, 42)), ('objects', (43, 44)), ('segmentation map', (46, 48)), ('bounding boxes of objects', (51, 55)), ('circumscribed quadrilaterals', (58, 60)), ('corresponding contours', (62, 64))]","[['contours', 'In', 'objects'], ['resulting heatmaps', 'of', 'given test image'], ['circumscribed quadrilaterals', 'of', 'corresponding contours'], ['resulting heatmaps', 'converted into', 'instance - aware segmentation map'], ['instance - aware segmentation map', 'based on', 'pixel - level logic operation'], ['contour tracing operation', 'conducted to generate', 'contours'], ['contours', 'for', 'objects'], ['objects', 'using', 'segmentation map'], ['bounding boxes of objects', 'created as', 'circumscribed quadrilaterals'], ['circumscribed quadrilaterals', 'of', 'corresponding contours']]","[['testing phase', 'has', 'resulting heatmaps']]","[['Approach', 'In', 'testing phase']]",[],face_detection,12,22
1986,approach,"WSMA - Seg has the following advantages : ( i ) as an NMS - free solution , WSMA - Seg avoids all hyperparameters related to anchor boxes and NMS ; so , the above - mentioned threshold selection problem is also avoided ; ( ii ) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation - like multimodal annotations ; and ( iii ) multimodal annotations are pixel - level annotations ; so , they can describe the objects more accurately and overcome the above - mentioned environment noise problem .","[('avoids', (21, 22)), ('related to', (24, 26)), ('utilizing', (55, 56)), ('of', (59, 60)), ('are', (72, 73))]","[('WSMA', (0, 1)), ('all hyperparameters', (22, 24)), ('anchor boxes and NMS', (26, 30)), ('topological structure', (57, 59)), ('segmentation', (60, 61))]","[['all hyperparameters', 'related to', 'anchor boxes and NMS'], ['topological structure', 'of', 'segmentation']]",[],[],"[['Approach', 'has', 'WSMA']]",face_detection,12,23
1987,approach,"Therefore , in this work , we further propose a multi-scale pooling segmentation ( MSP - Seg ) model , which is used as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation ( especially for extreme cases , e.g. , very small objects ) , and consequently enhances the detection accuracy of WSMA - Seg .","[('further propose', (7, 9)), ('used as', (22, 24)), ('of', (28, 29)), ('to achieve', (32, 34)), ('enhances', (53, 54)), ('of', (57, 58))]","[('multi-scale pooling segmentation ( MSP - Seg ) model', (10, 19)), ('underlying segmentation model', (25, 28)), ('WSMA - Seg', (29, 32)), ('more accurate segmentation', (35, 38)), ('extreme cases', (41, 43)), ('detection accuracy', (55, 57)), ('WSMA - Seg', (58, 61))]","[['multi-scale pooling segmentation ( MSP - Seg ) model', 'used as', 'underlying segmentation model'], ['underlying segmentation model', 'of', 'WSMA - Seg'], ['detection accuracy', 'of', 'WSMA - Seg'], ['underlying segmentation model', 'to achieve', 'more accurate segmentation'], ['multi-scale pooling segmentation ( MSP - Seg ) model', 'enhances', 'detection accuracy'], ['detection accuracy', 'of', 'WSMA - Seg']]","[['multi-scale pooling segmentation ( MSP - Seg ) model', 'name', 'underlying segmentation model']]","[['Approach', 'further propose', 'multi-scale pooling segmentation ( MSP - Seg ) model']]",[],face_detection,12,25
1988,approach,"We propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach to achieve an accurate and robust object detection without NMS , which is the first anchor-free and NMS - free object detection approach .","[('propose', (1, 2)), ('to achieve', (14, 16)), ('without', (22, 23))]","[('weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', (3, 14)), ('accurate and', (17, 19)), ('NMS', (23, 24))]","[['weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', 'to achieve', 'accurate and']]",[],"[['Approach', 'propose', 'weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach']]",[],face_detection,12,27
1989,approach,We propose multimodal annotations to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .,"[('propose', (1, 2)), ('to achieve', (4, 6)), ('using', (11, 12)), ('develop', (19, 20)), ('to trace', (26, 28))]","[('multimodal annotations', (2, 4)), ('instance - aware segmentation', (7, 11)), ('weakly supervised bounding boxes', (12, 16)), ('run-data - based following algorithm', (21, 26)), ('contours of objects', (28, 31))]","[['multimodal annotations', 'to achieve', 'instance - aware segmentation'], ['instance - aware segmentation', 'using', 'weakly supervised bounding boxes'], ['run-data - based following algorithm', 'to trace', 'contours of objects']]",[],"[['Approach', 'propose', 'multimodal annotations']]",[],face_detection,12,28
1990,approach,We propose a multi-scale pooling segmentation ( MSP - Seg ) model to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg .,"[('propose', (1, 2)), ('to achieve', (12, 14)), ('to enhance', (19, 21)), ('of', (24, 25))]","[('multi-scale pooling segmentation ( MSP - Seg ) model', (3, 12)), ('more accurate segmentation', (15, 18)), ('detection accuracy', (22, 24)), ('WSMA - Seg', (25, 28))]","[['multi-scale pooling segmentation ( MSP - Seg ) model', 'to achieve', 'more accurate segmentation'], ['multi-scale pooling segmentation ( MSP - Seg ) model', 'to enhance', 'detection accuracy'], ['detection accuracy', 'of', 'WSMA - Seg']]",[],"[['Approach', 'propose', 'multi-scale pooling segmentation ( MSP - Seg ) model']]",[],face_detection,12,29
1991,research-problem,Object Detection Using Segmentation Results and Contour Tracing,[],"[('Object Detection', (0, 2))]",[],[],[],[],face_detection,12,61
1992,experiments,"As shown in , our proposed method with Stack = 2 , Base = 40 , Depth = 5 has achieved the best performance among all solutions in terms of F1 Score .","[('with', (7, 8)), ('achieved', (20, 21)), ('among', (24, 25)), ('in terms of', (27, 30))]","[('Stack = 2', (8, 11)), ('best performance', (22, 24)), ('all solutions', (25, 27)), ('F1 Score', (30, 32))]","[['best performance', 'among', 'all solutions'], ['all solutions', 'in terms of', 'F1 Score']]",[],[],[],face_detection,12,98
1993,research-problem,RetinaFace : Single - stage Dense Face Localisation in the Wild,[],[],[],[],[],[],face_detection,13,2
1994,research-problem,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .",[],"[('uncontrolled face detection', (7, 10)), ('accurate and efficient face localisation', (11, 16))]",[],[],[],[],face_detection,13,4
1995,research-problem,"This paper presents a robust single - stage face detector , named RetinaFace , which performs pixel - wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self - supervised multi-task learning .","[('presents', (2, 3)), ('named', (11, 12)), ('performs', (15, 16)), ('on', (21, 22)), ('by taking advantages of', (26, 30))]","[('RetinaFace', (12, 13)), ('pixel - wise face localisation', (16, 21)), ('various scales of faces', (22, 26)), ('joint extra-supervised and self - supervised multi-task learning', (30, 38))]","[['RetinaFace', 'performs', 'pixel - wise face localisation'], ['pixel - wise face localisation', 'on', 'various scales of faces'], ['various scales of faces', 'by taking advantages of', 'joint extra-supervised and self - supervised multi-task learning']]",[],"[['Research problem', 'presents', 'RetinaFace']]",[],face_detection,13,5
1996,approach,( 2 ) We further add a selfsupervised mesh decoder branch for predicting a pixel - wise 3D shape face information in parallel with the existing supervised branches .,"[('add', (5, 6)), ('for predicting', (11, 13)), ('in parallel with', (21, 24))]","[('selfsupervised mesh decoder branch', (7, 11)), ('pixel - wise 3D shape face information', (14, 21)), ('existing supervised branches', (25, 28))]","[['selfsupervised mesh decoder branch', 'for predicting', 'pixel - wise 3D shape face information'], ['pixel - wise 3D shape face information', 'in parallel with', 'existing supervised branches']]",[],"[['Approach', 'add', 'selfsupervised mesh decoder branch']]",[],face_detection,13,9
1997,research-problem,Automatic face localisation is the prerequisite step of facial image analysis for many applications such as facial attribute ( e.g. expression and age ) and facial identity recognition .,[],"[('Automatic face localisation', (0, 3)), ('facial image analysis', (8, 11))]",[],[],[],[],face_detection,13,15
1998,approach,The proposed single - stage pixel - wise face localisation method employs extra-supervised and self - supervised multi-task learning in parallel with the existing box classification and regression branches .,"[('employs', (11, 12)), ('in parallel with', (19, 22))]","[('single - stage pixel - wise face localisation method', (2, 11)), ('extra-supervised and self - supervised multi-task learning', (12, 19)), ('existing box classification and regression branches', (23, 29))]","[['single - stage pixel - wise face localisation method', 'employs', 'extra-supervised and self - supervised multi-task learning'], ['extra-supervised and self - supervised multi-task learning', 'in parallel with', 'existing box classification and regression branches']]",[],[],"[['Approach', 'has', 'single - stage pixel - wise face localisation method']]",face_detection,13,18
1999,approach,"Each positive anchor outputs ( 1 ) a face score , ( 2 ) a face box , ( 3 ) five facial landmarks , and ( 4 ) dense 3 D face vertices projected on the image plane .","[('outputs', (3, 4)), ('projected on', (34, 36))]","[('Each positive anchor', (0, 3)), ('face score', (8, 10)), ('face box', (15, 17)), ('five facial landmarks', (21, 24)), ('dense 3 D face vertices', (29, 34)), ('image plane', (37, 39))]","[['dense 3 D face vertices', 'projected on', 'image plane']]",[],[],"[['Approach', 'has', 'Each positive anchor']]",face_detection,13,19
2000,approach,"Following this route , we improve the single - stage face detection framework and propose a state - of - the - art dense face localisation method by exploiting multi-task losses coming from strongly supervised and self - supervised signals .","[('improve', (5, 6)), ('propose', (14, 15)), ('by exploiting', (27, 29)), ('coming from', (31, 33))]","[('single - stage face detection framework', (7, 13)), ('state - of - the - art dense face localisation method', (16, 27)), ('multi-task losses', (29, 31)), ('strongly supervised and self - supervised signals', (33, 40))]","[['state - of - the - art dense face localisation method', 'by exploiting', 'multi-task losses'], ['multi-task losses', 'coming from', 'strongly supervised and self - supervised signals']]",[],"[['Approach', 'improve', 'single - stage face detection framework']]",[],face_detection,13,25
2001,approach,"Inspired by , MTCNN and STN simultaneously detected faces and five facial landmarks .","[('Inspired', (0, 1)), ('simultaneously detected', (6, 8))]","[('MTCNN and STN', (3, 6)), ('faces', (8, 9)), ('five facial landmarks', (10, 13))]","[['MTCNN and STN', 'simultaneously detected', 'faces'], ['MTCNN and STN', 'simultaneously detected', 'five facial landmarks']]",[],"[['Approach', 'Inspired', 'MTCNN and STN']]",[],face_detection,13,29
2002,approach,"In this paper , we employ a mesh decoder branch through self - supervision learning for predicting a pixel - wise 3 D face shape in parallel with the existing supervised branches .","[('employ', (5, 6)), ('through', (10, 11)), ('for predicting', (15, 17)), ('in parallel with', (25, 28))]","[('mesh decoder branch', (7, 10)), ('self - supervision learning', (11, 15)), ('pixel - wise 3 D face shape', (18, 25)), ('existing supervised branches', (29, 32))]","[['mesh decoder branch', 'through', 'self - supervision learning'], ['self - supervision learning', 'for predicting', 'pixel - wise 3 D face shape'], ['pixel - wise 3 D face shape', 'in parallel with', 'existing supervised branches']]",[],"[['Approach', 'employ', 'mesh decoder branch']]",[],face_detection,13,41
2003,approach,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .","[('Based on', (0, 2)), ('propose', (9, 10)), ('named', (18, 19)), ('employs', (23, 24)), ('to simultaneously predict', (28, 31)), ('of', (46, 47))]","[('single - stage design', (3, 7)), ('novel pixel - wise face localisation method', (11, 18)), ('Reti- naFace', (19, 21)), ('multi-task learning strategy', (25, 28)), ('face score', (31, 33)), ('face box', (34, 36)), ('five facial landmarks', (37, 40)), ('3D position and', (42, 45)), ('correspondence', (45, 46)), ('each facial pixel', (47, 50))]","[['single - stage design', 'propose', 'novel pixel - wise face localisation method'], ['novel pixel - wise face localisation method', 'named', 'Reti- naFace'], ['novel pixel - wise face localisation method', 'employs', 'multi-task learning strategy'], ['multi-task learning strategy', 'to simultaneously predict', 'face score'], ['multi-task learning strategy', 'to simultaneously predict', 'face box'], ['multi-task learning strategy', 'to simultaneously predict', 'five facial landmarks'], ['multi-task learning strategy', 'to simultaneously predict', '3D position and'], ['multi-task learning strategy', 'to simultaneously predict', 'correspondence'], ['correspondence', 'of', 'each facial pixel']]","[['single - stage design', 'name', 'novel pixel - wise face localisation method'], ['novel pixel - wise face localisation method', 'name', 'Reti- naFace'], ['3D position and', 'has', 'correspondence']]","[['Approach', 'Based on', 'single - stage design']]",[],face_detection,13,43
2004,experimental-setup,"For negative anchors , only classification loss is applied .","[('For', (0, 1)), ('applied', (8, 9))]","[('negative anchors', (1, 3)), ('classification loss', (5, 7))]","[['negative anchors', 'applied', 'classification loss']]","[['negative anchors', 'has', 'classification loss']]","[['Experimental setup', 'For', 'negative anchors']]",[],face_detection,13,132
2005,experimental-setup,"We employ a shared loss head ( 1 1 conv ) across different feature maps H n W n 256 , n ? { 2 , . . . , 6 }.","[('employ', (1, 2)), ('across', (11, 12))]","[('shared loss head ( 1 1 conv )', (3, 11)), ('different feature maps', (12, 15))]","[['shared loss head ( 1 1 conv )', 'across', 'different feature maps']]",[],"[['Experimental setup', 'employ', 'shared loss head ( 1 1 conv )']]",[],face_detection,13,134
2006,experimental-setup,"We set the scale step at 2 1 / 3 and the aspect ratio at 1 : During training , anchors are matched to a ground - truth box when IoU is larger than 0.5 , and to the background when IoU is less than 0.3 .","[('set', (1, 2)), ('at', (5, 6)), ('matched to', (22, 24)), ('when', (29, 30)), ('larger than', (32, 34))]","[('scale step', (3, 5)), ('2 1 / 3', (6, 10)), ('aspect ratio', (12, 14)), ('ground - truth box', (25, 29)), ('IoU', (30, 31)), ('0.5', (34, 35))]","[['scale step', 'at', '2 1 / 3'], ['ground - truth box', 'when', 'IoU'], ['IoU', 'larger than', '0.5']]","[['scale step', 'has', '2 1 / 3']]","[['Experimental setup', 'set', 'scale step']]",[],face_detection,13,140
2007,experimental-setup,"Since most of the anchors ( > 99 % ) are negative after the matching step , we employ standard OHEM to alleviate significant imbalance between the positive and negative training examples .","[('are', (10, 11)), ('after', (12, 13)), ('employ', (18, 19)), ('to alleviate', (21, 23)), ('between', (25, 26))]","[('most of the anchors ( > 99 % )', (1, 10)), ('negative', (11, 12)), ('matching step', (14, 16)), ('standard OHEM', (19, 21)), ('significant imbalance', (23, 25)), ('positive and negative training examples', (27, 32))]","[['most of the anchors ( > 99 % )', 'are', 'negative'], ['negative', 'after', 'matching step'], ['most of the anchors ( > 99 % )', 'employ', 'standard OHEM'], ['standard OHEM', 'to alleviate', 'significant imbalance'], ['significant imbalance', 'between', 'positive and negative training examples']]",[],[],[],face_detection,13,142
2008,experimental-setup,"More specifically , we sort negative anchors by the loss values and select the top ones so that the ratio between the negative and positive samples is at least 3:1 .","[('sort', (4, 5)), ('by', (7, 8)), ('select', (12, 13)), ('so that', (16, 18)), ('between', (20, 21)), ('is', (26, 27))]","[('negative anchors', (5, 7)), ('loss values', (9, 11)), ('top ones', (14, 16)), ('ratio', (19, 20)), ('negative and positive samples', (22, 26)), ('at least 3:1', (27, 30))]","[['negative anchors', 'by', 'loss values'], ['negative anchors', 'select', 'top ones'], ['top ones', 'so that', 'ratio'], ['ratio', 'between', 'negative and positive samples'], ['ratio', 'is', 'at least 3:1'], ['negative and positive samples', 'is', 'at least 3:1']]",[],"[['Experimental setup', 'sort', 'negative anchors']]",[],face_detection,13,143
2009,experimental-setup,"We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .","[('train', (1, 2)), ('using', (4, 5)), ('on', (23, 24))]","[('RetinaFace', (3, 4)), ('SGD optimiser', (5, 7)), ('momentum', (8, 9)), ('0.9', (10, 11)), ('four NVIDIA Tesla P40 ( 24GB ) GPUs', (24, 32))]","[['RetinaFace', 'using', 'SGD optimiser']]","[['SGD optimiser', 'has', 'momentum']]","[['Experimental setup', 'train', 'RetinaFace']]",[],face_detection,13,150
2010,experimental-setup,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .","[('starts from', (3, 5)), ('rising to', (9, 11)), ('after', (14, 15)), ('divided by', (19, 21)), ('at', (22, 23))]","[('learning rate', (1, 3)), ('10 ? 3', (5, 8)), ('10 ? 2', (11, 14)), ('5 epochs', (15, 17)), ('10', (21, 22))]","[['learning rate', 'starts from', '10 ? 3'], ['learning rate', 'rising to', '10 ? 2'], ['10 ? 3', 'rising to', '10 ? 2'], ['10 ? 2', 'after', '5 epochs'], ['learning rate', 'divided by', '10'], ['5 epochs', 'divided by', '10']]",[],[],"[['Experimental setup', 'has', 'learning rate']]",face_detection,13,151
2011,experimental-setup,The training process terminates at 80 epochs .,"[('terminates at', (3, 5))]","[('training process', (1, 3)), ('80 epochs', (5, 7))]","[['training process', 'terminates at', '80 epochs']]",[],[],"[['Experimental setup', 'has', 'training process']]",face_detection,13,152
2012,experimental-setup,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,"[('applied on', (6, 8)), ('using', (15, 16)), ('at', (19, 20))]","[('Box voting', (0, 2)), ('union set of predicted face boxes', (9, 15)), ('IoU threshold', (17, 19)), ('0.4', (20, 21))]","[['Box voting', 'applied on', 'union set of predicted face boxes'], ['union set of predicted face boxes', 'using', 'IoU threshold'], ['IoU threshold', 'at', '0.4']]","[['IoU threshold', 'has', '0.4']]",[],"[['Experimental setup', 'has', 'Box voting']]",face_detection,13,155
2013,ablation-analysis,"By applying the practices of state - of - the - art techniques ( i.e. FPN , context module , and deformable convolution ) , we setup a strong baseline ( 91.286 % ) , which is slightly better than ISRN ( 90.9 % ) .","[('applying', (1, 2)), ('i.e.', (14, 15)), ('setup', (26, 27)), ('slightly better than', (37, 40))]","[('practices', (3, 4)), ('state - of - the - art techniques', (5, 13)), ('context module', (17, 19)), ('deformable convolution', (21, 23)), ('strong baseline ( 91.286 % )', (28, 34)), ('ISRN ( 90.9 % )', (40, 45))]","[['state - of - the - art techniques', 'i.e.', 'context module'], ['state - of - the - art techniques', 'i.e.', 'deformable convolution'], ['state - of - the - art techniques', 'setup', 'strong baseline ( 91.286 % )'], ['strong baseline ( 91.286 % )', 'slightly better than', 'ISRN ( 90.9 % )']]","[['practices', 'has', 'state - of - the - art techniques'], ['state - of - the - art techniques', 'name', 'context module']]","[['Ablation analysis', 'applying', 'practices']]",[],face_detection,13,160
2014,ablation-analysis,"Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .","[('Adding', (0, 1)), ('on', (24, 25))]","[('branch of five facial landmark regression', (2, 8)), ('significantly improves', (8, 10)), ('face box AP ( 0.408 % )', (11, 18)), ('mAP ( 0.775 % )', (19, 24)), ('Hard subset', (26, 28))]","[['mAP ( 0.775 % )', 'on', 'Hard subset']]","[['branch of five facial landmark regression', 'has', 'significantly improves'], ['significantly improves', 'has', 'face box AP ( 0.408 % )']]","[['Ablation analysis', 'Adding', 'branch of five facial landmark regression']]",[],face_detection,13,161
2015,ablation-analysis,"By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .","[('adding', (3, 4)), ('increases', (8, 9)), ('on', (13, 14)), ('on', (23, 24))]","[('dense regression branch', (5, 8)), ('face box AP', (10, 13)), ('Easy and Medium subsets', (14, 18)), ('slightly deteriorates', (19, 21)), ('results', (22, 23)), ('Hard subset', (25, 27))]","[['dense regression branch', 'increases', 'face box AP'], ['face box AP', 'on', 'Easy and Medium subsets'], ['results', 'on', 'Hard subset'], ['results', 'on', 'Hard subset']]","[['slightly deteriorates', 'has', 'results']]","[['Ablation analysis', 'adding', 'dense regression branch']]",[],face_detection,13,162
2016,ablation-analysis,"Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .","[('jointly', (7, 8)), ('enables', (8, 9)), ('compared to', (12, 14))]","[('learning', (2, 3)), ('landmark and dense regression', (3, 7)), ('further improvement', (10, 12)), ('adding landmark regression only', (14, 18))]","[['landmark and dense regression', 'enables', 'further improvement'], ['further improvement', 'compared to', 'adding landmark regression only']]","[['learning', 'has', 'landmark and dense regression']]",[],"[['Ablation analysis', 'has', 'learning']]",face_detection,13,163
2017,ablation-analysis,"This demonstrates that landmark regression does help dense regression , which in turn boosts face detection performance even further .","[('demonstrates', (1, 2)), ('does help', (5, 7)), ('boosts', (13, 14))]","[('landmark regression', (3, 5)), ('dense regression', (7, 9)), ('face detection performance', (14, 17))]","[['landmark regression', 'does help', 'dense regression'], ['landmark regression', 'boosts', 'face detection performance'], ['dense regression', 'boosts', 'face detection performance']]",[],"[['Ablation analysis', 'demonstrates', 'landmark regression']]",[],face_detection,13,164
2018,results,Our approach outper - forms these state - of - the - art methods in terms of AP .,"[('in terms of', (14, 17))]","[('outper', (2, 3)), ('state - of - the - art methods', (6, 14)), ('AP', (17, 18))]","[['state - of - the - art methods', 'in terms of', 'AP']]","[['outper', 'has', 'state - of - the - art methods']]",[],"[['Results', 'has', 'outper']]",face_detection,13,171
2019,results,"More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .","[('produces', (4, 5)), ('in', (8, 9)), ('of', (11, 12)), ('i.e.', (18, 19)), ('for', (37, 38)), ('for', (59, 60))]","[('RetinaFace', (3, 4)), ('best AP', (6, 8)), ('all subsets', (9, 11)), ('validation and test sets', (13, 17)), ('96.9 % ( Easy )', (20, 25)), ('96.1 % ( Medium )', (26, 31)), ('91.8 % ( Hard )', (32, 37)), ('validation set', (38, 40)), ('96.3 % ( Easy )', (42, 47)), ('95.6 % ( Medium )', (48, 53)), ('91.4 % ( Hard )', (54, 59)), ('test set', (60, 62))]","[['RetinaFace', 'produces', 'best AP'], ['best AP', 'in', 'all subsets'], ['all subsets', 'of', 'validation and test sets'], ['all subsets', 'of', '91.8 % ( Hard )'], ['validation and test sets', 'i.e.', '96.9 % ( Easy )'], ['validation and test sets', 'i.e.', '96.1 % ( Medium )'], ['validation and test sets', 'i.e.', '91.8 % ( Hard )'], ['validation and test sets', 'i.e.', '95.6 % ( Medium )'], ['validation and test sets', 'i.e.', '91.4 % ( Hard )'], ['91.8 % ( Hard )', 'for', 'validation set'], ['91.4 % ( Hard )', 'for', 'test set']]","[['RetinaFace', 'has', 'best AP']]",[],"[['Results', 'has', 'RetinaFace']]",face_detection,13,172
2020,results,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .","[('Compared to', (0, 2)), ('sets up', (12, 14)), ('on', (25, 26)), ('which contains', (29, 31))]","[('recent best performed method', (3, 7)), ('Reti - na Face', (8, 12)), ('new impressive record ( 91.4 % v.s. 90.3 % )', (15, 25)), ('Hard subset', (27, 29)), ('large number of tiny faces', (32, 37))]","[['Reti - na Face', 'sets up', 'new impressive record ( 91.4 % v.s. 90.3 % )'], ['new impressive record ( 91.4 % v.s. 90.3 % )', 'on', 'Hard subset'], ['Hard subset', 'which contains', 'large number of tiny faces']]","[['recent best performed method', 'has', 'Reti - na Face']]","[['Results', 'Compared to', 'recent best performed method']]",[],face_detection,13,173
2021,results,"Besides accurate bounding boxes , the five facial landmarks predicted by Retina Face are also very robust under the variations of pose , occlusion and resolution .","[('Besides', (0, 1)), ('predicted by', (9, 11)), ('are', (13, 14)), ('under', (17, 18))]","[('accurate bounding boxes', (1, 4)), ('five facial landmarks', (6, 9)), ('Retina Face', (11, 13)), ('very robust', (15, 17)), ('variations of pose , occlusion and resolution', (19, 26))]","[['five facial landmarks', 'predicted by', 'Retina Face'], ['five facial landmarks', 'are', 'very robust'], ['very robust', 'under', 'variations of pose , occlusion and resolution']]","[['accurate bounding boxes', 'has', 'five facial landmarks']]","[['Results', 'Besides', 'accurate bounding boxes']]",[],face_detection,13,176
2022,results,RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .,"[('from', (10, 11)), ('to', (13, 14)), ('compared to', (17, 19))]","[('RetinaFace', (0, 1)), ('significantly decreases', (1, 3)), ('normalised mean errors ( NME )', (4, 10)), ('2.72 %', (11, 13)), ('2.21 %', (14, 16)), ('MTCNN', (19, 20))]","[['normalised mean errors ( NME )', 'from', '2.72 %'], ['2.72 %', 'to', '2.21 %'], ['normalised mean errors ( NME )', 'compared to', 'MTCNN'], ['2.21 %', 'compared to', 'MTCNN']]","[['RetinaFace', 'has', 'significantly decreases'], ['significantly decreases', 'has', 'normalised mean errors ( NME )']]",[],[],face_detection,13,183
2023,results,"Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .","[('Compared to', (0, 2)), ('from', (10, 11)), ('to', (13, 14))]","[('MTCNN', (2, 3)), ('RetinaFace', (4, 5)), ('significantly', (5, 6)), ('failure rate', (8, 10)), ('26.31 %', (11, 13)), ('9.37 %', (14, 16))]","[['failure rate', 'from', '26.31 %'], ['failure rate', 'to', '9.37 %'], ['26.31 %', 'to', '9.37 %']]","[['MTCNN', 'has', 'RetinaFace'], ['RetinaFace', 'has', 'significantly']]","[['Results', 'Compared to', 'MTCNN']]",[],face_detection,13,185
2024,results,"In this paper , we demonstrate how our face detection method can boost the performance of a state - of - the - art publicly available face recognition method , i.e. ArcFace .","[('demonstrate', (5, 6)), ('of', (15, 16)), ('i.e.', (30, 31))]","[('our face detection method', (7, 11)), ('boost', (12, 13)), ('performance', (14, 15)), ('state - of - the - art publicly available face recognition method', (17, 29)), ('ArcFace', (31, 32))]","[['performance', 'of', 'state - of - the - art publicly available face recognition method'], ['state - of - the - art publicly available face recognition method', 'i.e.', 'ArcFace']]","[['our face detection method', 'has', 'boost'], ['boost', 'has', 'performance'], ['state - of - the - art publicly available face recognition method', 'name', 'ArcFace']]","[['Results', 'demonstrate', 'our face detection method']]",[],face_detection,13,199
2025,results,"The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .","[('on', (2, 3)), ('demonstrate', (7, 8)), ('boost', (14, 15)), ('from', (19, 20)), ('to', (22, 23))]","[('CFP - FP', (3, 6)), ('Reti - na Face', (9, 13)), (""ArcFace 's verification accuracy"", (15, 19)), ('98.37 %', (20, 22)), ('99.49 %', (23, 25))]","[['CFP - FP', 'demonstrate', 'Reti - na Face'], ['Reti - na Face', 'boost', ""ArcFace 's verification accuracy""], [""ArcFace 's verification accuracy"", 'from', '98.37 %'], ['98.37 %', 'to', '99.49 %']]","[['CFP - FP', 'has', 'Reti - na Face']]","[['Results', 'on', 'CFP - FP']]",[],face_detection,13,204
2026,approach,We employ two tricks ( i.e. flip test and face detection score to weigh samples within templates ) to progressively improve the face verification accuracy .,"[('employ', (1, 2)), ('to', (12, 13)), ('within', (15, 16)), ('to progressively improve', (18, 21))]","[('two tricks', (2, 4)), ('flip test', (6, 8)), ('face detection score', (9, 12)), ('weigh samples', (13, 15)), ('templates', (16, 17)), ('face verification accuracy', (22, 25))]","[['two tricks', 'to', 'weigh samples'], ['face detection score', 'to', 'weigh samples'], ['weigh samples', 'within', 'templates'], ['two tricks', 'to progressively improve', 'face verification accuracy']]","[['two tricks', 'name', 'flip test']]","[['Approach', 'employ', 'two tricks']]",[],face_detection,13,209
2027,results,"Under fair comparison , TAR ( at FAR = 1 e ? 6 ) significantly improves from 88 . 29 % to 89.59 % simply by replacing MTCNN with RetinaFace .","[('Under', (0, 1)), ('at', (6, 7)), ('from', (16, 17)), ('to', (21, 22)), ('by replacing', (25, 27)), ('with', (28, 29))]","[('fair comparison', (1, 3)), ('TAR (', (4, 6)), ('FAR = 1 e ? 6 )', (7, 14)), ('significantly improves', (14, 16)), ('88 . 29 %', (17, 21)), ('89.59 %', (22, 24)), ('MTCNN', (27, 28)), ('RetinaFace', (29, 30))]","[['TAR (', 'at', 'FAR = 1 e ? 6 )'], ['significantly improves', 'from', '88 . 29 %'], ['significantly improves', 'to', '89.59 %'], ['88 . 29 %', 'to', '89.59 %'], ['significantly improves', 'by replacing', 'MTCNN'], ['89.59 %', 'by replacing', 'MTCNN'], ['MTCNN', 'with', 'RetinaFace']]","[['fair comparison', 'has', 'TAR ('], ['TAR (', 'has', 'FAR = 1 e ? 6 )'], ['FAR = 1 e ? 6 )', 'has', 'significantly improves']]","[['Results', 'Under', 'fair comparison']]",[],face_detection,13,210
2028,results,Inference Efficiency,[],[],[],[],[],[],face_detection,13,215
2029,research-problem,WIDER FACE : A Face Detection Benchmark,[],"[('Face Detection', (4, 6))]",[],[],[],[],face_detection,14,2
2030,dataset,We introduce a large - scale face detection dataset called WIDER FACE .,"[('introduce', (1, 2)), ('called', (9, 10))]","[('large - scale face detection dataset', (3, 9)), ('WIDER FACE', (10, 12))]","[['large - scale face detection dataset', 'called', 'WIDER FACE']]",[],"[['Dataset', 'introduce', 'large - scale face detection dataset']]",[],face_detection,14,32
2031,model,"We show an example of using WIDER FACE through proposing a multi-scale two - stage cascade framework , which uses divide and conquer strategy to deal with large scale variations .","[('through proposing', (8, 10)), ('uses', (19, 20)), ('to deal with', (24, 27))]","[('WIDER', (6, 7)), ('multi-scale two - stage cascade framework', (11, 17)), ('divide and conquer strategy', (20, 24)), ('large scale variations', (27, 30))]","[['WIDER', 'through proposing', 'multi-scale two - stage cascade framework'], ['multi-scale two - stage cascade framework', 'uses', 'divide and conquer strategy'], ['divide and conquer strategy', 'to deal with', 'large scale variations']]","[['WIDER', 'has', 'multi-scale two - stage cascade framework']]",[],[],face_detection,14,36
2032,results,"Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .","[('on', (4, 5)), ('with', (8, 9)), ('as', (12, 13))]","[('Faceness', (0, 1)), ('outperforms', (1, 2)), ('other methods', (2, 4)), ('three subsets', (5, 7)), ('DPM and ACF', (9, 12)), ('marginal second and third', (13, 17))]","[['outperforms', 'on', 'three subsets'], ['other methods', 'on', 'three subsets'], ['outperforms', 'with', 'DPM and ACF'], ['other methods', 'with', 'DPM and ACF'], ['three subsets', 'with', 'DPM and ACF'], ['outperforms', 'as', 'marginal second and third'], ['DPM and ACF', 'as', 'marginal second and third']]","[['Faceness', 'has', 'outperforms'], ['outperforms', 'has', 'other methods']]",[],[],face_detection,14,158
2033,results,The results of small scale are abysmal : none of the algorithms is able to achieve more than 12 % AP .,"[('of', (2, 3)), ('are', (5, 6))]","[('results', (1, 2)), ('small scale', (3, 5)), ('abysmal', (6, 7))]","[['results', 'of', 'small scale'], ['small scale', 'are', 'abysmal']]","[['results', 'has', 'small scale'], ['small scale', 'has', 'abysmal']]",[],"[['Results', 'has', 'results']]",face_detection,14,166
2034,results,"In , we show the impact of occlusion on detecting faces with a height of at least 30 pixels .","[('show', (3, 4)), ('of', (6, 7)), ('on', (8, 9)), ('with', (11, 12)), ('of', (14, 15))]","[('impact', (5, 6)), ('occlusion', (7, 8)), ('detecting faces', (9, 11)), ('height', (13, 14)), ('at least 30 pixels', (15, 19))]","[['impact', 'of', 'occlusion'], ['height', 'of', 'at least 30 pixels'], ['occlusion', 'on', 'detecting faces'], ['detecting faces', 'with', 'height'], ['height', 'of', 'at least 30 pixels']]","[['impact', 'has', 'occlusion']]","[['Results', 'show', 'impact']]",[],face_detection,14,170
2035,results,The maximum AP is only 26.5 % achieved by Faceness .,"[('is', (3, 4)), ('achieved by', (7, 9))]","[('maximum AP', (1, 3)), ('only 26.5 %', (4, 7)), ('Faceness', (9, 10))]","[['maximum AP', 'is', 'only 26.5 %'], ['only 26.5 %', 'achieved by', 'Faceness']]","[['maximum AP', 'has', 'only 26.5 %']]",[],"[['Results', 'has', 'maximum AP']]",face_detection,14,173
2036,results,The best performance of baseline methods drops to 14.4 % .,"[('of', (3, 4))]","[('best performance', (1, 3)), ('baseline methods', (4, 6)), ('drops', (6, 7)), ('14.4 %', (8, 10))]","[['best performance', 'of', 'baseline methods']]","[['best performance', 'has', 'baseline methods']]",[],[],face_detection,14,175
2037,results,"It is worth noting that Faceness and DPM , which are part based models , already perform relatively better than other methods on occlusion handling .","[('worth noting', (2, 4)), ('perform', (16, 17)), ('than', (19, 20)), ('on', (22, 23))]","[('Faceness and DPM', (5, 8)), ('part based models', (11, 14)), ('relatively better', (17, 19)), ('other methods', (20, 22)), ('occlusion handling', (23, 25))]","[['Faceness and DPM', 'perform', 'relatively better'], ['part based models', 'perform', 'relatively better'], ['relatively better', 'than', 'other methods'], ['other methods', 'on', 'occlusion handling']]",[],"[['Results', 'worth noting', 'Faceness and DPM']]",[],face_detection,14,176
2038,results,"The best performance is achieved by Faceness , with a recall below 20 % .","[('achieved by', (4, 6)), ('with', (8, 9))]","[('best performance', (1, 3)), ('Faceness', (6, 7)), ('recall below 20 %', (10, 14))]","[['best performance', 'achieved by', 'Faceness'], ['Faceness', 'with', 'recall below 20 %']]",[],[],"[['Results', 'has', 'best performance']]",face_detection,14,183
2039,results,"Among the four baseline methods , Faceness tends to outperform the other methods .","[('Among', (0, 1)), ('tends to', (7, 9))]","[('four baseline methods', (2, 5)), ('Faceness', (6, 7)), ('outperform', (9, 10)), ('other methods', (11, 13))]","[['Faceness', 'tends to', 'outperform']]","[['four baseline methods', 'has', 'Faceness'], ['outperform', 'has', 'other methods']]","[['Results', 'Among', 'four baseline methods']]",[],face_detection,14,186
2040,results,WIDER FACE as an Effective Training Source,[],"[('WIDER FACE', (0, 2))]",[],[],[],[],face_detection,14,191
2041,results,"As shown in , the retrained models perform consistently better than the baseline models .","[('perform', (7, 8)), ('than', (10, 11))]","[('retrained models', (5, 7)), ('consistently better', (8, 10)), ('baseline models', (12, 14))]","[['retrained models', 'perform', 'consistently better'], ['consistently better', 'than', 'baseline models']]","[['retrained models', 'has', 'consistently better']]",[],"[['Results', 'has', 'retrained models']]",face_detection,14,203
2042,results,The average AP improvement of retrained ACF detector is 5.4 % in comparison to baseline ACF detector .,"[('of', (4, 5)), ('is', (8, 9)), ('in comparison to', (11, 14))]","[('average AP improvement', (1, 4)), ('retrained ACF detector', (5, 8)), ('5.4 %', (9, 11)), ('baseline ACF detector', (14, 17))]","[['average AP improvement', 'of', 'retrained ACF detector'], ['average AP improvement', 'is', '5.4 %'], ['retrained ACF detector', 'is', '5.4 %'], ['5.4 %', 'in comparison to', 'baseline ACF detector']]",[],[],"[['Results', 'has', 'average AP improvement']]",face_detection,14,204
2043,results,"For the Faceness , the retrained Faceness model obtain 4.2 % improvement on WIDER hard test set .","[('For', (0, 1)), ('obtain', (8, 9)), ('on', (12, 13))]","[('Faceness', (2, 3)), ('retrained Faceness model', (5, 8)), ('4.2 % improvement', (9, 12)), ('WIDER hard test set', (13, 17))]","[['retrained Faceness model', 'obtain', '4.2 % improvement'], ['4.2 % improvement', 'on', 'WIDER hard test set']]","[['Faceness', 'has', 'retrained Faceness model'], ['retrained Faceness model', 'has', '4.2 % improvement']]","[['Results', 'For', 'Faceness']]",[],face_detection,14,205
2044,results,"The retrained ACF detector achieves a recall rate of 87.48 % , outperforms the baseline ACF by a considerable margin of 1.4 % .","[('achieves', (4, 5)), ('of', (8, 9)), ('by', (16, 17)), ('of', (20, 21))]","[('retrained ACF detector', (1, 4)), ('recall rate', (6, 8)), ('87.48 %', (9, 11)), ('outperforms', (12, 13)), ('baseline ACF', (14, 16)), ('considerable margin', (18, 20)), ('1.4 %', (21, 23))]","[['retrained ACF detector', 'achieves', 'recall rate'], ['recall rate', 'of', '87.48 %'], ['considerable margin', 'of', '1.4 %'], ['outperforms', 'by', 'considerable margin'], ['baseline ACF', 'by', 'considerable margin'], ['considerable margin', 'of', '1.4 %']]","[['retrained ACF detector', 'has', 'recall rate'], ['outperforms', 'has', 'baseline ACF']]",[],"[['Results', 'has', 'retrained ACF detector']]",face_detection,14,209
2045,results,The recall rate improvement of the retrained Faceness detector is 0.8 % in comparison to the baseline Faceness detector .,"[('of', (4, 5)), ('is', (9, 10)), ('in comparison to', (12, 15))]","[('recall rate improvement', (1, 4)), ('retrained Faceness detector', (6, 9)), ('0.8 %', (10, 12)), ('baseline Faceness detector', (16, 19))]","[['recall rate improvement', 'of', 'retrained Faceness detector'], ['recall rate improvement', 'is', '0.8 %'], ['retrained Faceness detector', 'is', '0.8 %'], ['0.8 %', 'in comparison to', 'baseline Faceness detector']]",[],[],"[['Results', 'has', 'recall rate improvement']]",face_detection,14,218
2046,results,"As shown in , the multi-scale cascade CNN obtains 8.5 % AP improvement on the WIDER Hard subset compared to the retrained Faceness , suggesting its superior capability in handling faces with different scales .","[('obtains', (8, 9)), ('on', (13, 14)), ('compared to', (18, 20))]","[('multi-scale cascade CNN', (5, 8)), ('8.5 % AP improvement', (9, 13)), ('WIDER Hard subset', (15, 18)), ('retrained Faceness', (21, 23))]","[['multi-scale cascade CNN', 'obtains', '8.5 % AP improvement'], ['8.5 % AP improvement', 'on', 'WIDER Hard subset'], ['WIDER Hard subset', 'compared to', 'retrained Faceness']]",[],[],[],face_detection,14,236
2047,results,"For the WIDER Medium subset , the multi-scale cascade CNN outperforms other baseline methods with a considerable margin .","[('For', (0, 1)), ('outperforms', (10, 11)), ('with', (14, 15))]","[('WIDER Medium subset', (2, 5)), ('multi-scale cascade CNN', (7, 10)), ('other baseline methods', (11, 14)), ('considerable margin', (16, 18))]","[['multi-scale cascade CNN', 'outperforms', 'other baseline methods'], ['other baseline methods', 'with', 'considerable margin']]","[['WIDER Medium subset', 'has', 'multi-scale cascade CNN']]","[['Results', 'For', 'WIDER Medium subset']]",[],face_detection,14,239
2048,research-problem,FaceBoxes : A CPU Real - time Face Detector with High Accuracy,[],"[('CPU Real - time Face Detector', (3, 9))]",[],[],[],[],face_detection,15,2
2049,research-problem,"To address this challenge , we propose a novel face detector , named FaceBoxes , with superior performance on both speed and accuracy .","[('propose', (6, 7)), ('named', (12, 13)), ('with', (15, 16)), ('on', (18, 19))]","[('novel face detector', (8, 11)), ('FaceBoxes', (13, 14)), ('superior performance', (16, 18)), ('both speed and accuracy', (19, 23))]","[['novel face detector', 'named', 'FaceBoxes'], ['novel face detector', 'with', 'superior performance'], ['superior performance', 'on', 'both speed and accuracy']]","[['novel face detector', 'name', 'FaceBoxes']]","[['Research problem', 'propose', 'novel face detector']]",[],face_detection,15,5
2050,code,Code is available at https://github.com/sfzhang15/FaceBoxes .,[],"[('https://github.com/sfzhang15/FaceBoxes', (4, 5))]",[],[],[],[],face_detection,15,13
2051,research-problem,Face detection is one of the fundamental problems in computer vision and pattern recognition .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,15,15
2052,model,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .","[('inspired by', (4, 6)), ('develop', (21, 22)), ('with', (32, 33)), ('on', (37, 38))]","[('RPN', (7, 8)), ('state - of - the - art face detector', (23, 32)), ('real - time speed', (33, 37)), ('CPU', (39, 40))]","[['state - of - the - art face detector', 'with', 'real - time speed'], ['real - time speed', 'on', 'CPU']]",[],"[['Model', 'inspired by', 'RPN']]",[],face_detection,15,41
2053,model,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .","[('propose', (3, 4)), ('named', (8, 9)), ('contains', (13, 14)), ('trained', (23, 24))]","[('novel face detector', (5, 8)), ('FaceBoxes', (9, 10)), ('single fully convolutional neural network', (15, 20)), ('end - to - end', (24, 29))]","[['novel face detector', 'named', 'FaceBoxes'], ['FaceBoxes', 'contains', 'single fully convolutional neural network'], ['novel face detector', 'trained', 'end - to - end']]","[['novel face detector', 'name', 'FaceBoxes']]","[['Model', 'propose', 'novel face detector']]",[],face_detection,15,42
2054,model,We design the Rapidly Digested Convolutional Layers ( RDCL ) to enable face detection to achieve real - time speed on the CPU ; We introduce the Multiple Scale Convolutional Layers ( MSCL ) to handle various scales of face via enriching receptive fields and discretizing anchors over layers .,"[('design', (1, 2)), ('to enable', (10, 12)), ('to achieve', (14, 16)), ('on', (20, 21)), ('introduce', (25, 26)), ('to handle', (34, 36)), ('via', (40, 41))]","[('Rapidly Digested Convolutional Layers ( RDCL )', (3, 10)), ('face detection', (12, 14)), ('real - time speed', (16, 20)), ('CPU', (22, 23)), ('Multiple Scale Convolutional Layers ( MSCL )', (27, 34)), ('various scales of face', (36, 40)), ('enriching receptive fields', (41, 44)), ('discretizing anchors over', (45, 48)), ('layers', (48, 49))]","[['Rapidly Digested Convolutional Layers ( RDCL )', 'to enable', 'face detection'], ['face detection', 'to achieve', 'real - time speed'], ['real - time speed', 'on', 'CPU'], ['Multiple Scale Convolutional Layers ( MSCL )', 'to handle', 'various scales of face'], ['various scales of face', 'via', 'enriching receptive fields'], ['various scales of face', 'via', 'discretizing anchors over']]","[['discretizing anchors over', 'has', 'layers']]","[['Model', 'design', 'Rapidly Digested Convolutional Layers ( RDCL )']]",[],face_detection,15,50
2055,hyperparameters,"All the parameters are randomly initialized with the "" xavier "" method .","[('with', (6, 7))]","[('parameters', (2, 3)), ('randomly initialized', (4, 6)), ('"" xavier "" method', (8, 12))]","[['randomly initialized', 'with', '"" xavier "" method']]","[['parameters', 'has', 'randomly initialized']]",[],"[['Hyperparameters', 'has', 'parameters']]",face_detection,15,165
2056,hyperparameters,"We finetune the resulting model using SGD with 0.9 momentum , 0.0005 weight decay and batch size 32 .","[('finetune', (1, 2)), ('using', (5, 6)), ('with', (7, 8))]","[('resulting model', (3, 5)), ('SGD', (6, 7)), ('0.9 momentum', (8, 10)), ('0.0005 weight decay', (11, 14)), ('batch size', (15, 17)), ('32', (17, 18))]","[['resulting model', 'using', 'SGD'], ['resulting model', 'using', 'batch size'], ['SGD', 'with', '0.9 momentum'], ['SGD', 'with', 'batch size']]","[['batch size', 'has', '32']]","[['Hyperparameters', 'finetune', 'resulting model']]",[],face_detection,15,166
2057,hyperparameters,"The maximum number of iterations is 120 k and we use 10 ? 3 learning rate for the first 80 k iterations , then continue training for 20 k iterations with 10 ? 4 and 10 ? 5 , respectively .","[('is', (5, 6)), ('use', (10, 11)), ('for', (16, 17)), ('continue', (24, 25)), ('for', (26, 27)), ('with', (30, 31))]","[('maximum number of iterations', (1, 5)), ('120 k', (6, 8)), ('10 ? 3 learning rate', (11, 16)), ('first 80 k iterations', (18, 22)), ('training', (25, 26)), ('20 k iterations', (27, 30)), ('10 ? 4 and 10 ? 5', (31, 38))]","[['maximum number of iterations', 'is', '120 k'], ['maximum number of iterations', 'use', '10 ? 3 learning rate'], ['10 ? 3 learning rate', 'for', 'first 80 k iterations'], ['training', 'for', '20 k iterations'], ['10 ? 3 learning rate', 'continue', 'training'], ['training', 'for', '20 k iterations'], ['20 k iterations', 'with', '10 ? 4 and 10 ? 5']]","[['maximum number of iterations', 'has', '120 k']]",[],"[['Hyperparameters', 'has', 'maximum number of iterations']]",face_detection,15,167
2058,hyperparameters,Our method is implemented in the Caffe library .,"[('implemented in', (3, 5))]","[('Caffe library', (6, 8))]",[],[],"[['Hyperparameters', 'implemented in', 'Caffe library']]",[],face_detection,15,168
2059,results,"As listed in Tab. 1 , comparing with recent CNN - based methods , our FaceBoxes can run at 20 FPS on the CPU with state - of - the - art accuracy .","[('comparing', (6, 7)), ('run at', (17, 19)), ('on', (21, 22)), ('with', (24, 25))]","[('our FaceBoxes', (14, 16)), ('20 FPS', (19, 21)), ('CPU', (23, 24)), ('state - of - the - art accuracy', (25, 33))]","[['our FaceBoxes', 'run at', '20 FPS'], ['20 FPS', 'on', 'CPU'], ['CPU', 'with', 'state - of - the - art accuracy']]",[],[],[],face_detection,15,178
2060,ablation-analysis,"2 indicates that MSCL effectively increases the m AP by 1.0 % , owning to the diverse receptive fields and the multi -scale anchor tiling mechanism .","[('indicates', (1, 2)), ('effectively increases', (4, 6)), ('by', (9, 10)), ('owning to', (13, 15))]","[('MSCL', (3, 4)), ('m AP', (7, 9)), ('1.0 %', (10, 12))]","[['MSCL', 'effectively increases', 'm AP'], ['m AP', 'by', '1.0 %']]",[],"[['Ablation analysis', 'indicates', 'MSCL']]",[],face_detection,15,201
2061,ablation-analysis,RDCL is efficient and accuracy - preserving .,[],"[('RDCL', (0, 1))]",[],[],[],"[['Ablation analysis', 'has', 'RDCL']]",face_detection,15,202
2062,results,"As illustrated in , our FaceBoxes outperforms all others by a large margin .","[('by', (9, 10))]","[('our FaceBoxes', (4, 6)), ('outperforms', (6, 7)), ('all others', (7, 9)), ('large margin', (11, 13))]","[['outperforms', 'by', 'large margin'], ['all others', 'by', 'large margin']]","[['our FaceBoxes', 'has', 'outperforms'], ['outperforms', 'has', 'all others']]",[],"[['Results', 'has', 'our FaceBoxes']]",face_detection,15,212
2063,results,shows some qualitative results on the AFW dataset .,"[('on', (4, 5))]","[('AFW dataset', (6, 8))]",[],[],"[['Results', 'on', 'AFW dataset']]",[],face_detection,15,213
2064,results,"Our method significantly outperforms all other methods and commercial face detectors ( e.g. , SkyBiometry , Face + + and Picasa ) .",[],"[('significantly outperforms', (2, 4)), ('all other methods and commercial face detectors', (4, 11))]",[],"[['significantly outperforms', 'has', 'all other methods and commercial face detectors']]",[],[],face_detection,15,217
2065,results,shows some qualitative results on the PASCAL face dataset .,"[('on', (4, 5))]","[('PASCAL face dataset', (6, 9))]",[],[],"[['Results', 'on', 'PASCAL face dataset']]",[],face_detection,15,218
2066,research-problem,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition",[],"[('Face Detection', (8, 10)), ('Landmark Localization', (11, 13)), ('Gender', (18, 19))]",[],[],[],[],face_detection,16,2
2067,research-problem,"D ETECTION and analysis of faces is a challenging problem in computer vision , and has been actively researched for applications such as face verification , face tracking , person identification , etc .",[],"[('D ETECTION and analysis of faces', (0, 6))]",[],[],[],[],face_detection,16,11
2068,model,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .","[('present', (5, 6)), ('based on', (9, 11)), ('for', (12, 13)), ('from', (27, 28))]","[('novel framework', (7, 9)), ('CNNs', (11, 12)), ('simultaneous face detection', (13, 16)), ('facial landmarks localization', (17, 20)), ('head pose estimation', (21, 24)), ('gender recognition', (25, 27)), ('given image', (29, 31))]","[['novel framework', 'based on', 'CNNs'], ['CNNs', 'for', 'simultaneous face detection'], ['CNNs', 'for', 'head pose estimation'], ['CNNs', 'for', 'gender recognition'], ['gender recognition', 'from', 'given image']]",[],"[['Model', 'present', 'novel framework']]",[],face_detection,16,15
2069,model,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,"[('design', (1, 2)), ('to learn', (5, 7)), ('for', (9, 10)), ('exploit', (13, 14))]","[('CNN architecture', (3, 5)), ('common features', (7, 9)), ('tasks', (11, 12)), ('synergy', (15, 16))]","[['CNN architecture', 'to learn', 'common features'], ['common features', 'for', 'tasks'], ['CNN architecture', 'exploit', 'synergy']]",[],"[['Model', 'design', 'CNN architecture']]",[],face_detection,16,16
2070,model,We exploit the fact that information contained in features is hierarchically distributed throughout the network as demonstrated in .,"[('exploit', (1, 2)), ('contained in', (6, 8))]","[('information', (5, 6)), ('features', (8, 9)), ('hierarchically distributed', (10, 12)), ('network', (14, 15))]","[['information', 'contained in', 'features']]",[],"[['Model', 'exploit', 'information']]",[],face_detection,16,17
2071,model,Features fusion aims to transform the features to a common subspace where they can be combined linearly or non-linearly .,[],"[('Features fusion', (0, 2))]",[],[],[],"[['Model', 'has', 'Features fusion']]",face_detection,16,27
2072,model,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .","[('construct', (3, 4)), ('to fuse', (9, 11))]","[('separate fusion - CNN', (5, 9)), ('hyperfeatures', (12, 13))]","[['separate fusion - CNN', 'to fuse', 'hyperfeatures']]",[],"[['Model', 'construct', 'separate fusion - CNN']]",[],face_detection,16,29
2073,model,Fusing the intermediate layer features provides additional performance boost .,"[('Fusing', (0, 1)), ('provides', (5, 6))]","[('intermediate layer features', (2, 5)), ('additional performance boost', (6, 9))]","[['intermediate layer features', 'provides', 'additional performance boost']]",[],"[['Model', 'Fusing', 'intermediate layer features']]",[],face_detection,16,37
2074,research-problem,"Since then , several approaches have adopted MTL for solving different problems in computer vision .",[],"[('MTL', (7, 8))]",[],[],[],[],face_detection,16,51
2075,model,This method is based on a mixture of trees with a shared pool of parts in the sense that every facial landmark is modeled as apart and uses global mixtures to capture the topological changes due to viewpoint variations .,"[('based on', (3, 5)), ('with', (9, 10)), ('modeled as', (23, 25)), ('uses', (27, 28)), ('to capture', (30, 32)), ('due to', (35, 37))]","[('mixture of trees', (6, 9)), ('shared pool of parts', (11, 15)), ('every facial landmark', (19, 22)), ('apart', (25, 26)), ('global mixtures', (28, 30)), ('topological changes', (33, 35)), ('viewpoint variations', (37, 39))]","[['mixture of trees', 'with', 'shared pool of parts'], ['every facial landmark', 'modeled as', 'apart'], ['every facial landmark', 'uses', 'global mixtures'], ['global mixtures', 'to capture', 'topological changes'], ['topological changes', 'due to', 'viewpoint variations']]",[],"[['Model', 'based on', 'mixture of trees']]",[],face_detection,16,53
2076,model,It fuses all the intermediate layers of a CNN at three different scales of the image pyramid for multi-task training on diverse sets .,"[('fuses', (1, 2)), ('of', (6, 7)), ('at', (9, 10)), ('of', (13, 14)), ('for', (17, 18)), ('on', (20, 21))]","[('all the intermediate layers', (2, 6)), ('CNN', (8, 9)), ('three different scales', (10, 13)), ('image pyramid', (15, 17)), ('multi-task training', (18, 20)), ('diverse sets', (21, 23))]","[['all the intermediate layers', 'of', 'CNN'], ['three different scales', 'of', 'image pyramid'], ['CNN', 'at', 'three different scales'], ['three different scales', 'of', 'image pyramid'], ['image pyramid', 'for', 'multi-task training'], ['multi-task training', 'on', 'diverse sets']]",[],"[['Model', 'fuses', 'all the intermediate layers']]",[],face_detection,16,60
2077,model,"Instead , we strategically design the network architecture such that the tasks exploit low level as well as high level features of the network .","[('strategically design', (3, 5)), ('such that', (8, 10)), ('exploit', (12, 13)), ('of', (21, 22))]","[('network architecture', (6, 8)), ('tasks', (11, 12)), ('low level as well as high level features', (13, 21)), ('network', (23, 24))]","[['network architecture', 'such that', 'tasks'], ['tasks', 'exploit', 'low level as well as high level features'], ['low level as well as high level features', 'of', 'network']]",[],"[['Model', 'strategically design', 'network architecture']]",[],face_detection,16,66
2078,research-problem,We also jointly predict the task of face detection and landmark localization .,"[('jointly predict', (2, 4))]","[('task', (5, 6)), ('face detection and landmark localization', (7, 12))]",[],[],"[['Research problem', 'jointly predict', 'task']]",[],face_detection,16,67
2079,model,Landmarks localization :,[],"[('Landmarks localization', (0, 2))]",[],[],[],"[['Model', 'has', 'Landmarks localization']]",face_detection,16,84
2080,model,"While the former learns the shape increment given a mean initial shape , the latter trains an appearance model to predict the keypoint locations .","[('learns', (3, 4)), ('given', (7, 8)), ('trains', (15, 16)), ('to predict', (19, 21))]","[('former', (2, 3)), ('shape increment', (5, 7)), ('mean initial shape', (9, 12)), ('appearance model', (17, 19)), ('keypoint locations', (22, 24))]","[['former', 'learns', 'shape increment'], ['shape increment', 'given', 'mean initial shape'], ['mean initial shape', 'trains', 'appearance model'], ['appearance model', 'to predict', 'keypoint locations']]",[],[],"[['Model', 'has', 'former']]",face_detection,16,88
2081,experiments,Gender recognition :,[],"[('Gender recognition', (0, 2))]",[],[],[],[],face_detection,16,100
2082,hyperparameters,"It provides annotations for 21 landmark points per face , along with the face bounding - box , face pose ( yaw , pitch and roll ) and gender information .","[('provides', (1, 2)), ('for', (3, 4)), ('along with', (10, 12))]","[('annotations', (2, 3)), ('21 landmark points per face', (4, 9)), ('face bounding - box', (13, 17)), ('face pose ( yaw , pitch and roll )', (18, 27)), ('gender information', (28, 30))]","[['annotations', 'for', '21 landmark points per face'], ['21 landmark points per face', 'along with', 'face bounding - box'], ['21 landmark points per face', 'along with', 'gender information']]",[],"[['Hyperparameters', 'provides', 'annotations']]",[],face_detection,16,144
2083,experiments,We use the Selective Search algorithm in R - CNN to generate region proposals for faces in an image .,"[('use', (1, 2)), ('in', (6, 7)), ('to generate', (10, 12)), ('for', (14, 15)), ('in', (16, 17))]","[('Selective Search algorithm', (3, 6)), ('R - CNN', (7, 10)), ('region proposals', (12, 14)), ('faces', (15, 16)), ('image', (18, 19))]","[['Selective Search algorithm', 'in', 'R - CNN'], ['faces', 'in', 'image'], ['Selective Search algorithm', 'to generate', 'region proposals'], ['R - CNN', 'to generate', 'region proposals'], ['region proposals', 'for', 'faces'], ['faces', 'in', 'image']]",[],[],[],face_detection,16,148
2084,hyperparameters,We use 21 point markups for face landmarks locations as provided in the AFLW dataset .,"[('use', (1, 2)), ('for', (5, 6)), ('provided', (10, 11))]","[('21 point markups', (2, 5)), ('face landmarks locations', (6, 9)), ('AFLW dataset', (13, 15))]","[['21 point markups', 'for', 'face landmarks locations']]",[],"[['Hyperparameters', 'use', '21 point markups']]",[],face_detection,16,157
2085,experiments,We also learn the visibility factor in order to test the presence of the predicted landmark .,"[('learn', (2, 3)), ('in order to test', (6, 10)), ('of', (12, 13))]","[('visibility factor', (4, 6)), ('presence', (11, 12)), ('predicted landmark', (14, 16))]","[['visibility factor', 'in order to test', 'presence'], ['presence', 'of', 'predicted landmark']]",[],[],[],face_detection,16,170
2086,experiments,Gender Recognition :,[],"[('Gender Recognition', (0, 2))]",[],[],[],[],face_detection,16,178
2087,experiments,Predicting gender is a two class problem similar to face detection .,"[('Predicting', (0, 1)), ('is', (2, 3)), ('similar to', (7, 9))]","[('two class problem', (4, 7)), ('face detection', (9, 11))]","[['two class problem', 'similar to', 'face detection']]",[],[],[],face_detection,16,179
2088,experiments,IRP improves the recall by generating more candidate proposals by using the predicted landmarks information from the initial set of region proposals .,"[('improves', (1, 2)), ('by generating', (4, 6)), ('by using', (9, 11)), ('from', (15, 16))]","[('IRP', (0, 1)), ('recall', (3, 4)), ('more candidate proposals', (6, 9)), ('predicted landmarks information', (12, 15)), ('initial set of region proposals', (17, 22))]","[['IRP', 'improves', 'recall'], ['recall', 'by generating', 'more candidate proposals'], ['more candidate proposals', 'by using', 'predicted landmarks information'], ['predicted landmarks information', 'from', 'initial set of region proposals']]",[],[],[],face_detection,16,211
2089,baselines,R- CNN,[],[],[],[],[],[],face_detection,16,263
2090,experiments,We also perform a linear bounding box regression to localize the face co-ordinates .,"[('perform', (2, 3)), ('to localize', (8, 10))]","[('linear bounding box regression', (4, 8)), ('face co-ordinates', (11, 13))]","[['linear bounding box regression', 'to localize', 'face co-ordinates']]",[],[],[],face_detection,16,271
2091,research-problem,Fast - HyperFace,[],[],[],[],[],[],face_detection,16,433
2092,experiments,The Hyperface method is tested on a machine with 8 cores and GTX TITAN - X GPU .,"[('tested on', (4, 6)), ('with', (8, 9))]","[('Hyperface method', (1, 3)), ('machine', (7, 8)), ('8 cores', (9, 11)), ('GTX TITAN - X GPU', (12, 17))]","[['Hyperface method', 'tested on', 'machine'], ['Hyperface method', 'tested on', 'GTX TITAN - X GPU'], ['machine', 'with', '8 cores']]","[['Hyperface method', 'has', 'machine']]",[],[],face_detection,16,434
2093,model,We also propose a fast version of HyperFace which uses a high recall fast face detector instead of Selective Search to generate candidate region proposals .,"[('propose', (2, 3)), ('of', (6, 7)), ('uses', (9, 10)), ('instead of', (16, 18)), ('to generate', (20, 22))]","[('fast version', (4, 6)), ('HyperFace', (7, 8)), ('high recall fast face detector', (11, 16)), ('Selective Search', (18, 20)), ('candidate region proposals', (22, 25))]","[['fast version', 'of', 'HyperFace'], ['HyperFace', 'uses', 'high recall fast face detector'], ['high recall fast face detector', 'instead of', 'Selective Search'], ['Selective Search', 'to generate', 'candidate region proposals']]",[],"[['Model', 'propose', 'fast version']]",[],face_detection,16,438
2094,model,We implement a face detector using Single Shot Detector ( SSD ) framework .,"[('implement', (1, 2)), ('using', (5, 6))]","[('face detector', (3, 5)), ('Single Shot Detector ( SSD ) framework', (6, 13))]","[['face detector', 'using', 'Single Shot Detector ( SSD ) framework']]",[],"[['Model', 'implement', 'face detector']]",[],face_detection,16,439
2095,experiments,"Fast - HyperFace consumes a total time of 0.15s ( 0.05 s for SSD face detector , and 0.1s for HyperFace ) on a GTX TITAN X GPU .","[('consumes', (3, 4)), ('of', (7, 8)), ('on', (22, 23))]","[('Fast - HyperFace', (0, 3)), ('total time', (5, 7)), ('0.15s', (8, 9)), ('GTX TITAN X GPU', (24, 28))]","[['Fast - HyperFace', 'consumes', 'total time'], ['total time', 'of', '0.15s']]",[],[],[],face_detection,16,445
2096,results,"The Fast - HyperFace achieves a m AP of 97.6 % on AFW face detection task , which is comparable to the HyperFace m AP of 97.9 % .","[('achieves', (4, 5)), ('of', (8, 9)), ('on', (11, 12))]","[('Fast - HyperFace', (1, 4)), ('m AP', (6, 8)), ('97.6 %', (9, 11)), ('AFW face detection task', (12, 16))]","[['Fast - HyperFace', 'achieves', 'm AP'], ['m AP', 'of', '97.6 %'], ['m AP', 'on', 'AFW face detection task'], ['97.6 %', 'on', 'AFW face detection task']]",[],[],"[['Results', 'has', 'Fast - HyperFace']]",face_detection,16,446
2097,experiments,"Thus , Fast - HyperFace improves the speed by a factor of 12 with negligible degradation in performance .","[('improves', (5, 6)), ('by', (8, 9)), ('with', (13, 14)), ('in', (16, 17))]","[('Fast - HyperFace', (2, 5)), ('speed', (7, 8)), ('factor of 12', (10, 13)), ('negligible degradation', (14, 16)), ('performance', (17, 18))]","[['Fast - HyperFace', 'improves', 'speed'], ['speed', 'by', 'factor of 12'], ['factor of 12', 'with', 'negligible degradation'], ['negligible degradation', 'in', 'performance']]",[],[],[],face_detection,16,447
2098,research-problem,Pyramid Box : A Context - assisted Single Shot Face Detector,[],[],[],[],[],[],face_detection,17,2
2099,code,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,[],[],[],[],[],[],face_detection,17,12
2100,research-problem,Face detection is a fundamental and essential task in various face applications .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,17,15
2101,model,Face R - FCN re-weights embedding responses on score maps and eliminates the effect of non-uniformed contribution in each facial part using a position - sensitive average pooling .,"[('re-weights', (4, 5)), ('on', (7, 8)), ('eliminates', (11, 12)), ('in', (17, 18)), ('using', (21, 22))]","[('Face R - FCN', (0, 4)), ('embedding responses', (5, 7)), ('score maps', (8, 10)), ('effect of non-uniformed contribution', (13, 17)), ('each facial part', (18, 21)), ('position - sensitive average pooling', (23, 28))]","[['Face R - FCN', 're-weights', 'embedding responses'], ['embedding responses', 'on', 'score maps'], ['Face R - FCN', 'eliminates', 'effect of non-uniformed contribution'], ['effect of non-uniformed contribution', 'in', 'each facial part'], ['each facial part', 'using', 'position - sensitive average pooling']]",[],[],"[['Model', 'has', 'Face R - FCN']]",face_detection,17,24
2102,model,FAN proposes an anchor - level attention by highlighting the features from the face region to detect the occluded faces .,"[('proposes', (1, 2)), ('by highlighting', (7, 9)), ('from', (11, 12)), ('to detect', (15, 17))]","[('FAN', (0, 1)), ('anchor - level attention', (3, 7)), ('features', (10, 11)), ('face region', (13, 15)), ('occluded faces', (18, 20))]","[['FAN', 'proposes', 'anchor - level attention'], ['anchor - level attention', 'by highlighting', 'features'], ['features', 'from', 'face region'], ['face region', 'to detect', 'occluded faces']]","[['FAN', 'has', 'anchor - level attention']]",[],"[['Model', 'has', 'FAN']]",face_detection,17,25
2103,model,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .","[('use', (5, 6)), ('to generate', (9, 11)), ('for', (13, 14)), ('related to', (16, 18)), ('called', (24, 25)), ('invented to be easily added to', (27, 33))]","[('semi-supervised solution', (7, 9)), ('approximate labels', (11, 13)), ('contextual parts', (14, 16)), ('faces', (18, 19)), ('series of anchors', (21, 24)), ('PyramidAnchors', (25, 26)), ('general anchor - based architectures', (33, 38))]","[['semi-supervised solution', 'to generate', 'approximate labels'], ['approximate labels', 'for', 'contextual parts'], ['contextual parts', 'related to', 'faces'], ['series of anchors', 'called', 'PyramidAnchors'], ['PyramidAnchors', 'invented to be easily added to', 'general anchor - based architectures']]","[['series of anchors', 'name', 'PyramidAnchors']]","[['Model', 'use', 'semi-supervised solution']]",[],face_detection,17,31
2104,model,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,"[('introduce', (1, 2)), ('to incorporate', (11, 13)), ('around', (15, 16)), ('with', (19, 20))]","[('Context - sensitive prediction module ( CPM )', (3, 11)), ('context information', (13, 15)), ('target face', (17, 19)), ('wider and deeper network', (21, 25))]","[['Context - sensitive prediction module ( CPM )', 'to incorporate', 'context information'], ['context information', 'around', 'target face'], ['context information', 'with', 'wider and deeper network'], ['target face', 'with', 'wider and deeper network']]",[],"[['Model', 'introduce', 'Context - sensitive prediction module ( CPM )']]",[],face_detection,17,36
2105,model,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .","[('propose', (3, 4)), ('for', (11, 12)), ('to further improve', (15, 18)), ('of', (20, 21))]","[('max - in - out layer', (5, 11)), ('prediction module', (13, 15)), ('capability', (19, 20)), ('classification network', (21, 23))]","[['max - in - out layer', 'for', 'prediction module'], ['max - in - out layer', 'to further improve', 'capability'], ['prediction module', 'to further improve', 'capability'], ['capability', 'of', 'classification network']]",[],"[['Model', 'propose', 'max - in - out layer']]",[],face_detection,17,37
2106,model,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .","[('propose', (4, 5)), ('named as', (8, 10)), ('to make', (15, 17)), ('on', (19, 20)), ('of', (22, 23))]","[('training strategy', (6, 8)), ('Data - anchor - sampling', (10, 15)), ('adjustment', (18, 19)), ('distribution', (21, 22)), ('training dataset', (24, 26))]","[['training strategy', 'named as', 'Data - anchor - sampling'], ['Data - anchor - sampling', 'to make', 'adjustment'], ['adjustment', 'on', 'distribution'], ['distribution', 'of', 'training dataset']]","[['training strategy', 'name', 'Data - anchor - sampling']]","[['Model', 'propose', 'training strategy']]",[],face_detection,17,38
2107,model,"We propose an anchor-based context assisted method , called PyramidAnchors , to introduce supervised information on learning contextual features for small , blurred and partially occluded faces .","[('propose', (1, 2)), ('called', (8, 9)), ('to introduce', (11, 13)), ('on learning', (15, 17)), ('for', (19, 20))]","[('anchor-based context assisted method', (3, 7)), ('PyramidAnchors', (9, 10)), ('supervised information', (13, 15)), ('contextual features', (17, 19)), ('small , blurred and partially occluded faces', (20, 27))]","[['anchor-based context assisted method', 'called', 'PyramidAnchors'], ['anchor-based context assisted method', 'to introduce', 'supervised information'], ['supervised information', 'on learning', 'contextual features'], ['contextual features', 'for', 'small , blurred and partially occluded faces']]","[['anchor-based context assisted method', 'name', 'PyramidAnchors']]","[['Model', 'propose', 'anchor-based context assisted method']]",[],face_detection,17,42
2108,model,2 . We design the Low - level Feature Pyramid Networks ( LFPN ) to merge contextual features and facial features better .,"[('design', (3, 4)), ('to merge', (14, 16))]","[('Low - level Feature Pyramid Networks ( LFPN )', (5, 14)), ('contextual features and facial features', (16, 21)), ('better', (21, 22))]","[['Low - level Feature Pyramid Networks ( LFPN )', 'to merge', 'contextual features and facial features']]","[['contextual features and facial features', 'has', 'better']]","[['Model', 'design', 'Low - level Feature Pyramid Networks ( LFPN )']]",[],face_detection,17,43
2109,model,"3 . We introduce a context - sensitive prediction module , consisting of a mixed network structure and max - in - out layer to learn accurate location and classification from the merged features .","[('introduce', (3, 4)), ('consisting of', (11, 13)), ('to learn', (24, 26)), ('from', (30, 31))]","[('context - sensitive prediction module', (5, 10)), ('mixed network structure', (14, 17)), ('max - in - out layer', (18, 24)), ('accurate location and classification', (26, 30)), ('merged features', (32, 34))]","[['context - sensitive prediction module', 'consisting of', 'mixed network structure'], ['context - sensitive prediction module', 'consisting of', 'max - in - out layer'], ['max - in - out layer', 'to learn', 'accurate location and classification'], ['accurate location and classification', 'from', 'merged features']]","[['context - sensitive prediction module', 'name', 'mixed network structure']]","[['Model', 'introduce', 'context - sensitive prediction module']]",[],face_detection,17,45
2110,experiments,"Data sampling is a classical subject in statistics , machine learning and pattern recognition , it achieves great development in recent years .",[],"[('Data sampling', (0, 2))]",[],[],[],[],face_detection,17,157
2111,experiments,"For the task of objection detection , Focus Loss address the class imbalance by reshaping the standard cross entropy loss .","[('of', (3, 4)), ('address', (9, 10)), ('by reshaping', (13, 15))]","[('objection detection', (4, 6)), ('Focus Loss', (7, 9)), ('class imbalance', (11, 13)), ('standard cross entropy loss', (16, 20))]","[['Focus Loss', 'address', 'class imbalance'], ['class imbalance', 'by reshaping', 'standard cross entropy loss']]","[['objection detection', 'has', 'Focus Loss']]",[],[],face_detection,17,158
2112,baselines,Here we utilize a data augment sample method named Data - anchor - sampling .,"[('utilize', (2, 3)), ('named', (8, 9))]","[('data augment sample method', (4, 8)), ('Data - anchor - sampling', (9, 14))]","[['data augment sample method', 'named', 'Data - anchor - sampling']]","[['data augment sample method', 'name', 'Data - anchor - sampling']]","[['Baselines', 'utilize', 'data augment sample method']]",[],face_detection,17,159
2113,experiments,2 ) generate smaller face samples through larger ones to increase the diversity of face samples of smaller scales .,"[('generate', (2, 3)), ('through', (6, 7)), ('to increase', (9, 11)), ('of', (16, 17))]","[('smaller face samples', (3, 6)), ('larger ones', (7, 9)), ('diversity of face samples', (12, 16)), ('smaller scales', (17, 19))]","[['smaller face samples', 'through', 'larger ones'], ['larger ones', 'to increase', 'diversity of face samples'], ['diversity of face samples', 'of', 'smaller scales']]",[],[],[],face_detection,17,168
2114,hyperparameters,"As for the parameter initialization , our PyramidBox use the pre-trained parameters from VGG16 .","[('use', (8, 9)), ('from', (12, 13))]","[('parameter initialization', (3, 5)), ('our PyramidBox', (6, 8)), ('pre-trained parameters', (10, 12)), ('VGG16', (13, 14))]","[['our PyramidBox', 'use', 'pre-trained parameters'], ['pre-trained parameters', 'from', 'VGG16']]","[['parameter initialization', 'has', 'our PyramidBox']]",[],[],face_detection,17,187
2115,hyperparameters,"The parameters of conv fc 67 and conv fc 7 are initialized by sub - sampling parameters from fc 6 and fc 7 of VGG16 and the other additional layers are randomly initialized with "" xavier "" in .","[('of', (2, 3)), ('initialized by', (11, 13)), ('from', (17, 18)), ('of', (23, 24)), ('randomly initialized with', (31, 34))]","[('parameters', (1, 2)), ('conv fc 67 and conv fc 7', (3, 10)), ('sub - sampling parameters', (13, 17)), ('fc 6 and fc 7', (18, 23)), ('VGG16', (24, 25)), ('other additional layers', (27, 30)), ('"" xavier ""', (34, 37))]","[['parameters', 'of', 'conv fc 67 and conv fc 7'], ['sub - sampling parameters', 'of', 'fc 6 and fc 7'], ['conv fc 67 and conv fc 7', 'initialized by', 'sub - sampling parameters'], ['sub - sampling parameters', 'from', 'fc 6 and fc 7'], ['fc 6 and fc 7', 'of', 'VGG16'], ['other additional layers', 'randomly initialized with', '"" xavier ""']]",[],[],"[['Hyperparameters', 'has', 'parameters']]",face_detection,17,188
2116,hyperparameters,"We use a learning rate of 10 ? 3 for 80 k iterations , and 10 ? 4 for the next 20 k iterations , and 10 ? 5 for the last 20 k iterations on the WIDER FACE training set with batch size 16 .","[('use', (1, 2)), ('of', (5, 6)), ('for', (9, 10)), ('on', (35, 36)), ('with', (41, 42))]","[('learning rate', (3, 5)), ('10 ? 3', (6, 9)), ('80 k iterations', (10, 13)), ('next 20 k iterations', (20, 24)), ('last 20 k iterations', (31, 35)), ('WIDER FACE training set', (37, 41)), ('batch size 16', (42, 45))]","[['learning rate', 'of', '10 ? 3'], ['10 ? 3', 'for', '80 k iterations'], ['last 20 k iterations', 'on', 'WIDER FACE training set'], ['WIDER FACE training set', 'with', 'batch size 16']]","[['learning rate', 'has', '10 ? 3']]","[['Hyperparameters', 'use', 'learning rate']]",[],face_detection,17,189
2117,hyperparameters,We also use a momentum of 0.9 and a weight decay of 0.0005 .,"[('use', (2, 3)), ('of', (5, 6))]","[('momentum', (4, 5)), ('0.9', (6, 7)), ('weight decay', (9, 11)), ('0.0005', (12, 13))]","[['momentum', 'of', '0.9'], ['weight decay', 'of', '0.0005']]",[],"[['Hyperparameters', 'use', 'momentum']]",[],face_detection,17,190
2118,baselines,Our Pyramid,[],[],[],[],[],[],face_detection,17,196
2119,results,"The results listed in prove that LFPN started from a middle layer , using conv fc7 in our Pyramid Box , is more powerful , which implies that features with large gap in scale may not help each other .","[('prove', (4, 5)), ('started from', (7, 9)), ('using', (13, 14)), ('in', (16, 17)), ('is', (21, 22)), ('implies', (26, 27))]","[('LFPN', (6, 7)), ('middle layer', (10, 12)), ('conv fc7', (14, 16)), ('our Pyramid Box', (17, 20)), ('more powerful', (22, 24))]","[['LFPN', 'started from', 'middle layer'], ['middle layer', 'using', 'conv fc7'], ['conv fc7', 'in', 'our Pyramid Box'], ['conv fc7', 'is', 'more powerful']]",[],"[['Results', 'prove', 'LFPN']]",[],face_detection,17,201
2120,results,The comparison between the first and forth column of indicates that LFPN increases the m AP by 1.9 % on hard subset .,"[('indicates', (9, 10)), ('increases', (12, 13)), ('by', (16, 17)), ('on', (19, 20))]","[('LFPN', (11, 12)), ('m AP', (14, 16)), ('1.9 %', (17, 19)), ('hard subset', (20, 22))]","[['LFPN', 'increases', 'm AP'], ['m AP', 'by', '1.9 %'], ['1.9 %', 'on', 'hard subset']]",[],[],[],face_detection,17,202
2121,results,We employ Data - anchor - sampling based on LFPN network and the result shows that our data - anchor - sampling effectively improves the performance .,"[('employ', (1, 2)), ('based on', (7, 9)), ('shows', (14, 15))]","[('Data - anchor - sampling', (2, 7)), ('LFPN network', (9, 11)), ('our data - anchor - sampling', (16, 22)), ('effectively improves', (22, 24)), ('performance', (25, 26))]","[['Data - anchor - sampling', 'based on', 'LFPN network']]","[['our data - anchor - sampling', 'has', 'effectively improves'], ['effectively improves', 'has', 'performance']]","[['Results', 'employ', 'Data - anchor - sampling']]",[],face_detection,17,205
2122,results,"The mAP is increased by 0.4 % , 0.4 % and 0.6 % on easy , medium and hard subset , respectively .","[('increased by', (3, 5)), ('on', (13, 14))]","[('mAP', (1, 2)), ('0.4 % , 0.4 % and 0.6 %', (5, 13)), ('easy , medium and hard subset', (14, 20))]","[['mAP', 'increased by', '0.4 % , 0.4 % and 0.6 %'], ['0.4 % , 0.4 % and 0.6 %', 'on', 'easy , medium and hard subset']]",[],[],"[['Results', 'has', 'mAP']]",face_detection,17,206
2123,results,"By comparing the first and last column in , one can see that PyamidAnchor effectively improves the performance , i.e. , 0.7 % , 0.6 % and 0.9 % on easy , medium and hard , respectively .","[('comparing', (1, 2)), ('see that', (11, 13)), ('effectively', (14, 15)), ('i.e.', (19, 20)), ('on', (29, 30))]","[('PyamidAnchor', (13, 14)), ('performance', (17, 18)), ('easy , medium and hard', (30, 35))]",[],[],[],[],face_detection,17,209
2124,results,Wider and deeper context prediction module is better .,"[('is', (6, 7))]","[('Wider and deeper context prediction module', (0, 6)), ('better', (7, 8))]","[['Wider and deeper context prediction module', 'is', 'better']]","[['Wider and deeper context prediction module', 'has', 'better']]",[],"[['Results', 'has', 'Wider and deeper context prediction module']]",face_detection,17,211
2125,results,shows that the performance of CPM is better than both DSSD module and SSH context module .,"[('shows', (0, 1)), ('of', (4, 5)), ('better than', (7, 9))]","[('performance', (3, 4)), ('CPM', (5, 6)), ('both', (9, 10)), ('DSSD module and SSH context module', (10, 16))]","[['performance', 'of', 'CPM'], ['performance', 'better than', 'DSSD module and SSH context module'], ['CPM', 'better than', 'both'], ['CPM', 'better than', 'DSSD module and SSH context module']]","[['performance', 'has', 'CPM'], ['both', 'has', 'DSSD module and SSH context module']]","[['Results', 'shows', 'performance']]",[],face_detection,17,212
2126,results,"Notice that the combination of SSH and DSSD gains very little compared to SSH alone , which indicates that large receptive field is more important to predict the accurate location and classification .","[('Notice', (0, 1)), ('gains', (8, 9)), ('compared to', (11, 13))]","[('combination of', (3, 5)), ('SSH and DSSD', (5, 8)), ('very little', (9, 11)), ('SSH alone', (13, 15))]","[['SSH and DSSD', 'gains', 'very little'], ['very little', 'compared to', 'SSH alone']]","[['combination of', 'has', 'SSH and DSSD']]","[['Results', 'Notice', 'combination of']]",[],face_detection,17,213
2127,results,"In addition , by comparing the last two column of , one can find that the method of Max - in - out improves the mAP on WIDER FACE validation set about + 0.2 % ( Easy ) , + 0.3 % ( Medium ) and + 0.1 % ( Hard ) , respectively .","[('find that', (13, 15)), ('improves', (23, 24)), ('on', (26, 27)), ('about', (31, 32))]","[('method of Max - in - out', (16, 23)), ('mAP', (25, 26)), ('WIDER FACE validation set', (27, 31)), ('+ 0.2 % ( Easy )', (32, 38)), ('+ 0.3 % ( Medium )', (39, 45)), ('+ 0.1 % ( Hard )', (46, 52))]","[['method of Max - in - out', 'improves', 'mAP'], ['mAP', 'on', 'WIDER FACE validation set'], ['WIDER FACE validation set', 'about', '+ 0.2 % ( Easy )'], ['WIDER FACE validation set', 'about', '+ 0.3 % ( Medium )'], ['WIDER FACE validation set', 'about', '+ 0.1 % ( Hard )']]",[],"[['Results', 'find that', 'method of Max - in - out']]",[],face_detection,17,214
2128,results,"To conclude this section , we summarize our results in , from which one can see that m AP increase 2.1 % , 2.3 % and 4.7 % on easy , medium and hard subset , respectively .","[('can', (14, 15)), ('see', (15, 16)), ('increase', (19, 20)), ('on', (28, 29))]","[('m AP', (17, 19)), ('2.1 % , 2.3 % and 4.7 %', (20, 28)), ('easy , medium and hard subset', (29, 35))]","[['m AP', 'increase', '2.1 % , 2.3 % and 4.7 %'], ['2.1 % , 2.3 % and 4.7 %', 'on', 'easy , medium and hard subset']]","[['m AP', 'has', '2.1 % , 2.3 % and 4.7 %']]","[['Results', 'can', 'm AP']]",[],face_detection,17,215
2129,results,"Our PyramidBox outperforms others across all three subsets , i.e. 0.961 ( easy ) , 0.950 ( medium ) , 0.889 ( hard ) for validation set , and 0.956 ( easy ) , 0.946 ( medium ) , 0.887 ( hard ) for testing set .","[('i.e.', (9, 10))]","[('Our PyramidBox', (0, 2)), ('outperforms', (2, 3)), ('others', (3, 4)), ('validation', (25, 26))]",[],"[['Our PyramidBox', 'has', 'outperforms'], ['outperforms', 'has', 'others']]",[],[],face_detection,17,228
2130,research-problem,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection,[],[],[],[],[],[],face_detection,18,2
2131,research-problem,"Detection and analysis on human subjects using facial feature based biometrics for access control , surveillance systems and other security applications have gained popularity over the past few years .",[],"[('Detection and analysis on human subjects using facial feature based biometrics', (0, 11))]",[],[],[],[],face_detection,18,14
2132,model,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .",[],"[('Contextual Multi - Scale Region - based CNN ( CMS - RCNN )', (9, 22)), ('face detection in', (27, 30))]",[],[],[],[],face_detection,18,21
2133,model,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .","[('allows', (7, 8)), ('to simultaneously look at', (10, 14)), ('to explicitly look', (20, 23)), ('as', (26, 27))]","[('network', (9, 10)), ('multi-scale features', (14, 16)), ('outside facial regions', (23, 26)), ('potential body regions', (28, 31))]","[['network', 'to simultaneously look at', 'multi-scale features'], ['outside facial regions', 'as', 'potential body regions']]",[],"[['Model', 'allows', 'network']]",[],face_detection,18,22
2134,model,"Therefore , it is able to robustly deal with the challenges in the problem of unconstrained face detection .","[('able to', (4, 6)), ('in', (11, 12))]","[('robustly deal', (6, 8)), ('challenges', (10, 11)), ('problem of unconstrained face detection', (13, 18))]","[['challenges', 'in', 'problem of unconstrained face detection']]","[['robustly deal', 'has', 'challenges']]","[['Model', 'able to', 'robustly deal']]",[],face_detection,18,25
2135,model,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,"[('introduces', (5, 6)), ('to generate', (18, 20)), ('to do inference', (39, 42)), ('on', (42, 43)), ('of', (46, 47))]","[('CMS - RCNN method', (1, 5)), ('Multi - Scale Region Proposal Network ( MS - RPN )', (7, 18)), ('set', (21, 22)), ('region candidates', (23, 25)), ('region candidates', (44, 46)), ('facial regions', (47, 49))]","[['CMS - RCNN method', 'introduces', 'Multi - Scale Region Proposal Network ( MS - RPN )'], ['Multi - Scale Region Proposal Network ( MS - RPN )', 'to generate', 'set'], ['region candidates', 'of', 'facial regions']]","[['CMS - RCNN method', 'name', 'Multi - Scale Region Proposal Network ( MS - RPN )']]",[],"[['Model', 'has', 'CMS - RCNN method']]",face_detection,18,26
2136,model,"Inside the network , skip pooling is used to extract information at multiple scales and levels of abstraction .","[('Inside', (0, 1)), ('used to', (7, 9)), ('at', (11, 12)), ('of', (16, 17))]","[('network', (2, 3)), ('skip pooling', (4, 6)), ('extract', (9, 10)), ('information', (10, 11)), ('multiple scales and levels', (12, 16)), ('abstraction', (17, 18))]","[['skip pooling', 'used to', 'extract'], ['information', 'at', 'multiple scales and levels'], ['multiple scales and levels', 'of', 'abstraction']]","[['network', 'has', 'skip pooling'], ['extract', 'has', 'information']]","[['Model', 'Inside', 'network']]",[],face_detection,18,78
2137,model,"Unlike all the previous approaches that select a feature extractor beforehand and incorporate a linear classifier with the depth descriptor beside RGB channels , our method solves the problem under a deep learning framework where the global and the local context features , i.e. multi scaling , are synchronized to Faster Region - based Convolutional Neural Networks in order to robustly achieve semantic detection .","[('incorporate', (12, 13)), ('with', (16, 17)), ('beside', (20, 21)), ('solves', (26, 27)), ('under', (29, 30)), ('where', (34, 35)), ('synchronized to', (48, 50)), ('to robustly achieve', (59, 62))]","[('depth descriptor', (18, 20)), ('RGB channels', (21, 23)), ('our', (24, 25)), ('problem', (28, 29)), ('deep learning framework', (31, 34)), ('global and the local context features', (36, 42)), ('Faster Region - based Convolutional Neural Networks', (50, 57)), ('semantic detection', (62, 64))]","[['depth descriptor', 'beside', 'RGB channels'], ['our', 'solves', 'problem'], ['problem', 'under', 'deep learning framework'], ['deep learning framework', 'where', 'global and the local context features'], ['global and the local context features', 'synchronized to', 'Faster Region - based Convolutional Neural Networks'], ['Faster Region - based Convolutional Neural Networks', 'to robustly achieve', 'semantic detection']]",[],"[['Model', 'incorporate', 'depth descriptor']]",[],face_detection,18,82
2138,hyperparameters,Our CMS - RCNN is implemented in the Caffe deep learning framework .,"[('implemented in', (5, 7))]","[('CMS - RCNN', (1, 4)), ('Caffe deep learning framework', (8, 12))]","[['CMS - RCNN', 'implemented in', 'Caffe deep learning framework']]",[],[],"[['Hyperparameters', 'has', 'CMS - RCNN']]",face_detection,18,228
2139,hyperparameters,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .","[('of', (4, 5)), ('have', (7, 8)), ('as', (11, 12)), ('during', (20, 21)), ('initialized from', (25, 27))]","[('first 5 sets', (1, 4)), ('convolution layers', (5, 7)), ('same architecture', (9, 11)), ('deep VGG - 16 model', (13, 18)), ('training', (21, 22)), ('parameters', (23, 24)), ('pre-trained VGG - 16', (28, 32))]","[['first 5 sets', 'of', 'convolution layers'], ['convolution layers', 'have', 'same architecture'], ['same architecture', 'as', 'deep VGG - 16 model'], ['parameters', 'initialized from', 'pre-trained VGG - 16']]","[['training', 'has', 'parameters']]",[],"[['Hyperparameters', 'has', 'first 5 sets']]",face_detection,18,229
2140,hyperparameters,"Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .","[('set', (2, 3)), ('of', (6, 7)), ('to be', (19, 21))]","[('initial scale', (4, 6)), (""' conv3 ' , ' conv4 '"", (7, 14)), ('66.84 , 94.52 , and 94.52', (21, 27))]","[['initial scale', 'of', ""' conv3 ' , ' conv4 '""]]",[],"[['Hyperparameters', 'set', 'initial scale']]",[],face_detection,18,236
2141,hyperparameters,"Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .","[('pooled from', (3, 5)), ('initialized with', (18, 20)), ('to be', (21, 23)), ('for', (31, 32))]","[('features', (2, 3)), (""' conv3 ' , ' conv4 '"", (5, 12)), ('scale', (20, 21)), ('57.75 , 81.67 , and 81.67', (23, 29))]","[['features', 'pooled from', ""' conv3 ' , ' conv4 '""], ['scale', 'to be', '57.75 , 81.67 , and 81.67']]",[],[],"[['Hyperparameters', 'has', 'features']]",face_detection,18,239
2142,experiments,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .","[('share', (9, 10)), ('for', (13, 14)), ('so', (17, 18)), ('can be done', (20, 23)), ('resulting in', (25, 27))]","[('MS - RPN and the CMS - CNN', (1, 9)), ('same parameters', (11, 13)), ('all convolution layers', (14, 17)), ('computation', (19, 20)), ('once', (23, 24)), ('higher efficiency', (27, 29))]","[['MS - RPN and the CMS - CNN', 'share', 'same parameters'], ['same parameters', 'for', 'all convolution layers'], ['all convolution layers', 'so', 'computation'], ['computation', 'can be done', 'once'], ['computation', 'resulting in', 'higher efficiency'], ['once', 'resulting in', 'higher efficiency']]",[],[],[],face_detection,18,240
2143,hyperparameters,"Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .","[('in order', (2, 4)), ('to shrink', (4, 6)), ('of', (9, 10)), ('employed', (21, 22))]","[('channel size', (7, 9)), ('concatenated feature map', (11, 14)), ('11 convolution layer', (16, 19))]","[['channel size', 'of', 'concatenated feature map']]",[],"[['Hyperparameters', 'in order', 'channel size']]",[],face_detection,18,241
2144,hyperparameters,Therefore the channel size of final feature map is at the same size as the original fifth convolution layer in Faster R - CNN .,"[('of', (4, 5)), ('at the same', (9, 12)), ('as', (13, 14)), ('in', (19, 20))]","[('channel size', (2, 4)), ('final feature map', (5, 8)), ('original fifth convolution layer', (15, 19)), ('Faster R - CNN', (20, 24))]","[['channel size', 'of', 'final feature map'], ['original fifth convolution layer', 'in', 'Faster R - CNN']]",[],[],"[['Hyperparameters', 'has', 'channel size']]",face_detection,18,242
2145,results,"Using this database , our proposed approach robustly outperforms strong baseline methods , including Two - stage CNN , Multi-scale Cascade CNN , Faceness and Aggregate Channel Features ( ACF ) , by a large margin .","[('including', (13, 14)), ('by', (32, 33))]","[('robustly outperforms', (7, 9)), ('strong baseline methods', (9, 12)), ('Two - stage CNN', (14, 18)), ('Multi-scale Cascade CNN', (19, 22)), ('Faceness', (23, 24)), ('Aggregate Channel Features ( ACF )', (25, 31)), ('large margin', (34, 36))]","[['strong baseline methods', 'including', 'Two - stage CNN'], ['strong baseline methods', 'including', 'Multi-scale Cascade CNN'], ['strong baseline methods', 'including', 'Faceness'], ['strong baseline methods', 'including', 'Aggregate Channel Features ( ACF )'], ['strong baseline methods', 'by', 'large margin'], ['Aggregate Channel Features ( ACF )', 'by', 'large margin']]","[['robustly outperforms', 'has', 'strong baseline methods']]",[],[],face_detection,18,246
2146,results,Experiments on WIDER FACE Dataset,"[('on', (1, 2))]",[],[],[],[],[],face_detection,18,249
2147,results,Our method outperforms those strong baselines by a large margin .,"[('by', (6, 7))]","[('Our method', (0, 2)), ('outperforms', (2, 3)), ('strong baselines', (4, 6)), ('large margin', (8, 10))]","[['strong baselines', 'by', 'large margin']]","[['Our method', 'has', 'outperforms'], ['outperforms', 'has', 'strong baselines']]",[],"[['Results', 'has', 'Our method']]",face_detection,18,267
2148,results,"It achieves the best average precision in all level faces , i.e. AP = 0.902 ( Easy ) , 0.874 ( Medium ) and 0.643 ( Hard ) , and outperforms the second best baseline by 26.0 % ( Easy ) , 37.4 % ( Medium ) and 60.8 % ( Hard ) .","[('achieves', (1, 2)), ('in', (6, 7)), ('i.e.', (11, 12)), ('by', (35, 36))]","[('best average precision', (3, 6)), ('all level faces', (7, 10)), ('AP', (12, 13)), ('0.902', (14, 15)), ('0.874', (19, 20)), ('0.643', (24, 25)), ('outperforms', (30, 31)), ('second best baseline', (32, 35)), ('26.0 % ( Easy )', (36, 41)), ('37.4 % ( Medium )', (42, 47)), ('60.8 % ( Hard )', (48, 53))]","[['best average precision', 'in', 'all level faces'], ['all level faces', 'i.e.', 'AP'], ['second best baseline', 'by', '26.0 % ( Easy )'], ['second best baseline', 'by', '60.8 % ( Hard )']]","[['AP', 'has', '0.902'], ['outperforms', 'has', 'second best baseline']]",[],[],face_detection,18,268
2149,results,"These results suggest that as the difficulty level goes up , CMS - RCNN can detect challenging faces better .","[('suggest', (2, 3)), ('goes', (8, 9)), ('can detect', (14, 16))]","[('difficulty level', (6, 8)), ('up', (9, 10)), ('CMS - RCNN', (11, 14)), ('challenging faces', (16, 18)), ('better', (18, 19))]","[['difficulty level', 'goes', 'up'], ['CMS - RCNN', 'can detect', 'challenging faces']]","[['difficulty level', 'has', 'up'], ['challenging faces', 'has', 'better']]","[['Results', 'suggest', 'difficulty level']]",[],face_detection,18,269
2150,results,With Context v.s. Without Context,[],[],[],[],[],[],face_detection,18,272
2151,results,"Additionally , the context model produces a longer PR curve , which means that contextual reasoning can help finding more faces .","[('produces', (5, 6))]","[('context model', (3, 5)), ('longer PR curve', (7, 10))]","[['context model', 'produces', 'longer PR curve']]","[['context model', 'has', 'longer PR curve']]",[],"[['Results', 'has', 'context model']]",face_detection,18,284
2152,results,Experiments on FDDB Face Database,"[('on', (1, 2))]",[],[],[],[],[],face_detection,18,295
2153,results,Our method achieves the best recall rate on this database .,"[('achieves', (2, 3))]","[('Our method', (0, 2)), ('best recall rate', (4, 7))]","[['Our method', 'achieves', 'best recall rate']]",[],[],"[['Results', 'has', 'Our method']]",face_detection,18,304
2154,results,The proposed CMS - RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods ( as shown ) .,"[('achieves', (15, 16)), ('comparing against', (21, 23))]","[('proposed', (1, 2)), ('CMS - RCNN approach', (2, 6)), ('outperforms', (6, 7)), ('most of the published face detection methods', (7, 14)), ('very high recall rate', (17, 21)), ('all other methods', (23, 26))]","[['proposed', 'achieves', 'very high recall rate'], ['CMS - RCNN approach', 'achieves', 'very high recall rate'], ['very high recall rate', 'comparing against', 'all other methods']]","[['proposed', 'has', 'CMS - RCNN approach'], ['CMS - RCNN approach', 'has', 'outperforms'], ['outperforms', 'has', 'most of the published face detection methods']]",[],"[['Results', 'has', 'proposed']]",face_detection,18,307
2155,baselines,"This paper has presented our proposed CMS - RCNN approach to robustly detect human facial regions from images collected under various challenging conditions , e.g. highly occlusions , low resolutions , facial expressions , illumination variations , etc .","[('from', (16, 17)), ('collected under', (18, 20))]","[('robustly detect', (11, 13)), ('human facial regions', (13, 16)), ('images', (17, 18))]","[['human facial regions', 'from', 'images']]","[['robustly detect', 'has', 'human facial regions']]",[],[],face_detection,18,311
2156,results,The experimental results show that our proposed approach outperforms strong baselines on the WIDER FACE and consistently achieves very competitive results against state - of - the - art methods on the FDDB .,"[('show', (3, 4)), ('on', (11, 12)), ('consistently achieves', (16, 18)), ('against', (21, 22)), ('on', (30, 31))]","[('our proposed approach', (5, 8)), ('outperforms', (8, 9)), ('strong baselines', (9, 11)), ('WIDER FACE', (13, 15)), ('very competitive results', (18, 21)), ('state - of - the - art methods', (22, 30)), ('FDDB', (32, 33))]","[['strong baselines', 'on', 'WIDER FACE'], ['state - of - the - art methods', 'on', 'FDDB'], ['our proposed approach', 'consistently achieves', 'very competitive results'], ['very competitive results', 'against', 'state - of - the - art methods'], ['state - of - the - art methods', 'on', 'FDDB']]","[['our proposed approach', 'has', 'outperforms'], ['outperforms', 'has', 'strong baselines']]","[['Results', 'show', 'our proposed approach']]",[],face_detection,18,313
2157,research-problem,Face Detection Using Improved Faster RCNN,[],"[('Face Detection', (0, 2))]",[],[],[],[],face_detection,19,2
2158,research-problem,"Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy set , medium set , hard set ) .","[('achieves', (2, 3)), ('over', (13, 14))]","[('Our method', (0, 2)), ('two 1th places', (3, 6)), ('one 2nd place', (7, 10)), ('WIDER FACE validation dataset', (14, 18))]","[['Our method', 'achieves', 'two 1th places'], ['Our method', 'achieves', 'one 2nd place']]",[],[],[],face_detection,19,7
2159,research-problem,"( 3 ) Our framework achieves two 1st places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy , medium , hard ) , one illustrative example of our results in the crowd case can be found in .","[('achieves', (5, 6)), ('in', (13, 14)), ('over', (16, 17))]","[('Our framework', (3, 5)), ('two 1st places', (6, 9)), ('one 2nd place', (10, 13)), ('three tasks', (14, 16)), ('WIDER FACE validation dataset ( easy , medium , hard )', (17, 28))]","[['Our framework', 'achieves', 'two 1st places'], ['Our framework', 'achieves', 'one 2nd place'], ['one 2nd place', 'in', 'three tasks'], ['three tasks', 'over', 'WIDER FACE validation dataset ( easy , medium , hard )']]",[],[],[],face_detection,19,25
2160,research-problem,"Face detection is one of the most fundamental and challenging problems in computer vision , and has been extensively studied for decades .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,19,26
2161,research-problem,Dense - Box employs a fully deep convolutional neural network to directly predict face confidence and corresponding bounding box .,"[('employs', (3, 4)), ('to directly predict', (10, 13))]","[('Dense - Box', (0, 3)), ('fully deep convolutional neural network', (5, 10)), ('face confidence', (13, 15)), ('corresponding bounding box', (16, 19))]","[['Dense - Box', 'employs', 'fully deep convolutional neural network'], ['fully deep convolutional neural network', 'to directly predict', 'face confidence'], ['fully deep convolutional neural network', 'to directly predict', 'corresponding bounding box']]",[],[],[],face_detection,19,35
2162,research-problem,"UnitBox introduces a novel intersection - over - union ( IoU ) loss to predict bounding box , which regresses the four bounds of a predicted box as a whole unit .","[('introduces', (1, 2)), ('to predict', (13, 15)), ('regresses', (19, 20)), ('of', (23, 24))]","[('UnitBox', (0, 1)), ('novel intersection - over - union ( IoU ) loss', (3, 13)), ('bounding box', (15, 17)), ('four bounds', (21, 23)), ('predicted', (25, 26))]","[['UnitBox', 'introduces', 'novel intersection - over - union ( IoU ) loss'], ['novel intersection - over - union ( IoU ) loss', 'to predict', 'bounding box'], ['bounding box', 'regresses', 'four bounds'], ['four bounds', 'of', 'predicted']]","[['UnitBox', 'name', 'novel intersection - over - union ( IoU ) loss']]",[],[],face_detection,19,36
2163,research-problem,S 3 FD presents a single shot scale - invariant face detector which achieves good result on WIDER FACE datasets .,"[('presents', (3, 4)), ('achieves', (13, 14)), ('on', (16, 17))]","[('S 3 FD', (0, 3)), ('single shot scale - invariant face detector', (5, 12)), ('good result', (14, 16)), ('WIDER FACE datasets', (17, 20))]","[['S 3 FD', 'presents', 'single shot scale - invariant face detector'], ['single shot scale - invariant face detector', 'achieves', 'good result'], ['good result', 'on', 'WIDER FACE datasets']]","[['S 3 FD', 'has', 'single shot scale - invariant face detector']]",[],[],face_detection,19,38
2164,experimental-setup,Single NVIDIA Tesla K80 is used for training and testing .,"[('used for', (5, 7))]","[('Single NVIDIA Tesla K80', (0, 4)), ('training and testing', (7, 10))]","[['Single NVIDIA Tesla K80', 'used for', 'training and testing']]",[],[],"[['Experimental setup', 'has', 'Single NVIDIA Tesla K80']]",face_detection,19,84
2165,experimental-setup,Mini batch size is set to 1 considering memory consumption .,"[('set to', (4, 6)), ('considering', (7, 8))]","[('Mini batch size', (0, 3)), ('1', (6, 7)), ('memory consumption', (8, 10))]","[['Mini batch size', 'set to', '1'], ['1', 'considering', 'memory consumption']]","[['Mini batch size', 'has', '1']]",[],"[['Experimental setup', 'has', 'Mini batch size']]",face_detection,19,85
2166,baselines,"A deformable layer is used to output a "" thin "" feature map with exploiting image context .","[('used to output', (4, 7)), ('with exploiting', (13, 15))]","[('deformable layer', (1, 3)), ('"" thin "" feature map', (8, 13)), ('image context', (15, 17))]","[['deformable layer', 'used to output', '"" thin "" feature map'], ['"" thin "" feature map', 'with exploiting', 'image context']]",[],[],[],face_detection,19,88
2167,experimental-setup,"Aspect ratios ( 1 , 1.5 , 2 ) and scales ( 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 ) are carefully designed to capture better locations of faces in the RPN stage , and the number of filters for the RPN layer is set as 512 .","[('carefully designed to capture', (31, 35)), ('of', (37, 38)), ('in', (39, 40)), ('for', (49, 50)), ('set as', (54, 56))]","[('Aspect ratios ( 1 , 1.5 , 2 )', (0, 9)), ('scales', (10, 11)), ('better locations', (35, 37)), ('faces', (38, 39)), ('RPN stage', (41, 43)), ('number of filters', (46, 49)), ('RPN layer', (51, 53)), ('512', (56, 57))]","[['scales', 'carefully designed to capture', 'better locations'], ['better locations', 'of', 'faces'], ['faces', 'in', 'RPN stage'], ['number of filters', 'for', 'RPN layer']]",[],[],"[['Experimental setup', 'has', 'Aspect ratios ( 1 , 1.5 , 2 )']]",face_detection,19,89
2168,experimental-setup,"By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .","[('of', (7, 8)), ('assigned as', (15, 17))]","[('batch size', (5, 7)), ('RPN and R - CNN', (8, 13)), ('256 and 128', (17, 20))]","[['batch size', 'of', 'RPN and R - CNN'], ['RPN and R - CNN', 'assigned as', '256 and 128']]",[],[],"[['Experimental setup', 'has', 'batch size']]",face_detection,19,94
2169,experimental-setup,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .","[('set to', (5, 7)), ('decrease to', (12, 14)), ('after', (17, 18))]","[('initial learning rate', (1, 4)), ('1e - 3', (7, 10)), ('1e - 4', (14, 17)), ('20w iterations', (18, 20))]","[['initial learning rate', 'set to', '1e - 3'], ['initial learning rate', 'decrease to', '1e - 4'], ['1e - 4', 'after', '20w iterations']]","[['initial learning rate', 'has', '1e - 3']]",[],"[['Experimental setup', 'has', 'initial learning rate']]",face_detection,19,95
2170,experimental-setup,Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,"[('set to', (6, 8))]","[('Weight decay', (0, 2)), ('momentum', (4, 5)), ('1e - 4 and 0.9', (8, 13))]","[['momentum', 'set to', '1e - 4 and 0.9']]",[],[],"[['Experimental setup', 'has', 'Weight decay']]",face_detection,19,96
2171,experimental-setup,"In testing stage , multi-scale testing strategy is adapted to be robust to different scale faces .","[('In', (0, 1)), ('adapted to be', (8, 11))]","[('testing stage', (1, 3)), ('multi-scale testing strategy', (4, 7)), ('robust', (11, 12)), ('different scale faces', (13, 16))]","[['multi-scale testing strategy', 'adapted to be', 'robust']]","[['testing stage', 'has', 'multi-scale testing strategy']]","[['Experimental setup', 'In', 'testing stage']]",[],face_detection,19,97
2172,results,"We also find top - ranked 6000 proposals are directly selected without NMS during testing can boost 0.1 % , 0.3 % and 0.6 % on easy set , medium set and hard set respectively .","[('find', (2, 3)), ('directly selected without', (9, 12)), ('during', (13, 14)), ('can', (15, 16)), ('on', (25, 26))]","[('top - ranked 6000 proposals', (3, 8)), ('NMS', (12, 13)), ('testing', (14, 15)), ('boost', (16, 17)), ('0.1 % , 0.3 % and 0.6 %', (17, 25)), ('easy set , medium set and hard set', (26, 34))]","[['top - ranked 6000 proposals', 'directly selected without', 'NMS'], ['top - ranked 6000 proposals', 'can', 'boost'], ['testing', 'can', 'boost'], ['0.1 % , 0.3 % and 0.6 %', 'on', 'easy set , medium set and hard set']]","[['testing', 'has', 'boost'], ['boost', 'has', '0.1 % , 0.3 % and 0.6 %']]",[],[],face_detection,19,100
2173,results,"Compared with the recently published top approaches , FDNet1.0 wins two 1st places ( easy set = 95.9 % , medium set = 94.5 % ) and one 2nd place ( hard set = 87.9 % ) on the validation set , as illustrated in .","[('Compared with', (0, 2)), ('wins', (9, 10)), ('on', (37, 38))]","[('FDNet1.0', (8, 9)), ('two 1st places', (10, 13)), ('easy set', (14, 16)), ('one 2nd place', (27, 30)), ('validation set', (39, 41))]","[['FDNet1.0', 'wins', 'two 1st places'], ['FDNet1.0', 'wins', 'one 2nd place'], ['one 2nd place', 'on', 'validation set']]","[['two 1st places', 'has', 'easy set']]","[['Results', 'Compared with', 'FDNet1.0']]",[],face_detection,19,103
2174,research-problem,Selective Refinement Network for High Performance Face Detection,[],[],[],[],[],[],face_detection,2,2
2175,research-problem,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .",[],"[('High performance face detection', (0, 4))]",[],[],[],[],face_detection,2,4
2176,research-problem,"Face detection is a long - standing problem in computer vision with extensive applications including face alignment , face analysis , face recognition , etc .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,2,12
2177,research-problem,To further improve the performance of face detection has become a challenging issue .,"[('of', (5, 6))]","[('performance', (4, 5)), ('face detection', (6, 8))]","[['performance', 'of', 'face detection']]",[],[],[],face_detection,2,15
2178,approach,R - CNN - like detectors ) address the class imbalance by a two - stage cascade and sampling heuristics .,"[('address', (7, 8)), ('by', (11, 12)), ('sampling', (18, 19))]","[('R - CNN - like detectors', (0, 6)), ('class imbalance', (9, 11)), ('two - stage cascade', (13, 17))]","[['R - CNN - like detectors', 'address', 'class imbalance'], ['class imbalance', 'by', 'two - stage cascade']]",[],[],"[['Approach', 'has', 'R - CNN - like detectors']]",face_detection,2,27
2179,approach,"As for single - shot detectors , RetinaNet proposes the focal loss to focus training on a sparse set of hard examples and down - weight the loss assigned to well - classified examples .","[('proposes', (8, 9)), ('to', (12, 13)), ('on', (15, 16)), ('of', (19, 20)), ('assigned to', (28, 30))]","[('single - shot detectors', (2, 6)), ('RetinaNet', (7, 8)), ('focal loss', (10, 12)), ('focus', (13, 14)), ('sparse set', (17, 19)), ('hard examples', (20, 22)), ('down - weight the loss', (23, 28)), ('well - classified examples', (30, 34))]","[['RetinaNet', 'proposes', 'focal loss'], ['focal loss', 'to', 'focus'], ['sparse set', 'of', 'hard examples'], ['down - weight the loss', 'assigned to', 'well - classified examples']]","[['single - shot detectors', 'has', 'RetinaNet'], ['RetinaNet', 'has', 'focal loss'], ['focal loss', 'has', 'focus']]",[],[],face_detection,2,28
2180,approach,"As shown in ( d ) , as the IoU threshold increases , the AP drops dramatically , indicating that the accuracy of the bounding box location needs to be improved .","[('as', (7, 8))]","[('IoU threshold', (9, 11)), ('increases', (11, 12)), ('AP', (14, 15)), ('drops', (15, 16)), ('dramatically', (16, 17))]",[],"[['IoU threshold', 'has', 'increases'], ['increases', 'has', 'AP'], ['AP', 'has', 'drops'], ['drops', 'has', 'dramatically']]",[],[],face_detection,2,38
2181,approach,Cascade R - CNN addresses this issue by cascading R - CNN with different IoU thresholds .,"[('by', (7, 8)), ('with', (12, 13))]","[('Cascade', (0, 1)), ('cascading', (8, 9)), ('R - CNN', (9, 12)), ('different IoU thresholds', (13, 16))]","[['cascading', 'with', 'different IoU thresholds'], ['R - CNN', 'with', 'different IoU thresholds']]","[['cascading', 'has', 'R - CNN']]",[],"[['Approach', 'has', 'Cascade']]",face_detection,2,40
2182,approach,RefineDet ) applies two - step regression to single - shot detector .,"[('applies', (2, 3)), ('to', (7, 8))]","[('RefineDet', (0, 1)), ('two - step regression', (3, 7)), ('single - shot detector', (8, 12))]","[['RefineDet', 'applies', 'two - step regression'], ['two - step regression', 'to', 'single - shot detector']]",[],[],"[['Approach', 'has', 'RefineDet']]",face_detection,2,41
2183,approach,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .","[('investigate', (5, 6)), ('of', (8, 9)), ('on', (15, 16)), ('propose', (22, 23)), ('named', (29, 30)), ('selectively applies', (38, 40)), ('to', (46, 47))]","[('two - step classification and regression', (9, 15)), ('different levels', (16, 18)), ('novel', (24, 25)), ('Selective Refinement Network ( SRN )', (30, 36)), ('two - step classification and regression', (40, 46)), ('specific levels of detection layers', (47, 52))]","[['two - step classification and regression', 'on', 'different levels'], ['Selective Refinement Network ( SRN )', 'selectively applies', 'two - step classification and regression'], ['two - step classification and regression', 'to', 'specific levels of detection layers']]",[],"[['Approach', 'investigate', 'two - step classification and regression']]",[],face_detection,2,43
2184,experiments,"As shown in , RetinaNet with STC improves the recall efficiency to a certain extent .","[('improves', (7, 8)), ('to', (11, 12))]","[('RetinaNet with STC', (4, 7)), ('recall efficiency', (9, 11)), ('certain extent', (13, 15))]","[['RetinaNet with STC', 'improves', 'recall efficiency'], ['recall efficiency', 'to', 'certain extent']]","[['recall efficiency', 'has', 'certain extent']]",[],[],face_detection,2,46
2185,approach,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .","[('design', (4, 5)), ('to provide', (12, 14)), ('to better capture', (18, 21))]","[('Receptive Field Enhancement ( RFE )', (6, 12)), ('more diverse receptive fields', (14, 18)), ('extreme - pose faces', (22, 26))]","[['Receptive Field Enhancement ( RFE )', 'to provide', 'more diverse receptive fields'], ['more diverse receptive fields', 'to better capture', 'extreme - pose faces']]",[],"[['Approach', 'design', 'Receptive Field Enhancement ( RFE )']]",[],face_detection,2,48
2186,approach,We present a STC module to filter out most simple negative samples from low level layers to reduce the classification search space .,"[('present', (1, 2)), ('to filter out', (5, 8)), ('from', (12, 13)), ('to reduce', (16, 18))]","[('STC module', (3, 5)), ('most simple negative samples', (8, 12)), ('low level layers', (13, 16)), ('classification search space', (19, 22))]","[['STC module', 'to filter out', 'most simple negative samples'], ['most simple negative samples', 'from', 'low level layers'], ['most simple negative samples', 'to reduce', 'classification search space'], ['low level layers', 'to reduce', 'classification search space']]",[],"[['Approach', 'present', 'STC module']]",[],face_detection,2,52
2187,experimental-setup,"It consists of 393 , 703 annotated face bounding boxes in 32 , 203 images with variations in pose , scale , facial expression , occlusion , and lighting condition .","[('consists of', (1, 3)), ('in', (10, 11)), ('with', (15, 16)), ('in', (17, 18))]","[('393 , 703 annotated face bounding boxes', (3, 10)), ('32', (11, 12)), ('203 images', (13, 15)), ('variations', (16, 17)), ('pose', (18, 19)), ('scale', (20, 21)), ('facial expression', (22, 24)), ('occlusion', (25, 26)), ('lighting condition', (28, 30))]","[['393 , 703 annotated face bounding boxes', 'in', '32'], ['variations', 'in', 'pose'], ['variations', 'in', 'lighting condition'], ['203 images', 'with', 'variations'], ['variations', 'in', 'pose'], ['variations', 'in', 'scale'], ['variations', 'in', 'facial expression'], ['variations', 'in', 'occlusion'], ['variations', 'in', 'lighting condition']]",[],"[['Experimental setup', 'consists of', '393 , 703 annotated face bounding boxes']]",[],face_detection,2,159
2188,experimental-setup,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .","[('initialized by', (4, 6)), ('in', (16, 17)), ('initialized by', (23, 25))]","[('backbone network', (1, 3)), ('pretrained ResNet - 50 model', (7, 12)), ('all the parameters', (13, 16)), ('newly added convolution layers', (18, 22)), ('"" xavier "" method', (26, 30))]","[['backbone network', 'initialized by', 'pretrained ResNet - 50 model'], ['all the parameters', 'in', 'newly added convolution layers'], ['newly added convolution layers', 'initialized by', '"" xavier "" method']]",[],[],"[['Experimental setup', 'has', 'backbone network']]",face_detection,2,176
2189,experimental-setup,"We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .","[('fine - tune', (1, 4)), ('using', (7, 8)), ('with', (9, 10))]","[('SRN model', (5, 7)), ('SGD', (8, 9)), ('0.9 momentum', (10, 12)), ('0.0001 weight decay', (13, 16)), ('batch size', (18, 20)), ('32', (20, 21))]","[['SRN model', 'using', 'SGD'], ['SRN model', 'using', 'batch size'], ['SGD', 'with', '0.9 momentum'], ['SGD', 'with', 'batch size']]","[['batch size', 'has', '32']]","[['Experimental setup', 'fine - tune', 'SRN model']]",[],face_detection,2,177
2190,experimental-setup,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .","[('set', (1, 2)), ('to', (5, 6)), ('for', (8, 9)), ('decay it to', (15, 18)), ('for', (25, 26))]","[('learning rate', (3, 5)), ('10 ?2', (6, 8)), ('first 100 epochs', (10, 13)), ('10 ? 3 and 10 ? 4', (18, 25)), ('20 and 10 epochs', (27, 31))]","[['learning rate', 'to', '10 ?2'], ['10 ?2', 'for', 'first 100 epochs'], ['10 ? 3 and 10 ? 4', 'for', '20 and 10 epochs'], ['10 ? 3 and 10 ? 4', 'for', '20 and 10 epochs']]","[['learning rate', 'has', '10 ?2']]","[['Experimental setup', 'set', 'learning rate']]",[],face_detection,2,178
2191,experimental-setup,We implement SRN using the Py - Torch library .,"[('implement', (1, 2)), ('using', (3, 4))]","[('SRN', (2, 3)), ('Py - Torch library', (5, 9))]","[['SRN', 'using', 'Py - Torch library']]",[],"[['Experimental setup', 'implement', 'SRN']]",[],face_detection,2,179
2192,experimental-setup,"In the inference phase , the STC first filters the regularly tiled anchors on the selected pyramid levels with the negative confidence scores larger than the threshold ? = 0.99 , and then STR adjusts the locations and sizes of selected anchors .","[('In', (0, 1)), ('first filters', (7, 9)), ('on', (13, 14)), ('with', (18, 19)), ('larger than', (23, 25)), ('adjusts', (34, 35)), ('of', (39, 40))]","[('inference phase', (2, 4)), ('STC', (6, 7)), ('regularly tiled anchors', (10, 13)), ('selected pyramid levels', (15, 18)), ('negative confidence scores', (20, 23)), ('threshold ? = 0.99', (26, 30)), ('STR', (33, 34)), ('locations and sizes', (36, 39)), ('selected anchors', (40, 42))]","[['STC', 'first filters', 'regularly tiled anchors'], ['regularly tiled anchors', 'on', 'selected pyramid levels'], ['selected pyramid levels', 'with', 'negative confidence scores'], ['negative confidence scores', 'larger than', 'threshold ? = 0.99'], ['STR', 'adjusts', 'locations and sizes'], ['locations and sizes', 'of', 'selected anchors']]","[['inference phase', 'has', 'STC']]","[['Experimental setup', 'In', 'inference phase']]",[],face_detection,2,181
2193,experiments,"Finally , we apply the non-maximum suppression ( NMS ) with jaccard overlap of 0.5 to generate the top 750 high confident detections per image as the final results .","[('apply', (3, 4)), ('with', (10, 11)), ('of', (13, 14)), ('to generate', (15, 17))]","[('non-maximum suppression ( NMS )', (5, 10)), ('jaccard overlap', (11, 13)), ('0.5', (14, 15)), ('top 750 high confident detections per image', (18, 25))]","[['non-maximum suppression ( NMS )', 'with', 'jaccard overlap'], ['jaccard overlap', 'of', '0.5'], ['0.5', 'to generate', 'top 750 high confident detections per image']]","[['jaccard overlap', 'has', '0.5']]",[],[],face_detection,2,183
2194,ablation-analysis,"Firstly , we use the ordinary prediction head in ) instead of the proposed RFE .","[('use', (3, 4)), ('instead of', (10, 12))]","[('ordinary prediction head', (5, 8)), ('proposed RFE', (13, 15))]","[['ordinary prediction head', 'instead of', 'proposed RFE']]",[],"[['Ablation analysis', 'use', 'ordinary prediction head']]",[],face_detection,2,193
2195,ablation-analysis,"Experimental results of applying two - step classification to each pyramid level are shown in , indicating that applying two - step classification to the low pyramid levels improves the performance , especially on tiny faces .","[('applying', (3, 4)), ('to', (8, 9)), ('applying', (18, 19)), ('to', (23, 24)), ('improves', (28, 29))]","[('two - step classification', (4, 8)), ('each pyramid level', (9, 12)), ('two - step classification', (19, 23)), ('low pyramid levels', (25, 28)), ('performance', (30, 31))]","[['two - step classification', 'to', 'each pyramid level'], ['two - step classification', 'to', 'low pyramid levels'], ['each pyramid level', 'applying', 'two - step classification'], ['two - step classification', 'to', 'low pyramid levels'], ['two - step classification', 'improves', 'performance'], ['low pyramid levels', 'improves', 'performance']]",[],"[['Ablation analysis', 'applying', 'two - step classification']]",[],face_detection,2,197
2196,ablation-analysis,"Therefore , the STC module selectively applies the two - step classification on the low pyramid levels ( i.e. , P2 , P3 , and P4 ) , since these levels are associated with lots of small anchors , which are the main source of false positives .","[('selectively applies', (5, 7)), ('on', (12, 13))]","[('STC module', (3, 5)), ('two - step classification', (8, 12)), ('low pyramid levels ( i.e. , P2 , P3 , and P4 )', (14, 27))]","[['STC module', 'selectively applies', 'two - step classification'], ['two - step classification', 'on', 'low pyramid levels ( i.e. , P2 , P3 , and P4 )']]",[],[],"[['Ablation analysis', 'has', 'STC module']]",face_detection,2,198
2197,ablation-analysis,"As listed in , our STC effectively reduces the false positives across different recall rates , demonstrating the effectiveness of the STC module . Selective Two - step Regression .","[('effectively reduces', (6, 8)), ('across', (11, 12))]","[('our STC', (4, 6)), ('false positives', (9, 11)), ('different recall rates', (12, 15)), ('Selective Two - step Regression', (24, 29))]","[['our STC', 'effectively reduces', 'false positives'], ['false positives', 'across', 'different recall rates']]",[],[],"[['Ablation analysis', 'has', 'our STC']]",face_detection,2,200
2198,results,"As shown in , it produces much better results than the baseline , with 0.8 % , 0.9 % and 0.8 % AP improvements on the Easy , Medium , and Hard subsets .","[('produces', (5, 6)), ('than', (9, 10)), ('with', (13, 14)), ('on', (24, 25))]","[('much better results', (6, 9)), ('baseline', (11, 12)), ('0.8 % , 0.9 % and 0.8 % AP improvements', (14, 24)), ('Easy , Medium , and Hard subsets', (26, 33))]","[['much better results', 'than', 'baseline'], ['much better results', 'with', '0.8 % , 0.9 % and 0.8 % AP improvements'], ['baseline', 'with', '0.8 % , 0.9 % and 0.8 % AP improvements'], ['0.8 % , 0.9 % and 0.8 % AP improvements', 'on', 'Easy , Medium , and Hard subsets']]",[],[],[],face_detection,2,202
2199,results,Experimental results of applying two - step regression to each pyramid level ( see ) confirm our previous analysis .,"[('to', (8, 9))]","[('two - step regression', (4, 8)), ('each pyramid level', (9, 12))]","[['two - step regression', 'to', 'each pyramid level']]",[],[],[],face_detection,2,203
2200,ablation-analysis,"As shown in , the STR module produces consistently accurate detection results than the baseline method .","[('produces', (7, 8)), ('than', (12, 13))]","[('STR module', (5, 7)), ('consistently accurate detection results', (8, 12)), ('baseline method', (14, 16))]","[['STR module', 'produces', 'consistently accurate detection results'], ['consistently accurate detection results', 'than', 'baseline method']]","[['STR module', 'has', 'consistently accurate detection results'], ['consistently accurate detection results', 'has', 'baseline method']]",[],"[['Ablation analysis', 'has', 'STR module']]",face_detection,2,205
2201,ablation-analysis,"The gap between the AP across all three subsets increases as the IoU threshold increases , which indicate that the STR module is important to produce more accurate detections .","[('between', (2, 3)), ('across', (5, 6)), ('as', (10, 11))]","[('gap', (1, 2)), ('AP', (4, 5)), ('all three subsets', (6, 9)), ('increases', (9, 10)), ('IoU threshold', (12, 14)), ('increases', (14, 15))]","[['gap', 'between', 'AP'], ['AP', 'across', 'all three subsets'], ['gap', 'as', 'IoU threshold'], ['increases', 'as', 'IoU threshold']]","[['gap', 'has', 'AP'], ['all three subsets', 'has', 'increases'], ['IoU threshold', 'has', 'increases']]",[],"[['Ablation analysis', 'has', 'gap']]",face_detection,2,206
2202,results,"In addition , coupled with the STC module , the performance is further improved to 96.1 % , 95.0 % and 90.1 % on the Easy , Medium and Hard subsets , respectively .","[('coupled with', (3, 5)), ('to', (14, 15)), ('on', (23, 24))]","[('STC module', (6, 8)), ('performance', (10, 11)), ('further improved', (12, 14)), ('96.1 % , 95.0 % and 90.1 %', (15, 23)), ('Easy , Medium and Hard subsets', (25, 31))]","[['further improved', 'to', '96.1 % , 95.0 % and 90.1 %'], ['96.1 % , 95.0 % and 90.1 %', 'on', 'Easy , Medium and Hard subsets']]","[['STC module', 'has', 'performance'], ['performance', 'has', 'further improved']]","[['Results', 'coupled with', 'STC module']]",[],face_detection,2,207
2203,ablation-analysis,The RFE is used to diversify the receptive fields of detection layers in order to capture faces with extreme poses .,"[('of', (9, 10)), ('with', (17, 18))]","[('RFE', (1, 2)), ('diversify', (5, 6)), ('receptive fields', (7, 9)), ('detection layers', (10, 12)), ('faces', (16, 17)), ('extreme poses', (18, 20))]","[['receptive fields', 'of', 'detection layers'], ['faces', 'with', 'extreme poses']]","[['RFE', 'has', 'diversify'], ['diversify', 'has', 'receptive fields']]",[],"[['Ablation analysis', 'has', 'RFE']]",face_detection,2,209
2204,ablation-analysis,"Comparing the detection results between fourth and fifth columns in , we notice that RFE consistently improves the AP scores in different subsets , i.e. , 0.3 % , 0.3 % , and 0.1 % APs on the Easy , Medium , and Hard categories .","[('Comparing', (0, 1)), ('notice', (12, 13)), ('consistently improves', (15, 17)), ('in', (20, 21)), ('i.e.', (24, 25)), ('on', (36, 37))]","[('detection results', (2, 4)), ('fourth', (5, 6)), ('RFE', (14, 15)), ('AP scores', (18, 20)), ('different subsets', (21, 23)), ('0.3 % , 0.3 % , and 0.1 % APs', (26, 36)), ('Easy , Medium , and Hard categories', (38, 45))]","[['RFE', 'consistently improves', 'AP scores'], ['AP scores', 'in', 'different subsets'], ['AP scores', 'i.e.', '0.3 % , 0.3 % , and 0.1 % APs'], ['different subsets', 'i.e.', '0.3 % , 0.3 % , and 0.1 % APs'], ['0.3 % , 0.3 % , and 0.1 % APs', 'on', 'Easy , Medium , and Hard categories']]","[['detection results', 'has', 'fourth']]","[['Ablation analysis', 'Comparing', 'detection results']]",[],face_detection,2,210
2205,results,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .","[('with', (15, 16))]","[('SRN', (4, 5)), ('outperforms', (5, 6)), ('top AP score ( 99.87 % )', (17, 24))]",[],"[['SRN', 'has', 'outperforms'], ['outperforms', 'has', 'top AP score ( 99.87 % )']]",[],[],face_detection,2,217
2206,results,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,"[('achieves', (1, 2)), ('by improving', (11, 13)), ('compared to', (17, 19))]","[('SRN', (0, 1)), ('state - of - the - art results', (3, 11)), ('4.99 % AP score', (13, 17)), ('second best method STN', (20, 24))]","[['SRN', 'achieves', 'state - of - the - art results'], ['state - of - the - art results', 'by improving', '4.99 % AP score'], ['4.99 % AP score', 'compared to', 'second best method STN']]","[['SRN', 'has', 'state - of - the - art results']]",[],"[['Results', 'has', 'SRN']]",face_detection,2,221
2207,results,"As shown in ( c ) , our SRN sets a new state - of - the - art performance , i.e. , 98.8 % true positive rate when the number of false positives is equal to 1000 .","[('sets', (9, 10)), ('i.e.', (21, 22)), ('when', (28, 29)), ('equal to', (35, 37))]","[('our SRN', (7, 9)), ('new state - of - the - art performance', (11, 20)), ('98.8 % true positive rate', (23, 28)), ('number of false positives', (30, 34)), ('1000', (37, 38))]","[['our SRN', 'sets', 'new state - of - the - art performance'], ['new state - of - the - art performance', 'i.e.', '98.8 % true positive rate'], ['98.8 % true positive rate', 'when', 'number of false positives'], ['number of false positives', 'equal to', '1000']]","[['our SRN', 'has', 'new state - of - the - art performance']]",[],"[['Results', 'has', 'our SRN']]",face_detection,2,225
2208,results,"As shown in , we find that SRN performs favourably against the state - of - the - art based on the average precision ( AP ) across the three subsets , especially on the Hard subset which contains a large amount of small faces .","[('find', (5, 6)), ('performs', (8, 9)), ('against', (10, 11)), ('based on', (19, 21)), ('across', (27, 28)), ('especially on', (32, 34)), ('contains', (38, 39))]","[('SRN', (7, 8)), ('favourably', (9, 10)), ('state - of - the - art', (12, 19)), ('average precision ( AP )', (22, 27)), ('three subsets', (29, 31)), ('Hard subset', (35, 37)), ('large amount', (40, 42))]","[['SRN', 'performs', 'favourably'], ['favourably', 'against', 'state - of - the - art'], ['state - of - the - art', 'based on', 'average precision ( AP )'], ['average precision ( AP )', 'across', 'three subsets'], ['average precision ( AP )', 'especially on', 'Hard subset'], ['Hard subset', 'contains', 'large amount']]",[],"[['Results', 'find', 'SRN']]",[],face_detection,2,230
2209,results,"Specifically , it produces the best AP scores in all subsets of both validation and testing sets , i.e. , 96.4 % ( Easy ) , 95.3 % ( Medium ) and 90.2 % ( Hard ) for validation set , and 95.9 % ( Easy ) , 94.9 % ( Medium ) and 89.7 % ( Hard ) for testing set , surpassing all approaches , which demonstrates the superiority of the proposed detector .","[('produces', (3, 4)), ('in', (8, 9)), ('of', (11, 12)), ('i.e.', (18, 19)), ('for', (37, 38)), ('for', (59, 60))]","[('best AP scores', (5, 8)), ('all subsets', (9, 11)), ('validation and testing sets', (13, 17)), ('96.4 % ( Easy )', (20, 25)), ('95.3 % ( Medium )', (26, 31)), ('90.2 % ( Hard )', (32, 37)), ('validation set', (38, 40)), ('95.9 % ( Easy )', (42, 47)), ('94.9 % ( Medium )', (48, 53)), ('89.7 % ( Hard )', (54, 59)), ('testing set', (60, 62))]","[['best AP scores', 'in', 'all subsets'], ['all subsets', 'of', 'validation and testing sets'], ['all subsets', 'of', '89.7 % ( Hard )'], ['validation and testing sets', 'i.e.', '96.4 % ( Easy )'], ['validation and testing sets', 'i.e.', '95.3 % ( Medium )'], ['validation and testing sets', 'i.e.', '90.2 % ( Hard )'], ['validation and testing sets', 'i.e.', '94.9 % ( Medium )'], ['90.2 % ( Hard )', 'for', 'validation set'], ['89.7 % ( Hard )', 'for', 'testing set']]",[],[],[],face_detection,2,231
2210,research-problem,Aggregate Channel Features for Multi-view Face Detection,[],[],[],[],[],[],face_detection,20,2
2211,model,The classifier learning process follows the VJ framework pipeline .,"[('follows', (4, 5))]","[('classifier learning process', (1, 4)), ('VJ framework pipeline', (6, 9))]","[['classifier learning process', 'follows', 'VJ framework pipeline']]","[['classifier learning process', 'has', 'VJ framework pipeline']]",[],"[['Model', 'has', 'classifier learning process']]",face_detection,20,24
2212,model,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .","[('adopt', (5, 6)), ('of', (8, 9)), ('called', (11, 12)), ('extracted directly as', (18, 21)), ('on', (23, 24))]","[('variant', (7, 8)), ('channel features', (9, 11)), ('aggregate channel features', (12, 15)), ('pixel values', (21, 23)), ('subsampled channels', (24, 26))]","[['variant', 'of', 'channel features'], ['channel features', 'called', 'aggregate channel features'], ['aggregate channel features', 'extracted directly as', 'pixel values'], ['pixel values', 'on', 'subsampled channels']]",[],"[['Model', 'adopt', 'variant']]",[],face_detection,20,25
2213,model,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .","[('proven to', (73, 75))]","[('multi-scaling', (11, 12)), ('feature representation', (13, 15)), ('representation capacity', (18, 20)), ('performance', (39, 40))]",[],"[['multi-scaling', 'has', 'feature representation']]",[],[],face_detection,20,30
2214,hyperparameters,"Summary : Based on observations above , we choose 2048 as the number of weak classifiers contained in the soft cascade .","[('choose', (8, 9)), ('as', (10, 11)), ('contained in', (16, 18))]","[('2048', (9, 10)), ('number of weak classifiers', (12, 16)), ('soft cascade', (19, 21))]","[['2048', 'as', 'number of weak classifiers'], ['number of weak classifiers', 'contained in', 'soft cascade']]",[],"[['Hyperparameters', 'choose', '2048']]",[],face_detection,20,164
2215,results,Evaluation on benchmark face database,[],[],[],[],[],[],face_detection,20,242
2216,results,"As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .","[('in', (4, 5)), ('achieves', (10, 11)), ('of', (14, 15)), ('by', (22, 23))]","[('AFW', (5, 6)), ('our multi-scale detector', (7, 10)), ('ap value', (12, 14)), ('96.8 %', (15, 17)), ('outperforming', (18, 19)), ('other academic methods', (19, 22)), ('large margin', (24, 26))]","[['our multi-scale detector', 'achieves', 'ap value'], ['ap value', 'of', '96.8 %'], ['outperforming', 'by', 'large margin'], ['other academic methods', 'by', 'large margin']]","[['AFW', 'has', 'our multi-scale detector'], ['our multi-scale detector', 'has', 'ap value'], ['outperforming', 'has', 'other academic methods']]",[],[],face_detection,20,243
2217,results,"In discrete score where evaluation metric is the same as in AFW , our detector achieves 83.7 % , which is a little better than Yan et al ..","[('In', (0, 1)), ('where', (3, 4)), ('achieves', (15, 16))]","[('discrete score', (1, 3)), ('our detector', (13, 15)), ('83.7 %', (16, 18))]","[['our detector', 'achieves', '83.7 %']]","[['discrete score', 'has', 'our detector']]",[],[],face_detection,20,248
2218,results,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .","[('using', (1, 2)), ('which takes', (4, 6)), ('as', (9, 10)), ('gets', (15, 16)), ('at', (21, 22)), ('for', (24, 25)), ('surpassing', (28, 29)), ('by', (35, 36))]","[('continuous score', (2, 4)), ('overlap ratio', (7, 9)), ('score', (11, 12)), ('our method', (13, 15)), ('61.9 % true positive rate', (16, 21)), ('1 FPPI', (22, 24)), ('multiscale version', (25, 27)), ('other methods', (29, 31)), ('rectangular detections', (33, 35)), ('notable margin', (37, 39))]","[['continuous score', 'which takes', 'overlap ratio'], ['overlap ratio', 'as', 'score'], ['continuous score', 'gets', '61.9 % true positive rate'], ['our method', 'gets', '61.9 % true positive rate'], ['61.9 % true positive rate', 'at', '1 FPPI'], ['1 FPPI', 'for', 'multiscale version'], ['61.9 % true positive rate', 'surpassing', 'other methods'], ['rectangular detections', 'by', 'notable margin']]","[['continuous score', 'has', 'overlap ratio']]","[['Results', 'using', 'continuous score']]",[],face_detection,20,250
2219,research-problem,Supervised Transformer Network for Efficient Face Detection,[],[],[],[],[],[],face_detection,21,2
2220,model,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .","[('simultaneously proposes', (14, 16)), ('along with', (19, 21))]","[('multi-task Region Proposal Network ( RPN )', (5, 12)), ('candidate face regions', (16, 19)), ('associated facial landmarks', (21, 24))]","[['multi-task Region Proposal Network ( RPN )', 'simultaneously proposes', 'candidate face regions'], ['candidate face regions', 'along with', 'associated facial landmarks']]",[],[],"[['Model', 'has', 'multi-task Region Proposal Network ( RPN )']]",face_detection,21,34
2221,model,It first uses a conventional boosting cascade to obtain a set of face candidate areas .,"[('uses', (2, 3)), ('to obtain', (7, 9))]","[('conventional boosting cascade', (4, 7)), ('set of face candidate areas', (10, 15))]","[['conventional boosting cascade', 'to obtain', 'set of face candidate areas']]",[],"[['Model', 'uses', 'conventional boosting cascade']]",[],face_detection,21,52
2222,model,"Instead , we use the ROI masks , so that different samples can share the feature in the overlapping area .","[('use', (3, 4)), ('so', (8, 9)), ('in', (16, 17))]","[('ROI masks', (5, 7)), ('different', (10, 11)), ('feature', (15, 16)), ('overlapping area', (18, 20))]","[['ROI masks', 'so', 'different'], ['feature', 'in', 'overlapping area']]","[['ROI masks', 'has', 'different']]","[['Model', 'use', 'ROI masks']]",[],face_detection,21,160
2223,hyperparameters,We use a Real - Boost algorithm for the cascade classification learning .,"[('use', (1, 2)), ('for', (7, 8))]","[('Real - Boost algorithm', (3, 7)), ('cascade classification learning', (9, 12))]","[['Real - Boost algorithm', 'for', 'cascade classification learning']]",[],"[['Hyperparameters', 'use', 'Real - Boost algorithm']]",[],face_detection,21,177
2224,experiments,We use GoogleNet in both the RPN and RCNN networks .,"[('use', (1, 2)), ('in both', (3, 5))]","[('GoogleNet', (2, 3)), ('RPN and RCNN networks', (6, 10))]","[['GoogleNet', 'in both', 'RPN and RCNN networks']]",[],[],[],face_detection,21,224
2225,results,"As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .","[('bring about', (14, 16))]","[('multi-task RPN , Supervised Transformer , and feature combination', (4, 13)), ('1 % , 1 % , and 2 % recall improvement', (16, 27))]","[['multi-task RPN , Supervised Transformer , and feature combination', 'bring about', '1 % , 1 % , and 2 % recall improvement']]",[],[],"[['Results', 'has', 'multi-task RPN , Supervised Transformer , and feature combination']]",face_detection,21,255
2226,hyperparameters,"In the training phase , in order to increase the variation of training samples , we randomly select K positive / negative samples from each image for the RCNN network .","[('In', (0, 1)), ('in', (5, 6)), ('to increase', (7, 9)), ('of', (11, 12)), ('randomly select', (16, 18)), ('from', (23, 24)), ('for', (26, 27))]","[('training phase', (2, 4)), ('variation', (10, 11)), ('training samples', (12, 14)), ('K positive / negative samples', (18, 23)), ('each image', (24, 26)), ('RCNN network', (28, 30))]","[['training phase', 'to increase', 'variation'], ['variation', 'of', 'training samples'], ['training samples', 'randomly select', 'K positive / negative samples'], ['K positive / negative samples', 'from', 'each image'], ['each image', 'for', 'RCNN network']]","[['training phase', 'has', 'variation']]","[['Hyperparameters', 'In', 'training phase']]",[],face_detection,21,257
2227,experiments,We found that NMS tend to include too much noisy low confidence candidates .,"[('found', (1, 2)), ('tend to include', (4, 7))]","[('NMS', (3, 4)), ('too much noisy low confidence candidates', (7, 13))]","[['NMS', 'tend to include', 'too much noisy low confidence candidates']]",[],[],[],face_detection,21,262
2228,experiments,"We also compare the PR curves of using all candidates , NMS , and non-top K suppression .","[('compare', (2, 3)), ('of using', (6, 8))]","[('PR curves', (4, 6)), ('all candidates , NMS , and non-top K suppression', (8, 17))]","[['PR curves', 'of using', 'all candidates , NMS , and non-top K suppression']]",[],[],[],face_detection,21,263
2229,experiments,"Our non - top K suppression is very close to using all candidates , and achieved consistently better results than NMS under the same number of candidates .","[('achieved', (15, 16)), ('than', (19, 20)), ('under', (21, 22))]","[('Our non - top K suppression', (0, 6)), ('using', (10, 11)), ('all candidates', (11, 13)), ('consistently better results', (16, 19)), ('NMS', (20, 21)), ('same number of candidates', (23, 27))]","[['Our non - top K suppression', 'achieved', 'consistently better results'], ['consistently better results', 'than', 'NMS'], ['consistently better results', 'under', 'same number of candidates'], ['NMS', 'under', 'same number of candidates']]","[['Our non - top K suppression', 'has', 'using'], ['using', 'has', 'all candidates']]",[],[],face_detection,21,264
2230,tasks,We also compare with the standard network without ROI convolution .,"[('compare with', (2, 4))]","[('standard network without', (5, 8)), ('ROI convolution', (8, 10))]",[],"[['standard network without', 'has', 'ROI convolution']]",[],[],face_detection,21,273
2231,tasks,Non- top K ( K = 3 ) suppression is adopted in all settings to make RCNN network more efficiency .,"[('adopted in', (10, 12)), ('to make', (14, 16))]","[('Non- top K ( K = 3 ) suppression', (0, 9)), ('all settings', (12, 14)), ('RCNN network', (16, 18)), ('more efficiency', (18, 20))]","[['Non- top K ( K = 3 ) suppression', 'adopted in', 'all settings'], ['all settings', 'to make', 'RCNN network']]","[['RCNN network', 'has', 'more efficiency']]",[],"[['Tasks', 'has', 'Non- top K ( K = 3 ) suppression']]",face_detection,21,274
2232,tasks,The original DNN detector can run at 10 FPS on CPU fora VGA image .,"[('run at', (5, 7)), ('on', (9, 10))]","[('original', (1, 2)), ('DNN detector', (2, 4)), ('10 FPS', (7, 9)), ('CPU fora VGA image', (10, 14))]","[['DNN detector', 'run at', '10 FPS'], ['10 FPS', 'on', 'CPU fora VGA image']]","[['original', 'has', 'DNN detector']]",[],"[['Tasks', 'has', 'original']]",face_detection,21,278
2233,tasks,"BC D. Qualitative face detection results on ( a ) FDDB , ( b ) AFW , ( c ) PASCAL faces datasets .","[('on', (6, 7)), ('faces', (21, 22))]","[('Qualitative face detection results', (2, 6)), ('datasets', (22, 23))]",[],"[['Qualitative face detection results', 'has', 'datasets']]",[],[],face_detection,21,282
2234,results,"On the FDDB dataset , we compare with all public methods .","[('On', (0, 1))]","[('FDDB dataset', (2, 4))]",[],[],"[['Results', 'On', 'FDDB dataset']]",[],face_detection,21,285
2235,baselines,"On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .","[('On', (0, 1)), ('compare with', (9, 11)), ('e.g.', (19, 20))]","[('AFW and PASCAL faces datasets', (2, 7)), ('deformable part based methods', (14, 18)), ('structure model', (20, 22)), ('Tree Parts Model ( TSM )', (23, 29)), ('cascade - based methods', (33, 37))]","[['deformable part based methods', 'e.g.', 'structure model'], ['deformable part based methods', 'e.g.', 'Tree Parts Model ( TSM )']]","[['deformable part based methods', 'name', 'structure model']]","[['Baselines', 'On', 'AFW and PASCAL faces datasets']]",[],face_detection,21,287
2236,baselines,"We learn a global regression from 5 facial points to face rectangles to match the annotation for each dataset , and use toolbox from for the evaluation .","[('learn', (1, 2)), ('from', (5, 6)), ('to', (9, 10)), ('to match', (12, 14))]","[('global regression', (3, 5)), ('5 facial points', (6, 9)), ('face rectangles', (10, 12)), ('annotation', (15, 16))]","[['global regression', 'from', '5 facial points'], ['5 facial points', 'to', 'face rectangles'], ['face rectangles', 'to match', 'annotation']]",[],[],[],face_detection,21,289
2237,research-problem,A Fast and Accurate Unconstrained Face Detector,[],[],[],[],[],[],face_detection,3,2
2238,research-problem,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .","[('to address', (4, 6)), ('in', (7, 8)), ('such as', (12, 14))]","[('challenges', (6, 7)), ('unconstrained face detection', (8, 11)), ('arbitrary', (14, 15))]","[['challenges', 'in', 'unconstrained face detection'], ['challenges', 'such as', 'arbitrary'], ['unconstrained face detection', 'such as', 'arbitrary']]",[],"[['Research problem', 'to address', 'challenges']]",[],face_detection,3,4
2239,research-problem,The objective of face detection is to find and locate faces in an image .,[],"[('face detection', (3, 5))]",[],[],[],[],face_detection,3,13
2240,research-problem,It is the first step in automatic face recognition applications .,"[('in', (5, 6))]","[('first step', (3, 5)), ('automatic face recognition applications', (6, 10))]","[['first step', 'in', 'automatic face recognition applications']]",[],[],[],face_detection,3,14
2241,research-problem,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .","[('refer to', (5, 7)), ('as', (13, 14))]","[('face detection with arbitrary facial variations', (7, 13))]",[],[],"[['Research problem', 'refer to', 'face detection with arbitrary facial variations']]",[],face_detection,3,19
2242,model,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .","[('propose', (3, 4)), ('called', (11, 12))]","[('Normalized Pixel Difference ( NPD )', (13, 19))]",[],[],"[['Model', 'propose', 'Normalized Pixel Difference ( NPD )']]",[],face_detection,3,34
2243,model,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .","[('such as', (8, 10)), ('to reconstruct', (17, 19))]","[('NPD feature', (1, 3)), ('several desirable properties', (4, 7)), ('scale invariance', (10, 12)), ('boundedness', (13, 14)), ('ability', (16, 17)), ('original image', (20, 22))]","[['several desirable properties', 'such as', 'scale invariance'], ['several desirable properties', 'such as', 'boundedness'], ['several desirable properties', 'such as', 'ability'], ['ability', 'to reconstruct', 'original image']]","[['NPD feature', 'has', 'several desirable properties']]",[],"[['Model', 'has', 'NPD feature']]",face_detection,3,36
2244,model,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .","[('show', (2, 3)), ('obtained from', (8, 10)), ('easily scaled for', (22, 25))]","[('NPD features', (4, 6)), ('lookup table', (11, 13)), ('resulting face detection template', (16, 20)), ('multiscale face detection', (25, 28))]","[['NPD features', 'obtained from', 'lookup table'], ['resulting face detection template', 'easily scaled for', 'multiscale face detection']]",[],"[['Model', 'show', 'NPD features']]",[],face_detection,3,37
2245,model,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .","[('propose', (3, 4)), ('construct', (11, 12)), ('to handle', (19, 21))]","[('deep quadratic tree learning method', (5, 10)), ('single soft - cascade AdaBoost classifier', (13, 19)), ('complex face manifolds', (21, 24)), ('arbitrary pose', (25, 27)), ('occlusion conditions', (28, 30))]","[['deep quadratic tree learning method', 'construct', 'single soft - cascade AdaBoost classifier'], ['single soft - cascade AdaBoost classifier', 'to handle', 'complex face manifolds'], ['single soft - cascade AdaBoost classifier', 'to handle', 'arbitrary pose'], ['single soft - cascade AdaBoost classifier', 'to handle', 'occlusion conditions']]",[],"[['Model', 'propose', 'deep quadratic tree learning method']]",[],face_detection,3,38
2246,model,"While individual NPD features may have "" weak "" discriminative ability , our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree .","[('may have', (4, 6)), ('indicates', (14, 15)), ('can be', (21, 23)), ('combined to construct', (26, 29)), ('in', (32, 33))]","[('individual NPD', (1, 3)), ('subset of NPD features', (17, 21)), ('optimally learned', (23, 25)), ('more discriminative features', (29, 32)), ('deep quadratic tree', (34, 37))]","[['subset of NPD features', 'can be', 'optimally learned'], ['subset of NPD features', 'combined to construct', 'more discriminative features'], ['more discriminative features', 'in', 'deep quadratic tree']]","[['subset of NPD features', 'has', 'optimally learned']]",[],"[['Model', 'has', 'individual NPD']]",face_detection,3,39
2247,model,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .","[('of', (15, 16)), ('in', (25, 26)), ('can', (30, 31)), ('partitioned in', (32, 34))]","[('different types of faces', (4, 8)), ('different leaves', (13, 15)), ('tree classifier', (17, 19)), ('complex face manifold', (22, 25)), ('high dimensional space', (27, 30)), ('learning process', (35, 37))]","[['different leaves', 'of', 'tree classifier'], ['complex face manifold', 'in', 'high dimensional space'], ['high dimensional space', 'partitioned in', 'learning process']]",[],[],"[['Model', 'has', 'different types of faces']]",face_detection,3,40
2248,model,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .","[('is', (1, 2)), ('to tackle', (9, 11)), ('in', (14, 15)), ('without pre-labeling', (19, 21)), ('in', (23, 24))]","[('"" divide and conquer "" strategy', (3, 9)), ('unconstrained face detection', (11, 14)), ('single classifier', (16, 18)), ('views', (22, 23)), ('training set of face images', (25, 30))]","[['"" divide and conquer "" strategy', 'to tackle', 'unconstrained face detection'], ['unconstrained face detection', 'in', 'single classifier'], ['views', 'in', 'training set of face images'], ['unconstrained face detection', 'without pre-labeling', 'views'], ['single classifier', 'without pre-labeling', 'views'], ['views', 'in', 'training set of face images']]",[],"[['Model', 'is', '"" divide and conquer "" strategy']]",[],face_detection,3,41
2249,model,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .","[('robust to', (5, 7)), ('as', (16, 17))]","[('resulting face detector', (1, 4)), ('variations in', (7, 9)), ('pose , occlusion , and illumination', (9, 15)), ('blur and low image resolution', (20, 25))]","[['resulting face detector', 'robust to', 'variations in'], ['resulting face detector', 'robust to', 'blur and low image resolution']]","[['variations in', 'has', 'pose , occlusion , and illumination']]",[],"[['Model', 'has', 'resulting face detector']]",face_detection,3,42
2250,model,"A new type of feature , called NPD is proposed , which is efficient to compute and has several desirable properties , including scale invariance , boundedness , and enabling reconstruction of the original image .","[('called', (6, 7)), ('including', (22, 23)), ('enabling', (29, 30)), ('of', (31, 32))]","[('NPD', (7, 8)), ('several desirable properties', (18, 21)), ('scale invariance', (23, 25)), ('boundedness', (26, 27)), ('original image', (33, 35))]","[['several desirable properties', 'including', 'scale invariance'], ['several desirable properties', 'including', 'boundedness']]",[],"[['Model', 'called', 'NPD']]",[],face_detection,3,44
2251,model,A deep quadratic tree learner is proposed to learn and combine an optimal subset of NPD features to boost their discriminability .,"[('proposed to learn and combine', (6, 11)), ('of', (14, 15)), ('to boost', (17, 19))]","[('deep quadratic tree learner', (1, 5)), ('optimal subset', (12, 14)), ('discriminability', (20, 21))]","[['deep quadratic tree learner', 'proposed to learn and combine', 'optimal subset']]",[],[],"[['Model', 'has', 'deep quadratic tree learner']]",face_detection,3,45
2252,model,"In this way , only a single soft - cascade AdaBoost classifier is needed to handle unconstrained faces with occlusions and arbitrary viewpoints , without pose labeling or clustering in the training stage .","[('to handle', (14, 16)), ('with', (18, 19)), ('without pose', (24, 26)), ('in', (29, 30))]","[('only a single soft - cascade AdaBoost classifier', (4, 12)), ('unconstrained faces', (16, 18)), ('occlusions and arbitrary viewpoints', (19, 23)), ('labeling', (26, 27)), ('clustering', (28, 29)), ('training stage', (31, 33))]","[['only a single soft - cascade AdaBoost classifier', 'to handle', 'unconstrained faces'], ['only a single soft - cascade AdaBoost classifier', 'to handle', 'clustering'], ['unconstrained faces', 'with', 'occlusions and arbitrary viewpoints'], ['unconstrained faces', 'with', 'clustering'], ['unconstrained faces', 'without pose', 'labeling'], ['unconstrained faces', 'without pose', 'clustering'], ['occlusions and arbitrary viewpoints', 'without pose', 'labeling'], ['clustering', 'in', 'training stage']]",[],[],"[['Model', 'has', 'only a single soft - cascade AdaBoost classifier']]",face_detection,3,46
2253,model,The unconstrained face detector does not depend on pose specific cascade structure design ; pose labeling or clustering in the training stage is also not required .,"[('does not depend on', (4, 8)), ('in', (18, 19))]","[('unconstrained face detector', (1, 4)), ('pose specific cascade structure design', (8, 13)), ('pose labeling', (14, 16)), ('training stage', (20, 22)), ('not required', (24, 26))]","[['unconstrained face detector', 'does not depend on', 'pose specific cascade structure design']]",[],[],"[['Model', 'has', 'unconstrained face detector']]",face_detection,3,50
2254,code,The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / .,[],"[('http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface', (10, 14))]",[],[],[],[],face_detection,3,52
2255,model,"In this paper , we show that the optimal ordinal / contrastive features and their combinations can be learned by integrating the proposed NPD features in a deep quadratic tree .","[('show', (5, 6)), ('by integrating', (19, 21)), ('in', (25, 26))]","[('optimal ordinal / contrastive features', (8, 13)), ('proposed NPD features', (22, 25)), ('deep quadratic tree', (27, 30))]","[['proposed NPD features', 'in', 'deep quadratic tree']]",[],"[['Model', 'show', 'optimal ordinal / contrastive features']]",[],face_detection,3,90
2256,model,"In this way , unconstrained face variations can be automatically partitioned into different leaves of the learned quadratic tree classifier .","[('of', (14, 15))]","[('unconstrained face variations', (4, 7)), ('different leaves', (12, 14)), ('learned quadratic tree classifier', (16, 20))]","[['different leaves', 'of', 'learned quadratic tree classifier']]",[],[],"[['Model', 'has', 'unconstrained face variations']]",face_detection,3,91
2257,model,The NPD feature measures the relative difference between two pixel values .,"[('measures', (3, 4)), ('between', (7, 8))]","[('NPD feature', (1, 3)), ('relative difference', (5, 7)), ('two pixel values', (8, 11))]","[['NPD feature', 'measures', 'relative difference'], ['relative difference', 'between', 'two pixel values']]",[],[],"[['Model', 'has', 'NPD feature']]",face_detection,3,144
2258,model,"Compared to the absolute difference | x ? y| , NPD is invariant to scale change of the pixel intensities .","[('Compared to', (0, 2)), ('invariant to', (12, 14)), ('of', (16, 17))]","[('absolute difference | x ? y|', (3, 9)), ('NPD', (10, 11)), ('scale change', (14, 16))]","[['NPD', 'invariant to', 'scale change']]","[['absolute difference | x ? y|', 'has', 'NPD']]","[['Model', 'Compared to', 'absolute difference | x ? y|']]",[],face_detection,3,147
2259,research-problem,NPD FOR FACE DETECTION,[],[],[],[],[],[],face_detection,3,175
2260,experiments,"For bootstrapping nonface images , we also used the AFLW images , but masked the facial regions with random images containing no faces , as shown in .","[('For', (0, 1)), ('used', (7, 8)), ('masked', (13, 14)), ('with', (17, 18)), ('containing', (20, 21))]","[('bootstrapping nonface images', (1, 4)), ('AFLW images', (9, 11)), ('facial regions', (15, 17)), ('random images', (18, 20)), ('no faces', (21, 23))]","[['bootstrapping nonface images', 'used', 'AFLW images'], ['bootstrapping nonface images', 'masked', 'facial regions'], ['facial regions', 'with', 'random images'], ['random images', 'containing', 'no faces']]",[],[],[],face_detection,3,231
2261,hyperparameters,We used a detection template of 24 24 pixels .,"[('used', (1, 2)), ('of', (5, 6))]","[('detection template', (3, 5)), ('24 24 pixels', (6, 9))]","[['detection template', 'of', '24 24 pixels']]","[['detection template', 'has', '24 24 pixels']]","[['Hyperparameters', 'used', 'detection template']]",[],face_detection,3,232
2262,hyperparameters,"We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .","[('set', (1, 2)), ('of', (5, 6)), ('to be learned as', (9, 13))]","[('maximum depth', (3, 5)), ('tree classifiers', (7, 9)), ('eight', (13, 14))]","[['maximum depth', 'of', 'tree classifiers'], ['tree classifiers', 'to be learned as', 'eight']]",[],"[['Hyperparameters', 'set', 'maximum depth']]",[],face_detection,3,233
2263,hyperparameters,"Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .","[('contains', (3, 4))]","[('final detector', (1, 3)), ('1,226 deep quadratic trees', (4, 8)), ('46,401 NPD features', (10, 13))]","[['final detector', 'contains', '1,226 deep quadratic trees'], ['final detector', 'contains', '46,401 NPD features']]",[],[],"[['Hyperparameters', 'has', 'final detector']]",face_detection,3,235
2264,hyperparameters,The detection template is 20 20 pixels .,"[('is', (3, 4))]","[('detection template', (1, 3)), ('20 20 pixels', (4, 7))]","[['detection template', 'is', '20 20 pixels']]","[['detection template', 'has', '20 20 pixels']]",[],"[['Hyperparameters', 'has', 'detection template']]",face_detection,3,239
2265,hyperparameters,"The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .","[('contains', (3, 4)), ('was', (17, 18)), ('with', (20, 21)), ('of', (24, 25))]","[('detector cascade', (1, 3)), ('15 stages', (4, 6)), ('target false accept rate', (13, 17)), ('0.5', (18, 19)), ('detection rate', (22, 24)), ('0.999', (25, 26))]","[['detector cascade', 'contains', '15 stages'], ['target false accept rate', 'was', '0.5'], ['0.5', 'with', 'detection rate'], ['detection rate', 'of', '0.999']]","[['detector cascade', 'has', '15 stages'], ['target false accept rate', 'has', '0.5']]",[],"[['Hyperparameters', 'has', 'detector cascade']]",face_detection,3,240
2266,hyperparameters,"In the test stage , a scale factor of 1.2 was set for multiscale detection .","[('of', (8, 9)), ('set for', (11, 13))]","[('test stage', (2, 4)), ('scale factor', (6, 8)), ('1.2', (9, 10)), ('multiscale detection', (13, 15))]","[['scale factor', 'of', '1.2'], ['scale factor', 'set for', 'multiscale detection']]","[['test stage', 'has', 'scale factor']]",[],[],face_detection,3,251
2267,results,The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,"[('proposed', (1, 2)), ('is', (5, 6)), ('at', (10, 11)), ('for', (14, 15)), ('for', (23, 24))]","[('NPD face detector', (2, 5)), ('second best one', (7, 10)), ('FP = 0', (11, 14)), ('discrete metric', (16, 18)), ('third best one', (20, 23)), ('continuous metric', (25, 27))]","[['NPD face detector', 'is', 'second best one'], ['NPD face detector', 'is', 'third best one'], ['second best one', 'at', 'FP = 0'], ['FP = 0', 'for', 'discrete metric'], ['third best one', 'for', 'continuous metric'], ['third best one', 'for', 'continuous metric']]","[['NPD face detector', 'has', 'second best one']]",[],[],face_detection,3,266
2268,results,"It can be observed that the proposed NPD detector is among the top performers for the discrete metric , though it is not as good as the four recent methods for the continuous metric .","[('observed that', (3, 5)), ('among', (10, 11)), ('for', (14, 15))]","[('proposed NPD detector', (6, 9)), ('top performers', (12, 14)), ('discrete metric', (16, 18))]","[['proposed NPD detector', 'among', 'top performers'], ['top performers', 'for', 'discrete metric']]","[['proposed NPD detector', 'has', 'top performers']]","[['Results', 'observed that', 'proposed NPD detector']]",[],face_detection,3,274
2269,results,"Compared to recent methods , the Joint Cascade algorithm is the most competitive one to us in terms of accuracy and speed ( see Sec. 5.6 ) .","[('is', (9, 10)), ('in terms of', (16, 19))]","[('Joint Cascade algorithm', (6, 9)), ('most competitive one', (11, 14)), ('accuracy and speed', (19, 22))]","[['Joint Cascade algorithm', 'is', 'most competitive one'], ['most competitive one', 'in terms of', 'accuracy and speed']]","[['Joint Cascade algorithm', 'has', 'most competitive one']]",[],"[['Results', 'has', 'Joint Cascade algorithm']]",face_detection,3,278
2270,results,The performance of the Zhu-Ramanan model is quite impressive considering such a small training data .,"[('of', (2, 3)), ('is', (6, 7))]","[('performance', (1, 2)), ('Zhu-Ramanan model', (4, 6)), ('quite impressive', (7, 9))]","[['performance', 'of', 'Zhu-Ramanan model'], ['Zhu-Ramanan model', 'is', 'quite impressive']]",[],[],"[['Results', 'has', 'performance']]",face_detection,3,286
2271,results,"Many rotated , occluded , and out - of - focus faces can be successfully detected by the proposed method .",[],"[('Many rotated , occluded , and out - of - focus faces', (0, 12)), ('successfully', (14, 15)), ('proposed method', (18, 20))]",[],"[['Many rotated , occluded , and out - of - focus faces', 'has', 'successfully']]",[],[],face_detection,3,291
2272,results,The results show that the proposed NPD face detector significantly outperforms both the Viola - Jones and PittPatt face detectors .,"[('show', (2, 3))]","[('proposed NPD face detector', (5, 9)), ('significantly outperforms', (9, 11)), ('Viola - Jones and PittPatt face detectors', (13, 20))]",[],"[['proposed NPD face detector', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'Viola - Jones and PittPatt face detectors']]","[['Results', 'show', 'proposed NPD face detector']]",[],face_detection,3,307
2273,results,Evaluation on CMU - MIT Database,[],[],[],[],[],[],face_detection,3,308
2274,results,"detector is better when FP < 3 , but SURF cascade method outperforms NPD at higher FPs .","[('when', (3, 4)), ('outperforms', (12, 13)), ('at', (14, 15))]","[('detector', (0, 1)), ('better', (2, 3)), ('FP < 3', (4, 7)), ('SURF cascade method', (9, 12)), ('NPD', (13, 14)), ('higher FPs', (15, 17))]","[['better', 'when', 'FP < 3'], ['SURF cascade method', 'outperforms', 'NPD'], ['NPD', 'at', 'higher FPs']]","[['detector', 'has', 'better']]",[],"[['Results', 'has', 'detector']]",face_detection,3,318
2275,results,"In addition , the proposed NPD method is not as good as the Soft cascade , the state - of - the - art method on the CMU - MIT dataset .","[('on', (25, 26))]","[('proposed NPD method', (4, 7)), ('not as good', (8, 11)), ('Soft cascade', (13, 15)), ('state - of - the - art method', (17, 25)), ('CMU - MIT dataset', (27, 31))]","[['state - of - the - art method', 'on', 'CMU - MIT dataset']]","[['proposed NPD method', 'has', 'not as good'], ['Soft cascade', 'has', 'state - of - the - art method']]",[],[],face_detection,3,321
2276,results,"Still , the proposed NPD method can detect about 80 % of the frontal faces without any false positives , which is promising .","[('can detect', (6, 8)), ('of', (11, 12)), ('without', (15, 16))]","[('proposed NPD method', (3, 6)), ('about 80 %', (8, 11)), ('frontal faces', (13, 15)), ('false positives', (17, 19))]","[['proposed NPD method', 'can detect', 'about 80 %'], ['about 80 %', 'of', 'frontal faces'], ['frontal faces', 'without', 'false positives']]",[],[],"[['Results', 'has', 'proposed NPD method']]",face_detection,3,322
2277,results,"The NPD detector performs better than the Haar , LBP , and POF detectors with the same CART based weak learners .","[('performs', (3, 4)), ('than', (5, 6)), ('with', (14, 15))]","[('NPD detector', (1, 3)), ('better', (4, 5)), ('Haar , LBP , and POF detectors', (7, 14)), ('same CART based weak learners', (16, 21))]","[['NPD detector', 'performs', 'better'], ['better', 'than', 'Haar , LBP , and POF detectors'], ['Haar , LBP , and POF detectors', 'with', 'same CART based weak learners']]",[],[],"[['Results', 'has', 'NPD detector']]",face_detection,3,339
2278,results,"The performance improvements due to NPD features over Haar , LBP , and POF features are about 6 % , 19 % , and 15 % , respectively , for discrete metric , and about 4 % , 13 % , and 10 % , respectively , for continuous metric , at FP = 1 .","[('due to', (3, 5)), ('over', (7, 8)), ('are', (15, 16)), ('for', (29, 30)), ('at', (51, 52))]","[('performance improvements', (1, 3)), ('NPD features', (5, 7)), ('Haar , LBP , and POF features', (8, 15)), ('about 6 % , 19 % , and 15 %', (16, 26)), ('discrete metric', (30, 32)), ('4 % , 13 % , and 10 %', (35, 44)), ('continuous', (48, 49)), ('FP', (52, 53))]","[['performance improvements', 'due to', 'NPD features'], ['NPD features', 'over', 'Haar , LBP , and POF features'], ['Haar , LBP , and POF features', 'are', 'about 6 % , 19 % , and 15 %'], ['about 6 % , 19 % , and 15 %', 'for', 'discrete metric']]",[],[],"[['Results', 'has', 'performance improvements']]",face_detection,3,340
2279,results,"NPD is better than POF , because with NPD features the regression tree learns optimal thresholds to form more robust ordinal rules .","[('better than', (2, 4))]","[('NPD', (0, 1)), ('POF', (4, 5))]","[['NPD', 'better than', 'POF']]",[],[],"[['Results', 'has', 'NPD']]",face_detection,3,341
2280,results,"NPD performs better than Haar and LBP , especially at low false positives , indicating that combining optimal pixel - level features in regression trees provides better discrimination between faces and nonfaces .","[('performs', (1, 2)), ('than', (3, 4)), ('especially at', (8, 10))]","[('NPD', (0, 1)), ('better', (2, 3)), ('Haar and LBP', (4, 7)), ('low false positives', (10, 13))]","[['NPD', 'performs', 'better'], ['better', 'than', 'Haar and LBP'], ['Haar and LBP', 'especially at', 'low false positives']]",[],[],"[['Results', 'has', 'NPD']]",face_detection,3,342
2281,results,"As illustrated , using CART instead of stump classifier improves the face detection performance by about 0 % - 17 % for discrete metric and 0 % - 11 % for continuous metric .","[('using', (3, 4)), ('instead of', (5, 7)), ('improves', (9, 10)), ('by', (14, 15)), ('for', (21, 22)), ('for', (30, 31))]","[('CART', (4, 5)), ('stump classifier', (7, 9)), ('face detection performance', (11, 14)), ('about 0 % - 17 %', (15, 21)), ('discrete metric', (22, 24)), ('0 % - 11 %', (25, 30)), ('continuous metric', (31, 33))]","[['CART', 'instead of', 'stump classifier'], ['CART', 'improves', 'face detection performance'], ['stump classifier', 'improves', 'face detection performance'], ['face detection performance', 'by', 'about 0 % - 17 %'], ['face detection performance', 'by', '0 % - 11 %'], ['about 0 % - 17 %', 'for', 'discrete metric'], ['0 % - 11 %', 'for', 'continuous metric']]",[],"[['Results', 'using', 'CART']]",[],face_detection,3,352
2282,results,"Besides , the DQT based detector further improves the performance , due to its quadratic splitting capability compared to linear splitting .","[('further improves', (6, 8)), ('due to', (11, 13)), ('compared to', (17, 19))]","[('DQT based detector', (3, 6)), ('performance', (9, 10)), ('quadratic splitting capability', (14, 17)), ('linear splitting', (19, 21))]","[['DQT based detector', 'further improves', 'performance'], ['performance', 'due to', 'quadratic splitting capability'], ['quadratic splitting capability', 'compared to', 'linear splitting']]",[],[],"[['Results', 'has', 'DQT based detector']]",face_detection,3,355
2283,results,"shows that the NPD face detector performs the best on the pose and illumination subsets , thanks to the scale - invariant NPD features and the deep quadratic trees .","[('shows', (0, 1)), ('performs', (6, 7)), ('on', (9, 10)), ('thanks to', (16, 18))]","[('NPD face detector', (3, 6)), ('best', (8, 9)), ('pose and illumination subsets', (11, 15)), ('scale - invariant NPD features and the deep quadratic trees', (19, 29))]","[['NPD face detector', 'performs', 'best'], ['best', 'on', 'pose and illumination subsets'], ['best', 'thanks to', 'scale - invariant NPD features and the deep quadratic trees']]",[],"[['Results', 'shows', 'NPD face detector']]",[],face_detection,3,363
2284,results,The original resolution is 1280 720 .,"[('is', (3, 4))]","[('original resolution', (1, 3)), ('1280 720', (4, 6))]","[['original resolution', 'is', '1280 720']]","[['original resolution', 'has', '1280 720']]",[],"[['Results', 'has', 'original resolution']]",face_detection,3,381
2285,results,The NPD detector achieves similar speed as that of Joint Cascade method .,"[('achieves', (3, 4)), ('as', (6, 7))]","[('NPD detector', (1, 3)), ('similar speed', (4, 6)), ('Joint Cascade method', (9, 12))]","[['NPD detector', 'achieves', 'similar speed'], ['similar speed', 'as', 'Joint Cascade method']]","[['NPD detector', 'has', 'similar speed']]",[],"[['Results', 'has', 'NPD detector']]",face_detection,3,399
2286,baselines,We have proposed a fast and accurate method for face detection in cluttered scenes .,[],"[('face detection in cluttered scenes', (9, 14))]",[],[],[],[],face_detection,3,403
2287,research-problem,LFFD : A Light and Fast Face Detector for Edge Devices,[],[],[],[],[],[],face_detection,4,2
2288,research-problem,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,4,4
2289,research-problem,"Under the new schema , the proposed method can achieve superior accuracy ( WIDER FACE Val / Test - Easy : 0.910/0.896 , Medium : 0.881/0.865 , Hard : 0.780/0.770 ; FDDB - discontinuous : 0.973 , continuous : 0.724 ) .","[('can achieve', (8, 10))]","[('proposed method', (6, 8)), ('superior accuracy', (10, 12)), ('WIDER', (13, 14))]","[['proposed method', 'can achieve', 'superior accuracy']]","[['superior accuracy', 'has', 'WIDER']]",[],[],face_detection,4,14
2290,research-problem,Face detection is a long - standing problem in computer vision .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,4,21
2291,research-problem,Face detection is a fast - growing branch of general object detection in the past decade .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,4,31
2292,model,One of its well - known followers is aggregate channel features ( ACF ) which can take advantages of channel features effectively .,"[('can', (15, 16)), ('take advantages of', (16, 19))]","[('aggregate channel features ( ACF )', (8, 14)), ('channel features', (19, 21))]","[['aggregate channel features ( ACF )', 'take advantages of', 'channel features']]",[],[],"[['Model', 'has', 'aggregate channel features ( ACF )']]",face_detection,4,33
2293,model,"Two - stage methods consist of proposal selection and localization regression , which are mainly originated from R - CNN series .","[('consist of', (4, 6)), ('mainly', (14, 15))]","[('Two - stage methods', (0, 4)), ('proposal selection', (6, 8))]","[['Two - stage methods', 'consist of', 'proposal selection']]",[],[],"[['Model', 'has', 'Two - stage methods']]",face_detection,4,37
2294,model,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .","[('propose', (5, 6)), ('for', (15, 16)), ('considerably balancing', (19, 21))]","[('Light and Fast Face Detector ( LFFD )', (7, 15)), ('edge devices', (16, 18)), ('running efficiency', (24, 26))]","[['Light and Fast Face Detector ( LFFD )', 'for', 'edge devices'], ['Light and Fast Face Detector ( LFFD )', 'considerably balancing', 'running efficiency']]",[],"[['Model', 'propose', 'Light and Fast Face Detector ( LFFD )']]",[],face_detection,4,42
2295,experimental-setup,We flip the cropped image with probability of 0.5 .,"[('flip', (1, 2)), ('with', (5, 6)), ('of', (7, 8))]","[('cropped image', (3, 5)), ('probability', (6, 7)), ('0.5', (8, 9))]","[['cropped image', 'with', 'probability'], ['probability', 'of', '0.5']]",[],"[['Experimental setup', 'flip', 'cropped image']]",[],face_detection,4,211
2296,experimental-setup,"For face classification , we use softmax with cross - entropy loss over two classes .","[('For', (0, 1)), ('use', (5, 6)), ('with', (7, 8)), ('over', (12, 13))]","[('face classification', (1, 3)), ('softmax', (6, 7)), ('cross - entropy loss', (8, 12)), ('two classes', (13, 15))]","[['face classification', 'use', 'softmax'], ['softmax', 'with', 'cross - entropy loss'], ['cross - entropy loss', 'over', 'two classes']]",[],"[['Experimental setup', 'For', 'face classification']]",[],face_detection,4,214
2297,experimental-setup,"For bbox regression , we adopt L2 loss directly .","[('For', (0, 1)), ('adopt', (5, 6))]","[('bbox regression', (1, 3)), ('L2 loss', (6, 8))]","[['bbox regression', 'adopt', 'L2 loss']]",[],"[['Experimental setup', 'For', 'bbox regression']]",[],face_detection,4,218
2298,experimental-setup,We initialize all parameters with xavier method and train the network from scratch .,"[('initialize', (1, 2)), ('with', (4, 5)), ('train', (8, 9)), ('from', (11, 12))]","[('all parameters', (2, 4)), ('xavier method', (5, 7)), ('network', (10, 11))]","[['all parameters', 'with', 'xavier method']]",[],"[['Experimental setup', 'initialize', 'all parameters']]",[],face_detection,4,227
2299,experimental-setup,"The optimization method is SGD with 0.9 momentum , zero weight decay and batch size 32 .","[('is', (3, 4)), ('with', (5, 6))]","[('optimization method', (1, 3)), ('SGD', (4, 5)), ('0.9 momentum', (6, 8)), ('zero weight decay', (9, 12)), ('batch size', (13, 15)), ('32', (15, 16))]","[['optimization method', 'is', 'SGD'], ['SGD', 'with', '0.9 momentum'], ['SGD', 'with', 'batch size']]","[['optimization method', 'has', 'SGD'], ['batch size', 'has', '32']]",[],"[['Experimental setup', 'has', 'optimization method']]",face_detection,4,229
2300,experimental-setup,The initial learning rate is 0.1 .,"[('is', (4, 5))]","[('initial learning rate', (1, 4)), ('0.1', (5, 6))]","[['initial learning rate', 'is', '0.1']]","[['initial learning rate', 'has', '0.1']]",[],"[['Experimental setup', 'has', 'initial learning rate']]",face_detection,4,232
2301,experimental-setup,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .","[('train', (1, 2)), ('reduce', (5, 6)), ('by multiplying', (9, 11)), ('at', (12, 13))]","[('1,500,000 iterations', (2, 4)), ('learning rate', (7, 9)), ('0.1', (11, 12)), ('iteration 600,000', (13, 15))]","[['1,500,000 iterations', 'reduce', 'learning rate'], ['learning rate', 'by multiplying', '0.1'], ['0.1', 'at', 'iteration 600,000']]",[],"[['Experimental setup', 'train', '1,500,000 iterations']]",[],face_detection,4,233
2302,experimental-setup,The training time is about 5 days with two NVIDIA GTX 1080 TI .,"[('is', (3, 4)), ('with', (7, 8))]","[('training time', (1, 3)), ('about 5 days', (4, 7)), ('two NVIDIA GTX 1080 TI', (8, 13))]","[['training time', 'is', 'about 5 days'], ['about 5 days', 'with', 'two NVIDIA GTX 1080 TI']]","[['training time', 'has', 'about 5 days']]",[],"[['Experimental setup', 'has', 'training time']]",face_detection,4,234
2303,results,"DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .","[('can achieve', (8, 10)), ('with', (12, 13))]","[('DSFD , Pyramid Box , S3FD and SSH', (0, 8)), ('high accuracy', (10, 12)), ('marginal gaps', (13, 15))]","[['DSFD , Pyramid Box , S3FD and SSH', 'can achieve', 'high accuracy'], ['high accuracy', 'with', 'marginal gaps']]",[],[],"[['Results', 'has', 'DSFD , Pyramid Box , S3FD and SSH']]",face_detection,4,268
2304,results,"Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .","[('obtains', (4, 5)), ('on', (8, 9))]","[('Pyramid Box', (2, 4)), ('best results', (6, 8)), ('Hard parts', (9, 11)), ('SSH', (16, 17))]","[['Pyramid Box', 'obtains', 'best results'], ['best results', 'on', 'Hard parts']]","[['Pyramid Box', 'has', 'best results']]",[],"[['Results', 'has', 'Pyramid Box']]",face_detection,4,285
2305,results,We can see that the results on Medium and Hard parts are improved remarkably .,"[('see', (2, 3)), ('on', (6, 7))]","[('results', (5, 6)), ('Medium and Hard parts', (7, 11)), ('improved', (12, 13)), ('remarkably', (13, 14))]","[['results', 'on', 'Medium and Hard parts']]","[['improved', 'has', 'remarkably']]","[['Results', 'see', 'results']]",[],face_detection,4,289
2306,results,"Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .","[('consistently', (6, 7))]","[('proposed method LFFD', (3, 6)), ('Face - Boxes', (8, 11)), ('state of the art methods', (16, 21))]","[['proposed method LFFD', 'consistently', 'Face - Boxes']]",[],[],[],face_detection,4,292
2307,results,"For fair comparison , FaceBoxes 3.2 is used here instead of FaceBoxes .","[('instead of', (9, 11))]","[('FaceBoxes 3.2', (4, 6)), ('FaceBoxes', (11, 12))]","[['FaceBoxes 3.2', 'instead of', 'FaceBoxes']]",[],[],"[['Results', 'has', 'FaceBoxes 3.2']]",face_detection,4,298
2308,results,"The proposed LFFD runs the fastest at 38402160 , and FaceBoxes 3.2 obtains the highest speed at other three resolutions .","[('runs', (3, 4)), ('at', (6, 7)), ('obtains', (12, 13)), ('at', (16, 17))]","[('proposed LFFD', (1, 3)), ('fastest', (5, 6)), ('38402160', (7, 8)), ('FaceBoxes 3.2', (10, 12)), ('highest speed', (14, 16)), ('other three resolutions', (17, 20))]","[['proposed LFFD', 'runs', 'fastest'], ['proposed LFFD', 'runs', 'FaceBoxes 3.2'], ['fastest', 'at', '38402160'], ['highest speed', 'at', 'other three resolutions'], ['FaceBoxes 3.2', 'obtains', 'highest speed'], ['highest speed', 'at', 'other three resolutions']]","[['proposed LFFD', 'has', 'fastest'], ['FaceBoxes 3.2', 'has', 'highest speed']]",[],"[['Results', 'has', 'proposed LFFD']]",face_detection,4,305
2309,research-problem,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",[],"[('Abstract', (0, 1)), ('Face detection and alignment in', (2, 7)), ('unconstrained environment', (7, 9))]",[],"[['Abstract', 'has', 'Face detection and alignment in'], ['Face detection and alignment in', 'has', 'unconstrained environment']]",[],[],face_detection,5,5
2310,baselines,2 ) Bounding box regression :,[],"[('Bounding box regression', (2, 5))]",[],[],[],"[['Baselines', 'has', 'Bounding box regression']]",face_detection,5,76
2311,baselines,4 ) Multi-source training :,[],"[('Multi-source training', (2, 4))]",[],[],[],"[['Baselines', 'has', 'Multi-source training']]",face_detection,5,87
2312,baselines,5 ) Online Hard sample mining :,[],"[('Online Hard sample mining', (2, 6))]",[],[],[],"[['Baselines', 'has', 'Online Hard sample mining']]",face_detection,5,98
2313,baselines,"1 ) P- Net : We randomly crop several patches from WIDER FACE to collect positives , negatives and part face .","[('randomly crop', (6, 8)), ('from', (10, 11)), ('to collect', (13, 15))]","[('P- Net', (2, 4)), ('several patches', (8, 10)), ('WIDER FACE', (11, 13)), ('positives', (15, 16)), ('part face', (19, 21))]","[['P- Net', 'randomly crop', 'several patches'], ['several patches', 'from', 'WIDER FACE'], ['WIDER FACE', 'to collect', 'positives']]",[],[],"[['Baselines', 'has', 'P- Net']]",face_detection,5,117
2314,baselines,"2 ) R - Net : We use first stage of our framework to detect faces from WIDER FACE to collect positives , negatives and part face while landmark faces are detected from CelebA .","[('use', (7, 8)), ('of', (10, 11)), ('to detect', (13, 15)), ('from', (16, 17)), ('to collect', (19, 21)), ('detected from', (31, 33))]","[('R - Net', (2, 5)), ('first stage', (8, 10)), ('our framework', (11, 13)), ('faces', (15, 16)), ('WIDER FACE', (17, 19)), ('positives , negatives', (21, 24)), ('part face', (25, 27)), ('landmark faces', (28, 30)), ('CelebA', (33, 34))]","[['R - Net', 'use', 'first stage'], ['first stage', 'of', 'our framework'], ['our framework', 'to detect', 'faces'], ['faces', 'from', 'WIDER FACE'], ['faces', 'to collect', 'positives , negatives'], ['faces', 'to collect', 'part face'], ['WIDER FACE', 'to collect', 'positives , negatives'], ['WIDER FACE', 'to collect', 'part face'], ['landmark faces', 'detected from', 'CelebA']]",[],[],"[['Baselines', 'has', 'R - Net']]",face_detection,5,119
2315,baselines,3 ) O - Net : Similar to R - Net to collect data but we use first two stages of our framework to detect faces .,"[('use', (16, 17)), ('of', (20, 21)), ('to detect', (23, 25))]","[('O - Net', (2, 5)), ('first two stages', (17, 20)), ('our framework', (21, 23)), ('faces', (25, 26))]","[['O - Net', 'use', 'first two stages'], ['first two stages', 'of', 'our framework'], ['our framework', 'to detect', 'faces']]",[],[],"[['Baselines', 'has', 'O - Net']]",face_detection,5,120
2316,experiments,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,"[('compare', (2, 3)), ('of', (5, 6)), ('suggests', (15, 16)), ('for', (24, 25))]","[('bounding box regression', (6, 9)), ('joint landmarks localization task learning', (17, 22)), ('beneficial', (23, 24)), ('face classification', (26, 28)), ('bounding box regression tasks', (29, 33))]","[['beneficial', 'for', 'face classification'], ['beneficial', 'for', 'bounding box regression tasks']]","[['joint landmarks localization task learning', 'has', 'beneficial']]",[],[],face_detection,5,132
2317,research-problem,Robust Face Detection via Learning Small Faces on Hard Images,[],"[('Face Detection', (1, 3))]",[],[],[],[],face_detection,6,2
2318,research-problem,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,6,13
2319,experiments,"In , we show that , even on the train set of WIDER FACE , the official pre-trained SSH 1 still fails on some of the images with extremely hard faces .","[('of', (11, 12)), ('on', (22, 23)), ('with', (27, 28))]","[('train set', (9, 11)), ('WIDER FACE', (12, 14)), ('official pre-trained SSH 1', (16, 20)), ('fails', (21, 22)), ('some of the images', (23, 27)), ('extremely hard faces', (28, 31))]","[['train set', 'of', 'WIDER FACE'], ['fails', 'on', 'some of the images'], ['some of the images', 'with', 'extremely hard faces']]","[['train set', 'has', 'WIDER FACE'], ['official pre-trained SSH 1', 'has', 'fails']]",[],[],face_detection,6,20
2320,model,"To address this issue , in this paper , we propose a robust face detector by putting more training focus on those hard images .","[('propose', (10, 11)), ('by putting', (15, 17)), ('on', (20, 21))]","[('robust face detector', (12, 15)), ('more training focus', (17, 20)), ('hard images', (22, 24))]","[['robust face detector', 'by putting', 'more training focus'], ['more training focus', 'on', 'hard images']]",[],"[['Model', 'propose', 'robust face detector']]",[],face_detection,6,24
2321,model,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .","[('propose to', (6, 8)), ('at', (11, 12)), ('in parallel with', (14, 17))]","[('mine hard examples', (8, 11)), ('image level', (12, 14)), ('anchor level', (17, 19))]","[['mine hard examples', 'at', 'image level'], ['mine hard examples', 'in parallel with', 'anchor level']]",[],"[['Model', 'propose to', 'mine hard examples']]",[],face_detection,6,27
2322,model,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .","[('to', (10, 11)), ('during', (13, 14))]","[('dynamically assign', (6, 8)), ('difficulty scores', (8, 10)), ('training images', (11, 13)), ('learning process', (15, 17)), ('well - detected', (26, 29))]","[['difficulty scores', 'to', 'training images'], ['training images', 'during', 'learning process']]","[['dynamically assign', 'has', 'difficulty scores']]",[],[],face_detection,6,28
2323,model,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .","[('by exclusively exploiting', (15, 18))]","[('detection quality', (13, 15)), ('small faces', (18, 20))]","[['detection quality', 'by exclusively exploiting', 'small faces']]",[],[],[],face_detection,6,31
2324,model,"To conclude , in this paper , we propose a novel face detector with the following contributions :","[('propose', (8, 9))]","[('novel face detector', (10, 13))]",[],[],"[['Model', 'propose', 'novel face detector']]",[],face_detection,6,37
2325,model,"This is done without any extra modules , parameters or computation overhead added on the existing detector .","[('done without', (2, 4)), ('added on', (12, 14))]","[('any extra modules , parameters', (4, 9)), ('computation overhead', (10, 12)), ('existing detector', (15, 17))]","[['computation overhead', 'added on', 'existing detector']]",[],"[['Model', 'done without', 'any extra modules , parameters']]",[],face_detection,6,39
2326,model,"We design a single shot detector with only one detection feature map , which focuses on small faces with a specific range of sizes .","[('design', (1, 2)), ('with', (6, 7)), ('focuses on', (14, 16)), ('with', (18, 19))]","[('single shot detector', (3, 6)), ('only one detection feature map', (7, 12)), ('small faces', (16, 18)), ('specific range of sizes', (20, 24))]","[['single shot detector', 'with', 'only one detection feature map'], ['small faces', 'with', 'specific range of sizes'], ['only one detection feature map', 'focuses on', 'small faces'], ['small faces', 'with', 'specific range of sizes']]",[],"[['Model', 'design', 'single shot detector']]",[],face_detection,6,40
2327,experimental-setup,"We flip all images horizontally , to double the size of our training dataset to 25760 .","[('flip', (1, 2)), ('to double', (6, 8)), ('of', (10, 11)), ('to', (14, 15))]","[('all images', (2, 4)), ('horizontally', (4, 5)), ('size', (9, 10)), ('our training dataset', (11, 14)), ('25760', (15, 16))]","[['all images', 'to double', 'size'], ['horizontally', 'to double', 'size'], ['our training dataset', 'to double', '25760'], ['size', 'of', 'our training dataset'], ['our training dataset', 'to', '25760']]","[['all images', 'has', 'horizontally']]","[['Experimental setup', 'flip', 'all images']]",[],face_detection,6,163
2328,experimental-setup,"We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .","[('use', (1, 2)), ('to initialize', (7, 9))]","[('ImageNet pretrained VGG16 model', (3, 7)), ('our network backbone', (9, 12)), ('our newly introduced layers', (14, 18)), ('randomly initialized', (19, 21)), ('Gaussian initialization', (22, 24))]","[['ImageNet pretrained VGG16 model', 'to initialize', 'our network backbone']]","[['our newly introduced layers', 'has', 'randomly initialized']]","[['Experimental setup', 'use', 'ImageNet pretrained VGG16 model']]",[],face_detection,6,165
2329,experimental-setup,"We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .","[('train', (1, 2)), ('with', (4, 5)), ('to be', (7, 9)), ('for', (11, 12)), ('with', (16, 17)), ('of', (20, 21)), ('with', (30, 31))]","[('model', (3, 4)), ('itersize', (6, 7)), ('2', (9, 10)), ('46 k iterations', (12, 15)), ('learning rate', (18, 20)), ('0.004', (21, 22)), ('14 k iterations', (27, 30)), ('smaller learning rate', (32, 35)), ('0.0004', (36, 37))]","[['model', 'with', 'itersize'], ['model', 'with', 'learning rate'], ['46 k iterations', 'with', 'learning rate'], ['14 k iterations', 'with', 'smaller learning rate'], ['itersize', 'to be', '2'], ['itersize', 'for', '46 k iterations'], ['46 k iterations', 'with', 'learning rate'], ['14 k iterations', 'with', 'smaller learning rate'], ['learning rate', 'of', '0.004'], ['smaller learning rate', 'of', '0.0004'], ['14 k iterations', 'with', 'smaller learning rate']]","[['learning rate', 'has', '0.004']]","[['Experimental setup', 'train', 'model']]",[],face_detection,6,166
2330,experimental-setup,"During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .","[('During', (0, 1)), ('use', (4, 5)), ('to simultaneously', (7, 9)), ('to compute', (9, 11)), ('update', (14, 15)), ('by', (17, 18)), ('with', (20, 21))]","[('training', (1, 2)), ('4 GPUs', (5, 7)), ('gradient', (12, 13)), ('weight', (16, 17)), ('synchronized SGD', (18, 20)), ('Momentum', (21, 22))]","[['training', 'use', '4 GPUs'], ['4 GPUs', 'to compute', 'gradient'], ['4 GPUs', 'update', 'weight'], ['weight', 'by', 'synchronized SGD'], ['synchronized SGD', 'with', 'Momentum']]",[],"[['Experimental setup', 'During', 'training']]",[],face_detection,6,167
2331,experimental-setup,"The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .","[('of', (4, 5)), ('during', (8, 9)), ('of', (16, 17)), ('set to have', (19, 22))]","[('first two blocks', (1, 4)), ('VGG16', (5, 6)), ('frozen', (7, 8)), ('training', (10, 11)), ('rest layers', (14, 16)), ('VGG16', (17, 18)), ('double learning rate', (23, 26))]","[['first two blocks', 'of', 'VGG16'], ['rest layers', 'of', 'VGG16'], ['frozen', 'during', 'training'], ['rest layers', 'of', 'VGG16'], ['rest layers', 'set to have', 'double learning rate']]",[],[],"[['Experimental setup', 'has', 'first two blocks']]",face_detection,6,168
2332,results,"As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .","[('achieves', (7, 8)), ('on', (11, 12)), ('by', (27, 28))]","[('our method', (5, 7)), ('best performance', (9, 11)), ('hard subset', (13, 15)), ('outperforms', (17, 18)), ('current state - of - the - art', (19, 27)), ('large margin', (29, 31))]","[['our method', 'achieves', 'best performance'], ['best performance', 'on', 'hard subset'], ['current state - of - the - art', 'by', 'large margin']]","[['outperforms', 'has', 'current state - of - the - art']]",[],"[['Results', 'has', 'our method']]",face_detection,6,175
2333,results,"Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - thearts .","[('on', (2, 3)), ('comparable to', (7, 9)), ('on', (22, 23)), ('is', (26, 27))]","[('Our performance', (0, 2)), ('medium subset', (4, 6)), ('most recent state - of - the - art', (10, 19)), ('performance', (21, 22)), ('easy subset', (24, 26)), ('bit worse', (28, 30))]","[['Our performance', 'on', 'medium subset'], ['performance', 'on', 'easy subset'], ['medium subset', 'comparable to', 'most recent state - of - the - art'], ['performance', 'on', 'easy subset'], ['performance', 'is', 'bit worse'], ['easy subset', 'is', 'bit worse']]",[],[],"[['Results', 'has', 'Our performance']]",face_detection,6,177
2334,results,"We show the PR curve at compared with , and our method achieves a new the state - of - the - art performance of AP = 99.0 .","[('show', (1, 2)), ('achieves', (12, 13)), ('of', (24, 25))]","[('PR curve', (3, 5)), ('our method', (10, 12)), ('new the state - of - the - art performance', (14, 24)), ('AP = 99.0', (25, 28))]","[['our method', 'achieves', 'new the state - of - the - art performance'], ['new the state - of - the - art performance', 'of', 'AP = 99.0']]","[['PR curve', 'has', 'our method']]",[],[],face_detection,6,185
2335,results,"As shown in compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 .","[('achieves', (8, 9)), ('with', (21, 22)), ('of', (24, 25))]","[('our method', (6, 8)), ('state - of - the - art and', (9, 17)), ('almost perfect performance', (17, 20)), ('AP', (23, 24)), ('99.60', (25, 26))]","[['our method', 'achieves', 'state - of - the - art and'], ['our method', 'achieves', 'almost perfect performance'], ['almost perfect performance', 'with', 'AP'], ['AP', 'of', '99.60']]","[['state - of - the - art and', 'has', 'almost perfect performance'], ['AP', 'has', '99.60']]",[],"[['Results', 'has', 'our method']]",face_detection,6,187
2336,results,"From , we can see that our single level baseline model can achieve performance comparable to the current : Ablation experiments .","[('see that', (4, 6)), ('comparable to', (14, 16))]","[('our single level baseline model', (6, 11)), ('performance', (13, 14)), ('current : Ablation experiments', (17, 21))]","[['performance', 'comparable to', 'current : Ablation experiments']]",[],"[['Results', 'see that', 'our single level baseline model']]",[],face_detection,6,191
2337,baselines,Baseline - Three is a face detector similar to SSH with three detection feature maps .,"[('similar to', (7, 9)), ('with', (10, 11))]","[('face detector', (5, 7)), ('SSH', (9, 10)), ('three detection feature maps', (11, 15))]","[['face detector', 'similar to', 'SSH'], ['SSH', 'with', 'three detection feature maps']]",[],[],"[['Baselines', 'has', 'face detector']]",face_detection,6,192
2338,baselines,Baseline - Single is our proposed detector with single detection feature map shown in .,"[('is', (3, 4)), ('with', (7, 8))]","[('Single', (2, 3)), ('our proposed detector', (4, 7)), ('single detection feature map', (8, 12))]","[['Single', 'is', 'our proposed detector'], ['our proposed detector', 'with', 'single detection feature map']]","[['Single', 'has', 'our proposed detector']]",[],"[['Baselines', 'has', 'Single']]",face_detection,6,193
2339,results,"Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors .","[('performs', (7, 8)), ('than', (9, 10))]","[('Our model with single detection feature map', (0, 7)), ('better', (8, 9)), ('one with three detection feature maps', (11, 17))]","[['Our model with single detection feature map', 'performs', 'better'], ['better', 'than', 'one with three detection feature maps']]",[],[],"[['Results', 'has', 'Our model with single detection feature map']]",face_detection,6,196
2340,results,Combining HIM and DH together can improve further towards the state - of - the - art performance .,"[('Combining', (0, 1))]","[('HIM and DH together', (1, 5)), ('improve', (6, 7)), ('state - of - the - art performance', (10, 18))]",[],"[['HIM and DH together', 'has', 'improve']]","[['Results', 'Combining', 'HIM and DH together']]",[],face_detection,6,201
2341,results,The ablation results evaluated on WIDER FACE val dataset are shown in .,"[('evaluated on', (3, 5))]","[('ablation results', (1, 3)), ('WIDER FACE val dataset', (5, 9))]","[['ablation results', 'evaluated on', 'WIDER FACE val dataset']]",[],[],"[['Results', 'has', 'ablation results']]",face_detection,6,210
2342,results,"Our full evaluation resizes the image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels respectively , to build an image pyramid .","[('resizes', (3, 4)), ('contains', (11, 12)), ('to build', (24, 26))]","[('image', (5, 6)), ('short side', (9, 11)), ('100 , 300 , 600 , 1000 and 1400 pixels', (12, 22)), ('image pyramid', (27, 29))]","[['short side', 'contains', '100 , 300 , 600 , 1000 and 1400 pixels'], ['100 , 300 , 600 , 1000 and 1400 pixels', 'to build', 'image pyramid']]",[],"[['Results', 'resizes', 'image']]",[],face_detection,6,217
2343,results,"Without resizing the short side to contain 100 and 300 pixels , the performance on easy subset is only 78.2 , which is even lower than the performance on medium and hard which contain much harder faces .","[('Without resizing', (0, 2)), ('to contain', (5, 7)), ('on', (14, 15)), ('is', (17, 18))]","[('short side', (3, 5)), ('100 and 300 pixels', (7, 11)), ('performance', (13, 14)), ('easy subset', (15, 17)), ('only 78.2', (18, 20))]","[['short side', 'to contain', '100 and 300 pixels'], ['performance', 'on', 'easy subset'], ['performance', 'is', 'only 78.2'], ['easy subset', 'is', 'only 78.2']]",[],[],[],face_detection,6,220
2344,results,"For fair comparison , we run all methods on the same machine , with one Titan X ( Maxwell ) GPU , and Intel","[('run', (5, 6)), ('on', (8, 9)), ('with', (13, 14))]","[('all methods', (6, 8)), ('same machine', (10, 12)), ('one Titan X ( Maxwell ) GPU', (14, 21))]","[['all methods', 'on', 'same machine'], ['all methods', 'with', 'one Titan X ( Maxwell ) GPU'], ['same machine', 'with', 'one Titan X ( Maxwell ) GPU']]",[],"[['Results', 'run', 'all methods']]",[],face_detection,6,225
2345,results,"All methods except for Pyramid Box are based on Caffe1 implementation , which is compiled with CUDA 9.0 and CUDNN 7 .","[('except for', (2, 4)), ('based on', (7, 9)), ('compiled with', (14, 16))]","[('Pyramid Box', (4, 6)), ('Caffe1 implementation', (9, 11)), ('CUDA 9.0 and CUDNN 7', (16, 21))]","[['Caffe1 implementation', 'compiled with', 'CUDA 9.0 and CUDNN 7']]",[],"[['Results', 'except for', 'Pyramid Box']]",[],face_detection,6,228
2346,results,We use the officially built Pad - dlePaddle with CUDA 9.0 and CUDNN 7 .,"[('use', (1, 2)), ('with', (8, 9))]","[('officially built Pad - dlePaddle', (3, 8)), ('CUDA 9.0 and CUDNN 7', (9, 14))]","[['officially built Pad - dlePaddle', 'with', 'CUDA 9.0 and CUDNN 7']]",[],"[['Results', 'use', 'officially built Pad - dlePaddle']]",[],face_detection,6,230
2347,results,"As shown in , our detector can outperform SSH , S 3 FD and PyramidBox significantly with a smaller inference time .","[('with', (16, 17))]","[('our detector', (4, 6)), ('outperform', (7, 8)), ('SSH , S 3 FD and PyramidBox', (8, 15)), ('significantly', (15, 16)), ('smaller inference time', (18, 21))]","[['outperform', 'with', 'smaller inference time'], ['SSH , S 3 FD and PyramidBox', 'with', 'smaller inference time'], ['significantly', 'with', 'smaller inference time']]","[['our detector', 'has', 'outperform'], ['outperform', 'has', 'SSH , S 3 FD and PyramidBox'], ['SSH , S 3 FD and PyramidBox', 'has', 'significantly']]",[],"[['Results', 'has', 'our detector']]",face_detection,6,235
2348,research-problem,Recurrent Scale Approximation for Object Detection in CNN,[],"[('Object Detection', (4, 6))]",[],[],[],[],face_detection,7,2
2349,research-problem,Object detection is one of the most important tasks in computer vision .,[],"[('Object detection', (0, 2))]",[],[],[],[],face_detection,7,14
2350,model,"Most of the appearance variations can now be handled in CNN , benefiting from the invariance property of convolution and pooling operations .","[('handled in', (8, 10)), ('benefiting from', (12, 14)), ('of', (17, 18))]","[('Most of the appearance variations', (0, 5)), ('CNN', (10, 11)), ('invariance property', (15, 17)), ('convolution', (18, 19)), ('pooling operations', (20, 22))]","[['Most of the appearance variations', 'handled in', 'CNN'], ['Most of the appearance variations', 'benefiting from', 'invariance property'], ['Most of the appearance variations', 'benefiting from', 'pooling operations'], ['invariance property', 'of', 'convolution'], ['invariance property', 'of', 'pooling operations']]",[],[],"[['Model', 'has', 'Most of the appearance variations']]",face_detection,7,18
2351,model,"The location variations can be naturally solved via sliding windows , which can be efficiently incorporated into CNN in a fully convolutional manner .","[('can be', (3, 5)), ('via', (7, 8)), ('can be efficiently incorporated into', (12, 17))]","[('location variations', (1, 3)), ('naturally', (5, 6)), ('sliding windows', (8, 10)), ('CNN', (17, 18)), ('fully convolutional manner', (20, 23))]","[['location variations', 'can be', 'naturally'], ['location variations', 'via', 'sliding windows'], ['naturally', 'via', 'sliding windows'], ['sliding windows', 'can be efficiently incorporated into', 'CNN']]","[['location variations', 'has', 'naturally']]",[],"[['Model', 'has', 'location variations']]",face_detection,7,19
2352,model,"The first way , as shown in , handles objects of different scales independently by resizing the input into different scales and then forwarding the resized images multiple times for detection .","[('handles', (8, 9)), ('of', (10, 11)), ('by resizing', (14, 16)), ('into', (18, 19)), ('forwarding', (23, 24)), ('for', (29, 30))]","[('first', (1, 2)), ('objects', (9, 10)), ('different scales independently', (11, 14)), ('input', (17, 18)), ('different scales', (19, 21)), ('resized images', (25, 27)), ('multiple times', (27, 29)), ('detection', (30, 31))]","[['first', 'handles', 'objects'], ['objects', 'of', 'different scales independently'], ['different scales independently', 'by resizing', 'input'], ['input', 'into', 'different scales'], ['first', 'forwarding', 'resized images'], ['multiple times', 'for', 'detection']]","[['objects', 'has', 'different scales independently'], ['resized images', 'has', 'multiple times']]",[],"[['Model', 'has', 'first']]",face_detection,7,22
2353,model,"The second way , as depicted in , forwards the image only once and then directly regresses objects at multiple scales .","[('forwards', (8, 9)), ('directly', (15, 16)), ('at', (18, 19))]","[('image', (10, 11)), ('only once', (11, 13)), ('objects', (17, 18)), ('multiple scales', (19, 21))]","[['objects', 'at', 'multiple scales']]","[['image', 'has', 'only once']]","[['Model', 'forwards', 'image']]",[],face_detection,7,24
2354,model,"Our solution to the feature pyramid in CNN descends from the observations of modern CNN - based detectors , including Faster - RCNN , R - FCN , SSD , YOLO and STN , where feature maps are first computed and the detection results are decoded from the maps afterwards .","[('to', (2, 3)), ('in', (6, 7)), ('descends from', (8, 10)), ('of', (12, 13)), ('including', (19, 20))]","[('feature pyramid', (4, 6)), ('CNN', (7, 8)), ('observations', (11, 12)), ('modern CNN - based detectors', (13, 18)), ('Faster - RCNN', (20, 23)), ('R - FCN', (24, 27)), ('SSD', (28, 29)), ('YOLO', (30, 31)), ('STN', (32, 33)), ('feature', (35, 36))]","[['feature pyramid', 'in', 'CNN'], ['feature pyramid', 'descends from', 'observations'], ['CNN', 'descends from', 'observations'], ['observations', 'of', 'modern CNN - based detectors'], ['modern CNN - based detectors', 'including', 'Faster - RCNN'], ['modern CNN - based detectors', 'including', 'R - FCN'], ['modern CNN - based detectors', 'including', 'SSD'], ['modern CNN - based detectors', 'including', 'YOLO'], ['modern CNN - based detectors', 'including', 'STN']]",[],"[['Model', 'to', 'feature pyramid']]",[],face_detection,7,31
2355,model,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .","[('propose', (5, 6)), ('to achieve', (16, 18))]","[('recurrent scale approximation ( RSA , see ) unit', (7, 16))]",[],[],"[['Model', 'propose', 'recurrent scale approximation ( RSA , see ) unit']]",[],face_detection,7,35
2356,model,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,"[('designed to be', (4, 7)), ('in', (12, 13)), ('to be fed with', (16, 20)), ('at', (24, 25))]","[('RSA unit', (1, 3)), ('plugged', (7, 8)), ('some specific depths', (9, 12)), ('network', (14, 15)), ('initial feature map', (21, 24)), ('largest scale', (26, 28))]","[['RSA unit', 'designed to be', 'plugged'], ['some specific depths', 'in', 'network'], ['RSA unit', 'to be fed with', 'initial feature map'], ['initial feature map', 'at', 'largest scale']]",[],[],"[['Model', 'has', 'RSA unit']]",face_detection,7,36
2357,model,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,"[('convolves', (2, 3)), ('in', (5, 6)), ('to generate', (9, 11)), ('of', (13, 14))]","[('input', (4, 5)), ('recurrent manner', (7, 9)), ('prediction', (12, 13)), ('feature map', (15, 17)), ('half the size', (19, 22)), ('input', (24, 25))]","[['input', 'in', 'recurrent manner'], ['recurrent manner', 'to generate', 'prediction'], ['prediction', 'of', 'feature map'], ['half the size', 'of', 'input']]",[],[],[],face_detection,7,37
2358,model,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,"[('to globally predict', (8, 11)), ('for', (13, 14)), ('compute', (19, 20)), ('for', (22, 23)), ('based on', (29, 31))]","[('scale - forecast network', (4, 8)), ('potential scales', (11, 13)), ('novel image', (15, 17)), ('feature pyramids', (20, 22)), ('prediction', (32, 33))]","[['scale - forecast network', 'to globally predict', 'potential scales'], ['potential scales', 'for', 'novel image']]",[],[],"[['Model', 'has', 'scale - forecast network']]",face_detection,7,40
2359,model,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,"[('that retraces', (7, 9)), ('of', (11, 12)), ('in', (15, 16)), ('generates', (20, 21)), ('for', (24, 25)), ('based on', (27, 29))]","[('landmark retracing network', (4, 7)), ('location', (10, 11)), ('regressed landmarks', (13, 15)), ('preceding layers', (17, 19)), ('confidence score', (22, 24)), ('each landmark', (25, 27)), ('landmark feature set', (30, 33))]","[['landmark retracing network', 'that retraces', 'location'], ['location', 'of', 'regressed landmarks'], ['regressed landmarks', 'in', 'preceding layers'], ['landmark retracing network', 'generates', 'confidence score'], ['confidence score', 'for', 'each landmark'], ['each landmark', 'based on', 'landmark feature set']]",[],[],"[['Model', 'has', 'landmark retracing network']]",face_detection,7,42
2360,model,The three components can be incorporated into a unified CNN framework and trained end - to - end .,"[('incorporated into', (5, 7)), ('trained', (12, 13))]","[('unified CNN framework', (8, 11))]",[],[],"[['Model', 'incorporated into', 'unified CNN framework']]",[],face_detection,7,46
2361,model,"1 ) We prove that deep CNN features for an image can be approximated from different scales using a portable recurrent unit ( RSA ) , which fully leverages efficiency and accuracy .","[('prove', (3, 4)), ('for', (8, 9)), ('approximated from', (13, 15)), ('using', (17, 18)), ('fully leverages', (27, 29))]","[('deep CNN features', (5, 8)), ('image', (10, 11)), ('different scales', (15, 17)), ('portable recurrent unit ( RSA )', (19, 25)), ('efficiency', (29, 30))]","[['deep CNN features', 'for', 'image'], ['deep CNN features', 'approximated from', 'different scales'], ['different scales', 'using', 'portable recurrent unit ( RSA )'], ['portable recurrent unit ( RSA )', 'fully leverages', 'efficiency']]",[],"[['Model', 'prove', 'deep CNN features']]",[],face_detection,7,49
2362,baselines,We use this model in scale - forecast network and LRN .,"[('in', (4, 5))]","[('scale - forecast network and LRN', (5, 11))]",[],[],[],[],face_detection,7,179
2363,hyperparameters,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .","[('set to', (5, 7)), ('of', (8, 9))]","[('numbers of channels', (1, 4)), ('half', (7, 8)), ('original ResNet model', (10, 13))]","[['numbers of channels', 'set to', 'half'], ['half', 'of', 'original ResNet model']]",[],[],"[['Hyperparameters', 'has', 'numbers of channels']]",face_detection,7,180
2364,experiments,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,"[('train', (2, 3)), ('use', (10, 11)), ('of', (13, 14)), ('to launch', (16, 18))]","[('scale - forecast network', (4, 8)), ('output', (12, 13)), ('predicted scales', (14, 16)), ('RSA unit and LRN', (19, 23))]","[['output', 'of', 'predicted scales'], ['predicted scales', 'to launch', 'RSA unit and LRN']]",[],[],[],face_detection,7,181
2365,experiments,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .","[('is', (3, 4)), ('set to', (10, 12)), ('with', (13, 14))]","[('batch size', (1, 3)), ('4', (4, 5)), ('base learning rate', (6, 9)), ('0.001', (12, 13)), ('decrease', (15, 16))]","[['batch size', 'is', '4'], ['base learning rate', 'set to', '0.001'], ['0.001', 'with', 'decrease']]","[['batch size', 'has', '4'], ['base learning rate', 'has', '0.001']]",[],[],face_detection,7,184
2366,hyperparameters,"The maximum training iteration is 1,000,000 .","[('is', (4, 5))]","[('maximum training iteration', (1, 4)), ('1,000,000', (5, 6))]","[['maximum training iteration', 'is', '1,000,000']]","[['maximum training iteration', 'has', '1,000,000']]",[],"[['Hyperparameters', 'has', 'maximum training iteration']]",face_detection,7,185
2367,hyperparameters,We use stochastic gradient descent as the optimizer .,"[('use', (1, 2)), ('as', (5, 6))]","[('stochastic gradient descent', (2, 5)), ('optimizer', (7, 8))]","[['stochastic gradient descent', 'as', 'optimizer']]",[],"[['Hyperparameters', 'use', 'stochastic gradient descent']]",[],face_detection,7,186
2368,experiments,"We can observe from the results that our trained scale network recalls almost 99 % at x = 1 , indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales .","[('observe', (2, 3)), ('recalls', (11, 12)), ('at', (15, 16))]","[('our trained scale network', (7, 11)), ('almost 99 %', (12, 15)), ('x = 1', (16, 19))]","[['our trained scale network', 'recalls', 'almost 99 %'], ['almost 99 %', 'at', 'x = 1']]",[],[],[],face_detection,7,190
2369,hyperparameters,"knowledge , during inference , we set the threshold for predicting potential scales of the input so that it has approximately two predictions .","[('during', (2, 3)), ('set', (6, 7)), ('for predicting', (9, 11)), ('of', (13, 14))]","[('inference', (3, 4)), ('threshold', (8, 9)), ('potential scales', (11, 13)), ('input', (15, 16)), ('approximately two predictions', (20, 23))]","[['inference', 'set', 'threshold'], ['threshold', 'for predicting', 'potential scales'], ['potential scales', 'of', 'input']]","[['inference', 'has', 'threshold']]","[['Hyperparameters', 'during', 'inference']]",[],face_detection,7,195
2370,results,Performance of Scale - forecast Network,[],[],[],[],[],[],face_detection,7,199
2371,baselines,Ablative Evaluation on RSA Unit,[],[],[],[],[],[],face_detection,7,200
2372,experiments,"The image is first resized to higher dimension being 2048 and the RSA unit predicts six scales defined in Section 3.1 ( 1024 , 512 , 256 , 128 and 64 ) .","[('first', (3, 4)), ('resized to', (4, 6)), ('being', (8, 9))]","[('image', (1, 2)), ('higher dimension', (6, 8)), ('2048', (9, 10)), ('RSA', (12, 13))]","[['image', 'resized to', 'higher dimension'], ['higher dimension', 'being', '2048']]",[],[],[],face_detection,7,203
2373,experiments,"However , results from the figure indicate that as we plug RSA at deeper layers , its performance decades .","[('indicate', (6, 7)), ('plug', (10, 11)), ('at', (12, 13))]","[('RSA', (11, 12)), ('deeper layers', (13, 15)), ('performance decades', (17, 19))]","[['RSA', 'at', 'deeper layers']]","[['deeper layers', 'has', 'performance decades']]",[],[],face_detection,7,207
2374,experiments,"For example , in case final feature which means RSA is plugged at the final convolution layer after res3c , the error rate is almost 100 % , indicating RSA 's incapability of handling the insufficient information in this layer .","[('in', (3, 4)), ('means', (8, 9)), ('plugged at', (11, 13)), ('after', (17, 18)), ('is', (23, 24))]","[('case final feature', (4, 7)), ('RSA', (9, 10)), ('final convolution layer', (14, 17)), ('res3c', (18, 19)), ('error rate', (21, 23)), ('almost 100 %', (24, 27))]","[['case final feature', 'means', 'RSA'], ['RSA', 'plugged at', 'final convolution layer'], ['final convolution layer', 'after', 'res3c'], ['error rate', 'is', 'almost 100 %']]","[['case final feature', 'has', 'RSA'], ['res3c', 'has', 'error rate'], ['error rate', 'has', 'almost 100 %']]",[],[],face_detection,7,209
2375,experiments,The error rate decreases in shallower cases .,[],"[('error rate', (1, 3)), ('decreases', (3, 4)), ('shallower cases', (5, 7))]",[],"[['error rate', 'has', 'decreases']]",[],[],face_detection,7,210
2376,experiments,The path during one - time forward from image to the input map right before RSA is shorter ; and the rolling out time increases accordingly .,"[('during', (2, 3)), ('from', (7, 8)), ('to', (9, 10)), ('right before', (13, 15)), ('is', (16, 17))]","[('path', (1, 2)), ('one - time forward', (3, 7)), ('image', (8, 9)), ('input map', (11, 13)), ('RSA', (15, 16)), ('shorter', (17, 18)), ('rolling out time', (21, 24)), ('increases', (24, 25))]","[['path', 'during', 'one - time forward'], ['one - time forward', 'from', 'image'], ['image', 'to', 'input map'], ['input map', 'right before', 'RSA'], ['RSA', 'is', 'shorter']]","[['rolling out time', 'has', 'increases']]",[],[],face_detection,7,216
2377,experiments,Most of the computation happens before layer res2 b and it has an acceptable error rate of 3.44 % .,"[('happens before', (4, 6)), ('of', (16, 17))]","[('computation', (3, 4)), ('layer res2 b', (6, 9)), ('acceptable error rate', (13, 16)), ('3.44 %', (17, 19))]","[['computation', 'happens before', 'layer res2 b'], ['acceptable error rate', 'of', '3.44 %']]",[],[],[],face_detection,7,219
2378,experiments,Our Algorithm vs. Baseline RPN,[],[],[],[],[],[],face_detection,7,225
2379,baselines,"We compare our model ( denoted as RSA + LRN ) , a combination of the RSA unit and a landmark retracing network , with the region proposal network ( RPN ) .","[('denoted', (5, 6)), ('combination of', (13, 15)), ('with', (24, 25))]","[('region proposal network ( RPN )', (26, 32))]",[],[],[],[],face_detection,7,226
2380,experiments,"In the first setting , we use the original RPN with multiple anchors ( denoted as RPN m ) to detect faces of various scales .","[('use', (6, 7)), ('with', (10, 11)), ('denoted as', (14, 16)), ('to detect', (19, 21))]","[('original RPN', (8, 10)), ('multiple anchors', (11, 13)), ('RPN m', (16, 18)), ('faces of various scales', (21, 25))]","[['original RPN', 'with', 'multiple anchors'], ['multiple anchors', 'denoted as', 'RPN m'], ['multiple anchors', 'to detect', 'faces of various scales']]","[['multiple anchors', 'name', 'RPN m']]",[],[],face_detection,7,227
2381,experiments,"On AFW , our algorithm achieves an AP of 99.17 % using the original annotation and an AP of 99 . 96 % using the revised annotation 7 ( c ) .","[('On', (0, 1)), ('achieves', (5, 6)), ('of', (8, 9)), ('using', (11, 12)), ('using', (23, 24))]","[('AFW', (1, 2)), ('our algorithm', (3, 5)), ('AP', (7, 8)), ('99.17 %', (9, 11)), ('original annotation', (13, 15)), ('AP', (17, 18)), ('99 . 96 %', (19, 23)), ('revised annotation', (25, 27))]","[['our algorithm', 'achieves', 'AP'], ['AP', 'of', '99.17 %'], ['AP', 'of', '99 . 96 %'], ['AP', 'using', 'original annotation'], ['99.17 %', 'using', 'original annotation'], ['99.17 %', 'using', 'AP'], ['99 . 96 %', 'using', 'revised annotation'], ['AP', 'using', 'revised annotation'], ['AP', 'using', 'revised annotation'], ['99 . 96 %', 'using', 'revised annotation']]","[['AFW', 'has', 'our algorithm']]",[],[],face_detection,7,242
2382,experiments,"On FDDB , RSA + LRN recalls 93.0 % faces with 50 false positives 7 ( a ) .","[('On', (0, 1)), ('recalls', (6, 7)), ('with', (10, 11))]","[('FDDB', (1, 2)), ('RSA + LRN', (3, 6)), ('93.0 % faces', (7, 10)), ('50 false positives', (11, 14))]","[['RSA + LRN', 'recalls', '93.0 % faces'], ['93.0 % faces', 'with', '50 false positives']]","[['FDDB', 'has', 'RSA + LRN']]",[],[],face_detection,7,243
2383,experiments,"On MALF , our method recalls 82.4 % faces with zero false positive 7 ( d ) .","[('On', (0, 1)), ('recalls', (5, 6)), ('with', (9, 10))]","[('MALF', (1, 2)), ('our method', (3, 5)), ('82.4 % faces', (6, 9)), ('zero false positive', (10, 13))]","[['our method', 'recalls', '82.4 % faces'], ['82.4 % faces', 'with', 'zero false positive']]","[['MALF', 'has', 'our method']]",[],[],face_detection,7,244
2384,experiments,"To address this , we learn a transformer to fit each annotation from the landmarks .","[('learn', (5, 6)), ('to fit', (8, 10)), ('from', (12, 13))]","[('transformer', (7, 8)), ('each annotation', (10, 12)), ('landmarks', (14, 15))]","[['transformer', 'to fit', 'each annotation'], ['each annotation', 'from', 'landmarks']]",[],[],[],face_detection,7,247
2385,experiments,"Our proposed model can detect faces at various scales , including the green annotations provided in AFW as well as faces marked in red that are of small sizes and not labeled in the dataset ..","[('at', (6, 7)), ('including', (10, 11)), ('provided in', (14, 16)), ('as well as', (17, 20)), ('marked in', (21, 23)), ('of', (26, 27)), ('not', (30, 31))]","[('faces', (5, 6)), ('various scales', (7, 9)), ('green annotations', (12, 14)), ('AFW', (16, 17)), ('faces', (20, 21)), ('red', (23, 24)), ('small sizes', (27, 29))]","[['faces', 'at', 'various scales'], ['various scales', 'including', 'green annotations'], ['green annotations', 'provided in', 'AFW'], ['green annotations', 'as well as', 'faces'], ['faces', 'marked in', 'red']]",[],[],[],face_detection,7,249
2386,experiments,"The proposed algorithm ( Scale - forecast network with RSA + LRN , tagged by LRN + RSA ) outperforms other methods by a large margin .","[('tagged by', (13, 15)), ('by', (22, 23))]","[('proposed algorithm ( Scale - forecast network with', (1, 9)), ('RSA + LRN', (9, 12)), ('LRN + RSA )', (15, 19)), ('outperforms', (19, 20)), ('other methods', (20, 22)), ('large margin', (24, 26))]","[['proposed algorithm ( Scale - forecast network with', 'tagged by', 'LRN + RSA )'], ['proposed algorithm ( Scale - forecast network with', 'tagged by', 'outperforms'], ['outperforms', 'by', 'large margin'], ['other methods', 'by', 'large margin']]","[['proposed algorithm ( Scale - forecast network with', 'name', 'RSA + LRN'], ['LRN + RSA )', 'has', 'outperforms'], ['outperforms', 'has', 'other methods']]",[],[],face_detection,7,251
2387,experiments,RSA on Generic Object Proposal,[],[],[],[],[],[],face_detection,7,254
2388,experiments,We now verify that the scale approximation learning by RSA unit also generalizes comparably well on the generic region proposal task .,"[('verify', (2, 3)), ('by', (8, 9)), ('on', (15, 16))]","[('scale approximation learning', (5, 8)), ('RSA unit', (9, 11)), ('generalizes', (12, 13)), ('comparably well', (13, 15)), ('generic region proposal task', (17, 21))]","[['scale approximation learning', 'by', 'RSA unit'], ['comparably well', 'on', 'generic region proposal task']]","[['scale approximation learning', 'has', 'RSA unit'], ['generalizes', 'has', 'comparably well']]",[],[],face_detection,7,255
2389,experiments,ILSVRC DET is a challenging dataset for generic object detection .,[],"[('ILSVRC DET', (0, 2)), ('generic object detection', (7, 10))]",[],[],[],[],face_detection,7,257
2390,experiments,We choose the single anchor RPN with ResNet - 101 as the baseline .,"[('choose', (1, 2)), ('with', (6, 7))]","[('single anchor RPN', (3, 6)), ('ResNet - 101', (7, 10))]","[['single anchor RPN', 'with', 'ResNet - 101']]",[],[],[],face_detection,7,260
2391,experiments,"The anchors are of size 128 ? 2 squared , 128256 and 256128 .","[('of size', (3, 5))]","[('anchors', (1, 2)), ('128 ? 2 squared', (5, 9)), ('128256', (10, 11)), ('256128', (12, 13))]","[['anchors', 'of size', '128 ? 2 squared'], ['anchors', 'of size', '256128']]",[],[],[],face_detection,7,262
2392,experiments,Scale - forecast network is also employed to predict the higher dimension of objects in the image .,"[('in', (14, 15))]","[('Scale - forecast network', (0, 4)), ('higher dimension of objects', (10, 14)), ('image', (16, 17))]","[['higher dimension of objects', 'in', 'image']]",[],[],[],face_detection,7,264
2393,experiments,"Without loss of recall , RPN + RSA reduces around 61.05 % computation cost compared with the single - scale RPN , when the number of boxes is over 100 .","[('Without loss of', (0, 3)), ('reduces around', (8, 10)), ('compared with', (14, 16)), ('when', (22, 23)), ('is', (27, 28))]","[('recall', (3, 4)), ('RPN + RSA', (5, 8)), ('61.05 % computation cost', (10, 14)), ('single - scale RPN', (17, 21)), ('number of boxes', (24, 27)), ('over 100', (28, 30))]","[['RPN + RSA', 'reduces around', '61.05 % computation cost'], ['61.05 % computation cost', 'compared with', 'single - scale RPN'], ['single - scale RPN', 'when', 'number of boxes'], ['number of boxes', 'is', 'over 100']]","[['recall', 'has', 'RPN + RSA'], ['number of boxes', 'has', 'over 100']]",[],[],face_detection,7,267
2394,experiments,RPN + RSA is also more efficient and recalls more objects than original RPN .,"[('recalls', (8, 9)), ('than', (11, 12))]","[('RPN + RSA', (0, 3)), ('more efficient', (5, 7)), ('more objects', (9, 11)), ('original RPN', (12, 14))]","[['RPN + RSA', 'recalls', 'more objects'], ['more objects', 'than', 'original RPN']]","[['RPN + RSA', 'has', 'more efficient']]",[],[],face_detection,7,268
2395,experiments,Our model and the single - anchor RPN both perform better than the original RPN .,"[('perform', (9, 10)), ('than', (11, 12))]","[('Our model and the single - anchor RPN', (0, 8)), ('better', (10, 11)), ('original RPN', (13, 15))]","[['Our model and the single - anchor RPN', 'perform', 'better'], ['better', 'than', 'original RPN']]",[],[],[],face_detection,7,269
2396,experiments,"Overall , our scheme of using RSA plus LRN competes comparably with the standard RPN method in terms of computation efficiency and accuracy .","[('competes', (9, 10)), ('in terms of', (16, 19))]","[('our scheme of', (2, 5)), ('RSA plus LRN', (6, 9)), ('comparably', (10, 11)), ('standard RPN method', (13, 16)), ('computation efficiency and accuracy', (19, 23))]","[['RSA plus LRN', 'competes', 'comparably'], ['standard RPN method', 'in terms of', 'computation efficiency and accuracy']]",[],[],[],face_detection,7,271
2397,research-problem,Detecting Faces Using Region - based Fully Convolutional Networks,[],"[('Detecting Faces', (0, 2))]",[],[],[],[],face_detection,8,2
2398,research-problem,"In this report , we propose a region - based face detector applying deep networks in a fully convolutional fashion , named Face R - FCN .","[('propose', (5, 6)), ('applying', (12, 13)), ('in', (15, 16)), ('named', (21, 22))]","[('region - based face detector', (7, 12)), ('deep networks', (13, 15)), ('fully convolutional fashion', (17, 20)), ('Face R - FCN', (22, 26))]","[['region - based face detector', 'applying', 'deep networks'], ['deep networks', 'in', 'fully convolutional fashion'], ['deep networks', 'named', 'Face R - FCN'], ['fully convolutional fashion', 'named', 'Face R - FCN']]",[],"[['Research problem', 'propose', 'region - based face detector']]",[],face_detection,8,5
2399,research-problem,"Based on Region - based Fully Convolutional Networks ( R - FCN ) , our face detector is more accurate and computationally efficient compared with the previous R - CNN based face detectors .","[('Based on', (0, 2)), ('is', (17, 18)), ('compared with', (23, 25))]","[('Region - based Fully Convolutional Networks ( R - FCN )', (2, 13)), ('our face detector', (14, 17)), ('more accurate', (18, 20)), ('previous R - CNN based face detectors', (26, 33))]","[['our face detector', 'is', 'more accurate']]","[['Region - based Fully Convolutional Networks ( R - FCN )', 'has', 'our face detector'], ['our face detector', 'has', 'more accurate']]","[['Research problem', 'Based on', 'Region - based Fully Convolutional Networks ( R - FCN )']]",[],face_detection,8,6
2400,research-problem,Face detection plays an important role in the modern face - relevant applications .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,8,11
2401,approach,"The ConvNet of R - FCN is built with the computations shared on the entire image , which leads to the improvement of training and testing efficiency .","[('built with', (7, 9)), ('shared on', (11, 13)), ('leads to', (18, 20)), ('of', (22, 23))]","[('ConvNet of R - FCN', (1, 6)), ('computations', (10, 11)), ('entire image', (14, 16)), ('improvement', (21, 22)), ('training and testing efficiency', (23, 27))]","[['ConvNet of R - FCN', 'built with', 'computations'], ['computations', 'shared on', 'entire image'], ['computations', 'leads to', 'improvement'], ['improvement', 'of', 'training and testing efficiency']]","[['ConvNet of R - FCN', 'has', 'computations']]",[],"[['Approach', 'has', 'ConvNet of R - FCN']]",face_detection,8,18
2402,approach,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .","[('develop', (5, 6)), ('on the top of', (9, 13)), ('with', (16, 17)), ('of', (19, 20)), ('achieves', (24, 25)), ('than', (28, 29))]","[('face detector', (7, 9)), ('R - FCN', (13, 16)), ('elaborate design', (17, 19)), ('details', (21, 22)), ('more decent performance', (25, 28))]","[['face detector', 'on the top of', 'R - FCN'], ['R - FCN', 'with', 'elaborate design'], ['elaborate design', 'of', 'details'], ['face detector', 'achieves', 'more decent performance']]","[['elaborate design', 'has', 'details']]","[['Approach', 'develop', 'face detector']]",[],face_detection,8,22
2403,approach,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .","[('introduce', (12, 13)), ('to generate', (19, 21)), ('for enhancing', (23, 25)), ('eliminate', (28, 29)), ('in', (34, 35))]","[('position - sensitive average pooling', (14, 19)), ('embedding features', (21, 23)), ('discrimination', (25, 26)), ('effect of non-uniformed contribution', (30, 34)), ('each facial part', (35, 38))]","[['position - sensitive average pooling', 'to generate', 'embedding features'], ['embedding features', 'for enhancing', 'discrimination'], ['position - sensitive average pooling', 'eliminate', 'effect of non-uniformed contribution'], ['effect of non-uniformed contribution', 'in', 'each facial part']]",[],"[['Approach', 'introduce', 'position - sensitive average pooling']]",[],face_detection,8,24
2404,approach,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,"[('integrated into', (12, 14)), ('for boosting', (18, 20)), ('on', (22, 23))]","[('on - line hard example mining ( OHEM ) technique', (1, 11)), ('our network', (14, 16)), ('learning', (21, 22)), ('hard examples', (23, 25))]","[['on - line hard example mining ( OHEM ) technique', 'integrated into', 'our network'], ['on - line hard example mining ( OHEM ) technique', 'for boosting', 'learning'], ['learning', 'on', 'hard examples']]",[],[],"[['Approach', 'has', 'on - line hard example mining ( OHEM ) technique']]",face_detection,8,26
2405,approach,"The proposed approach is based on R - FCN and is well suited for face detection , thus we call it Face R - FCN .","[('based on', (4, 6)), ('well suited for', (11, 14)), ('call it', (19, 21))]","[('R - FCN', (6, 9)), ('face detection', (14, 16)), ('Face R - FCN', (21, 25))]","[['face detection', 'call it', 'Face R - FCN']]",[],"[['Approach', 'based on', 'R - FCN']]",[],face_detection,8,29
2406,hyperparameters,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .","[('initialize', (8, 9)), ('with', (11, 12)), ('of', (15, 16)), ('trained on', (20, 22))]","[('our network', (9, 11)), ('pre-trained weights', (13, 15)), ('101 - layer ResNet', (16, 20)), ('Image Net', (22, 24))]","[['our network', 'with', 'pre-trained weights'], ['pre-trained weights', 'of', '101 - layer ResNet'], ['101 - layer ResNet', 'trained on', 'Image Net']]",[],"[['Hyperparameters', 'initialize', 'our network']]",[],face_detection,8,108
2407,hyperparameters,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .","[('at', (12, 13)), ('of', (16, 17)), ('throughout', (20, 21)), ('trained on', (33, 35))]","[('freeze', (3, 4)), ('general kernels ( weights of', (5, 10)), ('few layers', (10, 12)), ('beginning )', (14, 16)), ('pre-trained model', (18, 20)), ('entire training process', (22, 25)), ('essential feature extractor', (30, 33)), ('ImageNet', (35, 36))]","[['few layers', 'at', 'beginning )'], ['beginning )', 'of', 'pre-trained model'], ['pre-trained model', 'throughout', 'entire training process'], ['essential feature extractor', 'trained on', 'ImageNet']]","[['freeze', 'has', 'general kernels ( weights of'], ['general kernels ( weights of', 'has', 'few layers']]",[],"[['Hyperparameters', 'has', 'freeze']]",face_detection,8,109
2408,baselines,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .","[('In terms of', (0, 3)), ('enumerates', (11, 12)), ('of', (14, 15)), ('to accurately search for', (19, 23))]","[('RPN stage', (4, 6)), ('Face R - FCN', (7, 11)), ('multiple configurations', (12, 14)), ('anchor', (16, 17)), ('faces', (23, 24))]","[['Face R - FCN', 'enumerates', 'multiple configurations'], ['multiple configurations', 'of', 'anchor'], ['multiple configurations', 'to accurately search for', 'faces']]","[['RPN stage', 'has', 'Face R - FCN']]","[['Baselines', 'In terms of', 'RPN stage']]",[],face_detection,8,110
2409,hyperparameters,We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,"[('combine', (1, 2)), ('of', (4, 5)), ('to construct', (11, 13))]","[('range', (3, 4)), ('multi-scale anchors', (13, 15))]",[],[],"[['Hyperparameters', 'combine', 'range']]",[],face_detection,8,111
2410,baselines,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,"[('learned jointly with', (8, 11))]","[('RPN and R - FCN', (1, 6)), ('softmax loss', (12, 14)), ('smooth L1 loss', (16, 19))]","[['RPN and R - FCN', 'learned jointly with', 'softmax loss'], ['RPN and R - FCN', 'learned jointly with', 'smooth L1 loss']]",[],[],"[['Baselines', 'has', 'RPN and R - FCN']]",face_detection,8,115
2411,hyperparameters,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,"[('with', (12, 13))]","[('Non- maximum suppression ( NMS )', (0, 6)), ('anchors', (11, 12)), ('certain IoU scores', (13, 16))]","[['anchors', 'with', 'certain IoU scores']]",[],[],"[['Hyperparameters', 'has', 'Non- maximum suppression ( NMS )']]",face_detection,8,116
2412,hyperparameters,The proposals are processed by OHEM to train with hard examples .,"[('processed by', (3, 5)), ('to train with', (6, 9))]","[('proposals', (1, 2)), ('OHEM', (5, 6)), ('hard examples', (9, 11))]","[['proposals', 'processed by', 'OHEM'], ['OHEM', 'to train with', 'hard examples']]",[],[],"[['Hyperparameters', 'has', 'proposals']]",face_detection,8,117
2413,hyperparameters,We set the 256 for the size of RPN mini-batch and 128 for R - FCN respectively .,"[('set', (1, 2)), ('for', (4, 5)), ('for', (12, 13))]","[('256', (3, 4)), ('size', (6, 7)), ('RPN mini-batch', (8, 10)), ('128', (11, 12)), ('R - FCN', (13, 16))]","[['256', 'for', 'size'], ['256', 'for', '128'], ['256', 'for', 'R - FCN'], ['128', 'for', 'R - FCN'], ['128', 'for', 'R - FCN']]",[],"[['Hyperparameters', 'set', '256']]",[],face_detection,8,118
2414,hyperparameters,"We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .","[('utilize', (1, 2)), ('where', (4, 5)), ('resized with', (9, 11)), ('to', (13, 14))]","[('multi-scale training', (2, 4)), ('input image', (6, 8)), ('bilinear interpolation', (11, 13)), ('various scales', (14, 16))]","[['multi-scale training', 'where', 'input image'], ['input image', 'resized with', 'bilinear interpolation'], ['bilinear interpolation', 'to', 'various scales']]",[],"[['Hyperparameters', 'utilize', 'multi-scale training']]",[],face_detection,8,120
2415,hyperparameters,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .","[('In', (0, 1)), ('performed by', (8, 10)), ('into', (12, 13)), ('for', (16, 17)), ('on', (19, 20))]","[('testing stage', (2, 4)), ('multi-scale testing', (5, 7)), ('scale image', (10, 12)), ('image pyramid', (14, 16)), ('better detecting', (17, 19)), ('both tiny and general faces', (20, 25))]","[['multi-scale testing', 'performed by', 'scale image'], ['scale image', 'into', 'image pyramid'], ['image pyramid', 'for', 'better detecting'], ['better detecting', 'on', 'both tiny and general faces']]","[['testing stage', 'has', 'multi-scale testing']]","[['Hyperparameters', 'In', 'testing stage']]",[],face_detection,8,121
2416,results,"As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .","[('consistently wins', (7, 9)), ('across', (12, 13)), ('on', (16, 17)), ('of', (24, 25))]","[('our proposed approach', (4, 7)), ('1st place', (10, 12)), ('three subsets', (14, 16)), ('validation set and test set', (19, 24)), ('WIDER FACE', (25, 27)), ('significantly outperforms', (28, 30)), ('existing results', (31, 33))]","[['our proposed approach', 'consistently wins', '1st place'], ['1st place', 'across', 'three subsets'], ['three subsets', 'on', 'validation set and test set'], ['validation set and test set', 'of', 'WIDER FACE']]","[['significantly outperforms', 'has', 'existing results']]",[],"[['Results', 'has', 'our proposed approach']]",face_detection,8,125
2417,results,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .","[('on', (3, 4)), ('superior to', (12, 14)), ('performing one by', (18, 21)), ('demonstrates', (26, 27))]","[('WIDER FACE hard subset', (4, 8)), ('our approach', (9, 11)), ('prior best', (15, 17)), ('clear margin', (22, 24))]","[['our approach', 'superior to', 'prior best'], ['our approach', 'performing one by', 'clear margin']]","[['WIDER FACE hard subset', 'has', 'our approach']]","[['Results', 'on', 'WIDER FACE hard subset']]",[],face_detection,8,126
2418,experiments,We use the training set of the WIDER FACE dataset to train our model ( denoted as Model - A in ) and compare against the recently published top approaches on FDDB .,"[('use', (1, 2)), ('of', (5, 6)), ('to train', (10, 12)), ('on', (30, 31))]","[('training set', (3, 5)), ('WIDER FACE dataset', (7, 10)), ('our model', (12, 14)), ('FDDB', (31, 32))]","[['training set', 'of', 'WIDER FACE dataset'], ['WIDER FACE dataset', 'to train', 'our model']]",[],[],[],face_detection,8,130
2419,experiments,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .","[('consistently achieves', (10, 12)), ('in terms of both', (15, 19))]","[('Face R - FCN', (6, 10)), ('impressive performance', (13, 15)), ('discrete ROC curve', (20, 23)), ('continuous ROC curve', (24, 27))]","[['Face R - FCN', 'consistently achieves', 'impressive performance'], ['impressive performance', 'in terms of both', 'discrete ROC curve'], ['impressive performance', 'in terms of both', 'continuous ROC curve']]",[],[],[],face_detection,8,133
2420,experiments,Our discrete ROC curve is superior to the prior best - performing method .,"[('is', (4, 5)), ('to', (6, 7))]","[('discrete ROC curve', (1, 4)), ('superior', (5, 6)), ('prior best - performing method', (8, 13))]","[['discrete ROC curve', 'is', 'superior'], ['superior', 'to', 'prior best - performing method']]","[['discrete ROC curve', 'has', 'superior']]",[],[],face_detection,8,134
2421,experiments,We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,"[('obtain', (2, 3)), ('of', (8, 9)), ('at', (13, 14))]","[('best true positive rate', (4, 8)), ('discrete ROC curve', (10, 13)), ('1000/2000 false positives', (14, 17))]","[['best true positive rate', 'of', 'discrete ROC curve'], ['best true positive rate', 'at', '1000/2000 false positives'], ['discrete ROC curve', 'at', '1000/2000 false positives']]",[],[],[],face_detection,8,135
2422,experiments,But the competitive result we achieved is still noticeable .,"[('achieved', (5, 6))]","[('competitive result', (2, 4)), ('noticeable', (8, 9))]","[['competitive result', 'achieved', 'noticeable']]","[['competitive result', 'has', 'noticeable']]",[],[],face_detection,8,139
2423,experiments,"Face R - FCN shows the superior performance over the prior methods across the three subsets ( easy , medium and hard ) in both validation and test sets .","[('shows', (4, 5)), ('across', (12, 13)), ('in', (23, 24))]","[('Face R - FCN', (0, 4)), ('superior performance', (6, 8)), ('prior methods', (10, 12)), ('three subsets', (14, 16)), ('validation and test sets', (25, 29))]","[['Face R - FCN', 'shows', 'superior performance'], ['prior methods', 'across', 'three subsets'], ['three subsets', 'in', 'validation and test sets']]","[['Face R - FCN', 'has', 'superior performance']]",[],[],face_detection,8,141
2424,results,"As expected , the performance of Face R - FCN is further improved .","[('of', (5, 6))]","[('performance', (4, 5)), ('Face R - FCN', (6, 10)), ('further improved', (11, 13))]","[['performance', 'of', 'Face R - FCN']]","[['performance', 'has', 'Face R - FCN']]",[],[],face_detection,8,146
2425,research-problem,Finding Tiny Faces,[],[],[],[],[],[],face_detection,9,2
2426,research-problem,"We describe a detector that can find around 800 faces out of the reportedly 1000 present , by making use of novel characterizations of scale , resolution , and context to find small objects .","[('describe', (1, 2)), ('can find', (5, 7)), ('of', (11, 12)), ('by making use of', (17, 21)), ('of', (23, 24)), ('to find', (30, 32))]","[('detector', (3, 4)), ('around 800 faces out', (7, 11)), ('reportedly 1000 present', (13, 16)), ('novel characterizations', (21, 23)), ('scale , resolution , and context', (24, 30)), ('small objects', (32, 34))]","[['detector', 'can find', 'around 800 faces out'], ['around 800 faces out', 'of', 'reportedly 1000 present'], ['novel characterizations', 'of', 'scale , resolution , and context'], ['novel characterizations', 'of', 'scale , resolution , and context'], ['scale , resolution , and context', 'to find', 'small objects']]",[],"[['Research problem', 'describe', 'detector']]",[],face_detection,9,5
2427,approach,We make use of a coarse image pyramid to capture extreme scale challenges in ( c ) .,"[('use of', (2, 4)), ('to capture', (8, 10))]","[('coarse image pyramid', (5, 8)), ('extreme scale challenges', (10, 13))]","[['coarse image pyramid', 'to capture', 'extreme scale challenges']]",[],"[['Approach', 'use of', 'coarse image pyramid']]",[],face_detection,9,23
2428,approach,"Finally , to improve performance on small faces , we model additional context , which is efficiently implemented as a fixed - size receptive field across all scale - specific templates ( d ) .","[('to improve', (2, 4)), ('on', (5, 6)), ('model', (10, 11)), ('efficiently implemented as', (16, 19)), ('across', (25, 26))]","[('performance', (4, 5)), ('small faces', (6, 8)), ('additional context', (11, 13)), ('fixed - size receptive field', (20, 25)), ('all scale - specific templates', (26, 31))]","[['performance', 'on', 'small faces'], ['performance', 'model', 'additional context'], ['small faces', 'model', 'additional context'], ['additional context', 'efficiently implemented as', 'fixed - size receptive field'], ['fixed - size receptive field', 'across', 'all scale - specific templates']]",[],"[['Approach', 'to improve', 'performance']]",[],face_detection,9,24
2429,research-problem,Multi - task modeling of scales :,[],"[('Multi - task modeling of scales', (0, 6))]",[],[],[],[],face_detection,9,29
2430,approach,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .","[('Instead of', (0, 2)), ('train', (11, 12)), ('tuned for', (14, 16))]","[('"" one-size - fitsall "" approach', (3, 9)), ('separate detectors', (12, 14)), ('different scales ( and aspect ratios )', (16, 23))]","[['"" one-size - fitsall "" approach', 'train', 'separate detectors'], ['separate detectors', 'tuned for', 'different scales ( and aspect ratios )']]",[],"[['Approach', 'Instead of', '"" one-size - fitsall "" approach']]",[],face_detection,9,33
2431,approach,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .","[('train and run', (6, 9)), ('in', (13, 14)), ('make use of', (19, 22)), ('defined over', (23, 25)), ('of', (27, 28))]","[('scale - specific detectors', (9, 13)), ('multitask fashion', (15, 17)), ('features', (22, 23)), ('multiple layers', (25, 27)), ('single ( deep ) feature hierarchy', (28, 34))]","[['scale - specific detectors', 'in', 'multitask fashion'], ['scale - specific detectors', 'make use of', 'features'], ['multitask fashion', 'make use of', 'features'], ['features', 'defined over', 'multiple layers'], ['multiple layers', 'of', 'single ( deep ) feature hierarchy']]",[],"[['Approach', 'train and run', 'scale - specific detectors']]",[],face_detection,9,35
2432,approach,"In , we present a simple human experiment where users attempt to classify true and false positive faces ( as given by our detector ) .","[('attempt to', (10, 12))]","[('users', (9, 10)), ('classify', (12, 13)), ('true and false positive faces', (13, 18))]","[['users', 'attempt to', 'classify']]","[['classify', 'has', 'true and false positive faces']]",[],[],face_detection,9,48
2433,approach,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .","[('demonstrate', (1, 2)), ('extracted from', (6, 8)), ('known as', (12, 14)), ('are', (19, 20)), ('that capture', (25, 27)), ('across', (38, 39))]","[('convolutional deep features', (3, 6)), ('multiple layers', (8, 10)), ('hypercolumn "" features', (15, 18)), ('effective "" foveal "" descriptors', (20, 25)), ('high - resolution detail', (28, 32)), ('coarse low - resolution cues', (33, 38)), ('large receptive field ( )', (39, 44))]","[['convolutional deep features', 'extracted from', 'multiple layers'], ['convolutional deep features', 'known as', 'hypercolumn "" features'], ['multiple layers', 'known as', 'hypercolumn "" features'], ['convolutional deep features', 'are', 'effective "" foveal "" descriptors'], ['multiple layers', 'are', 'effective "" foveal "" descriptors'], ['effective "" foveal "" descriptors', 'that capture', 'high - resolution detail'], ['effective "" foveal "" descriptors', 'that capture', 'coarse low - resolution cues'], ['coarse low - resolution cues', 'across', 'large receptive field ( )']]",[],"[['Approach', 'demonstrate', 'convolutional deep features']]",[],face_detection,9,52
2434,approach,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,"[('show', (1, 2)), ('of', (5, 6)), ('extracted from', (10, 12))]","[('highresolution components', (3, 5)), ('our foveal descriptors', (6, 9)), ('lower convolutional layers', (12, 15))]","[['highresolution components', 'of', 'our foveal descriptors'], ['our foveal descriptors', 'extracted from', 'lower convolutional layers']]",[],"[['Approach', 'show', 'highresolution components']]",[],face_detection,9,53
2435,approach,"In particular , when compared to prior art on WIDER FACE , our results reduce error by a factor of 2 ( our models produce an AP of 82 % while prior art ranges from 29 - 64 % ) . :","[('compared to', (4, 6)), ('on', (8, 9)), ('reduce', (14, 15)), ('by', (16, 17))]","[('prior art', (6, 8)), ('WIDER FACE', (9, 11)), ('our results', (12, 14)), ('error', (15, 16)), ('factor of', (18, 20)), ('2', (20, 21))]","[['prior art', 'on', 'WIDER FACE'], ['our results', 'reduce', 'error'], ['error', 'by', 'factor of']]","[['prior art', 'has', 'WIDER FACE'], ['factor of', 'has', '2']]","[['Approach', 'compared to', 'prior art']]",[],face_detection,9,57
2436,experiments,Adding a fixed contextual window of 300 pixels dramatically reduces error on small faces by 20 % .,"[('Adding', (0, 1)), ('of', (5, 6)), ('dramatically reduces', (8, 10)), ('on', (11, 12)), ('by', (14, 15))]","[('fixed contextual window', (2, 5)), ('300 pixels', (6, 8)), ('error', (10, 11)), ('small faces', (12, 14)), ('20 %', (15, 17))]","[['fixed contextual window', 'of', '300 pixels'], ['fixed contextual window', 'dramatically reduces', 'error'], ['300 pixels', 'dramatically reduces', 'error'], ['error', 'on', 'small faces'], ['small faces', 'by', '20 %']]",[],[],[],face_detection,9,62
2437,hyperparameters,"We use a fixed learning rate of 10 ? 4 , a weight decay of 0.0005 , and a momentum of 0.9 .","[('use', (1, 2)), ('of', (6, 7))]","[('fixed learning rate', (3, 6)), ('10 ? 4', (7, 10)), ('weight decay', (12, 14)), ('0.0005', (15, 16)), ('momentum', (19, 20)), ('0.9', (21, 22))]","[['fixed learning rate', 'of', '10 ? 4'], ['weight decay', 'of', '0.0005'], ['momentum', 'of', '0.9']]",[],"[['Hyperparameters', 'use', 'fixed learning rate']]",[],face_detection,9,209
2438,results,"As shows , our hybrid - resolution model ( HR ) achieves state - of - the - art performance on all difficulty levels , but most importantly , reduces error on the "" hard "" set by 2X .","[('achieves', (11, 12)), ('on', (20, 21)), ('reduces', (29, 30)), ('on', (31, 32)), ('by', (37, 38))]","[('our hybrid - resolution model ( HR )', (3, 11)), ('state - of - the - art performance', (12, 20)), ('all difficulty levels', (21, 24)), ('error', (30, 31)), ('"" hard "" set', (33, 37)), ('2X', (38, 39))]","[['our hybrid - resolution model ( HR )', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', 'all difficulty levels'], ['error', 'on', '"" hard "" set'], ['our hybrid - resolution model ( HR )', 'reduces', 'error'], ['error', 'on', '"" hard "" set'], ['error', 'by', '2X'], ['"" hard "" set', 'by', '2X']]",[],[],"[['Results', 'has', 'our hybrid - resolution model ( HR )']]",face_detection,9,220
2439,results,"Our out - of - the - box detector ( HR ) outperforms all published results on the discrete score , which uses a standard 50 % intersection - over - union threshold to define correctness .","[('outperforms', (12, 13)), ('on', (16, 17)), ('uses', (22, 23)), ('to define', (33, 35))]","[('Our out - of - the - box detector ( HR )', (0, 12)), ('all published results', (13, 16)), ('discrete score', (18, 20)), ('standard 50 % intersection - over - union threshold', (24, 33)), ('correctness', (35, 36))]","[['Our out - of - the - box detector ( HR )', 'outperforms', 'all published results'], ['all published results', 'on', 'discrete score'], ['discrete score', 'uses', 'standard 50 % intersection - over - union threshold'], ['standard 50 % intersection - over - union threshold', 'to define', 'correctness']]",[],[],[],face_detection,9,226
2440,hyperparameters,Our regressor is trained with 10 - fold cross validation .,"[('trained with', (3, 5))]","[('Our regressor', (0, 2)), ('10 - fold cross validation', (5, 10))]","[['Our regressor', 'trained with', '10 - fold cross validation']]",[],[],"[['Hyperparameters', 'has', 'Our regressor']]",face_detection,9,229
2441,results,Our Resnet 101 - based detector runs at 1.4 FPS on 1080 p resolution and 3.1 FPS on 720 p resolution .,"[('runs at', (6, 8)), ('on', (10, 11)), ('on', (17, 18))]","[('Resnet 101 - based detector', (1, 6)), ('1.4 FPS', (8, 10)), ('1080 p resolution', (11, 14)), ('3.1 FPS', (15, 17)), ('720 p resolution', (18, 21))]","[['Resnet 101 - based detector', 'runs at', '1.4 FPS'], ['Resnet 101 - based detector', 'runs at', '3.1 FPS'], ['1.4 FPS', 'on', '1080 p resolution'], ['3.1 FPS', 'on', '720 p resolution'], ['3.1 FPS', 'on', '720 p resolution']]",[],[],"[['Results', 'has', 'Resnet 101 - based detector']]",face_detection,9,234
2442,baselines,"We propose a simple yet effective framework for finding small objects , demonstrating that both large context and scale - variant representations are crucial .",[],"[('finding small objects', (8, 11))]",[],[],[],[],face_detection,9,238
2443,ablation-analysis,We specifically show that massively - large receptive fields can be effectively encoded as a foveal descriptor that captures both coarse context ( necessary for detecting small objects ) and high - resolution image features ( helpful for localizing small objects ) .,"[('show', (2, 3)), ('can be', (9, 11)), ('captures', (18, 19))]","[('massively - large receptive fields', (4, 9)), ('effectively encoded', (11, 13)), ('foveal descriptor', (15, 17)), ('coarse context', (20, 22)), ('high - resolution image features', (30, 35))]","[['massively - large receptive fields', 'can be', 'effectively encoded'], ['foveal descriptor', 'captures', 'coarse context'], ['foveal descriptor', 'captures', 'high - resolution image features']]","[['massively - large receptive fields', 'has', 'effectively encoded']]","[['Ablation analysis', 'show', 'massively - large receptive fields']]",[],face_detection,9,239
2444,ablation-analysis,"We also explore the encoding of scale in existing pre-trained deep networks , suggesting a simple way to extrapolate networks tuned for limited scales to more extreme scenarios in a scale - variant fashion .","[('explore', (2, 3)), ('in', (7, 8)), ('tuned for', (20, 22))]","[('encoding of scale', (4, 7)), ('existing pre-trained deep networks', (8, 12))]","[['encoding of scale', 'in', 'existing pre-trained deep networks']]",[],"[['Ablation analysis', 'explore', 'encoding of scale']]",[],face_detection,9,240
2445,results,"By learning a post - hoc regressor that converts bounding boxes to ellipses , our approach ( HR - ER ) produces state - of the - art continuous overlaps as well ( right ) .","[('learning', (1, 2)), ('that converts', (7, 9)), ('to', (11, 12)), ('produces', (21, 22))]","[('post - hoc regressor', (3, 7)), ('bounding boxes', (9, 11)), ('ellipses', (12, 13)), ('our approach ( HR - ER )', (14, 21)), ('state - of the - art continuous overlaps', (22, 30))]","[['post - hoc regressor', 'that converts', 'bounding boxes'], ['bounding boxes', 'to', 'ellipses'], ['our approach ( HR - ER )', 'produces', 'state - of the - art continuous overlaps']]",[],[],[],face_detection,9,244
2446,results,"Our proposed detector is able to detect faces at a continuous range of scales , while being robust to challenges such as expression , blur , illumination etc .","[('able to detect', (4, 7)), ('at', (8, 9)), ('such as', (20, 22))]","[('proposed detector', (1, 3)), ('faces', (7, 8)), ('continuous range of scales', (10, 14)), ('robust', (17, 18)), ('challenges', (19, 20)), ('expression', (22, 23)), ('blur', (24, 25)), ('illumination', (26, 27))]","[['proposed detector', 'able to detect', 'faces'], ['faces', 'at', 'continuous range of scales'], ['challenges', 'such as', 'expression'], ['challenges', 'such as', 'blur'], ['challenges', 'such as', 'illumination']]",[],[],"[['Results', 'has', 'proposed detector']]",face_detection,9,248
2447,experiments,Online hard mining and balanced sampling,[],[],[],[],[],[],face_detection,9,269
2448,experiments,We set a small threshold ( 0.03 ) on classification loss to filter out easy locations .,"[('set', (1, 2)), ('on', (8, 9)), ('to filter', (11, 13))]","[('small threshold ( 0.03 )', (3, 8)), ('classification loss', (9, 11)), ('out', (13, 14)), ('easy locations', (14, 16))]","[['small threshold ( 0.03 )', 'on', 'classification loss'], ['classification loss', 'to filter', 'out']]","[['out', 'has', 'easy locations']]",[],[],face_detection,9,272
2449,results,Our detector is mostly affected by object scale ( from 0.044 to 0.896 ) and blur ( from 0.259 to 0.798 ) .,"[('mostly affected by', (3, 6))]","[('Our detector', (0, 2)), ('object scale', (6, 8)), ('from 0.044 to 0.896', (9, 13)), ('blur', (15, 16))]","[['Our detector', 'mostly affected by', 'object scale'], ['Our detector', 'mostly affected by', 'blur']]","[['object scale', 'has', 'from 0.044 to 0.896']]",[],"[['Results', 'has', 'Our detector']]",face_detection,9,277
2450,research-problem,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,[],"[('Unsupervised Hypernym Discovery', (13, 16))]",[],[],[],[],hypernym_discovery,0,2
2451,research-problem,This paper describes a simple but competitive unsupervised system for hypernym discovery .,[],"[('hypernym discovery', (10, 12))]",[],[],[],[],hypernym_discovery,0,4
2452,research-problem,"This shared task differs from recent taxonomy evaluation tasks by concentrating on Hypernym Discovery : the task of predicting ( discovering ) n hypernym candidates for a given input word , within the vocabulary of a specific domain .",[],"[('Hypernym Discovery', (12, 14))]",[],[],[],[],hypernym_discovery,0,16
2453,research-problem,There are several competing approaches for producing word embedding vectors .,"[('for', (5, 6))]","[('several competing approaches', (2, 5)), ('producing word embedding vectors', (6, 10))]","[['several competing approaches', 'for', 'producing word embedding vectors']]",[],[],[],hypernym_discovery,0,34
2454,results,Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 .,"[('ranked at', (3, 5)), ('on', (9, 10)), ('with', (14, 15)), ('of', (22, 23))]","[('eleven out of eighteen', (5, 9)), ('medical domain subtask', (11, 14)), ('Mean Average Precision ( MAP )', (16, 22)), ('8.13', (23, 24))]","[['eleven out of eighteen', 'on', 'medical domain subtask'], ['medical domain subtask', 'with', 'Mean Average Precision ( MAP )'], ['Mean Average Precision ( MAP )', 'of', '8.13']]",[],[],[],hypernym_discovery,0,61
2455,results,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .","[('On', (0, 1)), ('ranked', (9, 10)), ('with', (15, 16)), ('of', (18, 19)), ('ranking', (21, 22)), ('among', (23, 24))]","[('music industry domain subtask', (2, 6)), ('our system', (7, 9)), ('13th out', (10, 12)), ('16 places', (13, 15)), ('MAP', (17, 18)), ('1.88', (19, 20)), ('4th', (22, 23)), ('unsupervised systems', (25, 27))]","[['our system', 'ranked', '13th out'], ['16 places', 'with', 'MAP'], ['MAP', 'of', '1.88'], ['4th', 'among', 'unsupervised systems']]","[['music industry domain subtask', 'has', 'our system']]","[['Results', 'On', 'music industry domain subtask']]",[],hypernym_discovery,0,63
2456,research-problem,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings,[],"[('Neural Hypernym Discovery', (7, 10))]",[],[],[],[],hypernym_discovery,1,2
2457,research-problem,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .",[],"[('hypernym discovery', (4, 6))]",[],[],[],[],hypernym_discovery,1,4
2458,research-problem,"Various natural language processing ( NLP ) tasks , especially those semantically intensive ones aiming for inference and reasoning with generalization capability , such as question answering and textual entailment , can benefit from identifying semantic relations between words beyond synonymy .",[],[],[],[],[],[],hypernym_discovery,1,12
2459,research-problem,The hypernym discovery task aims to discover the most appropriate hypernym ( s ) for input concepts or entities from a pre-defined corpus .,[],"[('hypernym discovery', (1, 3))]",[],[],[],[],hypernym_discovery,1,13
2460,research-problem,"The other challenge is representation for terms , including words and phrases , where the phrase embedding could not be obtained byword embeddings directly .","[('including', (8, 9)), ('could not be obtained byword', (17, 22))]","[('representation for terms', (4, 7)), ('words and phrases', (9, 12)), ('phrase embedding', (15, 17))]","[['representation for terms', 'including', 'words and phrases']]",[],[],[],hypernym_discovery,1,17
2461,model,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .","[('introduce', (5, 6)), ('for', (10, 11)), ('empirically study', (15, 17)), ('to model', (20, 22)), ('for', (25, 26))]","[('neural network architecture', (7, 10)), ('concerned task', (12, 14)), ('various neural networks', (17, 20)), ('distributed representations', (23, 25)), ('words and phrases', (26, 29))]","[['neural network architecture', 'for', 'concerned task'], ['distributed representations', 'for', 'words and phrases'], ['various neural networks', 'to model', 'distributed representations'], ['distributed representations', 'for', 'words and phrases']]",[],"[['Model', 'introduce', 'neural network architecture']]",[],hypernym_discovery,1,23
2462,experimental-setup,Our model was implemented using the Theano 1 .,"[('implemented using', (3, 5))]","[('Our model', (0, 2)), ('Theano', (6, 7))]","[['Our model', 'implemented using', 'Theano']]",[],[],"[['Experimental setup', 'has', 'Our model']]",hypernym_discovery,1,83
2463,experimental-setup,The diagonal variant of Ada - Grad is used for neural network training .,"[('of', (3, 4)), ('used for', (8, 10))]","[('diagonal variant', (1, 3)), ('Ada - Grad', (4, 7)), ('neural network training', (10, 13))]","[['diagonal variant', 'of', 'Ada - Grad'], ['Ada - Grad', 'used for', 'neural network training']]",[],[],"[['Experimental setup', 'has', 'diagonal variant']]",hypernym_discovery,1,84
2464,experimental-setup,The hidden dimension of all neural models are 200 .,"[('of', (3, 4)), ('are', (7, 8))]","[('hidden dimension', (1, 3)), ('all neural models', (4, 7)), ('200', (8, 9))]","[['hidden dimension', 'of', 'all neural models'], ['all neural models', 'are', '200']]",[],[],"[['Experimental setup', 'has', 'hidden dimension']]",hypernym_discovery,1,87
2465,experimental-setup,The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,"[('set to', (4, 6)), ('set to', (16, 18))]","[('batch size', (1, 3)), ('20', (6, 7)), ('word embedding and sense embedding sizes', (9, 15)), ('300', (18, 19))]","[['batch size', 'set to', '20'], ['word embedding and sense embedding sizes', 'set to', '300']]","[['batch size', 'has', '20'], ['word embedding and sense embedding sizes', 'has', '300']]",[],"[['Experimental setup', 'has', 'batch size']]",hypernym_discovery,1,88
2466,experimental-setup,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .","[('trained on', (5, 7)), ('with', (17, 18)), ('for', (20, 21)), ('for', (25, 26)), ('for', (36, 37))]","[('single GPU ( NVIDIA GTX 980 Ti )', (8, 16)), ('roughly 1.5h', (18, 20)), ('general - purpose subtask', (21, 25)), ('English', (26, 27)), ('medical and music', (37, 40))]","[['single GPU ( NVIDIA GTX 980 Ti )', 'with', 'roughly 1.5h'], ['roughly 1.5h', 'for', 'general - purpose subtask'], ['general - purpose subtask', 'for', 'English']]",[],"[['Experimental setup', 'trained on', 'single GPU ( NVIDIA GTX 980 Ti )']]",[],hypernym_discovery,1,89
2467,results,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .","[('observe', (2, 3)), ('better than', (9, 11))]","[('CNN - based network performance', (3, 8)), ('RNN - based', (11, 14))]","[['CNN - based network performance', 'better than', 'RNN - based']]",[],[],[],hypernym_discovery,1,97
2468,results,"All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .","[('outperform', (4, 5)), ('in terms of', (8, 11)), ('performs', (20, 21)), ('than', (22, 23)), ('in', (27, 28)), ('using', (32, 33))]","[('All the neural models', (0, 4)), ('term embedding averaging', (5, 8)), ('all the metrics', (11, 14)), ('better', (21, 22)), ('RNN - based ones', (23, 27)), ('most of the metrics', (28, 32)), ('word embedding', (33, 35))]","[['All the neural models', 'outperform', 'term embedding averaging'], ['term embedding averaging', 'in terms of', 'all the metrics'], ['better', 'than', 'RNN - based ones'], ['RNN - based ones', 'in', 'most of the metrics'], ['RNN - based ones', 'using', 'word embedding'], ['most of the metrics', 'using', 'word embedding']]",[],[],"[['Results', 'has', 'All the neural models']]",hypernym_discovery,1,100
2469,results,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .","[('Compared with', (0, 2)), ('shows', (8, 9))]","[('word embedding', (2, 4)), ('sense embedding', (6, 8)), ('much poorer result', (10, 13))]","[['sense embedding', 'shows', 'much poorer result']]","[['word embedding', 'has', 'sense embedding'], ['sense embedding', 'has', 'much poorer result']]","[['Results', 'Compared with', 'word embedding']]",[],hypernym_discovery,1,101
2470,research-problem,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection,[],[],[],[],[],[],hypernym_discovery,2,2
2471,results,"The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .","[('show', (2, 3)), ('to', (4, 5))]","[('preference', (3, 4)), ('syntactic context - types ( dep and joint )', (6, 15))]","[['preference', 'to', 'syntactic context - types ( dep and joint )']]",[],"[['Results', 'show', 'preference']]",[],hypernym_discovery,2,131
2472,results,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .","[('in', (17, 18)), ('shown to', (34, 36))]","[('raw frequency', (11, 13)), ('successful', (16, 17)), ('hypernymy detection', (18, 20)), ('PPMI', (32, 33)), ('outperform', (36, 37))]","[['successful', 'in', 'hypernymy detection'], ['PPMI', 'shown to', 'outperform']]","[['raw frequency', 'has', 'successful']]",[],[],hypernym_discovery,2,132
2473,results,The inclusion hypothesis seems to be most effective in discriminating between hypernyms and meronyms under syntactic contexts .,"[('in', (8, 9)), ('under', (14, 15))]","[('inclusion hypothesis', (1, 3)), ('most effective', (6, 8)), ('discriminating', (9, 10)), ('hypernyms and meronyms', (11, 14)), ('syntactic contexts', (15, 17))]","[['most effective', 'in', 'discriminating'], ['hypernyms and meronyms', 'under', 'syntactic contexts']]","[['inclusion hypothesis', 'has', 'most effective']]",[],"[['Results', 'has', 'inclusion hypothesis']]",hypernym_discovery,2,143
2474,results,"For instance , on EVALution , SLQS performs worse ( ranked only as high as 13th ) , as this dataset has no such restriction on the basic level concepts , and may contain pairs like ( eye , animal ) .","[('on', (3, 4)), ('performs', (7, 8)), ('ranked only as', (10, 13))]","[('EVALution', (4, 5)), ('SLQS', (6, 7)), ('worse', (8, 9)), ('13th', (15, 16))]","[['SLQS', 'performs', 'worse']]","[['EVALution', 'has', 'SLQS']]",[],[],hypernym_discovery,2,158
2475,results,Hypernym vs. Attribute,[],[],[],[],[],[],hypernym_discovery,2,159
2476,results,"Hypernym vs. Synonym SLQS performs well also in discriminating between hypernyms and synonyms , in which y is also not more general than x .","[('performs', (4, 5)), ('in discriminating between', (7, 10))]","[('Hypernym vs. Synonym SLQS', (0, 4)), ('well', (5, 6)), ('hypernyms and synonyms', (10, 13))]","[['Hypernym vs. Synonym SLQS', 'performs', 'well'], ['well', 'in discriminating between', 'hypernyms and synonyms']]",[],[],"[['Results', 'has', 'Hypernym vs. Synonym SLQS']]",hypernym_discovery,2,168
2477,results,"We observed that in the joint context type , the difference in SLQS scores between synonyms and hypernyms was the largest .","[('observed', (1, 2)), ('in', (3, 4)), ('in', (11, 12)), ('between', (14, 15)), ('was', (18, 19))]","[('joint context type', (5, 8)), ('difference', (10, 11)), ('SLQS scores', (12, 14)), ('synonyms and hypernyms', (15, 18)), ('largest', (20, 21))]","[['difference', 'in', 'SLQS scores'], ['difference', 'in', 'SLQS scores'], ['SLQS scores', 'between', 'synonyms and hypernyms'], ['SLQS scores', 'was', 'largest'], ['synonyms and hypernyms', 'was', 'largest']]","[['joint context type', 'has', 'difference']]","[['Results', 'observed', 'joint context type']]",[],hypernym_discovery,2,169
2478,results,Hypernym vs. Coordination,[],[],[],[],[],[],hypernym_discovery,2,174
2479,results,"On Weeds , inclusion - based measures ( ClarkeDE , invCL and Weeds ) showed the best results .","[('On', (0, 1)), ('showed', (14, 15))]","[('Weeds', (1, 2)), ('inclusion - based measures', (3, 7)), ('ClarkeDE', (8, 9)), ('invCL', (10, 11)), ('Weeds', (12, 13)), ('best results', (16, 18))]","[['inclusion - based measures', 'showed', 'best results']]","[['Weeds', 'has', 'inclusion - based measures'], ['inclusion - based measures', 'name', 'ClarkeDE']]","[['Results', 'On', 'Weeds']]",[],hypernym_discovery,2,176
2480,results,"The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .","[('of', (4, 5)), ('is', (8, 9)), ('achieved using', (19, 21)), ('with', (24, 25))]","[('over all performance', (1, 4)), ('embeddingbased classifiers', (6, 8)), ('almost perfect', (9, 11)), ('best performance', (16, 18)), ('concatenation method', (22, 24)), ('GloVe', (26, 27)), ('dependency - based embeddings', (29, 33))]","[['over all performance', 'of', 'embeddingbased classifiers'], ['embeddingbased classifiers', 'is', 'almost perfect'], ['best performance', 'achieved using', 'concatenation method'], ['best performance', 'achieved using', 'dependency - based embeddings'], ['concatenation method', 'with', 'GloVe'], ['concatenation method', 'with', 'dependency - based embeddings']]",[],[],"[['Results', 'has', 'over all performance']]",hypernym_discovery,2,191
2481,results,"As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .","[('perform', (6, 7)), ('than', (8, 9))]","[('unsupervised measures', (4, 6)), ('worse', (7, 8)), ('embedding - based classifiers', (10, 14))]","[['unsupervised measures', 'perform', 'worse'], ['worse', 'than', 'embedding - based classifiers']]",[],[],[],hypernym_discovery,2,192
2482,research-problem,Supervised Distributional Hypernym Discovery via Domain Adaptation,[],"[('Supervised Distributional Hypernym Discovery', (0, 4))]",[],[],[],[],hypernym_discovery,3,2
2483,research-problem,"In this paper , we propose a supervised distributional framework for hypernym discovery which operates at the sense level , enabling large - scale automatic acquisition of dis ambiguated taxonomies .","[('propose', (5, 6)), ('for', (10, 11)), ('operates at', (14, 16)), ('enabling', (20, 21)), ('of', (26, 27))]","[('supervised distributional framework', (7, 10)), ('hypernym discovery', (11, 13)), ('large - scale automatic acquisition', (21, 26)), ('dis ambiguated taxonomies', (27, 30))]","[['supervised distributional framework', 'for', 'hypernym discovery'], ['hypernym discovery', 'enabling', 'large - scale automatic acquisition'], ['large - scale automatic acquisition', 'of', 'dis ambiguated taxonomies']]",[],"[['Research problem', 'propose', 'supervised distributional framework']]",[],hypernym_discovery,3,6
2484,research-problem,"By exploiting semantic regularities between hyponyms and hypernyms in embeddings spaces , and integrating a domain clustering algorithm , our model becomes sensitive to the target data .","[('exploiting', (1, 2)), ('between', (4, 5)), ('in', (8, 9)), ('integrating', (13, 14)), ('becomes', (21, 22))]","[('semantic regularities', (2, 4)), ('hyponyms and hypernyms', (5, 8)), ('embeddings spaces', (9, 11)), ('domain clustering algorithm', (15, 18)), ('our model', (19, 21)), ('sensitive', (22, 23)), ('target data', (25, 27))]","[['semantic regularities', 'between', 'hyponyms and hypernyms'], ['hyponyms and hypernyms', 'in', 'embeddings spaces'], ['semantic regularities', 'integrating', 'domain clustering algorithm'], ['our model', 'becomes', 'sensitive']]","[['domain clustering algorithm', 'has', 'our model']]","[['Research problem', 'exploiting', 'semantic regularities']]",[],hypernym_discovery,3,7
2485,research-problem,"By embedding cues about how we perceive concepts , and how these concepts generalize in a domain of knowledge , these resources bear a capacity for generalization that lies at the core of human cognition and have become key in Natural Language Processing ( NLP ) tasks where inference and reasoning have proved to be essential .","[('perceive', (6, 7))]",[],[],[],[],[],hypernym_discovery,3,12
2486,model,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .","[('propose', (4, 5)), ('based on', (12, 14)), ('easily applied to', (20, 23)), ('of', (25, 26))]","[('TAXOEMBED', (5, 6)), ('hypernym detection algorithm', (9, 12)), ('sense embeddings', (14, 16)), ('construction', (24, 25)), ('lexical taxonomies', (26, 28))]","[['hypernym detection algorithm', 'based on', 'sense embeddings'], ['hypernym detection algorithm', 'easily applied to', 'construction'], ['sense embeddings', 'easily applied to', 'construction'], ['construction', 'of', 'lexical taxonomies']]","[['TAXOEMBED', 'has', 'hypernym detection algorithm']]","[['Model', 'propose', 'TAXOEMBED']]",[],hypernym_discovery,3,25
2487,model,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .","[('designed to discover', (2, 5)), ('by exploiting', (7, 9)), ('in', (11, 12)), ('leverages', (20, 21)), ('to learn', (23, 25)), ('for', (30, 31))]","[('hypernymic relations', (5, 7)), ('linear transformations', (9, 11)), ('embedding spaces', (12, 14)), ('specific semanticallyaware transformation matrix', (26, 30)), ('each domain of knowledge', (31, 35))]","[['hypernymic relations', 'by exploiting', 'linear transformations'], ['linear transformations', 'in', 'embedding spaces'], ['specific semanticallyaware transformation matrix', 'for', 'each domain of knowledge']]",[],"[['Model', 'designed to discover', 'hypernymic relations']]",[],hypernym_discovery,3,26
2488,research-problem,"Compared to word - level taxonomy learning , TAXO - EMBED results in more refined and unambiguous hypernymic relations at the sense level , with a direct application in tasks such as semantic search .","[('Compared to', (0, 2)), ('results in', (11, 13)), ('at', (19, 20))]","[('word - level taxonomy learning', (2, 7)), ('TAXO - EMBED', (8, 11)), ('more refined and unambiguous hypernymic relations', (13, 19)), ('sense level', (21, 23))]","[['word - level taxonomy learning', 'results in', 'more refined and unambiguous hypernymic relations'], ['TAXO - EMBED', 'results in', 'more refined and unambiguous hypernymic relations'], ['more refined and unambiguous hypernymic relations', 'at', 'sense level']]","[['word - level taxonomy learning', 'has', 'TAXO - EMBED']]","[['Research problem', 'Compared to', 'word - level taxonomy learning']]",[],hypernym_discovery,3,32
2489,baselines,"KB - UNIFY 6 ( Delli Bovi et al. , 2015 a ) ( KB - U ) is a knowledge - based approach , based on BabelNet , for integrating the output of different OIE systems into a single unified and dis ambiguated knowledge repository .","[('based on', (25, 27)), ('for integrating', (29, 31)), ('of', (33, 34)), ('into', (37, 38))]","[('KB - UNIFY', (0, 3)), ('BabelNet', (27, 28)), ('different OIE systems', (34, 37)), ('single unified and dis ambiguated knowledge repository', (39, 46))]","[['different OIE systems', 'into', 'single unified and dis ambiguated knowledge repository']]",[],[],"[['Baselines', 'has', 'KB - UNIFY']]",hypernym_discovery,3,67
2490,baselines,"The unification algorithm takes as input a set K of OIE - derived resources , each of which is modeled as a set of entity , relation , entity triples , and comprises two subsequent stages : in the first dis ambiguation stage , each KB in K is linked to the sense inventory of Babel Net by dis ambiguating its relation argument pairs ; in the following alignment stage , equivalent relations across different KB in K are merged together .","[('takes as input', (3, 6)), ('modeled as', (19, 21)), ('linked to', (49, 51)), ('of', (54, 55)), ('by dis', (57, 59))]","[('unification algorithm', (1, 3)), ('set K of OIE - derived resources', (7, 14)), ('set', (22, 23)), ('each', (44, 45)), ('sense inventory', (52, 54)), ('Babel Net', (55, 57)), ('relation argument pairs', (61, 64))]","[['unification algorithm', 'takes as input', 'set K of OIE - derived resources'], ['set K of OIE - derived resources', 'modeled as', 'set'], ['sense inventory', 'of', 'Babel Net']]",[],[],"[['Baselines', 'has', 'unification algorithm']]",hypernym_discovery,3,68
2491,results,"As expected , Yago and WiBi achieve the best over all results .","[('achieve', (6, 7))]","[('Yago and WiBi', (3, 6)), ('best over all results', (8, 12))]","[['Yago and WiBi', 'achieve', 'best over all results']]","[['Yago and WiBi', 'has', 'best over all results']]",[],"[['Results', 'has', 'Yago and WiBi']]",hypernym_discovery,3,139
2492,results,Experiment 2 : Extra-Coverage,[],[],[],[],[],[],hypernym_discovery,3,149
2493,research-problem,"This paper proposes a simple but effective method for the discovery of hypernym sets based on word embedding , which can be used to measure the contextual similarities between words .","[('based on', (14, 16)), ('to measure', (23, 25)), ('between', (28, 29))]","[('discovery of', (10, 12)), ('hypernym sets', (12, 14)), ('word embedding', (16, 18)), ('contextual similarities', (26, 28)), ('words', (29, 30))]","[['hypernym sets', 'based on', 'word embedding'], ['word embedding', 'to measure', 'contextual similarities'], ['contextual similarities', 'between', 'words']]","[['discovery of', 'has', 'hypernym sets']]",[],[],hypernym_discovery,4,4
2494,research-problem,"In the SemEval 2018 Task 9 , the task has shifted to "" Hypernym Discovery "" , i.e. , given the search space of a domain 's vocabulary and an input hyponym , discover its best ( set of ) candidate hypernyms .",[],"[('Hypernym Discovery', (13, 15))]",[],[],[],[],hypernym_discovery,4,13
2495,hyperparameters,Word2vec is used to produce the word embeddings .,"[('to produce', (3, 5))]","[('Word2vec', (0, 1)), ('word embeddings', (6, 8))]","[['Word2vec', 'to produce', 'word embeddings']]",[],[],"[['Hyperparameters', 'has', 'Word2vec']]",hypernym_discovery,4,82
2496,hyperparameters,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,"[('used with', (11, 13)), ('set to', (16, 18))]","[('skip - gram model ( - cbow 0 )', (1, 10)), ('embedding dimension', (14, 16)), ('300', (18, 19))]","[['skip - gram model ( - cbow 0 )', 'used with', 'embedding dimension'], ['embedding dimension', 'set to', '300']]","[['embedding dimension', 'has', '300']]",[],"[['Hyperparameters', 'has', 'skip - gram model ( - cbow 0 )']]",hypernym_discovery,4,83
2497,results,Results Based on Projection Learning,[],[],[],[],[],[],hypernym_discovery,4,87
2498,results,"By using the same evaluating metrics as PRF in the cited paper , our best F - value on the validation set is 0.68 ( the paper result is 0.73 ) when the best cluster number is 2 and the threshold is ( 17.7 , 17.3 ) .","[('using', (1, 2)), ('as', (6, 7)), ('on', (18, 19)), ('is', (22, 23)), ('when', (31, 32)), ('is', (36, 37)), ('is', (41, 42))]","[('our best F - value', (13, 18)), ('validation set', (20, 22)), ('0.68', (23, 24)), ('best cluster number', (33, 36)), ('2', (37, 38)), ('threshold', (40, 41)), ('( 17.7 , 17.3 )', (42, 47))]","[['our best F - value', 'on', 'validation set'], ['our best F - value', 'is', '0.68'], ['0.68', 'when', 'best cluster number'], ['best cluster number', 'is', '2'], ['threshold', 'is', '( 17.7 , 17.3 )']]","[['best cluster number', 'has', '2'], ['threshold', 'has', '( 17.7 , 17.3 )']]",[],[],hypernym_discovery,4,91
2499,research-problem,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery,[],[],[],[],[],[],hypernym_discovery,5,2
2500,research-problem,The goal of the hypernym discovery task at Sem - Eval 2018 is to predict the hypernyms of a query given a large vocabulary of candidate hypernyms .,[],"[('hypernym discovery', (4, 6))]",[],[],[],[],hypernym_discovery,5,8
2501,model,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .","[('for', (7, 8))]","[('task of hypernym discovery exploits', (9, 14)), ('two approaches', (17, 19)), ('unsupervised , pattern - based approach', (21, 27)), ('supervised , projection learning approach', (29, 34))]",[],"[['two approaches', 'name', 'unsupervised , pattern - based approach']]","[['Model', 'for', 'task of hypernym discovery exploits']]",[],hypernym_discovery,5,12
2502,research-problem,Pattern - Based Hypernym Discovery,[],[],[],[],[],[],hypernym_discovery,5,14
2503,research-problem,Learning Projections for Hypernym Discovery,[],[],[],[],[],[],hypernym_discovery,5,48
2504,results,Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,"[('ranked', (4, 5)), ('on', (6, 7))]","[('Our hybrid system', (0, 3)), ('1st', (5, 6)), ('all three sub - tasks', (7, 12)), ('runs', (16, 17))]","[['Our hybrid system', 'ranked', '1st'], ['1st', 'on', 'all three sub - tasks']]","[['Our hybrid system', 'has', '1st']]",[],"[['Results', 'has', 'Our hybrid system']]",hypernym_discovery,5,134
2505,results,"If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .","[('of', (7, 8)), ('see that', (13, 15)), ('improved', (17, 18)), ('on', (21, 22)), ('increased them', (27, 29)), ('by', (29, 30)), ('on', (32, 33))]","[('runs', (3, 4)), ('data augmentation', (15, 17)), ('our scores', (18, 20)), ('slightly', (20, 21)), ('1A and 2B', (22, 25)), ('several points', (30, 32)), ('2A', (33, 34))]","[['runs', 'see that', 'data augmentation'], ['data augmentation', 'improved', 'our scores'], ['our scores', 'on', '1A and 2B'], ['slightly', 'on', '1A and 2B'], ['several points', 'on', '2A']]","[['runs', 'has', 'data augmentation'], ['our scores', 'has', 'slightly']]",[],[],hypernym_discovery,5,137
2506,results,"Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .","[('are', (3, 4)), ('computed using', (9, 11))]","[('Our cross-evaluation results', (0, 3)), ('better', (4, 5)), ('supervised baseline', (7, 9)), ('normal evaluation setup', (12, 15))]","[['Our cross-evaluation results', 'are', 'better'], ['supervised baseline', 'computed using', 'normal evaluation setup']]","[['Our cross-evaluation results', 'has', 'better']]",[],"[['Results', 'has', 'Our cross-evaluation results']]",hypernym_discovery,5,138
2507,results,"Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task , and even outperformed the supervised baseline on 2A .",[],"[('unsupervised system', (3, 5)), ('outperformed', (5, 6)), ('all other unsupervised systems', (6, 10)), ('outperformed', (17, 18)), ('supervised baseline', (19, 21))]",[],"[['unsupervised system', 'has', 'outperformed'], ['outperformed', 'has', 'all other unsupervised systems'], ['outperformed', 'has', 'supervised baseline']]",[],"[['Results', 'has', 'unsupervised system']]",hypernym_discovery,5,140
2508,results,"Combining the outputs of the 2 systems improves the best score of either system on all test sets , sometimes by as much as 10 points .","[('Combining', (0, 1)), ('of', (3, 4)), ('improves', (7, 8)), ('of', (11, 12)), ('on', (14, 15)), ('sometimes', (19, 20)), ('by', (20, 21))]","[('outputs', (2, 3)), ('2 systems', (5, 7)), ('best score', (9, 11)), ('either system', (12, 14)), ('all test sets', (15, 18)), ('as much as 10 points', (21, 26))]","[['outputs', 'of', '2 systems'], ['best score', 'of', 'either system'], ['outputs', 'improves', 'best score'], ['2 systems', 'improves', 'best score'], ['best score', 'of', 'either system'], ['best score', 'on', 'all test sets'], ['either system', 'on', 'all test sets'], ['best score', 'by', 'as much as 10 points'], ['all test sets', 'by', 'as much as 10 points']]","[['outputs', 'has', '2 systems']]","[['Results', 'Combining', 'outputs']]",[],hypernym_discovery,5,141
2509,results,"Notice also that the results obtained using only the supervised system indicate that data augmentation had a positive effect on our 2A scores only ( compare runs 1 and 2 ) , although our tests on the trial set suggested it would also have a positive effect on our 1A scores .","[('using', (6, 7)), ('indicate', (11, 12)), ('had', (15, 16)), ('on', (19, 20))]","[('only the supervised system', (7, 11)), ('data augmentation', (13, 15)), ('positive effect', (17, 19)), ('our 2A scores only', (20, 24))]","[['only the supervised system', 'indicate', 'data augmentation'], ['data augmentation', 'had', 'positive effect'], ['positive effect', 'on', 'our 2A scores only']]","[['data augmentation', 'has', 'positive effect']]","[['Results', 'using', 'only the supervised system']]",[],hypernym_discovery,5,142
2510,results,"Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .","[('is', (12, 13)), ('on', (15, 16)), ('when', (20, 21))]","[('run 1', (10, 12)), ('best', (14, 15)), ('all 3 test sets', (16, 20)), ('hybrid system', (24, 26))]","[['run 1', 'is', 'best'], ['best', 'on', 'all 3 test sets'], ['best', 'when', 'hybrid system']]","[['run 1', 'has', 'best']]",[],[],hypernym_discovery,5,143
2511,ablation-analysis,No subsampling : we sample positive examples uniformly from the training set .,"[('sample', (4, 5)), ('from', (8, 9))]","[('No subsampling', (0, 2)), ('positive examples', (5, 7)), ('uniformly', (7, 8)), ('training set', (10, 12))]","[['No subsampling', 'sample', 'positive examples'], ['positive examples', 'from', 'training set'], ['uniformly', 'from', 'training set']]","[['positive examples', 'has', 'uniformly']]",[],"[['Ablation analysis', 'has', 'No subsampling']]",hypernym_discovery,5,151
2512,baselines,"No MTL : instead of multi - task learning ( MTL ) , we use a single classifier for both named entities and concepts .","[('use', (14, 15)), ('for', (18, 19))]","[('No', (0, 1)), ('MTL', (1, 2)), ('single classifier', (16, 18)), ('named entities and concepts', (20, 24))]","[['No', 'use', 'single classifier'], ['MTL', 'use', 'single classifier'], ['single classifier', 'for', 'named entities and concepts']]","[['No', 'has', 'MTL']]",[],"[['Baselines', 'has', 'No']]",hypernym_discovery,5,153
2513,ablation-analysis,Frozen embeddings : the word embeddings are not fine - tuned during training .,"[('not fine - tuned', (7, 11)), ('during', (11, 12))]","[('Frozen embeddings', (0, 2)), ('word', (4, 5)), ('training', (12, 13))]",[],"[['Frozen embeddings', 'has', 'word']]",[],"[['Ablation analysis', 'has', 'Frozen embeddings']]",hypernym_discovery,5,163
2514,ablation-analysis,"It is worth noting that our supervised model outperforms the supervised baseline provided for this task ( see ) even when it exploits a single projection matrix , however the difference in scores between these 2 systems is only 2 or 3 points , depending on the evaluation metric .","[('worth noting', (2, 4))]","[('our supervised model', (5, 8)), ('outperforms', (8, 9)), ('supervised baseline', (10, 12)), ('scores', (32, 33))]",[],"[['our supervised model', 'has', 'outperforms'], ['outperforms', 'has', 'supervised baseline']]","[['Ablation analysis', 'worth noting', 'our supervised model']]",[],hypernym_discovery,5,172
2515,research-problem,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery,[],[],[],[],[],[],hypernym_discovery,6,2
2516,research-problem,"In this paper , we present our proposed system ( EXPR ) to participate in the hypernym discovery task of SemEval 2018 .",[],"[('hypernym discovery task', (16, 19))]",[],[],[],[],hypernym_discovery,6,4
2517,research-problem,The task addresses the challenge of discovering hypernym relations from a text corpus .,[],"[('discovering hypernym relations from a text corpus', (6, 13))]",[],[],[],[],hypernym_discovery,6,5
2518,research-problem,Hypernym detection focuses on deciding whether a hypernymic relation holds between a given pair of terms or not .,[],"[('Hypernym detection', (0, 2))]",[],[],[],[],hypernym_discovery,6,25
2519,research-problem,Hypernym discovery focuses on discovering a set containing the best hypernyms for a given term from a given vocabulary search space .,[],"[('Hypernym discovery', (0, 2))]",[],[],[],[],hypernym_discovery,6,26
2520,research-problem,The task is divided into two subtasks : General - Purpose Hypernym Discovery and Domain - Specific Hypernym Discovery .,"[('divided into', (3, 5))]","[('two', (5, 6)), ('General - Purpose Hypernym Discovery', (8, 13)), ('Domain - Specific Hypernym Discovery', (14, 19))]",[],[],"[['Research problem', 'divided into', 'two']]",[],hypernym_discovery,6,27
2521,dataset,"The second consists of discovering hypernym in a domain - specific corpus , thus they provide the participants with data for two specific domains : Medical and Music .","[('in', (6, 7)), ('provide', (15, 16)), ('for', (20, 21))]","[('discovering', (4, 5)), ('hypernym', (5, 6)), ('domain - specific corpus', (8, 12)), ('participants', (17, 18)), ('data', (19, 20)), ('two specific domains', (21, 24)), ('Medical and Music', (25, 28))]","[['hypernym', 'in', 'domain - specific corpus'], ['domain - specific corpus', 'provide', 'participants'], ['data', 'for', 'two specific domains']]","[['discovering', 'has', 'hypernym'], ['participants', 'has', 'data'], ['two specific domains', 'name', 'Medical and Music']]",[],"[['Dataset', 'has', 'discovering']]",hypernym_discovery,6,29
2522,model,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .","[('propose', (6, 7)), ('combines', (10, 11)), ('via concatenating', (19, 21)), ('constructed using', (28, 30)), ('obtained using', (37, 39))]","[('path - based technique and distributional technique', (12, 19)), ('two feature vectors', (21, 24)), ('feature vector', (26, 28)), ('dependency parser output', (30, 33)), ('feature vector', (35, 37)), ('term embeddings', (39, 41))]","[['path - based technique and distributional technique', 'via concatenating', 'two feature vectors'], ['feature vector', 'constructed using', 'dependency parser output'], ['feature vector', 'obtained using', 'term embeddings']]","[['two feature vectors', 'name', 'feature vector']]","[['Model', 'propose', 'path - based technique and distributional technique']]",[],hypernym_discovery,6,32
2523,model,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .","[('create', (8, 9)), ('based on', (14, 16))]","[('concatenated vector', (5, 7)), ('binary supervised classifier model', (10, 14)), ('support vector machine ( SVM ) algorithm', (16, 23))]","[['concatenated vector', 'create', 'binary supervised classifier model'], ['binary supervised classifier model', 'based on', 'support vector machine ( SVM ) algorithm']]",[],[],[],hypernym_discovery,6,33
2524,results,"For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .","[('For', (0, 1)), ('performs', (7, 8)), ('than', (9, 10)), ('performs', (15, 16)), ('than', (17, 18)), ('on', (21, 22))]","[('three corpora', (2, 4)), ('our system', (5, 7)), ('better', (8, 9)), ('STJU system', (10, 12)), ('better', (16, 17)), ('MFH system', (19, 21)), ('English corpora', (23, 25))]","[['our system', 'performs', 'better'], ['better', 'than', 'STJU system'], ['our system', 'performs', 'better'], ['better', 'than', 'MFH system'], ['better', 'on', 'English corpora'], ['MFH system', 'on', 'English corpora']]","[['three corpora', 'has', 'our system']]","[['Results', 'For', 'three corpora']]",[],hypernym_discovery,6,101
2525,results,"As shown in the table 2 , the candidate hypernym extraction ( CHE ) coverage for English testing terms is 950 ( 63 % ) , that means our system is unable to extract any candidate hypernym for 550 ( 37 % ) terms ( 398 entities and 152 concepts ) .","[('for', (15, 16)), ('is', (19, 20)), ('unable', (31, 32)), ('for', (37, 38))]","[('candidate hypernym extraction ( CHE ) coverage', (8, 15)), ('English testing terms', (16, 19)), ('950 ( 63 % )', (20, 25)), ('our', (28, 29)), ('550 ( 37 % ) terms', (38, 44))]","[['candidate hypernym extraction ( CHE ) coverage', 'for', 'English testing terms'], ['English testing terms', 'is', '950 ( 63 % )']]",[],[],[],hypernym_discovery,6,114
2526,model,utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other .,"[('utilizes', (0, 1)), ('for', (4, 5)), ('by training', (7, 9)), ('for', (12, 13)), ('such that', (16, 18)), ('correspond to', (20, 22))]","[('non-negative sparse coding', (1, 4)), ('word translation', (5, 7)), ('sparse word vectors', (9, 12)), ('two languages', (14, 16)), ('coding bases', (18, 20)), ('each other', (22, 24))]","[['non-negative sparse coding', 'for', 'word translation'], ['sparse word vectors', 'for', 'two languages'], ['word translation', 'by training', 'sparse word vectors'], ['sparse word vectors', 'for', 'two languages'], ['two languages', 'such that', 'coding bases'], ['coding bases', 'correspond to', 'each other']]",[],"[['Model', 'utilizes', 'non-negative sparse coding']]",[],hypernym_discovery,7,13
2527,model,Here we apply sparse feature pairs to hypernym extraction .,"[('apply', (2, 3)), ('to', (6, 7))]","[('sparse feature pairs', (3, 6)), ('hypernym extraction', (7, 9))]","[['sparse feature pairs', 'to', 'hypernym extraction']]",[],"[['Model', 'apply', 'sparse feature pairs']]",[],hypernym_discovery,7,14
2528,research-problem,The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis ( FCA ) is relatively new .,[],"[('Formal concept Analysis ( FCA )', (14, 20))]",[],[],[],[],hypernym_discovery,7,18
2529,experiments,Formal concept analysis,[],[],[],[],[],[],hypernym_discovery,7,31
2530,tasks,Generating more negative samples also provides some additional performance boost .,"[('provides', (5, 6))]","[('Generating', (0, 1)), ('more negative samples', (1, 4)), ('some additional performance boost', (6, 10))]","[['Generating', 'provides', 'some additional performance boost'], ['more negative samples', 'provides', 'some additional performance boost']]","[['Generating', 'has', 'more negative samples']]",[],[],hypernym_discovery,7,117
2531,research-problem,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies,[],"[('Detecting Hypernymy Relations', (6, 9))]",[],[],[],[],hypernym_discovery,8,2
2532,research-problem,"This paper presents the participation of Apollo 's team in the SemEval - 2018 Task 9 "" Hypernym Discovery "" , Subtask 1 : "" General - Purpose Hypernym Discovery "" , which tries to produce a ranked list of hypernyms for a specific term .",[],"[('Hypernym Discovery', (17, 19))]",[],[],[],[],hypernym_discovery,8,4
2533,research-problem,We propose a novel approach for automatic extraction of hypernymy relations from a corpus by using dependency patterns .,[],"[('automatic extraction of hypernymy relations from a corpus', (6, 14))]",[],[],[],[],hypernym_discovery,8,5
2534,research-problem,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,"[('presents', (2, 3)), ('participated in', (12, 14)), ('based on', (19, 21))]","[('hypernym discovery', (9, 11)), ('unsupervised machine learning', (21, 24))]",[],[],"[['Research problem', 'presents', 'hypernym discovery']]",[],hypernym_discovery,8,8
2535,model,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,"[('is', (1, 2)), ('exploits', (8, 9)), ('that generalize', (12, 14))]","[('rule - based system', (3, 7)), ('syntactic dependency paths', (9, 12)), ('Hearst - style lexical patterns', (14, 19))]","[['rule - based system', 'exploits', 'syntactic dependency paths'], ['syntactic dependency paths', 'that generalize', 'Hearst - style lexical patterns']]",[],[],[],hypernym_discovery,8,9
2536,research-problem,"It is well known that in natural language processing ( NLP ) , one of the biggest challenges is to understand the meaning of words .",[],[],[],[],[],[],hypernym_discovery,8,15
2537,research-problem,A new Approach to Detect Hypernymy Relation,[],[],[],[],[],[],hypernym_discovery,8,24
2538,research-problem,Neural Models for Reasoning over Multiple Mentions using Coreference,[],"[('Reasoning over Multiple Mentions', (3, 7))]",[],[],[],[],natural_language_inference,0,2
2539,research-problem,One important form of reasoning for Question Answering ( QA ) models is the ability to aggregate information from multiple mentions of entities .,[],"[('Question Answering ( QA )', (6, 11))]",[],[],[],[],natural_language_inference,0,11
2540,model,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .","[('call', (1, 2)), ('tied together', (25, 27)), ('with', (27, 28)), ('of', (30, 31)), ('which denote', (33, 35))]","[('coreference - based reasoning', (3, 7)), ('help', (29, 30)), ('referring expressions', (31, 33)), ('same real - world entity', (36, 41))]","[['help', 'of', 'referring expressions'], ['referring expressions', 'which denote', 'same real - world entity']]",[],"[['Model', 'call', 'coreference - based reasoning']]",[],natural_language_inference,0,12
2541,model,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .","[('extracted from', (9, 11)), ('introduce', (16, 17)), ('in', (19, 20)), ('for', (23, 24)), ('depends on', (31, 33)), ('of', (36, 37)), ('of', (40, 41))]","[('term', (18, 19)), ('update equations', (21, 23)), ('Gated Recurrent Units ( GRU )', (24, 30)), ('hidden state', (34, 36)), ('coreferent antecedent', (38, 40)), ('current token', (42, 44))]","[['term', 'in', 'update equations'], ['update equations', 'for', 'Gated Recurrent Units ( GRU )'], ['Gated Recurrent Units ( GRU )', 'depends on', 'hidden state'], ['hidden state', 'of', 'coreferent antecedent'], ['coreferent antecedent', 'of', 'current token']]",[],"[['Model', 'extracted from', 'term']]",[],natural_language_inference,0,20
2542,experiments,In each case we see clear improvements of using C - GRU layers over GRU layers .,"[('see', (4, 5)), ('of using', (7, 9)), ('over', (13, 14))]","[('clear improvements', (5, 7)), ('C - GRU layers', (9, 13)), ('GRU layers', (14, 16))]","[['clear improvements', 'of using', 'C - GRU layers'], ['C - GRU layers', 'over', 'GRU layers']]",[],[],[],natural_language_inference,0,100
2543,experiments,"The Bi - C - GRU model significantly improves on this baseline , which shows that , with less data , coreference annotations can provide a useful bias for a memory network on how to read and write memories .",[],"[('Bi - C - GRU model', (1, 7)), ('significantly improves', (7, 9))]",[],"[['Bi - C - GRU model', 'has', 'significantly improves']]",[],[],natural_language_inference,0,102
2544,experiments,"A break - down of task - wise performance is given in Appendix C. Comparing C - GRU to the GRU based method , we find that the main gains are on tasks 2 ( two supporting facts ) , 3 ( three supporting facts ) and 16 ( basic induction ) .","[('of', (4, 5)), ('Comparing', (14, 15)), ('to', (18, 19)), ('find', (25, 26))]","[('task - wise performance', (5, 9)), ('C - GRU', (15, 18)), ('GRU based method', (20, 23)), ('main gains', (28, 30))]","[['task - wise performance', 'Comparing', 'C - GRU'], ['C - GRU', 'to', 'GRU based method'], ['C - GRU', 'find', 'main gains']]",[],[],[],natural_language_inference,0,103
2545,experiments,"Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .","[('Comparing to', (0, 2)), ('found that', (7, 9)), ('was', (12, 13)), ('on', (15, 16))]","[('QRN baseline', (3, 5)), ('C - GRU', (9, 12)), ('significantly worse', (13, 15)), ('task', (16, 17))]","[['QRN baseline', 'found that', 'C - GRU'], ['C - GRU', 'was', 'significantly worse'], ['significantly worse', 'on', 'task']]","[['QRN baseline', 'has', 'C - GRU'], ['C - GRU', 'has', 'significantly worse']]",[],[],natural_language_inference,0,105
2546,experiments,"On the other hand , C - GRU was significantly better than QRN on task 16 ( basic induction ) .","[('was', (8, 9)), ('than', (11, 12))]","[('C - GRU', (5, 8)), ('significantly better', (9, 11)), ('QRN', (12, 13))]","[['C - GRU', 'was', 'significantly better'], ['significantly better', 'than', 'QRN']]","[['C - GRU', 'has', 'significantly better']]",[],[],natural_language_inference,0,107
2547,model,We also include a baseline which uses coreference features as 1 - hot vectors appended to the input word vectors ( GA w/ GRU + 1 - hot ) .,"[('uses', (6, 7)), ('as', (9, 10)), ('appended to', (14, 16))]","[('coreference features', (7, 9)), ('1 - hot vectors', (10, 14)), ('input word vectors ( GA w/ GRU + 1 - hot )', (17, 29))]","[['coreference features', 'as', '1 - hot vectors'], ['1 - hot vectors', 'appended to', 'input word vectors ( GA w/ GRU + 1 - hot )']]",[],"[['Model', 'uses', 'coreference features']]",[],natural_language_inference,0,108
2548,experiments,"In both cases there is a sharp drop in performance , showing that specifically using coreference for connecting mentions is important .","[('in', (8, 9))]","[('sharp drop', (6, 8)), ('performance', (9, 10))]","[['sharp drop', 'in', 'performance']]",[],[],[],natural_language_inference,0,111
2549,research-problem,"We see higher performance for the C - GRU model in the low data regime , and better generalization throughout the training curve for all three settings .","[('see', (1, 2)), ('for', (4, 5)), ('in', (10, 11)), ('throughout', (19, 20))]","[('higher performance', (2, 4)), ('C - GRU model', (6, 10)), ('low data regime', (12, 15)), ('better generalization', (17, 19)), ('training curve', (21, 23))]","[['higher performance', 'for', 'C - GRU model'], ['C - GRU model', 'in', 'low data regime'], ['better generalization', 'throughout', 'training curve']]",[],"[['Research problem', 'see', 'higher performance']]",[],natural_language_inference,0,120
2550,experiments,"Lastly , we note that both models vastly outperform the best reported result of BiDAf from 1 . We believe this is because the GA models select answers from the list of candidatees , whereas BiDAF ignores those candidates .","[('note', (3, 4)), ('of', (13, 14)), ('from', (15, 16))]","[('both models', (5, 7)), ('vastly outperform', (7, 9)), ('best reported result', (10, 13)), ('BiDAf', (14, 15))]","[['best reported result', 'of', 'BiDAf']]","[['both models', 'has', 'vastly outperform'], ['vastly outperform', 'has', 'best reported result']]",[],[],natural_language_inference,0,125
2551,experiments,We see a significant gain in performance when using the layer with coreference bias .,"[('see', (1, 2)), ('in', (5, 6)), ('when using', (7, 9)), ('with', (11, 12))]","[('significant gain', (3, 5)), ('performance', (6, 7)), ('layer', (10, 11)), ('coreference bias', (12, 14))]","[['significant gain', 'in', 'performance'], ['significant gain', 'when using', 'layer'], ['performance', 'when using', 'layer'], ['layer', 'with', 'coreference bias']]",[],[],[],natural_language_inference,0,138
2552,experiments,"Furthermore , the 1 - hot baseline which uses the same coreference information , but with sequential recency bias fails to improve over the regular GRU layer .","[('uses', (8, 9)), ('with', (15, 16)), ('fails to', (19, 21)), ('over', (22, 23))]","[('1 - hot baseline', (3, 7)), ('same coreference information', (10, 13)), ('sequential recency bias', (16, 19)), ('improve', (21, 22)), ('regular GRU layer', (24, 27))]","[['1 - hot baseline', 'uses', 'same coreference information'], ['1 - hot baseline', 'with', 'sequential recency bias'], ['sequential recency bias', 'fails to', 'improve'], ['improve', 'over', 'regular GRU layer']]",[],[],[],natural_language_inference,0,139
2553,hyperparameters,The maximum number of coreference clusters across all tasks was C = 13 .,"[('across', (6, 7)), ('was', (9, 10))]","[('maximum number of coreference clusters', (1, 6)), ('all tasks', (7, 9)), ('C = 13', (10, 13))]","[['maximum number of coreference clusters', 'across', 'all tasks'], ['maximum number of coreference clusters', 'was', 'C = 13'], ['all tasks', 'was', 'C = 13']]",[],[],"[['Hyperparameters', 'has', 'maximum number of coreference clusters']]",natural_language_inference,0,160
2554,hyperparameters,"We used dropout of 0.2 in between the intermediate layers , and initialized word embeddings with Glove .","[('used', (1, 2)), ('of', (3, 4)), ('in between', (5, 7)), ('with', (15, 16))]","[('dropout', (2, 3)), ('0.2', (4, 5)), ('intermediate layers', (8, 10)), ('initialized', (12, 13)), ('word embeddings', (13, 15)), ('Glove', (16, 17))]","[['dropout', 'of', '0.2'], ['0.2', 'in between', 'intermediate layers'], ['word embeddings', 'with', 'Glove']]","[['initialized', 'has', 'word embeddings']]","[['Hyperparameters', 'used', 'dropout']]",[],natural_language_inference,0,166
2555,research-problem,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension,[],[],[],[],[],[],natural_language_inference,1,2
2556,research-problem,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .,[],"[('Reading Comprehension ( RC )', (12, 17))]",[],[],[],[],natural_language_inference,1,4
2557,research-problem,Building Artificial Intelligence ( AI ) algorithms to teach machines to read and to comprehend text is a long - standing challenge in Natural Language Processing ( NLP ) .,[],"[('Building Artificial Intelligence ( AI ) algorithms to', (0, 8))]",[],[],[],[],natural_language_inference,1,10
2558,model,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .","[('develop', (6, 7)), ('for', (17, 18)), ('can', (22, 23)), ('skip through', (23, 25)), ('of', (27, 28)), ('generate', (31, 32)), ('using', (34, 35))]","[('novel context zoom - in network ( ConZNet )', (8, 17)), ('RC tasks', (18, 20)), ('irrelevant parts', (25, 27)), ('document', (29, 30)), ('answer', (33, 34)), ('only the relevant regions of text', (35, 41))]","[['novel context zoom - in network ( ConZNet )', 'for', 'RC tasks'], ['novel context zoom - in network ( ConZNet )', 'skip through', 'irrelevant parts'], ['irrelevant parts', 'of', 'document'], ['novel context zoom - in network ( ConZNet )', 'generate', 'answer'], ['answer', 'using', 'only the relevant regions of text']]",[],"[['Model', 'develop', 'novel context zoom - in network ( ConZNet )']]",[],natural_language_inference,1,19
2559,model,The ConZNet architecture consists of two phases .,"[('consists of', (3, 5))]","[('ConZNet architecture', (1, 3)), ('two phases', (5, 7))]","[['ConZNet architecture', 'consists of', 'two phases']]","[['ConZNet architecture', 'has', 'two phases']]",[],"[['Model', 'has', 'ConZNet architecture']]",natural_language_inference,1,20
2560,model,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,"[('identify', (5, 6)), ('by employing', (11, 13))]","[('relevant regions of text', (7, 11)), ('reinforcement learning algorithm', (14, 17))]","[['relevant regions of text', 'by employing', 'reinforcement learning algorithm']]",[],"[['Model', 'identify', 'relevant regions of text']]",[],natural_language_inference,1,21
2561,model,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .","[('based on', (4, 6)), ('comprehends', (13, 14)), ('generates', (20, 21)), ('by using', (23, 25)), ('as', (31, 32)), ('along with', (38, 40)), ('as', (43, 44))]","[('encoder - decoder architecture', (7, 11)), ('identified regions of text', (15, 19)), ('answer', (22, 23)), ('residual self - attention network', (26, 31)), ('encoder', (32, 33)), ('RNNbased sequence generator', (35, 38)), ('pointer network', (41, 43)), ('decoder', (45, 46))]","[['encoder - decoder architecture', 'comprehends', 'identified regions of text'], ['encoder - decoder architecture', 'generates', 'answer'], ['answer', 'by using', 'residual self - attention network'], ['answer', 'by using', 'RNNbased sequence generator'], ['residual self - attention network', 'as', 'encoder'], ['residual self - attention network', 'as', 'RNNbased sequence generator'], ['RNNbased sequence generator', 'along with', 'pointer network'], ['pointer network', 'as', 'decoder']]",[],"[['Model', 'based on', 'encoder - decoder architecture']]",[],natural_language_inference,1,23
2562,model,"Moreover , our decoder combines span prediction and sequence generation .","[('combines', (4, 5))]","[('our decoder', (2, 4)), ('span prediction and sequence generation', (5, 10))]","[['our decoder', 'combines', 'span prediction and sequence generation']]",[],[],"[['Model', 'has', 'our decoder']]",natural_language_inference,1,28
2563,baselines,In both baselines we replace the span prediction layer with an answer generation layer .,"[('replace', (4, 5)), ('with', (9, 10))]","[('span prediction layer', (6, 9)), ('answer generation layer', (11, 14))]","[['span prediction layer', 'with', 'answer generation layer']]",[],"[['Baselines', 'replace', 'span prediction layer']]",[],natural_language_inference,1,119
2564,baselines,In Baseline 1 we use an 1 please refer for more details attention based seq2seq layer without using copy mechanism in the answer generation unit similar to .,"[('use', (4, 5)), ('refer for', (8, 10)), ('without using', (16, 18)), ('in', (20, 21))]","[('1', (6, 7)), ('more details attention based seq2seq layer', (10, 16)), ('copy mechanism', (18, 20)), ('answer generation unit', (22, 25))]","[['1', 'refer for', 'more details attention based seq2seq layer'], ['more details attention based seq2seq layer', 'without using', 'copy mechanism'], ['copy mechanism', 'in', 'answer generation unit']]","[['1', 'has', 'more details attention based seq2seq layer']]","[['Baselines', 'use', '1']]",[],natural_language_inference,1,120
2565,experimental-setup,We split each document into sentences using the sentence tokenizer of the NLTK toolkit .,"[('split', (1, 2)), ('into', (4, 5)), ('using', (6, 7)), ('of', (10, 11))]","[('each document', (2, 4)), ('sentences', (5, 6)), ('sentence tokenizer', (8, 10)), ('NLTK toolkit', (12, 14))]","[['each document', 'into', 'sentences'], ['sentences', 'using', 'sentence tokenizer'], ['sentence tokenizer', 'of', 'NLTK toolkit']]",[],"[['Experimental setup', 'split', 'each document']]",[],natural_language_inference,1,123
2566,experimental-setup,"Similarly , we further tokenize each sentence , corresponding question and answer using the word tokenizer of NLTK .","[('further', (3, 4)), ('using', (12, 13)), ('of', (16, 17))]","[('tokenize', (4, 5)), ('each sentence', (5, 7)), ('word tokenizer', (14, 16)), ('NLTK', (17, 18))]","[['each sentence', 'using', 'word tokenizer'], ['word tokenizer', 'of', 'NLTK']]","[['tokenize', 'has', 'each sentence']]","[['Experimental setup', 'further', 'tokenize']]",[],natural_language_inference,1,124
2567,experimental-setup,The model is implemented using Python and Tensorflow .,"[('implemented using', (3, 5))]","[('Python and Tensorflow', (5, 8))]",[],[],"[['Experimental setup', 'implemented using', 'Python and Tensorflow']]",[],natural_language_inference,1,125
2568,experimental-setup,All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,"[('of', (3, 4)), ('initialized by', (7, 9)), ('initialized with', (14, 16))]","[('weights', (2, 3)), ('model', (5, 6)), ('Glorot Initialization', (9, 11)), ('biases', (12, 13)), ('zeros', (16, 17))]","[['weights', 'of', 'model'], ['weights', 'initialized by', 'Glorot Initialization'], ['model', 'initialized by', 'Glorot Initialization'], ['biases', 'initialized with', 'zeros']]",[],[],"[['Experimental setup', 'has', 'weights']]",natural_language_inference,1,126
2569,experimental-setup,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .","[('use', (1, 2)), ('from', (7, 8)), ('with', (10, 11)), ('to initialize', (16, 18))]","[('300 dimensional word vectors', (3, 7)), ('GloVe', (8, 9)), ('840 billion pre-trained vectors', (11, 15)), ('word embeddings', (19, 21)), ('training', (27, 28))]","[['300 dimensional word vectors', 'from', 'GloVe'], ['GloVe', 'with', '840 billion pre-trained vectors'], ['300 dimensional word vectors', 'to initialize', 'word embeddings']]",[],"[['Experimental setup', 'use', '300 dimensional word vectors']]",[],natural_language_inference,1,127
2570,experimental-setup,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,"[('do not appear in', (4, 8)), ('from', (13, 14))]","[('Glove', (8, 9)), ('initialized', (10, 11)), ('sampling', (12, 13)), ('uniform random distribution', (15, 18))]","[['sampling', 'from', 'uniform random distribution']]",[],"[['Experimental setup', 'do not appear in', 'Glove']]",[],natural_language_inference,1,128
2571,experimental-setup,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,"[('apply', (1, 2)), ('between', (3, 4)), ('with', (6, 7)), ('of', (9, 10))]","[('dropout', (2, 3)), ('layers', (5, 6)), ('keep probability', (7, 9)), ('0.8', (10, 11))]","[['dropout', 'between', 'layers'], ['dropout', 'with', 'keep probability'], ['keep probability', 'of', '0.8']]",[],"[['Experimental setup', 'apply', 'dropout']]",[],natural_language_inference,1,129
2572,experimental-setup,The number of hidden units are set to 100 .,"[('set to', (6, 8))]","[('number of hidden units', (1, 5)), ('100', (8, 9))]","[['number of hidden units', 'set to', '100']]","[['number of hidden units', 'has', '100']]",[],"[['Experimental setup', 'has', 'number of hidden units']]",natural_language_inference,1,130
2573,experiments,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .","[('trained', (1, 2)), ('with', (4, 5)), ('for', (13, 14))]","[('our model', (2, 4)), ('AdaDelta ( Zeiler , 2012 ) optimizer', (6, 13)), ('50 epochs', (14, 16)), ('initial learning rate', (18, 21)), ('0.1', (22, 23)), ('minibatch size', (26, 28)), ('32', (29, 30))]","[['our model', 'with', 'AdaDelta ( Zeiler , 2012 ) optimizer'], ['our model', 'with', 'minibatch size'], ['AdaDelta ( Zeiler , 2012 ) optimizer', 'for', '50 epochs']]","[['initial learning rate', 'has', '0.1']]",[],[],natural_language_inference,1,131
2574,experimental-setup,The hyperparameter ' sample size ' ( number of relevant sentences ) is chosen based on the model performance on the devset .,"[('chosen', (13, 14)), ('based on', (14, 16)), ('on', (19, 20))]","[('hyperparameter', (1, 2)), (""' sample size ' ( number of relevant sentences )"", (2, 12)), ('model performance', (17, 19)), ('devset', (21, 22))]","[[""' sample size ' ( number of relevant sentences )"", 'based on', 'model performance'], ['model performance', 'on', 'devset']]","[['hyperparameter', 'has', ""' sample size ' ( number of relevant sentences )""]]",[],"[['Experimental setup', 'has', 'hyperparameter']]",natural_language_inference,1,132
2575,results,shows the performance of various models on Narrative QA .,"[('on', (6, 7))]","[('Narrative QA', (7, 9))]",[],[],"[['Results', 'on', 'Narrative QA']]",[],natural_language_inference,1,133
2576,results,It can be noted that our model with sample size 5 ( choosing 5 relevant sentences ) outperforms the best ROUGE - L score available so far by 12.62 % compared to .,"[('noted', (3, 4)), ('with', (7, 8)), ('by', (27, 28))]","[('our model', (5, 7)), ('outperforms', (17, 18)), ('best ROUGE - L score', (19, 24)), ('12.62 %', (28, 30))]","[['best ROUGE - L score', 'by', '12.62 %']]","[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'best ROUGE - L score']]",[],[],natural_language_inference,1,134
2577,results,"The low performance of Baseline 1 shows that the hybrid approach ( ConZNet ) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models ( Seq2Seq , ASR , BiDAF , MRU ) .","[('of', (3, 4)), ('shows', (6, 7)), ('for', (14, 15)), ('from', (17, 18)), ('as well as copying', (21, 25)), ('from', (26, 27)), ('than', (32, 33))]","[('low performance', (1, 3)), ('hybrid approach ( ConZNet )', (9, 14)), ('generating words', (15, 17)), ('fixed vocabulary', (19, 21)), ('document', (28, 29)), ('better suited', (30, 32)), ('span prediction models ( Seq2Seq , ASR , BiDAF , MRU )', (33, 45))]","[['hybrid approach ( ConZNet )', 'of', 'generating words'], ['hybrid approach ( ConZNet )', 'for', 'generating words'], ['generating words', 'from', 'fixed vocabulary'], ['better suited', 'than', 'span prediction models ( Seq2Seq , ASR , BiDAF , MRU )']]","[['low performance', 'has', 'hybrid approach ( ConZNet )']]",[],"[['Results', 'has', 'low performance']]",natural_language_inference,1,135
2578,research-problem,A Simple and Effective Approach to the Story Cloze Test,[],[],[],[],[],[],natural_language_inference,10,2
2579,research-problem,"Following this approach , we present a simpler fully - neural approach to the Story Cloze Test using skip - thought embeddings of the stories in a feed - forward network that achieves close to state - of - the - art performance on this task without any feature engineering .","[('present', (5, 6)), ('to', (12, 13)), ('using', (17, 18)), ('of', (22, 23)), ('in', (25, 26)), ('achieves close to', (32, 35)), ('without', (46, 47))]","[('simpler fully - neural approach', (7, 12)), ('Story Cloze Test', (14, 17)), ('skip - thought embeddings', (18, 22)), ('stories', (24, 25)), ('feed - forward network', (27, 31)), ('state - of - the - art performance', (35, 43)), ('feature engineering', (48, 50))]","[['simpler fully - neural approach', 'to', 'Story Cloze Test'], ['Story Cloze Test', 'using', 'skip - thought embeddings'], ['skip - thought embeddings', 'of', 'stories'], ['stories', 'in', 'feed - forward network'], ['feed - forward network', 'achieves close to', 'state - of - the - art performance'], ['state - of - the - art performance', 'without', 'feature engineering']]",[],"[['Research problem', 'present', 'simpler fully - neural approach']]",[],natural_language_inference,10,6
2580,research-problem,We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach .,"[('of', (9, 10)), ('instead of', (12, 14)), ('yields', (17, 18)), ('with', (20, 21))]","[('considering', (4, 5)), ('just the last sentence', (5, 9)), ('prompt', (11, 12)), ('whole prompt', (15, 17)), ('higher accuracy', (18, 20)), ('our approach', (21, 23))]","[['just the last sentence', 'of', 'prompt'], ['prompt', 'instead of', 'whole prompt'], ['considering', 'yields', 'higher accuracy'], ['just the last sentence', 'yields', 'higher accuracy'], ['whole prompt', 'yields', 'higher accuracy'], ['higher accuracy', 'with', 'our approach']]","[['considering', 'has', 'just the last sentence']]",[],[],natural_language_inference,10,7
2581,experiments,We use two layer and three layer fully connected networks with Rectified Linear ( ReLU ) non-linearities ( refer to Appendix A for model - specific architecture ) .,"[('use', (1, 2)), ('with', (10, 11))]","[('two layer and three layer fully connected networks', (2, 10)), ('Rectified Linear ( ReLU ) non-linearities', (11, 17))]","[['two layer and three layer fully connected networks', 'with', 'Rectified Linear ( ReLU ) non-linearities']]",[],[],[],natural_language_inference,10,64
2582,baselines,"Full Context ( FC ) Here , we use a Gated Recurrent Unit ( GRU ) to encode the entire story prompt into a 4800 - dimensional vector , add it to the skipthought embedding of the story ending , and pass it as input to the neural network .","[('use', (8, 9)), ('to encode', (16, 18)), ('into', (22, 23)), ('add it to', (29, 32)), ('of', (35, 36)), ('pass it as', (41, 44)), ('to', (45, 46))]","[('Full Context ( FC )', (0, 5)), ('Gated Recurrent Unit ( GRU )', (10, 16)), ('entire story prompt', (19, 22)), ('4800 - dimensional vector', (24, 28)), ('skipthought embedding', (33, 35)), ('story ending', (37, 39)), ('input', (44, 45)), ('neural network', (47, 49))]","[['Full Context ( FC )', 'use', 'Gated Recurrent Unit ( GRU )'], ['Gated Recurrent Unit ( GRU )', 'to encode', 'entire story prompt'], ['entire story prompt', 'into', '4800 - dimensional vector'], ['Gated Recurrent Unit ( GRU )', 'add it to', 'skipthought embedding'], ['4800 - dimensional vector', 'add it to', 'skipthought embedding'], ['skipthought embedding', 'of', 'story ending'], ['Gated Recurrent Unit ( GRU )', 'pass it as', 'input'], ['input', 'to', 'neural network']]","[['Full Context ( FC )', 'name', 'Gated Recurrent Unit ( GRU )']]",[],"[['Baselines', 'has', 'Full Context ( FC )']]",natural_language_inference,10,71
2583,hyperparameters,We use cross-entropy loss and SGD with learning rate of 0.01 .,"[('use', (1, 2)), ('with', (6, 7)), ('of', (9, 10))]","[('cross-entropy loss', (2, 4)), ('SGD', (5, 6)), ('learning rate', (7, 9)), ('0.01', (10, 11))]","[['SGD', 'with', 'learning rate'], ['learning rate', 'of', '0.01']]",[],"[['Hyperparameters', 'use', 'cross-entropy loss']]",[],natural_language_inference,10,83
2584,results,The 3 - layer feed - forward neural network trained on the validation set by summing the skip - thought embeddings of the last sentence ( LS ) of the story prompt and the ending gives the best accuracy ( 76.5 % ) .,"[('trained on', (9, 11)), ('by summing', (14, 16)), ('of', (21, 22)), ('of', (28, 29)), ('gives', (35, 36))]","[('3 - layer feed - forward neural network', (1, 9)), ('validation set', (12, 14)), ('skip - thought embeddings', (17, 21)), ('last sentence ( LS )', (23, 28)), ('story prompt', (30, 32)), ('ending', (34, 35)), ('best accuracy', (37, 39))]","[['3 - layer feed - forward neural network', 'trained on', 'validation set'], ['validation set', 'by summing', 'skip - thought embeddings'], ['skip - thought embeddings', 'of', 'last sentence ( LS )'], ['skip - thought embeddings', 'of', 'last sentence ( LS )'], ['last sentence ( LS )', 'of', 'story prompt'], ['ending', 'gives', 'best accuracy']]",[],[],"[['Results', 'has', '3 - layer feed - forward neural network']]",natural_language_inference,10,89
2585,ablation-analysis,"This approach is far simpler than previous approaches in the literature ; it requires no feature engineering , nor intricate neural network architecture , and achieves close to state - of - the - art accuracy .","[('achieves', (25, 26))]","[('close to state - of - the - art accuracy', (26, 36))]",[],[],"[['Ablation analysis', 'achieves', 'close to state - of - the - art accuracy']]",[],natural_language_inference,10,90
2586,results,"We note that the model trained using only the last sentence ( LS ) of the story context has higher accuracy compared to the model that uses a GRU to encode the full context ( FC ) , and even the model which encodes the entire context .","[('note', (1, 2)), ('trained using', (5, 7)), ('of', (14, 15)), ('compared to', (21, 23)), ('uses', (26, 27)), ('to encode', (29, 31))]","[('model', (4, 5)), ('only the last sentence ( LS )', (7, 14)), ('story context', (16, 18)), ('higher accuracy', (19, 21)), ('model', (24, 25)), ('GRU', (28, 29)), ('full context ( FC )', (32, 37)), ('entire context', (45, 47))]","[['model', 'trained using', 'only the last sentence ( LS )'], ['only the last sentence ( LS )', 'of', 'story context'], ['higher accuracy', 'compared to', 'model'], ['model', 'uses', 'GRU'], ['GRU', 'to encode', 'full context ( FC )']]",[],"[['Results', 'note', 'model']]",[],natural_language_inference,10,95
2587,research-problem,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING,[],[],[],[],[],[],natural_language_inference,11,2
2588,research-problem,Several deep learning models have been proposed for question answering .,[],"[('question answering', (8, 10))]",[],[],[],[],natural_language_inference,11,4
2589,research-problem,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,[],"[('Question answering ( QA )', (0, 5))]",[],[],[],[],natural_language_inference,11,12
2590,model,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .","[('introduce', (1, 2)), ('for', (21, 22))]","[('Dynamic Coattention Network ( DCN )', (3, 9)), ('end - to - end neural network', (14, 21)), ('question answering', (22, 24))]","[['end - to - end neural network', 'for', 'question answering']]",[],"[['Model', 'introduce', 'Dynamic Coattention Network ( DCN )']]",[],natural_language_inference,11,22
2591,model,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .","[('consists of', (2, 4)), ('captures', (8, 9)), ('between', (11, 12)), ('as', (18, 19)), ('well as', (19, 21)), ('alternates', (26, 27)), ('between', (27, 28))]","[('coattentive encoder', (5, 7)), ('interactions', (10, 11)), ('question and the document', (13, 17)), ('dynamic pointing decoder', (22, 25)), ('estimating', (28, 29)), ('start and end of', (30, 34)), ('answer span', (35, 37))]","[['coattentive encoder', 'captures', 'interactions'], ['interactions', 'between', 'question and the document'], ['coattentive encoder', 'well as', 'dynamic pointing decoder'], ['dynamic pointing decoder', 'alternates', 'estimating']]","[['estimating', 'has', 'start and end of'], ['start and end of', 'has', 'answer span']]","[['Model', 'consists of', 'coattentive encoder']]",[],natural_language_inference,11,23
2592,experimental-setup,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .","[('preprocess', (1, 2)), ('use', (6, 7)), ('from', (9, 10))]","[('corpus', (3, 4)), ('tokenizer', (8, 9)), ('Stanford CoreNLP', (10, 12))]","[['corpus', 'use', 'tokenizer'], ['tokenizer', 'from', 'Stanford CoreNLP']]",[],"[['Experimental setup', 'preprocess', 'corpus']]",[],natural_language_inference,11,127
2593,experimental-setup,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,"[('use', (1, 2)), ('pretrained on', (7, 9))]","[('Glo Ve word vectors', (3, 7)), ('840B Common Crawl corpus', (10, 14))]","[['Glo Ve word vectors', 'pretrained on', '840B Common Crawl corpus']]",[],"[['Experimental setup', 'use', 'Glo Ve word vectors']]",[],natural_language_inference,11,128
2594,experimental-setup,We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,"[('limit', (1, 2)), ('to', (4, 5)), ('present in', (8, 10)), ('set', (15, 16)), ('for', (17, 18)), ('to', (24, 25))]","[('vocabulary', (3, 4)), ('words', (5, 6)), ('Common Crawl corpus', (11, 14)), ('embeddings', (16, 17)), ('out - of - vocabulary words', (18, 24)), ('zero', (25, 26))]","[['vocabulary', 'to', 'words'], ['out - of - vocabulary words', 'to', 'zero'], ['words', 'present in', 'Common Crawl corpus'], ['embeddings', 'for', 'out - of - vocabulary words'], ['out - of - vocabulary words', 'to', 'zero']]","[['vocabulary', 'has', 'words']]","[['Experimental setup', 'limit', 'vocabulary']]",[],natural_language_inference,11,129
2595,experiments,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .","[('use', (1, 2)), ('of', (6, 7)), ('during', (8, 9)), ('for', (17, 18))]","[('max sequence length', (3, 6)), ('600', (7, 8)), ('training', (9, 10)), ('hidden state size', (12, 15)), ('200', (16, 17)), ('all recurrent units', (18, 21)), ('maxout layers', (22, 24)), ('linear layers', (26, 28))]","[['max sequence length', 'of', '600'], ['hidden state size', 'of', '200'], ['600', 'during', 'training'], ['200', 'for', 'all recurrent units'], ['200', 'for', 'linear layers']]","[['max sequence length', 'has', '600'], ['hidden state size', 'has', '200']]",[],[],natural_language_inference,11,131
2596,experimental-setup,All LSTMs have randomly initialized parameters and an initial state of zero .,"[('have', (2, 3))]","[('randomly initialized parameters', (3, 6)), ('initial state of zero', (8, 12))]",[],[],"[['Experimental setup', 'have', 'randomly initialized parameters']]",[],natural_language_inference,11,132
2597,experimental-setup,Sentinel vectors are randomly initialized and optimized during training .,"[('are', (2, 3)), ('during', (7, 8))]","[('Sentinel vectors', (0, 2)), ('randomly initialized and optimized', (3, 7)), ('training', (8, 9))]","[['Sentinel vectors', 'are', 'randomly initialized and optimized'], ['randomly initialized and optimized', 'during', 'training']]","[['Sentinel vectors', 'has', 'randomly initialized and optimized']]",[],"[['Experimental setup', 'has', 'Sentinel vectors']]",natural_language_inference,11,133
2598,experimental-setup,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .","[('For', (0, 1)), ('set', (6, 7)), ('to', (12, 13)), ('use', (15, 16)), ('of', (20, 21))]","[('dynamic decoder', (2, 4)), ('maximum number of iterations', (8, 12)), ('4', (13, 14)), ('maxout pool size', (17, 20)), ('16', (21, 22))]","[['dynamic decoder', 'set', 'maximum number of iterations'], ['maximum number of iterations', 'to', '4'], ['dynamic decoder', 'use', 'maxout pool size'], ['maxout pool size', 'of', '16']]","[['dynamic decoder', 'has', 'maximum number of iterations'], ['maximum number of iterations', 'has', '4'], ['maxout pool size', 'has', '16']]","[['Experimental setup', 'For', 'dynamic decoder']]",[],natural_language_inference,11,134
2599,experimental-setup,"We use dropout to regularize our network during training , and optimize the model using ADAM .","[('use', (1, 2)), ('to regularize', (3, 5)), ('during', (7, 8)), ('optimize', (11, 12)), ('using', (14, 15))]","[('dropout', (2, 3)), ('our network', (5, 7)), ('training', (8, 9)), ('model', (13, 14)), ('ADAM', (15, 16))]","[['dropout', 'to regularize', 'our network'], ['our network', 'during', 'training'], ['dropout', 'optimize', 'model'], ['model', 'using', 'ADAM']]",[],"[['Experimental setup', 'use', 'dropout']]",[],natural_language_inference,11,135
2600,experimental-setup,All models are implemented and trained with Chainer .,"[('implemented and trained with', (3, 7))]","[('Chainer', (7, 8))]",[],[],"[['Experimental setup', 'implemented and trained with', 'Chainer']]",[],natural_language_inference,11,136
2601,results,"The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .","[('of', (2, 3)), ('compared to', (12, 14)), ('to estimate', (30, 32)), ('of', (37, 38)), ('conditioned on', (46, 48))]","[('performance', (1, 2)), ('Dynamic Coattention Network', (4, 7)), ('DCN', (26, 27)), ('capability', (29, 30)), ('start and end points', (33, 37)), ('answer span', (39, 41)), ('multiple times', (41, 43)), ('previous estimates', (49, 51))]","[['performance', 'of', 'Dynamic Coattention Network'], ['start and end points', 'of', 'answer span'], ['capability', 'to estimate', 'start and end points'], ['start and end points', 'of', 'answer span']]","[['performance', 'has', 'Dynamic Coattention Network'], ['DCN', 'has', 'capability'], ['answer span', 'has', 'multiple times']]",[],"[['Results', 'has', 'performance']]",natural_language_inference,11,145
2602,ablation-analysis,"On the decoder side , we experiment with various pool sizes for the HMN maxout layers , using a 2 - layer MLP instead of a HMN , and forcing the HMN decoder to a single iteration .","[('On', (0, 1)), ('experiment with', (6, 8)), ('for', (11, 12)), ('using', (17, 18)), ('instead of', (23, 25)), ('forcing', (29, 30)), ('to', (33, 34))]","[('decoder side', (2, 4)), ('various pool sizes', (8, 11)), ('HMN maxout layers', (13, 16)), ('2 - layer MLP', (19, 23)), ('HMN', (26, 27)), ('HMN decoder', (31, 33)), ('single iteration', (35, 37))]","[['decoder side', 'experiment with', 'various pool sizes'], ['various pool sizes', 'for', 'HMN maxout layers'], ['HMN maxout layers', 'using', '2 - layer MLP'], ['2 - layer MLP', 'instead of', 'HMN'], ['various pool sizes', 'forcing', 'HMN decoder'], ['HMN decoder', 'to', 'single iteration']]",[],"[['Ablation analysis', 'On', 'decoder side']]",[],natural_language_inference,11,169
2603,ablation-analysis,"Empirically , we achieve the best performance on the development set with an iterative HMN with pool size 16 , and find that the model consistently benefits from a deeper , iterative decoder network .","[('achieve', (3, 4)), ('on', (7, 8)), ('with', (11, 12)), ('with', (15, 16)), ('find', (21, 22)), ('consistently benefits from', (25, 28))]","[('best performance', (5, 7)), ('development set', (9, 11)), ('iterative HMN', (13, 15)), ('pool size 16', (16, 19)), ('model', (24, 25)), ('deeper , iterative decoder network', (29, 34))]","[['best performance', 'on', 'development set'], ['development set', 'with', 'iterative HMN'], ['iterative HMN', 'with', 'pool size 16'], ['iterative HMN', 'with', 'pool size 16'], ['model', 'consistently benefits from', 'deeper , iterative decoder network']]",[],"[['Ablation analysis', 'achieve', 'best performance']]",[],natural_language_inference,11,170
2604,ablation-analysis,"The performance improves as the number of maximum allowed iterations increases , with little improvement after 4 iterations .","[('as', (3, 4)), ('after', (15, 16))]","[('performance', (1, 2)), ('improves', (2, 3)), ('number of maximum allowed iterations', (5, 10)), ('increases', (10, 11)), ('little improvement', (13, 15))]","[['improves', 'as', 'number of maximum allowed iterations']]","[['performance', 'has', 'improves'], ['number of maximum allowed iterations', 'has', 'increases']]",[],"[['Ablation analysis', 'has', 'performance']]",natural_language_inference,11,171
2605,ablation-analysis,"On the encoder side , replacing the coattention mechanism with an attention mechanism similar to Wang & Jiang ( 2016 b ) by setting CD to QA D in equation 3 results in a 1.9 point F1 drop .","[('On', (0, 1)), ('replacing', (5, 6)), ('with', (9, 10)), ('setting', (23, 24)), ('to', (25, 26)), ('results in', (31, 33))]","[('encoder side', (2, 4)), ('coattention mechanism', (7, 9)), ('attention mechanism', (11, 13)), ('CD', (24, 25)), ('QA D', (26, 28)), ('1.9 point F1 drop', (34, 38))]","[['encoder side', 'replacing', 'coattention mechanism'], ['coattention mechanism', 'with', 'attention mechanism'], ['attention mechanism', 'setting', 'CD'], ['CD', 'to', 'QA D'], ['QA D', 'results in', '1.9 point F1 drop']]","[['encoder side', 'has', 'coattention mechanism']]","[['Ablation analysis', 'On', 'encoder side']]",[],natural_language_inference,11,172
2606,ablation-analysis,Performance across length,[],[],[],[],[],[],natural_language_inference,11,175
2607,ablation-analysis,"However , as in shown in , there is no notable performance degradation for longer documents and questions contrary to our expectations .","[('there', (7, 8)), ('for', (13, 14))]","[('notable performance degradation', (10, 13)), ('longer documents and questions', (14, 18))]","[['notable performance degradation', 'for', 'longer documents and questions']]",[],[],[],natural_language_inference,11,178
2608,ablation-analysis,"Namely , it becomes increasingly challenging to compute the correct word span as the number of words increases .","[('becomes', (3, 4)), ('to compute', (6, 8)), ('as', (12, 13))]","[('increasingly challenging', (4, 6)), ('correct word span', (9, 12)), ('number of words', (14, 17)), ('increases', (17, 18))]","[['increasingly challenging', 'to compute', 'correct word span'], ['correct word span', 'as', 'number of words']]","[['number of words', 'has', 'increases']]",[],[],natural_language_inference,11,182
2609,ablation-analysis,"In , we note that the mean F1 of DCN exceeds those of previous systems across all question types .","[('note', (3, 4)), ('of', (8, 9)), ('exceeds', (10, 11)), ('across', (15, 16))]","[('mean F1', (6, 8)), ('DCN', (9, 10)), ('previous systems', (13, 15)), ('all question types', (16, 19))]","[['mean F1', 'of', 'DCN'], ['mean F1', 'exceeds', 'previous systems'], ['previous systems', 'across', 'all question types']]",[],"[['Ablation analysis', 'note', 'mean F1']]",[],natural_language_inference,11,185
2610,research-problem,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING,[],"[('FINDING', (0, 1))]",[],[],[],[],natural_language_inference,12,2
2611,research-problem,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .","[('find out', (26, 28))]","[('text - based question and answering', (3, 9)), ('relational reasoning', (12, 14)), ('question relevant information', (29, 32))]","[['relational reasoning', 'find out', 'question relevant information']]",[],[],[],natural_language_inference,12,4
2612,model,The external memory enables the model to deal with a knowledge base without loss of information .,"[('enables', (3, 4)), ('to deal with', (6, 9)), ('without loss of', (12, 15))]","[('external memory', (1, 3)), ('model', (5, 6)), ('knowledge base', (10, 12)), ('information', (15, 16))]","[['external memory', 'enables', 'model'], ['external memory', 'to deal with', 'knowledge base'], ['model', 'to deal with', 'knowledge base'], ['knowledge base', 'without loss of', 'information']]",[],[],"[['Model', 'has', 'external memory']]",natural_language_inference,12,21
2613,model,Generalization updates old memories given the new input and output feature map finds relevant information from the memory .,[],"[('Generalization updates old memories given the new', (0, 7))]",[],[],[],"[['Model', 'has', 'Generalization updates old memories given the new']]",natural_language_inference,12,23
2614,model,"Based on the memory network architecture , neural network based models like end - to - end memory network ( Mem N2N ) , gated end - to - end memory network ( GMe m N2N ) , dynamic memory network ( DMN ) , and dynamic memory network + ( DMN + ) are proposed .","[('Based on', (0, 2)), ('like', (11, 12))]","[('memory network architecture', (3, 6)), ('neural network based models', (7, 11)), ('end - to - end memory network ( Mem N2N )', (12, 23)), ('gated end - to - end memory network ( GMe m N2N )', (24, 37)), ('dynamic memory network ( DMN )', (38, 44)), ('dynamic memory network + ( DMN + )', (46, 54))]","[['neural network based models', 'like', 'end - to - end memory network ( Mem N2N )'], ['neural network based models', 'like', 'gated end - to - end memory network ( GMe m N2N )'], ['neural network based models', 'like', 'dynamic memory network ( DMN )'], ['neural network based models', 'like', 'dynamic memory network + ( DMN + )']]","[['memory network architecture', 'has', 'neural network based models'], ['neural network based models', 'name', 'end - to - end memory network ( Mem N2N )']]","[['Model', 'Based on', 'memory network architecture']]",[],natural_language_inference,12,25
2615,model,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .","[('able to find', (14, 17)), ('when', (20, 21)), ('is', (25, 26))]","[('"" Relation Memory Network "" ( RMN )', (4, 12)), ('complex relation', (17, 19)), ('lot of information', (22, 25))]","[['"" Relation Memory Network "" ( RMN )', 'able to find', 'complex relation'], ['complex relation', 'when', 'lot of information']]",[],[],"[['Model', 'has', '"" Relation Memory Network "" ( RMN )']]",natural_language_inference,12,42
2616,model,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,"[('uses', (1, 2)), ('to find out', (3, 6)), ('with', (8, 9))]","[('MLP', (2, 3)), ('relevant information', (6, 8)), ('new generalization', (10, 12)), ('information', (16, 17))]","[['MLP', 'to find out', 'relevant information'], ['relevant information', 'with', 'new generalization']]",[],"[['Model', 'uses', 'MLP']]",[],natural_language_inference,12,43
2617,model,"Relation Memory Network ( RMN ) is composed of four components - embedding , attention , updating , and reasoning .","[('composed of', (7, 9))]","[('Relation Memory Network ( RMN )', (0, 6)), ('four components', (9, 11)), ('embedding', (12, 13)), ('attention', (14, 15)), ('updating', (16, 17)), ('reasoning', (19, 20))]","[['Relation Memory Network ( RMN )', 'composed of', 'four components']]","[['four components', 'name', 'embedding']]",[],"[['Model', 'has', 'Relation Memory Network ( RMN )']]",natural_language_inference,12,46
2618,model,EMBEDDING COMPONENT,[],[],[],[],[],[],natural_language_inference,12,51
2619,model,"To constitute the attention component , we applied simple MLP represented as gt ? .","[('To constitute', (0, 2)), ('applied', (7, 8)), ('represented as', (10, 12))]","[('attention component', (3, 5)), ('simple MLP', (8, 10)), ('gt ?', (12, 14))]","[['attention component', 'applied', 'simple MLP'], ['simple MLP', 'represented as', 'gt ?']]",[],"[['Model', 'To constitute', 'attention component']]",[],natural_language_inference,12,61
2620,model,"1 ) to control the intensity of attention , inspired by the way Neural Turing Machine reads from the memory .","[('to control', (2, 4)), ('inspired by', (9, 11)), ('reads from', (16, 18))]","[('intensity of attention', (5, 8)), ('Neural Turing Machine', (13, 16)), ('memory', (19, 20))]","[['Neural Turing Machine', 'reads from', 'memory']]",[],"[['Model', 'to control', 'intensity of attention']]",[],natural_language_inference,12,67
2621,model,"To forget the information already used , we use intuitive updating component to renew the memory .","[('To forget', (0, 2)), ('use', (8, 9)), ('to renew', (12, 14))]","[('information', (3, 4)), ('intuitive updating component', (9, 12)), ('memory', (15, 16))]","[['information', 'use', 'intuitive updating component'], ['intuitive updating component', 'to renew', 'memory']]",[],"[['Model', 'To forget', 'information']]",[],natural_language_inference,12,73
2622,experiments,REASONING COMPONENT,[],[],[],[],[],[],natural_language_inference,12,77
2623,model,"MemN2N first calculates the relatedness of sentences in the question and memory by taking the inner product , and the sentence with the highest relatedness is selected as the first supporting sentence for the given question .","[('in', (7, 8)), ('by taking', (12, 14)), ('with', (21, 22)), ('selected as', (26, 28)), ('for', (32, 33))]","[('MemN2N', (0, 1)), ('relatedness of sentences', (4, 7)), ('question and memory', (9, 12)), ('inner product', (15, 17)), ('sentence', (20, 21)), ('highest relatedness', (23, 25)), ('first supporting sentence', (29, 32)), ('given question', (34, 36))]","[['relatedness of sentences', 'in', 'question and memory'], ['question and memory', 'by taking', 'inner product'], ['sentence', 'with', 'highest relatedness'], ['highest relatedness', 'selected as', 'first supporting sentence'], ['first supporting sentence', 'for', 'given question']]",[],[],"[['Model', 'has', 'MemN2N']]",natural_language_inference,12,84
2624,research-problem,bAbI story - based QA dataset,[],[],[],[],[],[],natural_language_inference,12,125
2625,model,"For regularization , we use batch normalization for all MLPs .","[('For', (0, 1)), ('use', (4, 5)), ('for', (7, 8))]","[('regularization', (1, 2)), ('batch normalization', (5, 7)), ('all MLPs', (8, 10))]","[['batch normalization', 'For', 'all MLPs'], ['regularization', 'use', 'batch normalization'], ['batch normalization', 'for', 'all MLPs']]",[],"[['Model', 'For', 'regularization']]",[],natural_language_inference,12,132
2626,model,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,"[('optimized with', (4, 6)), ('using', (12, 13)), ('with', (16, 17)), ('of', (20, 21))]","[('softmax output', (1, 3)), ('cross - entropy loss function', (7, 12)), ('Adam optimizer', (14, 16)), ('learning rate', (18, 20)), ('2 e ?4', (21, 24))]","[['softmax output', 'optimized with', 'cross - entropy loss function'], ['cross - entropy loss function', 'using', 'Adam optimizer'], ['cross - entropy loss function', 'with', 'learning rate'], ['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '2 e ?4']]","[['learning rate', 'has', '2 e ?4']]",[],"[['Model', 'has', 'softmax output']]",natural_language_inference,12,133
2627,model,"While trained jointly , RMN learns these different solutions for each task .","[('trained', (1, 2)), ('learns', (5, 6)), ('for', (9, 10))]","[('RMN', (4, 5)), ('different solutions', (7, 9)), ('each task', (10, 12))]","[['RMN', 'learns', 'different solutions'], ['different solutions', 'for', 'each task']]",[],"[['Model', 'trained', 'RMN']]",[],natural_language_inference,12,148
2628,experiments,"For the task 3 , the only failed task , attention component still functions well ; it focuses sequentially on the supporting sentences .","[('focuses', (17, 18))]","[('attention component', (10, 12)), ('well', (14, 15)), ('sequentially', (18, 19)), ('supporting sentences', (21, 23))]","[['attention component', 'focuses', 'sequentially']]",[],[],[],natural_language_inference,12,149
2629,experiments,"With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .","[('With', (0, 1)), ('except for', (16, 18)), ('compared to', (20, 22))]","[('match type feature', (2, 5)), ('all models other', (6, 9)), ('RMN', (10, 11)), ('significantly improved', (12, 14)), ('performance', (15, 16)), ('plain condition', (23, 25))]","[['significantly improved', 'compared to', 'plain condition'], ['performance', 'compared to', 'plain condition']]","[['match type feature', 'has', 'all models other'], ['all models other', 'has', 'RMN'], ['significantly improved', 'has', 'performance']]",[],[],natural_language_inference,12,163
2630,experiments,"Different from other tasks , RMN yields the same error rate 25.1 % with MemN2N and GMe m N2N on the task 3 .","[('yields', (6, 7)), ('with', (13, 14))]","[('RMN', (5, 6)), ('same error rate 25.1 %', (8, 13)), ('MemN2N and GMe m N2N', (14, 19))]","[['RMN', 'yields', 'same error rate 25.1 %'], ['same error rate 25.1 %', 'with', 'MemN2N and GMe m N2N']]","[['RMN', 'has', 'same error rate 25.1 %']]",[],[],natural_language_inference,12,171
2631,experiments,"Overall , the number of hops is correlated with the number of supporting sentences .","[('correlated with', (7, 9))]","[('number of hops', (3, 6)), ('number of supporting sentences', (10, 14))]","[['number of hops', 'correlated with', 'number of supporting sentences']]",[],[],[],natural_language_inference,12,194
2632,research-problem,Natural Language Comprehension with the EpiReader,[],"[('Natural Language Comprehension', (0, 3))]",[],[],[],[],natural_language_inference,13,2
2633,research-problem,"We present the EpiReader , a novel model for machine comprehension of text .",[],"[('machine comprehension of text', (9, 13))]",[],[],[],[],natural_language_inference,13,4
2634,research-problem,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",[],"[('Machine comprehension', (0, 2))]",[],[],[],[],natural_language_inference,13,5
2635,model,"In this paper , we argue that the same principle can be applied to machine comprehension of natural language .","[('applied to', (12, 14)), ('of', (16, 17))]","[('same', (8, 9)), ('machine comprehension', (14, 16)), ('natural language', (17, 19))]","[['same', 'applied to', 'machine comprehension'], ['machine comprehension', 'of', 'natural language']]",[],[],[],natural_language_inference,13,12
2636,model,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .","[('propose', (1, 2)), ('call', (16, 17))]","[('deep , end - to - end , neural comprehension model', (3, 14)), ('EpiReader', (18, 19))]","[['deep , end - to - end , neural comprehension model', 'call', 'EpiReader']]","[['deep , end - to - end , neural comprehension model', 'name', 'EpiReader']]","[['Model', 'propose', 'deep , end - to - end , neural comprehension model']]",[],natural_language_inference,13,13
2637,research-problem,Machine comprehension ( MC ) has therefore garnered significant attention from the machine learning research community .,[],"[('Machine comprehension ( MC )', (0, 5))]",[],[],[],[],natural_language_inference,13,16
2638,model,The EpiReader factors into two components .,"[('factors', (2, 3)), ('into', (3, 4))]","[('EpiReader', (1, 2)), ('two components', (4, 6))]",[],[],[],"[['Model', 'has', 'EpiReader']]",natural_language_inference,13,26
2639,model,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .","[('combine', (5, 6)), ('with', (10, 11)), ('to produce', (16, 18)), ('of', (21, 22))]","[(""Reasoner 's evidence"", (7, 10)), (""Extractor 's probability estimates"", (12, 16)), ('final ranking', (19, 21)), ('answer candidates', (23, 25))]","[[""Reasoner 's evidence"", 'with', ""Extractor 's probability estimates""], [""Extractor 's probability estimates"", 'to produce', 'final ranking'], ['final ranking', 'of', 'answer candidates']]",[],"[['Model', 'combine', ""Reasoner 's evidence""]]",[],natural_language_inference,13,40
2640,experiments,Children 's Book Test,[],[],[],[],[],[],natural_language_inference,13,63
2641,model,The Extractor is a Pointer Network .,"[('is', (2, 3))]","[('Extractor', (1, 2)), ('Pointer Network', (4, 6))]","[['Extractor', 'is', 'Pointer Network']]","[['Extractor', 'has', 'Pointer Network']]",[],"[['Model', 'has', 'Extractor']]",natural_language_inference,13,94
2642,model,"At each step the biGRU outputs two d-dimensional encoding vectors , one for the forward direction and one for the backward direction .","[('outputs', (5, 6)), ('one for', (11, 13)), ('one for', (17, 19))]","[('biGRU', (4, 5)), ('two d-dimensional encoding vectors', (6, 10)), ('forward direction', (14, 16)), ('backward direction', (20, 22))]","[['biGRU', 'outputs', 'two d-dimensional encoding vectors'], ['two d-dimensional encoding vectors', 'one for', 'forward direction'], ['two d-dimensional encoding vectors', 'one for', 'backward direction']]",[],[],"[['Model', 'has', 'biGRU']]",natural_language_inference,13,109
2643,experimental-setup,"To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","[('used', (5, 6)), ('with', (9, 10)), ('with', (21, 22)), ('of', (26, 27))]","[('our model', (2, 4)), ('stochastic gradient descent', (6, 9)), ('ADAM optimizer', (11, 13)), ('initial learning rate', (23, 26)), ('0.001', (27, 28))]","[['our model', 'used', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'ADAM optimizer'], ['stochastic gradient descent', 'with', 'initial learning rate'], ['stochastic gradient descent', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]",[],[],[],natural_language_inference,13,221
2644,experimental-setup,"The word embeddings were initialized randomly , drawing from the uniform distribution over .","[('drawing from', (7, 9))]","[('word embeddings', (1, 3)), ('initialized randomly', (4, 6)), ('uniform distribution', (10, 12))]","[['word embeddings', 'drawing from', 'uniform distribution'], ['initialized randomly', 'drawing from', 'uniform distribution']]","[['word embeddings', 'has', 'initialized randomly']]",[],"[['Experimental setup', 'has', 'word embeddings']]",natural_language_inference,13,222
2645,experiments,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .","[('used', (1, 2)), ('of', (3, 4)), ('with', (10, 11)), ('of', (13, 14))]","[('batches', (2, 3)), ('32 examples', (4, 6)), ('early stopping', (8, 10)), ('patience', (12, 13)), ('2 epochs', (14, 16))]","[['batches', 'of', '32 examples'], ['patience', 'of', '2 epochs'], ['early stopping', 'with', 'patience'], ['patience', 'of', '2 epochs']]",[],[],[],natural_language_inference,13,223
2646,experimental-setup,Our model was implement in Theano using the Keras framework .,"[('implement in', (3, 5)), ('using', (6, 7))]","[('Theano', (5, 6)), ('Keras framework', (8, 10))]","[['Theano', 'using', 'Keras framework']]",[],"[['Experimental setup', 'implement in', 'Theano']]",[],natural_language_inference,13,224
2647,experimental-setup,"All our models used 2 - regularization at 0.001 , ? = 50 , and ? = 0.04 .","[('used', (3, 4)), ('at', (7, 8))]","[('2 - regularization', (4, 7)), ('0.001', (8, 9))]","[['2 - regularization', 'at', '0.001']]",[],"[['Experimental setup', 'used', '2 - regularization']]",[],natural_language_inference,13,229
2648,results,The EpiReader achieves state - of - the - art performance across the board for both datasets .,"[('achieves', (2, 3)), ('across', (11, 12))]","[('EpiReader', (1, 2)), ('state - of - the - art performance', (3, 11)), ('board', (13, 14)), ('both datasets', (15, 17))]","[['EpiReader', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'across', 'board']]",[],[],"[['Results', 'has', 'EpiReader']]",natural_language_inference,13,236
2649,results,"On CNN , we score 2.2 % higher on test than the best previous model of .","[('score', (4, 5)), ('on', (8, 9)), ('than', (10, 11))]","[('CNN', (1, 2)), ('2.2 % higher', (5, 8)), ('test', (9, 10)), ('best previous model', (12, 15))]","[['CNN', 'score', '2.2 % higher'], ['2.2 % higher', 'on', 'test'], ['2.2 % higher', 'than', 'best previous model']]",[],[],[],natural_language_inference,13,237
2650,results,The improvement on CBT - NE is more modest at 1.1 % .,"[('on', (2, 3)), ('is', (6, 7)), ('at', (9, 10))]","[('improvement', (1, 2)), ('CBT - NE', (3, 6)), ('more modest', (7, 9)), ('1.1 %', (10, 12))]","[['improvement', 'on', 'CBT - NE'], ['improvement', 'is', 'more modest'], ['CBT - NE', 'is', 'more modest'], ['more modest', 'at', '1.1 %']]",[],[],[],natural_language_inference,13,243
2651,hyperparameters,"For each mini-batch update , the 2 norm of the whole gradient of all parameters is measured 5 and if larger than L = 50 , then it is scaled down to have norm L.","[('For', (0, 1)), ('of', (8, 9)), ('of', (12, 13)), ('measured', (16, 17)), ('scaled down', (29, 31)), ('to have', (31, 33))]","[('each mini-batch update', (1, 4)), ('2 norm', (6, 8)), ('whole gradient', (10, 12)), ('all parameters', (13, 15)), ('5', (17, 18))]","[['2 norm', 'of', 'whole gradient'], ['whole gradient', 'of', 'all parameters'], ['whole gradient', 'of', 'all parameters'], ['all parameters', 'measured', '5']]","[['each mini-batch update', 'has', '2 norm']]","[['Hyperparameters', 'For', 'each mini-batch update']]",[],natural_language_inference,14,133
2652,hyperparameters,"We use the learning rate annealing schedule from , namely , if the validation cost has not decreased after one epoch , then the learning rate is scaled down by a factor 1.5 .","[('use', (1, 2)), ('after', (18, 19)), ('scaled down by', (27, 30))]","[('learning rate', (3, 5)), ('validation', (13, 14)), ('not decreased', (16, 18)), ('one epoch', (19, 21)), ('learning rate', (24, 26)), ('factor 1.5', (31, 33))]","[['not decreased', 'after', 'one epoch'], ['learning rate', 'scaled down by', 'factor 1.5']]","[['learning rate', 'has', 'validation'], ['validation', 'has', 'not decreased']]","[['Hyperparameters', 'use', 'learning rate']]",[],natural_language_inference,14,135
2653,hyperparameters,"Training terminates when the learning rate drops below 10 ? 5 , i.e. after 50 epochs or so .","[('terminates when', (1, 3))]","[('Training', (0, 1)), ('learning rate', (4, 6)), ('drops', (6, 7)), ('below', (7, 8)), ('10 ? 5', (8, 11))]","[['Training', 'terminates when', 'learning rate']]","[['learning rate', 'has', 'drops'], ['drops', 'has', 'below'], ['below', 'has', '10 ? 5']]",[],"[['Hyperparameters', 'has', 'Training']]",natural_language_inference,14,136
2654,hyperparameters,"Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .","[('initialized using', (2, 4)), ('set to', (14, 16))]","[('Weights', (0, 1)), ('N ( 0 , 0.05 )', (4, 10)), ('batch size', (11, 13)), ('128', (16, 17))]","[['Weights', 'initialized using', 'N ( 0 , 0.05 )'], ['batch size', 'set to', '128']]","[['batch size', 'has', '128']]",[],"[['Hyperparameters', 'has', 'Weights']]",natural_language_inference,14,137
2655,hyperparameters,"On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .","[('On', (0, 1)), ('repeat', (7, 8)), ('with', (12, 13)), ('pick', (17, 18))]","[('Penn tree dataset', (2, 5)), ('each training', (8, 10)), ('10 times', (10, 12)), ('different random initializations', (13, 16)), ('smallest validation cost', (21, 24))]","[['Penn tree dataset', 'repeat', 'each training'], ['10 times', 'with', 'different random initializations']]","[['each training', 'has', '10 times']]","[['Hyperparameters', 'On', 'Penn tree dataset']]",[],natural_language_inference,14,138
2656,hyperparameters,Note that the baseline architectures were tuned in to give optimal perplexity 6 .,"[('Note', (0, 1)), ('tuned in to give', (6, 10))]","[('baseline architectures', (3, 5)), ('optimal perplexity', (10, 12))]","[['baseline architectures', 'tuned in to give', 'optimal perplexity']]",[],"[['Hyperparameters', 'Note', 'baseline architectures']]",[],natural_language_inference,14,141
2657,experiments,"We also vary the number of hops and memory size of our MemN2N , showing the contribution of both to performance ; note in particular that increasing the number of hops helps .","[('vary', (2, 3)), ('of', (10, 11))]","[('number of hops and memory size', (4, 10)), ('our MemN2N', (11, 13))]","[['number of hops and memory size', 'of', 'our MemN2N']]",[],[],[],natural_language_inference,14,144
2658,experiments,"In , we show how Mem N2N operates on memory with multiple hops .","[('show', (3, 4)), ('operates', (7, 8)), ('with', (10, 11))]","[('Mem N2N', (5, 7)), ('on memory', (8, 10)), ('multiple hops', (11, 13))]","[['Mem N2N', 'operates', 'on memory'], ['on memory', 'with', 'multiple hops']]",[],[],[],natural_language_inference,14,145
2659,baselines,"MemNN : The strongly supervised AM + NG + NL Memory Networks approach , proposed in .",[],"[('MemNN', (0, 1)), ('strongly supervised AM + NG + NL Memory Networks approach', (3, 13))]",[],"[['MemNN', 'has', 'strongly supervised AM + NG + NL Memory Networks approach']]",[],"[['Baselines', 'has', 'MemNN']]",natural_language_inference,14,153
2660,baselines,It uses a max operation ( rather than softmax ) at each layer which is trained directly with supporting facts ( strong supervision ) .,"[('uses', (1, 2)), ('at', (10, 11)), ('trained directly with', (15, 18))]","[('max operation ( rather than softmax )', (3, 10)), ('each layer', (11, 13)), ('supporting facts', (18, 20)), ('strong supervision', (21, 23))]","[['max operation ( rather than softmax )', 'at', 'each layer'], ['each layer', 'trained directly with', 'supporting facts']]","[['supporting facts', 'has', 'strong supervision']]",[],[],natural_language_inference,14,155
2661,baselines,"It employs n-gram modeling , nonlinear layers and an adaptive number of hops per query .","[('employs', (1, 2)), ('per', (13, 14))]","[('n-gram modeling', (2, 4)), ('nonlinear layers', (5, 7)), ('adaptive number of hops', (9, 13)), ('query', (14, 15))]","[['adaptive number of hops', 'per', 'query']]",[],[],[],natural_language_inference,14,156
2662,baselines,"LSTM : A standard LSTM model , trained using question / answer pairs only ( i.e. also weakly supervised ) .","[('trained using', (7, 9))]","[('LSTM', (0, 1)), ('question / answer pairs only', (9, 14)), ('weakly supervised', (17, 19))]","[['LSTM', 'trained using', 'question / answer pairs only']]",[],[],"[['Baselines', 'has', 'LSTM']]",natural_language_inference,14,161
2663,experiments,Language Modeling Experiments,[],[],[],[],[],[],natural_language_inference,14,164
2664,experiments,"To aid training , we apply ReLU operations to half of the units in each layer .","[('To aid', (0, 2)), ('apply', (5, 6)), ('to', (8, 9)), ('in', (13, 14))]","[('training', (2, 3)), ('ReLU operations', (6, 8)), ('half of the units', (9, 13)), ('each layer', (14, 16))]","[['training', 'apply', 'ReLU operations'], ['ReLU operations', 'to', 'half of the units'], ['half of the units', 'in', 'each layer']]",[],[],[],natural_language_inference,14,174
2665,experiments,"We use layer - wise ( RNN - like ) weight sharing , i.e. the query weights of each layer are the same ; the output weights of each layer are the same .","[('use', (1, 2)), ('of', (17, 18)), ('are', (20, 21)), ('of', (27, 28)), ('are', (30, 31))]","[('layer - wise ( RNN - like ) weight sharing', (2, 12)), ('query weights', (15, 17)), ('each layer', (18, 20)), ('output weights', (25, 27)), ('each layer', (28, 30))]","[['query weights', 'of', 'each layer'], ['output weights', 'of', 'each layer']]",[],[],[],natural_language_inference,14,175
2666,research-problem,Neural Natural Language Inference Models Enhanced with External Knowledge,[],"[('Neural Natural Language Inference Models', (0, 5))]",[],[],[],[],natural_language_inference,15,2
2667,research-problem,Modeling natural language inference is a very challenging task .,[],"[('Modeling natural language inference', (0, 4))]",[],[],[],[],natural_language_inference,15,4
2668,research-problem,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",[],"[('Natural language inference ( NLI )', (0, 6)), ('recognizing textual entailment ( RTE )', (10, 16))]",[],[],[],[],natural_language_inference,15,12
2669,model,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .","[('enrich', (4, 5)), ('with', (12, 13)), ('in', (15, 16))]","[('neural - network - based NLI models', (5, 12)), ('external knowledge', (13, 15)), ('coattention', (16, 17)), ('local inference collection', (18, 21)), ('inference composition components', (23, 26))]","[['neural - network - based NLI models', 'with', 'external knowledge'], ['external knowledge', 'in', 'coattention'], ['external knowledge', 'in', 'local inference collection'], ['external knowledge', 'in', 'inference composition components']]",[],"[['Model', 'enrich', 'neural - network - based NLI models']]",[],natural_language_inference,15,23
2670,experiments,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .","[('of using', (2, 4)), ('is', (6, 7)), ('when', (9, 10)), ('is', (15, 16))]","[('The', (0, 1)), ('advantage', (1, 2)), ('external knowledge', (4, 6)), ('more significant', (7, 9)), ('size of training data', (11, 15)), ('restricted', (16, 17))]","[['advantage', 'of using', 'external knowledge'], ['external knowledge', 'is', 'more significant'], ['size of training data', 'is', 'restricted'], ['external knowledge', 'when', 'size of training data'], ['more significant', 'when', 'size of training data'], ['size of training data', 'is', 'restricted']]","[['The', 'has', 'advantage']]",[],[],natural_language_inference,15,25
2671,hyperparameters,The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,"[('of', (10, 11)), ('of', (14, 15)), ('are', (19, 20))]","[('dimension', (9, 10)), ('hidden states', (12, 14)), ('LSTMs', (15, 16)), ('word embeddings', (17, 19)), ('300', (20, 21))]","[['dimension', 'of', 'hidden states'], ['hidden states', 'of', 'LSTMs'], ['hidden states', 'of', 'LSTMs'], ['hidden states', 'are', '300'], ['word embeddings', 'are', '300']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",natural_language_inference,15,160
2672,hyperparameters,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .","[('initialized by', (4, 6)), ('among', (17, 18))]","[('word embeddings', (1, 3)), ('300D GloVe 840B', (6, 9)), ('out - of - vocabulary words', (11, 17)), ('initialized', (20, 21)), ('randomly', (21, 22))]","[['word embeddings', 'initialized by', '300D GloVe 840B']]","[['initialized', 'has', 'randomly']]",[],"[['Hyperparameters', 'has', 'word embeddings']]",natural_language_inference,15,161
2673,hyperparameters,All word embeddings are updated during training .,"[('updated during', (4, 6))]","[('word embeddings', (1, 3)), ('training', (6, 7))]","[['word embeddings', 'updated during', 'training']]",[],[],"[['Hyperparameters', 'has', 'word embeddings']]",natural_language_inference,15,162
2674,hyperparameters,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .","[('used for', (9, 11)), ('with', (12, 13)), ('of', (17, 18))]","[('Adam ( Kingma and Ba , 2014 )', (0, 8)), ('optimization', (11, 12)), ('initial learning rate', (14, 17)), ('0.0004', (18, 19))]","[['Adam ( Kingma and Ba , 2014 )', 'used for', 'optimization'], ['Adam ( Kingma and Ba , 2014 )', 'with', 'initial learning rate'], ['optimization', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.0004']]",[],[],"[['Hyperparameters', 'has', 'Adam ( Kingma and Ba , 2014 )']]",natural_language_inference,15,163
2675,hyperparameters,The mini - batch size is set to 32 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('32', (8, 9))]","[['mini - batch size', 'set to', '32']]","[['mini - batch size', 'has', '32']]",[],"[['Hyperparameters', 'has', 'mini - batch size']]",natural_language_inference,15,164
2676,code,ESIM is a strong NLI baseline framework with the source code made available at https://github.com/lukecq1231/nli,"[('is', (1, 2))]","[('ESIM', (0, 1))]",[],[],[],[],natural_language_inference,15,166
2677,results,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .","[('namely', (4, 5)), ('enriches', (15, 16)), ('with', (17, 18)), ('obtains', (21, 22)), ('of', (24, 25)), ('reported on', (34, 36))]","[('Knowledge - based Inference Model ( KIM )', (5, 13)), ('ESIM', (16, 17)), ('external knowledge', (18, 20)), ('accuracy', (23, 24)), ('88.6 %', (25, 27)), ('best single - model performance', (29, 34)), ('SNLI dataset', (37, 39))]","[['Knowledge - based Inference Model ( KIM )', 'enriches', 'ESIM'], ['ESIM', 'with', 'external knowledge'], ['accuracy', 'of', '88.6 %'], ['best single - model performance', 'reported on', 'SNLI dataset']]",[],"[['Results', 'namely', 'Knowledge - based Inference Model ( KIM )']]",[],natural_language_inference,15,170
2678,hyperparameters,"In addition to that , we also use 15 semantic relation features , which does not bring additional gains in performance .","[('use', (7, 8)), ('does not', (14, 16))]","[('15 semantic relation features', (8, 12)), ('performance', (20, 21))]",[],[],"[['Hyperparameters', 'use', '15 semantic relation features']]",[],natural_language_inference,15,173
2679,experiments,"To further investigate external knowledge , we add TransE relation embedding , and again no further improvement is observed on both the development and test sets when TransE relation embedding is used ( concatenated ) with the semantic relation vectors .","[('add', (7, 8)), ('observed on', (18, 20)), ('used', (31, 32))]","[('external knowledge', (3, 5)), ('TransE relation embedding', (8, 11)), ('TransE relation embedding', (27, 30)), ('semantic relation vectors', (37, 40))]","[['external knowledge', 'add', 'TransE relation embedding']]",[],[],[],natural_language_inference,15,175
2680,results,"The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .","[('achieves', (3, 4)), ('on', (9, 10))]","[('baseline ESIM', (1, 3)), ('76.8 % and 75.8 %', (4, 9)), ('in - domain and cross - domain test set', (10, 19))]","[['baseline ESIM', 'achieves', '76.8 % and 75.8 %'], ['76.8 % and 75.8 %', 'on', 'in - domain and cross - domain test set']]",[],[],"[['Results', 'has', 'baseline ESIM']]",natural_language_inference,15,178
2681,results,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .","[('extend', (2, 3)), ('with', (5, 6)), ('achieve', (10, 11)), ('to', (13, 14))]","[('ESIM', (4, 5)), ('external knowledge', (6, 8)), ('significant gains', (11, 13)), ('77.2 % and 76.4 %', (14, 19))]","[['ESIM', 'with', 'external knowledge'], ['ESIM', 'achieve', 'significant gains'], ['external knowledge', 'achieve', 'significant gains'], ['significant gains', 'to', '77.2 % and 76.4 %']]",[],"[['Results', 'extend', 'ESIM']]",[],natural_language_inference,15,179
2682,results,"Especially under the condition of restricted training data ( 0.8 % ) , the model obtains a large gain when using more than half of external knowledge . :","[('under', (1, 2)), ('obtains', (15, 16)), ('when using', (19, 21))]","[('condition of restricted training data ( 0.8 % )', (3, 12)), ('model', (14, 15)), ('large gain', (17, 19)), ('more than half of external knowledge', (21, 27))]","[['model', 'obtains', 'large gain'], ['large gain', 'when using', 'more than half of external knowledge']]","[['condition of restricted training data ( 0.8 % )', 'has', 'model'], ['model', 'has', 'large gain']]",[],[],natural_language_inference,15,205
2683,results,"Especially , for antonym category in cross - domain set , KIM outperform ESIM significantly (+ absolute 5.0 % ) as expected , because antonym feature captured by external knowledge would help unseen cross - domain samples .","[('for', (2, 3)), ('in', (5, 6))]","[('antonym category', (3, 5)), ('cross - domain set', (6, 10)), ('KIM', (11, 12)), ('outperform', (12, 13)), ('ESIM', (13, 14)), ('significantly', (14, 15)), ('(+ absolute 5.0 % )', (15, 20))]","[['antonym category', 'in', 'cross - domain set']]","[['antonym category', 'has', 'cross - domain set'], ['KIM', 'has', 'outperform'], ['outperform', 'has', 'ESIM'], ['ESIM', 'has', 'significantly'], ['significantly', 'has', '(+ absolute 5.0 % )']]","[['Results', 'for', 'antonym category']]",[],natural_language_inference,15,222
2684,research-problem,Text Understanding with the Attention Sum Reader Network,[],"[('Text Understanding', (0, 2))]",[],[],[],[],natural_language_inference,16,2
2685,research-problem,Ensemble of our models sets new state of the art on all evaluated datasets .,"[('sets', (4, 5)), ('on', (10, 11))]","[('new state of the art', (5, 10)), ('all evaluated datasets', (11, 14))]","[['new state of the art', 'on', 'all evaluated datasets']]",[],"[['Research problem', 'sets', 'new state of the art']]",[],natural_language_inference,16,8
2686,research-problem,Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence .,[],"[('teaching machines', (4, 6))]",[],[],[],[],natural_language_inference,16,11
2687,experiments,Children 's Book Test,[],[],[],[],[],[],natural_language_inference,16,56
2688,model,We call this the contextual embedding .,"[('call', (1, 2))]","[('contextual embedding', (4, 6))]",[],[],"[['Model', 'call', 'contextual embedding']]",[],natural_language_inference,16,79
2689,hyperparameters,To train the model we used stochastic gradient descent with the ADAM update rule and learning rate of 0.001 or 0.0005 .,"[('used', (5, 6)), ('with', (9, 10)), ('of', (17, 18))]","[('stochastic gradient descent', (6, 9)), ('ADAM update rule', (11, 14)), ('learning rate', (15, 17)), ('0.001 or 0.0005', (18, 21))]","[['stochastic gradient descent', 'with', 'ADAM update rule'], ['stochastic gradient descent', 'with', 'learning rate'], ['learning rate', 'of', '0.001 or 0.0005']]",[],"[['Hyperparameters', 'used', 'stochastic gradient descent']]",[],natural_language_inference,16,172
2690,hyperparameters,"2 . The initial weights in the word embedding matrix were drawn randomly uniformly from the interval [ ? 0.1 , 0.1 ] .","[('in', (5, 6)), ('drawn randomly', (11, 13)), ('from', (14, 15))]","[('initial weights', (3, 5)), ('word embedding matrix', (7, 10)), ('interval [ ? 0.1 , 0.1 ]', (16, 23))]","[['initial weights', 'in', 'word embedding matrix']]",[],[],"[['Hyperparameters', 'has', 'initial weights']]",natural_language_inference,16,177
2691,hyperparameters,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,"[('in', (1, 2)), ('initialized by', (6, 8)), ('initialized to', (14, 16))]","[('Weights', (0, 1)), ('GRU networks', (3, 5)), ('random orthogonal matrices', (8, 11)), ('biases', (12, 13)), ('zero', (16, 17))]","[['Weights', 'in', 'GRU networks'], ['Weights', 'initialized by', 'random orthogonal matrices'], ['GRU networks', 'initialized by', 'random orthogonal matrices'], ['biases', 'initialized to', 'zero']]",[],[],"[['Hyperparameters', 'has', 'Weights']]",natural_language_inference,16,178
2692,hyperparameters,We also used a gradient clipping threshold of 10 and batches of size 32 .,"[('used', (2, 3)), ('of', (7, 8))]","[('gradient clipping threshold', (4, 7)), ('10', (8, 9)), ('batches', (10, 11)), ('32', (13, 14))]","[['gradient clipping threshold', 'of', '10']]","[['gradient clipping threshold', 'has', '10']]","[['Hyperparameters', 'used', 'gradient clipping threshold']]",[],natural_language_inference,16,179
2693,hyperparameters,During training we randomly shuffled all examples in each epoch .,"[('During', (0, 1)), ('randomly shuffled', (3, 5)), ('in', (7, 8))]","[('training', (1, 2)), ('all examples', (5, 7)), ('each epoch', (8, 10))]","[['training', 'randomly shuffled', 'all examples'], ['all examples', 'in', 'each epoch']]",[],"[['Hyperparameters', 'During', 'training']]",[],natural_language_inference,16,180
2694,hyperparameters,For each batch of the CNN and Daily Mail datasets we randomly reshuffled the assignment of named entities to the corresponding word embedding vectors to match the procedure proposed in .,"[('For', (0, 1)), ('of', (3, 4)), ('randomly reshuffled', (11, 13)), ('of', (15, 16)), ('to', (18, 19)), ('to match', (24, 26))]","[('each batch', (1, 3)), ('CNN and Daily Mail datasets', (5, 10)), ('assignment', (14, 15)), ('named entities', (16, 18)), ('corresponding word embedding vectors', (20, 24))]","[['each batch', 'of', 'CNN and Daily Mail datasets'], ['assignment', 'of', 'named entities'], ['CNN and Daily Mail datasets', 'randomly reshuffled', 'assignment'], ['assignment', 'of', 'named entities'], ['assignment', 'to', 'corresponding word embedding vectors'], ['named entities', 'to', 'corresponding word embedding vectors']]",[],"[['Hyperparameters', 'For', 'each batch']]",[],natural_language_inference,16,183
2695,results,Ensembles of our models set new state - of - the - art results on all evaluated datasets .,"[('set', (4, 5)), ('on', (14, 15))]","[('new state - of - the - art results', (5, 14)), ('all evaluated datasets', (15, 18))]","[['new state - of - the - art results', 'on', 'all evaluated datasets']]",[],[],[],natural_language_inference,16,211
2696,results,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .",[],"[('outperforms', (6, 7))]",[],[],[],[],natural_language_inference,16,214
2697,results,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,"[('On', (0, 1)), ('with', (7, 8)), ('achieves', (11, 12)), ('of', (15, 16))]","[('CNN dataset', (2, 4)), ('our single model', (4, 7)), ('best validation accuracy', (8, 11)), ('test accuracy', (13, 15)), ('69.5 %', (16, 18))]","[['our single model', 'with', 'best validation accuracy'], ['best validation accuracy', 'achieves', 'test accuracy'], ['test accuracy', 'of', '69.5 %']]","[['CNN dataset', 'has', 'our single model']]","[['Results', 'On', 'CNN dataset']]",[],natural_language_inference,16,215
2698,results,The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,"[('of', (3, 4)), ('according to', (9, 11)), ('is', (13, 14)), ('is', (17, 18)), ('than', (22, 23))]","[('average performance', (1, 3)), ('top 20 % models', (5, 9)), ('validation accuracy', (11, 13)), ('69.9 %', (14, 16)), ('even 0.5 % better', (18, 22)), ('single best - validation model', (24, 29))]","[['average performance', 'of', 'top 20 % models'], ['top 20 % models', 'according to', 'validation accuracy'], ['validation accuracy', 'is', '69.9 %'], ['69.9 %', 'is', 'even 0.5 % better'], ['even 0.5 % better', 'than', 'single best - validation model']]",[],[],"[['Results', 'has', 'average performance']]",natural_language_inference,16,216
2699,results,Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets ..,"[('Fusing', (0, 1)), ('gives', (4, 5)), ('in', (9, 10)), ('on', (11, 12))]","[('multiple models', (1, 3)), ('significant further increase', (6, 9)), ('accuracy', (10, 11))]","[['multiple models', 'gives', 'significant further increase'], ['significant further increase', 'in', 'accuracy']]",[],"[['Results', 'Fusing', 'multiple models']]",[],natural_language_inference,16,218
2700,results,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .","[('In', (0, 1)), ('with', (8, 9)), ('performs', (13, 14)), ('than', (18, 19)), ('with', (21, 22))]","[('named entity prediction', (1, 4)), ('our best single model', (4, 8)), ('accuracy of', (9, 11)), ('68.6 %', (11, 13)), ('2 % absolute better', (14, 18)), ('MemNN', (20, 21)), ('self supervision', (22, 24))]","[['our best single model', 'with', 'accuracy of'], ['MemNN', 'with', 'self supervision'], ['68.6 %', 'performs', '2 % absolute better'], ['2 % absolute better', 'than', 'MemNN'], ['MemNN', 'with', 'self supervision']]","[['named entity prediction', 'has', 'our best single model'], ['accuracy of', 'has', '68.6 %']]","[['Results', 'In', 'named entity prediction']]",[],natural_language_inference,16,220
2701,results,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,"[('In', (0, 1)), ('is', (7, 8)), ('than', (12, 13)), ('improves', (18, 19)), ('to', (21, 22))]","[('common noun prediction', (1, 4)), ('our single models', (4, 7)), ('0.4 % absolute better', (8, 12)), ('Mem NN', (13, 15)), ('performance', (20, 21)), ('69 %', (22, 24))]","[['our single models', 'is', '0.4 % absolute better'], ['0.4 % absolute better', 'than', 'Mem NN'], ['performance', 'to', '69 %']]","[['common noun prediction', 'has', 'our single models']]",[],[],natural_language_inference,16,221
2702,research-problem,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING,[],"[('NATURAL LANGUAGE', (11, 13))]",[],[],[],[],natural_language_inference,17,2
2703,research-problem,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .",[],"[('natural language understanding ( NLU )', (1, 7))]",[],[],[],[],natural_language_inference,17,4
2704,dataset,"( ii ) An online evaluation platform and leaderboard , based primarily on privately - held test data .","[('based primarily on', (10, 13))]","[('online evaluation platform and leaderboard', (4, 9)), ('privately - held test data', (13, 18))]","[['online evaluation platform and leaderboard', 'based primarily on', 'privately - held test data']]",[],[],[],natural_language_inference,17,34
2705,model,"It is model - agnostic , allowing for any kind of representation or contextualization , including models that use no explicit vector or symbolic representations for sentences whatsoever .","[('is', (1, 2)), ('allowing for', (6, 8)), ('including', (15, 16)), ('for', (25, 26))]","[('model - agnostic', (2, 5)), ('any kind of', (8, 11)), ('representation', (11, 12)), ('models', (16, 17)), ('explicit', (20, 21)), ('sentences', (26, 27))]","[['model - agnostic', 'allowing for', 'any kind of']]","[['any kind of', 'has', 'representation']]",[],[],natural_language_inference,17,47
2706,experiments,GLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite .,"[('diverges from', (2, 4)), ('in', (5, 6)), ('included in', (13, 15))]","[('GLUE', (0, 1)), ('SentEval', (4, 5)), ('selection of', (7, 9)), ('evaluation tasks', (9, 11)), ('suite', (16, 17))]","[['GLUE', 'diverges from', 'SentEval'], ['SentEval', 'in', 'selection of'], ['evaluation tasks', 'included in', 'suite']]","[['selection of', 'has', 'evaluation tasks']]",[],[],natural_language_inference,17,48
2707,results,"Interannotator agreement is high , with a Fleiss 's ? of 0.73 .","[('is', (2, 3)), ('with', (5, 6)), ('of', (10, 11))]","[('Interannotator agreement', (0, 2)), ('high', (3, 4)), (""Fleiss 's ?"", (7, 10)), ('0.73', (11, 12))]","[['Interannotator agreement', 'is', 'high'], ['high', 'with', ""Fleiss 's ?""], [""Fleiss 's ?"", 'of', '0.73']]","[['Interannotator agreement', 'has', 'high']]",[],[],natural_language_inference,17,145
2708,hyperparameters,We implement our models in the AllenNLP library .,"[('implement', (1, 2)), ('in', (4, 5))]","[('AllenNLP library', (6, 8))]",[],[],"[['Hyperparameters', 'implement', 'AllenNLP library']]",[],natural_language_inference,17,156
2709,code,Original code for the baselines is available at https://github.com/nyu-mll/GLUE-baselines and a newer version is available at https://github.com/jsalt18-sentence-repl/jiant.,[],"[('https://github.com/nyu-mll/GLUE-baselines', (8, 9))]",[],[],[],[],natural_language_inference,17,157
2710,hyperparameters,"We train our models with Adam ( Kingma & Ba , 2015 ) with initial learning rate 10 ? 4 and batch size 128 .","[('train', (1, 2)), ('with', (4, 5)), ('with', (13, 14))]","[('Adam ( Kingma & Ba , 2015 )', (5, 13)), ('initial learning rate', (14, 17)), ('10 ? 4', (17, 20)), ('batch size', (21, 23)), ('128', (23, 24))]","[['Adam ( Kingma & Ba , 2015 )', 'with', 'initial learning rate'], ['Adam ( Kingma & Ba , 2015 )', 'with', 'batch size'], ['Adam ( Kingma & Ba , 2015 )', 'with', 'initial learning rate'], ['Adam ( Kingma & Ba , 2015 )', 'with', 'batch size']]","[['initial learning rate', 'has', '10 ? 4'], ['batch size', 'has', '128']]","[['Hyperparameters', 'train', 'Adam ( Kingma & Ba , 2015 )']]",[],natural_language_inference,17,172
2711,research-problem,Parameter Re-Initialization through Cyclical Batch Size Schedules,[],"[('Parameter Re-Initialization', (0, 2))]",[],[],[],[],natural_language_inference,18,2
2712,model,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,"[('explores', (2, 3)), ('to', (10, 11)), ('of', (14, 15))]","[('weight initialization', (8, 10)), ('optimization dynamics', (12, 14)), ('specific learning task', (16, 19))]","[['weight initialization', 'to', 'optimization dynamics'], ['optimization dynamics', 'of', 'specific learning task']]",[],"[['Model', 'explores', 'weight initialization']]",[],natural_language_inference,18,19
2713,model,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .","[('incorporate', (6, 7)), ('for', (12, 13)), ('where', (24, 25)), ('use', (26, 27)), ('to control', (31, 33)), ('of', (39, 40))]","[('"" adaptive initialization ""', (8, 12)), ('neural network training', (13, 16)), ('cyclical batch size schedules', (27, 31)), ('noise ( or temperature )', (34, 39)), ('SGD', (40, 41))]","[['"" adaptive initialization ""', 'for', 'neural network training'], ['neural network training', 'where', 'cyclical batch size schedules'], ['"" adaptive initialization ""', 'use', 'cyclical batch size schedules'], ['cyclical batch size schedules', 'to control', 'noise ( or temperature )'], ['noise ( or temperature )', 'of', 'SGD']]",[],"[['Model', 'incorporate', '"" adaptive initialization ""']]",[],natural_language_inference,18,23
2714,model,"We explore different cyclical batch size ( CBS ) schedules for training neural networks inspired by Bayesian statistics , particularly adaptive MCMC methods .",[],"[('training neural networks', (11, 14))]",[],[],[],[],natural_language_inference,18,29
2715,model,We propose a simple but effective ensembling method that combines models saved during different cycles at no additional training cost .,"[('propose', (1, 2)), ('combines', (9, 10)), ('saved during', (11, 13))]","[('ensembling method', (6, 8)), ('models', (10, 11)), ('different cycles', (13, 15))]","[['ensembling method', 'combines', 'models'], ['models', 'saved during', 'different cycles']]",[],"[['Model', 'propose', 'ensembling method']]",[],natural_language_inference,18,34
2716,experiments,Language Results,[],[],[],[],[],[],natural_language_inference,18,82
2717,results,Language modeling is a challenging problem due to the complex and long - range interactions between distant words .,[],"[('Language modeling', (0, 2))]",[],[],[],"[['Results', 'has', 'Language modeling']]",natural_language_inference,18,83
2718,results,"As we can see , the best performing CBS schedules result in significant improvements in perplexity ( up to 7.91 ) over the baseline schedules and also offer reductions in the number of SGD training iterations ( up to 33 % ) .","[('result in', (10, 12)), ('in', (14, 15)), ('over', (21, 22)), ('offer', (27, 28)), ('in', (29, 30)), ('up to', (37, 39))]","[('best performing CBS schedules', (6, 10)), ('significant improvements', (12, 14)), ('perplexity ( up to 7.91 )', (15, 21)), ('baseline schedules', (23, 25)), ('reductions', (28, 29)), ('number of SGD training iterations', (31, 36)), ('33 %', (39, 41))]","[['best performing CBS schedules', 'result in', 'significant improvements'], ['significant improvements', 'in', 'perplexity ( up to 7.91 )'], ['reductions', 'in', 'number of SGD training iterations'], ['significant improvements', 'over', 'baseline schedules'], ['perplexity ( up to 7.91 )', 'over', 'baseline schedules'], ['significant improvements', 'offer', 'reductions'], ['reductions', 'in', 'number of SGD training iterations'], ['number of SGD training iterations', 'up to', '33 %']]",[],[],[],natural_language_inference,18,88
2719,results,Notice that almost all CBS schedules outperform the baseline schedule .,"[('Notice', (0, 1))]","[('almost all CBS schedules', (2, 6)), ('outperform', (6, 7)), ('baseline schedule', (8, 10))]",[],"[['almost all CBS schedules', 'has', 'outperform'], ['outperform', 'has', 'baseline schedule']]",[],[],natural_language_inference,18,90
2720,results,"We see that the CBS schedules match baseline performance , but the number of training iterations used in CBS schedules is up to 2 fewer .","[('see that', (1, 3)), ('match', (6, 7)), ('used in', (16, 18)), ('up to', (21, 23))]","[('CBS schedules', (4, 6)), ('baseline performance', (7, 9)), ('number of training iterations', (12, 16)), ('CBS schedules', (18, 20)), ('2 fewer', (23, 25))]","[['CBS schedules', 'match', 'baseline performance'], ['number of training iterations', 'used in', 'CBS schedules'], ['number of training iterations', 'up to', '2 fewer'], ['CBS schedules', 'up to', '2 fewer']]","[['CBS schedules', 'has', 'baseline performance']]","[['Results', 'see that', 'CBS schedules']]",[],natural_language_inference,18,101
2721,results,We observe that CBS achieves similar performance to the baseline .,"[('observe', (1, 2)), ('achieves', (4, 5)), ('to', (7, 8))]","[('CBS', (3, 4)), ('similar performance', (5, 7)), ('baseline', (9, 10))]","[['CBS', 'achieves', 'similar performance'], ['similar performance', 'to', 'baseline']]",[],"[['Results', 'observe', 'CBS']]",[],natural_language_inference,18,105
2722,results,"With CBS - 15 , we see 90.71 % training accuracy and 56. 44 % testing accuracy , which is a larger improvement than that offered by CBS on convolutional models on Cifar - 10 .","[('With', (0, 1)), ('see', (6, 7))]","[('CBS - 15', (1, 4)), ('90.71 % training accuracy', (7, 11)), ('56. 44 % testing accuracy', (12, 17))]","[['CBS - 15', 'see', '90.71 % training accuracy'], ['CBS - 15', 'see', '56. 44 % testing accuracy']]",[],"[['Results', 'With', 'CBS - 15']]",[],natural_language_inference,18,107
2723,results,Combining CBS - 15 on C2 with this strategy improves accuracy to 94.82 % .,"[('Combining', (0, 1)), ('improves', (9, 10)), ('to', (11, 12))]","[('CBS - 15 on C2', (1, 6)), ('accuracy', (10, 11)), ('94.82 %', (12, 14))]","[['CBS - 15 on C2', 'improves', 'accuracy'], ['accuracy', 'to', '94.82 %']]",[],"[['Results', 'Combining', 'CBS - 15 on C2']]",[],natural_language_inference,18,109
2724,results,Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % .,"[('Applying', (0, 1)), ('on', (3, 4)), ('trained with', (5, 7)), ('of', (16, 17)), ('compared to', (21, 23))]","[('snapshot ensembling', (1, 3)), ('C3', (4, 5)), ('CBS - 15 - 2 leads', (7, 13)), ('improved accuracy', (14, 16)), ('93. 56 %', (17, 20)), ('92.58 %', (23, 25))]","[['snapshot ensembling', 'on', 'C3'], ['snapshot ensembling', 'trained with', 'CBS - 15 - 2 leads'], ['C3', 'trained with', 'CBS - 15 - 2 leads'], ['improved accuracy', 'of', '93. 56 %']]",[],"[['Results', 'Applying', 'snapshot ensembling']]",[],natural_language_inference,18,111
2725,results,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .","[('ensembling', (1, 2)), ('on', (3, 4)), ('with', (5, 6)), ('increases to', (15, 17)), ('from', (19, 20))]","[('ResNet50', (2, 3)), ('Imagenet', (4, 5)), ('snapshots', (6, 7)), ('last two cycles', (9, 12)), ('performance', (14, 15)), ('76.401 %', (17, 19)), ('75.336 %', (20, 22))]","[['ResNet50', 'on', 'Imagenet'], ['Imagenet', 'with', 'snapshots'], ['performance', 'increases to', '76.401 %'], ['76.401 %', 'from', '75.336 %']]",[],"[['Results', 'ensembling', 'ResNet50']]",[],natural_language_inference,18,112
2726,experiments,"During training , we crop the image to 224 224 . PTB ( language modeling ) .","[('During', (0, 1)), ('crop', (4, 5)), ('to', (7, 8))]","[('training', (1, 2)), ('image', (6, 7)), ('224 224', (8, 10))]","[['training', 'crop', 'image'], ['image', 'to', '224 224']]",[],[],[],natural_language_inference,18,155
2727,experiments,"The total vocabulary size is 10 k , and all words outside the vocabulary are replaced by a placeholder token . WikiText 2 ( language modeling ) .","[('is', (4, 5)), ('outside', (11, 12)), ('replaced by', (15, 17))]","[('total vocabulary size', (1, 4)), ('10 k', (5, 7)), ('all words', (9, 11)), ('vocabulary', (13, 14)), ('placeholder token', (18, 20)), ('WikiText', (21, 22))]","[['total vocabulary size', 'is', '10 k'], ['all words', 'outside', 'vocabulary'], ['all words', 'replaced by', 'placeholder token']]","[['total vocabulary size', 'has', '10 k']]",[],[],natural_language_inference,18,158
2728,research-problem,Natural Language Inference by Tree - Based Convolution and Heuristic Matching,[],"[('Natural Language Inference', (0, 3))]",[],[],[],[],natural_language_inference,19,2
2729,research-problem,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .","[('propose', (5, 6)), ('to recognize', (9, 11)), ('between', (14, 15))]","[('TBCNNpair model', (7, 9)), ('entailment and contradiction', (11, 14)), ('two sentences', (15, 17))]","[['TBCNNpair model', 'to recognize', 'entailment and contradiction'], ['entailment and contradiction', 'between', 'two sentences']]",[],"[['Research problem', 'propose', 'TBCNNpair model']]",[],natural_language_inference,19,4
2730,research-problem,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .,[],"[('natural language inference ( NLI )', (18, 24))]",[],[],[],[],natural_language_inference,19,8
2731,model,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .","[('propose', (5, 6)), ('to recognize', (12, 14)), ('between', (17, 18))]","[('TBCNN - pair neural model', (7, 12)), ('entailment and contradiction', (14, 17)), ('two sentences', (18, 20))]","[['TBCNN - pair neural model', 'to recognize', 'entailment and contradiction'], ['entailment and contradiction', 'between', 'two sentences']]",[],"[['Model', 'propose', 'TBCNN - pair neural model']]",[],natural_language_inference,19,33
2732,hyperparameters,"All our neural layers , including embeddings , were set to 300 dimensions .","[('including', (5, 6)), ('set to', (9, 11))]","[('embeddings', (6, 7)), ('300 dimensions', (11, 13))]",[],[],"[['Hyperparameters', 'including', 'embeddings']]",[],natural_language_inference,19,128
2733,results,"The model is mostly robust when the dimension is large , e.g. , several hundred .","[('is', (2, 3)), ('when', (5, 6)), ('is', (8, 9))]","[('model', (1, 2)), ('mostly robust', (3, 5)), ('dimension', (7, 8)), ('large , e.g. , several hundred', (9, 15))]","[['model', 'is', 'mostly robust'], ['dimension', 'is', 'large , e.g. , several hundred'], ['mostly robust', 'when', 'dimension'], ['dimension', 'is', 'large , e.g. , several hundred']]","[['model', 'has', 'mostly robust']]",[],"[['Results', 'has', 'model']]",natural_language_inference,19,129
2734,hyperparameters,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,"[('pretrained ourselves by', (3, 6)), ('on', (7, 8)), ('fined tuned during', (13, 16)), ('as', (17, 18))]","[('Word embeddings', (0, 2)), ('word2vec', (6, 7)), ('English Wikipedia corpus', (9, 12)), ('training', (16, 17)), ('apart', (18, 19)), ('model parameters', (20, 22))]","[['Word embeddings', 'pretrained ourselves by', 'word2vec'], ['word2vec', 'on', 'English Wikipedia corpus'], ['Word embeddings', 'fined tuned during', 'training'], ['training', 'as', 'apart']]","[['apart', 'has', 'model parameters']]",[],"[['Hyperparameters', 'has', 'Word embeddings']]",natural_language_inference,19,130
2735,hyperparameters,We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,"[('applied', (1, 2)), ('of', (4, 5)), ('chosen by', (11, 13)), ('with', (14, 15)), ('of', (17, 18))]","[('2 penalty', (2, 4)), ('310 ? 4', (5, 8)), ('dropout', (9, 10)), ('validation', (13, 14)), ('0.1', (18, 19))]","[['2 penalty', 'of', '310 ? 4'], ['dropout', 'chosen by', 'validation']]",[],"[['Hyperparameters', 'applied', '2 penalty']]",[],natural_language_inference,19,131
2736,hyperparameters,"Initial learning rate was set to 1 , and a power decay was applied .","[('set to', (4, 6)), ('applied', (13, 14))]","[('Initial learning rate', (0, 3)), ('1', (6, 7)), ('power decay', (10, 12))]","[['Initial learning rate', 'set to', '1']]","[['Initial learning rate', 'has', '1']]",[],"[['Hyperparameters', 'has', 'Initial learning rate']]",natural_language_inference,19,134
2737,hyperparameters,We used stochastic gradient descent with a batch size of 50 .,"[('used', (1, 2)), ('with', (5, 6)), ('of', (9, 10))]","[('stochastic gradient descent', (2, 5)), ('batch size', (7, 9)), ('50', (10, 11))]","[['stochastic gradient descent', 'with', 'batch size'], ['batch size', 'of', '50']]",[],"[['Hyperparameters', 'used', 'stochastic gradient descent']]",[],natural_language_inference,19,135
2738,results,"As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of humanengineered features , long short term memory .","[('followed by', (9, 11)), ('without', (23, 24)), ('including', (27, 28)), ('using', (33, 34)), ('of', (36, 37))]","[('TBCNN sentence pair model', (4, 8)), ('simple concatenation alone', (11, 14)), ('outperforms', (15, 16)), ('existing sentence encoding - based approaches', (16, 22)), ('pretraining', (24, 25)), ('feature - rich method', (29, 33)), ('6 groups', (34, 36)), ('humanengineered features', (37, 39)), ('long short term memory', (40, 44))]","[['TBCNN sentence pair model', 'followed by', 'simple concatenation alone'], ['existing sentence encoding - based approaches', 'without', 'pretraining'], ['existing sentence encoding - based approaches', 'including', 'feature - rich method'], ['feature - rich method', 'using', '6 groups'], ['6 groups', 'of', 'humanengineered features']]","[['simple concatenation alone', 'has', 'outperforms'], ['outperforms', 'has', 'existing sentence encoding - based approaches'], ['existing sentence encoding - based approaches', 'name', 'pretraining'], ['humanengineered features', 'name', 'long short term memory']]",[],[],natural_language_inference,19,138
2739,experiments,Model Variant,[],[],[],[],[],[],natural_language_inference,19,140
2740,results,We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,"[('using', (7, 8)), ('is', (13, 14)), ('than', (16, 17))]","[('each heuristic', (3, 5)), ('element - wise product alone', (8, 13)), ('significantly worse', (14, 16)), ('concatenation or element - wise difference', (17, 23))]","[['each heuristic', 'using', 'element - wise product alone'], ['element - wise product alone', 'is', 'significantly worse'], ['significantly worse', 'than', 'concatenation or element - wise difference']]","[['element - wise product alone', 'has', 'significantly worse']]",[],[],natural_language_inference,19,144
2741,results,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .","[('Combining', (0, 1)), ('improves', (4, 5)), ('with', (13, 14)), ('yields', (22, 23)), ('of', (26, 27))]","[('different matching heuristics', (1, 4)), ('result', (6, 7)), ('TBCNN - pair model', (9, 13)), ('concatenation', (14, 15)), ('element - wise product', (16, 20)), ('highest performance', (24, 26)), ('82.1 %', (27, 29))]","[['different matching heuristics', 'improves', 'result'], ['TBCNN - pair model', 'with', 'concatenation'], ['highest performance', 'of', '82.1 %']]","[['different matching heuristics', 'has', 'result'], ['result', 'has', 'TBCNN - pair model']]","[['Results', 'Combining', 'different matching heuristics']]",[],natural_language_inference,19,145
2742,results,Further applying element - wise product improves the accuracy by another 0.5 % .,"[('applying', (1, 2)), ('improves', (6, 7)), ('by', (9, 10))]","[('element - wise product', (2, 6)), ('accuracy', (8, 9)), ('another 0.5 %', (10, 13))]","[['element - wise product', 'improves', 'accuracy'], ['accuracy', 'by', 'another 0.5 %']]",[],"[['Results', 'applying', 'element - wise product']]",[],natural_language_inference,19,148
2743,results,"The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .","[('in - cluding', (15, 18)), ('with', (29, 30))]","[('full TBCNN - pair model', (1, 6)), ('outperforms', (6, 7)), ('all existing sentence encoding - based approaches', (7, 14)), ('1024d gated recurrent unit ( GRU ) - based RNN', (19, 29)), ('"" skip - thought "" pretraining', (30, 36))]","[['all existing sentence encoding - based approaches', 'in - cluding', '1024d gated recurrent unit ( GRU ) - based RNN'], ['1024d gated recurrent unit ( GRU ) - based RNN', 'with', '"" skip - thought "" pretraining']]","[['full TBCNN - pair model', 'has', 'outperforms'], ['outperforms', 'has', 'all existing sentence encoding - based approaches']]",[],"[['Results', 'has', 'full TBCNN - pair model']]",natural_language_inference,19,149
2744,research-problem,Stochastic Answer Networks for Natural Language Inference,[],[],[],[],[],[],natural_language_inference,2,2
2745,research-problem,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .",[],"[('natural language inference', (1, 4)), ('recognizing textual entailment ( RTE )', (9, 15))]",[],[],[],[],natural_language_inference,2,9
2746,research-problem,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .","[('on', (22, 23))]","[('Machine Reading Comprehension ( MRC )', (9, 15)), ('NLI', (23, 24))]",[],[],[],[],natural_language_inference,2,17
2747,experimental-setup,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .,"[('used to', (5, 7)), ('to implement', (15, 17))]","[('spaCy tool', (1, 3)), ('tokenize', (7, 8)), ('all the dataset', (8, 11)), ('our models', (17, 19))]","[['spaCy tool', 'used to', 'tokenize']]","[['tokenize', 'has', 'all the dataset']]",[],"[['Experimental setup', 'has', 'spaCy tool']]",natural_language_inference,2,80
2748,experimental-setup,We fix word embedding with 300 - dimensional GloVe word vectors .,"[('fix', (1, 2)), ('with', (4, 5))]","[('word embedding', (2, 4)), ('300 - dimensional GloVe word vectors', (5, 11))]","[['word embedding', 'with', '300 - dimensional GloVe word vectors']]",[],"[['Experimental setup', 'fix', 'word embedding']]",[],natural_language_inference,2,81
2749,experimental-setup,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .","[('For', (0, 1)), ('use', (6, 7)), ('of', (9, 10)), ('with', (15, 16))]","[('character encoding', (2, 4)), ('concatenation', (8, 9)), ('multi-filter Convolutional Neural Nets', (11, 15)), ('windows 1 , 3 , 5', (16, 22)), ('hidden size', (24, 26)), ('50 , 100 , 150', (26, 31))]","[['character encoding', 'use', 'concatenation'], ['concatenation', 'of', 'multi-filter Convolutional Neural Nets'], ['multi-filter Convolutional Neural Nets', 'with', 'windows 1 , 3 , 5'], ['multi-filter Convolutional Neural Nets', 'with', 'hidden size']]","[['character encoding', 'has', 'concatenation'], ['hidden size', 'has', '50 , 100 , 150']]","[['Experimental setup', 'For', 'character encoding']]",[],natural_language_inference,2,82
2750,experimental-setup,So lexicon embeddings are d =600 - dimensions .,"[('are', (3, 4))]","[('lexicon embeddings', (1, 3)), ('d =600 - dimensions', (4, 8))]","[['lexicon embeddings', 'are', 'd =600 - dimensions']]","[['lexicon embeddings', 'has', 'd =600 - dimensions']]",[],"[['Experimental setup', 'has', 'lexicon embeddings']]",natural_language_inference,2,84
2751,experimental-setup,The embedding for the out - of - vocabulary is zeroed .,"[('for', (2, 3)), ('is', (9, 10))]","[('embedding', (1, 2)), ('out - of - vocabulary', (4, 9)), ('zeroed', (10, 11))]","[['embedding', 'for', 'out - of - vocabulary'], ['embedding', 'is', 'zeroed'], ['out - of - vocabulary', 'is', 'zeroed']]",[],[],"[['Experimental setup', 'has', 'embedding']]",natural_language_inference,2,85
2752,experimental-setup,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .","[('of', (3, 4)), ('in', (5, 6)), ('set to', (15, 17)), ('of', (23, 24)), ('is', (26, 27))]","[('hidden size', (1, 3)), ('LSTM', (4, 5)), ('contextual encoding layer', (7, 10)), ('memory generation layer', (11, 14)), ('128', (17, 18)), ('input size', (21, 23)), ('output layer', (24, 26)), ('1024 ( 128 * 2 * 4 )', (27, 35))]","[['hidden size', 'of', 'LSTM'], ['input size', 'of', 'output layer'], ['LSTM', 'in', 'contextual encoding layer'], ['memory generation layer', 'set to', '128'], ['input size', 'of', 'output layer'], ['output layer', 'is', '1024 ( 128 * 2 * 4 )']]","[['hidden size', 'has', 'LSTM']]",[],"[['Experimental setup', 'has', 'hidden size']]",natural_language_inference,2,86
2753,experimental-setup,The projection size in the attention layer is set to 256 .,"[('in', (3, 4)), ('set to', (8, 10))]","[('projection size', (1, 3)), ('attention layer', (5, 7)), ('256', (10, 11))]","[['projection size', 'in', 'attention layer'], ['projection size', 'set to', '256']]",[],[],"[['Experimental setup', 'has', 'projection size']]",natural_language_inference,2,87
2754,experimental-setup,"To speedup training , we use weight normalization .","[('To speedup', (0, 2)), ('use', (5, 6))]","[('weight normalization', (6, 8))]",[],[],"[['Experimental setup', 'To speedup', 'weight normalization']]",[],natural_language_inference,2,88
2755,experimental-setup,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .","[('is', (3, 4)), ('fixed through', (11, 13)), ('in', (15, 16))]","[('dropout rate', (1, 3)), ('0.2', (4, 5)), ('dropout mask', (8, 10)), ('time steps', (13, 15)), ('LSTM', (16, 17))]","[['dropout rate', 'is', '0.2'], ['dropout mask', 'fixed through', 'time steps'], ['time steps', 'in', 'LSTM']]","[['dropout rate', 'has', '0.2']]",[],"[['Experimental setup', 'has', 'dropout rate']]",natural_language_inference,2,89
2756,experimental-setup,The mini - batch size is set to 32 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('32', (8, 9))]","[['mini - batch size', 'set to', '32']]","[['mini - batch size', 'has', '32']]",[],"[['Experimental setup', 'has', 'mini - batch size']]",natural_language_inference,2,90
2757,experimental-setup,Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,"[('is', (2, 3)), ('initialized as', (9, 11)), ('decreased by', (13, 15)), ('after', (16, 17))]","[('Our optimizer', (0, 2)), ('Adamax', (3, 4)), ('learning rate', (6, 8)), ('0.002', (11, 12)), ('0.5', (15, 16)), ('each 10 epochs', (17, 20))]","[['Our optimizer', 'is', 'Adamax'], ['learning rate', 'initialized as', '0.002'], ['learning rate', 'decreased by', '0.5'], ['0.5', 'after', 'each 10 epochs']]","[['Our optimizer', 'has', 'Adamax']]",[],"[['Experimental setup', 'has', 'Our optimizer']]",natural_language_inference,2,91
2758,results,"Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .","[('in', (6, 7)), ('except on', (12, 14))]","[('Our model', (0, 2)), ('outperforms', (2, 3)), ('best system', (4, 6)), ('RepEval 2017', (7, 9)), ('"" Conditional "" and "" Tense Difference "" categories', (14, 23))]","[['outperforms', 'in', 'RepEval 2017'], ['best system', 'in', 'RepEval 2017']]","[['Our model', 'has', 'outperforms'], ['outperforms', 'has', 'best system']]",[],"[['Results', 'has', 'Our model']]",natural_language_inference,2,128
2759,results,"We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .","[('find', (2, 3)), ('works', (5, 6)), ('on', (8, 9))]","[('SAN', (4, 5)), ('extremely well', (6, 8)), ('"" Active / Passive "" and "" Paraphrase "" categories', (9, 19))]","[['SAN', 'works', 'extremely well'], ['extremely well', 'on', '"" Active / Passive "" and "" Paraphrase "" categories']]",[],"[['Results', 'find', 'SAN']]",[],natural_language_inference,2,129
2760,results,"Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .","[('Comparing with', (0, 2)), ('of', (9, 10)), ('on', (23, 24))]","[(""Chen 's model"", (2, 5)), ('biggest improvement', (7, 9)), ('SAN', (10, 11)), ('Matched and Mismatched settings', (24, 28)), ('"" Antonym "" category', (33, 37))]","[['biggest improvement', 'of', 'SAN']]","[[""Chen 's model"", 'has', 'biggest improvement']]","[['Results', 'Comparing with', ""Chen 's model""]]",[],natural_language_inference,2,130
2761,research-problem,Neural Tree Indexers for Text Understanding,[],[],[],[],[],[],natural_language_inference,20,2
2762,research-problem,NTI constructs a full n-ary tree by processing the input text with its node function in a bottom - up fashion .,"[('constructs', (1, 2)), ('by processing', (6, 8)), ('with', (11, 12)), ('in', (15, 16))]","[('NTI', (0, 1)), ('full n-ary tree', (3, 6)), ('input text', (9, 11)), ('node function', (13, 15)), ('bottom - up fashion', (17, 21))]","[['NTI', 'constructs', 'full n-ary tree'], ['full n-ary tree', 'by processing', 'input text'], ['input text', 'with', 'node function'], ['node function', 'in', 'bottom - up fashion']]",[],[],[],natural_language_inference,20,8
2763,model,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .","[('introduce', (5, 6))]","[('Neural Tree Indexers ( NTI )', (6, 12))]",[],[],"[['Model', 'introduce', 'Neural Tree Indexers ( NTI )']]",[],natural_language_inference,20,19
2764,model,"Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling .","[('for', (8, 9)), ('is', (10, 11)), ('to', (22, 23))]","[('tree structure', (6, 8)), ('NTI', (9, 10)), ('relaxed', (11, 12)), ('input', (20, 21))]","[['tree structure', 'for', 'NTI'], ['tree structure', 'is', 'relaxed'], ['NTI', 'is', 'relaxed']]",[],[],"[['Model', 'has', 'tree structure']]",natural_language_inference,20,22
2765,model,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :","[('such as', (6, 8)), ('chosen', (10, 11)), ('forms', (15, 16)), ('taking advantage of', (22, 25)), ('of', (30, 31))]","[('sequential leaf node transformer', (2, 6)), ('LSTM', (8, 9)), ('NTI network', (13, 15)), ('sequence - tree hybrid model', (17, 22)), ('conditional and compositional powers', (26, 30)), ('sequential and recursive models', (31, 35))]","[['sequential leaf node transformer', 'such as', 'LSTM'], ['LSTM', 'chosen', 'NTI network'], ['NTI network', 'forms', 'sequence - tree hybrid model'], ['sequence - tree hybrid model', 'taking advantage of', 'conditional and compositional powers'], ['conditional and compositional powers', 'of', 'sequential and recursive models']]","[['sequential leaf node transformer', 'name', 'LSTM']]",[],[],natural_language_inference,20,24
2766,model,( b ) NTI learns representations for the premise and hypothesis sentences and then attentively combines them for classification .,"[('learns', (4, 5)), ('for', (6, 7)), ('attentively', (14, 15)), ('combines them for', (15, 18))]","[('NTI', (3, 4)), ('representations', (5, 6)), ('premise and hypothesis sentences', (8, 12)), ('classification', (18, 19))]","[['NTI', 'learns', 'representations'], ['representations', 'for', 'premise and hypothesis sentences']]",[],[],"[['Model', 'has', 'NTI']]",natural_language_inference,20,29
2767,hyperparameters,We trained NTI using Adam with hyperparameters selected on development set .,"[('trained', (1, 2)), ('using', (3, 4))]","[('NTI', (2, 3)), ('Adam', (4, 5))]","[['NTI', 'using', 'Adam']]",[],"[['Hyperparameters', 'trained', 'NTI']]",[],natural_language_inference,20,136
2768,hyperparameters,The pre-trained 300 - D Glove 840B vectors were obtained for the word embeddings,"[('obtained for', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (1, 8))]",[],[],[],"[['Hyperparameters', 'has', 'pre-trained 300 - D Glove 840B vectors']]",natural_language_inference,20,137
2769,hyperparameters,The word embeddings are fixed during training .,"[('fixed during', (4, 6))]","[('word embeddings', (1, 3)), ('training', (6, 7))]","[['word embeddings', 'fixed during', 'training']]",[],[],"[['Hyperparameters', 'has', 'word embeddings']]",natural_language_inference,20,139
2770,hyperparameters,The embeddings for out - ofvocabulary words were set to zero vector .,"[('for', (2, 3)), ('set to', (8, 10))]","[('embeddings', (1, 2)), ('out - ofvocabulary words', (3, 7)), ('zero vector', (10, 12))]","[['embeddings', 'for', 'out - ofvocabulary words'], ['embeddings', 'set to', 'zero vector']]",[],[],"[['Hyperparameters', 'has', 'embeddings']]",natural_language_inference,20,140
2771,hyperparameters,The size of hidden units of the NTI modules were set to 300 .,"[('of', (2, 3)), ('of', (5, 6)), ('set to', (10, 12))]","[('size', (1, 2)), ('hidden units', (3, 5)), ('NTI modules', (7, 9)), ('300', (12, 13))]","[['size', 'of', 'hidden units'], ['hidden units', 'of', 'NTI modules'], ['size', 'of', 'hidden units'], ['hidden units', 'of', 'NTI modules'], ['size', 'set to', '300'], ['hidden units', 'set to', '300']]",[],[],"[['Hyperparameters', 'has', 'size']]",natural_language_inference,20,144
2772,hyperparameters,The models were regularized by using dropouts and an l 2 weight decay .,"[('regularized by', (3, 5))]","[('dropouts', (6, 7)), ('l 2 weight decay', (9, 13))]",[],[],"[['Hyperparameters', 'regularized by', 'dropouts']]",[],natural_language_inference,20,145
2773,experiments,Natural Language Inference,[],[],[],[],[],[],natural_language_inference,20,147
2774,experiments,"For each model , we set the batch size to 32 .","[('set', (5, 6)), ('to', (9, 10))]","[('batch size', (7, 9)), ('32', (10, 11))]","[['batch size', 'to', '32']]","[['batch size', 'has', '32']]",[],[],natural_language_inference,20,152
2775,experiments,"The initial learning , the regularization strength and the number of epoch to be trained are varied for each model .","[('to', (12, 13))]","[('initial learning', (1, 3)), ('regularization strength', (5, 7)), ('number of epoch', (9, 12)), ('trained', (14, 15)), ('each model', (18, 20))]","[['number of epoch', 'to', 'trained']]",[],[],[],natural_language_inference,20,153
2776,experiments,NTI - SLSTM : this model does not rely on f leaf transformer but uses the S - LSTM units for the non-leaf node function .,"[('uses', (14, 15)), ('for', (20, 21))]","[('NTI - SLSTM', (0, 3)), ('S - LSTM units', (16, 20)), ('non-leaf node function', (22, 25))]","[['NTI - SLSTM', 'uses', 'S - LSTM units'], ['S - LSTM units', 'for', 'non-leaf node function']]",[],[],[],natural_language_inference,20,154
2777,experiments,"We set the initial learning rate to 1e - 3 and l 2 regularizer strength to 3 e - 5 , and train the model for 90 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('1e - 3', (7, 10)), ('regularizer strength', (13, 15)), ('3 e - 5', (16, 20)), ('model', (24, 25)), ('90 epochs', (26, 28))]","[['initial learning rate', 'to', '1e - 3'], ['regularizer strength', 'to', '3 e - 5'], ['regularizer strength', 'to', '3 e - 5'], ['model', 'for', '90 epochs']]","[['initial learning rate', 'has', '1e - 3'], ['regularizer strength', 'has', '3 e - 5']]",[],[],natural_language_inference,20,155
2778,experiments,The neural net was regularized by 10 % input dropouts and the 20 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('10 % input dropouts', (6, 10)), ('20 % output dropouts', (12, 16))]","[['neural net', 'regularized by', '10 % input dropouts'], ['neural net', 'regularized by', '20 % output dropouts']]",[],[],[],natural_language_inference,20,156
2779,experiments,NTI - SLSTM - LSTM : we use LSTM for the leaf node function f leaf .,"[('use', (7, 8)), ('for', (9, 10))]","[('NTI - SLSTM - LSTM', (0, 5)), ('leaf node function f leaf', (11, 16))]",[],[],[],[],natural_language_inference,20,157
2780,experiments,NTI - SLSTM node - by - node global attention :,[],"[('NTI - SLSTM node - by - node global attention', (0, 10))]",[],[],[],[],natural_language_inference,20,162
2781,experiments,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 40 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('3e - 4', (7, 10)), ('regularizer strength', (13, 15)), ('1 e - 5', (16, 20)), ('model', (24, 25)), ('40 epochs', (26, 28))]","[['initial learning rate', 'to', '3e - 4'], ['regularizer strength', 'to', '1 e - 5'], ['regularizer strength', 'to', '1 e - 5'], ['model', 'for', '40 epochs']]","[['initial learning rate', 'has', '3e - 4'], ['regularizer strength', 'has', '1 e - 5']]",[],[],natural_language_inference,20,165
2782,experiments,The neural net was regularized by 15 % input dropouts and the 15 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('15 % input dropouts', (6, 10)), ('15 % output dropouts', (12, 16))]","[['neural net', 'regularized by', '15 % input dropouts'], ['neural net', 'regularized by', '15 % output dropouts']]",[],[],[],natural_language_inference,20,166
2783,experiments,NTI - SLSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,[],"[('NTI - SLSTM node - by - node tree attention', (0, 10))]",[],[],[],[],natural_language_inference,20,167
2784,experiments,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 10 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('3e - 4', (7, 10)), ('regularizer strength', (13, 15)), ('1 e - 5', (16, 20)), ('model', (24, 25)), ('10 epochs', (26, 28))]","[['initial learning rate', 'to', '3e - 4'], ['regularizer strength', 'to', '1 e - 5'], ['regularizer strength', 'to', '1 e - 5'], ['model', 'for', '10 epochs']]","[['initial learning rate', 'has', '3e - 4'], ['regularizer strength', 'has', '1 e - 5']]",[],[],natural_language_inference,20,171
2785,experiments,The neural net was regularized by 10 % input dropouts and the 15 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('10 % input dropouts', (6, 10)), ('15 % output dropouts', (12, 16))]","[['neural net', 'regularized by', '10 % input dropouts'], ['neural net', 'regularized by', '15 % output dropouts']]",[],[],[],natural_language_inference,20,172
2786,experiments,NTI - SLSTM - LSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,[],"[('NTI - SLSTM - LSTM node - by - node tree attention', (0, 12))]",[],[],[],[],natural_language_inference,20,173
2787,experiments,Tree matching NTI - SLSTM - LSTM global attention : this model first constructs the premise and hypothesis trees simultaneously with the NTI - SLSTM - LSTM model and then computes their matching vector by using the global attention and an additional LSTM .,"[('first constructs', (12, 14)), ('with', (20, 21)), ('computes', (30, 31)), ('by using', (34, 36))]","[('Tree matching NTI - SLSTM - LSTM global attention', (0, 9)), ('premise and hypothesis', (15, 18)), ('NTI - SLSTM - LSTM model', (22, 28)), ('matching vector', (32, 34)), ('global attention and an additional LSTM', (37, 43))]","[['Tree matching NTI - SLSTM - LSTM global attention', 'first constructs', 'premise and hypothesis'], ['Tree matching NTI - SLSTM - LSTM global attention', 'computes', 'matching vector'], ['matching vector', 'by using', 'global attention and an additional LSTM']]",[],[],[],natural_language_inference,20,175
2788,experiments,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train the model for 20 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('3e - 4', (7, 10)), ('regularizer strength', (13, 15)), ('3 e - 5', (16, 20)), ('model', (24, 25)), ('20 epochs', (26, 28))]","[['initial learning rate', 'to', '3e - 4'], ['regularizer strength', 'to', '3 e - 5'], ['regularizer strength', 'to', '3 e - 5'], ['model', 'for', '20 epochs']]","[['initial learning rate', 'has', '3e - 4'], ['regularizer strength', 'has', '3 e - 5']]",[],[],natural_language_inference,20,180
2789,experiments,The neural net was regularized by 20 % input dropouts and the 20 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('20 % input dropouts', (6, 10)), ('20 % output dropouts', (12, 16))]","[['neural net', 'regularized by', '20 % input dropouts'], ['neural net', 'regularized by', '20 % output dropouts']]",[],[],[],natural_language_inference,20,181
2790,experiments,Tree matching NTI - SLSTM - LSTM tree attention : we replace the global attention with the tree attention .,"[('replace', (11, 12)), ('with', (15, 16))]","[('Tree matching NTI - SLSTM - LSTM tree attention', (0, 9)), ('global attention', (13, 15)), ('tree attention', (17, 19))]","[['Tree matching NTI - SLSTM - LSTM tree attention', 'replace', 'global attention'], ['global attention', 'with', 'tree attention']]",[],[],[],natural_language_inference,20,182
2791,experiments,"Full tree matching NTI - SLSTM - LSTM global attention : this model produces two sets of the attention vectors , one by attending over the premise tree regarding each hypothesis tree node and another by attending over the hypothesis tree regarding each premise tree node .","[('attending', (23, 24)), ('attending over', (36, 38)), ('regarding', (41, 42))]","[('Full tree matching NTI - SLSTM - LSTM global attention', (0, 10)), ('each premise tree node', (42, 46))]",[],[],[],[],natural_language_inference,20,184
2792,experiments,Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model .,"[('is', (6, 7)), ('obtained with', (10, 12))]","[('best score', (1, 3)), ('87.3 % accuracy', (7, 10)), ('full tree matching NTI model', (13, 18))]","[['best score', 'is', '87.3 % accuracy'], ['87.3 % accuracy', 'obtained with', 'full tree matching NTI model']]","[['best score', 'has', '87.3 % accuracy']]",[],[],natural_language_inference,20,193
2793,experiments,Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % .,"[('show', (2, 3)), ('improved', (7, 8)), ('of', (10, 11)), ('by', (15, 16))]","[('NTI - SLSTM', (4, 7)), ('performance', (9, 10)), ('sequential LSTM encoder', (12, 15)), ('approximately 2 %', (16, 19))]","[['NTI - SLSTM', 'improved', 'performance'], ['performance', 'of', 'sequential LSTM encoder'], ['sequential LSTM encoder', 'by', 'approximately 2 %']]",[],[],[],natural_language_inference,20,195
2794,experiments,"Not surprisingly , using LSTM as leaf node function helps in learning better representations .","[('using', (3, 4)), ('as', (5, 6)), ('helps in', (9, 11))]","[('LSTM', (4, 5)), ('leaf node function', (6, 9)), ('learning', (11, 12)), ('better representations', (12, 14))]","[['LSTM', 'as', 'leaf node function'], ['LSTM', 'helps in', 'learning']]","[['learning', 'has', 'better representations']]",[],[],natural_language_inference,20,196
2795,experiments,Our NTI - SLSTM - LSTM is a hybrid model which encodes a sequence sequentially through its leaf node function and then hierarchically composes the output representations .,"[('is', (6, 7)), ('encodes', (11, 12)), ('through', (15, 16)), ('hierarchically composes', (22, 24))]","[('Our', (0, 1)), ('NTI - SLSTM - LSTM', (1, 6)), ('hybrid model', (8, 10)), ('sequence sequentially', (13, 15)), ('leaf node function', (17, 20)), ('output representations', (25, 27))]","[['NTI - SLSTM - LSTM', 'is', 'hybrid model'], ['hybrid model', 'encodes', 'sequence sequentially'], ['sequence sequentially', 'through', 'leaf node function'], ['hybrid model', 'hierarchically composes', 'output representations']]","[['Our', 'has', 'NTI - SLSTM - LSTM'], ['NTI - SLSTM - LSTM', 'has', 'hybrid model']]",[],[],natural_language_inference,20,197
2796,experiments,"The node - by - node attention models improve the performance , indicating that modeling inter-sentence interaction is an important element in NLI .","[('improve', (8, 9))]","[('node - by - node attention models', (1, 8)), ('performance', (10, 11))]","[['node - by - node attention models', 'improve', 'performance']]","[['node - by - node attention models', 'has', 'performance']]",[],[],natural_language_inference,20,198
2797,experiments,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .","[('by', (12, 13))]","[('Deep LSTM and LSTM attention models', (1, 7)), ('outperform', (7, 8)), ('previous best result', (9, 12)), ('large margin', (14, 16)), ('nearly 5 - 6 %', (17, 22))]","[['outperform', 'by', 'large margin'], ['previous best result', 'by', 'large margin']]","[['Deep LSTM and LSTM attention models', 'has', 'outperform'], ['outperform', 'has', 'previous best result'], ['large margin', 'has', 'nearly 5 - 6 %']]",[],[],natural_language_inference,20,221
2798,experiments,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,"[('improves', (1, 2)), ('sets', (6, 7)), ('by combining', (10, 12)), ('with', (14, 15))]","[('NASM', (0, 1)), ('result', (3, 4)), ('strong baseline', (8, 10)), ('variational autoencoder', (12, 14)), ('soft attention', (16, 18))]","[['NASM', 'improves', 'result'], ['NASM', 'sets', 'strong baseline'], ['strong baseline', 'by combining', 'variational autoencoder'], ['variational autoencoder', 'with', 'soft attention']]",[],[],[],natural_language_inference,20,222
2799,results,"Surprisingly , attention degree for the single word expression like "" stone "" , "" wall "" and "" leaves "" is lower to compare with multiword phrases .","[('for', (4, 5)), ('like', (9, 10)), ('lower', (22, 23)), ('to compare with', (23, 26))]","[('attention degree', (2, 4)), ('single word expression', (6, 9)), ('"" stone ""', (10, 13)), ('multiword phrases', (26, 28))]","[['attention degree', 'for', 'single word expression'], ['single word expression', 'like', '"" stone ""']]",[],[],"[['Results', 'has', 'attention degree']]",natural_language_inference,20,263
2800,experiments,Overall the NTI model is robust to the length of the phrases being matched .,"[('to', (6, 7))]","[('NTI model', (2, 4)), ('robust', (5, 6)), ('length of the phrases', (8, 12))]","[['robust', 'to', 'length of the phrases']]","[['NTI model', 'has', 'robust']]",[],[],natural_language_inference,20,276
2801,results,"When the padding size is less ( up to 10 ) , the NTI - SLSTM - LSTM model performs better .","[('When', (0, 1)), ('is', (4, 5)), ('performs', (19, 20))]","[('padding size', (2, 4)), ('less ( up to 10 )', (5, 11)), ('NTI - SLSTM - LSTM model', (13, 19)), ('better', (20, 21))]","[['padding size', 'is', 'less ( up to 10 )'], ['NTI - SLSTM - LSTM model', 'performs', 'better']]","[['padding size', 'has', 'less ( up to 10 )'], ['less ( up to 10 )', 'has', 'NTI - SLSTM - LSTM model']]","[['Results', 'When', 'padding size']]",[],natural_language_inference,20,288
2802,results,Overall we do not observe any significant performance drop for both models as the padding size increases .,"[('do not observe', (2, 5)), ('for', (9, 10)), ('as', (12, 13))]","[('significant performance drop', (6, 9)), ('both models', (10, 12)), ('padding size', (14, 16)), ('increases', (16, 17))]","[['significant performance drop', 'for', 'both models'], ['significant performance drop', 'as', 'padding size'], ['both models', 'as', 'padding size']]","[['padding size', 'has', 'increases']]","[['Results', 'do not observe', 'significant performance drop']]",[],natural_language_inference,20,290
2803,research-problem,Attention - over - Attention Neural Networks for Reading Comprehension,[],[],[],[],[],[],natural_language_inference,21,2
2804,model,"In addition to the primary model , we also propose an N - best re-ranking strategy to double check the validity of the candidates and further improve the performance .","[('propose', (9, 10)), ('to double check', (16, 19)), ('of', (21, 22))]","[('N - best re-ranking strategy', (11, 16)), ('validity', (20, 21)), ('candidates', (23, 24)), ('performance', (28, 29))]","[['N - best re-ranking strategy', 'to double check', 'validity'], ['validity', 'of', 'candidates']]",[],"[['Model', 'propose', 'N - best re-ranking strategy']]",[],natural_language_inference,21,8
2805,model,"In this paper , we present a novel neural network architecture , called attention - over - attention model .","[('present', (5, 6)), ('called', (12, 13))]","[('novel neural network architecture', (7, 11)), ('attention - over - attention model', (13, 19))]","[['novel neural network architecture', 'called', 'attention - over - attention model']]","[['novel neural network architecture', 'name', 'attention - over - attention model']]","[['Model', 'present', 'novel neural network architecture']]",[],natural_language_inference,21,20
2806,model,We also propose an N - best re-ranking strategy to re-score the candidates in various aspects and further improve the performance .,"[('propose', (2, 3)), ('to re-score', (9, 11)), ('in', (13, 14))]","[('N - best re-ranking strategy', (4, 9)), ('candidates', (12, 13)), ('various aspects', (14, 16)), ('performance', (20, 21))]","[['N - best re-ranking strategy', 'to re-score', 'candidates'], ['candidates', 'in', 'various aspects']]",[],"[['Model', 'propose', 'N - best re-ranking strategy']]",[],natural_language_inference,21,26
2807,experiments,Children 's Book Test,[],[],[],[],[],[],natural_language_inference,21,57
2808,baselines,Embedding Layer :,[],"[('Embedding Layer', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Embedding Layer']]",natural_language_inference,21,141
2809,experimental-setup,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ].","[('are', (3, 4)), ('in', (10, 11))]","[('embedding weights', (1, 3)), ('randomly initialized', (4, 6)), ('uniformed distribution', (8, 10))]","[['embedding weights', 'are', 'randomly initialized']]","[['embedding weights', 'has', 'randomly initialized']]",[],"[['Experimental setup', 'has', 'embedding weights']]",natural_language_inference,21,142
2810,experiments,CB Test NE CB Test CN Valid Test Valid Test Valid Test,[],"[('CB Test', (0, 2))]",[],[],[],[],natural_language_inference,21,145
2811,experimental-setup,Hidden Layer : Internal weights of GRUs are initialized with random orthogonal matrices .,"[('of', (5, 6)), ('initialized with', (8, 10))]","[('Hidden Layer', (0, 2)), ('Internal weights', (3, 5)), ('GRUs', (6, 7)), ('random orthogonal matrices', (10, 13))]","[['Internal weights', 'of', 'GRUs'], ['Hidden Layer', 'initialized with', 'random orthogonal matrices']]","[['Hidden Layer', 'has', 'Internal weights']]",[],"[['Experimental setup', 'has', 'Hidden Layer']]",natural_language_inference,21,148
2812,experiments,"We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .","[('adopted', (1, 2)), ('for', (4, 5)), ('with', (8, 9)), ('of', (13, 14))]","[('ADAM optimizer', (2, 4)), ('weight updating', (5, 7)), ('initial learning rate', (10, 13)), ('0.001', (14, 15))]","[['ADAM optimizer', 'for', 'weight updating'], ['ADAM optimizer', 'with', 'initial learning rate'], ['weight updating', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]","[['initial learning rate', 'has', '0.001']]",[],[],natural_language_inference,21,150
2813,experiments,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .","[('suffer from', (5, 7)), ('set', (13, 14)), ('to', (18, 19))]","[('GRU units', (2, 4)), ('gradient exploding issues', (8, 11)), ('gradient clipping threshold', (15, 18)), ('5', (19, 20))]","[['GRU units', 'suffer from', 'gradient exploding issues'], ['GRU units', 'set', 'gradient clipping threshold'], ['gradient exploding issues', 'set', 'gradient clipping threshold'], ['gradient clipping threshold', 'to', '5']]","[['gradient clipping threshold', 'has', '5']]",[],[],natural_language_inference,21,151
2814,experiments,We used batched training strategy of 32 samples .,"[('used', (1, 2)), ('of', (5, 6))]","[('batched training strategy', (2, 5)), ('32 samples', (6, 8))]","[['batched training strategy', 'of', '32 samples']]",[],[],[],natural_language_inference,21,152
2815,experiments,"In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .","[('In', (0, 1)), ('generate', (5, 6)), ('from', (10, 11))]","[('re-ranking step', (1, 3)), ('5 - best list', (6, 10)), ('baseline neural network model', (12, 16))]","[['re-ranking step', 'generate', '5 - best list'], ['5 - best list', 'from', 'baseline neural network model']]",[],[],[],natural_language_inference,21,154
2816,experimental-setup,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .","[('trained on', (5, 7)), ('of', (10, 11)), ('with', (14, 15)), ('trained by', (25, 27))]","[('language model features', (1, 4)), ('training proportion', (8, 10)), ('each dataset', (11, 13)), ('8 - gram wordbased setting', (15, 20)), ('Kneser - Ney smoothing', (21, 25)), ('SRILM toolkit', (27, 29))]","[['language model features', 'trained on', 'training proportion'], ['language model features', 'trained on', 'Kneser - Ney smoothing'], ['training proportion', 'of', 'each dataset'], ['training proportion', 'with', '8 - gram wordbased setting'], ['each dataset', 'with', '8 - gram wordbased setting'], ['Kneser - Ney smoothing', 'trained by', 'SRILM toolkit']]",[],[],"[['Experimental setup', 'has', 'language model features']]",natural_language_inference,21,155
2817,experiments,"The ensemble model is made up of four best models , which are trained using different random seed .","[('made up of', (4, 7)), ('trained using', (13, 15))]","[('ensemble model', (1, 3)), ('four best models', (7, 10)), ('different random seed', (15, 18))]","[['ensemble model', 'made up of', 'four best models'], ['four best models', 'trained using', 'different random seed']]",[],[],[],natural_language_inference,21,157
2818,experimental-setup,"Implementation is done with Theano ( Theano Development Team , 2016 ) and Keras , and all models are trained on Tesla K40 GPU . :","[('done with', (2, 4)), ('trained on', (19, 21))]","[('Implementation', (0, 1)), ('Theano ( Theano Development Team , 2016 ) and Keras', (4, 14))]","[['Implementation', 'done with', 'Theano ( Theano Development Team , 2016 ) and Keras']]",[],[],"[['Experimental setup', 'has', 'Implementation']]",natural_language_inference,21,158
2819,results,"As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .","[('by', (18, 19)), ('where', (23, 24)), ('over', (31, 32)), ('in', (33, 34)), ('demonstrate', (42, 43))]","[('our AoA Reader', (6, 9)), ('outperforms', (9, 10)), ('state - of - the - art systems', (10, 18)), ('large margin', (20, 22)), ('2.3 % and 2.0 % absolute improvements', (24, 31)), ('EpiReader', (32, 33)), ('CBTest NE and CN test sets', (34, 40))]","[['state - of - the - art systems', 'by', 'large margin'], ['large margin', 'where', '2.3 % and 2.0 % absolute improvements'], ['2.3 % and 2.0 % absolute improvements', 'over', 'EpiReader'], ['EpiReader', 'in', 'CBTest NE and CN test sets']]","[['our AoA Reader', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art systems']]",[],[],natural_language_inference,21,163
2820,results,"Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .","[('adding', (2, 3)), ('in', (5, 6)), ('there is', (10, 12)), ('over', (20, 21)), ('in', (24, 25))]","[('additional features', (3, 5)), ('re-ranking step', (7, 9)), ('another significant boost 2.0 % to 3.7 %', (12, 20)), ('Ao A Reader', (21, 24)), ('CBTest NE / CN test sets', (25, 31))]","[['additional features', 'in', 're-ranking step'], ['Ao A Reader', 'in', 'CBTest NE / CN test sets'], ['additional features', 'there is', 'another significant boost 2.0 % to 3.7 %'], ['re-ranking step', 'there is', 'another significant boost 2.0 % to 3.7 %'], ['another significant boost 2.0 % to 3.7 %', 'over', 'Ao A Reader'], ['Ao A Reader', 'in', 'CBTest NE / CN test sets']]",[],[],[],natural_language_inference,21,164
2821,results,"We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .","[('found', (3, 4)), ('with', (12, 13)), ('of', (26, 27)), ('beyond', (29, 30)), ('in', (38, 39))]","[('our single model', (5, 8)), ('stay', (9, 10)), ('on par', (10, 12)), ('previous best ensemble system', (14, 18)), ('absolute improvement', (24, 26)), ('0.9 %', (27, 29)), ('best ensemble model ( Iterative Attention )', (31, 38)), ('CBTest NE validation set', (40, 44))]","[['on par', 'with', 'previous best ensemble system'], ['absolute improvement', 'of', '0.9 %'], ['0.9 %', 'beyond', 'best ensemble model ( Iterative Attention )'], ['best ensemble model ( Iterative Attention )', 'in', 'CBTest NE validation set']]","[['our single model', 'has', 'stay'], ['stay', 'has', 'on par']]",[],[],natural_language_inference,21,165
2822,results,"When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .","[('shows', (11, 12)), ('over', (14, 15)), ('by', (19, 20)), ('setup', (24, 25))]","[('ensemble model', (4, 6)), ('our AoA Reader', (7, 10)), ('significant improvements', (12, 14)), ('previous best ensemble models', (15, 19)), ('large margin', (21, 23)), ('new state - of - the - art system', (26, 35))]","[['our AoA Reader', 'shows', 'significant improvements'], ['significant improvements', 'over', 'previous best ensemble models'], ['significant improvements', 'by', 'large margin'], ['previous best ensemble models', 'by', 'large margin'], ['significant improvements', 'setup', 'new state - of - the - art system'], ['previous best ensemble models', 'setup', 'new state - of - the - art system']]","[['ensemble model', 'has', 'our AoA Reader']]",[],[],natural_language_inference,21,166
2823,results,"Instead of using pre-defined merging heuristics , and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance , where 4.1 % and 3.7 % improvements can be made in CNN validation and test set against CAS Reader .","[('letting', (8, 9)), ('explicitly learn', (11, 13)), ('between', (15, 16)), ('results in', (18, 20)), ('in', (23, 24)), ('against', (43, 44))]","[('pre-defined merging heuristics', (3, 6)), ('model', (10, 11)), ('weights', (14, 15)), ('individual attentions', (16, 18)), ('significant boost', (21, 23)), ('performance', (25, 26)), ('CNN', (38, 39)), ('CAS Reader', (44, 46))]","[['model', 'explicitly learn', 'weights'], ['weights', 'between', 'individual attentions'], ['weights', 'results in', 'significant boost'], ['individual attentions', 'results in', 'significant boost'], ['significant boost', 'in', 'performance']]","[['pre-defined merging heuristics', 'has', 'model']]",[],[],natural_language_inference,21,168
2824,research-problem,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems,[],"[('Question Answering', (9, 11))]",[],[],[],[],natural_language_inference,22,2
2825,research-problem,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,[],"[('Question Answering ( QA )', (0, 5))]",[],[],[],[],natural_language_inference,22,4
2826,research-problem,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,[],"[('Sentence matching', (0, 2)), ('QA', (8, 9)), ('Paraphrase Identification ( PI )', (16, 21))]",[],[],[],[],natural_language_inference,22,5
2827,research-problem,"In addition , we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor ( ANN ) technology , which enables us to find the most similar question from all available candidates very quickly during online serving .","[('implement', (4, 5)), ('that combines', (10, 12)), ('enables', (26, 27)), ('during', (40, 41))]","[('general semantic retrieval framework', (6, 10)), ('our proposed model and the Approximate Nearest Neighbor ( ANN ) technology', (12, 24)), ('most similar question from all available candidates', (31, 38)), ('online serving', (41, 43))]","[['general semantic retrieval framework', 'that combines', 'our proposed model and the Approximate Nearest Neighbor ( ANN ) technology'], ['most similar question from all available candidates', 'during', 'online serving']]",[],"[['Research problem', 'implement', 'general semantic retrieval framework']]",[],natural_language_inference,22,8
2828,research-problem,Question answering systems have been widely studied in both the academic and industrial community and are widely applied to various scenarios .,[],"[('Question answering', (0, 2))]",[],[],[],[],natural_language_inference,22,11
2829,model,"In this work , we focus on building an IR - based QA system to answer the Frequently Asked Questions ( FAQ ) .","[('to answer', (14, 16))]","[('building', (7, 8)), ('IR - based QA system', (9, 14))]",[],"[['building', 'has', 'IR - based QA system']]",[],[],natural_language_inference,22,14
2830,research-problem,"The critical part of IRbased QA system is to find the most similar question from a massive QA knowledge base , which could be further reformulated as a Paraphrase Identification ( PI ) problem , also known as sentence matching .","[('known as', (36, 38))]","[('IRbased QA', (4, 6)), ('Paraphrase Identification ( PI ) problem', (28, 34)), ('sentence matching', (38, 40))]","[['Paraphrase Identification ( PI ) problem', 'known as', 'sentence matching']]",[],[],[],natural_language_inference,22,15
2831,model,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .","[('employ', (1, 2)), ('to depict', (5, 7)), ('between', (10, 11)), ('for', (12, 13)), ('propose', (18, 19)), ('solves', (27, 28))]","[('connected graph', (3, 5)), ('paraphrase relation', (8, 10)), ('sentences', (11, 12)), ('PI task', (14, 16)), ('multi-task sentence - encoding model', (20, 25)), ('paraphrase identification task', (29, 32)), ('sentence intent classification task', (34, 38))]","[['connected graph', 'to depict', 'paraphrase relation'], ['paraphrase relation', 'between', 'sentences'], ['sentences', 'for', 'PI task'], ['connected graph', 'propose', 'multi-task sentence - encoding model'], ['multi-task sentence - encoding model', 'solves', 'paraphrase identification task'], ['multi-task sentence - encoding model', 'solves', 'sentence intent classification task']]",[],"[['Model', 'employ', 'connected graph']]",[],natural_language_inference,22,28
2832,model,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .","[('propose', (1, 2)), ('integrates', (7, 8)), ('with', (15, 16)), ('allows', (24, 25)), ('from', (34, 35))]","[('semantic retrieval framework', (3, 6)), ('encoding - based sentence matching model', (9, 15)), ('approximate nearest neighbor search technology', (17, 22)), ('most similar question', (29, 32)), ('all available questions', (35, 38)), ('QA knowledge base', (49, 52))]","[['semantic retrieval framework', 'integrates', 'encoding - based sentence matching model'], ['encoding - based sentence matching model', 'with', 'approximate nearest neighbor search technology'], ['most similar question', 'from', 'all available questions']]",[],"[['Model', 'propose', 'semantic retrieval framework']]",[],natural_language_inference,22,29
2833,research-problem,Natural language sentence matching ( NLSM ) has gone through substantial developments in recent years .,[],"[('Natural language sentence matching ( NLSM )', (0, 7))]",[],[],[],[],natural_language_inference,22,36
2834,research-problem,"For the paraphrase identification ( PI ) task , NLSM is utilized to determine whether two sentences are paraphrases or not .",[],"[('paraphrase identification ( PI ) task', (2, 8)), ('NLSM', (9, 10))]",[],"[['paraphrase identification ( PI ) task', 'has', 'NLSM']]",[],[],natural_language_inference,22,38
2835,experiments,Bank Question ( BQ ) dataset is a specific - domain Chinese dataset for sentence semantic equivalence identification ( SSEI ) .,[],"[('sentence semantic equivalence identification ( SSEI )', (14, 21))]",[],[],[],[],natural_language_inference,22,149
2836,hyperparameters,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .","[('For', (0, 1)), ('use', (5, 6)), ('as', (13, 14))]","[('Quora dataset', (1, 3)), ('Glove - 840B - 300D vector', (7, 13)), ('pre-trained word embedding', (15, 18))]","[['Quora dataset', 'use', 'Glove - 840B - 300D vector'], ['Glove - 840B - 300D vector', 'as', 'pre-trained word embedding']]",[],"[['Hyperparameters', 'For', 'Quora dataset']]",[],natural_language_inference,22,153
2837,hyperparameters,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,"[('with', (6, 7)), ('of', (13, 14)), ('set to', (16, 18))]","[('character embedding', (1, 3)), ('randomly initialized', (4, 6)), ('150 D', (7, 9)), ('hidden size', (11, 13)), ('BiGRU', (14, 15)), ('300', (18, 19))]","[['randomly initialized', 'with', '150 D'], ['hidden size', 'of', 'BiGRU']]","[['character embedding', 'has', 'randomly initialized']]",[],"[['Hyperparameters', 'has', 'character embedding']]",natural_language_inference,22,154
2838,hyperparameters,We set = 0.8 in the multi - task loss function .,"[('set', (1, 2)), ('in', (4, 5))]","[('= 0.8', (2, 4)), ('multi - task loss function', (6, 11))]","[['= 0.8', 'in', 'multi - task loss function']]",[],"[['Hyperparameters', 'set', '= 0.8']]",[],natural_language_inference,22,155
2839,hyperparameters,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .","[('applied to', (4, 6)), ('of', (8, 9)), ('with', (14, 15)), ('of', (18, 19))]","[('Dropout layer', (0, 2)), ('output', (7, 8)), ('attentive pooling layer', (10, 13)), ('dropout rate', (16, 18)), ('0.1', (19, 20))]","[['Dropout layer', 'applied to', 'output'], ['output', 'of', 'attentive pooling layer'], ['dropout rate', 'of', '0.1'], ['Dropout layer', 'with', 'dropout rate'], ['attentive pooling layer', 'with', 'dropout rate'], ['dropout rate', 'of', '0.1']]","[['dropout rate', 'has', '0.1']]",[],"[['Hyperparameters', 'has', 'Dropout layer']]",natural_language_inference,22,157
2840,hyperparameters,An Adam optimizer is used to optimize all the trainable weights .,"[('to optimize', (5, 7))]","[('Adam optimizer', (1, 3)), ('all the trainable weights', (7, 11))]","[['Adam optimizer', 'to optimize', 'all the trainable weights']]",[],[],"[['Hyperparameters', 'has', 'Adam optimizer']]",natural_language_inference,22,158
2841,hyperparameters,The learning rate is set to 4e - 4 and the batch size is set to 200 .,"[('set to', (4, 6)), ('set to', (14, 16))]","[('learning rate', (1, 3)), ('4e - 4', (6, 9)), ('batch size', (11, 13)), ('200', (16, 17))]","[['learning rate', 'set to', '4e - 4'], ['batch size', 'set to', '200']]","[['learning rate', 'has', '4e - 4'], ['batch size', 'has', '200']]",[],"[['Hyperparameters', 'has', 'learning rate']]",natural_language_inference,22,159
2842,hyperparameters,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .","[('When', (0, 1)), ('of', (3, 4)), ('is', (6, 7)), ('with', (14, 15)), ('of', (18, 19)), ('to find', (24, 26))]","[('performance', (2, 3)), ('model', (5, 6)), ('no longer improved', (7, 10)), ('SGD optimizer', (12, 14)), ('learning rate', (16, 18)), ('1e - 3', (19, 22)), ('better local optimum', (27, 30))]","[['performance', 'of', 'model'], ['learning rate', 'of', '1e - 3'], ['performance', 'is', 'no longer improved'], ['model', 'is', 'no longer improved'], ['SGD optimizer', 'with', 'learning rate'], ['learning rate', 'of', '1e - 3'], ['1e - 3', 'to find', 'better local optimum']]","[['performance', 'has', 'model'], ['learning rate', 'has', '1e - 3']]","[['Hyperparameters', 'When', 'performance']]",[],natural_language_inference,22,160
2843,baselines,ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .,"[('is', (6, 7)), ('for', (12, 13))]","[('ESIM', (0, 1)), ('Enhanced Sequential Inference Model', (2, 6)), ('interaction - based model', (8, 12)), ('natural language inference', (13, 16))]","[['Enhanced Sequential Inference Model', 'is', 'interaction - based model'], ['interaction - based model', 'for', 'natural language inference']]","[['ESIM', 'has', 'Enhanced Sequential Inference Model']]",[],"[['Baselines', 'has', 'ESIM']]",natural_language_inference,22,163
2844,baselines,It uses BiLSTM to encode sentence contexts and uses the attention mechanism to calculate the information between two sentences .,"[('uses', (1, 2)), ('to encode', (3, 5)), ('uses', (8, 9)), ('to calculate', (12, 14)), ('between', (16, 17))]","[('BiLSTM', (2, 3)), ('sentence contexts', (5, 7)), ('attention mechanism', (10, 12)), ('information', (15, 16)), ('two sentences', (17, 19))]","[['BiLSTM', 'to encode', 'sentence contexts'], ['BiLSTM', 'uses', 'attention mechanism'], ['attention mechanism', 'to calculate', 'information'], ['information', 'between', 'two sentences']]",[],[],[],natural_language_inference,22,164
2845,results,ESIM has shown excellent performance on the SNLI dataset .,"[('shown', (2, 3)), ('on', (5, 6))]","[('ESIM', (0, 1)), ('excellent performance', (3, 5)), ('SNLI dataset', (7, 9))]","[['ESIM', 'shown', 'excellent performance'], ['excellent performance', 'on', 'SNLI dataset']]","[['ESIM', 'has', 'excellent performance']]",[],[],natural_language_inference,22,165
2846,baselines,BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .,"[('is', (7, 8)), ('with', (15, 16))]","[('BiMPM', (0, 1)), ('Bilateral Multi- Perspective Matching model', (2, 7)), ('interaction - based sentence matching model', (9, 15)), ('superior', (16, 17)), ('performance', (17, 18))]","[['Bilateral Multi- Perspective Matching model', 'is', 'interaction - based sentence matching model'], ['interaction - based sentence matching model', 'with', 'superior']]","[['BiMPM', 'has', 'Bilateral Multi- Perspective Matching model'], ['superior', 'has', 'performance']]",[],"[['Baselines', 'has', 'BiMPM']]",natural_language_inference,22,166
2847,baselines,"The model uses a BiLSTM layer to learn the sentence representation , four different types of multiperspective matching layers to match two sentences , an additional BiLSTM layer to aggregate the matching results , and a two - layer feed - forward network for prediction .","[('uses', (2, 3)), ('to learn', (6, 8)), ('to match', (19, 21)), ('to aggregate', (28, 30)), ('for', (43, 44))]","[('BiLSTM layer', (4, 6)), ('sentence representation', (9, 11)), ('four different types of multiperspective matching layers', (12, 19)), ('two sentences', (21, 23)), ('additional BiLSTM layer', (25, 28)), ('matching results', (31, 33)), ('two - layer feed - forward network', (36, 43)), ('prediction', (44, 45))]","[['BiLSTM layer', 'to learn', 'sentence representation'], ['four different types of multiperspective matching layers', 'to match', 'two sentences'], ['additional BiLSTM layer', 'to aggregate', 'matching results'], ['two - layer feed - forward network', 'for', 'prediction']]",[],[],[],natural_language_inference,22,167
2848,baselines,"SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .","[('is', (7, 8)), ('enhances', (16, 17)), ('with', (21, 22))]","[('SSE', (0, 1)), ('Shortcut - Stacked Sentence Encoder', (2, 7)), ('encodingbased sentence - matching model', (9, 14)), ('multi - layer BiLSTM', (17, 21)), ('short - cut connections', (22, 26))]","[['Shortcut - Stacked Sentence Encoder', 'is', 'encodingbased sentence - matching model'], ['encodingbased sentence - matching model', 'enhances', 'multi - layer BiLSTM'], ['multi - layer BiLSTM', 'with', 'short - cut connections']]","[['SSE', 'has', 'Shortcut - Stacked Sentence Encoder'], ['Shortcut - Stacked Sentence Encoder', 'has', 'encodingbased sentence - matching model']]",[],"[['Baselines', 'has', 'SSE']]",natural_language_inference,22,168
2849,baselines,DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .,"[('is', (6, 7)), ('for', (12, 13))]","[('Densely Interactive Inference Network', (2, 6)), ('interaction - based model', (8, 12)), ('natural language inference ( NLI )', (13, 19))]","[['Densely Interactive Inference Network', 'is', 'interaction - based model'], ['interaction - based model', 'for', 'natural language inference ( NLI )']]",[],[],"[['Baselines', 'has', 'Densely Interactive Inference Network']]",natural_language_inference,22,170
2850,baselines,It hierarchically extracts semantic features from interaction space to achieve a high - level understanding of the sentence pair .,"[('hierarchically extracts', (1, 3)), ('from', (5, 6)), ('to achieve', (8, 10)), ('of', (15, 16))]","[('semantic features', (3, 5)), ('interaction space', (6, 8)), ('high - level understanding', (11, 15)), ('sentence pair', (17, 19))]","[['semantic features', 'from', 'interaction space'], ['semantic features', 'to achieve', 'high - level understanding'], ['high - level understanding', 'of', 'sentence pair']]",[],[],[],natural_language_inference,22,171
2851,results,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,[],"[('LCQMC dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'LCQMC dataset']]",natural_language_inference,22,179
2852,results,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .","[('by', (15, 16)), ('reaching', (20, 21)), ('recording', (26, 27))]","[('our model', (4, 6)), ('outperforms', (6, 7)), ('state - of - the - art models', (7, 15)), ('large margin', (17, 19)), ('83 . 62 %', (21, 25)), ('state - of - the - art performance', (28, 36))]","[['state - of - the - art models', 'by', 'large margin'], ['state - of - the - art models', 'reaching', '83 . 62 %'], ['large margin', 'reaching', '83 . 62 %'], ['state - of - the - art models', 'recording', 'state - of - the - art performance'], ['83 . 62 %', 'recording', 'state - of - the - art performance']]","[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art models']]",[],"[['Results', 'has', 'our model']]",natural_language_inference,22,182
2853,results,As shown in show that our MSEM model achieves the best performance .,"[('show', (3, 4)), ('achieves', (8, 9))]","[('our MSEM model', (5, 8)), ('best performance', (10, 12))]","[['our MSEM model', 'achieves', 'best performance']]","[['our MSEM model', 'has', 'best performance']]",[],[],natural_language_inference,22,184
2854,results,And the model with multi-task learning further improved performance ranging from 0.4 % to 1 % .,"[('with', (3, 4)), ('further improved', (6, 8)), ('ranging from', (9, 11)), ('to', (13, 14))]","[('model', (2, 3)), ('multi-task learning', (4, 6)), ('performance', (8, 9)), ('0.4 %', (11, 13))]","[['model', 'with', 'multi-task learning'], ['multi-task learning', 'further improved', 'performance'], ['performance', 'ranging from', '0.4 %']]","[['model', 'has', 'multi-task learning']]",[],[],natural_language_inference,22,187
2855,results,"Compared with existing models , our model shows great advantages on datasets with low average overlap rate , which is known to be very common in realworld question answering scenarios .","[('shows', (7, 8)), ('on', (10, 11)), ('with', (12, 13))]","[('great advantages', (8, 10)), ('datasets', (11, 12)), ('low average overlap rate', (13, 17))]","[['great advantages', 'on', 'datasets'], ['datasets', 'with', 'low average overlap rate']]",[],"[['Results', 'shows', 'great advantages']]",[],natural_language_inference,22,188
2856,ablation-analysis,It turns out that the attentive pooling is better than max pooling .,"[('turns out that', (1, 4)), ('better than', (8, 10))]","[('attentive pooling', (5, 7)), ('max pooling', (10, 12))]","[['attentive pooling', 'better than', 'max pooling']]",[],"[['Ablation analysis', 'turns out that', 'attentive pooling']]",[],natural_language_inference,22,196
2857,ablation-analysis,"Then if we remove the highway network , the accuracy will drop to 88.36 % .","[('remove', (3, 4)), ('to', (12, 13))]","[('highway network', (5, 7)), ('accuracy', (9, 10)), ('drop', (11, 12)), ('88.36 %', (13, 15))]","[['drop', 'to', '88.36 %']]","[['highway network', 'has', 'accuracy'], ['accuracy', 'has', 'drop']]","[['Ablation analysis', 'remove', 'highway network']]",[],natural_language_inference,22,197
2858,ablation-analysis,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .","[('remove', (3, 4)), ('to', (14, 15))]","[('character - level embedding', (5, 9)), ('accuracy', (11, 12)), ('drop', (13, 14)), ('88.26 %', (15, 17))]","[['drop', 'to', '88.26 %']]","[['character - level embedding', 'has', 'accuracy']]","[['Ablation analysis', 'remove', 'character - level embedding']]",[],natural_language_inference,22,198
2859,results,"As shown in , the F 1 score of the new system is 14 . 26 % higher than the baseline system .","[('of', (8, 9)), ('is', (12, 13)), ('higher than', (17, 19))]","[('F 1 score', (5, 8)), ('new system', (10, 12)), ('14 . 26 %', (13, 17)), ('baseline system', (20, 22))]","[['F 1 score', 'of', 'new system'], ['F 1 score', 'is', '14 . 26 %'], ['new system', 'is', '14 . 26 %'], ['14 . 26 %', 'higher than', 'baseline system']]","[['F 1 score', 'has', 'new system']]",[],[],natural_language_inference,22,206
2860,research-problem,Deep Fusion LSTMs for Text Semantic Matching,[],[],[],[],[],[],natural_language_inference,23,2
2861,research-problem,"Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .",[],"[('modelling the interactions of text pair', (7, 13))]",[],[],[],[],natural_language_inference,23,4
2862,research-problem,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .",[],"[('modelling the relevance / similarity of a pair of texts', (25, 35)), ('text semantic matching', (40, 43))]",[],[],[],[],natural_language_inference,23,14
2863,model,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .","[('adopt', (5, 6)), ('to model', (10, 12)), ('of', (15, 16))]","[('deep fusion strategy', (7, 10)), ('strong interactions', (13, 15)), ('two sentences', (16, 18))]","[['deep fusion strategy', 'to model', 'strong interactions'], ['strong interactions', 'of', 'two sentences']]",[],"[['Model', 'adopt', 'deep fusion strategy']]",[],natural_language_inference,23,25
2864,research-problem,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .","[('regarded as', (6, 8)), ('of', (11, 12)), ('in', (14, 15))]","[('text matching', (2, 4)), ('modelling', (8, 9)), ('interaction', (10, 11)), ('two texts', (12, 14)), ('recursive matching way', (16, 19))]","[['text matching', 'regarded as', 'modelling'], ['interaction', 'of', 'two texts'], ['two texts', 'in', 'recursive matching way']]","[['text matching', 'has', 'modelling'], ['modelling', 'has', 'interaction']]",[],[],natural_language_inference,23,28
2865,model,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .","[('propose', (5, 6)), ('to model', (20, 22))]","[('deep fusion long short - term memory neural networks ( DF - LSTMs )', (6, 20)), ('interactions', (23, 24))]","[['deep fusion long short - term memory neural networks ( DF - LSTMs )', 'to model', 'interactions']]",[],"[['Model', 'propose', 'deep fusion long short - term memory neural networks ( DF - LSTMs )']]",[],natural_language_inference,23,29
2866,model,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .","[('consist of', (6, 8)), ('models', (16, 17)), ('under', (20, 21))]","[('DF - LSTMs', (3, 6)), ('two interconnected conditional LSTMs', (8, 12)), ('apiece of', (17, 19)), ('text', (19, 20)), ('influence', (22, 23)), ('another', (24, 25))]","[['DF - LSTMs', 'consist of', 'two interconnected conditional LSTMs'], ['two interconnected conditional LSTMs', 'models', 'apiece of'], ['apiece of', 'under', 'influence'], ['text', 'under', 'influence']]","[['apiece of', 'has', 'text']]",[],"[['Model', 'has', 'DF - LSTMs']]",natural_language_inference,23,30
2867,model,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,"[('of', (3, 4)), ('fed into', (8, 10)), ('to compute', (16, 18))]","[('output vector', (1, 3)), ('DF - LSTMs', (4, 7)), ('task - specific output layer', (11, 16)), ('match - ing score', (19, 23))]","[['output vector', 'of', 'DF - LSTMs'], ['DF - LSTMs', 'fed into', 'task - specific output layer'], ['task - specific output layer', 'to compute', 'match - ing score']]",[],[],"[['Model', 'has', 'output vector']]",natural_language_inference,23,31
2868,model,"Different with previous models , DF - LSTMs model the strong interactions of two texts in a recursive matching way , which consist of two inter -and intra-dependent LSTMs .","[('model', (8, 9)), ('of', (12, 13)), ('in', (15, 16)), ('consist of', (22, 24))]","[('strong interactions', (10, 12)), ('two texts', (13, 15)), ('recursive matching way', (17, 20)), ('two inter -and intra-dependent LSTMs', (24, 29))]","[['strong interactions', 'of', 'two texts'], ['two texts', 'in', 'recursive matching way'], ['strong interactions', 'consist of', 'two inter -and intra-dependent LSTMs'], ['recursive matching way', 'consist of', 'two inter -and intra-dependent LSTMs']]",[],"[['Model', 'model', 'strong interactions']]",[],natural_language_inference,23,34
2869,research-problem,Recursively Text Semantic Matching,[],[],[],[],[],[],natural_language_inference,23,40
2870,model,Long Short - Term Memory Network,[],[],[],[],[],[],natural_language_inference,23,53
2871,research-problem,Deep Fusion LSTMs for Recursively Semantic Matching,[],[],[],[],[],[],natural_language_inference,23,67
2872,baselines,Neural bag - of - words ( NBOW ) :,[],"[('Neural bag - of - words ( NBOW )', (0, 9))]",[],[],[],"[['Baselines', 'has', 'Neural bag - of - words ( NBOW )']]",natural_language_inference,23,139
2873,baselines,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .","[('represented as', (3, 5)), ('of', (7, 8)), ('of', (10, 11)), ('to', (22, 23))]","[('Each sequence', (0, 2)), ('sum', (6, 7)), ('embeddings', (9, 10)), ('words it contains', (12, 15)), ('concatenated and fed', (19, 22)), ('MLP', (24, 25))]","[['Each sequence', 'represented as', 'sum'], ['sum', 'of', 'embeddings'], ['embeddings', 'of', 'words it contains'], ['embeddings', 'of', 'words it contains'], ['concatenated and fed', 'to', 'MLP']]",[],[],"[['Baselines', 'has', 'Each sequence']]",natural_language_inference,23,140
2874,baselines,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .","[('encoded by', (6, 8))]","[('Single LSTM', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Single LSTM']]",natural_language_inference,23,141
2875,baselines,"Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .","[('encoded by', (7, 9))]","[('Parallel LSTMs', (0, 2)), ('two LSTMs', (9, 11)), ('concatenated', (16, 17)), ('MLP', (21, 22))]",[],[],[],"[['Baselines', 'has', 'Parallel LSTMs']]",natural_language_inference,23,142
2876,baselines,"Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .","[('encoded by', (6, 8)), ('with', (9, 10))]","[('Attention LSTMs', (0, 2)), ('Two sequences', (3, 5)), ('LSTMs', (8, 9)), ('attention mechanism', (10, 12))]","[['Two sequences', 'encoded by', 'LSTMs'], ['LSTMs', 'with', 'attention mechanism']]","[['Attention LSTMs', 'has', 'Two sequences']]",[],"[['Baselines', 'has', 'Attention LSTMs']]",natural_language_inference,23,143
2877,baselines,"Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :","[('of', (11, 12)), ('introduces', (16, 17))]","[('Word - by - word Attention LSTMs', (0, 7)), ('improved strategy', (9, 11)), ('word - by - word attention mechanism', (17, 24))]",[],"[['Word - by - word Attention LSTMs', 'has', 'improved strategy']]",[],"[['Baselines', 'has', 'Word - by - word Attention LSTMs']]",natural_language_inference,23,144
2878,results,Experiment - I : Recognizing Textual Entailment,[],[],[],[],[],[],natural_language_inference,23,146
2879,results,"The results of DF - LSTMs outperform all the competitor models with the same number of hidden states while achieving comparable results to the state - of - the - art and using much fewer parameters , which indicate that it is effective to model the strong interactions of two texts in a recursive matching way .","[('results of', (1, 3)), ('with', (11, 12)), ('achieving', (19, 20)), ('to', (22, 23))]","[('DF - LSTMs', (3, 6)), ('outperform', (6, 7)), ('all the competitor models', (7, 11)), ('same number of hidden states', (13, 18)), ('comparable results', (20, 22)), ('state - of - the - art', (24, 31)), ('much fewer parameters', (33, 36))]","[['all the competitor models', 'with', 'same number of hidden states'], ['comparable results', 'to', 'state - of - the - art']]","[['DF - LSTMs', 'has', 'outperform'], ['outperform', 'has', 'all the competitor models']]","[['Results', 'results of', 'DF - LSTMs']]",[],natural_language_inference,23,152
2880,results,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .","[('analyzing', (1, 2)), ('of', (5, 6)), ('can see', (11, 13)), ('consistently outperform', (25, 27)), ('with', (37, 38))]","[('evaluation results', (3, 5)), ('questionanswer matching', (6, 8)), ('strong interaction models ( attention LSTMs , our DF - LSTMs )', (13, 25)), ('weak interaction models ( NBOW , parallel LSTMs )', (28, 37)), ('large margin', (39, 41))]","[['evaluation results', 'of', 'questionanswer matching'], ['evaluation results', 'can see', 'strong interaction models ( attention LSTMs , our DF - LSTMs )'], ['questionanswer matching', 'can see', 'strong interaction models ( attention LSTMs , our DF - LSTMs )'], ['strong interaction models ( attention LSTMs , our DF - LSTMs )', 'consistently outperform', 'weak interaction models ( NBOW , parallel LSTMs )'], ['weak interaction models ( NBOW , parallel LSTMs )', 'with', 'large margin']]","[['evaluation results', 'has', 'questionanswer matching']]","[['Results', 'analyzing', 'evaluation results']]",[],natural_language_inference,23,156
2881,research-problem,Using Wikipedia articles as the knowledge source causes the task of question answering ( QA ) to combine the challenges of both large - scale open - domain QA and of machine comprehension of text .,[],"[('question answering ( QA )', (11, 16))]",[],[],[],[],natural_language_inference,24,13
2882,research-problem,"We term this setting , machine reading at scale ( MRS ) .",[],"[('machine reading at scale ( MRS )', (5, 12))]",[],[],[],[],natural_language_inference,24,15
2883,model,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,"[('treats', (2, 3)), ('as', (4, 5))]","[('Wikipedia', (3, 4)), ('collection of articles', (6, 9))]","[['Wikipedia', 'as', 'collection of articles']]",[],"[['Model', 'treats', 'Wikipedia']]",[],natural_language_inference,24,16
2884,model,"Instead MRS is focused on simultaneously maintaining the challenge of machine comprehension , which requires the deep understanding of text , while keeping the realistic constraint of searching over a large open resource .","[('focused on', (3, 5))]","[('MRS', (1, 2)), ('simultaneously', (5, 6)), ('machine comprehension', (10, 12))]","[['MRS', 'focused on', 'simultaneously']]",[],[],"[['Model', 'has', 'MRS']]",natural_language_inference,24,24
2885,model,"In this paper , we show how multiple existing QA datasets can be used to evaluate MRS by requiring an open - domain system to perform well on all of them at once .","[('show', (5, 6)), ('by requiring', (17, 19)), ('to perform', (24, 26)), ('on', (27, 28))]","[('multiple existing QA datasets', (7, 11)), ('MRS', (16, 17)), ('open - domain system', (20, 24)), ('well', (26, 27)), ('all of them', (28, 31))]","[['MRS', 'by requiring', 'open - domain system'], ['MRS', 'to perform', 'well'], ['well', 'on', 'all of them']]",[],"[['Model', 'show', 'multiple existing QA datasets']]",[],natural_language_inference,24,25
2886,model,"We develop DrQA , a strong system for question answering from Wikipedia composed of : ( 1 ) Document Retriever , a module using bigram hashing and TF - IDF matching designed to , given a question , efficiently return a subset of relevant articles and ( 2 ) Document Reader , a multi - layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents .","[('develop', (1, 2)), ('composed of', (12, 14)), ('using', (23, 24)), ('designed to', (31, 33)), ('efficiently return', (38, 40)), ('of', (42, 43))]","[('DrQA', (2, 3)), ('question answering', (8, 10)), ('Document Retriever', (18, 20)), ('module', (22, 23)), ('bigram hashing and TF - IDF matching', (24, 31)), ('subset', (41, 42)), ('relevant articles', (43, 45)), ('Document Reader', (49, 51)), ('multi - layer', (53, 56)), ('answer spans', (65, 67))]","[['module', 'using', 'bigram hashing and TF - IDF matching'], ['question answering', 'efficiently return', 'subset'], ['bigram hashing and TF - IDF matching', 'efficiently return', 'subset'], ['subset', 'of', 'relevant articles']]","[['Document Retriever', 'has', 'module']]","[['Model', 'develop', 'DrQA']]",[],natural_language_inference,24,26
2887,experiments,Our System : DrQA,[],"[('Our System', (0, 2))]",[],[],[],[],natural_language_inference,24,59
2888,results,WikiMovies,[],[],[],[],[],[],natural_language_inference,24,136
2889,experimental-setup,We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,"[('use', (1, 2)), ('with', (7, 8)), ('for', (13, 14))]","[('3 - layer bidirectional LSTMs', (2, 7)), ('h = 128 hidden units', (8, 13)), ('both', (14, 15)), ('paragraph and question encoding', (15, 19))]","[['3 - layer bidirectional LSTMs', 'with', 'h = 128 hidden units'], ['h = 128 hidden units', 'for', 'both'], ['h = 128 hidden units', 'for', 'paragraph and question encoding']]","[['both', 'has', 'paragraph and question encoding']]","[['Experimental setup', 'use', '3 - layer bidirectional LSTMs']]",[],natural_language_inference,24,174
2890,experimental-setup,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .","[('apply', (1, 2)), ('for', (6, 7)), ('generating', (10, 11))]","[('Stanford CoreNLP toolkit', (3, 6)), ('tokenization', (7, 8)), ('lemma', (11, 12)), ('partof - speech', (13, 16)), ('named entity tags', (18, 21))]","[['Stanford CoreNLP toolkit', 'for', 'tokenization'], ['Stanford CoreNLP toolkit', 'generating', 'named entity tags']]",[],"[['Experimental setup', 'apply', 'Stanford CoreNLP toolkit']]",[],natural_language_inference,24,175
2891,experimental-setup,We use Adamax for optimization as described in .,"[('use', (1, 2)), ('for', (3, 4))]","[('Adamax', (2, 3)), ('optimization', (4, 5))]","[['Adamax', 'for', 'optimization']]",[],"[['Experimental setup', 'use', 'Adamax']]",[],natural_language_inference,24,177
2892,experimental-setup,Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,"[('with', (1, 2)), ('applied to', (6, 8)), ('of', (15, 16))]","[('Dropout', (0, 1)), ('p = 0.3', (2, 5)), ('word embeddings and all the hidden units', (8, 15)), ('LSTMs', (16, 17))]","[['Dropout', 'with', 'p = 0.3'], ['Dropout', 'applied to', 'word embeddings and all the hidden units'], ['p = 0.3', 'applied to', 'word embeddings and all the hidden units'], ['word embeddings and all the hidden units', 'of', 'LSTMs']]","[['Dropout', 'has', 'p = 0.3']]",[],"[['Experimental setup', 'has', 'Dropout']]",natural_language_inference,24,178
2893,results,"Our system ( single model ) can achieve 70.0 % exact match and 79.0 % F 1 scores on the test set , which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing .","[('achieve', (7, 8)), ('on', (18, 19))]","[('Our system ( single model )', (0, 6)), ('70.0 % exact match', (8, 12)), ('79.0 % F 1 scores', (13, 18)), ('test set', (20, 22))]","[['Our system ( single model )', 'achieve', '70.0 % exact match'], ['Our system ( single model )', 'achieve', '79.0 % F 1 scores'], ['79.0 % F 1 scores', 'on', 'test set']]",[],[],"[['Results', 'has', 'Our system ( single model )']]",natural_language_inference,24,182
2894,results,As shown in all the features contribute to the performance of our final system .,"[('contribute to', (6, 8)), ('of', (10, 11))]","[('performance', (9, 10)), ('our final system', (11, 14))]","[['performance', 'of', 'our final system']]",[],[],[],natural_language_inference,24,185
2895,results,"Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .","[('Without', (0, 1)), ('able to achieve', (21, 24)), ('over', (25, 26))]","[('aligned question embedding feature', (2, 6)), ('our system', (17, 19)), ('F1', (24, 25)), ('77 %', (26, 28))]","[['our system', 'able to achieve', 'F1'], ['F1', 'over', '77 %']]","[['aligned question embedding feature', 'has', 'our system']]","[['Results', 'Without', 'aligned question embedding feature']]",[],natural_language_inference,24,186
2896,baselines,SQuAD : A single Document Reader model is trained on the SQuAD training set only and used on all evaluation sets .,"[('trained on', (8, 10)), ('used on', (16, 18))]","[('SQuAD', (0, 1)), ('single Document Reader model', (3, 7)), ('SQuAD training set', (11, 14)), ('all evaluation sets', (18, 21))]","[['single Document Reader model', 'trained on', 'SQuAD training set'], ['SQuAD', 'used on', 'all evaluation sets']]","[['SQuAD', 'has', 'single Document Reader model']]",[],"[['Baselines', 'has', 'SQuAD']]",natural_language_inference,24,191
2897,baselines,Fine-tune ( DS ) : A Document Reader model is pre-trained on SQuAD and then fine - tuned for each dataset independently using its distant supervision ( DS ) training set .,"[('pre-trained on', (10, 12)), ('fine - tuned for', (15, 19)), ('using', (22, 23))]","[('Fine-tune ( DS )', (0, 4)), ('SQuAD', (12, 13)), ('each dataset independently', (19, 22)), ('distant supervision ( DS ) training set', (24, 31))]","[['each dataset independently', 'using', 'distant supervision ( DS ) training set']]",[],[],"[['Baselines', 'has', 'Fine-tune ( DS )']]",natural_language_inference,24,192
2898,baselines,Multitask ( DS ) :,[],"[('Multitask ( DS )', (0, 4))]",[],[],[],"[['Baselines', 'has', 'Multitask ( DS )']]",natural_language_inference,24,193
2899,results,"Despite the difficulty of the task compared to machine comprehension ( where you are given the right paragraph ) and unconstrained QA ( using redundant resources ) , Dr QA still provides reasonable performance across all four datasets .",[],"[('unconstrained QA', (20, 22)), ('Dr QA', (28, 30)), ('reasonable performance', (32, 34))]",[],[],[],[],natural_language_inference,24,201
2900,research-problem,A Deep Cascade Model for Multi - Document Reading Comprehension,[],[],[],[],[],[],natural_language_inference,25,2
2901,research-problem,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .",[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[],natural_language_inference,25,13
2902,model,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .","[('propose', (7, 8)), ('combines', (13, 14)), ('in', (19, 20))]","[('deep cascade model', (9, 12)), ('advantages', (15, 16)), ('coarse - to - fine manner', (21, 27))]","[['deep cascade model', 'combines', 'advantages'], ['advantages', 'in', 'coarse - to - fine manner']]",[],"[['Model', 'propose', 'deep cascade model']]",[],natural_language_inference,25,29
2903,model,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,"[('passed to', (5, 7)), ('for extracting', (14, 16)), ('at', (20, 21))]","[('selected paragraphs', (2, 4)), ('attention - based deep MRC model', (8, 14)), ('actual answer span', (17, 20)), ('word level', (21, 23))]","[['selected paragraphs', 'passed to', 'attention - based deep MRC model'], ['attention - based deep MRC model', 'for extracting', 'actual answer span'], ['actual answer span', 'at', 'word level']]",[],[],"[['Model', 'has', 'selected paragraphs']]",natural_language_inference,25,32
2904,model,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .","[('introduce', (9, 10)), ('as', (16, 17))]","[('answer extraction', (4, 6)), ('document extraction and paragraph extraction', (11, 16)), ('two auxiliary tasks', (17, 20))]","[['answer extraction', 'introduce', 'document extraction and paragraph extraction'], ['document extraction and paragraph extraction', 'as', 'two auxiliary tasks']]",[],[],[],natural_language_inference,25,33
2905,model,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .","[('jointly optimize', (1, 3)), ('in', (7, 8)), ('shares', (15, 16))]","[('all the three tasks', (3, 7)), ('unified deep MRC model', (9, 13)), ('some common bottom layers', (16, 20))]","[['all the three tasks', 'in', 'unified deep MRC model'], ['unified deep MRC model', 'shares', 'some common bottom layers']]",[],"[['Model', 'jointly optimize', 'all the three tasks']]",[],natural_language_inference,25,34
2906,model,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .","[('enables', (3, 4)), ('to perform', (6, 8)), ('at', (15, 16)), ('can be learnt', (21, 24))]","[('models', (5, 6)), ('coarse - to - fine pruning', (9, 15)), ('different stages', (16, 18)), ('better models', (19, 21))]","[['models', 'to perform', 'coarse - to - fine pruning'], ['coarse - to - fine pruning', 'at', 'different stages']]",[],"[['Model', 'enables', 'models']]",[],natural_language_inference,25,35
2907,model,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .","[('consists of', (11, 13))]","[('three modules', (13, 15)), ('document retrieval', (16, 18)), ('paragraph retrieval', (19, 21)), ('answer extraction', (22, 24))]",[],"[['three modules', 'name', 'document retrieval']]","[['Model', 'consists of', 'three modules']]",[],natural_language_inference,25,36
2908,model,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .","[('at', (2, 3)), ('consumes', (6, 7)), ('from', (9, 10)), ('further prunes', (15, 17)), ('given', (24, 25))]","[('module', (1, 2)), ('each subsequent stage', (3, 6)), ('output', (8, 9)), ('previous stage', (11, 13)), ('documents', (18, 19)), ('paragraphs and', (20, 22)), ('answer spans', (22, 24)), ('question', (26, 27))]","[['module', 'at', 'each subsequent stage'], ['module', 'consumes', 'output'], ['each subsequent stage', 'consumes', 'output'], ['output', 'from', 'previous stage'], ['module', 'further prunes', 'documents'], ['answer spans', 'given', 'question']]","[['module', 'has', 'each subsequent stage'], ['paragraphs and', 'has', 'answer spans']]",[],"[['Model', 'has', 'module']]",natural_language_inference,25,38
2909,model,"For each of the first two modules , we define a ranking function and an extraction function .","[('define', (9, 10))]","[('ranking function', (11, 13)), ('extraction function', (15, 17))]",[],[],"[['Model', 'define', 'ranking function']]",[],natural_language_inference,25,39
2910,model,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .","[('used as', (5, 7)), ('to discard', (10, 12)), ('to keep', (22, 24))]","[('ranking function', (1, 3)), ('preliminary filter', (8, 10)), ('most of the irrelevant documents or paragraphs', (12, 19)), ('our framework efficient', (24, 27))]","[['ranking function', 'used as', 'preliminary filter'], ['preliminary filter', 'to discard', 'most of the irrelevant documents or paragraphs'], ['ranking function', 'to keep', 'our framework efficient']]","[['ranking function', 'has', 'preliminary filter']]",[],"[['Model', 'has', 'ranking function']]",natural_language_inference,25,40
2911,model,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .","[('designed to deal with', (5, 9)), ('jointly optimized with', (19, 22)), ('for', (27, 28))]","[('extraction function', (1, 3)), ('auxiliary document and paragraph extraction tasks', (10, 16)), ('final answer extraction module', (23, 27)), ('better extraction performance', (28, 31))]","[['extraction function', 'designed to deal with', 'auxiliary document and paragraph extraction tasks'], ['auxiliary document and paragraph extraction tasks', 'jointly optimized with', 'final answer extraction module'], ['final answer extraction module', 'for', 'better extraction performance']]",[],[],"[['Model', 'has', 'extraction function']]",natural_language_inference,25,41
2912,model,"The main contributions can be summarized as follow : We propose a deep cascade learning framework to address the practical multi-document machine reading comprehension task , which considers both the effectiveness and efficiency in a coarse - to - fine manner .","[('propose', (10, 11)), ('to address', (16, 18)), ('considers', (27, 28))]","[('deep cascade learning framework', (12, 16))]",[],[],"[['Model', 'propose', 'deep cascade learning framework']]",[],natural_language_inference,25,43
2913,model,"We incorporate the auxiliary document extraction and paragraph extraction tasks to the pure answer span prediction , which helps to narrow down the search space and improves the final extraction result in multi-document MRC scenario .","[('incorporate', (1, 2)), ('to', (10, 11))]","[('auxiliary document extraction and paragraph extraction tasks', (3, 10)), ('pure answer span prediction', (12, 16))]","[['auxiliary document extraction and paragraph extraction tasks', 'to', 'pure answer span prediction']]",[],"[['Model', 'incorporate', 'auxiliary document extraction and paragraph extraction tasks']]",[],natural_language_inference,25,44
2914,research-problem,Related Work Machine Reading Comprehension,[],[],[],[],[],[],natural_language_inference,25,48
2915,model,Cascade Learning,[],[],[],[],[],[],natural_language_inference,25,59
2916,experimental-setup,"Since the Trivia QA documents often contain many small paragraphs , we also restructure the documents by merging consecutive paragraphs to a maximum size of 600 words for each paragraph as in ( Clark and Gardner 2017 ) .","[('restructure', (13, 14)), ('by merging', (16, 18)), ('to', (20, 21)), ('of', (24, 25)), ('for', (27, 28))]","[('Trivia QA', (2, 4)), ('documents', (15, 16)), ('consecutive paragraphs', (18, 20)), ('maximum size', (22, 24)), ('600 words', (25, 27)), ('each paragraph', (28, 30))]","[['documents', 'by merging', 'consecutive paragraphs'], ['consecutive paragraphs', 'to', 'maximum size'], ['maximum size', 'of', '600 words'], ['600 words', 'for', 'each paragraph']]",[],[],[],natural_language_inference,25,209
2917,experimental-setup,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .","[('For', (0, 1)), ('adopt', (8, 9)), ('for', (12, 13)), ('with', (15, 16))]","[('multi-task deep attention framework', (2, 6)), ('Adam optimizer', (10, 12)), ('training', (13, 14)), ('mini-batch size', (17, 19)), ('32', (20, 21)), ('initial learning rate', (22, 25)), ('0.0005', (26, 27))]","[['Adam optimizer', 'For', 'training'], ['multi-task deep attention framework', 'adopt', 'Adam optimizer'], ['Adam optimizer', 'for', 'training'], ['Adam optimizer', 'with', 'mini-batch size'], ['Adam optimizer', 'with', 'initial learning rate'], ['training', 'with', 'mini-batch size'], ['training', 'with', 'initial learning rate']]","[['mini-batch size', 'has', '32'], ['initial learning rate', 'has', '0.0005']]","[['Experimental setup', 'For', 'multi-task deep attention framework']]",[],natural_language_inference,25,211
2918,experimental-setup,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,"[('use', (1, 2)), ('in', (8, 9)), ('train', (11, 12)), ('with', (17, 18)), ('for', (22, 23))]","[('GloVe 300 dimensional word embeddings', (3, 8)), ('TriviaQA', (9, 10)), ('word2 vec word embeddings', (13, 17)), ('whole DuReader corpus', (19, 22)), ('DuReader', (23, 24))]","[['GloVe 300 dimensional word embeddings', 'in', 'TriviaQA'], ['word2 vec word embeddings', 'with', 'whole DuReader corpus'], ['whole DuReader corpus', 'for', 'DuReader']]",[],"[['Experimental setup', 'use', 'GloVe 300 dimensional word embeddings']]",[],natural_language_inference,25,212
2919,experimental-setup,The word embeddings are fixed during training .,"[('fixed during', (4, 6))]","[('word embeddings', (1, 3)), ('training', (6, 7))]","[['word embeddings', 'fixed during', 'training']]",[],[],"[['Experimental setup', 'has', 'word embeddings']]",natural_language_inference,25,213
2920,experimental-setup,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,"[('of', (3, 4)), ('set as', (6, 8)), ('for', (9, 10))]","[('hidden size', (1, 3)), ('LSTM', (4, 5)), ('150', (8, 9)), ('TriviaQA', (10, 11)), ('128', (12, 13)), ('DuReader', (14, 15))]","[['hidden size', 'of', 'LSTM'], ['hidden size', 'set as', '150'], ['LSTM', 'set as', '150'], ['150', 'for', 'TriviaQA'], ['128', 'for', 'DuReader']]",[],[],"[['Experimental setup', 'has', 'hidden size']]",natural_language_inference,25,214
2921,experimental-setup,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,"[('set as', (16, 18))]","[('Regularization parameter', (26, 28))]",[],[],"[['Experimental setup', 'set as', 'Regularization parameter']]",[],natural_language_inference,25,215
2922,experimental-setup,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,"[('trained on', (3, 5)), ('with', (9, 10)), ('in', (13, 14))]","[('Nvidia Tesla M40 GPU', (5, 9)), ('Cudnn LSTM cell', (10, 13)), ('Tensorflow 1.3', (14, 16))]","[['Nvidia Tesla M40 GPU', 'with', 'Cudnn LSTM cell'], ['Cudnn LSTM cell', 'in', 'Tensorflow 1.3']]",[],"[['Experimental setup', 'trained on', 'Nvidia Tesla M40 GPU']]",[],natural_language_inference,25,217
2923,results,"We can see that by adopting the deep cascade learning framework , the proposed model outperforms the previous state - of - the - art methods by an evident margin on both datasets , which validates the effectiveness of the proposed method in addressing the challenging multi-document MRC task .","[('adopting', (5, 6)), ('by', (26, 27))]","[('deep cascade learning framework', (7, 11)), ('proposed model', (13, 15)), ('outperforms', (15, 16)), ('previous state - of - the - art methods', (17, 26)), ('evident margin', (28, 30)), ('both datasets', (31, 33)), ('proposed method', (40, 42))]","[['previous state - of - the - art methods', 'by', 'evident margin']]","[['deep cascade learning framework', 'has', 'proposed model'], ['proposed model', 'has', 'outperforms'], ['outperforms', 'has', 'previous state - of - the - art methods']]","[['Results', 'adopting', 'deep cascade learning framework']]",[],natural_language_inference,25,221
2924,ablation-analysis,"From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .","[('see', (6, 7)), ('in', (18, 19)), ('among', (21, 22)), ('from', (42, 43)), ('extracted from', (49, 51)), ('can keep', (61, 63)), ('from', (66, 67))]","[('shared LSTM', (12, 14)), ('important', (16, 17)), ('answer extraction', (19, 21)), ('multiple documents', (22, 24)), ('content probability score', (39, 42)), ('multiple documents', (43, 45)), ('ranking order', (64, 66)), ('document ranking component', (67, 70))]","[['answer extraction', 'among', 'multiple documents'], ['content probability score', 'from', 'multiple documents'], ['shared LSTM', 'can keep', 'ranking order'], ['ranking order', 'from', 'document ranking component']]","[['shared LSTM', 'has', 'important']]","[['Ablation analysis', 'see', 'shared LSTM']]",[],natural_language_inference,25,226
2925,ablation-analysis,"2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .","[('serve as', (20, 22)), ('between', (27, 28))]","[('Both the preliminary cascade ranking and multi-task answer extraction strategy', (2, 12)), ('vital', (13, 14)), ('final performance', (16, 18)), ('good trade - off', (23, 27)), ('pure pipeline method and fully joint learning method', (29, 37))]","[['vital', 'serve as', 'good trade - off'], ['final performance', 'serve as', 'good trade - off'], ['good trade - off', 'between', 'pure pipeline method and fully joint learning method']]","[['Both the preliminary cascade ranking and multi-task answer extraction strategy', 'has', 'vital']]",[],"[['Ablation analysis', 'has', 'Both the preliminary cascade ranking and multi-task answer extraction strategy']]",natural_language_inference,25,228
2926,ablation-analysis,"Jointly training the three extraction tasks can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .","[('Jointly training', (0, 2)), ('provide', (7, 8)), ('shows', (12, 13)), ('with', (26, 27)), ('at', (29, 30))]","[('three extraction tasks', (3, 6)), ('great benefits', (8, 10)), ('three tasks', (15, 17)), ('closely related', (19, 21)), ('boost', (23, 24)), ('each other', (24, 26)), ('shared representations', (27, 29)), ('bottom layers', (30, 32))]","[['three extraction tasks', 'provide', 'great benefits'], ['great benefits', 'shows', 'three tasks'], ['each other', 'with', 'shared representations'], ['shared representations', 'at', 'bottom layers']]","[['boost', 'has', 'each other']]","[['Ablation analysis', 'Jointly training', 'three extraction tasks']]",[],natural_language_inference,25,230
2927,ablation-analysis,Effectiveness v.s. Efficiency Trade - off,[],[],[],[],[],[],natural_language_inference,25,231
2928,ablation-analysis,The result on DuReader development set is presented in .,"[('on', (2, 3))]","[('DuReader development set', (3, 6))]",[],[],"[['Ablation analysis', 'on', 'DuReader development set']]",[],natural_language_inference,25,233
2929,ablation-analysis,"We can see that : 1 ) By properly taking more documents or paragraphs into consideration , the performance of the model gradually increases when it reaches 4 documents and 2 paragraphs , and then the performance decreases slightly which maybe due to that much noisy data is introduced .","[('By properly', (7, 9)), ('into', (14, 15)), ('of', (19, 20)), ('when', (24, 25)), ('reaches', (26, 27))]","[('more documents or paragraphs', (10, 14)), ('consideration', (15, 16)), ('performance', (18, 19)), ('model', (21, 22)), ('gradually increases', (22, 24)), ('4 documents and 2 paragraphs', (27, 32)), ('performance', (36, 37)), ('decreases slightly', (37, 39))]","[['more documents or paragraphs', 'into', 'consideration'], ['performance', 'of', 'model']]","[['more documents or paragraphs', 'has', 'consideration'], ['consideration', 'has', 'performance'], ['model', 'has', 'gradually increases'], ['performance', 'has', 'decreases slightly']]","[['Ablation analysis', 'By properly', 'more documents or paragraphs']]",[],natural_language_inference,25,234
2930,ablation-analysis,"2 ) The time cost can be largely reduced by removing more irrelevant documents and paragraphs in the cascade ranking stage , while keeping the performance not change that much .","[('can be', (5, 7)), ('by removing', (9, 11)), ('in', (16, 17))]","[('time cost', (3, 5)), ('largely reduced', (7, 9)), ('more irrelevant documents and paragraphs', (11, 16)), ('cascade ranking stage', (18, 21)), ('performance', (25, 26)), ('not change that much', (26, 30))]","[['time cost', 'can be', 'largely reduced'], ['time cost', 'by removing', 'more irrelevant documents and paragraphs'], ['largely reduced', 'by removing', 'more irrelevant documents and paragraphs'], ['more irrelevant documents and paragraphs', 'in', 'cascade ranking stage']]","[['time cost', 'has', 'largely reduced'], ['performance', 'has', 'not change that much']]",[],"[['Ablation analysis', 'has', 'time cost']]",natural_language_inference,25,235
2931,ablation-analysis,The performance of jointly training the answer extraction module with different auxiliary tasks on DuReader development set is shown in Table 5 .,"[('of', (2, 3)), ('on', (13, 14))]","[('performance', (1, 2)), ('jointly training', (3, 5)), ('answer extraction module', (6, 9)), ('DuReader development set', (14, 17))]","[['performance', 'of', 'jointly training']]","[['jointly training', 'has', 'answer extraction module']]",[],"[['Ablation analysis', 'has', 'performance']]",natural_language_inference,25,239
2932,ablation-analysis,"We can see that by incorporating the auxiliary document extraction or paragraph extraction task in the joint learning framework , the performance can always improve which again shows the advantage of introducing auxiliary tasks for helping to learn shared bottom representations .","[('incorporating', (5, 6)), ('in', (14, 15)), ('can', (22, 23))]","[('auxiliary document extraction or paragraph extraction task', (7, 14)), ('joint learning framework', (16, 19)), ('performance', (21, 22)), ('always improve', (23, 25))]","[['auxiliary document extraction or paragraph extraction task', 'in', 'joint learning framework'], ['performance', 'can', 'always improve']]","[['performance', 'has', 'always improve']]","[['Ablation analysis', 'incorporating', 'auxiliary document extraction or paragraph extraction task']]",[],natural_language_inference,25,240
2933,ablation-analysis,"Besides , the performance gain by adding document extraction task is larger , which maybe due to that it can better lay the foundation of the model with that information from different documents can be distinguished .","[('by adding', (5, 7)), ('is', (10, 11))]","[('performance gain', (3, 5)), ('document extraction task', (7, 10)), ('larger', (11, 12))]","[['performance gain', 'by adding', 'document extraction task'], ['document extraction task', 'is', 'larger']]",[],[],"[['Ablation analysis', 'has', 'performance gain']]",natural_language_inference,25,241
2934,results,Results on E-commerce and Tax data,[],[],[],[],[],[],natural_language_inference,25,243
2935,results,"Besides , the performance with respect to F 1 score is also largely improved with the proposed multi-document MRC model , which demonstrates the effectiveness of our method for removing the rich irrelevant noisy content in our online scenario .","[('with respect to', (4, 7))]","[('performance', (3, 4)), ('F 1 score', (7, 10)), ('largely improved', (12, 14)), ('proposed multi-document MRC model', (16, 20))]","[['performance', 'with respect to', 'F 1 score']]",[],[],[],natural_language_inference,25,247
2936,results,Results on Different Document Lengths,[],[],[],[],[],[],natural_language_inference,25,248
2937,results,"We can see that without incorporating with the cascade ranking module , the answer extraction module performs rather poorly both in effectiveness and efficiency as the document length increases .","[('without incorporating with', (4, 7)), ('performs', (16, 17)), ('both in', (19, 21)), ('as', (24, 25))]","[('cascade ranking module', (8, 11)), ('answer extraction module', (13, 16)), ('rather poorly', (17, 19)), ('effectiveness and efficiency', (21, 24)), ('document length', (26, 28)), ('increases', (28, 29))]","[['answer extraction module', 'performs', 'rather poorly'], ['rather poorly', 'both in', 'effectiveness and efficiency'], ['rather poorly', 'as', 'document length']]","[['cascade ranking module', 'has', 'answer extraction module'], ['document length', 'has', 'increases']]","[['Results', 'without incorporating with', 'cascade ranking module']]",[],natural_language_inference,25,251
2938,research-problem,U - Net : Machine Reading Comprehension with Unanswerable Questions,[],"[('Machine Reading Comprehension', (4, 7))]",[],[],[],[],natural_language_inference,26,2
2939,research-problem,Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing .,[],"[('Machine reading comprehension', (0, 3))]",[],[],[],[],natural_language_inference,26,4
2940,research-problem,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .",[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[],natural_language_inference,26,12
2941,experimental-setup,"Glove embedding ( Pennington , Socher , and Manning 2014 ) and Elmo embedding ) are used as basic embeddings .","[('used as', (16, 18))]","[('Glove embedding', (0, 2)), ('Elmo embedding', (12, 14)), ('basic embeddings', (18, 20))]","[['Elmo embedding', 'used as', 'basic embeddings']]",[],[],"[['Experimental setup', 'has', 'Glove embedding']]",natural_language_inference,26,60
2942,experimental-setup,"We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .","[('use', (1, 2)), ('to process', (3, 5)), ('to obtain', (9, 11)), ('of', (21, 22))]","[('Spacy', (2, 3)), ('each question and passage', (5, 9)), ('tokens', (11, 12)), ('POS tags', (13, 15)), ('NER tags', (16, 18)), ('lemmas tags', (19, 21)), ('each text', (22, 24))]","[['Spacy', 'to process', 'each question and passage'], ['each question and passage', 'to obtain', 'tokens'], ['lemmas tags', 'of', 'each text']]",[],"[['Experimental setup', 'use', 'Spacy']]",[],natural_language_inference,26,180
2943,experimental-setup,"We use 12 dimensions to embed POS tags , 8 for NER tags .","[('use', (1, 2)), ('to embed', (4, 6)), ('for', (10, 11))]","[('12 dimensions', (2, 4)), ('POS tags', (6, 8)), ('8', (9, 10)), ('NER tags', (11, 13))]","[['12 dimensions', 'to embed', 'POS tags'], ['POS tags', 'for', 'NER tags'], ['8', 'for', 'NER tags']]",[],"[['Experimental setup', 'use', '12 dimensions']]",[],natural_language_inference,26,181
2944,experimental-setup,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .","[('use', (1, 2)), ('between', (16, 17))]","[('3 binary features', (2, 5)), ('exact match', (6, 8)), ('lower - case match', (9, 13)), ('lemma match', (14, 16)), ('question and', (18, 20)), ('passage', (20, 21))]","[['3 binary features', 'between', 'question and'], ['3 binary features', 'between', 'passage'], ['lemma match', 'between', 'question and'], ['lemma match', 'between', 'passage']]","[['question and', 'has', 'passage']]","[['Experimental setup', 'use', '3 binary features']]",[],natural_language_inference,26,182
2945,experimental-setup,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,"[('use', (1, 2))]","[('100 - dim Glove pretrained word embeddings', (2, 9)), ('1024 - dim Elmo embeddings', (10, 15))]",[],[],"[['Experimental setup', 'use', '100 - dim Glove pretrained word embeddings']]",[],natural_language_inference,26,183
2946,experimental-setup,All the LSTM blocks are bi-directional with one single layer .,"[('are', (4, 5)), ('with', (6, 7))]","[('LSTM blocks', (2, 4)), ('bi-directional', (5, 6)), ('one single layer', (7, 10))]","[['LSTM blocks', 'are', 'bi-directional'], ['bi-directional', 'with', 'one single layer']]","[['LSTM blocks', 'has', 'bi-directional']]",[],"[['Experimental setup', 'has', 'LSTM blocks']]",natural_language_inference,26,184
2947,experimental-setup,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .","[('set', (1, 2)), ('as', (6, 7))]","[('hidden layer dimension', (3, 6)), ('125', (7, 8)), ('attention layer dimension', (9, 12)), ('250', (13, 14))]","[['hidden layer dimension', 'as', '125']]","[['hidden layer dimension', 'has', '125'], ['attention layer dimension', 'has', '250']]","[['Experimental setup', 'set', 'hidden layer dimension']]",[],natural_language_inference,26,185
2948,experimental-setup,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .","[('added', (1, 2)), ('overall', (5, 6)), ('including', (10, 11)), ('at', (15, 16)), ('of', (19, 20))]","[('dropout layer', (3, 5)), ('modeling layers', (7, 9)), ('embedding layer', (12, 14)), ('dropout rate', (17, 19)), ('0.3', (20, 21))]","[['dropout layer', 'overall', 'modeling layers'], ['modeling layers', 'including', 'embedding layer'], ['dropout layer', 'at', 'dropout rate'], ['dropout rate', 'of', '0.3']]","[['dropout layer', 'has', 'modeling layers'], ['dropout rate', 'has', '0.3']]","[['Experimental setup', 'added', 'dropout layer']]",[],natural_language_inference,26,186
2949,experimental-setup,We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,"[('use', (1, 2)), ('with', (4, 5)), ('of', (8, 9))]","[('Adam optimizer', (2, 4)), ('learning rate', (6, 8)), ('0.002', (9, 10))]","[['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.002']]",[],"[['Experimental setup', 'use', 'Adam optimizer']]",[],natural_language_inference,26,187
2950,results,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .","[('achieves', (2, 3))]","[('Our model', (0, 2)), ('F 1 score', (4, 7)), ('74.0', (8, 9)), ('EM score', (11, 13)), ('70.3', (14, 15)), ('development set', (17, 19)), ('F 1 score', (22, 25)), ('EM score', (29, 31))]","[['Our model', 'achieves', 'F 1 score'], ['Our model', 'achieves', 'EM score'], ['Our model', 'achieves', 'F 1 score'], ['Our model', 'achieves', 'EM score']]",[],[],"[['Results', 'has', 'Our model']]",natural_language_inference,26,191
2951,results,Our model outperforms most of the previous approaches .,[],"[('Our model', (0, 2)), ('outperforms', (2, 3)), ('most of the previous approaches', (3, 8))]",[],"[['Our model', 'has', 'outperforms'], ['outperforms', 'has', 'most of the previous approaches']]",[],[],natural_language_inference,26,192
2952,results,"Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .","[('Comparing to', (0, 2)), ('is', (15, 16))]","[('best - performing systems', (3, 7)), ('our model', (8, 10)), ('simple architecture', (12, 14)), ('end - to - end model', (17, 23))]","[['best - performing systems', 'is', 'end - to - end model']]","[['best - performing systems', 'has', 'our model'], ['our model', 'has', 'simple architecture']]","[['Results', 'Comparing to', 'best - performing systems']]",[],natural_language_inference,26,193
2953,results,"In fact , among all the end - to - end models , we achieve the best F1 scores .","[('among', (3, 4)), ('achieve', (14, 15))]","[('all the end - to - end models', (4, 12)), ('best F1 scores', (16, 19))]","[['all the end - to - end models', 'achieve', 'best F1 scores']]",[],"[['Results', 'among', 'all the end - to - end models']]",[],natural_language_inference,26,194
2954,ablation-analysis,"Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .","[('show', (1, 2))]","[('performance', (4, 5)), ('dropped slightly', (5, 7))]",[],"[['performance', 'has', 'dropped slightly']]","[['Ablation analysis', 'show', 'performance']]",[],natural_language_inference,26,206
2955,ablation-analysis,"Compared to the bi-attention model , the F1 - score decreases 0.5 % .","[('Compared to', (0, 2))]","[('bi-attention model', (3, 5)), ('F1 - score', (7, 10)), ('decreases', (10, 11)), ('0.5 %', (11, 13))]",[],"[['bi-attention model', 'has', 'F1 - score'], ['F1 - score', 'has', 'decreases'], ['decreases', 'has', '0.5 %']]","[['Ablation analysis', 'Compared to', 'bi-attention model']]",[],natural_language_inference,26,217
2956,ablation-analysis,Multi- task Study,[],[],[],[],[],[],natural_language_inference,26,218
2957,ablation-analysis,Results ( the first two rows in ) show that there is a large gain when using the multi - task model .,"[('show', (8, 9)), ('when using', (15, 17))]","[('large gain', (13, 15)), ('multi - task model', (18, 22))]","[['large gain', 'when using', 'multi - task model']]",[],"[['Ablation analysis', 'show', 'large gain']]",[],natural_language_inference,26,225
2958,ablation-analysis,"For the answer boundary detection task , we find that the multi -task setup ( i.e. , the classification layer participates in the training process ) does not help its performance .","[('For', (0, 1)), ('find that', (8, 10)), ('does not', (26, 28))]","[('answer boundary detection task', (2, 6)), ('multi -task setup', (11, 14)), ('help', (28, 29)), ('its', (29, 30)), ('performance', (30, 31))]","[['answer boundary detection task', 'find that', 'multi -task setup'], ['multi -task setup', 'does not', 'help']]","[['answer boundary detection task', 'has', 'multi -task setup'], ['multi -task setup', 'has', 'help'], ['help', 'has', 'its'], ['its', 'has', 'performance']]","[['Ablation analysis', 'For', 'answer boundary detection task']]",[],natural_language_inference,26,227
2959,ablation-analysis,"But as shown above , our model achieves a good score in SQuAD 2.0 test , which shows this model has the potential to achieve higher performance by making progress on both the answer detection and classification tasks .","[('achieves', (7, 8)), ('in', (11, 12))]","[('our model', (5, 7)), ('good score', (9, 11)), ('SQuAD 2.0 test', (12, 15))]","[['our model', 'achieves', 'good score'], ['good score', 'in', 'SQuAD 2.0 test']]","[['our model', 'has', 'good score']]",[],"[['Ablation analysis', 'has', 'our model']]",natural_language_inference,26,234
2960,ablation-analysis,"Overall , we can conclude that our multi-task model works well since the performance of unanswerability classification improves significantly when the answer pointer and answer verifier work simultaneously .","[('conclude that', (4, 6)), ('works', (9, 10)), ('of', (14, 15)), ('when', (19, 20)), ('work', (26, 27))]","[('our multi-task model', (6, 9)), ('well', (10, 11)), ('performance', (13, 14)), ('unanswerability classification', (15, 17)), ('improves', (17, 18)), ('significantly', (18, 19)), ('answer pointer and answer verifier', (21, 26)), ('simultaneously', (27, 28))]","[['our multi-task model', 'works', 'well'], ['performance', 'of', 'unanswerability classification'], ['improves', 'when', 'answer pointer and answer verifier'], ['significantly', 'when', 'answer pointer and answer verifier'], ['answer pointer and answer verifier', 'work', 'simultaneously']]","[['our multi-task model', 'has', 'well'], ['unanswerability classification', 'has', 'improves'], ['improves', 'has', 'significantly'], ['answer pointer and answer verifier', 'has', 'simultaneously']]","[['Ablation analysis', 'conclude that', 'our multi-task model']]",[],natural_language_inference,26,235
2961,ablation-analysis,"As we can see , when the threshold is set to 0.5 , F1 score of answerable questions is similar to that of unanswerable questions .","[('set to', (9, 11)), ('of', (15, 16)), ('similar to', (19, 21))]","[('threshold', (7, 8)), ('0.5', (11, 12)), ('F1 score', (13, 15)), ('answerable questions', (16, 18)), ('unanswerable questions', (23, 25))]","[['threshold', 'set to', '0.5'], ['F1 score', 'of', 'answerable questions'], ['answerable questions', 'similar to', 'unanswerable questions']]","[['threshold', 'has', '0.5']]",[],[],natural_language_inference,26,245
2962,ablation-analysis,"When we increase the threshold ( i.e. , more likely to predict the question as unanswerable ) , performance for answerable questions degrades , and improves for unanswerable questions .","[('increase', (2, 3)), ('for', (19, 20))]","[('threshold', (4, 5)), ('performance', (18, 19)), ('answerable questions', (20, 22)), ('degrades', (22, 23)), ('improves', (25, 26)), ('unanswerable questions', (27, 29))]","[['performance', 'for', 'answerable questions']]","[['threshold', 'has', 'performance'], ['answerable questions', 'has', 'degrades']]","[['Ablation analysis', 'increase', 'threshold']]",[],natural_language_inference,26,246
2963,ablation-analysis,"We can see that the overall F 1 score is slightly better , which is consistent with the idea from SQ uAD 2.0 .","[('see that', (2, 4)), ('is', (9, 10)), ('consistent with', (15, 17))]","[('overall F 1 score', (5, 9)), ('slightly better', (10, 12))]","[['overall F 1 score', 'is', 'slightly better']]","[['overall F 1 score', 'has', 'slightly better']]","[['Ablation analysis', 'see that', 'overall F 1 score']]",[],natural_language_inference,26,248
2964,ablation-analysis,"Finally , we set the threshold to be 0.7 for the submission system to SQuAD evaluation .","[('set', (3, 4)), ('to be', (6, 8)), ('for', (9, 10)), ('to', (13, 14))]","[('threshold', (5, 6)), ('0.7', (8, 9)), ('submission system', (11, 13)), ('SQuAD evaluation', (14, 16))]","[['threshold', 'to be', '0.7'], ['0.7', 'for', 'submission system'], ['submission system', 'to', 'SQuAD evaluation']]","[['threshold', 'has', '0.7']]","[['Ablation analysis', 'set', 'threshold']]",[],natural_language_inference,26,250
2965,research-problem,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING,[],"[('CONVERSATIONAL QUESTION', (9, 11))]",[],[],[],[],natural_language_inference,27,2
2966,research-problem,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,[],"[('Conversational question answering ( CQA )', (0, 6))]",[],[],[],[],natural_language_inference,27,4
2967,research-problem,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",[],"[('machine reading comprehension ( MRC )', (6, 12)), ('CQA', (14, 15)), ('passage', (16, 17))]",[],[],[],[],natural_language_inference,27,5
2968,research-problem,Traditional machine reading comprehension ( MRC ) tasks share the single - turn setting of answering a single question related to a passage .,[],"[('machine reading comprehension ( MRC )', (1, 7))]",[],[],[],[],natural_language_inference,27,12
2969,model,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .","[('propose', (5, 6)), ('for', (16, 17))]","[('SDNet', (6, 7)), ('contextual attention - based deep neural network', (9, 16)), ('task', (18, 19)), ('conversational question answering', (20, 23))]","[['contextual attention - based deep neural network', 'for', 'task']]","[['SDNet', 'has', 'contextual attention - based deep neural network']]","[['Model', 'propose', 'SDNet']]",[],natural_language_inference,27,19
2970,model,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .","[('leverages', (3, 4)), ('in', (7, 8))]","[('SDNet', (2, 3)), ('latest breakthrough', (5, 7)), ('NLP', (8, 9)), ('BERT contextual embedding', (10, 13))]","[['SDNet', 'leverages', 'latest breakthrough'], ['latest breakthrough', 'in', 'NLP']]",[],[],"[['Model', 'has', 'SDNet']]",natural_language_inference,27,22
2971,hyperparameters,"For training , we use all questions / answers for one passage as a batch .","[('For', (0, 1)), ('use', (4, 5)), ('for', (9, 10)), ('as', (12, 13))]","[('training', (1, 2)), ('all questions / answers', (5, 9)), ('one passage', (10, 12)), ('batch', (14, 15))]","[['training', 'use', 'all questions / answers'], ['all questions / answers', 'for', 'one passage'], ['all questions / answers', 'as', 'batch']]",[],"[['Hyperparameters', 'For', 'training']]",[],natural_language_inference,27,107
2972,baselines,"We compare SDNet with the following baseline models : PGNet ( Seq2 Seq with copy mechanism ) , DrQA , DrQA + PGNet , BiDAF ++ and .","[('compare', (1, 2))]","[('SDNet', (2, 3)), ('PGNet ( Seq2 Seq with copy mechanism )', (9, 17)), ('DrQA', (18, 19)), ('DrQA + PGNet', (20, 23)), ('BiDAF ++', (24, 26))]",[],[],"[['Baselines', 'compare', 'SDNet']]",[],natural_language_inference,27,122
2973,experiments,"As shown , SDNet achieves significantly better results than baseline models .","[('achieves', (4, 5)), ('than', (8, 9))]","[('SDNet', (3, 4)), ('significantly better results', (5, 8)), ('baseline models', (9, 11))]","[['SDNet', 'achieves', 'significantly better results'], ['significantly better results', 'than', 'baseline models']]",[],[],[],natural_language_inference,27,128
2974,experiments,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .","[('improves', (7, 8)), ('by', (11, 12)), ('compared with', (15, 17)), ('on', (24, 25))]","[('single SDNet model', (4, 7)), ('overall F 1', (8, 11)), ('1.6 %', (12, 14)), ('previous state - of - art model', (17, 24)), ('CoQA', (25, 26))]","[['single SDNet model', 'improves', 'overall F 1'], ['overall F 1', 'by', '1.6 %'], ['previous state - of - art model', 'on', 'CoQA']]",[],[],[],natural_language_inference,27,129
2975,experiments,"Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .","[('further improves', (3, 5)), ('by', (9, 10)), ('on', (27, 28))]","[('Ensemble SDNet model', (0, 3)), ('overall F 1 score', (5, 9)), ('2.7 %', (10, 12)), ('over', (21, 22)), ('in - domain datasets', (28, 32))]","[['Ensemble SDNet model', 'further improves', 'overall F 1 score'], ['overall F 1 score', 'by', '2.7 %']]",[],[],[],natural_language_inference,27,130
2976,experiments,"As seen , SDNet overpasses all but one baseline models after the second epoch , and achieves state - of - the - art results only after 8 epochs .","[('overpasses', (4, 5)), ('after', (10, 11)), ('achieves', (16, 17)), ('after', (26, 27))]","[('SDNet', (3, 4)), ('all but one baseline models', (5, 10)), ('second epoch', (12, 14)), ('state - of - the - art results', (17, 25)), ('8 epochs', (27, 29))]","[['SDNet', 'overpasses', 'all but one baseline models'], ['all but one baseline models', 'after', 'second epoch'], ['SDNet', 'achieves', 'state - of - the - art results'], ['state - of - the - art results', 'after', '8 epochs']]",[],[],[],natural_language_inference,27,132
2977,ablation-analysis,The results show that removing BERT can reduce the F 1 score on development set by 7.15 % .,"[('show', (2, 3)), ('removing', (4, 5)), ('on', (12, 13)), ('by', (15, 16))]","[('BERT', (5, 6)), ('F 1 score', (9, 12)), ('development set', (13, 15)), ('7.15 %', (16, 18))]","[['F 1 score', 'on', 'development set'], ['F 1 score', 'by', '7.15 %'], ['development set', 'by', '7.15 %']]",[],"[['Ablation analysis', 'show', 'BERT']]",[],natural_language_inference,27,135
2978,ablation-analysis,"Our proposed weight sum of per-layer output from BERT is crucial , which can boost the performance by 1.75 % , compared with using only last layer 's output .","[('of', (4, 5)), ('from', (7, 8)), ('is', (9, 10)), ('can', (13, 14)), ('by', (17, 18)), ('compared with using', (21, 24))]","[('Our', (0, 1)), ('proposed weight sum', (1, 4)), ('per-layer output', (5, 7)), ('BERT', (8, 9)), ('crucial', (10, 11)), ('boost', (14, 15)), ('performance', (16, 17)), ('1.75 %', (18, 20)), (""only last layer 's output"", (24, 29))]","[['proposed weight sum', 'of', 'per-layer output'], ['per-layer output', 'from', 'BERT'], ['per-layer output', 'is', 'crucial'], ['crucial', 'can', 'boost'], ['performance', 'by', '1.75 %'], ['boost', 'compared with using', ""only last layer 's output""], ['performance', 'compared with using', ""only last layer 's output""], ['1.75 %', 'compared with using', ""only last layer 's output""]]","[['Our', 'has', 'proposed weight sum'], ['boost', 'has', 'performance']]",[],"[['Ablation analysis', 'has', 'Our']]",natural_language_inference,27,136
2979,ablation-analysis,"Using BERT - base instead of BERT - large pretrained model hurts the F 1 score by 2.61 % , which manifests the superiority of BERT - large model .","[('Using', (0, 1)), ('instead of', (4, 6)), ('hurts', (11, 12)), ('by', (16, 17))]","[('BERT - base', (1, 4)), ('BERT - large pretrained model', (6, 11)), ('F 1 score', (13, 16)), ('2.61 %', (17, 19))]","[['BERT - base', 'instead of', 'BERT - large pretrained model'], ['BERT - large pretrained model', 'hurts', 'F 1 score'], ['F 1 score', 'by', '2.61 %']]",[],"[['Ablation analysis', 'Using', 'BERT - base']]",[],natural_language_inference,27,139
2980,ablation-analysis,"Variational dropout and self attention can each improve the performance by 0.24 % and 0.75 % , respectively .","[('by', (10, 11))]","[('Variational dropout and self attention', (0, 5)), ('performance', (9, 10)), ('0.24 % and 0.75 %', (11, 16))]","[['performance', 'by', '0.24 % and 0.75 %']]",[],[],"[['Ablation analysis', 'has', 'Variational dropout and self attention']]",natural_language_inference,27,140
2981,research-problem,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS,[],"[('TRACKING THE WORLD STATE', (0, 4))]",[],[],[],[],natural_language_inference,28,2
2982,model,"It may also learn basic rules of approximate ( logical ) inference , such as the fact that objects belonging to the same category tend to have similar properties ( light objects can be carried over from rooms to rooms for instance ) .","[('of', (6, 7)), ('belonging to', (19, 21)), ('tend to have', (24, 27))]","[('basic rules', (4, 6)), ('approximate ( logical ) inference', (7, 12)), ('same category', (22, 24)), ('similar properties', (27, 29))]","[['basic rules', 'of', 'approximate ( logical ) inference'], ['same category', 'tend to have', 'similar properties']]",[],[],"[['Model', 'has', 'basic rules']]",natural_language_inference,28,27
2983,model,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,"[('with', (6, 7)), ('uses', (17, 18))]","[('new kind of memory - augmented neural network', (8, 16)), ('distributed memory and processor architecture', (19, 24)), ('Recurrent Entity Network ( EntNet )', (26, 32))]","[['new kind of memory - augmented neural network', 'uses', 'distributed memory and processor architecture']]","[['distributed memory and processor architecture', 'name', 'Recurrent Entity Network ( EntNet )']]","[['Model', 'with', 'new kind of memory - augmented neural network']]",[],natural_language_inference,28,28
2984,results,SYNTHETIC WORLD MODEL TASK,[],[],[],[],[],[],natural_language_inference,28,127
2985,hyperparameters,"For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .","[('For', (0, 1)), ('set', (5, 6)), ('equal to', (10, 12))]","[('MemN2N', (2, 3)), ('number of hops', (7, 10)), ('T ? 2', (12, 15)), ('embedding dimension', (17, 19)), ('d = 20', (20, 23))]","[['MemN2N', 'set', 'number of hops'], ['number of hops', 'equal to', 'T ? 2']]","[['number of hops', 'has', 'T ? 2'], ['embedding dimension', 'has', 'd = 20']]","[['Hyperparameters', 'For', 'MemN2N']]",[],natural_language_inference,28,134
2986,hyperparameters,"All models were trained with ADAM with initial learning rates set by grid search over { 0.1 , 0.01 , 0.001 } and divided by 2 every 10,000 updates .","[('trained with', (3, 5)), ('with', (6, 7)), ('set by', (10, 12)), ('over', (14, 15)), ('divided by', (23, 25))]","[('ADAM', (5, 6)), ('initial learning rates', (7, 10)), ('grid search', (12, 14)), ('{ 0.1 , 0.01 , 0.001 }', (15, 22)), ('2 every 10,000 updates', (25, 29))]","[['ADAM', 'with', 'initial learning rates'], ['initial learning rates', 'set by', 'grid search'], ['grid search', 'over', '{ 0.1 , 0.01 , 0.001 }'], ['grid search', 'divided by', '2 every 10,000 updates']]",[],"[['Hyperparameters', 'trained with', 'ADAM']]",[],natural_language_inference,28,137
2987,results,"The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .","[('as', (10, 11))]","[('MemN2N', (1, 2)), ('worst performance', (4, 6)), ('degrades', (8, 9)), ('quickly', (9, 10)), ('length of the sequence', (12, 16)), ('increases', (16, 17))]","[['degrades', 'as', 'length of the sequence'], ['quickly', 'as', 'length of the sequence']]","[['MemN2N', 'has', 'worst performance'], ['worst performance', 'has', 'degrades'], ['degrades', 'has', 'quickly'], ['length of the sequence', 'has', 'increases']]",[],"[['Results', 'has', 'MemN2N']]",natural_language_inference,28,139
2988,results,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .","[('performs', (2, 3)), ('as', (9, 10))]","[('LSTM', (1, 2)), ('better', (3, 4)), ('loses', (7, 8)), ('accuracy', (8, 9)), ('length of the sequence', (11, 15)), ('increases', (15, 16))]","[['LSTM', 'performs', 'better'], ['loses', 'as', 'length of the sequence'], ['accuracy', 'as', 'length of the sequence']]","[['loses', 'has', 'accuracy'], ['length of the sequence', 'has', 'increases']]",[],"[['Results', 'has', 'LSTM']]",natural_language_inference,28,140
2989,results,"In contrast , the EntNet is able to solve the task in all cases .","[('able to solve', (6, 9)), ('in', (11, 12))]","[('EntNet', (4, 5)), ('task', (10, 11)), ('all cases', (12, 14))]","[['EntNet', 'able to solve', 'task'], ['task', 'in', 'all cases']]",[],[],[],natural_language_inference,28,141
2990,results,We see that the model is able to achieve good performance several times past its training horizon .,"[('see', (1, 2)), ('able to achieve', (6, 9))]","[('model', (4, 5)), ('good performance', (9, 11)), ('several times past', (11, 14)), ('its training horizon', (14, 17))]","[['model', 'able to achieve', 'good performance']]","[['good performance', 'has', 'several times past'], ['several times past', 'has', 'its training horizon']]","[['Results', 'see', 'model']]",[],natural_language_inference,28,146
2991,experiments,BABI TASKS,[],[],[],[],[],[],natural_language_inference,28,147
2992,hyperparameters,"All models were trained with ADAM using a learning rate of ? = 0.01 , which was divided by 2 every 25 epochs until 200 epochs were reached .","[('trained with', (3, 5)), ('using', (6, 7)), ('of', (10, 11)), ('divided by', (17, 19)), ('until', (23, 24))]","[('ADAM', (5, 6)), ('learning rate', (8, 10)), ('? = 0.01', (11, 14)), ('2 every 25 epochs', (19, 23)), ('200 epochs', (24, 26))]","[['ADAM', 'using', 'learning rate'], ['learning rate', 'of', '? = 0.01'], ['? = 0.01', 'divided by', '2 every 25 epochs'], ['2 every 25 epochs', 'until', '200 epochs']]",[],"[['Hyperparameters', 'trained with', 'ADAM']]",[],natural_language_inference,28,155
2993,hyperparameters,"In all experiments , our model had embedding dimension size d = 100 and 20 memory slots .",[],"[('our model', (4, 6)), ('embedding dimension size d = 100', (7, 13)), ('20 memory slots', (14, 17))]",[],"[['our model', 'has', 'embedding dimension size d = 100']]",[],"[['Hyperparameters', 'has', 'our model']]",natural_language_inference,28,158
2994,experiments,"Our model is able to solve all the tasks , outperforming the other models in terms of both the number of solved tasks and the average error .","[('able to', (3, 5)), ('in terms of', (14, 17))]","[('Our model', (0, 2)), ('solve', (5, 6)), ('all the tasks', (6, 9)), ('outperforming', (10, 11)), ('other models', (12, 14)), ('number of solved tasks', (19, 23)), ('average error', (25, 27))]","[['Our model', 'able to', 'solve'], ['other models', 'in terms of', 'number of solved tasks'], ['other models', 'in terms of', 'average error']]","[['solve', 'has', 'all the tasks'], ['outperforming', 'has', 'other models']]",[],[],natural_language_inference,28,160
2995,experiments,"Note that it does not store useful or correct information in the memory slots corresponding to locations , most likely because this task does not contain questions about locations ( such as "" who is in the kitchen ? "" ) .","[('does not store', (3, 6)), ('in', (10, 11)), ('corresponding to', (14, 16))]","[('useful or correct information', (6, 10)), ('memory slots', (12, 14)), ('locations', (16, 17))]","[['useful or correct information', 'in', 'memory slots'], ['memory slots', 'corresponding to', 'locations']]",[],[],[],natural_language_inference,28,170
2996,experiments,CHILDRE N'S BOOK TEST ( CBT ),[],[],[],[],[],[],natural_language_inference,28,171
2997,hyperparameters,All models were trained using standard stochastic gradient descent ( SGD ) with a fixed learning rate of 0.001 .,"[('trained using', (3, 5)), ('with', (12, 13)), ('of', (17, 18))]","[('standard stochastic gradient descent ( SGD )', (5, 12)), ('fixed learning rate', (14, 17)), ('0.001', (18, 19))]","[['standard stochastic gradient descent ( SGD )', 'with', 'fixed learning rate'], ['fixed learning rate', 'of', '0.001']]",[],"[['Hyperparameters', 'trained using', 'standard stochastic gradient descent ( SGD )']]",[],natural_language_inference,28,181
2998,hyperparameters,"We used separate input encodings for the update and gating functions , and applied a dropout rate of 0.5 to the word embedding dimensions .","[('used', (1, 2)), ('for', (5, 6)), ('applied', (13, 14)), ('of', (17, 18)), ('to', (19, 20))]","[('separate input encodings', (2, 5)), ('update and gating functions', (7, 11)), ('dropout rate', (15, 17)), ('0.5', (18, 19)), ('word embedding dimensions', (21, 24))]","[['separate input encodings', 'for', 'update and gating functions'], ['dropout rate', 'of', '0.5'], ['0.5', 'to', 'word embedding dimensions']]","[['dropout rate', 'has', '0.5']]","[['Hyperparameters', 'used', 'separate input encodings']]",[],natural_language_inference,28,182
2999,results,"The general EntNet performs better than the LSTMs and n-gram model on the Named Entities Task , but lags behind on the Common Nouns task .","[('performs', (3, 4)), ('than', (5, 6)), ('on', (11, 12)), ('lags behind', (18, 20))]","[('general EntNet', (1, 3)), ('better', (4, 5)), ('LSTMs and n-gram model', (7, 11)), ('Named Entities Task', (13, 16)), ('Common Nouns task', (22, 25))]","[['general EntNet', 'performs', 'better'], ['better', 'than', 'LSTMs and n-gram model'], ['LSTMs and n-gram model', 'on', 'Named Entities Task']]",[],[],"[['Results', 'has', 'general EntNet']]",natural_language_inference,28,195
3000,results,"The simplified EntNet outperforms all other single - pass models on both tasks , and also performs better than the Memory Network which does not use the self - supervision heuristic .","[('performs', (16, 17)), ('than', (18, 19)), ('which does not use', (22, 26))]","[('simplified EntNet', (1, 3)), ('outperforms', (3, 4)), ('all other single - pass models', (4, 10)), ('better', (17, 18)), ('Memory Network', (20, 22)), ('self - supervision heuristic', (27, 31))]","[['simplified EntNet', 'performs', 'better'], ['better', 'than', 'Memory Network'], ['Memory Network', 'which does not use', 'self - supervision heuristic']]","[['simplified EntNet', 'has', 'outperforms'], ['outperforms', 'has', 'all other single - pass models']]",[],"[['Results', 'has', 'simplified EntNet']]",natural_language_inference,28,196
3001,results,The fact that the simplified EntNet is able to obtain decent performance is encouraging since it indicates that the model is able to build an internal representation of the story which it can then use to answer a relatively diverse set of queries .,"[('able to obtain', (7, 10)), ('is', (12, 13))]","[('simplified EntNet', (4, 6)), ('decent performance', (10, 12)), ('encouraging', (13, 14))]","[['simplified EntNet', 'able to obtain', 'decent performance'], ['decent performance', 'is', 'encouraging']]",[],[],[],natural_language_inference,28,198
3002,model,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .","[('introduce', (12, 13)), ('for the use of', (17, 21))]","[('general framework PhaseCond', (14, 17)), ('multiple attention layers', (21, 24))]","[['general framework PhaseCond', 'for the use of', 'multiple attention layers']]",[],"[['Model', 'introduce', 'general framework PhaseCond']]",[],natural_language_inference,29,19
3003,model,"This perspective leads to a different attention - based architecture containing two sequential phases , question - aware passage representation phase and evidence propagation phase. , RNET , MReader , and PhaseCond ( our proposed model ) .","[('leads to', (2, 4)), ('containing', (10, 11))]","[('different attention - based architecture', (5, 10)), ('two sequential phases', (11, 14)), ('question - aware passage representation phase', (15, 21)), ('evidence propagation phase.', (22, 25)), ('RNET', (26, 27)), ('MReader', (28, 29)), ('PhaseCond', (31, 32))]","[['different attention - based architecture', 'containing', 'two sequential phases']]","[['different attention - based architecture', 'name', 'two sequential phases'], ['two sequential phases', 'name', 'question - aware passage representation phase']]","[['Model', 'leads to', 'different attention - based architecture']]",[],natural_language_inference,29,23
3004,model,"Moreover , we observe several meaningful trends : a ) during the questionpassage attention phase , repeatedly attending the passage with the same question representation "" forces "" each passage word to become increasingly closer to the original question representation , and therefore increasing the number of layers has a risk of degrading the network performance , b ) during the self - attention phase , the self - attention 's alignment weights of the second layer become noticeably "" sharper "" than the first layer , suggesting the importance of fully propagating evidence through the passage itself .","[('observe', (3, 4)), ('during', (10, 11)), ('repeatedly attending', (16, 18)), ('with', (20, 21)), ('forces', (26, 27)), ('to become', (31, 33)), ('during', (59, 60)), ('of', (73, 74)), ('become', (77, 78)), ('than', (82, 83))]","[('several meaningful trends', (4, 7)), ('questionpassage attention phase', (12, 15)), ('passage', (19, 20)), ('same question representation', (22, 25)), ('each passage word', (28, 31)), ('increasingly closer', (33, 35)), ('original question representation', (37, 40)), ('self - attention phase', (61, 65)), (""self - attention 's alignment weights"", (67, 73)), ('second layer', (75, 77)), ('noticeably "" sharper ""', (78, 82)), ('first layer', (84, 86))]","[['several meaningful trends', 'during', 'questionpassage attention phase'], ['several meaningful trends', 'during', 'self - attention phase'], ['questionpassage attention phase', 'repeatedly attending', 'passage'], ['passage', 'with', 'same question representation'], ['same question representation', 'forces', 'each passage word'], ['each passage word', 'to become', 'increasingly closer'], ['several meaningful trends', 'during', 'self - attention phase'], [""self - attention 's alignment weights"", 'of', 'second layer'], [""self - attention 's alignment weights"", 'become', 'noticeably "" sharper ""'], ['second layer', 'become', 'noticeably "" sharper ""'], ['noticeably "" sharper ""', 'than', 'first layer']]","[['self - attention phase', 'has', ""self - attention 's alignment weights""], [""self - attention 's alignment weights"", 'has', 'second layer']]","[['Model', 'observe', 'several meaningful trends']]",[],natural_language_inference,29,32
3005,hyperparameters,"We use pre-trained GloVe 100 - dimensional word vectors , parts - of - speech tag features , named - entity tag feature , and binary features of exact matching which indicate if a passage word can be exactly matched to any question word and vice versa .","[('use', (1, 2)), ('of', (27, 28))]","[('pre-trained GloVe 100 - dimensional word vectors', (2, 9)), ('parts - of - speech tag features', (10, 17)), ('named - entity tag feature', (18, 23)), ('binary features', (25, 27)), ('exact matching', (28, 30))]","[['binary features', 'of', 'exact matching']]",[],"[['Hyperparameters', 'use', 'pre-trained GloVe 100 - dimensional word vectors']]",[],natural_language_inference,29,104
3006,hyperparameters,"Following , we also use question type ( what , how , who , when , which , where , why , be , and other ) features where each type is represented by a trainable embedding .","[('use', (4, 5)), ('where', (28, 29)), ('represented by', (32, 34))]","[('question type ( what , how , who , when , which , where , why , be , and other ) features', (5, 28)), ('each type', (29, 31)), ('trainable embedding', (35, 37))]","[['question type ( what , how , who , when , which , where , why , be , and other ) features', 'where', 'each type'], ['each type', 'represented by', 'trainable embedding']]","[['question type ( what , how , who , when , which , where , why , be , and other ) features', 'has', 'each type']]","[['Hyperparameters', 'use', 'question type ( what , how , who , when , which , where , why , be , and other ) features']]",[],natural_language_inference,29,105
3007,hyperparameters,We use CNN with 100 one - dimensional filters with width 5 to encode character level embedding .,"[('use', (1, 2)), ('with', (3, 4)), ('with', (9, 10)), ('to encode', (12, 14))]","[('CNN', (2, 3)), ('100 one - dimensional filters', (4, 9)), ('width 5', (10, 12)), ('character level embedding', (14, 17))]","[['CNN', 'with', '100 one - dimensional filters'], ['100 one - dimensional filters', 'with', 'width 5'], ['100 one - dimensional filters', 'with', 'width 5'], ['width 5', 'to encode', 'character level embedding']]",[],"[['Hyperparameters', 'use', 'CNN']]",[],natural_language_inference,29,106
3008,hyperparameters,The hidden size is set as 128 for all the LSTM layers .,"[('set as', (4, 6)), ('for', (7, 8))]","[('hidden size', (1, 3)), ('128', (6, 7)), ('all the LSTM layers', (8, 12))]","[['hidden size', 'set as', '128'], ['128', 'for', 'all the LSTM layers']]","[['hidden size', 'has', '128']]",[],"[['Hyperparameters', 'has', 'hidden size']]",natural_language_inference,29,107
3009,hyperparameters,Dropout are used for all the learnable parameters with a ratio as 0.2 .,"[('used for', (2, 4)), ('with', (8, 9)), ('as', (11, 12))]","[('Dropout', (0, 1)), ('all the learnable parameters', (4, 8)), ('ratio', (10, 11)), ('0.2', (12, 13))]","[['Dropout', 'used for', 'all the learnable parameters'], ['all the learnable parameters', 'with', 'ratio'], ['ratio', 'as', '0.2']]","[['ratio', 'has', '0.2']]",[],"[['Hyperparameters', 'has', 'Dropout']]",natural_language_inference,29,108
3010,hyperparameters,"We use the Adam optimizer ( Kingma & Ba , 2014 ) with an initial learning rate of 0.0006 , which is halved when a bad checkpoint is met .","[('use', (1, 2)), ('with', (12, 13)), ('of', (17, 18))]","[('Adam optimizer ( Kingma & Ba , 2014 )', (3, 12)), ('initial learning rate', (14, 17)), ('0.0006', (18, 19)), ('bad', (25, 26))]","[['Adam optimizer ( Kingma & Ba , 2014 )', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.0006']]","[['initial learning rate', 'has', '0.0006']]","[['Hyperparameters', 'use', 'Adam optimizer ( Kingma & Ba , 2014 )']]",[],natural_language_inference,29,109
3011,results,Single Model Ensemble Models Dev Set,[],[],[],[],[],[],natural_language_inference,29,123
3012,results,"The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .","[('of', (3, 4)), ('lower than', (9, 11))]","[('EM result', (1, 3)), ('our', (4, 5)), ('baseline Iterative Aligner', (5, 8)), ('RNET', (11, 12))]","[['EM result', 'of', 'our'], ['EM result', 'of', 'baseline Iterative Aligner'], ['baseline Iterative Aligner', 'lower than', 'RNET']]","[['EM result', 'has', 'our'], ['our', 'has', 'baseline Iterative Aligner']]",[],"[['Results', 'has', 'EM result']]",natural_language_inference,29,126
3013,results,"For the question - passage attention phase , using single layer does n't degrade the performance significantly from the default setting of two layers , resulting in a different conclusion from ; .","[('For', (0, 1)), ('using', (8, 9)), (""does n't degrade"", (11, 14)), ('from', (17, 18)), ('of', (21, 22)), ('resulting in', (25, 27))]","[('question - passage attention phase', (2, 7)), ('single layer', (9, 11)), ('performance', (15, 16)), ('significantly', (16, 17)), ('default setting', (19, 21)), ('two layers', (22, 24))]","[['question - passage attention phase', 'using', 'single layer'], ['single layer', ""does n't degrade"", 'performance'], ['performance', 'from', 'default setting'], ['significantly', 'from', 'default setting'], ['default setting', 'of', 'two layers']]","[['question - passage attention phase', 'has', 'single layer'], ['performance', 'has', 'significantly']]","[['Results', 'For', 'question - passage attention phase']]",[],natural_language_inference,29,130
3014,experiments,ANALYSIS ON ATTENTION LAYERS,[],[],[],[],[],[],natural_language_inference,29,134
3015,experiments,"First , the first layer of the question - passage attention phase can successfully align question keywords with the corresponding passage keywords , as shown in .","[('of', (5, 6)), ('successfully align', (13, 15)), ('with', (17, 18))]","[('first layer', (3, 5)), ('question - passage attention phase', (7, 12)), ('question keywords', (15, 17)), ('corresponding passage keywords', (19, 22))]","[['first layer', 'of', 'question - passage attention phase'], ['first layer', 'successfully align', 'question keywords'], ['question - passage attention phase', 'successfully align', 'question keywords'], ['question keywords', 'with', 'corresponding passage keywords']]","[['first layer', 'has', 'question - passage attention phase']]",[],[],natural_language_inference,29,138
3016,research-problem,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering,[],[],[],[],[],[],natural_language_inference,3,2
3017,research-problem,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .,[],"[('machine comprehension ( MC )', (15, 20)), ('question answering ( QA )', (21, 26))]",[],[],[],[],natural_language_inference,3,4
3018,dataset,The question - answer pairs are annotated through crowdsourcing .,"[('annotated through', (6, 8))]","[('question - answer pairs', (1, 5)), ('crowdsourcing', (8, 9))]","[['question - answer pairs', 'annotated through', 'crowdsourcing']]",[],[],"[['Dataset', 'has', 'question - answer pairs']]",natural_language_inference,3,19
3019,model,The bi-directional attention flow ( BIDAF ) used the bi-directional attention to obtain a question - aware context representation .,"[('used', (7, 8)), ('to obtain', (11, 13))]","[('bi-directional attention flow ( BIDAF )', (1, 7)), ('bi-directional attention', (9, 11)), ('question - aware context representation', (14, 19))]","[['bi-directional attention flow ( BIDAF )', 'used', 'bi-directional attention'], ['bi-directional attention', 'to obtain', 'question - aware context representation']]","[['bi-directional attention flow ( BIDAF )', 'name', 'bi-directional attention']]",[],"[['Model', 'has', 'bi-directional attention flow ( BIDAF )']]",natural_language_inference,3,28
3020,model,"In this paper , we introduce syntactic information to encode questions with a specific form of recursive neural networks .","[('introduce', (5, 6)), ('to encode', (8, 10)), ('with', (11, 12))]","[('syntactic information', (6, 8)), ('questions', (10, 11)), ('specific form of recursive neural networks', (13, 19))]","[['syntactic information', 'to encode', 'questions'], ['questions', 'with', 'specific form of recursive neural networks']]",[],"[['Model', 'introduce', 'syntactic information']]",[],natural_language_inference,3,29
3021,model,"More specifically , we explore a tree - structured LSTM which extends the linear - chain long short - term memory ( LSTM ) ] to a recursive structure , which has the potential to capture long - distance interactions over the structures .","[('explore', (4, 5)), ('extends', (11, 12)), ('to', (25, 26)), ('over', (40, 41))]","[('tree - structured LSTM', (6, 10)), ('linear - chain long short - term memory ( LSTM )', (13, 24)), ('recursive structure', (27, 29)), ('long - distance interactions', (36, 40)), ('structures', (42, 43))]","[['tree - structured LSTM', 'extends', 'linear - chain long short - term memory ( LSTM )'], ['linear - chain long short - term memory ( LSTM )', 'to', 'recursive structure'], ['long - distance interactions', 'over', 'structures']]","[['tree - structured LSTM', 'name', 'linear - chain long short - term memory ( LSTM )']]","[['Model', 'explore', 'tree - structured LSTM']]",[],natural_language_inference,3,30
3022,experiments,Word embedding,[],[],[],[],[],[],natural_language_inference,3,35
3023,results,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,"[('use', (1, 2)), ('to initialize', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (2, 9)), ('our word embeddings', (11, 14))]","[['pre-trained 300 - D Glove 840B vectors', 'to initialize', 'our word embeddings']]",[],"[['Results', 'use', 'pre-trained 300 - D Glove 840B vectors']]",[],natural_language_inference,3,168
3024,hyperparameters,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"[('with', (12, 13))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('initialized randomly', (10, 12)), ('Gaussian samples', (13, 15))]","[['initialized randomly', 'with', 'Gaussian samples']]","[['Out - of - vocabulary ( OOV ) words', 'has', 'initialized randomly']]",[],"[['Hyperparameters', 'has', 'Out - of - vocabulary ( OOV ) words']]",natural_language_inference,3,169
3025,hyperparameters,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .","[('is', (3, 4))]","[('CharCNN filter length', (0, 3)), ('1 , 3 , 5', (4, 9)), ('50 dimensions', (12, 14))]","[['CharCNN filter length', 'is', '1 , 3 , 5'], ['CharCNN filter length', 'is', '50 dimensions']]","[['CharCNN filter length', 'has', '1 , 3 , 5']]",[],"[['Hyperparameters', 'has', 'CharCNN filter length']]",natural_language_inference,3,170
3026,results,The cluster number K in discriminative block is 100 .,"[('in', (4, 5)), ('is', (7, 8))]","[('cluster number K', (1, 4)), ('discriminative block', (5, 7)), ('100', (8, 9))]","[['cluster number K', 'in', 'discriminative block'], ['discriminative block', 'is', '100']]",[],[],"[['Results', 'has', 'cluster number K']]",natural_language_inference,3,172
3027,hyperparameters,The Adam method is used for optimization .,"[('used for', (4, 6))]","[('Adam method', (1, 3)), ('optimization', (6, 7))]","[['Adam method', 'used for', 'optimization']]",[],[],"[['Hyperparameters', 'has', 'Adam method']]",natural_language_inference,3,173
3028,results,The initial learning rate is 0.0004 and the batch size is 32 .,"[('is', (4, 5))]","[('initial learning rate', (1, 4)), ('0.0004', (5, 6)), ('batch size', (8, 10)), ('32', (11, 12))]","[['initial learning rate', 'is', '0.0004'], ['batch size', 'is', '32']]","[['initial learning rate', 'has', '0.0004'], ['batch size', 'has', '32']]",[],"[['Results', 'has', 'initial learning rate']]",natural_language_inference,3,175
3029,hyperparameters,"All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .","[('of', (3, 4)), ('are', (8, 9)), ('is', (19, 20))]","[('hidden states', (1, 3)), ('GRUs , and TreeLSTMs', (4, 8)), ('500 dimensions', (9, 11)), ('word - level embedding d w', (13, 19)), ('300 dimensions', (20, 22))]","[['hidden states', 'of', 'GRUs , and TreeLSTMs'], ['GRUs , and TreeLSTMs', 'are', '500 dimensions'], ['word - level embedding d w', 'is', '300 dimensions']]","[['word - level embedding d w', 'has', '300 dimensions']]",[],"[['Hyperparameters', 'has', 'hidden states']]",natural_language_inference,3,178
3030,results,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .","[('set', (1, 2)), ('of', (4, 5)), ('to', (6, 7)), ('drop', (10, 11)), ('on', (18, 19))]","[('max length', (2, 4)), ('document', (5, 6)), ('500', (7, 8)), ('question - document pairs', (12, 16)), ('training set', (19, 21))]","[['max length', 'of', 'document'], ['document', 'to', '500'], ['question - document pairs', 'on', 'training set']]",[],"[['Results', 'set', 'max length']]",[],natural_language_inference,3,179
3031,results,We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,"[('apply', (1, 2)), ('to', (3, 4)), ('with', (10, 11)), ('of', (14, 15))]","[('dropout', (2, 3)), ('Encoder layer and aggregation layer', (5, 10)), ('dropout rate', (12, 14)), ('0.5', (15, 16))]","[['dropout', 'to', 'Encoder layer and aggregation layer'], ['Encoder layer and aggregation layer', 'with', 'dropout rate'], ['dropout rate', 'of', '0.5']]",[],"[['Results', 'apply', 'dropout']]",[],natural_language_inference,3,181
3032,results,"Our model achieves a 68.73 % EM score and 77.39 % F1 score , which is ranked among the state of the art single models ( without model ensembling shows the ablation performances of various Q- code on the development set .","[('achieves', (2, 3)), ('ranked among', (16, 18))]","[('Our model', (0, 2)), ('68.73 % EM score', (4, 8)), ('77.39 % F1 score', (9, 13)), ('state of the art single models', (19, 25))]","[['Our model', 'achieves', '68.73 % EM score'], ['Our model', 'achieves', '77.39 % F1 score'], ['77.39 % F1 score', 'ranked among', 'state of the art single models']]",[],[],"[['Results', 'has', 'Our model']]",natural_language_inference,3,192
3033,results,"Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .","[('using', (3, 4)), ('achieved', (7, 8))]","[('Our baseline model', (0, 3)), ('no Q- code', (4, 7)), ('68.00 % and 77.36 % EM and F 1 scores', (9, 19))]","[['Our baseline model', 'using', 'no Q- code'], ['no Q- code', 'achieved', '68.00 % and 77.36 % EM and F 1 scores']]",[],[],[],natural_language_inference,3,194
3034,results,"When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .","[('added', (2, 3)), ('into', (10, 11)), ('improved slightly to', (18, 21))]","[('explicit question type T - code', (4, 10)), ('baseline model', (12, 14)), ('performance', (16, 17)), ('68.16 % ( EM )', (21, 26)), ('77.58 % ( F1 )', (27, 32))]","[['explicit question type T - code', 'into', 'baseline model'], ['performance', 'improved slightly to', '68.16 % ( EM )'], ['performance', 'improved slightly to', '77.58 % ( F1 )']]",[],"[['Results', 'added', 'explicit question type T - code']]",[],natural_language_inference,3,195
3035,results,"We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .","[('used', (2, 3)), ('introduce', (4, 5)), ('for', (7, 8)), ('consistently shows', (24, 26))]","[('TreeLSTM', (3, 4)), ('syntactic parses', (5, 7)), ('question representation and understanding', (8, 12))]","[['TreeLSTM', 'introduce', 'syntactic parses'], ['syntactic parses', 'for', 'question representation and understanding']]","[['TreeLSTM', 'has', 'syntactic parses']]","[['Results', 'used', 'TreeLSTM']]",[],natural_language_inference,3,196
3036,results,"When letting the number of hidden question types ( K ) to be 20 , the performance improves to 68.73%/77.74 % on EM and F1 , respectively , which corresponds to the results of our model reported in .","[('letting', (1, 2)), ('to be', (11, 13)), ('on', (21, 22))]","[('number of hidden question types ( K )', (3, 11)), ('20', (13, 14)), ('performance', (16, 17)), ('improves', (17, 18)), ('68.73%/77.74 %', (19, 21)), ('EM and F1', (22, 25))]","[['number of hidden question types ( K )', 'to be', '20'], ['improves', 'to be', '68.73%/77.74 %'], ['68.73%/77.74 %', 'on', 'EM and F1']]","[['number of hidden question types ( K )', 'has', '20'], ['20', 'has', 'performance'], ['performance', 'has', 'improves'], ['improves', 'has', '68.73%/77.74 %']]","[['Results', 'letting', 'number of hidden question types ( K )']]",[],natural_language_inference,3,198
3037,research-problem,Convolutional Neural Network Architectures for Matching Natural Language Sentences,[],[],[],[],[],[],natural_language_inference,30,2
3038,research-problem,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",[],"[('Semantic matching', (0, 2))]",[],[],[],[],natural_language_inference,30,4
3039,research-problem,Matching two potentially heterogenous language objects is central to many natural language applications .,[],"[('Matching two potentially heterogenous language objects', (0, 6)), ('central', (7, 8))]",[],"[['Matching two potentially heterogenous language objects', 'has', 'central']]",[],[],natural_language_inference,30,11
3040,model,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .","[('generalizes', (1, 2)), ('of', (5, 6))]","[('conventional notion', (3, 5)), ('similarity', (6, 7))]","[['conventional notion', 'of', 'similarity']]",[],"[['Model', 'generalizes', 'conventional notion']]",[],natural_language_inference,30,12
3041,model,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .","[('propose', (5, 6)), ('adapt', (12, 13)), ('to', (24, 25))]","[('deep neural network models', (6, 10)), ('convolutional strategy', (14, 16)), ('natural language', (25, 27))]","[['deep neural network models', 'adapt', 'convolutional strategy'], ['convolutional strategy', 'to', 'natural language']]",[],"[['Model', 'propose', 'deep neural network models']]",[],natural_language_inference,30,16
3042,model,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .","[('devise', (13, 14)), ('can naturally host', (18, 21)), ('for', (25, 26)), ('of', (35, 36)), ('with', (38, 39))]","[('novel model', (15, 17)), ('sentences', (26, 27)), ('simple - to - comprehensive fusion', (29, 35)), ('matching patterns', (36, 38)), ('same convolutional architecture', (40, 43))]","[['novel model', 'can naturally host', 'simple - to - comprehensive fusion'], ['simple - to - comprehensive fusion', 'of', 'matching patterns'], ['matching patterns', 'with', 'same convolutional architecture']]",[],"[['Model', 'devise', 'novel model']]",[],natural_language_inference,30,17
3043,experiments,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,"[('perform', (4, 5)), ('with', (6, 7)), ('can be easily', (16, 19)), ('with', (23, 24))]","[('better', (5, 6)), ('mini-batch ( 100 ? 200 in sizes )', (7, 15)), ('parallelized', (19, 20)), ('single machine', (21, 23)), ('multi-cores', (24, 25))]","[['better', 'with', 'mini-batch ( 100 ? 200 in sizes )'], ['single machine', 'with', 'multi-cores'], ['mini-batch ( 100 ? 200 in sizes )', 'can be easily', 'parallelized'], ['single machine', 'with', 'multi-cores']]",[],[],[],natural_language_inference,30,128
3044,experiments,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .","[('For', (0, 1)), ('find that for', (4, 7)), ('is enough for', (12, 15)), ('with', (16, 17))]","[('regularization', (1, 2)), ('both architectures', (7, 9)), ('early stopping', (10, 12)), ('models', (15, 16)), ('medium size and large training sets ( with', (17, 25)), ('over 500K instances )', (25, 29))]","[['regularization', 'find that for', 'both architectures'], ['early stopping', 'is enough for', 'models'], ['models', 'with', 'medium size and large training sets ( with']]","[['both architectures', 'has', 'early stopping']]",[],[],natural_language_inference,30,129
3045,hyperparameters,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .","[('use', (1, 2)), ('trained with', (7, 9))]","[('50 - dimensional word embedding', (2, 7)), ('Word2 Vec', (10, 12))]","[['50 - dimensional word embedding', 'trained with', 'Word2 Vec']]",[],"[['Hyperparameters', 'use', '50 - dimensional word embedding']]",[],natural_language_inference,30,131
3046,experiments,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .","[('use', (1, 2)), ('as', (3, 4)), ('for', (7, 8)), ('yields', (18, 19)), ('to', (23, 24))]","[('ReLu', (2, 3)), ('activation function', (5, 7)), ('all of models ( convolution and MLP )', (8, 16)), ('comparable or better results', (19, 23)), ('sigmoid - like functions', (24, 28)), ('converges', (30, 31)), ('faster', (31, 32))]","[['ReLu', 'as', 'activation function'], ['activation function', 'for', 'all of models ( convolution and MLP )'], ['activation function', 'yields', 'comparable or better results'], ['all of models ( convolution and MLP )', 'yields', 'comparable or better results'], ['comparable or better results', 'to', 'sigmoid - like functions']]","[['converges', 'has', 'faster']]",[],[],natural_language_inference,30,136
3047,baselines,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,"[('as', (9, 10)), ('of', (15, 16))]","[('WORDEMBED', (0, 1)), ('each short - text', (5, 9)), ('sum', (11, 12)), ('embedding', (14, 15)), ('words it contains', (17, 20))]","[['each short - text', 'as', 'sum'], ['embedding', 'of', 'words it contains']]",[],[],"[['Baselines', 'has', 'WORDEMBED']]",natural_language_inference,30,142
3048,baselines,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :","[('of', (3, 4)), ('calculated with', (9, 11)), ('with', (13, 14)), ('of', (16, 17)), ('train it on', (32, 35)), ('with', (37, 38)), ('in', (45, 46))]","[('matching score', (1, 3)), ('two short - texts', (4, 8)), ('MLP', (12, 13)), ('embedding', (15, 16)), ('two documents', (18, 20)), ('DEEPMATCH', (23, 24)), ('our datasets', (35, 37)), ('3 hidden layers', (38, 41)), ('1,000 hidden nodes', (42, 45)), ('first hidden layer', (47, 50)), ('URAE+ MLP', (51, 53))]","[['matching score', 'of', 'two short - texts'], ['embedding', 'of', 'two documents'], ['two short - texts', 'calculated with', 'MLP'], ['MLP', 'with', 'embedding'], ['our datasets', 'with', '3 hidden layers'], ['embedding', 'of', 'two documents'], ['DEEPMATCH', 'train it on', 'our datasets'], ['our datasets', 'with', '3 hidden layers'], ['our datasets', 'with', '1,000 hidden nodes'], ['1,000 hidden nodes', 'in', 'first hidden layer']]",[],[],"[['Baselines', 'has', 'matching score']]",natural_language_inference,30,143
3049,baselines,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :","[('use', (1, 2)), ('to get', (6, 8)), ('of', (13, 14)), ('put', (18, 19)), ('on', (21, 22)), ('as in', (24, 26))]","[('Unfolding Recursive Autoencoder', (3, 6)), ('100 dimensional vector representation', (9, 13)), ('each sentence', (14, 16)), ('MLP', (20, 21)), ('top', (23, 24)), ('WORDEMBED', (26, 27)), ('SENNA + MLP / SIM', (28, 33))]","[['Unfolding Recursive Autoencoder', 'to get', '100 dimensional vector representation'], ['100 dimensional vector representation', 'of', 'each sentence'], ['MLP', 'on', 'top'], ['top', 'as in', 'WORDEMBED']]",[],[],[],natural_language_inference,30,144
3050,baselines,We use the SENNA - type sentence model for sentence representation ;,"[('use', (1, 2)), ('for', (8, 9))]","[('SENNA - type sentence model', (3, 8)), ('sentence representation', (9, 11))]","[['SENNA - type sentence model', 'for', 'sentence representation']]",[],[],[],natural_language_inference,30,145
3051,baselines,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .","[('take', (3, 4)), ('as', (7, 8)), ('with', (10, 11)), ('use', (18, 19)), ('to obtain', (21, 23))]","[('SENMLP', (0, 1)), ('whole sentence', (5, 7)), ('input', (8, 9)), ('word embedding aligned sequentially', (11, 15)), ('MLP', (20, 21)), ('score of coherence', (24, 27))]","[['SENMLP', 'take', 'whole sentence'], ['whole sentence', 'as', 'input'], ['input', 'with', 'word embedding aligned sequentially'], ['SENMLP', 'use', 'MLP'], ['MLP', 'to obtain', 'score of coherence']]",[],[],"[['Baselines', 'has', 'SENMLP']]",natural_language_inference,30,146
3052,results,Experiment I : Sentence Completion,[],[],[],[],[],[],natural_language_inference,30,148
3053,results,"The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .","[('get', (4, 5)), ('of', (7, 8)), ('with', (13, 14)), ('over', (16, 17)), ('without', (22, 23))]","[('two proposed', (1, 3)), ('nearly half', (5, 7)), ('cases right', (9, 11)), ('large margin', (14, 16)), ('other sentence models', (17, 20)), ('explicit sequence modeling', (23, 26))]","[['two proposed', 'get', 'nearly half'], ['nearly half', 'of', 'cases right'], ['cases right', 'with', 'large margin'], ['large margin', 'over', 'other sentence models']]",[],[],"[['Results', 'has', 'two proposed']]",natural_language_inference,30,156
3054,results,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .","[('showing', (9, 10))]","[('ARC - II', (0, 3)), ('outperforms', (3, 4)), ('ARC - I', (4, 7)), ('significantly', (7, 8))]",[],"[['ARC - II', 'has', 'outperforms'], ['outperforms', 'has', 'ARC - I'], ['ARC - I', 'has', 'significantly']]",[],"[['Results', 'has', 'ARC - II']]",natural_language_inference,30,157
3055,results,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .","[('performs', (8, 9))]","[('SENNA + MLP', (5, 8)), ('fairly well', (9, 11))]","[['SENNA + MLP', 'performs', 'fairly well']]",[],[],"[['Results', 'has', 'SENNA + MLP']]",natural_language_inference,30,158
3056,results,"Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .","[('beats', (4, 5)), ('with', (7, 8))]","[('ARC', (1, 2)), ('other models', (5, 7)), ('large margins', (8, 10)), ('two convolutional', (12, 14))]","[['ARC', 'beats', 'other models'], ['other models', 'with', 'large margins']]",[],[],[],natural_language_inference,30,162
3057,research-problem,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes,[],"[('Scaling Memory - Augmented Neural Networks', (0, 6))]",[],[],[],[],natural_language_inference,31,2
3058,approach,"External memory allows MANNs to learn algorithmic solutions to problems that have eluded the capabilities of traditional LSTMs , and to generalize to longer sequence lengths .","[('allows', (2, 3)), ('to learn', (4, 6)), ('to', (8, 9)), ('to generalize', (20, 22)), ('to', (22, 23))]","[('External memory', (0, 2)), ('MANNs', (3, 4)), ('algorithmic solutions', (6, 8)), ('problems', (9, 10)), ('capabilities', (14, 15)), ('longer sequence lengths', (23, 26))]","[['External memory', 'allows', 'MANNs'], ['MANNs', 'to learn', 'algorithmic solutions'], ['algorithmic solutions', 'to', 'problems'], ['External memory', 'to generalize', 'longer sequence lengths']]",[],[],"[['Approach', 'has', 'External memory']]",natural_language_inference,31,16
3059,approach,"In this paper , we present a MANN named SAM ( sparse access memory ) .","[('present', (5, 6)), ('named', (8, 9))]","[('MANN', (7, 8)), ('SAM ( sparse access memory )', (9, 15))]","[['MANN', 'named', 'SAM ( sparse access memory )']]","[['MANN', 'name', 'SAM ( sparse access memory )']]","[['Approach', 'present', 'MANN']]",[],natural_language_inference,31,23
3060,approach,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .","[('thresholding', (1, 2)), ('to', (4, 5)), ('using', (10, 11)), ('for', (14, 15)), ('is', (23, 24)), ('in', (25, 26)), ('with respect to', (29, 32)), ('retaining', (36, 37))]","[('memory modifications', (2, 4)), ('sparse subset', (6, 8)), ('efficient data structures', (11, 14)), ('content - based read operations', (15, 20)), ('our model', (21, 23)), ('optimal', (24, 25)), ('space and time', (26, 29)), ('memory size', (32, 34)), ('end - to - end gradient based optimization', (37, 45))]","[['memory modifications', 'to', 'sparse subset'], ['efficient data structures', 'for', 'content - based read operations'], ['our model', 'is', 'optimal'], ['optimal', 'in', 'space and time'], ['space and time', 'with respect to', 'memory size'], ['our model', 'retaining', 'end - to - end gradient based optimization']]","[['memory modifications', 'has', 'sparse subset'], ['our model', 'has', 'optimal']]","[['Approach', 'thresholding', 'memory modifications']]",[],natural_language_inference,31,24
3061,approach,"This Sparse Differentiable Neural Computer ( SDNC ) is over 400 faster than the canonical dense variant fora memory size of 2,000 slots , and achieves the best reported result in the Babi tasks without supervising the memory access .","[('is', (8, 9)), ('than', (12, 13)), ('fora', (17, 18)), ('of', (20, 21)), ('achieves', (25, 26)), ('in', (30, 31)), ('without supervising', (34, 36))]","[('Sparse Differentiable Neural Computer ( SDNC )', (1, 8)), ('over 400 faster', (9, 12)), ('canonical dense variant', (14, 17)), ('memory size', (18, 20)), ('2,000 slots', (21, 23)), ('best reported result', (27, 30)), ('Babi tasks', (32, 34)), ('memory access', (37, 39))]","[['Sparse Differentiable Neural Computer ( SDNC )', 'is', 'over 400 faster'], ['over 400 faster', 'than', 'canonical dense variant'], ['canonical dense variant', 'fora', 'memory size'], ['memory size', 'of', '2,000 slots'], ['Sparse Differentiable Neural Computer ( SDNC )', 'achieves', 'best reported result'], ['best reported result', 'in', 'Babi tasks'], ['best reported result', 'without supervising', 'memory access'], ['Babi tasks', 'without supervising', 'memory access']]","[['Sparse Differentiable Neural Computer ( SDNC )', 'has', 'over 400 faster']]",[],"[['Approach', 'has', 'Sparse Differentiable Neural Computer ( SDNC )']]",natural_language_inference,31,30
3062,results,Speed and memory benchmarks,[],[],[],[],[],[],natural_language_inference,31,136
3063,results,Question answering on the Babi tasks,[],[],[],[],[],[],natural_language_inference,31,164
3064,results,"The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .","[('except', (3, 4)), ('able to learn', (8, 11)), ('comparable to', (12, 14))]","[('MANNs', (1, 2)), ('NTM', (5, 6)), ('solutions', (11, 12)), ('previous best results', (15, 18)), ('failing', (19, 20)), ('only 2 of the tasks', (21, 26))]","[['MANNs', 'except', 'NTM'], ['MANNs', 'able to learn', 'solutions'], ['solutions', 'comparable to', 'previous best results']]",[],[],[],natural_language_inference,31,169
3065,results,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .","[('manages to', (2, 4))]","[('SDNC', (1, 2)), ('solve', (4, 5)), ('all but 1 of the tasks', (5, 11))]","[['SDNC', 'manages to', 'solve']]","[['SDNC', 'has', 'solve'], ['solve', 'has', 'all but 1 of the tasks']]",[],"[['Results', 'has', 'SDNC']]",natural_language_inference,31,170
3066,results,"More directly comparable previous work with end - to - end memory networks , which did not use supervision , fails at 6 of the tasks .","[('with', (5, 6)), ('which did not use', (14, 18)), ('at', (21, 22))]","[('end - to - end memory networks', (6, 13)), ('supervision', (18, 19)), ('fails', (20, 21)), ('6 of the tasks', (22, 26))]","[['end - to - end memory networks', 'which did not use', 'supervision'], ['fails', 'at', '6 of the tasks']]","[['supervision', 'has', 'fails'], ['fails', 'has', '6 of the tasks']]",[],[],natural_language_inference,31,172
3067,results,"Both the sparse and dense perform comparably at this task , again indicating the sparse approximations do not impair learning .","[('perform', (5, 6))]","[('Both the sparse and dense', (0, 5)), ('comparably', (6, 7))]","[['Both the sparse and dense', 'perform', 'comparably']]",[],[],"[['Results', 'has', 'Both the sparse and dense']]",natural_language_inference,31,173
3068,results,"SAM outperformed other models , presumably due to its much larger memory capacity .",[],"[('SAM', (0, 1)), ('outperformed', (1, 2)), ('other models', (2, 4))]",[],"[['SAM', 'has', 'outperformed'], ['outperformed', 'has', 'other models']]",[],[],natural_language_inference,31,187
3069,results,"SAM is able to outperform other approaches , presumably because it can utilize a much larger memory .","[('able to', (2, 4))]","[('SAM', (0, 1)), ('outperform', (4, 5)), ('other approaches', (5, 7))]","[['SAM', 'able to', 'outperform']]","[['SAM', 'has', 'outperform'], ['outperform', 'has', 'other approaches']]",[],"[['Results', 'has', 'SAM']]",natural_language_inference,31,195
3070,results,G Babi results,[],[],[],[],[],[],natural_language_inference,31,329
3071,results,"SDNC achieves the best reported result on this task with unsupervised memory access , solving all but 1 task .","[('achieves', (1, 2)), ('with', (9, 10)), ('solving', (14, 15))]","[('SDNC', (0, 1)), ('best reported result', (3, 6)), ('unsupervised memory access', (10, 13)), ('all but 1 task', (15, 19))]","[['SDNC', 'achieves', 'best reported result'], ['best reported result', 'with', 'unsupervised memory access'], ['best reported result', 'solving', 'all but 1 task']]","[['SDNC', 'has', 'best reported result']]",[],"[['Results', 'has', 'SDNC']]",natural_language_inference,31,339
3072,research-problem,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller,[],"[('Large - Scale Reading Comprehension', (2, 7))]",[],[],[],[],natural_language_inference,4,2
3073,research-problem,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .","[('propose', (5, 6)), ('to handle', (12, 14)), ('in', (19, 20))]","[('novel deep neural network architecture', (7, 12)), ('long - range dependency', (15, 19)), ('RC tasks', (20, 22))]","[['novel deep neural network architecture', 'to handle', 'long - range dependency'], ['long - range dependency', 'in', 'RC tasks']]",[],"[['Research problem', 'propose', 'novel deep neural network architecture']]",[],natural_language_inference,4,6
3074,research-problem,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,[],"[('Reading comprehension ( RC )', (0, 5))]",[],[],[],[],natural_language_inference,4,13
3075,model,"Inspired by these approaches , we develop a customized memory controller along with an external memory augmentation for complicated RC tasks .","[('develop', (6, 7)), ('along with', (11, 13)), ('for', (17, 18))]","[('customized memory controller', (8, 11)), ('external memory augmentation', (14, 17)), ('complicated RC tasks', (18, 21))]","[['customized memory controller', 'along with', 'external memory augmentation'], ['external memory augmentation', 'for', 'complicated RC tasks']]",[],"[['Model', 'develop', 'customized memory controller']]",[],natural_language_inference,4,21
3076,model,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,"[('extend', (1, 2)), ('with', (5, 6)), ('to alleviate', (9, 11))]","[('memory controller', (3, 5)), ('residual connection', (7, 9)), ('information distortion', (12, 14))]","[['memory controller', 'with', 'residual connection'], ['residual connection', 'to alleviate', 'information distortion']]","[['memory controller', 'has', 'residual connection']]","[['Model', 'extend', 'memory controller']]",[],natural_language_inference,4,24
3077,model,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,"[('expand', (2, 3)), ('with', (10, 11)), ('conveys', (15, 16)), ('to', (18, 19)), ('containing', (22, 23)), ('as well as', (25, 28))]","[('gated recurrent unit ( GRU )', (4, 10)), ('dense connection', (12, 14)), ('enriched features', (16, 18)), ('next layer', (20, 22)), ('original', (24, 25)), ('transformed information', (29, 31))]","[['gated recurrent unit ( GRU )', 'with', 'dense connection'], ['dense connection', 'conveys', 'enriched features'], ['enriched features', 'to', 'next layer'], ['next layer', 'containing', 'original']]","[['gated recurrent unit ( GRU )', 'has', 'dense connection']]","[['Model', 'expand', 'gated recurrent unit ( GRU )']]",[],natural_language_inference,4,25
3078,model,( 1 ) We propose an extended memory controller module for RC tasks .,"[('propose', (4, 5)), ('for', (10, 11))]","[('extended memory controller module', (6, 10)), ('RC tasks', (11, 13))]","[['extended memory controller module', 'for', 'RC tasks']]",[],"[['Model', 'propose', 'extended memory controller module']]",[],natural_language_inference,4,31
3079,experiments,NLTK is used for tokenizing words .,"[('used for', (2, 4))]","[('NLTK', (0, 1)), ('tokenizing words', (4, 6))]","[['NLTK', 'used for', 'tokenizing words']]",[],[],[],natural_language_inference,4,121
3080,experiments,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .","[('In', (0, 1)), ('use', (6, 7)), ('set to', (20, 22)), ('initialized as', (27, 29))]","[('memory controller', (2, 4)), ('four read heads', (7, 10)), ('one write head', (11, 14)), ('memory size', (17, 19)), ('100 36', (22, 24)), ('0', (29, 30))]","[['memory controller', 'use', 'four read heads'], ['memory controller', 'use', 'one write head'], ['memory size', 'set to', '100 36']]","[['memory size', 'has', '100 36']]",[],[],natural_language_inference,4,122
3081,experiments,The hidden vector dimension l is set to 200 .,"[('set to', (6, 8))]","[('hidden vector dimension l', (1, 5)), ('200', (8, 9))]","[['hidden vector dimension l', 'set to', '200']]","[['hidden vector dimension l', 'has', '200']]",[],[],natural_language_inference,4,123
3082,experiments,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .","[('use', (1, 2)), ('as', (8, 9)), ('with', (11, 12)), ('of', (15, 16))]","[('AdaDelta ( Zeiler , 2012 )', (2, 8)), ('optimizer', (10, 11)), ('learning rate', (13, 15)), ('0.5', (16, 17))]","[['AdaDelta ( Zeiler , 2012 )', 'as', 'optimizer'], ['AdaDelta ( Zeiler , 2012 )', 'with', 'learning rate'], ['optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.5']]","[['learning rate', 'has', '0.5']]",[],[],natural_language_inference,4,124
3083,experiments,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,"[('set to', (4, 6)), ('for', (7, 8))]","[('batch size', (1, 3)), ('20', (6, 7)), ('TriviaQA', (8, 9)), ('30', (10, 11)), ('SQuAD', (12, 13)), ('QUASAR', (14, 15))]","[['batch size', 'set to', '20'], ['20', 'for', 'TriviaQA'], ['30', 'for', 'SQuAD']]","[['batch size', 'has', '20']]",[],[],natural_language_inference,4,125
3084,experiments,We use an exponential moving average of weights with a decaying factor of 0.001 .,"[('use', (1, 2)), ('of', (6, 7)), ('with', (8, 9)), ('of', (12, 13))]","[('exponential moving average', (3, 6)), ('weights', (7, 8)), ('decaying factor', (10, 12)), ('0.001', (13, 14))]","[['exponential moving average', 'of', 'weights'], ['decaying factor', 'of', '0.001'], ['weights', 'with', 'decaying factor'], ['decaying factor', 'of', '0.001']]",[],[],[],natural_language_inference,4,126
3085,results,"For our quantitative comparisons , we use BiDAF with self attention ) as a baseline , which maintains the best results published on both TriviaQA and SQuAD datasets .","[('use', (6, 7)), ('as', (12, 13)), ('maintains', (17, 18)), ('published on', (21, 23))]","[('BiDAF with self attention', (7, 11)), ('best results', (19, 21)), ('TriviaQA and SQuAD datasets', (24, 28))]","[['best results', 'published on', 'TriviaQA and SQuAD datasets']]",[],"[['Results', 'use', 'BiDAF with self attention']]",[],natural_language_inference,4,129
3086,results,"In TriviaQA and QUASAR - T dataset , we compare our model with BiDAF as well as its variant called ' BiDAF + DNC , ' which is augmented with an existing external memory architecture just before the answer prediction layer in the BiDAF .","[('compare', (9, 10)), ('called', (19, 20)), ('augmented with', (28, 30)), ('just before', (35, 37)), ('in', (41, 42))]","[('TriviaQA and QUASAR - T dataset', (1, 7)), ('BiDAF', (13, 14)), ('BiDAF + DNC', (21, 24)), ('existing external memory architecture', (31, 35)), ('answer prediction layer', (38, 41)), ('BiDAF', (43, 44))]","[['BiDAF + DNC', 'augmented with', 'existing external memory architecture'], ['existing external memory architecture', 'just before', 'answer prediction layer'], ['answer prediction layer', 'in', 'BiDAF']]",[],[],[],natural_language_inference,4,130
3087,results,"Overall , in lengthy - document cases such as Trivi aQA and QUASAR - T , our model outperforms all the published results , as seen in Tables 2 and 3 , while in the short - document case such as SQuAD , we mostly achieve the best results , as seen in .","[('in', (2, 3)), ('such as', (7, 9)), ('achieve', (45, 46))]","[('lengthy - document cases', (3, 7)), ('Trivi aQA', (9, 11)), ('our model', (16, 18)), ('outperforms', (18, 19)), ('all the published results', (19, 23)), ('short - document case', (35, 39)), ('best results', (47, 49))]","[['lengthy - document cases', 'such as', 'Trivi aQA'], ['short - document case', 'achieve', 'best results']]","[['lengthy - document cases', 'name', 'Trivi aQA'], ['our model', 'has', 'outperforms'], ['outperforms', 'has', 'all the published results']]","[['Results', 'in', 'lengthy - document cases']]",[],natural_language_inference,4,131
3088,results,TriviaQA .,[],"[('TriviaQA', (0, 1))]",[],[],[],"[['Results', 'has', 'TriviaQA']]",natural_language_inference,4,133
3089,results,"As shown in , our model , even without DEBS , outperforms the existing state - of - the - art method such as ' BiDAF + SA + SN ' by a large margin in all the cases .","[('such as', (22, 24)), ('by', (31, 32))]","[('our model', (4, 6)), ('outperforms', (11, 12)), ('existing state - of - the - art method', (13, 22)), (""BiDAF + SA + SN '"", (25, 31)), ('large margin', (33, 35))]","[['existing state - of - the - art method', 'such as', ""BiDAF + SA + SN '""], [""BiDAF + SA + SN '"", 'by', 'large margin']]","[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'existing state - of - the - art method'], ['existing state - of - the - art method', 'name', ""BiDAF + SA + SN '""]]",[],"[['Results', 'has', 'our model']]",natural_language_inference,4,134
3090,results,"Our model with DEBS , which replaces BiGRU encoder blocks , performs even better than that without it in all the cases except for the combination of the ' full ' and ' Wikipedia ' case , which involves documents containing no relevant information for the answer .","[('replaces', (6, 7)), ('performs', (11, 12)), ('than', (14, 15)), ('except for', (22, 24))]","[('Our model with', (0, 3)), ('DEBS', (3, 4)), ('BiGRU encoder blocks', (7, 10)), ('even better', (12, 14)), ('all the cases', (19, 22)), ('combination of', (25, 27)), (""' full ' and ' Wikipedia ' case"", (28, 36))]","[['Our model with', 'replaces', 'BiGRU encoder blocks'], ['DEBS', 'replaces', 'BiGRU encoder blocks'], ['Our model with', 'performs', 'even better'], ['DEBS', 'performs', 'even better'], ['all the cases', 'except for', 'combination of']]","[['Our model with', 'has', 'DEBS'], ['combination of', 'has', ""' full ' and ' Wikipedia ' case""]]",[],"[['Results', 'has', 'Our model with']]",natural_language_inference,4,135
3091,results,"We note that our method achieves these outstanding results without any additional features. , our simple baseline ' BiDAF + DNC , ' which involves an existing memory architecture , improves performance over BiDAF , indicating the efficacy of incorporating an external memory .","[('note', (1, 2)), ('achieves', (5, 6)), ('involves', (24, 25)), ('over', (32, 33))]","[('our method', (3, 5)), ('outstanding results', (7, 9)), ('BiDAF + DNC', (18, 21)), ('existing memory architecture', (26, 29)), ('improves', (30, 31)), ('performance', (31, 32)), ('BiDAF', (33, 34))]","[['our method', 'achieves', 'outstanding results'], ['BiDAF + DNC', 'involves', 'existing memory architecture'], ['performance', 'over', 'BiDAF']]","[['our method', 'has', 'outstanding results'], ['improves', 'has', 'performance']]","[['Results', 'note', 'our method']]",[],natural_language_inference,4,137
3092,results,"Moreover , our model with the proposed memory controller achieves significantly better results compared to other models .","[('with', (4, 5)), ('achieves', (9, 10)), ('compared to', (13, 15))]","[('our model', (2, 4)), ('proposed memory controller', (6, 9)), ('significantly better results', (10, 13)), ('other models', (15, 17))]","[['our model', 'with', 'proposed memory controller'], ['our model', 'achieves', 'significantly better results'], ['proposed memory controller', 'achieves', 'significantly better results'], ['significantly better results', 'compared to', 'other models']]","[['our model', 'has', 'proposed memory controller']]",[],[],natural_language_inference,4,138
3093,ablation-analysis,"First , we adopt ELMo to our model ( without DEBS ) , which uses word embedding as the weighted sum of the hidden layers of a language model with regularization as an additional feature to our word embeddings .","[('adopt', (3, 4)), ('to', (5, 6)), ('uses', (14, 15)), ('as', (17, 18)), ('with', (29, 30))]","[('ELMo', (4, 5)), ('our model ( without DEBS )', (6, 12)), ('word embedding', (15, 17)), ('weighted', (19, 20)), ('regularization', (30, 31)), ('our word embeddings', (36, 39))]","[['ELMo', 'to', 'our model ( without DEBS )'], ['ELMo', 'uses', 'word embedding'], ['our model ( without DEBS )', 'uses', 'word embedding'], ['word embedding', 'as', 'weighted']]","[['ELMo', 'has', 'our model ( without DEBS )']]","[['Ablation analysis', 'adopt', 'ELMo']]",[],natural_language_inference,4,141
3094,results,"This improves the F 1 score of our model up to 85.13 and EM to 77. 44 , showing the highest performances among all the methods without using self attention .","[('improves', (1, 2)), ('of', (6, 7)), ('up to', (9, 11))]","[('F 1 score', (3, 6)), ('our model', (7, 9)), ('85.13', (11, 12)), ('EM', (13, 14)), ('77. 44', (15, 17))]","[['F 1 score', 'of', 'our model'], ['our model', 'up to', '85.13']]","[['EM', 'has', '77. 44']]",[],[],natural_language_inference,4,142
3095,results,"Due to the relatively short document length in SQuAD compared to TriviaQA and QUASAR - T , our model without DEBS performs worse than the baseline ' BiDAF + Self Attention + ELMo. '","[('without', (19, 20)), ('performs', (21, 22)), ('than', (23, 24))]","[('our model', (17, 19)), ('DEBS', (20, 21)), ('worse', (22, 23)), ('baseline', (25, 26)), ('BiDAF + Self Attention + ELMo.', (27, 33))]","[['our model', 'without', 'DEBS'], ['our model', 'performs', 'worse'], ['worse', 'than', 'baseline'], ['worse', 'than', 'BiDAF + Self Attention + ELMo.']]","[['baseline', 'name', 'BiDAF + Self Attention + ELMo.']]",[],"[['Results', 'has', 'our model']]",natural_language_inference,4,143
3096,results,QUASAR - T .,[],[],[],[],[],[],natural_language_inference,4,146
3097,results,"As can be seen in , using DEBS in all the places improves the performance most , and furthermore , the memory controller with DEBS gives the largest performance margin .","[('using', (6, 7)), ('in', (8, 9)), ('improves', (12, 13)), ('with', (23, 24)), ('gives', (25, 26))]","[('DEBS', (7, 8)), ('all the places', (9, 12)), ('performance', (14, 15)), ('most', (15, 16)), ('memory controller', (21, 23)), ('DEBS', (24, 25)), ('largest performance margin', (27, 30))]","[['DEBS', 'in', 'all the places'], ['DEBS', 'improves', 'performance'], ['all the places', 'improves', 'performance'], ['memory controller', 'with', 'DEBS'], ['memory controller', 'gives', 'largest performance margin'], ['DEBS', 'gives', 'largest performance margin']]","[['DEBS', 'has', 'all the places'], ['performance', 'has', 'most'], ['memory controller', 'has', 'DEBS']]","[['Results', 'using', 'DEBS']]",[],natural_language_inference,4,166
3098,research-problem,Sentence Similarity Learning by Lexical Decomposition and Composition,[],"[('Sentence Similarity Learning', (0, 3))]",[],[],[],[],natural_language_inference,5,2
3099,research-problem,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .","[('to determine', (12, 14)), ('are', (17, 18))]","[('paraphrase identification', (4, 6)), ('sentence similarity', (8, 10)), ('two sentences', (15, 17)), ('paraphrases', (18, 19))]","[['sentence similarity', 'to determine', 'two sentences'], ['two sentences', 'are', 'paraphrases']]",[],[],[],natural_language_inference,5,13
3100,research-problem,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,[],"[('paraphrase identification', (1, 3))]",[],[],[],[],natural_language_inference,5,166
3101,hyperparameters,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .","[('set', (5, 6)), ('of', (8, 9)), ('as', (12, 13)), ('pre-train', (18, 19)), ('with', (21, 22)), ('on', (26, 27))]","[('size', (7, 8)), ('word vector dimension', (9, 12)), ('d = 300', (13, 16)), ('vectors', (20, 21)), ('word2 vec toolkit', (23, 26)), ('English Gigaword ( LDC2011T07 )', (28, 33))]","[['size', 'of', 'word vector dimension'], ['word vector dimension', 'as', 'd = 300'], ['vectors', 'with', 'word2 vec toolkit'], ['word2 vec toolkit', 'on', 'English Gigaword ( LDC2011T07 )']]",[],"[['Hyperparameters', 'set', 'size']]",[],natural_language_inference,5,170
3102,research-problem,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding,[],[],[],[],[],[],natural_language_inference,6,2
3103,model,"The vector is then used for various downstream tasks , e.g. , sentiment analysis , natural language inference , etc .","[('used for', (4, 6)), ('e.g.', (10, 11))]","[('vector', (1, 2)), ('various downstream tasks', (6, 9)), ('sentiment analysis', (12, 14)), ('natural language inference', (15, 18))]","[['vector', 'used for', 'various downstream tasks'], ['various downstream tasks', 'e.g.', 'sentiment analysis'], ['various downstream tasks', 'e.g.', 'natural language inference']]",[],[],"[['Model', 'has', 'vector']]",natural_language_inference,6,10
3104,model,Self - attention computes attention weights by the inner product between words and the learnable weight vector .,"[('computes', (3, 4)), ('by', (6, 7)), ('between', (10, 11))]","[('Self - attention', (0, 3)), ('attention weights', (4, 6)), ('inner product', (8, 10)), ('words and the learnable weight vector', (11, 17))]","[['Self - attention', 'computes', 'attention weights'], ['attention weights', 'by', 'inner product'], ['inner product', 'between', 'words and the learnable weight vector']]",[],[],"[['Model', 'has', 'Self - attention']]",natural_language_inference,6,15
3105,model,"The weight vector is important in that it detects informative words , yet it is static during inference .","[('detects', (8, 9)), ('is', (14, 15)), ('during', (16, 17))]","[('weight vector', (1, 3)), ('informative words', (9, 11)), ('static', (15, 16)), ('inference', (17, 18))]","[['weight vector', 'detects', 'informative words'], ['weight vector', 'is', 'static'], ['static', 'during', 'inference']]",[],[],"[['Model', 'has', 'weight vector']]",natural_language_inference,6,16
3106,model,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .","[('propose', (7, 8)), ('for', (14, 15)), ('namely', (18, 19))]","[('new self - attention mechanism', (9, 14)), ('sentence embedding', (15, 17)), ('Dynamic Self - Attention ( DSA )', (19, 26))]","[['new self - attention mechanism', 'for', 'sentence embedding'], ['sentence embedding', 'namely', 'Dynamic Self - Attention ( DSA )']]",[],"[['Model', 'propose', 'new self - attention mechanism']]",[],natural_language_inference,6,23
3107,model,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .","[('modify', (5, 6)), ('such that', (8, 10)), ('functions as', (11, 13)), ('with', (16, 17))]","[('dynamic routing', (6, 8)), ('self - attention', (13, 16)), ('dynamic weight vector', (18, 21))]","[['dynamic routing', 'functions as', 'self - attention'], ['self - attention', 'with', 'dynamic weight vector']]",[],"[['Model', 'modify', 'dynamic routing']]",[],natural_language_inference,6,24
3108,model,"We design and implement Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .","[('design and implement', (1, 4)), ('for', (18, 19))]","[('Dynamic Self - Attention ( DSA )', (4, 11)), ('sentence embedding', (19, 21))]",[],[],"[['Model', 'design and implement', 'Dynamic Self - Attention ( DSA )']]",[],natural_language_inference,6,28
3109,model,We devise the dynamic weight vector with which DSA computes attention weights .,"[('devise', (1, 2)), ('with', (6, 7)), ('computes', (9, 10))]","[('dynamic weight vector', (3, 6)), ('DSA', (8, 9)), ('attention weights', (10, 12))]","[['dynamic weight vector', 'with', 'DSA'], ['dynamic weight vector', 'computes', 'attention weights']]",[],"[['Model', 'devise', 'dynamic weight vector']]",[],natural_language_inference,6,29
3110,baselines,"We implement single DSA , multiple DSA and self - attention in Eq. 1 as a baseline .","[('implement', (1, 2))]","[('single DSA', (2, 4)), ('multiple DSA', (5, 7)), ('self - attention', (8, 11))]",[],[],"[['Baselines', 'implement', 'single DSA']]",[],natural_language_inference,6,96
3111,baselines,Both DSA and self - attention are stacked on CNN with Dense Connection for fair comparison .,"[('stacked on', (7, 9)), ('for', (13, 14))]","[('DSA and self - attention', (1, 6)), ('CNN with Dense Connection', (9, 13))]","[['DSA and self - attention', 'stacked on', 'CNN with Dense Connection']]",[],[],"[['Baselines', 'has', 'DSA and self - attention']]",natural_language_inference,6,97
3112,hyperparameters,"For our implementations , we initialize word embeddings by 300D Glo Ve 840B pretrained vectors , and fix them during training .","[('initialize', (5, 6)), ('by', (8, 9)), ('fix them during', (17, 20))]","[('word embeddings', (6, 8)), ('300D Glo Ve 840B pretrained vectors', (9, 15)), ('training', (20, 21))]","[['word embeddings', 'by', '300D Glo Ve 840B pretrained vectors'], ['word embeddings', 'fix them during', 'training']]",[],"[['Hyperparameters', 'initialize', 'word embeddings']]",[],natural_language_inference,6,98
3113,hyperparameters,We use cross-entropy loss as an objective function for both tasks .,"[('use', (1, 2)), ('as', (4, 5))]","[('cross-entropy loss', (2, 4)), ('objective function', (6, 8)), ('both tasks', (9, 11))]","[['cross-entropy loss', 'as', 'objective function']]",[],"[['Hyperparameters', 'use', 'cross-entropy loss']]",[],natural_language_inference,6,99
3114,experiments,"We set do = 600 , m = 1 for single DSA and do = 300 , m = 8 for multiple DSA .","[('set', (1, 2)), ('for', (9, 10))]","[('do = 600', (2, 5)), ('single DSA', (10, 12))]",[],[],[],[],natural_language_inference,6,100
3115,experiments,Natural Language Inference Results,[],[],[],[],[],[],natural_language_inference,6,102
3116,results,"With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .","[('With', (0, 1)), ('in terms of', (2, 5)), ('outperforms', (14, 15)), ('by', (17, 18))]","[('tradeoffs', (1, 2)), ('parameters and learning time per epoch', (5, 11)), ('multiple DSA', (12, 14)), ('other models', (15, 17)), ('large margin ( + 1.1 % )', (19, 26))]","[['tradeoffs', 'in terms of', 'parameters and learning time per epoch'], ['multiple DSA', 'outperforms', 'other models'], ['other models', 'by', 'large margin ( + 1.1 % )']]",[],"[['Results', 'With', 'tradeoffs']]",[],natural_language_inference,6,115
3117,results,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .","[('In comparison to', (0, 3)), ('shows', (8, 9)), ('than', (11, 12))]","[('single DSA', (6, 8)), ('better performance', (9, 11)), ('self - attention', (12, 15))]","[['single DSA', 'shows', 'better performance'], ['better performance', 'than', 'self - attention']]",[],"[['Results', 'In comparison to', 'single DSA']]",[],natural_language_inference,6,116
3118,results,"Note that our implementation of the baseline , selfattention stacked on CNN with Dense Connection , shows better performance ( + 0.4 % ) than the one stacked on BiLSTM .","[('of', (4, 5)), ('stacked on', (9, 11)), ('with', (12, 13)), ('shows', (16, 17)), ('than', (24, 25)), ('stacked on', (27, 29))]","[('our implementation', (2, 4)), ('baseline', (6, 7)), ('selfattention', (8, 9)), ('CNN', (11, 12)), ('Dense Connection', (13, 15)), ('better performance ( + 0.4 % )', (17, 24)), ('BiLSTM', (29, 30))]","[['our implementation', 'of', 'baseline'], ['selfattention', 'stacked on', 'CNN'], ['CNN', 'with', 'Dense Connection'], ['baseline', 'shows', 'better performance ( + 0.4 % )'], ['selfattention', 'shows', 'better performance ( + 0.4 % )'], ['Dense Connection', 'shows', 'better performance ( + 0.4 % )']]","[['our implementation', 'name', 'baseline'], ['baseline', 'name', 'selfattention']]",[],[],natural_language_inference,6,118
3119,results,Sentiment Analysis Results,[],[],[],[],[],[],natural_language_inference,6,119
3120,results,"Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .","[('in', (7, 8)), ('achieves', (14, 15)), ('in', (17, 18))]","[('Single DSA', (0, 2)), ('outperforms', (2, 3)), ('all the baseline models', (3, 7)), ('SST - 2 dataset', (8, 12)), ('comparative results', (15, 17)), ('SST - 5', (18, 21))]","[['all the baseline models', 'in', 'SST - 2 dataset'], ['comparative results', 'in', 'SST - 5'], ['Single DSA', 'achieves', 'comparative results'], ['comparative results', 'in', 'SST - 5']]","[['Single DSA', 'has', 'outperforms'], ['outperforms', 'has', 'all the baseline models']]",[],[],natural_language_inference,6,127
3121,results,"In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .","[('in', (6, 7)), ('in', (15, 16)), ('in', (22, 23)), ('between', (25, 26))]","[('SNLI dataset', (7, 9)), ('SST dataset', (16, 18)), ('performance', (24, 25)), ('DSA and the previous self - attentive models', (26, 34))]","[['SNLI dataset', 'in', 'SST dataset'], ['performance', 'between', 'DSA and the previous self - attentive models']]",[],"[['Results', 'in', 'SNLI dataset']]",[],natural_language_inference,6,128
3122,research-problem,Teaching Machines to Read and Comprehend,[],[],[],[],[],[],natural_language_inference,7,2
3123,research-problem,Teaching machines to read natural language documents remains an elusive challenge .,[],"[('Teaching machines to read natural language documents', (0, 7))]",[],[],[],[],natural_language_inference,7,4
3124,research-problem,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",[],"[('Machine reading', (0, 2))]",[],[],[],[],natural_language_inference,7,5
3125,model,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,"[('directly', (6, 7)), ('by introducing', (16, 18)), ('to building', (21, 23))]","[('lack of real natural language training data', (9, 16)), ('novel approach', (19, 21)), ('supervised reading comprehension data set', (24, 29))]","[['lack of real natural language training data', 'by introducing', 'novel approach'], ['novel approach', 'to building', 'supervised reading comprehension data set']]",[],"[['Model', 'directly', 'lack of real natural language training data']]",[],natural_language_inference,7,16
3126,dataset,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,"[('collected', (5, 6)), ('of', (9, 10)), ('with', (15, 16)), ('from', (18, 19))]","[('two new corpora', (6, 9)), ('roughly a million news stories', (10, 15)), ('associated queries', (16, 18)), ('CNN and Daily Mail websites', (20, 25))]","[['two new corpora', 'of', 'roughly a million news stories'], ['roughly a million news stories', 'with', 'associated queries'], ['associated queries', 'from', 'CNN and Daily Mail websites']]",[],"[['Dataset', 'collected', 'two new corpora']]",[],natural_language_inference,7,18
3127,model,"Here we propose a methodology for creating real - world , large scale supervised training data for learning reading comprehension models .",[],"[('learning', (17, 18)), ('reading comprehension models', (18, 21))]",[],"[['learning', 'has', 'reading comprehension models']]",[],[],natural_language_inference,7,32
3128,model,"Inspired by work in summarisation , we create two machine reading corpora by exploiting online newspaper articles and their matching summaries .","[('create', (7, 8)), ('by exploiting', (12, 14))]","[('two machine reading corpora', (8, 12)), ('online newspaper articles', (14, 17)), ('matching summaries', (19, 21))]","[['two machine reading corpora', 'by exploiting', 'online newspaper articles'], ['two machine reading corpora', 'by exploiting', 'matching summaries']]",[],"[['Model', 'create', 'two machine reading corpora']]",[],natural_language_inference,7,33
3129,experiments,We tune the maximum penalty per word ( m = 8 ) on the validation data .,"[('tune', (1, 2)), ('on', (12, 13))]","[('maximum penalty per word ( m = 8 )', (3, 12)), ('validation data', (14, 16))]","[['maximum penalty per word ( m = 8 )', 'on', 'validation data']]",[],[],[],natural_language_inference,7,95
3130,results,Word distance benchmark,[],[],[],[],[],[],natural_language_inference,7,156
3131,research-problem,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention,[],"[('Natural Language Inference', (1, 4))]",[],[],[],[],natural_language_inference,8,2
3132,research-problem,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .","[('proposed', (5, 6)), ('for recognizing', (12, 14))]","[('sentence encoding - based model', (7, 12)), ('text entailment', (14, 16))]","[['sentence encoding - based model', 'for recognizing', 'text entailment']]",[],"[['Research problem', 'proposed', 'sentence encoding - based model']]",[],natural_language_inference,8,4
3133,research-problem,"With less number of parameters , our model outperformed the existing best sentence encoding - based approach by a large margin .","[('With', (0, 1)), ('by', (17, 18))]","[('less number of parameters', (1, 5)), ('our model', (6, 8)), ('outperformed', (8, 9)), ('existing best sentence encoding - based approach', (10, 17)), ('large margin', (19, 21))]","[['existing best sentence encoding - based approach', 'by', 'large margin']]","[['less number of parameters', 'has', 'our model'], ['our model', 'has', 'outperformed'], ['outperformed', 'has', 'existing best sentence encoding - based approach']]","[['Research problem', 'With', 'less number of parameters']]",[],natural_language_inference,8,11
3134,research-problem,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",[],"[('recognizing text entailment ( RTE )', (9, 15))]",[],[],[],[],natural_language_inference,8,13
3135,approach,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .","[('proposed', (5, 6)), ('for recognizing', (11, 13))]","[('unified deep learning framework', (7, 11)), ('textual entailment', (13, 15))]","[['unified deep learning framework', 'for recognizing', 'textual entailment']]",[],"[['Approach', 'proposed', 'unified deep learning framework']]",[],natural_language_inference,8,30
3136,approach,The basic model is based on building biL - STM models on both premises and hypothesis .,"[('based on', (4, 6))]","[('building biL - STM models', (6, 11))]",[],[],"[['Approach', 'based on', 'building biL - STM models']]",[],natural_language_inference,8,31
3137,hyperparameters,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .","[('is', (6, 7)), ('use', (14, 15)), ('with', (17, 18)), ('for', (27, 28))]","[('training objective', (1, 3)), ('cross - entropy loss', (7, 11)), ('minibatch SGD', (15, 17)), ('Rmsprop', (19, 20)), ('optimization', (28, 29))]","[['training objective', 'use', 'minibatch SGD'], ['minibatch SGD', 'with', 'Rmsprop']]","[['training objective', 'has', 'cross - entropy loss']]",[],"[['Hyperparameters', 'has', 'training objective']]",natural_language_inference,8,77
3138,hyperparameters,The batch size is 128 .,"[('is', (3, 4))]","[('batch size', (1, 3)), ('128', (4, 5))]","[['batch size', 'is', '128']]","[['batch size', 'has', '128']]",[],"[['Hyperparameters', 'has', 'batch size']]",natural_language_inference,8,78
3139,hyperparameters,A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,"[('applied in', (4, 6)), ('of', (8, 9)), ('with', (11, 12)), ('set to', (15, 17))]","[('dropout layer', (1, 3)), ('output', (7, 8)), ('network', (10, 11)), ('dropout rate', (13, 15)), ('0.25', (17, 18))]","[['dropout layer', 'applied in', 'output'], ['output', 'of', 'network'], ['dropout layer', 'with', 'dropout rate'], ['output', 'with', 'dropout rate'], ['network', 'with', 'dropout rate'], ['dropout rate', 'set to', '0.25']]","[['dropout rate', 'has', '0.25']]",[],"[['Hyperparameters', 'has', 'dropout layer']]",natural_language_inference,8,79
3140,hyperparameters,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .","[('used', (5, 6)), ('to initialize', (11, 13))]","[('pretrained 300D Glove 840B vectors', (6, 11)), ('word embedding', (14, 16))]","[['pretrained 300D Glove 840B vectors', 'to initialize', 'word embedding']]",[],"[['Hyperparameters', 'used', 'pretrained 300D Glove 840B vectors']]",[],natural_language_inference,8,80
3141,hyperparameters,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .","[('in', (6, 7)), ('by sampling', (13, 15)), ('from', (17, 18))]","[('Out - of - vocabulary words', (0, 6)), ('training set', (8, 10)), ('randomly initialized', (11, 13)), ('values', (15, 16)), ('uniformly', (16, 17)), ('( 0.05 , 0.05 )', (18, 23))]","[['Out - of - vocabulary words', 'in', 'training set'], ['randomly initialized', 'by sampling', 'values'], ['values', 'from', '( 0.05 , 0.05 )'], ['uniformly', 'from', '( 0.05 , 0.05 )']]","[['values', 'has', 'uniformly']]",[],"[['Hyperparameters', 'has', 'Out - of - vocabulary words']]",natural_language_inference,8,81
3142,ablation-analysis,"2 . Keep their representation stays close to unseen similar words in inference time , which improved the model 's generation ability .","[('Keep', (2, 3)), ('stays close to', (5, 8)), ('in', (11, 12)), ('improved', (16, 17))]","[('representation', (4, 5)), ('unseen similar words', (8, 11)), ('inference time', (12, 14)), ('model', (18, 19))]","[['representation', 'stays close to', 'unseen similar words'], ['unseen similar words', 'in', 'inference time'], ['inference time', 'improved', 'model']]",[],[],[],natural_language_inference,8,85
3143,code,"This baseline has been submitted to the official NQ leaderboard . Code , preprocessed data and pretrained model are available . https://ai.google.com/research/NaturalQuestions https://github.com/google-research/language/tree/",[],[],[],[],[],[],natural_language_inference,9,6
3144,approach,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .","[('in', (16, 17)), ('into', (33, 34)), ('by using', (37, 39)), ('of', (41, 42)), ('at', (69, 70)), ('to create', (72, 74)), ('at', (90, 91)), ('to predict', (93, 95))]","[('short and long answers', (12, 16)), ('single model', (18, 20)), ('pipeline approach', (24, 26)), ('each document', (31, 33)), ('multiple training instances', (34, 37)), ('overlapping windows', (39, 41)), ('tokens', (42, 43)), ('aggressively downsample', (58, 60)), ('null instances ( i.e. instances without an answer )', (60, 69)), ('training time', (70, 72)), ('balanced training set', (75, 78)), ('training time', (91, 93))]","[['short and long answers', 'in', 'single model'], ['each document', 'into', 'multiple training instances'], ['multiple training instances', 'by using', 'overlapping windows'], ['overlapping windows', 'of', 'tokens'], ['null instances ( i.e. instances without an answer )', 'at', 'training time'], ['null instances ( i.e. instances without an answer )', 'to create', 'balanced training set'], ['training time', 'to create', 'balanced training set'], ['null instances ( i.e. instances without an answer )', 'at', 'training time']]","[['aggressively downsample', 'has', 'null instances ( i.e. instances without an answer )']]",[],[],natural_language_inference,9,17
3145,approach,We refer to our model as BERT joint to emphasize the fact that we are modeling short and long answers in a single model rather than in a pipeline of two models .,"[('as', (5, 6)), ('in', (20, 21)), ('rather than', (24, 26))]","[('BERT joint', (6, 8)), ('single model', (22, 24))]",[],[],"[['Approach', 'as', 'BERT joint']]",[],natural_language_inference,9,18
3146,hyperparameters,We initialized our model from a BERT model already finetuned on SQ u AD 1.1 .,"[('initialized', (1, 2)), ('from', (4, 5)), ('finetuned on', (9, 11))]","[('our model', (2, 4)), ('BERT model', (6, 8)), ('SQ u AD 1.1', (11, 15))]","[['our model', 'from', 'BERT model'], ['our model', 'finetuned on', 'SQ u AD 1.1'], ['BERT model', 'finetuned on', 'SQ u AD 1.1']]",[],"[['Hyperparameters', 'initialized', 'our model']]",[],natural_language_inference,9,54
3147,hyperparameters,"We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .","[('trained', (1, 2)), ('by', (4, 5)), ('with', (11, 12)), ('with', (22, 23)), ('of', (26, 27))]","[('model', (3, 4)), ('minimizing', (5, 6)), ('loss L', (6, 8)), ('Adam optimizer', (13, 15)), ('batch size', (24, 26)), ('8', (27, 28))]","[['model', 'by', 'minimizing'], ['loss L', 'with', 'Adam optimizer'], ['minimizing', 'with', 'batch size'], ['Adam optimizer', 'with', 'batch size'], ['batch size', 'of', '8']]","[['minimizing', 'has', 'loss L']]","[['Hyperparameters', 'trained', 'model']]",[],natural_language_inference,9,56
3148,results,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,"[('for', (3, 4)), ('performs', (5, 6)), ('than', (8, 9))]","[('Our BERT model', (0, 3)), ('NQ', (4, 5)), ('dramatically better', (6, 8)), ('original NQ paper', (14, 17))]","[['Our BERT model', 'for', 'NQ'], ['Our BERT model', 'performs', 'dramatically better'], ['NQ', 'performs', 'dramatically better']]",[],[],"[['Results', 'has', 'Our BERT model']]",natural_language_inference,9,60
