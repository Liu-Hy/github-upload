{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cross-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in the hyperparameter space with W&B sweep\n",
    "import logging\n",
    "from ast import literal_eval as load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import sklearn\n",
    "\n",
    "from simpletransformers.classification import (\n",
    "    ClassificationArgs,\n",
    "    ClassificationModel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "df = pd.read_csv('triples.csv')\n",
    "df = df.rename(columns={'idx': 'indx'})\n",
    "df.insert(loc=0, column='idx', value=np.arange(len(df)))\n",
    "sent_num=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collectible-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(arr):\n",
    "    trip_list=[] # not trip_ls\n",
    "    ls=[]\n",
    "    for i in range(len(arr)):\n",
    "        uni_name=arr[i,1]\n",
    "        uni_name=(uni_name[0].upper()+uni_name[1:]).replace('-',' ')\n",
    "        if arr[i,3]!='[]' and arr[i,4]!='[]':\n",
    "            pre = load(arr[i,3])[0]\n",
    "            np = load(arr[i,4])[0]\n",
    "            if pre[1][0]<np[1][0]:\n",
    "                triple=[uni_name,pre[0],np[0]]\n",
    "                trip_list.append(triple)\n",
    "                \n",
    "                word_ls = arr[i,2].split(' ')\n",
    "                word_ls.insert(pre[1][0], '<<')\n",
    "                word_ls.insert(pre[1][1]+1, '>>')\n",
    "                word_ls.insert(np[1][0]+2, '[[')\n",
    "                word_ls.insert(np[1][1]+3, ']]')\n",
    "                unit = arr[i,1]\n",
    "                unit = (unit[0].upper()+unit[1:]).replace('-',' ')\n",
    "                unit_ls = ['[[']+(unit.split(' '))+[']]']\n",
    "                word_ls = unit_ls+[':']+word_ls\n",
    "                flg=0\n",
    "                if arr[i,7]=='[]':\n",
    "                    trip_ls = []\n",
    "                else:\n",
    "                    trip_ls = load(arr[i,7])\n",
    "                    for trip in trip_ls:\n",
    "                        if trip[1]==pre[0] and trip[2]==np[0]:\n",
    "                            flg=1\n",
    "                            break\n",
    "                ls.append([int(arr[i, 0]), unit, pre[0], np[0], trip_ls, ' '.join(word_ls), flg])\n",
    "    dataframe = pd.DataFrame(ls)\n",
    "    dataframe.columns = ['idx', 'info_unit', 'pre', 'np', 'triples', 'text', 'labels']\n",
    "    return dataframe,trip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "framed-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.values\n",
    "df,trip_list=convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welsh-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.manual_seed = 1\n",
    "model_args.fp16 = False\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.do_lower_case = True  # when using uncased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "missing-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_F1(ref, pred):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olympic-orbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e3ba29427847f2af9f84526547bb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05347118f3c94a079f0cc33cf5e4a32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hl/lib/python3.9/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 0, 'tn': 282, 'fp': 1201, 'fn': 0, 'F1_score': 0, 'eval_loss': 2.5842002955295387}\n"
     ]
    }
   ],
   "source": [
    "# Create a TransformerModel\n",
    "model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"../rel/outputsC/best_model\",\n",
    "    args=model_args,\n",
    ")\n",
    "result, model_outputs, wrong_predictions = model.eval_model(df, F1_score=triple_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brief-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(model_outputs.argmax(axis=1))\n",
    "df['preds']=preds\n",
    "df['cand']=trip_list\n",
    "df.loc[df['preds']==0,'cand']=None\n",
    "data=[]\n",
    "for i in range(sent_num):\n",
    "    temp = list(df[df['idx']==i]['cand'])\n",
    "    temp = [t for t in temp if t]\n",
    "    data.append(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enabling-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[]',\n",
       " \"[['Model', 'introduce', 'recurrent neural network grammars ( RNNGs']]\",\n",
       " \"[['Model', 'give', 'two variants']]\",\n",
       " '[]',\n",
       " \"[['Model', 'present', 'simple importance sampling algorithm']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'show', 'even larger performance gains']]\",\n",
       " \"[['Approach', 'introducing', 'cloze - style training objective']]\",\n",
       " \"[['Experimental setup', 'subsample', 'up to 18B tokens']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'on', 'DGX - 1 machines']]\",\n",
       " \"[['Experimental setup', 'use', 'NCCL2 library']]\",\n",
       " \"[['Experimental setup', 'train', 'models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'investigates', 'general purpose sequence - to - sequence models']]\",\n",
       " \"[['Research problem', 'incorporate', 'sequenceto - sequence model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'data pre-processing']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'using', 'subword information']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'trained on', 'large high - confidence corpus']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'achieve', 'slightly higher score ( 84.8 )']]\",\n",
       " \"[['Results', 'On', 'QTB']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'LSTM generative model ( LM )']]\",\n",
       " \"[['Hyperparameters', 'use', 'actionsynchronous beam search']]\",\n",
       " \"[['Results', 'found', 'higher performance']]\",\n",
       " \"[['Results', 'combining', 'scores']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'novel transition system']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'is', 'regularization hyperparameter (? = 10 ?6 )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'on', 'English test dataset']]\",\n",
       " \"[['Results', 'find that', 'bottom - up parser and the top - down parser']]\",\n",
       " \"[['Results', 'with', 'supervised reranking']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'present', 'neural - net parse reranker']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'Through', 'attention mechanism']]\",\n",
       " \"[['Research problem', 'training', 'grammars']]\",\n",
       " \"[['Model', 'focus on', 'probability distributions']]\",\n",
       " \"[['Model', 'focus on', 'RNNGs']]\",\n",
       " \"[['Model', 'manipulates', 'inductive bias']]\",\n",
       " \"[['Model', 'augment', 'RNNG composition function']]\",\n",
       " \"[['Model', 'parameterizes', 'information']]\",\n",
       " \"[['Model', 'To generate', 'sentence x']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'At', 'each timestep']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'On', 'test data']]\",\n",
       " '[]',\n",
       " \"[['Research problem', 'demonstrate', 'LSTM encoder']]\",\n",
       " \"[['Model', 'introduce', 'parser']]\",\n",
       " \"[['Model', 'uses', 'character LSTM']]\",\n",
       " \"[['Results', 'achieves', 'score']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'see', 'our model']]\",\n",
       " \"[['Results', 'see', 'content - based attention']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'train', 'deep neural network']]\",\n",
       " \"[['Model', 'captures', 'entity - level information']]\",\n",
       " \"[['Model', 'At', 'test time']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'using', 'learning - to - search algorithm']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'initialized', 'our word embeddings']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'set', 'hidden layer sizes']]\",\n",
       " \"[['Hyperparameters', 'To regularize', 'network']]\",\n",
       " \"[['Hyperparameters', 'found', 'pretraining']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'find', 'small number of non-embedding features']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'Using', 'pretrained weights']]\",\n",
       " \"[['Ablation analysis', 'find', 'easy - first approach']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'goal - directed endto - end deep reinforcement learning framework']]\",\n",
       " \"[['Model', 'leverage', 'neural architecture']]\",\n",
       " \"[['Model', 'introduce', 'entropy regularization term']]\",\n",
       " \"[['Model', 'update', 'regularized policy network parameters']]\",\n",
       " \"[['Hyperparameters', 'pretrain', 'our model']]\",\n",
       " \"[['Hyperparameters', 'set', 'number of sampled trajectories N s = 100']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Built on top of', 'model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'introducing', 'context - dependent ELMo embedding']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'explore using', 'two variants of reinforcement learning']]\",\n",
       " \"[['Model', 'modify', 'max-margin coreference objective']]\",\n",
       " \"[['Model', 'is', 'neural mention - ranking model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'find that', 'REINFORCE']]\",\n",
       " \"[['Ablation analysis', 'During', 'training']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'introduce', 'approximation']]\",\n",
       " \"[['Model', 'propose', 'coarseto - fine approach']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'using', 'Glo Ve word embeddings']]\",\n",
       " \"[['Results', 'On', 'development set']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'observe', 'further improvement']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'inspired by', 'mention - ranking model']]\",\n",
       " \"[['Model', 'Given', 'anaphoric sentence']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'initialized', 'weight matrices']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'trained', 'our model']]\",\n",
       " \"[['Hyperparameters', 'clip', 'gradients']]\",\n",
       " \"[['Hyperparameters', 'train for', '10 epochs']]\",\n",
       " \"[['Hyperparameters', 'used', 'l 2 - regularization']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'In terms of', 's@1 score']]\",\n",
       " \"[['Results', 'with', 'HPs']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Contrary to', 'syntactic information']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose to', 'learn']]\",\n",
       " '[]',\n",
       " \"[['Model', 'incorporate', 'mention - ranking style coreference system']]\",\n",
       " \"[['Model', 'train', 'model']]\",\n",
       " \"[['Results', 'on', 'CoNLL English test set']]\",\n",
       " \"[['Results', 'see', 'statistically significant improvement']]\",\n",
       " \"[['Results', 'consider', 'impact']]\",\n",
       " \"[['Results', 'see', 'RNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'present', 'word embedding model']]\",\n",
       " \"[['Model', 'propose', 'cross - sentence encoder']]\",\n",
       " \"[['Model', 'Borrowing', 'idea']]\",\n",
       " \"[['Model', 'With', 'context']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'optimized with', 'Adam algorithm']]\",\n",
       " \"[['Hyperparameters', 'In', 'co-reference prediction']]\",\n",
       " \"[['Results', 'Comparing with', 'baseline model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'show', 'cross - sentence dependency']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'provides', 'entity - level representation']]\",\n",
       " '[]',\n",
       " \"[['Model', 'done by using', 'BERT']]\",\n",
       " \"[['Model', 'first to use', 'BERT']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Removing', 'second - order span - representations']]\",\n",
       " \"[['Results', 'Replacing', 'secondorder span representations']]\",\n",
       " \"[['Results', 'set', 'new state of the art']]\",\n",
       " '[]',\n",
       " \"[['Research problem', 'introduce', 'first end - to - end coreference resolution model']]\",\n",
       " \"[['Model', 'present', 'first state - of - the - art neural coreference resolution model']]\",\n",
       " \"[['Model', 'demonstrate', 'first time']]\",\n",
       " '[]',\n",
       " \"[['Model', 'includes', 'span - ranking model']]\",\n",
       " \"[['Model', 'are', 'vector embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'encode', 'speaker information']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'prune', 'spans']]\",\n",
       " \"[['Hyperparameters', 'use', 'ADAM']]\",\n",
       " \"[['Hyperparameters', 'apply', '0.2 dropout']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'With', 'oracle mentions']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'For', 'spans']]\",\n",
       " '[]',\n",
       " \"[['Research problem', 'apply', 'BERT']]\",\n",
       " \"[['Model', 'present', 'two ways']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'fine tune', 'all models']]\",\n",
       " \"[['Results', 'trained', 'separate models']]\",\n",
       " \"[['Ablation analysis', 'refines', 'span representations']]\",\n",
       " '[]',\n",
       " \"[['Results', 'in', 'GAP dataset']]\",\n",
       " '[]',\n",
       " \"[['Results', 'shows', 'BERT']]\",\n",
       " '[]',\n",
       " \"[['Results', 'shows', 'BERT - base']]\",\n",
       " '[]',\n",
       " \"[['Results', 'observe', 'overlap variant']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'new structured - data encoder']]\",\n",
       " \"[['Model', 'focuses on', 'encoding']]\",\n",
       " \"[['Model', 'model', 'general structure']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'encoder module']]\",\n",
       " \"[['Hyperparameters', 'To fit with', 'small number of record keys']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'dropout']]\",\n",
       " \"[['Hyperparameters', 'trained with', 'batch size']]\",\n",
       " \"[['Hyperparameters', 'trained with', 'Adam optimizer']]\",\n",
       " \"[['Hyperparameters', 'used', 'beam search']]\",\n",
       " \"[['Hyperparameters', 'implemented in', 'Open NMT - py']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'introducing', 'Transformer architecture']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'comparison', 'Puduppully - updt']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'proposed', 'hierarchical encoder']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'focuses on', 'language generators']]\",\n",
       " \"[['Approach', 'present', 'neural ensemble natural language generator']]\",\n",
       " \"[['Experimental setup', 'built', 'our ensemble model']]\",\n",
       " \"[['Experimental setup', 'use', 'bidirectional LSTM encoder']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'experimenting with', 'different beam search parameters']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'show', 'both the LSTM and the CNN models']]\",\n",
       " \"[['Results', 'Testing', 'ensembling approach']]\",\n",
       " \"[['Results', 'observed', 'CNN model']]\",\n",
       " \"[['Results', 'observe', 'hybrid ensemble model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'focus on', 'two generation scenarios']]\",\n",
       " \"[['Research problem', 'generation of', 'multi-sentence descriptions of Knowledge Base ( KB ) entities']]\",\n",
       " \"[['Model', 'rely on', 'recurrent data encoders']]\",\n",
       " \"[['Model', 'explicitly encodes', 'structure']]\",\n",
       " \"[['Model', 'use', 'Graph Convolutional Network ( GCN ; )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'take as input', 'linearised version']]\",\n",
       " \"[['Hyperparameters', 'For', 'WebNLG baseline']]\",\n",
       " \"[['Results', 'obtained', 'best results']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'on', 'impact of the number of layers and the type of skip connections']]\",\n",
       " \"[['Ablation analysis', 'notice', 'importance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'improve', 'informativeness']]\",\n",
       " \"[['Model', 'show', 'pragmatic reasoning']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'explicitly modeling', 'content selection and planning']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'applied', 'dropout']]\",\n",
       " \"[['Hyperparameters', 'trained for', '25 epochs']]\",\n",
       " \"[['Hyperparameters', 'For', 'text decoding']]\",\n",
       " \"[['Hyperparameters', 'set', 'beam size']]\",\n",
       " \"[['Hyperparameters', 'implemented in', 'Open NMT - py']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Compared to', 'best reported system']]\",\n",
       " \"[['Results', 'of', 'oracle system ( NCP + OR )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Compared to', 'full system ( NCP + CC )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'find', 'NCP + CC over all']]\",\n",
       " '[]',\n",
       " \"[['Model', 'given', 'set of RDF triplets']]\",\n",
       " \"[['Model', 'propose', 'explicit , symbolic , text planning stage']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'map', 'DBPedia relations to sequences']]\",\n",
       " \"[['Hyperparameters', 'use', 'Open NMT toolkit']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'present', 'character - level sequence - to - sequence model']]\",\n",
       " '[]',\n",
       " \"[['Model', 'shows', 'two important features']]\",\n",
       " \"[['Experimental setup', 'developed', 'our system']]\",\n",
       " \"[['Experimental setup', 'to have', 'same dimensions']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'minimize', 'negative log - likelihood loss']]\",\n",
       " \"[['Baselines', 'propose', 'new formulation']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'used', 'official code']]\",\n",
       " \"[['Results', 'is', 'our model EDA_CS']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[[\\'Results\\', \\'highlight\\', \"EDA_CS \\'s model \\'s\"]]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'present', 'POS tagging and dependency paring']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'implemented using', 'DYNET v2.0']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'learning character - level word embeddings']]\",\n",
       " \"[['Hyperparameters', 'apply', 'dropout']]\",\n",
       " \"[['Hyperparameters', 'apply', 'word dropout']]\",\n",
       " \"[['Hyperparameters', 'optimize', 'objective loss']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', '100 - dimensional word embeddings']]\",\n",
       " \"[['Hyperparameters', 'fix', 'number of hidden nodes']]\",\n",
       " \"[['Hyperparameters', 'perform', 'minimal grid search']]\",\n",
       " \"[['Hyperparameters', 'fix', 'number of BiLSTM layers']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'centered around', 'BiRNNs']]\",\n",
       " '[]',\n",
       " \"[['Model', 'represent', 'each word']]\",\n",
       " \"[['Model', 'In', 'graphbased parser']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'LSTM variant']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'train', 'parsers']]\",\n",
       " \"[['Results', 'When not using', 'external embeddings']]\",\n",
       " \"[['Results', 'Moving from', 'simple ( 4 features )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'to build', 'firstorder graph - based ( FOG ) ensemble parser']]\",\n",
       " \"[['Model', 'distilling', 'ensemble']]\",\n",
       " \"[['Model', 'derive', 'cost']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'used', 'standard splits']]\",\n",
       " \"[['Hyperparameters', 'For', 'German']]\",\n",
       " \"[['Hyperparameters', 'augmented with', 'pretrained structured - skipgram embeddings']]\",\n",
       " \"[['Hyperparameters', 'For', 'Adam optimizer']]\",\n",
       " \"[['Results', 'training', 'same model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'For', 'three BiLSTM - CRF - based models']]\",\n",
       " \"[['Experimental setup', 'perform', 'grid search']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'For', 'Stanford - NNdep']]\",\n",
       " \"[['Experimental setup', 'For', 'jPTDP']]\",\n",
       " \"[['Experimental setup', 'fix', 'number of BiLSTM layers']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'find', 'six retrained models']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Using', 'character - level word embeddings']]\",\n",
       " '[]',\n",
       " \"[['Results', 'On', 'GENIA']]\",\n",
       " \"[['Results', 'Note', 'pre-trained NNdep and Biaffine models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'for', 'parsers']]\",\n",
       " \"[['Results', 'Among', 'four dependency parsers']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'novel neural network architecture']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'all the parsing models']]\",\n",
       " \"[['Hyperparameters', 'For', 'Chinese , Dutch , English , German and Spanish']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'To reduce', 'effects']]\",\n",
       " \"[['Hyperparameters', 'To mitigate', 'overfitting']]\",\n",
       " \"[['Hyperparameters', 'For', 'BLSTM']]\",\n",
       " \"[['Hyperparameters', 'use', 'embedding dropout']]\",\n",
       " \"[['Results', 'with', 'different decoder inputs']]\",\n",
       " '[]',\n",
       " \"[['Results', 'On', 'LCM and UCM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'On', 'nine languages']]\",\n",
       " \"[['Results', 'On', 'Bulgarian']]\",\n",
       " \"[['Results', 'On', 'Italian and Romanian']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'In', 'transition - based parsing']]\",\n",
       " \"[['Model', 'able to model', 'lexical']]\",\n",
       " \"[['Model', 'combine', 'representational power']]\",\n",
       " \"[['Research problem', 'incorporating', 'unlabeled data']]\",\n",
       " \"[['Model', 'start with', 'basic structure']]\",\n",
       " \"[['Model', 'use', 'activations']]\",\n",
       " \"[['Model', 'generate', 'large quantities']]\",\n",
       " \"[['Model', 'generate', 'large quantities']]\",\n",
       " \"[['Model', 'comes from', 'tri-training']]\",\n",
       " \"[['Results', 'use', 'CRF - based POS tagger']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'process it with', 'Berkeley - Parser']]\",\n",
       " \"[['Hyperparameters', 'randomly using', 'Gaussian distribution']]\",\n",
       " \"[['Hyperparameters', 'used', 'fixed initialization']]\",\n",
       " \"[['Hyperparameters', 'For', 'word embedding matrix E word']]\",\n",
       " \"[['Hyperparameters', 'For', 'words']]\",\n",
       " \"[['Hyperparameters', 'tuned using', 'Section 24']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'Treebank Union setup']]\",\n",
       " \"[['Baselines', 'compare to', 'best dependency parsers']]\",\n",
       " \"[['Results', 'On', 'WSJ and Web tasks']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'adding', 'second hidden layer']]\",\n",
       " \"[['Results', 'For', 'our neural network model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'using', 'structured perceptron']]\",\n",
       " '[]',\n",
       " \"[['Model', 'modify', 'neural graphbased approach']]\",\n",
       " '[]',\n",
       " \"[['Model', 'call', 'deep bilinear attention mechanism']]\",\n",
       " \"[['Model', 'use', '100 - dimensional uncased word vectors']]\",\n",
       " \"[['Model', 'apply', 'dropout']]\",\n",
       " \"[['Model', 'optimize', 'network']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'Coupled with', 'recursive tree composition function']]\",\n",
       " \"[['Model', 'use of', 'novel stack LSTM data structure']]\",\n",
       " \"[['Model', 'At', 'test time']]\",\n",
       " \"[['Model', 'adapt', 'training criterion']]\",\n",
       " \"[['Model', 'interpolating', 'algorithm states']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'demonstrate', 'simple feed - forward networks']]\",\n",
       " \"[['Model', 'use', 'any recurrence']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'provide', 'pre-trained , state - of - the art English dependency parser']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'averaged stochastic gradient descent']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'extract', 'features']]\",\n",
       " \"[['Hyperparameters', 'use', 'single hidden layer']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'compare to', 'sentence compression system']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'extract', 'window 3 tokens']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Using', 'beam search']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'focus on', 'two approaches']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'adopt', 'second approach']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Looking at', 'Ft ( XLM ) results']]\",\n",
       " \"[['Results', 'In', 'sentiment classification task']]\",\n",
       " '[]',\n",
       " \"[['Results', 'In', 'sentiment classification task']]\",\n",
       " '[]',\n",
       " \"[['Results', 'In', 'MLdoc dataset']]\",\n",
       " \"[['Results', 'comparing with', 'best cross - lingual results and monolingual fine - tune baseline']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'conclude that', 't2t']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'proposes', 'Neural Attentive Bagof - Entities ( NABoE ) model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'initialized', 'embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'based on', 'dictionarybased entity detection']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'task - oriented word embedding method']]\",\n",
       " \"[['Model', 'focus on', 'text classification']]\",\n",
       " \"[['Model', 'In', 'joint learning framework']]\",\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'task - oriented word embedding method']]\",\n",
       " \"[['Model', 'introduces', 'function - aware component']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'use', 'text classification task']]\",\n",
       " \"[['Experimental setup', 'regard', 'document embedding']]\",\n",
       " \"[['Experimental setup', 'tokenized', 'corpus']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'tuned from', '0 to 1']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'new graph neural networkbased method']]\",\n",
       " \"[['Model', 'model', 'Graph Convolutional Network ( GCN )']]\",\n",
       " \"[['Model', 'turn', 'text classification problem']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'used', 'Logistic Regression']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'used', 'Logistic Regression']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'Text GCN']]\",\n",
       " \"[['Hyperparameters', 'For', 'pre-trained word embeddings']]\",\n",
       " '[]',\n",
       " \"[['Results', 'note', 'TF - IDF + LR']]\",\n",
       " \"[['Results', 'When', 'pre-trained Glo Ve word embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'of', 'PV - DBOW and PV - DM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'observed', 'Text GCN']]\",\n",
       " \"[['Results', 'note', 'Text GCN']]\",\n",
       " '[]',\n",
       " \"[['Research problem', 'proposes', 'text categorization']]\",\n",
       " \"[['Model', 'call', 'deep pyramid CNN ( DPCNN )']]\",\n",
       " \"[['Model', 'converting', 'discrete text']]\",\n",
       " '[]',\n",
       " \"[['Model', 'use', 'max pooling']]\",\n",
       " \"[['Hyperparameters', 'To minimize', 'log loss']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Making', 'deeper']]\",\n",
       " '[]',\n",
       " \"[['Results', 'On', 'DPCNN']]\",\n",
       " '[]',\n",
       " \"[['Results', 'from', 'large dataset results']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'In', 'convolution layer']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'build on', 'general framework']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'on', 'RCV1']]\",\n",
       " '[]',\n",
       " \"[['Results', 'on', 'RCV1']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Compared with', 'supervised oh - 2 LSTMp']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Increasing', 'dimensionality']]\",\n",
       " \"[['Results', 'adding', 'CNN tv-embeddings']]\",\n",
       " \"[['Results', 'indicate', 'LSTM tv-embeddings and CNN tv-embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'extend', 'adversarial and virtual adversarial training']]\",\n",
       " \"[['Model', 'improves', 'robustness']]\",\n",
       " \"[['Model', 'done by', 'regularizing']]\",\n",
       " \"[['Model', 'define', 'perturbation']]\",\n",
       " \"[['Experimental setup', 'used', 'TensorFlow']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'applied', 'gradient clipping']]\",\n",
       " \"[['Experimental setup', 'To reduce', 'runtime']]\",\n",
       " \"[['Experimental setup', 'For', 'regularization']]\",\n",
       " \"[['Experimental setup', 'For', 'bidirectional LSTM model']]\",\n",
       " \"[['Results', 'on', 'classification performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'applied', 'gradient clipping']]\",\n",
       " '[]',\n",
       " \"[['Results', 'For', 'baseline and random perturbation method']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'see that', 'baseline method']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'introduce', 'C - LSTM']]\",\n",
       " \"[['Model', 'To benefit from', 'advantages']]\",\n",
       " '[]',\n",
       " \"[['Model', 'to learn', 'sequential correlations']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'implement', 'Theano']]\",\n",
       " \"[['Experimental setup', 'To benefit from', 'efficiency of parallel computation']]\",\n",
       " \"[['Experimental setup', 'For', 'text preprocessing']]\",\n",
       " \"[['Experimental setup', 'For', 'SST']]\",\n",
       " \"[['Experimental setup', 'For', 'TREC']]\",\n",
       " \"[['Experimental setup', 'use', 'one convolutional layer']]\",\n",
       " \"[['Experimental setup', 'For', 'filter size']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'exploit', 'different combinations']]\",\n",
       " \"[['Experimental setup', 'choose', 'single convolutional layer']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'For', 'TREC']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'add', 'L2 regularization']]\",\n",
       " \"[['Results', 'For', 'binary classification task']]\",\n",
       " \"[['Results', 'Comparing', 'single CNN and LSTM models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'investigate', 'impact']]\",\n",
       " \"[['Ablation analysis', 'For the case of', 'multiple convolutional layers']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'consider', 'feature extraction and classification']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Going from', 'depth 9 to 17 and 29']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'initialize', 'convolutional layers']]\",\n",
       " \"[['Experimental setup', 'done using', 'Torch']]\",\n",
       " \"[['Experimental setup', 'performed on', 'single NVidia K40 GPU']]\",\n",
       " \"[['Experimental setup', 'use', 'temporal batch norm']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'observe', 'small depth']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'explore treating', 'raw signal']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'proposes', 'Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )']]\",\n",
       " \"[['Model', 'utilizes', 'Bidirectional Long Short - Term Memory Networks ( BLSTM )']]\",\n",
       " '[]',\n",
       " \"[['Model', 'applies', '2D convolution ( BLSTM - 2DCNN )']]\",\n",
       " \"[['Model', 'proposes', 'combined framework']]\",\n",
       " \"[['Model', 'introduces', 'two combined models']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', '100 convolutional filters each']]\",\n",
       " \"[['Hyperparameters', 'set', 'mini-batch size']]\",\n",
       " \"[['Hyperparameters', 'For', 'regularization']]\",\n",
       " \"[['Hyperparameters', 'chosen via', 'grid search']]\",\n",
       " \"[['Hyperparameters', 'tune', 'hyperparameters']]\",\n",
       " \"[['Results', 'implements', 'four models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'As for', 'Subj and MR datasets']]\",\n",
       " \"[['Results', 'Compared with', 'RCNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'found that', 'both BLSTM - 2DPooling and BLSTM - 2DCNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'performed on', 'Nvidia GTX 1080 and RTX 2080 Ti GPUs']]\",\n",
       " \"[['Experimental setup', 'use', 'Scikitlearn 0.19.2']]\",\n",
       " \"[['Experimental setup', 'For', 'HAN']]\",\n",
       " \"[['Experimental setup', 'train', 'XML - CNN']]\",\n",
       " \"[['Experimental setup', 'For', 'KimCNN']]\",\n",
       " \"[['Experimental setup', 'For', 'LSTM reg and LSTM base']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'set', 'default TA exponential smoothing coefficient']]\",\n",
       " \"[['Experimental setup', 'choose', '512 hidden units']]\",\n",
       " \"[['Experimental setup', 'regularize', 'input-hidden and hidden - hidden Bi - LSTM connections']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'use', '300 - dimensional word vectors']]\",\n",
       " \"[['Experimental setup', 'train', 'all neural models']]\",\n",
       " \"[['Results', 'see that', 'our simple LSTM reg model']]\",\n",
       " \"[['Results', 'observe', 'LSTM reg']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'On', 'AAPD']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'training', 'attention - based Transformer network']]\",\n",
       " \"[['Model', 'train', 'large 40 GB text dataset']]\",\n",
       " \"[['Model', 'training', 'language model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'find', 'inclusion']]\",\n",
       " \"[['Results', 'For', 'multihead MLP and the single linear layer']]\",\n",
       " \"[['Results', 'find', 'our models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'compare', 'deep learning architectures']]\",\n",
       " '[]',\n",
       " \"[['Results', 'performed', 'significantly better']]\",\n",
       " \"[['Results', 'Applying', 'SemEval - trained Transformer']]\",\n",
       " \"[['Results', 'Looking at', 'rater agreement by dataset']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'performed on', 'NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU']]\",\n",
       " \"[['Results', 'use of', 'TDSCs']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'use', '300 - dimensional Glo Ve word embeddings']]\",\n",
       " '[]',\n",
       " '[[\\'Results\\', \\'train\\', \"our model \\'s parameters\"]]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'implemented using', 'Tensorflow']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'uses', 'document classification']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'For', 'data set W OS ? 11967']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Testing on', 'combinations']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'interprets', 'parameter matrix']]\",\n",
       " \"[['Model', 'introduce', 'interaction mechanism']]\",\n",
       " \"[['Model', 'From', 'word - level representation']]\",\n",
       " \"[['Model', 'Based upon', 'EXplicit interAction Model ( dubbed']]\",\n",
       " \"[['Model', 'consists of', 'three main components']]\",\n",
       " '[]',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fewer-diameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3149"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mobile-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>triple_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[['Model', 'introduce', 'recurrent neural netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[['Model', 'give', 'two variants']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[['Model', 'present', 'simple importance sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>[['Approach', 'as', 'BERT joint']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>[['Hyperparameters', 'initialized', 'our model']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>[['Hyperparameters', 'trained', 'model']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3149 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               triple_C\n",
       "0                                                    []\n",
       "1     [['Model', 'introduce', 'recurrent neural netw...\n",
       "2                   [['Model', 'give', 'two variants']]\n",
       "3                                                    []\n",
       "4     [['Model', 'present', 'simple importance sampl...\n",
       "...                                                 ...\n",
       "3144                                                 []\n",
       "3145                 [['Approach', 'as', 'BERT joint']]\n",
       "3146  [['Hyperparameters', 'initialized', 'our model']]\n",
       "3147          [['Hyperparameters', 'trained', 'model']]\n",
       "3148                                                 []\n",
       "\n",
       "[3149 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(data,columns=['triple_C'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "serious-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('C.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-union",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
