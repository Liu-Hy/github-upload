2	0	55	Cloze - driven Pretraining of Self - attention Networks
4	30	41	pretraining
11	19	23	show
11	29	58	even larger performance gains
11	95	110	both directions
11	111	113	of
11	116	178	large language - model - inspired self - attention cloze model
13	19	30	introducing
13	33	65	cloze - style training objective
13	66	71	where
13	76	81	model
13	82	94	must predict
13	99	110	center word
13	111	116	given
13	117	180	left - to - right and right - to - left context representations
117	3	12	subsample
117	13	29	up to 18B tokens
128	0	3	CNN
128	11	14	use
128	18	34	adaptive softmax
128	35	37	in
128	42	48	output
128	55	63	headband
128	64	72	contains
128	77	100	60K most frequent types
128	101	105	with
128	128	139	followed by
128	142	152	160 K band
128	153	157	with
128	158	176	dimensionality 256
128	179	183	with
128	186	194	momentum
128	198	202	0.99
128	210	221	renormalize
128	222	231	gradients
128	232	234	if
128	241	245	norm
128	246	253	exceeds
128	254	257	0.1
129	4	17	learning rate
129	21	39	linearly warmed up
129	40	44	from
129	45	56	10 ? 7 to 1
129	57	60	for
129	61	71	16 K steps
129	81	95	annealed using
129	98	127	cosine learning rate schedule
129	128	132	with
129	135	147	single phase
129	148	150	to
129	151	157	0.0001
130	19	21	on
130	22	38	DGX - 1 machines
130	39	43	with
130	44	62	8 NVIDIA V100 GPUs
130	80	97	interconnected by
130	98	108	Infiniband
131	8	11	use
131	16	29	NCCL2 library
131	38	43	torch
133	3	8	train
133	9	15	models
133	16	20	with
133	21	52	16 bit floating point precision
151	15	25	outperform
151	30	72	uni-directional transformer ( OpenAI GPT )
151	88	97	our model
151	101	118	about 50 % larger
151	119	123	than
153	0	18	Our CNN base model
153	19	30	performs as
153	39	45	STILTs
153	46	48	in
153	49	58	aggregate
153	121	129	performs
153	130	134	much
159	0	21	Structured Prediction
