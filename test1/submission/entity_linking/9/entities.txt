2	58	85	Named Entity Disambiguation
4	0	35	Named Entity Disambiguation ( NED )
9	3	12	combining
9	13	21	contexts
9	22	30	based on
9	35	53	proposed embedding
9	54	58	with
9	59	80	standard NED features
9	86	94	achieved
9	95	123	state - of - theart accuracy
9	124	126	of
9	127	133	93.1 %
9	134	136	on
9	141	163	standard CoNLL dataset
9	168	174	85.2 %
9	175	177	on
9	182	198	TAC 2010 dataset
11	0	35	Named Entity Disambiguation ( NED )
20	16	35	designed to capture
20	40	59	semantic similarity
20	60	62	of
20	63	68	words
20	69	73	when
20	74	87	similar words
20	92	103	placed near
20	104	115	one another
20	116	118	in
20	121	162	relatively low - dimensional vector space
21	19	26	propose
21	29	35	method
21	36	48	to construct
21	51	66	novel embedding
21	85	103	words and entities
21	104	108	into
21	113	141	same continuous vector space
26	10	21	consists of
26	49	57	based on
26	62	79	skip - gram model
27	8	38	conventional skip - gram model
27	62	79	neighboring words
27	80	85	given
27	90	101	target word
27	102	104	in
27	105	117	text corpora
27	128	142	KB graph model
27	167	187	neighboring entities
27	188	193	given
27	198	211	target entity
27	212	214	in
27	219	229	link graph
27	230	232	of
27	237	239	KB
27	254	274	anchor context model
27	340	345	using
27	346	353	anchors
27	385	387	KB
28	3	21	jointly optimizing
28	37	47	our method
28	48	69	simultaneously learns
28	74	83	embedding
28	84	86	of
28	87	105	words and entities
29	42	49	develop
29	52	78	straightforward NED method
29	84	92	computes
29	93	105	two contexts
29	106	111	using
29	116	134	proposed embedding
29	137	163	textual context similarity
29	170	179	coherence
32	0	3	Our
32	4	14	NED method
32	15	23	combines
32	30	38	contexts
32	39	43	with
32	44	69	several standard features
32	79	96	prior probability
32	99	104	using
32	105	132	supervised machine learning
40	21	36	Word Similarity
99	3	6	use
99	7	42	stochastic gradient descent ( SGD )
99	43	46	for
99	51	63	optimization
164	20	24	used
164	25	48	learning rate ? = 0.025
164	55	73	linearly decreased
164	83	93	iterations
164	94	96	of
164	101	115	Wikipedia dump
225	18	26	attained
225	27	34	results
225	35	50	comparable with
225	60	95	some state - of - the - art methods
225	117	119	by
225	131	144	base features
226	0	6	Adding
226	7	33	string similarity features
226	34	71	slightly further improved performance
228	0	10	Our method
228	11	23	outperformed
228	24	59	some state - of - the - art methods
228	60	73	without using
228	74	83	coherence
