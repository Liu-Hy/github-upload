2	75	107	Neural Word Sense Disambiguation
4	21	27	tackle
4	32	37	issue
4	45	97	limited quantity of manually sense annotated corpora
4	98	101	for
4	106	110	task
4	111	113	of
4	114	139	word sense disambiguation
4	142	155	by exploiting
4	160	182	semantic relationships
4	183	190	between
4	191	197	senses
4	198	205	such as
4	206	214	synonymy
4	267	283	sense vocabulary
4	284	286	of
4	287	304	Princeton WordNet
4	316	322	reduce
4	327	357	number of different sense tags
4	396	405	all words
4	406	408	of
4	413	429	lexical database
6	32	39	present
6	42	52	WSD system
6	59	68	relies on
6	69	98	pre-trained BERT word vectors
6	108	118	to achieve
6	119	126	results
6	127	131	that
6	132	157	significantly outperforms
6	162	178	state of the art
6	179	181	on
6	182	206	all WSD evaluation tasks
8	0	33	Word Sense Disambiguation ( WSD )
24	15	22	propose
24	23	44	two different methods
24	45	48	for
24	49	57	building
24	63	69	subset
24	77	86	call them
24	87	123	sense vocabulary compression methods
65	25	39	supervised WSD
71	43	50	propose
71	53	59	method
71	60	81	for grouping together
71	82	101	multiple sense tags
71	107	123	refer in fact to
71	128	140	same concept
73	27	49	Vocabulary Compression
132	0	3	For
132	4	8	BERT
132	14	18	used
132	23	34	model named
132	35	56	" bert - largecased "
132	57	59	of
132	64	86	PyTorch implementation
132	97	108	consists of
132	109	116	vectors
132	117	119	of
132	120	134	dimension 1024
132	137	147	trained on
132	148	161	Book s Corpus
132	166	183	English Wikipedia
134	0	3	For
134	8	34	Transformer encoder layers
134	40	44	used
134	49	64	same parameters
134	65	67	as
134	72	86	" base " model
134	100	108	6 layers
134	109	113	with
134	114	131	8 attention heads
134	136	147	hidden size
134	151	155	2048
134	164	171	dropout
134	175	178	0.1
140	3	12	performed
140	13	27	every training
140	28	31	for
140	32	41	20 epochs
147	6	30	" all relations " system
147	37	44	applies
147	45	78	our second vocabulary compression
147	79	86	through
147	87	100	all relations
147	101	103	on
147	108	123	training corpus
148	3	15	trained with
148	16	28	mini-batches
148	29	31	of
148	32	45	100 sentences
148	48	60	truncated to
148	61	69	80 words
148	79	83	used
148	84	88	Adam
148	89	93	with
148	96	109	learning rate
148	110	112	of
148	113	119	0.0001
148	120	122	as
148	127	146	optimization method
149	21	31	trained on
149	32	57	one Nvidia 's Titan X GPU
164	42	53	our systems
164	59	62	use
164	67	95	sense vocabulary compression
164	96	103	through
164	104	116	hypernyms or
164	117	138	through all relations
164	139	145	obtain
164	146	152	scores
164	153	161	that are
164	162	180	overall equivalent
164	181	183	to
164	188	214	systems that do not use it
169	35	44	thanks to
169	49	79	Princeton WordNet Gloss Corpus
169	80	88	added to
169	93	106	training data
169	115	121	use of
169	122	126	BERT
169	127	129	as
169	130	146	input embeddings
169	152	162	outperform
169	163	177	systematically
169	182	198	state of the art
169	199	201	on
169	202	212	every task
180	23	58	additional training corpus ( WNGC )
180	73	83	the use of
180	84	88	BERT
180	89	91	as
180	92	108	input embeddings
180	114	118	have
180	121	133	major impact
180	134	136	on
180	137	148	our results
180	153	160	lead to
180	161	194	scores above the state of the art
181	0	5	Using
181	6	10	BERT
181	11	21	instead of
181	22	36	ELMo or Glo Ve
181	37	45	improves
181	63	68	score
181	69	71	by
181	72	100	approximately 3 and 5 points
181	127	133	adding
181	138	142	WNGC
181	143	145	to
181	150	163	training data
181	164	172	improves
181	176	178	by
181	179	201	approximately 2 points
185	10	17	through
185	22	28	scores
185	29	40	obtained by
185	41	77	invidual models ( without ensemble )
185	87	97	observe on
185	102	121	standard deviations
185	131	160	vocabulary compression method
185	161	168	through
185	169	178	hypernyms
185	179	184	never
185	192	205	significantly
185	210	221	final score
186	14	32	compression method
186	33	40	through
186	41	54	all relations
186	55	63	seems to
186	64	81	negatively impact
186	86	93	results
186	94	96	in
186	97	107	some cases
186	121	134	ELMo or GloVe
