2	43	70	biomedical event extraction
12	42	85	https://github.com/datquocnguyen/BioPosDep.
97	0	3	For
97	8	41	three BiLSTM - CRF - based models
97	44	60	Stanford - NNdep
97	63	68	jPTDP
97	73	92	Stanford - Biaffine
97	99	107	utilizes
97	108	135	pre-trained word embeddings
97	141	147	employ
97	148	188	200 dimensional pre-trained word vectors
101	3	10	perform
101	13	24	grid search
101	25	27	of
101	28	43	hyperparameters
101	44	53	to select
101	58	81	number of BiLSTM layers
101	82	86	from
101	87	96	{ 1 , 2 }
101	105	125	number of LSTM units
101	126	128	in
101	129	139	each layer
101	140	144	from
101	145	176	{ 100 , 150 , 200 , 250 , 300 }
102	0	14	Early stopping
102	18	30	applied when
102	31	57	no performance improvement
102	58	60	on
102	65	80	development set
102	84	98	obtained after
102	99	119	10 contiguous epochs
103	0	3	For
103	4	20	Stanford - NNdep
103	26	32	select
103	37	48	word CutOff
103	49	53	from
103	54	63	{ 1 , 2 }
103	72	83	size of the
103	84	96	hidden layer
103	97	101	from
103	102	145	{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }
103	150	153	fix
103	154	175	other hyperparameters
103	176	180	with
103	187	201	default values
104	0	3	For
104	4	9	jPTDP
104	15	18	use
104	19	56	50 - dimensional character embeddings
104	61	64	fix
104	69	90	initial learning rate
104	91	93	at
104	94	100	0.0005
105	8	11	fix
105	16	39	number of BiLSTM layers
105	40	42	at
105	43	44	2
105	49	55	select
105	60	80	number of LSTM units
105	81	83	in
105	84	94	each layer
105	95	99	from
105	100	131	{ 100 , 150 , 200 , 250 , 300 }
109	0	35	https://github.com/tdozat/Parser-v2
113	0	35	Corpus - level accuracy differences
113	36	38	of
113	39	54	at least 0.17 %
113	55	57	in
113	58	63	GENIA
113	68	74	0.26 %
113	78	83	CRAFT
113	84	91	between
113	92	114	two POS tagging models
118	0	19	POS tagging results
119	16	20	find
119	30	50	six retrained models
119	51	58	produce
119	59	78	competitive results
124	0	12	BiLSTM - CRF
124	13	20	obtains
124	21	31	accuracies
124	32	34	of
124	35	42	98.44 %
124	43	45	on
124	46	54	GE - NIA
124	59	66	97.25 %
124	67	69	on
124	70	75	CRAFT
125	0	5	Using
125	6	39	character - level word embeddings
125	40	56	helps to produce
125	57	68	about 0.5 %
125	73	83	Trained on
125	114	124	accuracies
125	125	128	for
125	133	145	GENIA tagger
125	148	163	Stanford tagger
125	166	172	MarMoT
125	175	186	NLP4J - POS
125	189	200	BiLSTM- CRF
125	231	233	on
125	238	256	benchmark test set
125	257	259	of
125	286	297	reported at
125	298	305	97.05 %
130	0	4	Note
130	10	13	for
130	14	17	PTB
130	20	65	CNN - based character - level word embeddings
130	71	79	provided
130	82	99	0.1 % improvement
130	100	102	to
130	103	115	BiLSTM - CRF
148	0	2	On
148	3	8	GENIA
148	11	16	among
148	17	35	pre-trained models
148	38	43	BLLIP
148	44	51	obtains
148	52	67	highest results
152	0	4	Note
152	14	51	pre-trained NNdep and Biaffine models
152	52	61	result in
152	62	100	no significant performance differences
152	101	116	irrespective of
152	121	127	source
152	128	130	of
152	131	139	POS tags
152	142	146	i.e.
153	0	9	Regarding
153	14	38	retrained parsing models
153	41	43	on
153	49	64	GENIA and CRAFT
153	67	86	Stanford - Biaffine
153	87	95	achieves
159	14	25	all parsers
159	26	33	produce
159	34	48	better results
159	49	52	for
159	53	70	shorter sentences
159	71	73	on
159	74	86	both corpora
159	89	105	longer sentences
159	110	124	likely to have
159	125	144	longer dependencies
237	0	17	Impact of parsing
237	18	20	on
237	21	37	event extraction
238	12	15	for
238	16	23	parsers
238	24	36	trained with
238	41	55	GENIA treebank
238	99	151	http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi
244	0	5	Among
244	10	33	four dependency parsers
244	34	44	trained on
244	45	50	GENIA
244	53	72	Stanford - Biaffine
244	75	80	jPTDP
244	97	104	produce
244	105	136	similar event extraction scores
244	137	139	on
244	144	159	development set
244	168	170	on
244	179	187	test set
244	188	207	jPTDP and NLP4 Jdep
244	208	214	obtain
244	219	244	lowest and highest scores
