2	20	38	Dependency Parsing
4	45	63	dependency parsing
4	73	81	based on
10	30	56	feature representation for
10	57	75	dependency parsing
24	30	45	centered around
24	46	52	BiRNNs
24	77	84	BiLSTMs
24	87	96	which are
24	97	133	strong and trainable sequence models
25	4	10	BiLSTM
25	11	20	excels at
25	21	33	representing
25	34	42	elements
25	43	45	in
25	48	73	sequence ( i.e. , words )
25	94	102	contexts
25	105	114	capturing
25	119	126	element
25	134	153	" infinite " window
26	3	12	represent
26	13	22	each word
26	23	25	by
26	52	55	use
26	58	71	concatenation
26	72	74	of
26	77	88	minimal set
26	89	91	of
26	114	116	as
26	117	137	our feature function
26	154	163	passed to
26	166	222	non-linear scoring function ( multi - layer perceptron )
30	0	2	In
30	7	24	graphbased parser
30	30	43	jointly train
30	46	75	structured - prediction model
30	76	85	on top of
30	88	94	BiLSTM
30	97	108	propagating
30	109	115	errors
30	116	120	from
30	125	145	structured objective
30	170	194	BiLSTM feature - encoder
278	4	11	Chinese
278	17	20	use
278	25	59	Penn Chinese Treebank 5.1 ( CTB5 )
278	62	67	using
278	72	97	train / test / dev splits
278	98	105	of with
278	106	131	gold partof - speech tags
281	4	11	parsers
281	16	30	implemented in
281	31	37	python
281	40	45	using
281	67	70	for
281	71	94	neural network training
282	47	87	https://github.com/elikip / bist -parser
283	3	6	use
283	11	23	LSTM variant
283	24	38	implemented in
283	39	44	PyCNN
283	51	65	optimize using
283	70	84	Adam optimizer
286	3	36	https://github.com/clab/cnn/tree/
288	4	51	word and POS embeddings e ( w i ) and e ( p i )
288	56	70	initialized to
288	71	84	random values
288	89	110	trained together with
288	115	145	rest of the parsers ' networks
295	3	8	train
295	13	20	parsers
295	21	24	for
295	25	44	up to 30 iterations
295	51	57	choose
295	62	72	best model
295	73	85	according to
295	90	102	UAS accuracy
295	103	105	on
295	110	125	development set
300	0	14	When not using
300	15	34	external embeddings
300	41	75	first - order graph - based parser
300	76	80	with
300	81	91	2 features
300	92	103	outperforms
300	104	121	all other systems
300	122	129	thatare
300	130	139	not using
300	140	158	external resources
300	161	170	including
300	175	200	third - order TurboParser
302	0	11	Moving from
302	16	37	simple ( 4 features )
302	38	40	to
302	45	81	extended ( 11 features ) feature set
302	82	90	leads to
302	91	113	some gains in accuracy
302	114	117	for
311	0	23	Dynamic oracle training
311	24	30	yields
311	31	41	nice gains
311	42	45	for
