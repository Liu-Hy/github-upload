2	51	72	Reading Comprehension
4	70	98	Reading Comprehension ( RC )
10	0	53	Building Artificial Intelligence ( AI ) algorithms to
19	31	38	develop
19	41	84	novel context zoom - in network ( ConZNet )
19	85	88	for
19	89	97	RC tasks
19	106	109	can
19	110	122	skip through
19	123	139	irrelevant parts
19	140	142	of
19	145	153	document
19	158	166	generate
19	170	176	answer
19	177	182	using
19	183	216	only the relevant regions of text
20	4	24	ConZNet architecture
20	25	36	consists of
20	37	47	two phases
21	22	30	identify
21	35	59	relevant regions of text
21	60	72	by employing
21	75	107	reinforcement learning algorithm
23	20	28	based on
23	32	62	encoder - decoder architecture
23	71	82	comprehends
23	87	113	identified regions of text
23	118	127	generates
23	132	138	answer
23	139	147	by using
23	150	183	residual self - attention network
23	184	186	as
23	187	194	encoder
23	201	228	RNNbased sequence generator
23	229	239	along with
23	242	257	pointer network
23	258	260	as
23	265	272	decoder
28	11	22	our decoder
28	23	31	combines
28	32	71	span prediction and sequence generation
119	21	28	replace
119	33	54	span prediction layer
119	55	59	with
119	63	86	answer generation layer
120	17	20	use
120	24	25	1
120	33	42	refer for
120	43	85	more details attention based seq2seq layer
120	86	99	without using
120	100	114	copy mechanism
120	115	117	in
120	122	144	answer generation unit
123	3	8	split
123	9	22	each document
123	23	27	into
123	28	37	sentences
123	38	43	using
123	48	66	sentence tokenizer
123	67	69	of
123	74	86	NLTK toolkit
124	15	22	further
124	23	31	tokenize
124	32	45	each sentence
124	82	87	using
124	92	106	word tokenizer
124	107	109	of
124	110	114	NLTK
125	13	30	implemented using
125	31	52	Python and Tensorflow
126	8	15	weights
126	16	18	of
126	23	28	model
126	33	47	initialized by
126	48	69	Glorot Initialization
126	74	80	biases
126	85	101	initialized with
126	102	107	zeros
127	3	6	use
127	9	37	300 dimensional word vectors
127	38	42	from
127	43	48	GloVe
127	51	55	with
127	56	87	840 billion pre-trained vectors
127	90	103	to initialize
127	108	123	word embeddings
127	156	164	training
128	19	35	do not appear in
128	36	41	Glove
128	46	57	initialized
128	61	69	sampling
128	70	74	from
128	77	104	uniform random distribution
129	3	8	apply
129	9	16	dropout
129	17	24	between
129	29	35	layers
129	36	40	with
129	41	57	keep probability
129	58	60	of
129	61	64	0.8
130	4	26	number of hidden units
130	31	37	set to
130	38	41	100
131	3	10	trained
131	11	20	our model
131	21	25	with
131	30	66	AdaDelta ( Zeiler , 2012 ) optimizer
131	67	70	for
131	71	80	50 epochs
131	86	107	initial learning rate
131	111	114	0.1
131	123	137	minibatch size
131	141	143	32
132	4	18	hyperparameter
132	19	67	' sample size ' ( number of relevant sentences )
132	71	77	chosen
132	78	86	based on
132	91	108	model performance
132	109	111	on
132	116	122	devset
133	40	42	on
133	43	55	Narrative QA
134	10	15	noted
134	21	30	our model
134	31	35	with
134	84	95	outperforms
134	100	120	best ROUGE - L score
134	138	140	by
134	141	148	12.62 %
135	4	19	low performance
135	20	22	of
135	34	39	shows
135	49	76	hybrid approach ( ConZNet )
135	77	80	for
135	81	97	generating words
135	98	102	from
135	105	121	fixed vocabulary
135	122	140	as well as copying
135	147	151	from
135	156	164	document
135	168	181	better suited
135	182	186	than
135	187	241	span prediction models ( Seq2Seq , ASR , BiDAF , MRU )
