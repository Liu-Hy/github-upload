2	58	81	CONVERSATIONAL QUESTION
4	0	41	Conversational question answering ( CQA )
5	41	78	machine reading comprehension ( MRC )
5	87	90	CQA
5	100	107	passage
12	12	49	machine reading comprehension ( MRC )
19	19	26	propose
19	27	32	SDNet
19	37	85	contextual attention - based deep neural network
19	86	89	for
19	94	98	task
19	102	135	conversational question answering
22	11	16	SDNet
22	17	26	leverages
22	31	50	latest breakthrough
22	51	53	in
22	54	57	NLP
22	60	85	BERT contextual embedding
107	0	3	For
107	4	12	training
107	18	21	use
107	22	45	all questions / answers
107	46	49	for
107	50	61	one passage
107	62	64	as
107	67	72	batch
122	3	10	compare
122	11	16	SDNet
122	54	92	PGNet ( Seq2 Seq with copy mechanism )
122	95	99	DrQA
122	102	114	DrQA + PGNet
122	117	125	BiDAF ++
128	11	16	SDNet
128	17	25	achieves
128	26	54	significantly better results
128	55	59	than
128	60	75	baseline models
129	16	34	single SDNet model
129	35	43	improves
129	44	55	overall F 1
129	56	58	by
129	59	64	1.6 %
129	67	80	compared with
129	81	112	previous state - of - art model
129	113	115	on
129	116	120	CoQA
130	0	20	Ensemble SDNet model
130	21	37	further improves
130	38	55	overall F 1 score
130	56	58	by
130	59	64	2.7 %
130	104	108	over
130	124	126	on
130	127	147	in - domain datasets
132	10	15	SDNet
132	16	26	overpasses
132	27	54	all but one baseline models
132	55	60	after
132	65	77	second epoch
132	84	92	achieves
132	93	123	state - of - the - art results
132	129	134	after
132	135	143	8 epochs
135	12	16	show
135	22	30	removing
135	31	35	BERT
135	51	60	F 1 score
135	61	63	on
135	64	79	development set
135	80	82	by
135	83	89	7.15 %
136	0	3	Our
136	4	23	proposed weight sum
136	24	26	of
136	27	43	per-layer output
136	44	48	from
136	49	53	BERT
136	54	56	is
136	57	64	crucial
136	73	76	can
136	77	82	boost
136	87	98	performance
136	99	101	by
136	102	108	1.75 %
136	111	130	compared with using
136	131	156	only last layer 's output
139	0	5	Using
139	6	17	BERT - base
139	18	28	instead of
139	29	58	BERT - large pretrained model
139	59	64	hurts
139	69	78	F 1 score
139	79	81	by
139	82	88	2.61 %
140	0	38	Variational dropout and self attention
140	60	71	performance
140	72	74	by
140	75	92	0.24 % and 0.75 %
