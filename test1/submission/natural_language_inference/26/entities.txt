2	10	39	Machine Reading Comprehension
4	0	29	Machine reading comprehension
12	0	37	Machine reading comprehension ( MRC )
60	0	15	Glove embedding
60	63	77	Elmo embedding
60	84	91	used as
60	92	108	basic embeddings
180	3	6	use
180	7	12	Spacy
180	13	23	to process
180	24	49	each question and passage
180	50	59	to obtain
180	60	66	tokens
180	69	77	POS tags
180	80	88	NER tags
180	93	104	lemmas tags
180	105	107	of
180	108	117	each text
181	3	6	use
181	7	20	12 dimensions
181	21	29	to embed
181	30	38	POS tags
181	41	42	8
181	43	46	for
181	47	55	NER tags
182	3	6	use
182	7	24	3 binary features
182	27	38	exact match
182	41	59	lower - case match
182	64	75	lemma match
182	76	83	between
182	88	100	question and
182	101	108	passage
183	3	6	use
183	7	49	100 - dim Glove pretrained word embeddings
183	54	80	1024 - dim Elmo embeddings
184	8	19	LSTM blocks
184	20	23	are
184	24	38	bi-directional
184	39	43	with
184	44	60	one single layer
185	3	6	set
185	11	33	hidden layer dimension
185	34	36	as
185	37	40	125
185	43	68	attention layer dimension
185	72	75	250
186	3	8	added
186	11	24	dropout layer
186	25	32	overall
186	37	52	modeling layers
186	55	64	including
186	69	84	embedding layer
186	87	89	at
186	92	104	dropout rate
186	105	107	of
186	108	111	0.3
187	3	6	use
187	7	21	Adam optimizer
187	22	26	with
187	29	42	learning rate
187	43	45	of
187	46	51	0.002
191	0	9	Our model
191	10	18	achieves
191	22	31	F 1 score
191	35	39	74.0
191	47	55	EM score
191	59	63	70.3
191	71	86	development set
191	96	105	F 1 score
191	121	129	EM score
192	0	9	Our model
192	10	21	outperforms
192	22	53	most of the previous approaches
193	0	12	Comparing to
193	17	42	best - performing systems
193	45	54	our model
193	61	80	simple architecture
193	85	87	is
193	91	111	end - to - end model
194	10	15	among
194	16	45	all the end - to - end models
194	51	58	achieve
194	63	77	best F1 scores
206	8	12	show
206	22	33	performance
206	34	50	dropped slightly
217	0	11	Compared to
217	16	34	bi-attention model
217	41	51	F1 - score
217	52	61	decreases
217	62	67	0.5 %
218	0	17	Multi- task Study
225	34	38	show
225	55	65	large gain
225	66	76	when using
225	81	99	multi - task model
227	0	3	For
227	8	38	answer boundary detection task
227	44	53	find that
227	58	75	multi -task setup
227	149	157	does not
227	158	162	help
227	163	166	its
227	167	178	performance
234	21	30	our model
234	31	39	achieves
234	42	52	good score
234	53	55	in
234	56	70	SQuAD 2.0 test
235	17	30	conclude that
235	31	51	our multi-task model
235	52	57	works
235	58	62	well
235	73	84	performance
235	85	87	of
235	88	118	unanswerability classification
235	119	127	improves
235	128	141	significantly
235	142	146	when
235	151	185	answer pointer and answer verifier
235	186	190	work
235	191	205	simultaneously
245	25	34	threshold
245	38	44	set to
245	45	48	0.5
245	51	59	F1 score
245	60	62	of
245	63	83	answerable questions
245	87	97	similar to
245	106	128	unanswerable questions
246	8	16	increase
246	21	30	threshold
246	96	107	performance
246	108	111	for
246	112	132	answerable questions
246	133	141	degrades
246	148	156	improves
246	161	183	unanswerable questions
248	7	15	see that
248	20	37	overall F 1 score
248	38	40	is
248	41	56	slightly better
248	68	83	consistent with
250	13	16	set
250	21	30	threshold
250	31	36	to be
250	37	40	0.7
250	41	44	for
250	49	66	submission system
250	67	69	to
250	70	86	SQuAD evaluation
