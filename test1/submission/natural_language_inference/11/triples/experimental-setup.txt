(corpus||use||tokenizer)
(tokenizer||from||Stanford CoreNLP)
(Experimental setup||preprocess||corpus)
(Glo Ve word vectors||pretrained on||840B Common Crawl corpus)
(Experimental setup||use||Glo Ve word vectors)
(vocabulary||to||words)
(out - of - vocabulary words||to||zero)
(words||present in||Common Crawl corpus)
(embeddings||for||out - of - vocabulary words)
(out - of - vocabulary words||to||zero)
(vocabulary||has||words)
(Experimental setup||limit||vocabulary)
(Experimental setup||have||randomly initialized parameters)
(Sentinel vectors||are||randomly initialized and optimized)
(randomly initialized and optimized||during||training)
(Sentinel vectors||has||randomly initialized and optimized)
(Experimental setup||has||Sentinel vectors)
(dynamic decoder||set||maximum number of iterations)
(maximum number of iterations||to||4)
(dynamic decoder||use||maxout pool size)
(maxout pool size||of||16)
(dynamic decoder||has||maximum number of iterations)
(maximum number of iterations||has||4)
(maxout pool size||has||16)
(Experimental setup||For||dynamic decoder)
(dropout||to regularize||our network)
(our network||during||training)
(dropout||optimize||model)
(model||using||ADAM)
(Experimental setup||use||dropout)
(Experimental setup||implemented and trained with||Chainer)
(Contribution||has||Experimental setup)
