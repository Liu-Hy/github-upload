19	62	71	introduce
19	74	101	general framework PhaseCond
19	102	116	for the use of
19	117	142	multiple attention layers
23	17	25	leads to
23	28	68	different attention - based architecture
23	69	79	containing
23	80	101	two sequential phases
23	104	149	question - aware passage representation phase
23	154	181	evidence propagation phase.
23	184	188	RNET
23	191	198	MReader
23	205	214	PhaseCond
32	14	21	observe
32	22	47	several meaningful trends
32	54	60	during
32	65	96	questionpassage attention phase
32	99	119	repeatedly attending
32	124	131	passage
32	132	136	with
32	141	169	same question representation
32	172	178	forces
32	181	198	each passage word
32	199	208	to become
32	209	228	increasingly closer
32	236	268	original question representation
32	371	377	during
32	382	404	self - attention phase
32	411	448	self - attention 's alignment weights
32	449	451	of
32	456	468	second layer
32	469	475	become
32	476	498	noticeably " sharper "
32	499	503	than
32	508	519	first layer
104	3	6	use
104	7	55	pre-trained GloVe 100 - dimensional word vectors
104	58	90	parts - of - speech tag features
104	93	119	named - entity tag feature
104	126	141	binary features
104	142	144	of
104	145	159	exact matching
105	20	23	use
105	24	113	question type ( what , how , who , when , which , where , why , be , and other ) features
105	114	119	where
105	120	129	each type
105	133	147	represented by
105	150	169	trainable embedding
106	3	6	use
106	7	10	CNN
106	11	15	with
106	16	45	100 one - dimensional filters
106	46	50	with
106	51	58	width 5
106	59	68	to encode
106	69	94	character level embedding
107	4	15	hidden size
107	19	25	set as
107	26	29	128
107	30	33	for
107	34	53	all the LSTM layers
108	0	7	Dropout
108	12	20	used for
108	21	49	all the learnable parameters
108	50	54	with
108	57	62	ratio
108	63	65	as
108	66	69	0.2
109	3	6	use
109	11	48	Adam optimizer ( Kingma & Ba , 2014 )
109	49	53	with
109	57	78	initial learning rate
109	79	81	of
109	82	88	0.0006
109	114	117	bad
123	0	36	Single Model Ensemble Models Dev Set
126	4	13	EM result
126	14	16	of
126	17	20	our
126	21	47	baseline Iterative Aligner
126	51	61	lower than
126	62	66	RNET
130	0	3	For
130	8	42	question - passage attention phase
130	45	50	using
130	51	63	single layer
130	64	80	does n't degrade
130	85	96	performance
130	97	110	significantly
130	111	115	from
130	120	135	default setting
130	136	138	of
130	139	149	two layers
130	152	164	resulting in
134	12	28	ATTENTION LAYERS
138	12	23	first layer
138	24	26	of
138	31	65	question - passage attention phase
138	70	88	successfully align
138	89	106	question keywords
138	107	111	with
138	116	146	corresponding passage keywords
