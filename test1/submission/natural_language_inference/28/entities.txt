2	0	24	TRACKING THE WORLD STATE
27	18	29	basic rules
27	30	32	of
27	33	66	approximate ( logical ) inference
27	99	111	belonging to
27	116	129	same category
27	130	142	tend to have
27	143	161	similar properties
28	35	39	with
28	42	87	new kind of memory - augmented neural network
28	93	97	uses
28	100	145	distributed memory and processor architecture
28	152	187	Recurrent Entity Network ( EntNet )
127	0	26	SYNTHETIC WORLD MODEL TASK
134	0	3	For
134	8	14	MemN2N
134	20	23	set
134	28	42	number of hops
134	43	51	equal to
134	52	57	T ? 2
134	66	85	embedding dimension
134	89	95	d = 20
137	16	28	trained with
137	29	33	ADAM
137	34	38	with
137	39	61	initial learning rates
137	62	68	set by
137	69	80	grid search
137	81	85	over
137	86	108	{ 0.1 , 0.01 , 0.001 }
137	113	123	divided by
137	124	146	2 every 10,000 updates
139	4	10	MemN2N
139	19	36	worst performance
139	45	53	degrades
139	54	61	quickly
139	62	64	as
139	69	91	length of the sequence
139	92	101	increases
140	4	8	LSTM
140	9	17	performs
140	18	24	better
140	37	42	loses
140	43	51	accuracy
140	52	54	as
140	59	81	length of the sequence
140	82	91	increases
141	18	24	EntNet
141	28	41	able to solve
141	46	50	task
141	51	53	in
141	54	63	all cases
146	3	6	see
146	16	21	model
146	25	40	able to achieve
146	41	57	good performance
146	58	76	several times past
146	77	97	its training horizon
147	0	10	BABI TASKS
155	16	28	trained with
155	29	33	ADAM
155	34	39	using
155	42	55	learning rate
155	56	58	of
155	59	67	? = 0.01
155	80	90	divided by
155	91	108	2 every 25 epochs
155	109	114	until
155	115	125	200 epochs
158	21	30	our model
158	35	67	embedding dimension size d = 100
158	72	87	20 memory slots
160	0	9	Our model
160	13	20	able to
160	21	26	solve
160	27	40	all the tasks
160	43	56	outperforming
160	61	73	other models
160	74	85	in terms of
160	95	117	number of solved tasks
160	126	139	average error
170	13	27	does not store
170	28	57	useful or correct information
170	58	60	in
170	65	77	memory slots
170	78	94	corresponding to
170	95	104	locations
171	0	29	CHILDRE N'S BOOK TEST ( CBT )
181	16	29	trained using
181	30	74	standard stochastic gradient descent ( SGD )
181	75	79	with
181	82	101	fixed learning rate
181	102	104	of
181	105	110	0.001
182	3	7	used
182	8	32	separate input encodings
182	33	36	for
182	41	68	update and gating functions
182	75	82	applied
182	85	97	dropout rate
182	98	100	of
182	101	104	0.5
182	105	107	to
182	112	137	word embedding dimensions
195	4	18	general EntNet
195	19	27	performs
195	28	34	better
195	35	39	than
195	44	66	LSTMs and n-gram model
195	67	69	on
195	74	93	Named Entities Task
195	100	111	lags behind
195	119	136	Common Nouns task
196	4	21	simplified EntNet
196	22	33	outperforms
196	34	64	all other single - pass models
196	90	98	performs
196	99	105	better
196	106	110	than
196	115	129	Memory Network
196	130	148	which does not use
196	153	181	self - supervision heuristic
198	18	35	simplified EntNet
198	39	53	able to obtain
198	54	72	decent performance
198	73	75	is
198	76	87	encouraging
