(batch size||to||32)
(batch size||has||32)
(number of epoch||to||trained)
(NTI - SLSTM||uses||S - LSTM units)
(S - LSTM units||for||non-leaf node function)
(initial learning rate||to||1e - 3)
(regularizer strength||to||3 e - 5)
(regularizer strength||to||3 e - 5)
(model||for||90 epochs)
(initial learning rate||has||1e - 3)
(regularizer strength||has||3 e - 5)
(neural net||regularized by||10 % input dropouts)
(neural net||regularized by||20 % output dropouts)
(initial learning rate||to||3e - 4)
(regularizer strength||to||1 e - 5)
(regularizer strength||to||1 e - 5)
(model||for||40 epochs)
(initial learning rate||has||3e - 4)
(regularizer strength||has||1 e - 5)
(neural net||regularized by||15 % input dropouts)
(neural net||regularized by||15 % output dropouts)
(initial learning rate||to||3e - 4)
(regularizer strength||to||1 e - 5)
(regularizer strength||to||1 e - 5)
(model||for||10 epochs)
(initial learning rate||has||3e - 4)
(regularizer strength||has||1 e - 5)
(neural net||regularized by||10 % input dropouts)
(neural net||regularized by||15 % output dropouts)
(Tree matching NTI - SLSTM - LSTM global attention||first constructs||premise and hypothesis)
(Tree matching NTI - SLSTM - LSTM global attention||computes||matching vector)
(matching vector||by using||global attention and an additional LSTM)
(initial learning rate||to||3e - 4)
(regularizer strength||to||3 e - 5)
(regularizer strength||to||3 e - 5)
(model||for||20 epochs)
(initial learning rate||has||3e - 4)
(regularizer strength||has||3 e - 5)
(neural net||regularized by||20 % input dropouts)
(neural net||regularized by||20 % output dropouts)
(Tree matching NTI - SLSTM - LSTM tree attention||replace||global attention)
(global attention||with||tree attention)
(best score||is||87.3 % accuracy)
(87.3 % accuracy||obtained with||full tree matching NTI model)
(best score||has||87.3 % accuracy)
(NTI - SLSTM||improved||performance)
(performance||of||sequential LSTM encoder)
(sequential LSTM encoder||by||approximately 2 %)
(LSTM||as||leaf node function)
(LSTM||helps in||learning)
(learning||has||better representations)
(NTI - SLSTM - LSTM||is||hybrid model)
(hybrid model||encodes||sequence sequentially)
(sequence sequentially||through||leaf node function)
(hybrid model||hierarchically composes||output representations)
(Our||has||NTI - SLSTM - LSTM)
(NTI - SLSTM - LSTM||has||hybrid model)
(node - by - node attention models||improve||performance)
(node - by - node attention models||has||performance)
(outperform||by||large margin)
(previous best result||by||large margin)
(Deep LSTM and LSTM attention models||has||outperform)
(outperform||has||previous best result)
(large margin||has||nearly 5 - 6 %)
(NASM||improves||result)
(NASM||sets||strong baseline)
(strong baseline||by combining||variational autoencoder)
(variational autoencoder||with||soft attention)
(robust||to||length of the phrases)
(NTI model||has||robust)
(Contribution||has||Experiments)
