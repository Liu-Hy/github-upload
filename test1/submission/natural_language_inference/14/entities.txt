133	0	3	For
133	4	26	each mini-batch update
133	33	39	2 norm
133	40	42	of
133	47	61	whole gradient
133	62	64	of
133	65	79	all parameters
133	83	91	measured
133	92	93	5
133	133	144	scaled down
133	145	152	to have
133	153	160	norm L.
135	3	6	use
135	11	24	learning rate
135	67	77	validation
135	87	100	not decreased
135	101	106	after
135	107	116	one epoch
135	128	141	learning rate
135	145	159	scaled down by
135	162	172	factor 1.5
136	0	8	Training
136	9	24	terminates when
136	29	42	learning rate
136	43	48	drops
136	49	54	below
136	55	61	10 ? 5
137	0	7	Weights
137	12	29	initialized using
137	30	44	N ( 0 , 0.05 )
137	49	59	batch size
137	63	69	set to
137	70	73	128
138	0	2	On
138	7	24	Penn tree dataset
138	30	36	repeat
138	37	50	each training
138	51	59	10 times
138	60	64	with
138	65	97	different random initializations
138	102	106	pick
138	120	144	smallest validation cost
141	0	4	Note
141	14	36	baseline architectures
141	42	58	tuned in to give
141	59	77	optimal perplexity
144	8	12	vary
144	17	47	number of hops and memory size
144	48	50	of
144	51	61	our MemN2N
145	8	12	show
145	17	24	Mem N2N
145	25	33	operates
145	34	43	on memory
145	44	48	with
145	49	62	multiple hops
153	0	5	MemNN
153	12	69	strongly supervised AM + NG + NL Memory Networks approach
155	3	7	uses
155	10	47	max operation ( rather than softmax )
155	48	50	at
155	51	61	each layer
155	71	92	trained directly with
155	93	109	supporting facts
155	112	130	strong supervision
156	3	10	employs
156	11	26	n-gram modeling
156	29	45	nonlinear layers
156	53	76	adaptive number of hops
156	77	80	per
156	81	86	query
161	0	4	LSTM
161	31	44	trained using
161	45	73	question / answer pairs only
161	86	103	weakly supervised
164	0	29	Language Modeling Experiments
174	0	6	To aid
174	7	15	training
174	21	26	apply
174	27	42	ReLU operations
174	43	45	to
174	46	63	half of the units
174	64	66	in
174	67	77	each layer
175	3	6	use
175	7	49	layer - wise ( RNN - like ) weight sharing
175	61	74	query weights
175	75	77	of
175	78	88	each layer
175	89	92	are
175	108	122	output weights
175	123	125	of
175	126	136	each layer
175	137	140	are
