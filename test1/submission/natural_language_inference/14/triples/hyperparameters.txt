(2 norm||of||whole gradient)
(whole gradient||of||all parameters)
(whole gradient||of||all parameters)
(all parameters||measured||5)
(each mini-batch update||has||2 norm)
(Hyperparameters||For||each mini-batch update)
(not decreased||after||one epoch)
(learning rate||scaled down by||factor 1.5)
(learning rate||has||validation)
(validation||has||not decreased)
(Hyperparameters||use||learning rate)
(Training||terminates when||learning rate)
(learning rate||has||drops)
(drops||has||below)
(below||has||10 ? 5)
(Hyperparameters||has||Training)
(Weights||initialized using||N ( 0 , 0.05 ))
(batch size||set to||128)
(batch size||has||128)
(Hyperparameters||has||Weights)
(Penn tree dataset||repeat||each training)
(10 times||with||different random initializations)
(each training||has||10 times)
(Hyperparameters||On||Penn tree dataset)
(baseline architectures||tuned in to give||optimal perplexity)
(Hyperparameters||Note||baseline architectures)
(Contribution||has||Hyperparameters)
