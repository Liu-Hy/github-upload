(spaCy tool||used to||tokenize)
(tokenize||has||all the dataset)
(Experimental setup||has||spaCy tool)
(word embedding||with||300 - dimensional GloVe word vectors)
(Experimental setup||fix||word embedding)
(character encoding||use||concatenation)
(concatenation||of||multi-filter Convolutional Neural Nets)
(multi-filter Convolutional Neural Nets||with||windows 1 , 3 , 5)
(multi-filter Convolutional Neural Nets||with||hidden size)
(character encoding||has||concatenation)
(hidden size||has||50 , 100 , 150)
(Experimental setup||For||character encoding)
(lexicon embeddings||are||d =600 - dimensions)
(lexicon embeddings||has||d =600 - dimensions)
(Experimental setup||has||lexicon embeddings)
(embedding||for||out - of - vocabulary)
(embedding||is||zeroed)
(out - of - vocabulary||is||zeroed)
(Experimental setup||has||embedding)
(hidden size||of||LSTM)
(input size||of||output layer)
(LSTM||in||contextual encoding layer)
(memory generation layer||set to||128)
(input size||of||output layer)
(output layer||is||1024 ( 128 * 2 * 4 ))
(hidden size||has||LSTM)
(Experimental setup||has||hidden size)
(projection size||in||attention layer)
(projection size||set to||256)
(Experimental setup||has||projection size)
(Experimental setup||To speedup||weight normalization)
(dropout rate||is||0.2)
(dropout mask||fixed through||time steps)
(time steps||in||LSTM)
(dropout rate||has||0.2)
(Experimental setup||has||dropout rate)
(mini - batch size||set to||32)
(mini - batch size||has||32)
(Experimental setup||has||mini - batch size)
(Our optimizer||is||Adamax)
(learning rate||initialized as||0.002)
(learning rate||decreased by||0.5)
(0.5||after||each 10 epochs)
(Our optimizer||has||Adamax)
(Experimental setup||has||Our optimizer)
(Contribution||has||Experimental setup)
