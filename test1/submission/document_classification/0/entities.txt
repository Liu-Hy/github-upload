2	18	47	Efficient Text Classification
4	56	75	text classification
8	0	19	Text classification
53	0	18	Sentiment analysis
61	15	21	adding
61	22	40	bigram information
61	41	49	improves
61	54	65	performance
61	66	68	by
61	69	76	1 - 4 %
62	8	20	our accuracy
62	24	44	slightly better than
62	45	71	char - CNN and char - CRNN
62	78	79	a
62	80	94	bit worse than
62	95	100	VDCNN
63	17	25	increase
63	30	47	accuracy slightly
63	48	56	by using
63	57	69	more n-grams
63	72	88	for example with
63	89	97	trigrams
63	104	118	performance on
63	119	124	Sogou
63	125	135	goes up to
63	136	142	97.1 %
72	3	11	focus on
72	12	22	predicting
72	27	31	tags
72	32	44	according to
72	49	66	title and caption
76	4	19	vocabulary size
76	20	22	is
76	23	30	297,141
76	45	64	312,116 unique tags
79	3	11	consider
79	14	40	frequency - based baseline
79	47	55	predicts
79	60	77	most frequent tag
80	8	20	compare with
80	21	54	Tagspace ( Weston et al. , 2014 )
80	68	71	tag
80	111	119	based on
80	124	136	Wsabie model
81	10	24	Tagspace model
81	28	43	described using
81	44	56	convolutions
81	62	70	consider
81	75	89	linear version
81	98	106	achieves
81	107	129	comparable performance
81	134	136	is
81	137	148	much faster
83	0	4	Both
83	12	19	achieve
83	22	41	similar performance
83	42	46	with
83	49	67	small hidden layer
83	74	80	adding
83	81	88	bigrams
83	89	94	gives
83	100	117	significant boost
83	118	120	in
83	121	129	accuracy
84	0	2	At
84	3	12	test time
84	15	23	Tagspace
84	24	40	needs to compute
84	45	51	scores
84	52	55	for
84	56	71	all the classes
84	78	86	makes it
84	87	102	relatively slow
84	111	129	our fast inference
84	130	135	gives
84	138	160	significant speed - up
84	161	165	when
84	170	221	number of classes is large ( more than 300 K here )
