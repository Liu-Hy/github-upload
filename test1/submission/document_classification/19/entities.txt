2	0	19	Text Classification
31	23	31	proposes
31	32	136	Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )
31	137	147	to capture
31	148	156	features
31	157	164	on both
31	169	190	time - step dimension
31	199	223	feature vector dimension
32	9	17	utilizes
32	18	75	Bidirectional Long Short - Term Memory Networks ( BLSTM )
32	76	88	to transform
32	93	97	text
32	98	102	into
32	103	110	vectors
33	9	33	2D max pooling operation
33	37	55	utilized to obtain
33	58	79	fixed - length vector
34	16	23	applies
34	24	56	2D convolution ( BLSTM - 2DCNN )
34	57	67	to capture
34	68	92	more meaningful features
34	93	105	to represent
34	110	120	input text
36	11	19	proposes
36	22	40	combined framework
36	49	57	utilizes
36	58	63	BLSTM
36	64	74	to capture
36	75	108	long - term sentence dependencies
36	115	123	extracts
36	124	132	features
36	136	179	2D convolution and 2D max pooling operation
36	180	183	for
36	184	207	sequence modeling tasks
38	10	20	introduces
38	21	40	two combined models
38	41	58	BLSTM - 2DPooling
38	100	129	six text classification tasks
38	132	141	including
38	142	160	sentiment analysis
38	163	186	question classification
38	189	216	subjectivity classification
38	223	248	newsgroups classification
170	4	13	dimension
170	14	16	of
170	17	32	word embeddings
170	33	35	is
170	36	39	300
170	46	58	hidden units
170	59	61	of
170	62	66	LSTM
170	67	69	is
170	70	73	300
171	3	6	use
171	7	37	100 convolutional filters each
171	38	41	for
171	42	54	window sizes
171	58	67	( 3 , 3 )
171	70	85	2D pooling size
171	89	98	( 2 , 2 )
172	3	6	set
172	11	26	mini-batch size
172	27	29	as
172	30	32	10
172	41	54	learning rate
172	55	57	of
172	58	66	AdaDelta
172	67	69	as
172	74	91	default value 1.0
173	0	3	For
173	4	18	regularization
173	24	30	employ
173	31	48	Dropout operation
173	49	53	with
173	54	66	dropout rate
173	67	69	of
173	70	73	0.5
173	74	77	for
173	82	97	word embeddings
173	100	103	0.2
173	112	123	BLSTM layer
173	128	131	0.4
173	140	157	penultimate layer
173	168	171	use
173	172	183	l 2 penalty
173	184	188	with
173	189	207	coefficient 10 ? 5
173	208	212	over
173	217	227	parameters
174	17	27	chosen via
174	30	41	grid search
174	42	44	on
174	49	72	SST - 1 development set
175	8	12	tune
175	19	34	hyperparameters
175	41	58	more finer tuning
175	61	74	such as using
175	75	108	different numbers of hidden units
175	109	111	of
175	112	122	LSTM layer
175	134	150	wide convolution
175	177	188	performance
178	10	20	implements
178	21	32	four models
178	35	40	BLSTM
178	43	54	BLSTM - Att
178	57	74	BLSTM - 2DPooling
178	81	94	BLSTM - 2DCNN
179	4	23	BLSTM - 2DCNN model
179	24	32	achieves
179	33	54	excellent performance
179	55	57	on
179	58	74	4 out of 6 tasks
180	16	24	achieves
180	25	58	52.4 % and 89.5 % test accuracies
180	59	61	on
180	62	81	SST - 1 and SST - 2
181	0	17	BLSTM - 2DPooling
181	18	26	performs
181	27	32	worse
181	33	37	than
181	42	71	state - of - the - art models
183	0	11	BLSTM - CNN
183	12	17	beats
183	18	31	all baselines
183	32	34	on
183	35	44	SST - 1 ,
183	45	52	SST - 2
183	59	72	TREC datasets
184	0	6	As for
184	7	27	Subj and MR datasets
184	30	43	BLSTM - 2DCNN
184	44	48	gets
184	51	75	second higher accuracies
188	0	13	Compared with
188	14	18	RCNN
188	21	34	BLSTM - 2DCNN
188	35	43	achieves
188	46	63	comparable result
192	0	11	BLSTM-2DCNN
192	18	30	extension of
192	31	48	BLSTM - 2DPooling
192	67	71	show
192	77	84	BLSTM -
192	95	102	capture
192	103	120	more dependencies
192	121	123	in
192	124	128	text
193	0	8	Ada Sent
193	9	17	utilizes
193	20	42	more complicated model
193	43	50	to form
193	53	62	hierarchy
193	63	65	of
193	91	102	outperforms
193	103	116	BLSTM - 2DCNN
193	117	119	on
193	120	140	Subj and MR datasets
198	0	4	DRNN
198	7	37	Deep recursive neural networks
198	38	41	for
198	42	70	compositionality in language
200	0	19	CNN -nonstatic / MC
200	22	51	Convolutional neural networks
200	52	55	for
200	56	79	sentence classification
202	0	13	Molding - CNN
202	16	32	Molding CNNs for
202	33	37	text
202	40	50	non-linear
204	0	5	MVCNN
204	8	48	Multichannel variable - size convolution
204	49	52	for
204	53	76	sentence classification
205	51	70	Text Classification
206	0	7	S- LSTM
206	10	34	Long short - term memory
206	35	39	over
206	40	60	recursive structures
207	0	24	LSTM / BLSTM / Tree-LSTM
209	0	5	LSTMN
209	8	43	Long short - term memory - networks
209	44	47	for
209	48	63	machine reading
217	41	60	Text Classification
221	0	25	Effect of Sentence Length
222	6	16	found that
222	17	57	both BLSTM - 2DPooling and BLSTM - 2DCNN
222	58	68	outperform
222	73	89	other two models
232	4	17	best accuracy
232	18	20	is
232	21	25	52.6
232	26	30	with
232	31	55	2D filter size ( 5 , 5 )
232	60	89	2D max pooling size ( 5 , 5 )
