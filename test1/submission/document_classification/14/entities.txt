2	0	51	Supervised and Semi- Supervised Text Categorization
16	0	2	In
16	7	24	convolution layer
16	29	60	small region of data ( e.g. , a
16	85	87	at
16	88	102	every location
16	106	118	converted to
16	121	143	low-dimensional vector
16	144	148	with
16	149	160	information
16	161	172	relevant to
16	177	187	task being
16	188	197	preserved
16	209	221	loosely term
16	224	233	embedding
17	4	22	embedding function
17	26	38	shared among
17	39	56	all the locations
17	67	82	useful features
19	2	10	document
19	14	28	represented as
19	31	39	sequence
19	40	42	of
19	123	140	convolution layer
19	141	149	converts
19	150	163	small regions
19	164	166	of
19	171	179	document
19	205	207	to
19	208	231	low-dimensional vectors
19	232	234	at
19	235	279	every location ( embedding of text regions )
19	284	297	pooling layer
19	298	308	aggregates
19	313	337	region embedding results
19	338	340	to
19	343	358	document vector
19	359	368	by taking
19	369	401	componentwise maximum or average
19	458	470	linear model
31	18	26	build on
31	31	48	general framework
31	49	51	of
31	87	94	explore
31	97	132	more sophisticated region embedding
31	133	136	via
31	137	170	Long Short - Term Memory ( LSTM )
31	173	192	seeking to overcome
31	197	209	shortcomings
31	218	220	in
31	225	264	supervised and semi-supervised settings
32	0	4	LSTM
32	7	9	is
32	12	36	recurrent neural network
42	8	46	both our LSTM models and one - hot CNN
42	47	66	strongly outperform
42	67	80	other methods
42	81	90	including
42	91	104	previous LSTM
43	4	16	best results
43	21	29	obtained
43	33	42	combining
43	47	119	two types of region embeddings ( LSTM embed - dings and CNN embeddings )
43	120	130	trained on
43	131	145	unlabeled data
46	51	90	http://riejohnson.com/cnn download.html
76	20	39	text categorization
93	0	36	Pooling : simplifying sub - problems
102	13	33	speeding up training
130	7	33	neural network experiments
130	36	46	vocabulary
130	51	61	reduced to
130	66	90	most frequent 30 K words
130	91	93	of
130	98	111	training data
130	112	121	to reduce
130	122	142	computational burden
130	145	156	square loss
130	161	175	minimized with
130	176	183	dropout
130	184	194	applied to
130	199	207	input to
130	212	221	top layer
130	224	231	weights
130	237	251	initialized by
133	40	45	20 NG
133	50	53	for
133	54	122	topic categorization of Reuters news articles and newsgroup messages
135	0	12	Optimization
135	17	26	done with
135	27	30	SGD
135	31	35	with
135	36	61	mini-batch size 50 or 100
135	62	66	with
135	67	75	momentum
135	79	89	optionally
135	90	97	rmsprop
135	98	101	for
135	102	114	acceleration
139	0	9	Comparing
139	40	48	see that
139	49	81	our one - hot bidirectional LSTM
139	82	86	with
139	87	111	pooling ( oh - 2 LSTMp )
139	112	123	outperforms
139	124	156	word - vector LSTM ( wv - LSTM )
139	157	159	on
139	160	176	all the datasets
142	10	21	obtained by
142	22	31	bow - CNN
142	104	108	with
142	109	123	region size 20
142	124	126	on
142	127	131	RCV1
142	138	146	seq -CNN
142	149	153	with
142	158	185	regular concatenation input
142	193	204	region size
143	5	7	on
143	8	38	three out of the four datasets
143	41	53	oh - 2 LSTMp
143	54	65	outperforms
143	66	81	SVM and the CNN
144	10	12	on
144	13	17	RCV1
144	23	36	underperforms
147	8	12	RCV1
147	15	25	n-gram SVM
147	26	28	is
147	29	38	no better
147	39	43	than
147	44	63	bag - of - word SVM
147	95	106	outperforms
147	107	114	seq-CNN
148	10	12	on
148	13	17	RCV1
148	20	33	bags of words
148	34	38	in a
148	39	51	window of 20
148	52	54	at
148	55	69	every location
148	70	73	are
148	74	85	more useful
148	86	90	than
148	91	96	words
148	97	99	in
148	100	112	strict order
150	7	11	LSTM
150	20	33	does not have
150	52	57	words
150	63	67	bags
150	70	78	loses to
150	79	88	bow - CNN
156	20	28	strength
156	29	31	of
156	32	36	LSTM
156	37	45	to embed
156	46	60	larger regions
156	61	78	appears not to be
156	81	96	big contributor
158	10	23	one - hot CNN
158	24	29	works
158	30	45	surprising well
160	4	29	previous best performance
160	30	32	on
160	33	37	20NG
160	38	40	is
160	41	45	15.3
160	73	75	of
160	76	80	DL15
160	83	94	obtained by
160	95	117	pre-training wv - LSTM
160	118	120	of
160	121	131	1024 units
160	132	136	with
160	137	158	labeled training data
161	0	16	Our oh - 2 LSTMp
161	17	25	achieved
161	26	31	13.32
161	43	53	2 % better
175	4	26	obtained tv-embeddings
175	37	47	to produce
175	48	64	additional input
175	65	67	to
175	70	97	supervised region embedding
175	98	100	of
175	101	114	one - hot CNN
175	117	129	resulting in
175	130	145	higher accuracy
205	0	13	Compared with
205	18	41	supervised oh - 2 LSTMp
205	44	74	clear performance improvements
205	80	91	obtained on
205	92	108	all the datasets
210	13	34	pre-trained wv - LSTM
210	35	55	clearly outperformed
210	60	80	supervised wv - LSTM
210	86	100	underperformed
210	105	111	models
210	112	116	with
210	117	137	region tv-embeddings
216	15	27	wv - 2 LSTMp
216	28	33	using
216	38	57	Google News vectors
216	70	79	performed
216	80	97	relatively poorly
217	5	13	word2vec
217	18	30	trained with
217	35	56	domain unlabeled data
217	59	73	better results
217	79	93	observed after
217	97	103	scaled
217	104	116	word vectors
218	11	25	underperformed
218	30	36	models
218	42	64	region tv - embeddings
218	89	93	used
218	98	124	same domain unlabeled data
221	4	8	LSTM
221	21	27	rivals
221	47	50	CNN
221	63	65	on
221	66	77	IMDB / Elec
221	82	95	underperforms
221	99	101	on
221	102	106	RCV1
222	0	10	Increasing
222	15	29	dimensionality
222	30	32	of
222	33	50	LSTM tvembeddings
222	51	55	from
222	56	66	100 to 300
222	67	69	on
222	70	74	RCV1
222	80	86	obtain
222	87	91	8.62
238	14	20	adding
238	25	42	CNN tv-embeddings
238	71	81	error rate
238	82	84	on
238	85	89	IMDB
238	90	103	improved from
238	104	111	6.66 to
238	112	116	5.94
238	181	191	error rate
238	192	194	on
238	195	199	RCV1
238	200	213	improved from
238	214	218	7.71
238	222	226	7.15
239	12	20	indicate
239	42	82	LSTM tv-embeddings and CNN tv-embeddings
239	83	93	complement
239	94	104	each other
239	109	116	improve
239	117	128	performance
239	129	133	when
239	134	142	combined
243	4	27	best supervised results
243	28	30	on
243	31	51	IMDB / Elec of JZ15a
243	75	98	obtained by integrating
243	101	125	document embedding layer
243	126	130	into
243	131	144	one - hot CNN
246	30	43	our new model
246	44	63	further improved it
246	64	66	to
246	67	71	5.94
246	79	81	on
246	82	95	Elec and RCV1
246	98	113	our best models
246	114	122	exceeded
246	127	148	previous best results
