2	52	75	Document Classification
7	15	31	our simple model
7	35	50	able to achieve
7	57	64	results
7	65	72	without
7	73	93	attention mechanisms
25	79	102	document classification
67	27	39	performed on
67	40	76	Nvidia GTX 1080 and RTX 2080 Ti GPUs
67	79	83	with
67	84	97	PyTorch 0.4.1
67	98	100	as
67	105	122	backend framework
68	3	6	use
68	7	25	Scikitlearn 0.19.2
68	26	39	for computing
68	44	60	tf - idf vectors
68	65	77	implementing
68	78	89	LR and SVMs
82	0	3	For
82	4	7	HAN
82	13	16	use
82	19	29	batch size
82	30	32	of
82	33	35	32
82	36	42	across
82	43	59	all the datasets
82	62	66	with
82	69	82	learning rate
82	86	90	0.01
82	107	112	0.001
83	3	8	train
83	9	18	XML - CNN
83	24	30	select
83	33	62	dynamic pooling window length
83	66	71	eight
83	76	89	learning rate
83	93	98	0.001
83	105	124	128 output channels
83	127	131	with
83	132	143	batch sizes
83	147	156	32 and 64
83	157	160	for
83	161	199	single - label and multilabel datasets
84	0	3	For
84	4	10	KimCNN
84	16	19	use
84	22	32	batch size
84	33	35	of
84	36	38	64
84	39	43	with
84	46	59	learning rate
84	60	62	of
84	63	67	0.01
87	0	3	For
87	4	26	LSTM reg and LSTM base
87	32	35	use
87	40	54	Adam optimizer
87	55	59	with
87	62	75	learning rate
87	76	78	of
87	79	83	0.01
87	84	86	on
87	87	94	Reuters
87	99	104	0.001
87	112	132	rest of the datasets
87	135	140	using
87	141	152	batch sizes
87	153	155	of
87	156	165	32 and 64
87	166	169	for
87	170	206	multi-label and single - label tasks
88	4	12	LSTM reg
88	23	28	apply
88	29	54	temporal averaging ( TA )
89	3	6	set
89	11	55	default TA exponential smoothing coefficient
89	56	58	of
89	59	64	? EMA
89	65	67	to
89	68	72	0.99
90	3	9	choose
90	10	26	512 hidden units
90	27	30	for
90	35	51	Bi - LSTM models
90	54	59	whose
90	60	79	max - pooled output
90	83	94	regularized
90	95	100	using
90	103	115	dropout rate
90	116	118	of
90	119	122	0.5
91	8	18	regularize
91	23	77	input-hidden and hidden - hidden Bi - LSTM connections
91	78	83	using
91	84	121	embedding dropout and weight dropping
91	139	143	with
91	144	157	dropout rates
91	158	160	of
91	161	172	0.1 and 0.2
92	8	30	optimization objective
92	36	39	use
92	40	84	crossentropy and binary cross - entropy loss
92	85	88	for
92	89	122	singlelabel and multi-label tasks
93	32	35	use
93	36	66	300 - dimensional word vectors
93	67	81	pre-trained on
93	82	93	Google News
94	3	8	train
94	9	26	all neural models
94	27	30	for
94	31	40	30 epochs
94	41	45	with
94	46	63	five random seeds
94	66	75	reporting
94	80	84	mean
111	3	11	see that
111	12	37	our simple LSTM reg model
111	38	46	achieves
111	47	63	state of the art
111	64	66	on
111	67	83	Reuters and IMDB
111	110	122	establishing
111	123	134	mean scores
111	138	151	87.0 and 52.8
111	152	155	for
111	156	165	F 1 score
111	179	181	on
111	186	195	test sets
111	196	198	of
113	3	10	observe
113	16	24	LSTM reg
113	25	46	consistently improves
113	47	51	upon
113	56	67	performance
113	68	70	of
113	71	80	LSTM base
113	148	162	regularization
113	163	169	yields
113	170	179	increases
113	180	182	of
113	183	201	1.5 and 0.5 points
113	202	205	for
113	206	215	F 1 score
116	8	12	find
116	17	25	accuracy
116	26	28	of
116	29	74	LSTM reg and our reimplemented version of HAN
116	88	93	to be
116	94	117	almost two points lower
116	118	122	than
116	127	140	copied result
116	141	143	of
116	144	147	HAN
117	39	46	surpass
117	51	66	original result
117	67	69	by
117	70	87	nearly two points
117	88	91	for
117	96	108	IMDB dataset
119	20	51	non-neural LR and SVM baselines
119	52	59	perform
119	60	75	remarkably well
120	0	2	On
120	3	10	Reuters
120	31	34	SVM
120	35	40	beats
120	41	62	many neural baselines
120	65	74	including
120	75	104	our non-regularized LSTM base
121	0	2	On
121	3	7	AAPD
121	14	17	SVM
121	25	38	ties or beats
121	43	55	other models
121	58	72	losing only to
121	73	76	SGM
