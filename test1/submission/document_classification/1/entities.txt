2	27	66	CROSS - LINGUAL DOCUMENT CLASSIFICATION
5	23	60	cross - lingual understanding ( XLU )
35	30	37	jointly
35	45	78	both language and domain transfer
37	183	222	cross - lingual document classification
38	19	27	focus on
38	28	42	two approaches
38	43	46	for
38	47	64	domain adaptation
39	4	16	first method
39	20	28	based on
39	29	71	masked language model ( MLM ) pre-training
39	82	87	using
39	88	121	unlabeled target language corpora
46	44	69	self - training technique
46	70	75	to do
46	80	97	domain adaptation
46	98	102	from
46	107	122	source language
46	123	127	into
46	132	147	target language
79	19	24	adopt
79	29	44	second approach
79	45	47	as
79	52	63	basic model
79	70	77	utilize
79	82	91	XLM model
79	94	96	as
79	97	111	our base model
79	129	143	pre-trained by
79	144	187	large - scale parallel and monolingual data
82	0	21	SEMI - SUPERVISED XLU
86	0	15	Masked Language
106	0	11	Alleviating
106	16	40	Train - Test Discrepancy
106	41	43	of
106	48	58	UDA Method
106	59	63	With
106	68	81	UDA algorithm
106	102	115	able to learn
106	139	141	on
106	146	159	target domain
106	179	191	suffers from
106	196	208	train - test
118	27	33	obtain
118	36	50	new classifier
118	51	72	trained only based on
118	77	90	target domain
118	124	153	train - test mismatch problem
147	7	36	sentiment classification task
147	123	134	fine - tune
147	138	141	XLM
147	142	146	with
147	147	155	MLM loss
147	156	159	for
147	160	178	each target domain
152	0	16	Fine-tune ( Ft )
152	19	32	Fine - tuning
152	37	54	pre-trained model
152	55	59	with
152	64	92	source - domain training set
171	0	10	Looking at
171	11	29	Ft ( XLM ) results
171	49	68	without the help of
171	69	83	unlabeled data
171	84	88	from
171	93	106	target domain
171	130	145	substantial gap
171	146	153	between
171	158	175	model performance
171	176	178	of
171	183	206	cross -lingual settings
171	215	236	monolingual baselines
171	244	254	when using
171	255	320	state - of - the - art pre-trained cross -lingual representations
173	0	2	In
173	7	36	sentiment classification task
173	39	44	where
173	49	68	unlabeled data size
173	69	71	is
173	72	78	larger
173	81	123	Ft ( XLM ft ) model usnig MLM pre-training
173	124	145	consistently provides
173	146	165	larger improvements
173	166	179	compared with
173	184	194	UDA method
176	4	18	combination of
176	34	39	as in
176	44	64	UDA ( XLM ft ) model
176	67	91	consistently outperforms
176	92	111	either method alone
178	0	2	In
178	7	36	sentiment classification task
178	42	49	observe
178	54	79	self - training technique
178	80	101	consistently improves
178	102	106	over
178	111	124	teacher model
179	3	9	offers
179	10	22	best results
179	23	25	in
179	26	63	both XLM and XLM ft based classifiers
181	0	2	In
181	7	20	MLdoc dataset
181	23	38	self - training
181	44	52	achieves
181	57	69	best results
183	10	24	comparing with
183	29	94	best cross - lingual results and monolingual fine - tune baseline
183	104	111	able to
183	112	128	completely close
183	133	148	performance gap
183	149	161	by utilizing
183	162	176	unlabeled data
183	177	179	in
183	184	199	target language
184	14	27	our framework
184	28	35	reaches
184	36	70	new state - of - the - art results
184	73	87	improving over
184	88	109	vanilla XLM baselines
184	110	112	by
184	113	117	44 %
186	4	21	experment results
186	22	26	show
186	35	46	lags behind
186	51	76	ones using unlabeled data
186	77	81	from
186	86	99	target domain
194	18	29	performance
194	30	32	of
194	37	42	model
194	43	51	improves
194	52	64	consistently
194	65	69	with
194	70	87	more labeled data
194	88	90	in
194	95	114	monolingual setting
213	22	35	conclude that
213	36	39	t2t
213	40	42	is
213	47	71	best performing approach
213	107	120	target domain
