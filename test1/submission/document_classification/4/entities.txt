2	40	59	Text Classification
15	0	19	Text classification
107	0	14	All approaches
107	19	30	better than
107	31	66	traditional bag - of - words method
108	18	29	outperforms
108	34	64	state - of - the - art methods
108	65	67	on
108	68	88	two largest datasets
108	98	115	Yahoo and DBPedia
119	0	4	LEAM
119	5	17	consistently
119	30	43	other methods
119	44	48	with
119	49	85	different proportion of labeled data
199	9	12	use
199	13	53	300 - dimensional Glo Ve word embeddings
199	54	56	as
199	57	71	initialization
199	72	75	for
199	76	112	word embeddings and label embeddings
199	113	115	in
199	116	125	our model
200	0	35	Out - Of - Vocabulary ( OOV ) words
200	40	56	initialized from
200	59	79	uniform distribution
200	80	84	with
200	85	108	range [ ? 0.01 , 0.01 ]
202	3	8	train
202	9	32	our model 's parameters
202	33	37	with
202	42	56	Adam Optimizer
202	92	113	initial learning rate
202	117	122	0.001
202	131	145	minibatch size
202	149	152	100
203	0	22	Dropout regularization
203	26	37	employed on
203	42	57	final MLP layer
203	60	64	with
203	65	81	dropout rate 0.5
204	13	30	implemented using
204	31	41	Tensorflow
204	49	59	trained on
204	60	72	GPU Titan X.
205	53	87	https://github.com/guoyinwang/LEAM
240	60	65	apply
240	66	70	LEAM
240	71	74	for
240	104	127	medical code prediction
240	128	130	on
240	135	168	Electronic Health Records dataset
250	46	89	multi-label classification of clinical text
250	92	101	including
250	102	141	Condensed Memory Networks ( C - MemNN )
250	144	158	Attentive LSTM
250	163	195	Convolutional Attention ( CAML )
256	0	4	LEAM
256	5	13	provides
256	18	32	best AUC score
256	39	63	better F1 and P@5 values
256	64	68	than
256	69	80	all methods
256	81	87	except
256	88	91	CNN
257	0	3	CNN
257	4	16	consistently
257	33	60	basic Bi - GRU architecture
257	71	99	logistic regression baseline
257	100	108	performs
257	109	114	worse
257	115	119	than
257	120	151	all deep learning architectures
