(two - layers multi-head self - attention||with||two heads)
(Hyperparameters||For||encoder module)
(small number of record keys||in||our dataset)
(embedding size||fixed to||20)
(small number of record keys||has||our dataset)
(Hyperparameters||To fit with||small number of record keys)
(size||of||record value embeddings and hidden layers)
(record value embeddings and hidden layers||of||Transformer encoders)
(size||of||record value embeddings and hidden layers)
(record value embeddings and hidden layers||of||Transformer encoders)
(Transformer encoders||set to||300)
(Hyperparameters||has||size)
(dropout||at||rate 0.5)
(Hyperparameters||use||dropout)
(batch size||of||64)
(Hyperparameters||trained with||batch size)
(initial learning rate||is||0.001)
(initial learning rate||reduced by||half)
(half||every||10 K steps)
(initial learning rate||has||0.001)
(Hyperparameters||trained with||Adam optimizer)
(beam search||with||beam size)
(beam size||of||5)
(beam size||during||inference)
(5||during||inference)
(Hyperparameters||used||beam search)
(Hyperparameters||implemented in||Open NMT - py)
(Contribution||has||Hyperparameters)
