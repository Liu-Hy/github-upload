2	38	72	Structured Data to Text Generation
11	90	109	machine translation
14	16	24	focus on
14	25	49	two generation scenarios
14	50	55	where
14	60	71	source data
14	72	74	is
14	75	91	graph structured
15	11	24	generation of
15	25	86	multi-sentence descriptions of Knowledge Base ( KB ) entities
15	107	113	namely
15	118	129	WebNLG task
23	5	12	rely on
23	13	36	recurrent data encoders
23	37	41	with
23	42	70	memory and gating mechanisms
25	44	62	explicitly encodes
25	63	72	structure
25	80	87	trained
25	88	102	end - to - end
26	16	19	use
26	22	59	Graph Convolutional Network ( GCN ; )
26	60	62	as
26	63	74	our encoder
37	34	78	text generation from graph - structured data
37	79	99	considering as input
37	102	110	directed
37	139	144	where
37	145	146	V
37	147	149	is
37	152	155	set
37	189	196	between
37	197	202	nodes
72	4	10	WebNLG
103	5	18	take as input
103	21	39	linearised version
103	40	42	of
103	47	59	source graph
104	0	3	For
104	8	23	WebNLG baseline
104	29	32	use
104	37	59	linearis ation scripts
115	3	11	obtained
115	16	28	best results
115	29	33	with
115	37	44	encoder
115	45	49	with
115	50	65	four GCN layers
115	66	70	with
115	71	91	residual connections
126	0	52	Encoder ( decoder ) embeddings and hidden dimensions
126	58	64	set to
126	65	68	300
135	4	13	GCN model
135	14	16	is
135	22	33	more stable
135	34	38	than
135	43	51	baseline
135	52	56	with
135	59	77	standard deviation
135	78	80	of
135	81	94	.004 vs . 010
137	4	16	GCN EC model
137	17	28	outperforms
137	29	38	PKUWRITER
137	39	48	that uses
137	52	72	ensemble of 7 models
137	79	114	further reinforcement learning step
137	115	117	by
137	118	134	.047 BLEU points
137	141	150	MELBOURNE
137	151	153	by
137	154	170	.014 BLEU points
138	0	6	GCN EC
138	10	16	behind
138	17	22	ADAPT
138	29	38	relies on
138	39	56	sub-word encoding
139	0	14	SR11 Deep task
162	40	42	on
162	47	110	impact of the number of layers and the type of skip connections
162	111	113	on
162	118	132	WebNLG dataset
163	19	25	notice
163	33	43	importance
163	44	46	of
163	47	63	skip connections
163	64	71	between
163	72	82	GCN layers
164	0	30	Residual and dense connections
164	31	38	lead to
164	39	54	similar results
165	0	17	Dense connections
165	39	46	produce
165	47	53	models
165	54	60	bigger
165	67	89	slightly less accurate
165	92	96	than
165	97	117	residual connections
