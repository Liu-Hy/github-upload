2	41	86	Character - based Data - to - text Generation
4	110	137	natural language generation
16	0	35	Sequence - to - sequence frameworks
16	72	107	natural language generation ( NLG )
23	74	81	present
23	84	132	character - level sequence - to - sequence model
23	133	137	with
23	138	157	attention mechanism
23	163	173	results in
23	176	221	completely neural end - to - end architecture
24	50	66	does not require
24	67	83	delexicalization
24	86	98	tokenization
24	103	114	lowercasing
27	30	35	shows
27	36	58	two important features
27	61	76	with respect to
27	81	110	state - of - art architecture
27	133	164	character - wise copy mechanism
27	167	180	consisting in
27	183	194	soft switch
27	195	202	between
27	203	227	generation and copy mode
27	235	245	disengages
27	250	255	model
27	256	264	to learn
27	265	306	rare and unhelpful self - correspondences
27	322	349	peculiar training procedure
27	358	366	improves
27	371	407	internal representation capabilities
27	410	419	enhancing
27	420	426	recall
105	3	12	developed
105	13	23	our system
105	24	29	using
105	34	51	PyTorch framework
106	110	117	to have
106	122	137	same dimensions
106	140	151	in terms of
106	152	162	input size
106	165	176	hidden size
106	179	195	number of layers
106	200	211	presence of
106	214	223	bias term
107	21	31	have to be
107	32	45	bidirectional
108	3	11	minimize
108	16	46	negative log - likelihood loss
108	47	52	using
108	53	77	teacher forcing and Adam
108	115	123	computes
108	124	158	individual adaptive learning rates
110	8	15	propose
110	18	33	new formulation
110	34	36	of
110	37	44	P ( c )
110	60	65	model
110	86	95	necessary
110	107	120	copying phase
121	18	22	TGen
121	73	84	integrating
121	87	123	beam search mechanism and a reranker
121	124	128	over
121	133	146	top k outputs
121	158	174	to dis advantage
121	175	185	utterances
121	186	207	that do not verbalize
121	208	227	all the information
121	228	240	contained in
121	245	247	MR
123	3	7	used
123	12	25	official code
123	26	37	provided in
123	42	67	E2E NLG Challenge website
123	68	71	for
123	72	76	TGen
123	86	95	developed
123	96	114	our models and EDA
123	115	117	in
123	118	125	PyTorch
123	128	144	training them on
123	145	156	NVIDIA GPUs
137	27	29	is
137	35	51	our model EDA_CS
137	52	66	always obtains
137	67	87	higher metric values
137	88	103	with respect to
137	104	108	TGen
137	109	111	on
137	116	145	Hotel and Restaurant datasets
137	152	191	three out of five higher metrics values
137	192	194	on
137	199	210	E2E dataset
138	10	24	in the case of
138	25	30	E2E +
138	33	37	TGen
138	38	46	achieves
138	47	86	three out of five higher metrics values
140	37	45	approach
140	56	72	allows to obtain
140	73	91	better performance
140	92	107	with respect to
140	108	116	training
140	117	123	EDA_CS
140	124	126	in
140	131	143	standard way
140	144	146	on
140	151	180	Hotel and Restaurant datasets
140	234	245	outperforms
140	246	252	EDA_CS
140	261	269	one case
141	11	20	EDA_CS TL
141	21	26	shows
141	29	43	bleu increment
141	44	46	of
141	47	60	at least 14 %
141	61	76	with respect to
141	77	90	TGen 's score
141	91	107	when compared to
141	113	142	Hotel and Restaurant datasets
142	14	28	baseline model
142	31	34	EDA
142	40	60	largely outperformed
142	61	63	by
142	64	90	all other examined methods
144	3	12	highlight
144	18	36	EDA_CS 's model 's
144	37	49	good results
144	50	53	are
144	54	62	achieved
