2	0	14	Face Alignment
6	17	24	present
6	25	29	3DDE
6	34	40	robust
6	80	88	based on
6	91	117	coarse - to - fine cascade
6	118	120	of
6	121	130	ensembles
6	131	133	of
6	134	150	regression trees
11	31	36	3 DDE
11	37	45	improves
11	50	72	state - of - the - art
11	73	75	in
11	76	113	300W , COFW , AFLW and WFLW data sets
23	17	24	present
23	29	81	3 DDE ( 3D Deeply - initialized Ensemble ) regressor
23	86	131	robust and efficient face alignment algorithm
23	132	140	based on
23	143	169	coarse - to - fine cascade
23	170	172	of
24	3	5	is
24	8	23	hybrid approach
24	29	37	inherits
24	38	53	good properties
24	54	56	of
24	57	60	ERT
24	63	70	such as
24	75	82	ability
24	95	105	face shape
24	122	135	robustness of
24	136	147	deep models
25	6	20	initialized by
25	21	37	robustly fitting
25	40	54	3 D face model
25	55	57	to
25	62	78	probability maps
25	79	90	produced by
25	93	96	CNN
26	28	34	tackle
26	61	63	of
26	64	67	ERT
26	81	91	difficulty
26	95	107	initializing
26	112	121	regressor
26	122	140	in the presence of
26	141	176	occlusions and large face rotations
27	24	27	ERT
27	39	46	imposes
27	49	65	prior face shape
27	66	68	on
27	73	81	solution
27	84	94	addressing
27	99	111	shortcomings
27	112	114	of
27	115	126	deep models
27	127	131	when
27	132	176	occlusions and ambiguous face configurations
27	177	180	are
27	181	188	present
28	14	42	coarse - to - fine structure
28	43	50	tackles
28	55	78	combinatorial explosion
28	79	81	of
28	82	99	parts deformation
28	118	132	key limitation
28	133	135	of
28	136	146	approaches
28	147	152	using
28	153	170	shape constraints
31	9	16	improve
31	21	35	initialization
31	36	44	by using
31	47	70	RANSAC - like procedure
31	90	100	robustness
31	101	119	in the presence of
31	120	130	occlusions
207	0	3	For
207	4	17	each data set
207	23	33	train from
207	46	49	CNN
207	50	59	selecting
207	64	80	model parameters
207	81	85	with
207	86	109	lowest validation error
208	3	7	crop
208	8	13	faces
208	14	19	using
208	24	63	ground truth bounding boxes annotations
208	64	75	enlarged by
208	76	80	30 %
209	3	11	generate
209	12	38	different training samples
209	39	41	in
209	42	52	each epoch
209	53	64	by applying
209	65	71	random
209	72	74	in
209	75	90	plane rotations
209	91	98	between
209	99	101	45
209	104	117	scale changes
209	118	120	by
209	121	125	15 %
209	130	142	translations
209	143	145	by
209	146	149	5 %
209	150	152	of
209	153	170	bounding box size
209	173	191	randomly mirroring
209	192	198	images
209	199	211	horizontally
209	216	226	generating
209	227	256	random rectangular occlusions
210	3	6	use
210	7	35	Adam stochastic optimization
210	36	40	with
210	41	89	? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters
211	3	8	train
211	9	14	until
211	15	26	convergence
211	27	31	with
211	35	66	initial learning rate ? = 0.001
212	5	21	validation error
212	22	28	levels
212	29	32	out
212	37	46	10 epochs
212	52	60	multiply
212	65	78	learning rate
212	79	81	by
212	82	87	decay
212	88	89	=
212	90	94	0.05
213	0	2	In
213	7	10	CNN
213	15	33	cropped input face
213	37	49	reduced from
213	50	56	160160
213	57	59	to
213	70	91	gradually dividing by
213	92	107	half their size
213	108	114	across
213	115	129	B = 8 branches
213	130	138	applying
213	139	160	astride 2 convolution
213	161	165	with
213	166	172	kernel
214	3	8	apply
214	9	28	batch normalization
214	29	34	after
214	35	51	each convolution
216	3	8	apply
216	11	26	Gaussian filter
216	27	31	with
216	32	38	? = 33
216	39	41	to
216	46	69	output probability maps
216	70	82	to stabilize
216	87	101	initialization
219	4	12	depth of
219	13	18	trees
219	22	28	set to
219	29	30	4
220	4	19	number of tests
220	20	29	to choose
220	34	55	best split parameters
220	65	71	set to
220	72	75	200
221	3	9	resize
221	10	20	each image
221	21	27	to set
221	32	41	face size
221	42	44	to
221	45	58	160160 pixels
222	0	3	For
222	4	22	feature extraction
222	29	51	FREAK pattern diameter
222	55	62	reduced
222	63	72	gradually
222	73	75	in
222	76	86	each stage
223	3	11	generate
223	12	34	Z = 25 initializations
223	35	37	in
223	42	66	robust soft POSIT scheme
223	67	69	of
223	70	73	g 0
225	0	8	To avoid
225	9	20	overfitting
225	24	27	use
225	30	54	shrinkage factor ? = 0.1
225	59	85	subsampling factor ? = 0.5
225	86	88	in
225	93	96	ERT
227	0	8	Training
227	13	61	CNN and the coarse - to - fine ensemble of trees
227	62	67	takes
227	68	76	48 hours
227	77	82	using
227	85	125	NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU
227	133	164	dual Intel Xeon Silver 4114 CPU
227	165	167	at
227	168	176	2.20 GHz
227	220	224	with
227	227	237	batch size
227	241	243	32
236	28	45	representative of
236	50	82	three main families of solutions
236	89	118	ensembles of regression trees
236	148	172	CNN - based approaches (
236	173	188	LAB , DAN , RCN
236	199	215	mixed approaches
236	216	220	with
236	221	264	deep nets and ensembles of regression trees
236	267	271	3DDE
237	10	15	3 DDE
237	16	18	is
237	19	25	better
237	26	30	than
237	31	34	any
237	41	50	providing
237	53	74	public implementation
239	18	25	able to
239	26	36	improve by
239	39	51	large margin
239	52	69	other ERT methods
239	70	72	as
239	73	77	RCPR
239	80	93	ERT or c GPRT
240	8	18	outperform
240	19	22	RCN
240	25	32	without
247	0	12	Our approach
247	13	20	obtains
247	25	49	best overall performance
247	50	52	in
247	57	83	indoor and outdoor subsets
247	84	86	of
247	91	110	private competition
247	130	141	full subset
247	142	144	of
247	149	169	300W public test set
249	0	2	In
249	7	25	challenging subset
249	26	28	of
249	33	56	300W public competition
249	59	62	SHN
249	63	67	gets
249	68	82	better results
249	83	87	than
249	88	92	3DDE
255	2	5	DDE
255	6	13	obtains
255	18	30	best results
255	33	41	NME 5.11
255	44	56	establishing
266	0	11	In terms of
266	12	42	landmark visibility estimation
266	53	61	obtained
266	62	78	better precision
266	79	83	with
266	87	108	overall better recall
266	109	113	than
266	118	140	best previous approach
266	143	147	DCFE
267	12	26	regularization
267	27	40	together with
267	45	63	new initialization
267	64	78	contributes to
267	79	91	improve DCFE
270	114	117	get
270	121	124	NME
270	125	127	of
270	128	132	2.06
270	133	137	with
270	142	163	full 21 landmarks set
277	0	4	3DDE
277	5	16	outperforms
277	17	32	its competitors
277	33	35	in
277	36	56	all the WFLW subsets
277	57	59	by
277	62	74	large margin
286	0	4	3DDE
286	8	16	based on
286	35	52	3D initialization
286	57	79	cascaded ERT regressor
286	80	92	operating on
286	93	119	probabilistic CNN features
286	126	151	coarse - to - fine scheme
288	6	10	show
288	23	34	obtained by
288	35	59	different configurations
288	60	62	of
288	63	76	our framework
288	82	94	evaluated on
288	95	99	WFLW
296	5	18	combined with
296	23	35	cascaded ERT
296	42	59	3D initialization
296	63	77	key to achieve
296	78	101	top overall performance
296	104	107	see
296	108	138	CNN + MS + DE vs CNN + 3D + DE
300	8	16	provides
300	21	40	largest improvement
300	41	43	in
300	48	59	pose subset
301	4	10	use of
301	11	31	CNN probability maps
301	32	40	improves
301	45	48	NME
301	49	51	in
301	56	69	full data set
301	70	72	in
301	73	83	about 20 %
303	4	31	coarse - to - fine strategy
303	32	34	in
303	35	51	our cascaded ERT
303	52	60	provides
303	61	93	significative local improvements
303	94	96	in
303	97	112	difficult cases
303	115	119	with
303	120	149	rare facial part combinations
322	4	21	smallest database
322	24	28	COFW
322	39	68	worst cross - dataset results
323	24	32	data set
323	33	37	with
323	38	56	greatest diversity
323	74	86	best results
324	15	24	model All
324	27	39	trained with
324	44	75	training sets of all data bases
324	81	88	able to
324	89	96	improve
324	99	101	in
324	102	133	all cross - dataset experiments
324	140	146	models
324	147	157	trained in
324	160	175	single data set
329	4	13	landmarks
329	14	18	with
329	19	30	highest NME
329	31	34	are
329	41	51	related to
329	56	60	ears
329	67	86	bottom of the mouth
329	95	99	chin
