labels,text,predicates,subj/obj,triple_A,triple_B,triple_C,triple_D
research-problem,Recurrent Neural Network Grammars,[],[],[],[],[],[]
model,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .","[('introduce', (5, 6)), ('of', (21, 22)), ('explicitly models', (24, 26)), ('among', (30, 31))]","[('recurrent neural network grammars ( RNNGs', (6, 12)), ('sentences', (22, 23)), ('nested , hierarchical relationships', (26, 30)), ('words and phrases', (31, 34))]",[],[],[],[]
model,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .","[('give', (1, 2)), ('of', (4, 5)), ('one for', (8, 10))]","[('two variants', (2, 4)), ('algorithm', (6, 7)), ('parsing', (10, 11)), ('generation', (27, 28))]",[],[],[],[]
model,"Similar to previously published discriminative bottomup transition - based parsers , greedy prediction with our model yields a linear -","[('with', (13, 14)), ('yields', (16, 17))]","[('greedy prediction', (11, 13)), ('our', (14, 15))]",[],[],[],[]
model,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,"[('present', (1, 2)), ('which uses', (7, 9)), ('from', (10, 11)), ('to solve', (14, 16)), ('in', (18, 19))]","[('simple importance sampling algorithm', (3, 7)), ('samples', (9, 10)), ('discriminative parser', (12, 14)), ('inference problems', (16, 18)), ('generative model', (20, 22))]",[],[],[],[]
research-problem,Transition Sequences from Trees,[],[],[],[],[],[]
hyperparameters,For training we used stochastic gradient descent with a learning rate of 0.1 .,"[('used', (3, 4)), ('with', (7, 8)), ('of', (11, 12))]","[('training', (1, 2)), ('stochastic gradient descent', (4, 7)), ('learning rate', (9, 11)), ('0.1', (12, 13))]",[],[],[],[]
results,"Despite slightly higher language modeling perplexity on PTB 23 , the fixed RNNG still outperforms a highly optimized sequential LSTM baseline . :",[],"[('fixed RNNG', (11, 13)), ('outperforms', (14, 15)), ('highly optimized sequential LSTM baseline', (16, 21))]",[],[],[],[]
research-problem,Cloze - driven Pretraining of Self - attention Networks,[],[],[],[],[],[]
research-problem,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .,[],"[('pretraining', (6, 7))]",[],[],[],[]
approach,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .","[('show', (5, 6)), ('of', (18, 19))]","[('even larger performance gains', (7, 11)), ('both directions', (16, 18)), ('large language - model - inspired self - attention cloze model', (20, 31))]",[],[],[],[]
approach,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,"[('introducing', (4, 5)), ('where', (11, 12)), ('must predict', (14, 16)), ('given', (19, 20))]","[('cloze - style training objective', (6, 11)), ('model', (13, 14)), ('center word', (17, 19)), ('left - to - right and right - to - left context representations', (20, 33))]",[],[],[],[]
experimental-setup,We subsample up to 18B tokens .,"[('subsample', (1, 2))]","[('up to 18B tokens', (2, 6))]",[],[],[],[]
experimental-setup,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .","[('use', (2, 3)), ('in', (6, 7)), ('contains', (12, 13)), ('with', (18, 19)), ('followed by', (22, 24)), ('with', (28, 29)), ('with', (32, 33)), ('if', (41, 42)), ('exceeds', (44, 45))]","[('CNN', (0, 1)), ('adaptive softmax', (4, 6)), ('output', (8, 9)), ('headband', (11, 12)), ('60K most frequent types', (14, 18)), ('160 K band', (25, 28)), ('dimensionality 256', (29, 31)), ('momentum', (34, 35)), ('0.99', (36, 37)), ('renormalize', (39, 40)), ('gradients', (40, 41)), ('norm', (43, 44)), ('0.1', (45, 46))]",[],[],[],[]
experimental-setup,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,"[('from', (7, 8)), ('for', (13, 14)), ('annealed using', (19, 21)), ('with', (26, 27)), ('to', (30, 31))]","[('learning rate', (1, 3)), ('linearly warmed up', (4, 7)), ('10 ? 7 to 1', (8, 13)), ('16 K steps', (14, 17)), ('cosine learning rate schedule', (22, 26)), ('single phase', (28, 30)), ('0.0001', (31, 32))]",[],[],[],[]
experimental-setup,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,"[('on', (3, 4)), ('with', (8, 9)), ('interconnected by', (16, 18))]","[('DGX - 1 machines', (4, 8)), ('8 NVIDIA V100 GPUs', (9, 13)), ('Infiniband', (18, 19))]",[],[],[],[]
experimental-setup,We also use the NCCL2 library and the torch .,"[('use', (2, 3))]","[('NCCL2 library', (4, 6)), ('torch', (8, 9))]",[],[],[],[]
experimental-setup,"We train models with 16 bit floating point precision , following .","[('train', (1, 2)), ('with', (3, 4))]","[('models', (2, 3)), ('16 bit floating point precision', (4, 9))]",[],[],[],[]
results,"All our models outperform the uni-directional transformer ( OpenAI GPT ) of , however , our model is about 50 % larger than their model .","[('than', (22, 23))]","[('outperform', (3, 4)), ('uni-directional transformer ( OpenAI GPT )', (5, 11)), ('our model', (15, 17)), ('about 50 % larger', (18, 22))]",[],[],[],[]
results,"Our CNN base model performs as well as STILTs in aggregate , however , on some tasks involving sentence - pairs , STILTs performs much better ( MRPC , RTE ) ; there is a similar trend for BERT .","[('performs as', (4, 6)), ('in', (9, 10)), ('performs', (23, 24))]","[('Our CNN base model', (0, 4)), ('STILTs', (8, 9)), ('aggregate', (10, 11)), ('much', (24, 25))]",[],[],[],[]
results,Structured Prediction,[],[],[],[],[],[]
research-problem,An Empirical Study of Building a Strong Baseline for Constituency Parsing,[],[],[],[],[],[]
research-problem,This paper investigates the construction of a strong baseline based on general purpose sequence - to - sequence models for constituency parsing .,"[('investigates', (2, 3)), ('based on', (9, 11)), ('for', (19, 20))]","[('general purpose sequence - to - sequence models', (11, 19)), ('constituency parsing', (20, 22))]",[],[],[],[]
research-problem,"We incorporate several techniques that were mainly developed in natural language generation tasks , e.g. , machine translation and summarization , and demonstrate that the sequenceto - sequence model achieves the current top - notch parsers ' performance without requiring explicit task - specific knowledge or architecture of constituent parsing .","[('incorporate', (1, 2)), ('demonstrate', (22, 23))]","[('sequenceto - sequence model', (25, 29))]",[],[],[],[]
research-problem,"Sequence - to - sequence ( Seq2seq ) models have successfully improved many well - studied NLP tasks , especially for natural language generation ( NLG ) tasks , such as machine translation ( MT ) and abstractive summarization .",[],"[('Sequence - to - sequence ( Seq2seq )', (0, 8))]",[],[],[],[]
model,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,"[('of', (14, 15))]","[('Seq2seq approach', (6, 8)), ('constituency parsing', (15, 17))]",[],[],[],[]
research-problem,Constituency Parsing by Seq2seq,[],"[('Constituency Parsing', (0, 2))]",[],[],[],[]
experiments,Model ensemble,[],[],[],[],[],[]
experiments,Ensembling several independently trained models together significantly improves many NLP tasks .,"[('Ensembling', (0, 1))]","[('several independently trained models together', (1, 6)), ('significantly improves', (6, 8))]",[],[],[],[]
experiments,Language model ( LM ) reranking,[],[],[],[],[],[]
hyperparameters,"For data pre-processing , all the parse trees were transformed into linearized forms , which include standard UNK replacement for OOV words and POS - tag normalization by XX - tags .","[('For', (0, 1)), ('transformed into', (9, 11)), ('which', (14, 15)), ('include', (15, 16)), ('for', (19, 20)), ('by', (27, 28))]","[('data pre-processing', (1, 3)), ('all the parse trees', (4, 8)), ('linearized forms', (11, 13)), ('standard UNK replacement', (16, 19)), ('OOV words', (20, 22)), ('POS - tag normalization', (23, 27)), ('XX - tags', (28, 31))]",[],[],[],[]
results,Ensembling and Reranking : shows the results of our models with model ensembling and LM- reranking .,[],"[('Ensembling and Reranking', (0, 3))]",[],[],[],[]
results,( 1 ) Smaller mini-batch size M and gradient clipping G provided the better performance .,"[('provided', (11, 12))]","[('Smaller mini-batch size M and gradient clipping G', (3, 11)), ('better performance', (13, 15))]",[],[],[],[]
ablation-analysis,"( 2 ) Larger layer size , hidden state dimension , and beam size have little impact on the performance ; our setting , L = 2 , H = 200 , and B = 5 looks adequate in terms of speed / performance trade - off .","[('have', (14, 15)), ('on', (17, 18)), ('looks', (36, 37)), ('in terms of', (38, 41))]","[('Larger layer size', (3, 6)), ('little impact', (15, 17)), ('performance', (19, 20)), ('our', (21, 22)), ('adequate', (37, 38)), ('speed / performance trade - off', (41, 47))]",[],[],[],[]
results,( e ) shows the results of utilizing subword splits .,"[('of', (6, 7))]","[('results', (5, 6)), ('utilizing subword splits', (7, 10))]",[],[],[],[]
results,"Thus , using subword information as features is one promising approach for leveraging subword information into constituency parsing .","[('using', (2, 3)), ('as', (5, 6)), ('is', (7, 8)), ('for leveraging', (11, 13)), ('into', (15, 16))]","[('subword information', (3, 5)), ('features', (6, 7)), ('promising approach', (9, 11)), ('subword information', (13, 15)), ('constituency parsing', (16, 18))]",[],[],[],[]
results,Our Seq2seq approach successfully achieved the competitive level as the current top - notch methods : RNNG and its variants .,"[('successfully achieved', (3, 5)), ('as', (8, 9))]","[('Our Seq2seq approach', (0, 3)), ('competitive level', (6, 8)), ('current', (10, 11)), ('RNNG', (16, 17))]",[],[],[],[]
research-problem,Grammar as a Foreign Language,[],[],[],[],[],[]
research-problem,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .,[],"[('Syntactic constituency parsing', (0, 3))]",[],[],[],[]
results,But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,"[('gets to', (5, 7)), ('achieves', (17, 18)), ('matching', (19, 20)), ('on', (25, 26))]","[('single attention model', (2, 5)), ('88.3', (7, 8)), ('ensemble of 5 LSTM', (10, 14)), ('90.5', (18, 19)), ('single - model BerkeleyParser', (21, 25))]",[],[],[],[]
results,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .","[('trained on', (1, 3)), ('achieves', (16, 17))]","[('large high - confidence corpus', (4, 9)), ('single LSTM + A model', (11, 16)), ('92.5', (17, 18)), ('outperforms', (20, 21)), ('best single model', (24, 27)), ('best ensemble result', (31, 34))]",[],[],[],[]
results,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,"[('further improves', (7, 9)), ('to', (11, 12))]","[('ensemble of 5 LSTM+ A models', (1, 7)), ('score', (10, 11)), ('92.8', (12, 13))]",[],[],[],[]
results,"The difference between the F 1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser , 1.7 for the baseline LSTM , and 0.7 for LSTM + A .","[('between', (2, 3)), ('on', (7, 8)), ('of length', (9, 11)), ('upto', (11, 12)), ('upto', (15, 16)), ('is', (17, 18)), ('for', (19, 20))]","[('difference', (1, 2)), ('F 1 score', (4, 7)), ('sentences', (8, 9)), ('30', (12, 13)), ('70', (16, 17)), ('1.3', (18, 19)), ('BerkeleyParser', (21, 22)), ('1.7', (23, 24)), ('baseline LSTM', (26, 28)), ('0.7', (30, 31)), ('LSTM + A', (32, 35))]",[],[],[],[]
results,"Surprisingly , LSTM +A shows less degradation with length than BerkeleyParser - a full O ( n 3 ) chart parser that uses a lot more memory .","[('shows', (4, 5)), ('with', (7, 8)), ('than', (9, 10))]","[('LSTM +A', (2, 4)), ('less degradation', (5, 7)), ('length', (8, 9)), ('BerkeleyParser', (10, 11))]",[],[],[],[]
experiments,"As described in the previous section , we initialized the word - vector embedding with pre-trained word vectors obtained from word2 vec .","[('initialized', (8, 9)), ('with', (14, 15)), ('obtained from', (18, 20))]","[('word - vector embedding', (10, 14)), ('pre-trained word vectors', (15, 18)), ('word2 vec', (20, 22))]",[],[],[],[]
results,LSTM + A trained on the high - confidence corpus ( which only includes text from news ) achieved an F 1 score of 95.7 on QTB and 84.6 on WEB .,"[('trained on', (3, 5)), ('achieved', (18, 19)), ('of', (23, 24)), ('on', (25, 26)), ('on', (29, 30))]","[('LSTM + A', (0, 3)), ('high - confidence corpus', (6, 10)), ('F 1 score', (20, 23)), ('95.7', (24, 25)), ('QTB', (26, 27)), ('84.6', (28, 29)), ('WEB', (30, 31))]",[],[],[],[]
results,Our score on WEB is higher both than the best score reported in ( 83.5 ) and the best score we achieved with an in - house reimplementation of Berkeley Parser trained on human - annotated data ( 84.4 ) .,"[('on', (2, 3)), ('higher both than', (5, 8)), ('achieved', (21, 22)), ('of', (28, 29)), ('trained on', (31, 33))]","[('Our score', (0, 2)), ('WEB', (3, 4)), ('best score', (9, 11)), ('( 83.5 )', (13, 16)), ('best score', (18, 20)), ('in - house reimplementation', (24, 28)), ('Berkeley Parser', (29, 31)), ('human - annotated data ( 84.4 )', (33, 40))]",[],[],[],[]
results,We managed to achieve a slightly higher score ( 84.8 ) with the in - house Berkeley Parser trained on a large corpus .,"[('achieve', (3, 4)), ('with', (11, 12)), ('trained on', (18, 20))]","[('slightly higher score ( 84.8 )', (5, 11)), ('in - house Berkeley Parser', (13, 18)), ('large corpus', (21, 23))]",[],[],[],[]
results,"On QTB , the 95.7 score of LSTM + A is also lower than the best score of our in - house BerkeleyParser ( 96.2 ) .","[('On', (0, 1)), ('of', (6, 7)), ('lower than', (12, 14)), ('of', (17, 18))]","[('QTB', (1, 2)), ('95.7 score', (4, 6)), ('LSTM + A', (7, 10)), ('best score', (15, 17)), ('our', (18, 19)), ('in - house BerkeleyParser ( 96.2 )', (19, 26))]",[],[],[],[]
research-problem,Improving Neural Parsing by Disentangling Model Combination and Reranking Effects,[],"[('Neural Parsing', (1, 3))]",[],[],[],[]
hyperparameters,"For the LSTM generative model ( LM ) , we use the pre-trained model released by Choe and .","[('For', (0, 1)), ('use', (10, 11))]","[('LSTM generative model ( LM )', (2, 8)), ('pre-trained model', (12, 14))]",[],[],[],[]
hyperparameters,We use actionsynchronous beam search ( Section 2.1 ) with beam size K = 100 for RD and word - synchronous beam ( Section 2.2 ) with K w = 100 and K a = 1000 for the generative models RG and LM .,"[('use', (1, 2)), ('with', (9, 10)), ('for', (15, 16)), ('with', (26, 27)), ('for', (36, 37))]","[('actionsynchronous beam search', (2, 5)), ('beam size K = 100', (10, 15)), ('RD', (16, 17)), ('word - synchronous beam', (18, 22)), ('K w', (27, 29)), ('generative models', (38, 40)), ('RG and LM', (40, 43))]",[],[],[],[]
results,"In comparison , we found higher performance for the LM model when using a candidate list from the RD parser : 93.66 F1 versus 92.79 F1 on the development data .","[('found', (4, 5)), ('for', (7, 8)), ('when using', (11, 13)), ('from', (16, 17)), ('versus', (23, 24)), ('on', (26, 27))]","[('higher performance', (5, 7)), ('LM model', (9, 11)), ('candidate list', (14, 16)), ('RD parser', (18, 20)), ('93.66 F1', (21, 23)), ('92.79 F1', (24, 26)), ('development data', (28, 30))]",[],[],[],[]
results,"We find that combining the scores of both models improves on using the score of either model alone , regardless of the source of candidates .","[('combining', (3, 4)), ('of', (6, 7)), ('of', (14, 15))]","[('scores', (5, 6)), ('both models', (7, 9)), ('improves', (9, 10)), ('using', (11, 12)), ('score', (13, 14)), ('either model alone', (15, 18))]",[],[],[],[]
results,Score combination also more than compensates for the decrease in performance we saw previously when adding in candidates from the generative model : RD ?,"[('more than compensates for', (3, 7)), ('when', (14, 15)), ('adding in', (15, 17)), ('from', (18, 19))]","[('Score combination', (0, 2)), ('decrease in', (8, 10)), ('performance', (10, 11)), ('candidates', (17, 18)), ('generative model', (20, 22)), ('RD', (23, 24))]",[],[],[],[]
experiments,"The same trends we observed on the development data , on which the interpolation parameters were tuned , hold here : score combination improves results for all models ( row 3 vs. row 2 ; row 6 vs. row 5 ) , with candidate augmentation from the generative models giving a further increase ( rows 4 and 7 ) .","[('improves', (23, 24)), ('for', (25, 26)), ('with', (42, 43)), ('from', (45, 46))]","[('score combination', (21, 23)), ('results', (24, 25)), ('all models', (26, 28)), ('candidate augmentation', (43, 45)), ('generative models', (47, 49)), ('further increase', (51, 53))]",[],[],[],[]
experiments,"As in the PTB training data setting , using all models for candidates and score combinations is best , achieving 94.66 F1 ( row 9 ) .","[('using', (8, 9)), ('for', (11, 12)), ('is', (16, 17)), ('achieving', (19, 20))]","[('PTB training data setting', (3, 7)), ('all models', (9, 11)), ('candidates and score combinations', (12, 16)), ('best', (17, 18)), ('94.66 F1', (20, 22))]",[],[],[],[]
experiments,"Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .","[('using', (2, 3)), ('with', (20, 21)), ('of', (23, 24))]","[('Performance', (0, 1)), ('only the ensembled RD models', (3, 8)), ('lower', (13, 14)), ('rescoring', (15, 16)), ('single RD model', (17, 20)), ('score combinations', (21, 23)), ('single models', (24, 26)), ('RD + RG', (28, 31)), ('RD + LM', (36, 39))]",[],[],[],[]
experiments,"In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .","[('In', (0, 1)), ('with', (6, 7)), ('achieves', (9, 10)), ('of', (15, 16))]","[('PTB setting', (2, 4)), ('ensembling', (5, 6)), ('score combination', (7, 9)), ('best over all result', (11, 15)), ('94.25', (16, 17))]",[],[],[],[]
research-problem,In- Order Transition - based Constituent Parsing,[],[],[],[],[],[]
research-problem,"Furthermore , the system achieves 93.6 F 1 with supervised reranking and 94.2 F 1 with semi-supervised reranking , which are the best results on the WSJ benchmark .","[('achieves', (4, 5)), ('with', (8, 9)), ('with', (15, 16))]","[('system', (3, 4)), ('93.6 F 1', (5, 8)), ('supervised reranking', (9, 11)), ('94.2 F 1', (12, 15)), ('semi-supervised reranking', (16, 18))]",[],[],[],[]
research-problem,Transition - based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences .,"[('employs', (5, 6))]","[('Transition - based constituent parsing', (0, 5))]",[],[],[],[]
model,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .","[('propose', (5, 6)), ('for', (10, 11)), ('mitigating', (14, 15)), ('of', (16, 17)), ('by finding', (26, 28)), ('between', (30, 31))]","[('novel transition system', (7, 10)), ('constituent parsing', (11, 13)), ('issues', (15, 16)), ('bottom - up and top - down systems', (18, 26)), ('compromise', (29, 30)), ('bottom - up constituent information', (31, 36)), ('top - down lookahead information', (37, 42))]",[],[],[],[]
code,We release our code at https://github.com/LeonCrashCode/InOrderParser .,[],"[('https://github.com/LeonCrashCode/InOrderParser', (5, 6))]",[],[],[],[]
hyperparameters,is a regularization hyperparameter (? = 10 ?6 ) . We use stochastic gradient descent with a 0.1 initialized learning rate with a 0.05 learning rate decay .,"[('is', (0, 1)), ('use', (11, 12)), ('with', (15, 16)), ('with', (21, 22))]","[('regularization hyperparameter (? = 10 ?6 )', (2, 9)), ('stochastic gradient descent', (12, 15)), ('0.1 initialized learning rate', (17, 21)), ('0.05 learning rate decay', (23, 27))]",[],[],[],[]
results,The bottom - up system performs slightly better than the top - down system .,"[('performs', (5, 6)), ('than', (8, 9))]","[('bottom - up system', (1, 5)), ('slightly better', (6, 8)), ('top - down system', (10, 14))]",[],[],[],[]
results,The inorder system outperforms both the bottom - up and the top - down system .,[],"[('inorder system', (1, 3)), ('outperforms', (3, 4)), ('bottom - up and the top - down system', (6, 15))]",[],[],[],[]
results,shows the parsing results on the English test dataset .,"[('on', (4, 5))]","[('English test dataset', (6, 9))]",[],[],[],[]
results,"We find that the bottom - up parser and the top - down parser have similar results under the greedy setting , and the in - order parser outperforms both of them .","[('find that', (1, 3)), ('have', (14, 15)), ('under', (17, 18))]","[('bottom - up parser and the top - down parser', (4, 14)), ('similar results', (15, 17)), ('greedy setting', (19, 21)), ('in - order parser', (24, 28)), ('outperforms', (28, 29))]",[],[],[],[]
results,"Also , with supervised reranking , the in - order parser achieves the best results .","[('with', (2, 3)), ('achieves', (11, 12))]","[('supervised reranking', (3, 5)), ('in - order parser', (7, 11)), ('best results', (13, 15))]",[],[],[],[]
results,English constituent results,[],[],[],[],[],[]
results,"With the fully - supervise setting 5 , the inorder parser outperforms the state - of - the - art discrete parser , the state - of - the - art neural parsers","[('With', (0, 1)), ('outperforms', (11, 12))]","[('fully - supervise setting', (2, 6)), ('inorder parser', (9, 11)), ('state - of - the - art discrete parser', (13, 22))]",[],[],[],[]
results,Chinese dependency results,[],[],[],[],[],[]
results,UAS LAS,[],[],[],[],[],[]
results,"The in - order parser performs the best on all constituent types , demonstrating that the in - order parser can benefit from both bottom - up and top - down information .","[('performs', (5, 6)), ('on', (8, 9))]","[('in - order parser', (1, 5)), ('best', (7, 8)), ('all constituent types', (9, 12))]",[],[],[],[]
experiments,Parsing as Language Modeling,[],[],[],[],[],[]
research-problem,"When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .","[('converted to', (3, 5)), ('are', (11, 12))]","[('trees', (1, 2)), ('Stanford dependencies', (5, 7)), ('UAS and LAS', (8, 11)), ('95.9 % and 94.1 %', (12, 17))]",[],[],[],[]
model,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .","[('present', (14, 15)), ('achieves', (22, 23)), ('with', (31, 32))]","[('neural - net parse reranker', (16, 21)), ('very good results', (23, 26)), ('93.8 F 1', (27, 30)), ('comparatively simple architecture', (33, 36))]",[],[],[],[]
experiments,Language Modeling,[],[],[],[],[],[]
experiments,Parsing as Language Modeling,[],[],[],[],[],[]
hyperparameters,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,"[('applied to', (2, 4)), ('when', (10, 11))]","[('Dropout', (0, 1)), ('non-recurrent connections', (4, 6)), ('gradients', (7, 8)), ('clipped', (9, 10)), ('norm', (12, 13)), ('bigger', (14, 15)), ('20', (16, 17))]",[],[],[],[]
hyperparameters,The learning rate is 0.25 0.85 max where is an epoch number .,"[('is', (3, 4))]","[('learning rate', (1, 3)), ('0.25 0.85 max', (4, 7))]",[],[],[],[]
research-problem,What Do Recurrent Neural Network Grammars Learn About Syntax ?,[],"[('Syntax', (8, 9))]",[],[],[],[]
research-problem,We find that explicit modeling of composition is crucial for achieving the best performance .,"[('of', (5, 6))]","[('explicit modeling', (3, 5)), ('composition', (6, 7)), ('best performance', (12, 14))]",[],[],[],[]
research-problem,"Through the attention mechanism , we find that headedness plays a central role in phrasal representation ( with the model 's latent attention largely agreeing with predictions made by hand - crafted head rules , albeit with some important differences ) .","[('Through', (0, 1)), ('find', (6, 7)), ('plays', (9, 10)), ('in', (13, 14)), ('made by', (27, 29))]","[('attention mechanism', (2, 4)), ('headedness', (8, 9)), ('phrasal representation', (14, 16)), ('hand - crafted head rules', (29, 34))]",[],[],[],[]
research-problem,"By training grammars without nonterminal labels , we find that phrasal representations depend minimally on nonterminals , providing support for the endocentricity hypothesis .","[('training', (1, 2)), ('without', (3, 4)), ('find that', (8, 10)), ('depend minimally on', (12, 15)), ('providing support for', (17, 20))]","[('grammars', (2, 3)), ('nonterminal labels', (4, 6)), ('phrasal representations', (10, 12)), ('nonterminals', (15, 16)), ('endocentricity hypothesis', (21, 23))]",[],[],[],[]
model,"In this paper , we focus on a recently proposed class of probability distributions , recurrent neural network grammars ( RNNGs ; ) , designed to model syntactic derivations of sentences .","[('focus on', (5, 7)), ('designed to model', (24, 27)), ('of', (29, 30))]","[('probability distributions', (12, 14)), ('recurrent neural network grammars ( RNNGs ; )', (15, 23)), ('syntactic derivations', (27, 29)), ('sentences', (30, 31))]",[],[],[],[]
model,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .","[('focus on', (1, 3)), ('as', (4, 5))]","[('RNNGs', (3, 4)), ('generative probabilistic models over', (5, 9))]",[],[],[],[]
model,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,"[('manipulates', (2, 3)), ('of', (6, 7)), ('to test', (8, 10))]","[('inductive bias', (4, 6)), ('RNNGs', (7, 8)), ('linguistic hypotheses', (10, 12))]",[],[],[],[]
model,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .","[('augment', (6, 7)), ('with', (11, 12)), ('leading to', (18, 20)), ('to incorporate', (25, 27)), ('into', (29, 30))]","[('RNNG composition function', (8, 11)), ('novel gated attention mechanism', (13, 17)), ('GA - RNNG', (21, 24)), ('more interpretability', (27, 29)), ('model', (31, 32))]",[],[],[],[]
model,"Unlike previous works that rely on hand - crafted rules to compose more fine - grained phrase representations , the RNNG implicitly parameterizes the information passed through compositions of phrases ( in ? and the neural network architecture ) , hence weakening the strong independence assumptions in classical probabilistic context - free grammars .","[('parameterizes', (22, 23)), ('passed through', (25, 27)), ('of', (28, 29))]","[('information', (24, 25)), ('compositions', (27, 28)), ('phrases ( in ? and the neural network architecture )', (29, 39))]",[],[],[],[]
model,"To generate a sentence x and its phrase - structure tree y , the RNNG samples a sequence of actions to construct y top - down .","[('To generate', (0, 2)), ('samples', (15, 16)), ('to', (20, 21))]","[('sentence x', (3, 5)), ('RNNG', (14, 15)), ('sequence of actions', (17, 20)), ('construct', (21, 22))]",[],[],[],[]
model,The RNNG uses three different actions :,"[('uses', (2, 3))]","[('RNNG', (1, 2)), ('three different actions', (3, 6))]",[],[],[],[]
model,"The RNNG consists of a stack , buffer of generated words , and list of past actions that lead to the current configuration .","[('consists of', (2, 4)), ('of', (8, 9)), ('lead to', (18, 20))]","[('RNNG', (1, 2)), ('stack', (5, 6)), ('buffer', (7, 8)), ('generated words', (9, 11)), ('list of past actions', (13, 17)), ('current configuration', (21, 23))]",[],[],[],[]
model,"At each timestep , the model encodes the stack , buffer , and past actions , with a separate LSTM for each component as features to define a distribution over the next action to take ( conditioned on the full algorithmic state ) .","[('At', (0, 1)), ('encodes', (6, 7)), ('with', (16, 17)), ('for', (20, 21)), ('as', (23, 24)), ('to define', (25, 27)), ('over', (29, 30)), ('conditioned on', (36, 38))]","[('each timestep', (1, 3)), ('stack , buffer , and past actions', (8, 15)), ('separate LSTM', (18, 20)), ('each component', (21, 23)), ('features', (24, 25)), ('distribution', (28, 29)), ('next action to take', (31, 35)), ('full algorithmic state', (39, 42))]",[],[],[],[]
model,Both inference problems can be solved using an importance sampling procedure .,"[('solved using', (5, 7))]","[('Both inference problems', (0, 3)), ('importance sampling procedure', (8, 11))]",[],[],[],[]
hyperparameters,The generative model did not use any pretrained word embeddings or POS tags ; a discriminative variant of the standard RNNG was used to obtain tree samples for the generative model .,"[('did not use', (3, 6)), ('to obtain', (23, 25))]","[('generative', (1, 2)), ('pretrained word embeddings or POS tags', (7, 13))]",[],[],[],[]
results,Do the phrasal representations learned by RN - NGs depend on individual lexical heads or multiple heads ?,"[('learned by', (4, 6)), ('depend on', (9, 11))]","[('Do the phrasal representations', (0, 4)), ('RN - NGs', (6, 9)), ('individual lexical heads', (11, 14))]",[],[],[],[]
results,NPs .,[],"[('NPs', (0, 1))]",[],[],[],[]
experiments,"The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .","[('better for', (4, 6)), ('much better for', (15, 18)), ('with respect to', (29, 32))]","[('conversion accuracy', (1, 3)), ('nouns ( ? 50 % error )', (6, 13)), ('determiners ( 30 % ) and particles ( 6 % )', (18, 29)), ('Collins head rules', (33, 36))]",[],[],[],[]
results,"On test data ( with the usual split ) , the GA - RNNG achieves 94.2 % , while the U - GA - RNNG achieves 93.5 % .","[('On', (0, 1)), ('achieves', (14, 15)), ('achieves', (25, 26))]","[('test data', (1, 3)), ('usual', (6, 7)), ('GA - RNNG', (11, 14)), ('94.2 %', (15, 17)), ('U - GA - RNNG', (20, 25)), ('93.5 %', (26, 28))]",[],[],[],[]
research-problem,Constituency Parsing with a Self - Attentive Encoder,[],"[('Constituency Parsing', (0, 2))]",[],[],[],[]
research-problem,We demonstrate that replacing an LSTM encoder with a self - attentive architecture can lead to improvements to a state - of the - art discriminative constituency parser .,"[('demonstrate', (1, 2)), ('replacing', (3, 4)), ('with', (7, 8)), ('lead to', (14, 16)), ('to', (17, 18))]","[('LSTM encoder', (5, 7)), ('self - attentive architecture', (9, 13)), ('improvements', (16, 17)), ('state - of the - art discriminative constituency parser', (19, 28))]",[],[],[],[]
model,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .","[('introduce', (5, 6)), ('combines', (9, 10)), ('built using', (12, 14)), ('with', (21, 22)), ('customized for', (24, 26))]","[('parser', (7, 8)), ('encoder', (11, 12)), ('decoder', (23, 24)), ('parsing', (26, 27))]",[],[],[],[]
model,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .","[('uses', (9, 10)), ('performs', (15, 16)), ('than', (17, 18))]","[('character LSTM', (11, 13)), ('better', (16, 17)), ('other lexical representationseven', (18, 21))]",[],[],[],[]
results,The model presented above achieves a score of 92.67 F1 on the Penn Treebank WSJ development set .,"[('achieves', (4, 5)), ('of', (7, 8)), ('on', (10, 11))]","[('score', (6, 7)), ('92.67 F1', (8, 10)), ('Penn Treebank WSJ development set', (12, 17))]",[],[],[],[]
results,"For comparison , a model that uses the same decode procedure with an LSTM - based encoder achieves a development set score of 92.24 .","[('uses', (6, 7)), ('with', (11, 12)), ('achieves', (17, 18)), ('of', (22, 23))]","[('same decode procedure', (8, 11)), ('LSTM - based encoder', (13, 17)), ('development set score', (19, 22)), ('92.24', (23, 24))]",[],[],[],[]
results,Content vs. Position Attention,[],[],[],[],[],[]
ablation-analysis,"We can see that our model learns to use a combination of the two attention types , with positionbased attention being the most important .","[('see', (2, 3)), ('learns to use', (6, 9)), ('of', (11, 12)), ('with', (17, 18))]","[('our model', (4, 6)), ('combination', (10, 11)), ('two attention types', (13, 16)), ('positionbased attention', (18, 20))]",[],[],[],[]
results,"We also see that content - based attention is more useful at later layers in the network , which is consistent with the idea that the initial layers of our model act similarly to a dilated convolutional network while the upper layers have a greater balance between the two attention types .","[('see', (2, 3)), ('is', (8, 9)), ('at', (11, 12)), ('in', (14, 15))]","[('content - based attention', (4, 8)), ('more useful', (9, 11)), ('later layers', (12, 14)), ('network', (16, 17))]",[],[],[],[]
research-problem,Improving Coreference Resolution by Learning Entity - Level Distributed Representations,[],"[('Coreference Resolution', (1, 3))]",[],[],[],[]
research-problem,A long - standing challenge in coreference resolution has been the incorporation of entity - level information - features defined over clusters of mentions instead of mention pairs .,[],"[('coreference resolution', (6, 8))]",[],[],[],[]
research-problem,"Coreference resolution , the task of identifying which mentions in a text refer to the same realworld entity , is fundamentally a clustering problem .",[],"[('Coreference resolution', (0, 2))]",[],[],[],[]
model,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .","[('train', (6, 7)), ('to build', (11, 13)), ('of', (15, 16)), ('of', (17, 18))]","[('deep neural network', (8, 11)), ('distributed representations', (13, 15)), ('pairs', (16, 17)), ('coreference clusters', (18, 20))]",[],[],[],[]
model,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .","[('captures', (1, 2)), ('with', (6, 7)), ('instead of', (15, 17))]","[('entity - level information', (2, 6)), ('large number of learned , continuous features', (8, 15)), ('small number of hand - crafted categorical ones', (18, 26))]",[],[],[],[]
model,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .","[('At', (0, 1)), ('builds up', (4, 6)), ('starting with', (10, 12)), ('in', (14, 15))]","[('test time', (1, 3)), ('coreference clusters', (6, 8)), ('incrementally', (8, 9)), ('each mention', (12, 14)), ('merging', (20, 21)), ('pair of clusters', (22, 25))]",[],[],[],[]
model,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,"[('with', (4, 5)), ('combines', (15, 16)), ('of', (18, 19))]","[('decisions', (3, 4)), ('novel easy - first cluster - ranking procedure', (6, 14)), ('strengths', (17, 18)), ('cluster - ranking ( Rahman and and easy - first coreference algorithms', (19, 31))]",[],[],[],[]
research-problem,Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made .,[],"[('Training incremental coreference systems', (0, 4))]",[],[],[],[]
model,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,"[('using', (4, 5)), ('inspired by', (12, 14)), ('to train', (15, 17))]","[('learning - to - search algorithm', (6, 12)), ('SEARN', (14, 15)), ('our neural network', (17, 20))]",[],[],[],[]
model,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,"[('lead to', (28, 30))]","[('which action ( a cluster merge ) available', (7, 15)), ('current state', (17, 19)), ('partially completed coreference clustering', (21, 25)), ('high - scoring coreference partition', (31, 36))]",[],[],[],[]
hyperparameters,"We initialized our word embeddings with 50 dimensional ones produced by word2vec on the Gigaword corpus for English and 64 dimensional ones provided by Polyglot ( Al - Rfou et al. , 2013 ) for Chinese .","[('initialized', (1, 2)), ('with', (5, 6)), ('produced by', (9, 11)), ('on', (12, 13)), ('for', (16, 17)), ('provided by', (22, 24))]","[('our word embeddings', (2, 5)), ('50 dimensional ones', (6, 9)), ('word2vec', (11, 12)), ('Gigaword corpus', (14, 16)), ('English', (17, 18)), ('64 dimensional ones', (19, 22)), ('Chinese', (35, 36))]",[],[],[],[]
hyperparameters,Averaged word embeddings were held fixed during training while the embeddings used for single words were updated .,"[('held', (4, 5)), ('during', (6, 7)), ('used for', (11, 13))]","[('Averaged word embeddings', (0, 3)), ('fixed', (5, 6)), ('training', (7, 8)), ('embeddings', (10, 11)), ('single words', (13, 15)), ('updated', (16, 17))]",[],[],[],[]
hyperparameters,"We set our hidden layer sizes to M 1 = 1000 , M 2 = d = 500 and minimized the training objective using RMS - Prop .","[('set', (1, 2)), ('to', (6, 7)), ('minimized', (19, 20)), ('using', (23, 24))]","[('hidden layer sizes', (3, 6)), ('M 1 = 1000', (7, 11)), ('M 2 = d = 500', (12, 18)), ('training objective', (21, 23)), ('RMS - Prop', (24, 27))]",[],[],[],[]
hyperparameters,"To regularize the network , we applied L2 regularization to the model weights and dropout with a rate of 0.5 on the word embeddings and the output of each hidden layer .","[('To regularize', (0, 2)), ('applied', (6, 7)), ('to', (9, 10)), ('with', (15, 16)), ('of', (18, 19)), ('on', (20, 21)), ('of', (27, 28))]","[('network', (3, 4)), ('L2 regularization', (7, 9)), ('model weights', (11, 13)), ('dropout', (14, 15)), ('rate', (17, 18)), ('0.5', (19, 20)), ('word embeddings', (22, 24)), ('output', (26, 27)), ('each hidden layer', (28, 31))]",[],[],[],[]
hyperparameters,"As in , we found that pretraining is crucial for the mentionranking model 's success .","[('found', (4, 5)), ('crucial for', (8, 10))]","[('pretraining', (6, 7)), (""mentionranking model 's success"", (11, 15))]",[],[],[],[]
experiments,Mention - Ranking Model Experiments,[],[],[],[],[],[]
ablation-analysis,"We find the small number of non-embedding features substantially improves model performance , especially the distance and string matching features .","[('find', (1, 2)), ('especially', (13, 14))]","[('small number of non-embedding features', (3, 8)), ('substantially improves', (8, 10)), ('model performance', (10, 12)), ('distance and string matching features', (15, 20))]",[],[],[],[]
ablation-analysis,Cluster - Ranking Model Experiments,[],[],[],[],[],[]
ablation-analysis,Using pretrained weights greatly improves performance .,"[('Using', (0, 1))]","[('pretrained weights', (1, 3)), ('greatly improves', (3, 5)), ('performance', (5, 6))]",[],[],[],[]
ablation-analysis,We find the easy - first approach slightly outperforms using a left - to - right ordering of mentions .,"[('find', (1, 2)), ('using', (9, 10)), ('of', (17, 18))]","[('easy - first approach', (3, 7)), ('slightly outperforms', (7, 9)), ('left - to - right ordering', (11, 17)), ('mentions', (18, 19))]",[],[],[],[]
results,Our mention - ranking model surpasses all previous systems .,"[('surpasses', (5, 6))]","[('mention - ranking model', (1, 5)), ('all previous systems', (6, 9))]",[],[],[],[]
results,"The cluster - ranking model improves results further across both languages and all evaluation metrics , demonstrating the utility of incorporating entity - level information .","[('improves', (5, 6)), ('further across', (7, 9))]","[('cluster - ranking model', (1, 5)), ('results', (6, 7)), ('both languages and all evaluation metrics', (9, 15))]",[],[],[],[]
results,"However , it is worth noting that in practice the much more complicated cluster - ranking model brings only fairly modest gains in performance .","[('brings', (17, 18)), ('in', (22, 23))]","[('much more complicated cluster - ranking model', (10, 17)), ('fairly modest gains', (19, 22)), ('performance', (23, 24))]",[],[],[],[]
research-problem,End - to - end Deep Reinforcement Learning Based Coreference Resolution,[],[],[],[],[],[]
research-problem,Recent neural network models have significantly advanced the task of coreference resolution .,[],"[('coreference resolution', (10, 12))]",[],[],[],[]
research-problem,"Coreference resolution is one of the most fundamental tasks in natural language processing ( NLP ) , which has a significant impact on many downstream applications including information extraction ) , question answering , and entity linking .",[],"[('Coreference resolution', (0, 2))]",[],[],[],[]
model,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .","[('propose', (5, 6)), ('to resolve', (17, 19))]","[('goal - directed endto - end deep reinforcement learning framework', (7, 17)), ('coreference', (19, 20))]",[],[],[],[]
model,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .","[('leverage', (3, 4)), ('includes', (14, 15)), ('scoring', (19, 20)), ('generating', (25, 26)), ('over', (29, 30)), ('from', (35, 36)), ('to', (39, 40))]","[('neural architecture', (5, 7)), ('our policy network', (9, 12)), ('learning span representation', (15, 18)), ('potential entity mentions', (20, 23)), ('probability distribution', (27, 29)), ('all possible coreference linking actions', (30, 35)), ('current mention', (37, 39))]",[],[],[],[]
model,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .","[('introduce', (3, 4)), ('to encourage', (8, 10)), ('prevent', (12, 13)), ('from', (15, 16)), ('to', (18, 19))]","[('entropy regularization term', (5, 8)), ('exploration', (10, 11)), ('policy', (14, 15)), ('prematurely converging', (16, 18)), ('bad local optimum', (20, 23))]",[],[],[],[]
model,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .","[('update', (3, 4)), ('based on', (9, 11)), ('associated with', (13, 15)), ('of', (16, 17)), ('computed on', (22, 24))]","[('regularized policy network parameters', (5, 9)), ('rewards', (12, 13)), ('sequences', (15, 16)), ('whole input document', (25, 28))]",[],[],[],[]
hyperparameters,"First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .","[('pretrain', (3, 4)), ('for', (11, 12)), ('use', (17, 18)), ('for', (21, 22))]","[('our model', (4, 6)), ('around 200 K steps', (12, 16)), ('learned parameters', (19, 21)), ('initialization', (22, 23))]",[],[],[],[]
hyperparameters,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .","[('set', (3, 4)), ('tune', (14, 15)), ('expr in', (19, 21)), ('set it to', (37, 40)), ('based on', (43, 45))]","[('number of sampled trajectories N s = 100', (5, 13)), ('regularization parameter ?', (16, 19)), ('{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }', (21, 36)), ('10 ? 4', (40, 43)), ('development set', (46, 48))]",[],[],[],[]
results,"In , we compare our model with the coreference systems that have produced significant improvement over the last 3 years on the OntoNotes benchmark .","[('produced', (12, 13)), ('on', (20, 21))]","[('coreference systems', (8, 10)), ('significant improvement', (13, 15)), ('OntoNotes benchmark', (22, 24))]",[],[],[],[]
results,"Built on top of the model in but excluding ELMo , our base reinforced model improves the average F 1 score around 2 points ( statistical significant t- test with p < 0.05 ) compared with .","[('Built on top of', (0, 4)), ('excluding', (8, 9)), ('improves', (15, 16)), ('around', (21, 22))]","[('model', (5, 6)), ('ELMo', (9, 10)), ('our base reinforced model', (11, 15)), ('average F 1 score', (17, 21)), ('2 points', (22, 24))]",[],[],[],[]
results,"Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .","[('using', (4, 5)), ('to encourage', (7, 9)), ('can improve', (10, 12)), ('by', (14, 15))]","[('entropy regularization', (5, 7)), ('exploration', (9, 10)), ('result', (13, 14)), ('1 point', (15, 17))]",[],[],[],[]
results,"Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .","[('introducing', (2, 3)), ('to', (9, 10)), ('can further boosts', (13, 16))]","[('context - dependent ELMo embedding', (4, 9)), ('our base model', (10, 13)), ('performance', (17, 18))]",[],[],[],[]
results,"We also notice that our full model 's improvement is mainly from higher precision scores and reasonably good recall scores , which indicates that our reinforced model combined with more active exploration produces better coreference scores to reduce false positive coreference links .","[('notice', (2, 3)), ('mainly from', (10, 12))]","[(""our full model 's improvement"", (4, 9)), ('higher precision scores', (12, 15)), ('reasonably good recall scores', (16, 20))]",[],[],[],[]
results,"Overall , our full model achieves the state - of the - art performance of 73.8 % F1 - score when using ELMo and entropy regularization ( compared to models marked with * in , and our approach simultaneously obtains the best F1 -score of 70.5 % when using fixed word embedding only .","[('achieves', (5, 6)), ('of', (14, 15)), ('when using', (20, 22)), ('of', (44, 45)), ('when using', (47, 49))]","[('our full model', (2, 5)), ('state - of the - art performance', (7, 14)), ('73.8 % F1 - score', (15, 20)), ('ELMo and entropy regularization', (22, 26)), ('our', (36, 37)), ('best F1 -score', (41, 44)), ('70.5 %', (45, 47)), ('fixed word embedding', (49, 52))]",[],[],[],[]
research-problem,Deep Reinforcement Learning for Mention - Ranking Coreference Models,[],"[('Mention - Ranking Coreference', (4, 8))]",[],[],[],[]
model,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .","[('explore using', (5, 7)), ('to directly optimize', (12, 15)), ('for', (18, 19))]","[('two variants of reinforcement learning', (7, 12)), ('coreference system', (16, 18)), ('coreference evaluation metrics', (19, 22))]",[],[],[],[]
model,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .","[('modify', (4, 5)), ('by incorporating', (11, 13)), ('associated with', (15, 17)), ('into', (20, 21))]","[('max-margin coreference objective', (6, 9)), ('reward', (14, 15)), ('each coreference decision', (17, 20)), (""loss 's slack rescaling"", (22, 26))]",[],[],[],[]
model,Our model is a neural mention - ranking model .,"[('is', (2, 3))]","[('neural mention - ranking model', (4, 9))]",[],[],[],[]
experiments,Reinforcement Learning,[],[],[],[],[],[]
results,"We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .","[('find that', (1, 3)), ('does', (4, 5)), ('than', (7, 8)), ('performs', (15, 16))]","[('REINFORCE', (3, 4)), ('slightly better', (5, 7)), ('heuristic loss', (9, 11)), ('reward rescaling', (13, 15)), ('significantly better', (16, 18))]",[],[],[],[]
ablation-analysis,"During training it optimizes the model 's performance in expectation , but at test - time it takes the most probable sequence of actions .","[('During', (0, 1)), ('optimizes', (3, 4)), ('in', (8, 9)), ('at', (12, 13)), ('takes', (17, 18)), ('of', (22, 23))]","[('training', (1, 2)), (""model 's performance"", (5, 8)), ('expectation', (9, 10)), ('test - time', (13, 16)), ('most probable sequence', (19, 22)), ('actions', (23, 24))]",[],[],[],[]
results,"The reward - rescaled max - margin loss combines the best of both worlds , resulting in superior performance .","[('combines', (8, 9)), ('resulting in', (15, 17))]","[('reward - rescaled max - margin loss', (1, 8)), ('best of both worlds', (10, 14)), ('superior performance', (17, 19))]",[],[],[],[]
research-problem,Higher - order Coreference Resolution with Coarse - to - fine Inference,[],"[('Higher', (0, 1))]",[],[],[],[]
model,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,"[('introduce', (1, 2)), ('of', (4, 5)), ('uses', (10, 11))]","[('approximation', (3, 4)), ('higher - order inference', (5, 9)), ('span - ranking architecture', (12, 16))]",[],[],[],[]
model,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .","[('propose', (13, 14)), ('learned with', (21, 23))]","[('coarseto - fine approach', (15, 19)), ('single endto - end objective', (24, 29))]",[],[],[],[]
experiments,1 . increasing the maximum span width from 10 to 30 words .,"[('increasing', (2, 3)), ('from', (7, 8))]","[('maximum span width', (4, 7)), ('10 to 30 words', (8, 12))]",[],[],[],[]
code,1 https://github.com/kentonl/e2e-coref,[],[],[],[],[],[]
experiments,2 . using 3 highway LSTMs instead of 1 .,"[('using', (2, 3)), ('instead of', (6, 8))]","[('3 highway LSTMs', (3, 6)), ('1', (8, 9))]",[],[],[],[]
hyperparameters,3 . using Glo Ve word embeddings with a window size of 2 for the headword embeddings and a window size of 10 for the LSTM inputs .,"[('using', (2, 3)), ('with', (7, 8)), ('of', (11, 12)), ('for', (13, 14)), ('for', (23, 24))]","[('Glo Ve word embeddings', (3, 7)), ('window size', (9, 11)), ('2', (12, 13)), ('headword embeddings', (15, 17)), ('window size', (19, 21)), ('10', (22, 23)), ('LSTM inputs', (25, 27))]",[],[],[],[]
results,"On the development set , the second - order model ( N = 2 ) outperforms the first - order model by 0.8 F1 , but the third order model only provides an additional 0.1 F1 improvement .","[('On', (0, 1)), ('by', (21, 22)), ('provides', (31, 32))]","[('development set', (2, 4)), ('second - order model ( N = 2 )', (6, 15)), ('outperforms', (15, 16)), ('first - order model', (17, 21)), ('0.8 F1', (22, 24)), ('third order model', (27, 30)), ('additional 0.1 F1 improvement', (33, 37))]",[],[],[],[]
results,"The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .","[('is', (6, 7)), ('from augmented with', (12, 15)), ('achieves', (22, 23))]","[('span - ranking model', (8, 12)), ('ELMo and hyperparameter tuning', (16, 20)), ('72.3 F1', (23, 25))]",[],[],[],[]
results,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .","[('achieves', (3, 4)), ('setting', (7, 8)), ('for', (14, 15))]","[('Our full approach', (0, 3)), ('73.0 F1', (4, 6)), ('new state of the art', (9, 14)), ('coreference resolution', (15, 17))]",[],[],[],[]
results,We also observe further improvement by including the second - order inference ( Section 3 ) .,"[('observe', (2, 3)), ('by including', (5, 7))]","[('further improvement', (3, 5)), ('second - order inference', (8, 12))]",[],[],[],[]
results,"The improvement is largely driven by the over all increase in precision , which is expected since the higher - order inference mainly serves to rule out inconsistent clusters .","[('largely driven by', (3, 6)), ('in', (10, 11))]","[('improvement', (1, 2)), ('over all increase', (7, 10)), ('precision', (11, 12))]",[],[],[],[]
research-problem,A Mention - Ranking Model for Abstract Anaphora Resolution,[],[],[],[],[],[]
research-problem,"Resolving abstract anaphora is an important , but difficult task for text understanding .",[],"[('Resolving abstract anaphora', (0, 3))]",[],[],[],[]
model,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .","[('inspired by', (3, 5)), ('for', (10, 11)), ('combines it with', (14, 17)), ('for learning', (21, 23)), ('between', (24, 25))]","[('mention - ranking model', (6, 10)), ('coreference resolution', (11, 13)), ('Siamese Net', (18, 20)), ('similarity', (23, 24)), ('sentences', (25, 26))]",[],[],[],[]
model,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .","[('Given', (0, 1)), ('learns', (42, 43)), ('for', (44, 45)), ('in', (51, 52))]","[('anaphoric sentence', (2, 4)), ('representations', (43, 44)), ('candidate and the anaphoric sentence', (46, 51)), ('shared space', (53, 55))]",[],[],[],[]
model,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,"[('combined into', (3, 5)), ('to calculate', (9, 11)), ('characterizes', (14, 15))]","[('joint representation', (6, 8)), ('score', (12, 13)), ('relation', (16, 17))]",[],[],[],[]
code,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora.,[],"[('data extraction', (11, 13))]",[],[],[],[]
results,"PS BL always performs worse than the KZH13 model on the ASN , so we report it only for ARRAU - AA .","[('performs', (3, 4)), ('than', (5, 6)), ('on', (9, 10))]","[('PS BL', (0, 2)), ('worse', (4, 5)), ('KZH13 model', (7, 9)), ('ASN', (11, 12))]",[],[],[],[]
hyperparameters,"Embeddings for tags are initialized with values drawn from the uniform distribution U ? 1 ? d+t , 1 ? d+t , where t is the number of tags 16 and d ? { 50 , qlog - U ( 30 , 100 ) } the size of the tag embeddings .","[('for', (1, 2)), ('drawn from', (7, 9))]","[('Embeddings', (0, 1)), ('tags', (2, 3)), ('initialized', (4, 5)), ('values', (6, 7)), ('uniform distribution U ? 1 ? d+t , 1 ? d+t', (10, 21)), ('tag embeddings', (49, 51))]",[],[],[],[]
hyperparameters,"The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .","[('of', (2, 3)), ('set to', (8, 10))]","[('size', (1, 2)), ('LSTMs hidden states', (4, 7)), ('{ 100 , qlog - U ( 30 , 150 ) }', (10, 22))]",[],[],[],[]
hyperparameters,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .","[('initialized', (1, 2)), ('of', (5, 6)), ('with', (8, 9))]","[('weight matrices', (3, 5)), ('LSTMs', (7, 8)), ('random orthogonal matrices', (9, 12))]",[],[],[],[]
hyperparameters,The first feed - forward layer size is set to a value in Optimization .,"[('set to', (8, 10)), ('in', (12, 13))]","[('first', (1, 2)), ('feed - forward layer size', (2, 7)), ('value', (11, 12)), ('Optimization', (13, 14))]",[],[],[],[]
hyperparameters,"We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .","[('trained', (1, 2)), ('in', (4, 5)), ('using', (6, 7)), ('with', (15, 16))]","[('our model', (2, 4)), ('minibatches', (5, 6)), ('Adam ( Kingma and Ba , 2015 )', (7, 15)), ('learning rate', (17, 19)), ('10 ? 4', (20, 23)), ('maximal batch size', (24, 27)), ('64', (27, 28))]",[],[],[],[]
hyperparameters,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .","[('clip', (1, 2)), ('by', (3, 4)), ('with', (7, 8)), ('in', (11, 12))]","[('gradients', (2, 3)), ('global norm', (4, 6)), ('clipping value', (9, 11)), ('{ 1.0 , U ( 1 , 100 ) }', (12, 22))]",[],[],[],[]
hyperparameters,We train for 10 epochs and choose the model that performs best on the devset .,"[('train for', (1, 3)), ('choose', (6, 7)), ('on', (12, 13))]","[('10 epochs', (3, 5)), ('model', (8, 9)), ('performs', (10, 11)), ('best', (11, 12)), ('devset', (14, 15))]",[],[],[],[]
hyperparameters,"We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) }.","[('used', (1, 2)), ('with', (7, 8))]","[('l 2 - regularization', (3, 7)), ('? ? { 10 ?5', (8, 13))]",[],[],[],[]
hyperparameters,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .","[('with', (1, 2)), ('applied to', (18, 20)), ('of', (22, 23))]","[('Dropout', (0, 1)), ('keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) }', (3, 17)), ('outputs', (21, 22)), ('LSTMs', (24, 25)), ('both feed - forward layers', (26, 31)), ('input', (35, 36)), ('k p ? U (0.8 , 1.0 )', (37, 45))]",[],[],[],[]
results,6 Results and analysis 6.1 Results on shell noun resolution dataset provides the results of the mentionranking model ( MR - LSTM ) on the ASN corpus using default HPs .,"[('on', (23, 24)), ('using', (27, 28))]","[('shell noun resolution dataset', (7, 11)), ('mentionranking model ( MR - LSTM )', (16, 23)), ('ASN corpus', (25, 27)), ('default HPs', (28, 30))]",[],[],[],[]
results,"In terms of s@1 score , MR - LSTM outperforms both KZH13 's results and TAG BL without even necessitating HP tuning .","[('In terms of', (0, 3)), ('without even necessitating', (17, 20))]","[('s@1 score', (3, 5)), ('MR - LSTM', (6, 9)), ('outperforms', (9, 10)), (""KZH13 's results"", (11, 14)), ('TAG BL', (15, 17)), ('HP tuning', (20, 22))]",[],[],[],[]
results,"From we observe : ( 1 ) with HPs tuned on ARRAU - AA , we obtain results well beyond KZH13 , ( 2 ) all ablated model variants perform worse than the full model , ( 3 ) a large performance drop when omitting syntactic information ( tag , cut ) suggests that the model makes good use of it .","[('with', (7, 8)), ('tuned on', (9, 11)), ('obtain', (16, 17)), ('perform', (29, 30)), ('than', (31, 32)), ('when omitting', (43, 45))]","[('HPs', (8, 9)), ('ARRAU - AA', (11, 14)), ('results', (17, 18)), ('well beyond', (18, 20)), ('KZH13', (20, 21)), ('all ablated model variants', (25, 29)), ('worse', (30, 31)), ('full model', (33, 35)), ('large performance drop', (40, 43)), ('syntactic information ( tag , cut )', (45, 52))]",[],[],[],[]
results,Results on the ARRAU corpus,"[('on', (1, 2))]",[],[],[],[],[]
results,"The MR - LSTM is more successful in resolving nominal than pronominal anaphors , although the training data provides only pronominal ones .","[('in', (7, 8)), ('than', (10, 11))]","[('MR - LSTM', (1, 4)), ('more successful', (5, 7)), ('resolving', (8, 9)), ('nominal', (9, 10)), ('pronominal anaphors', (11, 13))]",[],[],[],[]
results,"Contrary to shell noun resolution , omitting syntactic information boosts performance in ARRAU - AA .","[('Contrary to', (0, 2)), ('omitting', (6, 7)), ('boosts', (9, 10)), ('in', (11, 12))]","[('syntactic information', (7, 9)), ('performance', (10, 11)), ('ARRAU - AA', (12, 15))]",[],[],[],[]
results,"This is what we can observe from row 2 vs. row 6 in Table 5 : the MR - LSTM without context embedding ( ctx ) achieves a comparable s@ 2 score with the variant that omits syntactic information , but better s@3 - 4 scores .","[('achieves', (26, 27)), ('with', (32, 33))]","[('MR - LSTM without context embedding ( ctx )', (17, 26)), ('comparable s@ 2 score', (28, 32)), ('variant', (34, 35)), ('better s@3 - 4 scores', (41, 46))]",[],[],[],[]
research-problem,Learning Global Features for Coreference Resolution,[],[],[],[],[],[]
model,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .","[('propose to', (4, 6)), ('of', (8, 9)), ('by embedding', (11, 13)), ('using', (15, 16))]","[('learn', (6, 7)), ('representations', (7, 8)), ('mention clusters', (9, 11)), ('sequentially', (14, 15)), ('recurrent neural network', (17, 20))]",[],[],[],[]
model,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .","[('learns', (11, 12)), ('from', (15, 16)), ('present in', (19, 21))]","[('manually defined cluster features', (4, 8)), ('global representation', (13, 15)), ('individual mentions', (17, 19)), ('each cluster', (21, 23))]",[],[],[],[]
model,We incorporate these representations into a mention - ranking style coreference system .,"[('incorporate', (1, 2)), ('into', (4, 5))]","[('mention - ranking style coreference system', (6, 12))]",[],[],[],[]
model,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .","[('train', (1, 2)), ('as', (4, 5)), ('with', (8, 9))]","[('model', (3, 4)), ('local classifier', (6, 8)), ('fixed context', (9, 11))]",[],[],[],[]
results,"In we present our main results on the CoNLL English test set , and compare with other recent stateof - the - art systems .","[('on', (6, 7))]","[('CoNLL English test set', (8, 12))]",[],[],[],[]
results,"We see a statistically significant improvement of over 0.8 Co NLL points over the previous state of the art , and the highest F 1 scores to date on all three CoNLL metrics .","[('see', (1, 2)), ('of', (6, 7)), ('over', (12, 13)), ('on', (28, 29))]","[('statistically significant improvement', (3, 6)), ('over 0.8 Co NLL points', (7, 12)), ('previous state of the art', (14, 19)), ('highest F 1 scores', (22, 26))]",[],[],[],[]
results,We now consider in more detail the impact of global features and RNNs on performance .,"[('consider', (2, 3)), ('of', (8, 9)), ('on', (13, 14))]","[('impact', (7, 8)), ('global features and RNNs', (9, 13)), ('performance', (14, 15))]",[],[],[],[]
results,"In we see that the RNN improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .","[('see', (2, 3)), ('improves', (6, 7))]","[('RNN', (5, 6)), ('performance over all', (7, 10))]",[],[],[],[]
results,"While WL errors also decrease for both these mention - categories under the RNN model , FN errors increase .","[('for', (5, 6)), ('under', (11, 12))]","[('WL errors', (1, 3)), ('decrease', (4, 5)), ('both', (6, 7)), ('RNN model', (13, 15)), ('FN errors', (16, 18)), ('increase', (18, 19))]",[],[],[],[]
results,"Importantly , the RNN performance is significantly better than that of the Avg baseline , which barely improves over mention - ranking , even with oracle history .","[('than', (8, 9)), ('over', (18, 19)), ('even with', (23, 25))]","[('RNN performance', (3, 5)), ('significantly better', (6, 8)), ('Avg baseline', (12, 14)), ('barely improves', (16, 18)), ('mention - ranking', (19, 22)), ('oracle history', (25, 27))]",[],[],[],[]
results,"We also note that while RNN performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting , we are still able to recover a significant portion of the possible performance improvement .","[('degrades', (7, 8)), ('in', (8, 9)), ('when moving from', (13, 16)), ('upperbound to', (19, 21)), ('able to recover', (28, 31)), ('of', (34, 35))]","[('RNN performance', (5, 7)), ('both precision and recall', (9, 13)), ('oracle history', (17, 19)), ('greedy setting', (22, 24)), ('significant portion', (32, 34)), ('possible performance improvement', (36, 39))]",[],[],[],[]
research-problem,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution,[],[],[],[],[],[]
research-problem,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .","[('present', (5, 6)), ('that learns', (10, 12)), ('for improving', (16, 18))]","[('word embedding model', (7, 10)), ('cross - sentence dependency', (12, 16)), ('end - to - end co-reference resolution ( E2E - CR )', (18, 30))]",[],[],[],[]
model,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .","[('propose', (46, 47)), ('for', (52, 53))]","[('cross - sentence encoder', (48, 52)), ('end - to - end co-reference ( E2E - CR )', (53, 64))]",[],[],[],[]
model,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .","[('Borrowing', (0, 1)), ('of', (3, 4)), ('containing', (14, 15)), ('from', (19, 20)), ('added to', (23, 25))]","[('idea', (2, 3)), ('external memory module', (5, 8)), ('external memory block', (11, 14)), ('syntactic and semantic information', (15, 19)), ('context sentences', (20, 22)), ('standard LSTM model', (26, 29))]",[],[],[],[]
model,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .","[('With', (0, 1)), ('as', (15, 16)), ('calculate', (21, 22)), ('of', (24, 25)), ('by taking', (27, 29)), ('into', (35, 36))]","[('context', (2, 3)), ('proposed', (7, 8)), ('encode', (12, 13)), ('input sentences', (13, 15)), ('batch', (17, 18)), ('representations', (23, 24)), ('input words', (25, 27)), ('target sentences and context sentences', (30, 35)), ('consideration', (36, 37))]",[],[],[],[]
experiments,Language Representation Learning,[],[],[],[],[],[]
research-problem,LSTMs with Cross - Sentence Attention,[],[],[],[],[],[]
hyperparameters,"In practice , the LSTM modules applied in our model have 200 output units .","[('applied in', (6, 8)), ('have', (10, 11))]","[('LSTM modules', (4, 6)), ('our model', (8, 10)), ('200 output units', (11, 14))]",[],[],[],[]
hyperparameters,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .","[('calculate', (4, 5)), ('using', (9, 10)), ('with', (13, 14)), ('consisting of', (17, 19))]","[('ASL', (1, 2)), ('cross - sentence dependency', (5, 9)), ('multilayer perceptron', (11, 13)), ('one hidden layer', (14, 17)), ('150 hidden units', (19, 22))]",[],[],[],[]
hyperparameters,The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,"[('set as', (5, 7)), ('every', (12, 13))]","[('initial learning rate', (1, 4)), ('0.001', (7, 8)), ('decays', (9, 10)), ('0.001 %', (10, 12)), ('100 steps', (13, 15))]",[],[],[],[]
hyperparameters,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .","[('optimized with', (3, 5))]","[('Adam algorithm', (6, 8))]",[],[],[],[]
hyperparameters,"In co-reference prediction , we select 250 candidate antecedents as our baseline model .","[('In', (0, 1)), ('select', (5, 6)), ('as', (9, 10))]","[('co-reference prediction', (1, 3)), ('250 candidate antecedents', (6, 9)), ('our baseline model', (10, 13))]",[],[],[],[]
results,"Comparing with the baseline model that achieved 67.2 % F1 score , the ASL model improved the performance by 0.6 % and achieved 67.8 % average F1 .","[('Comparing with', (0, 2)), ('achieved', (6, 7)), ('improved', (15, 16)), ('by', (18, 19)), ('achieved', (22, 23))]","[('baseline model', (3, 5)), ('67.2 % F1 score', (7, 11)), ('ASL model', (13, 15)), ('performance', (17, 18)), ('0.6 %', (19, 21)), ('67.8 % average F1', (23, 27))]",[],[],[],[]
results,- Uh- huh .,[],"[('Uh- huh', (1, 3))]",[],[],[],[]
results,"show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .","[('show', (0, 1)), ('consider', (5, 6)), ('encodes', (17, 18)), ('from', (20, 21))]","[('cross - sentence dependency', (6, 10)), ('significantly outperform', (10, 12)), ('baseline model', (13, 15)), ('each sentence', (18, 20)), ('input document', (22, 24))]",[],[],[],[]
results,"With the proposed context gate , ASL takes knowledge from context sentences if local inputs are not informative enough .","[('takes knowledge', (7, 9)), ('from', (9, 10)), ('if', (12, 13)), ('are', (15, 16))]","[('ASL', (6, 7)), ('context sentences', (10, 12)), ('local inputs', (13, 15)), ('not informative enough', (16, 19))]",[],[],[],[]
research-problem,Coreference Resolution with Entity Equalization,[],[],[],[],[],[]
research-problem,"The problem of entity - level representation ( also referred to as high - order coreference models ) has attracted considerable interest recently , with methods ranging from imitation learning to iterative refinement .",[],"[('entity - level representation', (3, 7)), ('high', (12, 13))]",[],[],[],[]
model,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .","[('provides', (6, 7)), ('facilitates', (21, 22))]","[('entity - level representation', (8, 12)), ('end - to - end optimization', (22, 28))]",[],[],[],[]
model,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .","[('motivated by', (15, 17)), ('of', (21, 22))]","[('BERT embeddings', (12, 14)), ('impressive empirical performance', (18, 21)), ('BERT', (22, 23))]",[],[],[],[]
model,We show that this can be done by using BERT in a fully convolutional manner .,"[('done by using', (6, 9)), ('in', (10, 11))]","[('BERT', (9, 10)), ('fully convolutional manner', (12, 15))]",[],[],[],[]
model,"Our work is the first to use BERT for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .","[('first to use', (4, 7)), ('for', (8, 9)), ('demonstrate', (17, 18)), ('results in', (20, 22)), ('over', (24, 25))]","[('BERT', (7, 8)), ('task of coreference resolution', (10, 14)), ('significant improvement', (22, 24)), ('current state - of - the - art', (25, 33))]",[],[],[],[]
baselines,Using BERT,"[('Using', (0, 1))]",[],[],[],[],[]
results,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg.","[('is', (2, 3)), ('from with', (8, 10)), ('achieves', (21, 22))]","[('span - ranking model', (4, 8)), ('ELMo input features and second - order span representations', (10, 19))]",[],[],[],[]
results,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,"[('Replacing', (2, 3)), ('with', (6, 7)), ('achieves', (9, 10))]","[('F1', (0, 1)), ('ELMo features', (4, 6)), ('BERT features', (7, 9)), ('76. 25 % average F1', (10, 15))]",[],[],[],[]
results,"Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .","[('Removing', (0, 1)), ('while using', (8, 10)), ('achieves', (12, 13)), ('achieving', (17, 18)), ('on', (23, 24))]","[('second - order span - representations', (2, 8)), ('BERT features', (10, 12)), ('76.37 % F1', (13, 16)), ('higher recall and lower precision', (18, 23)), ('all evaluation metrics', (24, 27))]",[],[],[],[]
results,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .","[('Replacing', (0, 1)), ('achieves', (7, 8)), ('consistently achieving', (16, 18))]","[('secondorder span representations', (1, 4)), ('Entity Equalization', (5, 7)), ('76. 64 % average F1', (8, 13)), ('highest F 1 score', (19, 23))]",[],[],[],[]
results,"Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .","[('set', (2, 3)), ('for', (9, 10)), ('improving', (13, 14)), ('by', (20, 21))]","[('new state of the art', (4, 9)), ('coreference resolution', (10, 12)), ('previous state of the art', (15, 20)), ('3.6 % average F1', (21, 25))]",[],[],[],[]
research-problem,End - to - end Neural Coreference Resolution,[],"[('End - to - end', (0, 5))]",[],[],[],[]
research-problem,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .,"[('introduce', (1, 2)), ('show', (13, 14)), ('without using', (21, 23))]","[('first end - to - end coreference resolution model', (3, 12)), ('significantly outperforms', (16, 18)), ('all previous work', (18, 21)), ('syntactic parser or handengineered mention detector', (24, 30))]",[],[],[],[]
model,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .,"[('present', (1, 2)), ('learned', (17, 18)), ('given', (21, 22))]","[('first state - of - the - art neural coreference resolution model', (3, 15)), ('end - toend', (18, 21)), ('only gold mention clusters', (22, 26))]",[],[],[],[]
model,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .","[('demonstrate', (1, 2)), ('by training', (24, 26)), ('jointly learns', (35, 37)), ('are', (39, 40))]","[('first time', (4, 6)), ('resources', (8, 9)), ('not required', (10, 12)), ('performance', (16, 17)), ('improved significantly', (19, 21)), ('end - to - end neural model', (27, 34)), ('which spans', (37, 39)), ('entity mentions', (40, 42)), ('best cluster', (45, 47))]",[],[],[],[]
model,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,"[('over', (3, 4)), ('spans up to', (8, 11)), ('directly optimizes', (15, 17)), ('of', (20, 21)), ('from', (23, 24))]","[('Our model reasons', (0, 3)), ('maximum length', (12, 14)), ('marginal likelihood', (18, 20)), ('gold coreference clusters', (24, 27))]",[],[],[],[]
model,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .","[('includes', (1, 2)), ('decides', (8, 9)), ('for', (10, 11)), ('which of', (14, 16)), ('is', (23, 24))]","[('span - ranking model', (3, 7)), ('each span', (11, 13)), ('previous spans', (17, 19)), ('good antecedent', (25, 27))]",[],[],[],[]
model,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .","[('are', (6, 7)), ('representing', (9, 10)), ('in', (13, 14)), ('combine', (18, 19)), ('with', (24, 25)), ('over', (31, 32))]","[('vector embeddings', (7, 9)), ('spans of text', (10, 13)), ('document', (15, 16)), ('context - dependent boundary representations', (19, 24)), ('head - finding attention mechanism', (26, 31)), ('span', (33, 34))]",[],[],[],[]
model,The head - finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions .,"[('reveals', (7, 8)), ('contribute most to', (11, 14))]","[('head - finding attention mechanism', (1, 6)), ('mentioninternal words', (9, 11)), ('coreference decisions', (14, 16))]",[],[],[],[]
hyperparameters,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .","[('of', (6, 7))]","[('word embeddings area fixed concatenation', (1, 6)), ('300 - dimensional GloVe embeddings', (7, 12)), ('50 - dimensional embeddings', (13, 17)), ('normalized', (20, 21)), ('unit vectors', (23, 25))]",[],[],[],[]
hyperparameters,The hidden states in the LSTMs have 200 dimensions .,"[('in', (3, 4)), ('have', (6, 7))]","[('hidden states', (1, 3)), ('LSTMs', (5, 6)), ('200 dimensions', (7, 9))]",[],[],[],[]
hyperparameters,We encode speaker information as a binary feature indicating whether a pair of spans are from the same speaker .,"[('encode', (1, 2)), ('as', (4, 5)), ('indicating whether', (8, 10)), ('are', (14, 15)), ('from', (15, 16))]","[('speaker information', (2, 4)), ('binary feature', (6, 8)), ('pair of spans', (11, 14))]",[],[],[],[]
hyperparameters,"All features ( speaker , genre , span distance , mention width ) are represented as learned 20 - dimensional embeddings .","[('represented as', (14, 16))]","[('features ( speaker , genre , span distance , mention width )', (1, 13)), ('learned 20 - dimensional embeddings', (16, 21))]",[],[],[],[]
hyperparameters,"We prune the spans such that the maximum span width L = 10 , the number of spans per word ? = 0.4 , and the maximum number of antecedents K = 250 .","[('prune', (1, 2)), ('such that', (4, 6)), ('=', (11, 12))]","[('spans', (3, 4)), ('maximum span width L', (7, 11)), ('10', (12, 13)), ('number of spans per word ?', (15, 21)), ('0.4', (22, 23)), ('maximum number of antecedents K', (26, 31))]",[],[],[],[]
hyperparameters,We use ADAM for learning with a minibatch size of 1 .,"[('use', (1, 2)), ('for', (3, 4)), ('with', (5, 6)), ('of', (9, 10))]","[('ADAM', (2, 3)), ('learning', (4, 5)), ('minibatch size', (7, 9)), ('1', (10, 11))]",[],[],[],[]
hyperparameters,We apply 0.2 dropout to all hidden layers and feature embeddings .,"[('apply', (1, 2)), ('to', (4, 5))]","[('0.2 dropout', (2, 4)), ('all hidden layers and feature embeddings', (5, 11))]",[],[],[],[]
hyperparameters,The learning rate is decayed by 0.1 % every 100 steps .,"[('by', (5, 6)), ('every', (8, 9))]","[('learning rate', (1, 3)), ('decayed', (4, 5)), ('0.1 %', (6, 8)), ('100 steps', (9, 11))]",[],[],[],[]
hyperparameters,Ensembling is performed for both the span pruning and antecedent decisions .,"[('performed for both', (2, 5))]","[('Ensembling', (0, 1)), ('span pruning', (6, 8)), ('antecedent decisions', (9, 11))]",[],[],[],[]
results,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .","[('improves', (6, 7)), ('by', (17, 18)), ('improves', (26, 27)), ('by', (28, 29))]","[('our single model', (3, 6)), ('state - of - the - art average F1', (8, 17)), ('1.5', (18, 19)), ('our 5 - model ensemble', (21, 26)), ('3.1', (29, 30))]",[],[],[],[]
results,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .","[('come from', (4, 6)), ('in', (7, 8))]","[('most significant gains', (1, 4)), ('improvements', (6, 7)), ('recall', (8, 9))]",[],[],[],[]
ablation-analysis,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .","[('are', (9, 10)), ('for', (12, 13))]","[('distance between', (1, 3)), ('spans and the width of spans', (3, 9)), ('crucial signals', (10, 12)), ('coreference resolution', (13, 15))]",[],[],[],[]
ablation-analysis,"With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .","[('With', (0, 1)), ('see', (5, 6)), ('of', (8, 9)), ('suggesting', (12, 13))]","[('oracle mentions', (1, 3)), ('improvement', (7, 8)), ('17.5 F1', (9, 11))]",[],[],[],[]
ablation-analysis,Mention Precision,[],[],[],[],[],[]
ablation-analysis,"For spans with 2 - 5 words , 75 - 90 % of the predictions are constituents , indicating that the vast majority of the mentions are syntactically plausible .","[('For', (0, 1)), ('with', (2, 3)), ('of', (12, 13)), ('are', (15, 16))]","[('spans', (1, 2)), ('2 - 5 words', (3, 7)), ('75 - 90 %', (8, 12)), ('predictions', (14, 15)), ('constituents', (16, 17))]",[],[],[],[]
research-problem,BERT for Coreference Resolution : Baselines and Analysis,[],"[('Coreference Resolution', (2, 4))]",[],[],[],[]
research-problem,"We apply BERT to coreference resolution , achieving strong improvements on the OntoNotes ( + 3.9 F1 ) and GAP ( + 11.5 F1 ) benchmarks .","[('apply', (1, 2)), ('to', (3, 4)), ('achieving', (7, 8)), ('on', (10, 11))]","[('BERT', (2, 3)), ('coreference resolution', (4, 6)), ('strong improvements', (8, 10)), ('OntoNotes ( + 3.9 F1 ) and', (12, 19)), ('GAP ( + 11.5 F1 ) benchmarks', (19, 26))]",[],[],[],[]
model,We present two ways of extending the c 2f - coref model in .,"[('present', (1, 2)), ('of', (4, 5))]","[('two ways', (2, 4)), ('extending', (5, 6)), ('c 2f - coref model', (7, 12))]",[],[],[],[]
model,The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT .,"[('uses', (3, 4)), ('acts as', (9, 11)), ('for', (14, 15))]","[('independent variant', (1, 3)), ('non-overlapping segments', (4, 6)), ('independent instance', (12, 14)), ('BERT', (15, 16))]",[],[],[],[]
model,The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens .,"[('splits', (3, 4)), ('into', (6, 7)), ('so as to provide', (9, 13)), ('with', (15, 16)), ('beyond', (17, 18))]","[('overlap variant', (1, 3)), ('document', (5, 6)), ('overlapping segments', (7, 9)), ('model', (14, 15)), ('context', (16, 17)), ('512 tokens', (18, 20))]",[],[],[],[]
code,1 https://github.com/mandarjoshi90/coref,[],[],[],[],[],[]
experiments,We also find that BERT - large benefits from using longer context windows ( 384 word pieces ) while BERT - base performs better with shorter contexts ( 128 word pieces ) .,"[('find', (2, 3)), ('benefits from using', (7, 10)), ('performs', (22, 23)), ('with', (24, 25))]","[('BERT - large', (4, 7)), ('longer context windows ( 384 word pieces )', (10, 18)), ('BERT - base', (19, 22)), ('better', (23, 24)), ('shorter contexts ( 128 word pieces )', (25, 32))]",[],[],[],[]
baselines,We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,"[('extend', (1, 2)), ('of', (6, 7))]","[('original Tensorflow implementations', (3, 6)), ('c 2f - coref 3 and BERT', (7, 14))]",[],[],[],[]
hyperparameters,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .","[('fine tune', (1, 3)), ('on', (5, 6)), ('for', (10, 11)), ('using', (13, 14)), ('of', (16, 17)), ('of', (22, 23)), ('with', (31, 32)), ('for', (34, 35))]","[('all models', (3, 5)), ('OntoNotes English data', (7, 10)), ('20 epochs', (11, 13)), ('dropout', (15, 16)), ('0.3', (17, 18)), ('learning rates', (20, 22)), ('1 10 ?5 and 2 10 ? 4', (23, 31)), ('linear decay', (32, 34)), ('BERT parameters', (36, 38)), ('task parameters', (40, 42))]",[],[],[],[]
results,"We trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for BERT - base and BERT - large respectively .","[('trained', (1, 2)), ('with', (4, 5)), ('of', (8, 9)), ('trained on', (20, 22)), ('performed', (27, 28)), ('for', (30, 31))]","[('separate models', (2, 4)), ('max segment len', (5, 8)), ('128 , 256 , 384 , and 512', (9, 17)), ('128 and 384 word pieces', (22, 27)), ('best', (29, 30)), ('BERT - base', (31, 34))]",[],[],[],[]
ablation-analysis,"In addition to being more computationally efficient than e2e -coref , c2 f - coref iteratively refines span representations using attention for higher - order reasoning .","[('refines', (16, 17)), ('using', (19, 20)), ('for', (21, 22))]","[('span representations', (17, 19)), ('attention', (20, 21)), ('higher - order reasoning', (22, 26))]",[],[],[],[]
hyperparameters,GAP ) is a human - labeled corpus of ambiguous pronoun - name pairs derived from Wikipedia snippets .,"[('is', (2, 3)), ('of', (8, 9)), ('derived from', (14, 16))]","[('GAP', (0, 1)), ('human - labeled corpus', (4, 8)), ('ambiguous pronoun - name pairs', (9, 14)), ('Wikipedia snippets', (16, 18))]",[],[],[],[]
results,"Examples in the GAP dataset fit within a single BERT segment , thus eliminating the need for cross - segment inference .","[('in', (1, 2)), ('fit within', (5, 7)), ('eliminating', (13, 14))]","[('GAP dataset', (3, 5)), ('single BERT segment', (8, 11)), ('cross - segment inference', (17, 21))]",[],[],[],[]
experiments,"Following , we trained our BERT - based c 2f - coref model on OntoNotes .","[('trained', (3, 4)), ('on', (13, 14))]","[('BERT - based c 2f - coref model', (5, 13)), ('OntoNotes', (14, 15))]",[],[],[],[]
results,Table 2 shows that BERT improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .,"[('shows', (2, 3)), ('improves', (5, 6)), ('by', (11, 12)), ('for', (17, 18))]","[('BERT', (4, 5)), ('c 2 f - coref', (6, 11)), ('9 % and 11.5 %', (12, 17)), ('base and large models', (19, 23))]",[],[],[],[]
results,Document Level : OntoNotes,[],[],[],[],[],[]
results,shows that BERT - base offers an improvement of 0.9 % over the ELMo - based c2 fcoref model .,"[('shows', (0, 1)), ('offers', (5, 6)), ('of', (8, 9)), ('over', (11, 12))]","[('BERT - base', (2, 5)), ('improvement', (7, 8)), ('0.9 %', (9, 11)), ('ELMo - based c2 fcoref model', (13, 19))]",[],[],[],[]
experiments,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .","[('improves', (6, 7)), ('by', (12, 13)), ('of', (17, 18))]","[('BERT - large', (0, 3)), ('c 2 f - coref', (7, 12)), ('much larger margin', (14, 17)), ('3.9 %', (18, 20))]",[],[],[],[]
results,We also observe that the overlap variant offers no improvement over independent .,"[('observe', (2, 3)), ('offers', (7, 8)), ('over', (10, 11))]","[('overlap variant', (5, 7)), ('no improvement', (8, 10)), ('independent', (11, 12))]",[],[],[],[]
results,"Also concurrent , Span BERT , another self - supervised method , pretrains span representations achieving state of the art results ( Avg. F1 79.6 ) with the independent variant .","[('pretrains', (12, 13)), ('achieving', (15, 16)), ('with', (26, 27))]","[('Span BERT', (3, 5)), ('span representations', (13, 15)), ('state of the art results ( Avg. F1 79.6 )', (16, 26)), ('independent variant', (28, 30))]",[],[],[],[]
results,"BERT - large improves over BERT - base in a variety of ways including pronoun resolution and lexical matching ( e.g. , racetrack and track ) .","[('improves over', (3, 5)), ('including', (13, 14))]","[('BERT - large', (0, 3)), ('BERT - base', (5, 8)), ('pronoun resolution', (14, 16)), ('lexical matching', (17, 19))]",[],[],[],[]
experiments,Longer documents in OntoNotes generally contain larger and more spread - out clusters .,"[('in', (2, 3)), ('contain', (5, 6))]","[('Longer documents', (0, 2)), ('OntoNotes', (3, 4)), ('larger and more spread - out clusters', (6, 13))]",[],[],[],[]
experiments,These observations suggest that future research in pretraining methods should look at more effectively encoding document - level context using sparse representations .,"[('using', (19, 20))]","[('more effectively encoding document - level context', (12, 19)), ('sparse representations', (20, 22))]",[],[],[],[]
research-problem,A Hierarchical Model for Data - to - Text Generation,[],[],[],[],[],[]
research-problem,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .",[],"[('Transcribing structured data into natural language descriptions', (0, 7))]",[],[],[],[]
research-problem,Recent datato - text models mostly rely on an encoder - decoder architecture in which the data - structure is first encoded sequentially into a fixed - size vectorial representation by an encoder .,[],[],[],[],[],[]
model,"Then , a decoder generates words conditioned on this representation .","[('generates', (4, 5)), ('conditioned on', (6, 8))]","[('decoder', (3, 4)), ('words', (5, 6))]",[],[],[],[]
model,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .","[('propose', (6, 7)), ('assuming that', (13, 15)), ('should be', (16, 18))]","[('new structured - data encoder', (8, 13)), ('structures', (15, 16)), ('hierarchically captured', (18, 20))]",[],[],[],[]
model,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .","[('focuses on', (2, 4)), ('of', (6, 7)), ('chosen to be', (16, 19))]","[('encoding', (5, 6)), ('data - structure', (8, 11)), ('decoder', (14, 15)), ('classical module', (20, 22))]",[],[],[],[]
model,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .","[('model', (2, 3)), ('of', (6, 7)), ('using', (9, 10)), ('first encoding', (16, 18)), ('on the basis of', (20, 24)), ('then', (27, 28)), ('encoding', (28, 29)), ('on the basis of', (32, 36)), ('introduce', (41, 42)), ('in', (45, 46)), ('to ensure', (52, 54)), ('of', (56, 57)), ('in comparison to', (61, 64)), ('integrate', (75, 76)), ('to compute', (80, 82)), ('fed into', (85, 87))]","[('general structure', (4, 6)), ('two - level architecture', (11, 15)), ('all entities', (18, 20)), ('their elements', (24, 26)), ('data structure', (30, 32)), ('its entities', (36, 38)), ('Transformer encoder', (43, 45)), ('data - to - text models', (46, 52)), ('robust encoding', (54, 56)), ('each element / entities', (57, 61)), ('all others', (64, 66)), ('hierarchical attention mechanism', (77, 80)), ('hierarchical context', (83, 85)), ('decoder', (88, 89))]",[],[],[],[]
baselines,"Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .","[('is', (1, 2)), ('with', (7, 8)), ('first generated with', (15, 18)), ('replaced by', (22, 24)), ('extracted from', (26, 28)), ('by', (30, 31))]","[('Li', (0, 1)), ('standard encoder - decoder', (3, 7)), ('delayed copy mechanism', (9, 12)), ('text', (13, 14)), ('placeholders', (18, 19)), ('salient records', (24, 26)), ('table', (29, 30)), ('pointer network', (32, 34))]",[],[],[],[]
baselines,"It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .","[('consists in', (1, 3)), ('with', (9, 10)), ('aimed at updating', (13, 16)), ('during', (18, 19))]","[('standard encoder - decoder', (4, 8)), ('added module', (11, 13)), ('record representations', (16, 18)), ('generation process', (20, 22))]",[],[],[],[]
baselines,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .","[('At', (0, 1)), ('computes', (9, 10)), ('should', (12, 13))]","[('each decoding step', (1, 4)), ('gated recurrent network', (6, 9)), ('new representation', (20, 22))]",[],[],[],[]
hyperparameters,"For the encoder module , both the low - level and high - level encoders use a two - layers multi-head self - attention with two heads .","[('For', (0, 1)), ('use', (15, 16)), ('with', (24, 25))]","[('encoder module', (2, 4)), ('two - layers multi-head self - attention', (17, 24)), ('two heads', (25, 27))]",[],[],[],[]
hyperparameters,"To fit with the small number of record keys in our dataset , their embedding size is fixed to 20 .","[('To fit with', (0, 3)), ('in', (9, 10)), ('fixed to', (17, 19))]","[('small number of record keys', (4, 9)), ('our dataset', (10, 12)), ('embedding size', (14, 16)), ('20', (19, 20))]",[],[],[],[]
hyperparameters,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,"[('of', (2, 3)), ('of', (10, 11)), ('set to', (16, 18))]","[('size', (1, 2)), ('record value embeddings and hidden layers', (4, 10)), ('Transformer encoders', (12, 14)), ('300', (18, 19))]",[],[],[],[]
hyperparameters,We use dropout at rate 0.5 .,"[('use', (1, 2)), ('at', (3, 4))]","[('dropout', (2, 3)), ('rate 0.5', (4, 6))]",[],[],[],[]
hyperparameters,The models are trained with a batch size of 64 .,"[('trained with', (3, 5)), ('of', (8, 9))]","[('batch size', (6, 8)), ('64', (9, 10))]",[],[],[],[]
hyperparameters,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .","[('trained with', (3, 5)), ('is', (13, 14)), ('reduced by', (18, 20)), ('every', (21, 22))]","[('Adam optimizer', (6, 8)), ('initial learning rate', (10, 13)), ('0.001', (14, 15)), ('half', (20, 21)), ('10 K steps', (22, 25))]",[],[],[],[]
hyperparameters,We used beam search with beam size of 5 during inference .,"[('used', (1, 2)), ('with', (4, 5)), ('of', (7, 8)), ('during', (9, 10))]","[('beam search', (2, 4)), ('beam size', (5, 7)), ('5', (8, 9)), ('inference', (10, 11))]",[],[],[],[]
hyperparameters,All the models are implemented in Open NMT - py .,"[('implemented in', (4, 6))]","[('Open NMT - py', (6, 10))]",[],[],[],[]
code,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,[],[],[],[],[],[]
ablation-analysis,"Second , the comparison between scenario Hierarchical - kv and Hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted CS - R% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .","[('between', (4, 5)), ('shows', (12, 13)), ('omitting', (14, 15)), ('of', (18, 19)), ('in', (22, 23)), ('is', (26, 27)), ('excepted', (39, 40))]","[('comparison', (3, 4)), ('scenario Hierarchical - kv and Hierarchical -k', (5, 12)), ('entirely the influence', (15, 18)), ('record values', (20, 22)), ('attention mechanism', (24, 26)), ('more effective', (27, 29))]",[],[],[],[]
ablation-analysis,"Scores of Hierarchical -k are sharp , with all of the weight on the correct record ( PTS QTR1 , 26 ) whereas scores of Hierarchical - kv are more distributed over all PTS QTR records , ultimately failing to retrieve the correct one .","[('are', (4, 5)), ('with', (7, 8)), ('on', (12, 13)), ('distributed over', (30, 32))]","[('Scores of Hierarchical -k', (0, 4)), ('sharp', (5, 6)), ('all of the weight', (8, 12)), ('correct record', (14, 16)), ('scores of Hierarchical - kv', (23, 28)), ('more', (29, 30)), ('all PTS QTR records', (32, 36)), ('failing', (38, 39)), ('correct one', (42, 44))]",[],[],[],[]
ablation-analysis,over all models ; our best model Hierarchical -k reaching 17.5 vs. 16.5 against the best baseline .,"[('reaching', (9, 10)), ('against', (13, 14))]","[('our best model', (4, 7)), ('Hierarchical -k', (7, 9)), ('17.5 vs. 16.5', (10, 13)), ('best baseline', (15, 17))]",[],[],[],[]
ablation-analysis,"Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman , reinforcing the crucial role of structure in data semantics and saliency .","[('achieve', (3, 4)), ('on', (7, 8))]","[('Our hierarchical models', (0, 3)), ('significantly better scores', (4, 7)), ('all metrics', (8, 10)), ('flat architecture Wiseman', (14, 17))]",[],[],[],[]
ablation-analysis,"However , Wiseman achieves only 75 . 62 % of precision , effectively mentioning on average a total of 22.25 records ( wrong or accurate ) , where our model Hierarchical -k scores a precision of 89 . 46 % , leading to 23.66 total mentions , just slightly above Wiseman .","[('achieves', (3, 4)), ('of', (9, 10)), ('scores', (32, 33)), ('leading to', (41, 43))]","[('only 75 . 62 %', (4, 9)), ('precision', (10, 11))]",[],[],[],[]
ablation-analysis,This suggests that introducing the Transformer architecture is promising way to implicitly account for data structure .,"[('introducing', (3, 4)), ('is', (7, 8)), ('to implicitly account for', (10, 14))]","[('Transformer architecture', (5, 7)), ('promising way', (8, 10)), ('data structure', (14, 16))]",[],[],[],[]
ablation-analysis,"Our hierarchical models outperform the two - step decoders of Li and Puduppully - plan on both BLEU and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .","[('of', (9, 10)), ('on', (15, 16))]","[('Our hierarchical models', (0, 3)), ('outperform', (3, 4)), ('two - step decoders', (5, 9)), ('Li and Puduppully - plan', (10, 15)), ('BLEU and all qualitative metrics', (17, 22))]",[],[],[],[]
ablation-analysis,"While our models sensibly outperform in precision at factual mentions , the baseline Puduppully - plan reaches 34.28 mentions on average , showing that incorporating modules dedicated to entity extraction leads to over- focusing on entities ; contrasting with our models that learn to generate more balanced descriptions .","[('sensibly outperform', (3, 5)), ('at', (7, 8)), ('reaches', (16, 17))]","[('precision', (6, 7)), ('factual mentions', (8, 10)), ('baseline Puduppully - plan', (12, 16)), ('34.28 mentions', (17, 19))]",[],[],[],[]
ablation-analysis,The comparison with Puduppully - updt shows that dynamically updating the encoding across the generation process can lead to better Content Ordering ( CO ) and RG - P% .,"[('comparison', (1, 2)), ('with', (2, 3)), ('shows', (6, 7)), ('across', (12, 13)), ('lead to', (17, 19))]","[('Puduppully - updt', (3, 6)), ('dynamically updating', (8, 10)), ('encoding', (11, 12)), ('generation process', (14, 16)), ('better Content Ordering ( CO )', (19, 25))]",[],[],[],[]
ablation-analysis,"In contrast , our model encodes saliency among records / entities more effectively ( CS metric ) .","[('encodes', (5, 6)), ('among', (7, 8))]","[('saliency', (6, 7)), ('records / entities', (8, 11)), ('more effectively', (11, 13))]",[],[],[],[]
ablation-analysis,"In this work we have proposed a hierarchical encoder for structured data , which 1 ) leverages the structure to form efficient representation of its input ; 2 ) has strong synergy with the hierarchical attention of its associated decoder .","[('proposed', (5, 6)), ('for', (9, 10)), ('leverages', (16, 17)), ('to form', (19, 21)), ('of', (23, 24)), ('with', (32, 33)), ('of', (36, 37))]","[('hierarchical encoder', (7, 9)), ('structured data', (10, 12)), ('structure', (18, 19)), ('efficient representation', (21, 23)), ('its input', (24, 26)), ('strong synergy', (30, 32)), ('hierarchical attention', (34, 36)), ('its associated decoder', (37, 40))]",[],[],[],[]
research-problem,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation,[],[],[],[],[],[]
approach,Our work focuses on language generators whose inputs are structured meaning representations ( MRs ) .,"[('focuses on', (2, 4)), ('are', (8, 9))]","[('language generators', (4, 6)), ('inputs', (7, 8)), ('structured meaning representations ( MRs )', (9, 15))]",[],[],[],[]
approach,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .","[('present', (2, 3)), ('train and test on', (12, 16)), ('in', (20, 21))]","[('neural ensemble natural language generator', (4, 9)), ('three large unaligned datasets', (16, 20)), ('restaurant , television , and laptop domains', (22, 29))]",[],[],[],[]
experimental-setup,We built our ensemble model using the seq2seq framework for TensorFlow .,"[('built', (1, 2)), ('using', (5, 6)), ('for', (9, 10))]","[('our ensemble model', (2, 5)), ('seq2seq framework', (7, 9)), ('TensorFlow', (10, 11))]",[],[],[],[]
experimental-setup,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .","[('use', (4, 5)), ('with', (9, 10)), ('use', (19, 20))]","[('bidirectional LSTM encoder', (6, 9)), ('512 cells per layer', (10, 14)), ('CNN', (17, 18)), ('pooling encoder', (21, 23))]",[],[],[],[]
experimental-setup,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,"[('was', (5, 6)), ('with', (12, 13))]","[('decoder', (1, 2)), ('4 - layer RNN decoder', (7, 12)), ('512 LSTM cells per', (13, 17)), ('layer', (17, 18))]",[],[],[],[]
experimental-setup,"After experimenting with different beam search parameters , we settled on the beam width of 10 .","[('experimenting with', (1, 3)), ('settled on', (9, 11))]","[('different beam search parameters', (3, 7)), ('beam width of 10', (12, 16))]",[],[],[],[]
experiments,"The length penalty providing the best results on the E2E dataset was 0.6 , whereas for the TV and Laptop datasets it was 0.9 and 1.0 , respectively .","[('providing', (3, 4)), ('on', (7, 8)), ('was', (11, 12)), ('was', (22, 23))]","[('length penalty', (1, 3)), ('best results', (5, 7)), ('E2E dataset', (9, 11)), ('0.6', (12, 13)), ('TV and Laptop datasets', (17, 21)), ('0.9 and 1.0', (23, 26))]",[],[],[],[]
experiments,Experiments on the E2E Dataset,"[('on', (1, 2))]",[],[],[],[],[]
results,The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo - samples in the training set .,"[('show', (3, 4)), ('benefit from', (13, 15)), ('in', (19, 20))]","[('both the LSTM and the CNN models', (5, 12)), ('additional pseudo - samples', (15, 19)), ('training set', (21, 23))]",[],[],[],[]
results,Testing our ensembling approach reveals that reranking predictions pooled from different models produces an ensemble model that is over all more robust than the individual submodels .,"[('Testing', (0, 1)), ('reveals', (4, 5)), ('pooled from', (8, 10)), ('produces', (12, 13)), ('that is', (16, 18)), ('than', (22, 23))]","[('ensembling approach', (2, 4)), ('reranking predictions', (6, 8)), ('different models', (10, 12)), ('ensemble model', (14, 16)), ('over all more robust', (18, 22)), ('individual submodels', (24, 26))]",[],[],[],[]
results,"Analyzing the outputs , we also observed that the CNN model surpassed the two LSTM models in the ability to realize the "" fast food "" and "" pub "" values reliably , both of which were hardly present in the validation set but very frequent in the test set .","[('observed', (6, 7)), ('surpassed', (11, 12)), ('in the', (16, 18)), ('to realize', (19, 21))]","[('CNN model', (9, 11)), ('two LSTM models', (13, 16)), ('ability', (18, 19)), ('"" fast food "" and "" pub "" values', (22, 31)), ('reliably', (31, 32))]",[],[],[],[]
results,"We observe , however , that a hybrid ensemble model manages to perform the best in terms of the error rate , as well as the naturalness .","[('observe', (1, 2)), ('manages to perform', (10, 13)), ('in terms of', (15, 18)), ('as well as', (22, 25))]","[('hybrid ensemble model', (7, 10)), ('best', (14, 15)), ('error rate', (19, 21)), ('naturalness', (26, 27))]",[],[],[],[]
experiments,"As shows , our ensemble model performs competitively with the baseline on the TV dataset , and it outperforms it on the Laptop dataset by a wide margin .","[('performs', (6, 7)), ('with', (8, 9)), ('on', (11, 12)), ('on', (20, 21)), ('by', (24, 25))]","[('our ensemble model', (3, 6)), ('competitively', (7, 8)), ('baseline', (10, 11)), ('TV dataset', (13, 15)), ('outperforms', (18, 19)), ('Laptop dataset', (22, 24)), ('wide margin', (26, 28))]",[],[],[],[]
research-problem,Deep Graph Convolutional Encoders for Structured Data to Text Generation,[],[],[],[],[],[]
research-problem,Recent neural generation approaches build on encoder - decoder architectures proposed for machine translation .,[],"[('machine translation', (12, 14))]",[],[],[],[]
model,In this work we focus on two generation scenarios where the source data is graph structured .,"[('focus on', (4, 6)), ('where', (9, 10)), ('is', (13, 14))]","[('two generation scenarios', (6, 9)), ('source data', (11, 13)), ('graph structured', (14, 16))]",[],[],[],[]
research-problem,"One is the generation of multi-sentence descriptions of Knowledge Base ( KB ) entities from RDF graphs ) , namely the WebNLG task .","[('generation of', (3, 5)), ('namely', (19, 20))]","[('multi-sentence descriptions of Knowledge Base ( KB ) entities', (5, 14)), ('WebNLG task', (21, 23))]",[],[],[],[]
model,They rely on recurrent data encoders with memory and gating mechanisms ( LSTM ; ) .,"[('rely on', (1, 3)), ('with', (6, 7))]","[('recurrent data encoders', (3, 6)), ('memory and gating mechanisms', (7, 11))]",[],[],[],[]
model,"In this work , we compare with a model that explicitly encodes structure and is trained end - to - end .","[('explicitly encodes', (10, 12)), ('trained', (15, 16))]","[('structure', (12, 13)), ('end - to - end', (16, 21))]",[],[],[],[]
model,"Concretely , we use a Graph Convolutional Network ( GCN ; ) as our encoder .","[('use', (3, 4)), ('as', (12, 13))]","[('Graph Convolutional Network ( GCN ; )', (5, 12)), ('our encoder', (13, 15))]",[],[],[],[]
research-problem,"Formally , we address the task of text generation from graph - structured data considering as input a directed labeled graph X = ( V , E ) where V is a set of nodes and E is a set of edges between nodes in V .","[('considering as input', (14, 17)), ('where', (28, 29)), ('is', (30, 31)), ('between', (42, 43))]","[('text generation from graph - structured data', (7, 14)), ('directed', (18, 19)), ('V', (29, 30)), ('set', (32, 33)), ('nodes', (43, 44))]",[],[],[],[]
research-problem,The WebNLG task aims at the generation of entity descriptions from a set of RDF triples related to an entity of a given category .,[],"[('WebNLG', (1, 2))]",[],[],[],[]
baselines,Both take as input a linearised version of the source graph .,"[('take as input', (1, 4)), ('of', (7, 8))]","[('linearised version', (5, 7)), ('source graph', (9, 11))]",[],[],[],[]
hyperparameters,"For the WebNLG baseline , we use the linearis ation scripts provided by .","[('For', (0, 1)), ('use', (6, 7))]","[('WebNLG baseline', (2, 4)), ('linearis ation scripts', (8, 11))]",[],[],[],[]
results,We obtained the best results with an encoder with four GCN layers with residual connections .,"[('obtained', (1, 2)), ('with', (5, 6)), ('with', (8, 9)), ('with', (12, 13))]","[('best results', (3, 5)), ('encoder', (7, 8)), ('four GCN layers', (9, 12)), ('residual connections', (13, 15))]",[],[],[],[]
tasks,Encoder ( decoder ) embeddings and hidden dimensions were set to 300 .,"[('set to', (9, 11))]","[('Encoder ( decoder ) embeddings and hidden dimensions', (0, 8)), ('300', (11, 12))]",[],[],[],[]
results,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,"[('is', (3, 4)), ('than', (7, 8)), ('with', (10, 11)), ('of', (14, 15))]","[('GCN model', (1, 3)), ('more stable', (5, 7)), ('baseline', (9, 10)), ('standard deviation', (12, 14)), ('.004 vs . 010', (15, 19))]",[],[],[],[]
results,The GCN EC model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points ; and MELBOURNE by .014 BLEU points .,"[('that uses', (6, 8)), ('by', (19, 20)), ('by', (26, 27))]","[('GCN EC model', (1, 4)), ('outperforms', (4, 5)), ('PKUWRITER', (5, 6)), ('ensemble of 7 models', (9, 13)), ('further reinforcement learning step', (15, 19)), ('.047 BLEU points', (20, 23)), ('MELBOURNE', (25, 26)), ('.014 BLEU points', (27, 30))]",[],[],[],[]
baselines,GCN EC is behind ADAPT which relies on sub-word encoding .,"[('behind', (3, 4)), ('relies on', (6, 8))]","[('GCN EC', (0, 2)), ('ADAPT', (4, 5)), ('sub-word encoding', (8, 10))]",[],[],[],[]
results,SR11 Deep task,[],[],[],[],[],[]
ablation-analysis,In ( BLEU ) we report an ablation study on the impact of the number of layers and the type of skip connections on the WebNLG dataset .,"[('on', (9, 10)), ('on', (23, 24))]","[('impact of the number of layers and the type of skip connections', (11, 23)), ('WebNLG dataset', (25, 27))]",[],[],[],[]
ablation-analysis,The first thing we notice is the importance of skip connections between GCN layers .,"[('notice', (4, 5)), ('of', (8, 9)), ('between', (11, 12))]","[('importance', (7, 8)), ('skip connections', (9, 11)), ('GCN layers', (12, 14))]",[],[],[],[]
ablation-analysis,Residual and dense connections lead to similar results .,"[('lead to', (4, 6))]","[('Residual and dense connections', (0, 4)), ('similar results', (6, 8))]",[],[],[],[]
ablation-analysis,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .","[('produce', (9, 10)), ('than', (18, 19))]","[('Dense connections', (0, 2)), ('models', (10, 11)), ('bigger', (11, 12)), ('slightly less accurate', (14, 17)), ('residual connections', (19, 21))]",[],[],[],[]
research-problem,Pragmatically Informative Text Generation,[],[],[],[],[],[]
research-problem,We improve the informativeness of models for conditional text generation using techniques from computational pragmatics .,"[('improve', (1, 2)), ('for', (6, 7))]","[('informativeness', (3, 4)), ('conditional text generation', (7, 10))]",[],[],[],[]
model,"In this paper , we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations ) and summarization .","[('show', (5, 6))]","[('pragmatic reasoning', (7, 9))]",[],[],[],[]
model,Reconstructor - based pragmatic system ( S R 1 ),[],"[('Reconstructor - based pragmatic system', (0, 5))]",[],[],[],[]
experiments,"We also report two extractive baselines : Lead - 3 , which uses the first three sentences of the document as the summary , and Inputs , the concatenation of the extracted sentences used as inputs to our models ( i.e. , i ( 1 ) , . . . , i ( P ) ) .","[('report', (2, 3)), ('of', (29, 30)), ('used as', (33, 35)), ('to', (36, 37))]","[('two extractive baselines', (3, 6)), ('Lead - 3', (7, 10)), ('first three sentences', (14, 17)), ('Inputs', (25, 26)), ('concatenation', (28, 29)), ('extracted sentences', (31, 33)), ('inputs', (35, 36)), ('our models', (37, 39))]",[],[],[],[]
experiments,"The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .","[('obtain', (3, 4)), ('of', (5, 6)), ('in', (7, 8)), ('over', (13, 14)), ('with', (20, 21)), ('outperforming', (28, 29))]","[('pragmatic methods', (1, 3)), ('improvements', (4, 5)), ('0.2-0.5', (6, 7)), ('ROUGE scores', (8, 10)), ('0.2-1.8 METEOR', (11, 13)), ('base S 0 model', (15, 19)), ('distractor - based approach', (22, 26)), ('SD 1', (26, 28)), ('reconstructorbased approach S R 1', (30, 35))]",[],[],[],[]
research-problem,Data - to - Text Generation with Content Selection and Planning,[],"[('Data - to - Text Generation', (0, 6))]",[],[],[],[]
model,"In this paper , we address these shortcomings by explicitly modeling content selection and planning within a neural data - to - text architecture .","[('explicitly modeling', (9, 11)), ('within', (15, 16))]","[('content selection and planning', (11, 15)), ('neural data - to - text architecture', (17, 24))]",[],[],[],[]
hyperparameters,"For each stage , we utilize beam search to approximately obtain the best results .","[('utilize', (5, 6)), ('to approximately obtain', (8, 11))]","[('each stage', (1, 3)), ('beam search', (6, 8)), ('best results', (12, 14))]",[],[],[],[]
hyperparameters,Input feeding was employed for the text decoder .,"[('employed for', (3, 5))]","[('Input feeding', (0, 2)), ('text decoder', (6, 8))]",[],[],[],[]
hyperparameters,We applied dropout ) at a rate of 0.3 .,"[('applied', (1, 2)), ('at', (4, 5)), ('of', (7, 8))]","[('dropout', (2, 3)), ('rate', (6, 7)), ('0.3', (8, 9))]",[],[],[],[]
hyperparameters,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .","[('trained for', (2, 4)), ('with', (6, 7)), ('selected from', (22, 24))]","[('25 epochs', (4, 6)), ('Adagrad optimizer', (8, 10)), ('initial learning rate', (12, 15)), ('0.15', (16, 17)), ('learning rate decay', (18, 21)), ('{ 0.5 , 0.97 }', (24, 29)), ('batch size', (31, 33)), ('5', (34, 35))]",[],[],[],[]
hyperparameters,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .","[('For', (0, 1)), ('made use of', (5, 8)), ('set', (11, 12)), ('to', (15, 16))]","[('text decoding', (1, 3)), ('BPTT', (8, 9)), ('truncation size', (13, 15)), ('100', (16, 17))]",[],[],[],[]
hyperparameters,We set the beam size to 5 during inference .,"[('set', (1, 2)), ('to', (5, 6)), ('during', (7, 8))]","[('beam size', (3, 5)), ('5', (6, 7)), ('inference', (8, 9))]",[],[],[],[]
hyperparameters,All models are implemented in Open NMT - py .,"[('implemented in', (3, 5))]","[('Open NMT - py', (5, 9))]",[],[],[],[]
results,"As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .","[('improves upon', (6, 8))]","[('NCP', (5, 6)), ('vanilla encoderdecoder models ( ED + JC , ED + CC )', (8, 20))]",[],[],[],[]
results,"In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .","[('achieves', (4, 5)), ('with', (7, 8))]","[('NCP', (3, 4)), ('comparable scores', (5, 7)), ('joint or conditional copy mechanism', (9, 14))]",[],[],[],[]
results,"Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .","[('achieves', (5, 6)), ('in terms of', (13, 16))]","[('NCP + CC', (2, 5)), ('best content selection and content ordering scores', (6, 13)), ('BLEU', (16, 17))]",[],[],[],[]
results,"Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .","[('Compared to', (0, 2)), ('in', (6, 7)), ('achieve', (12, 13)), ('of', (16, 17)), ('in terms of', (20, 23)), ('improves', (30, 31))]","[('best reported system', (3, 6)), ('Wiseman et al.', (7, 10)), ('absolute improvement', (14, 16)), ('approximately 12 %', (17, 20)), ('relation generation', (23, 25)), ('content selection precision', (26, 29)), ('5 %', (32, 34)), ('recall', (35, 36)), ('15 %', (37, 39)), ('content ordering', (40, 42)), ('increases', (42, 43)), ('3 %', (44, 46)), ('BLEU', (48, 49)), ('1.5 points', (50, 52))]",[],[],[],[]
results,The results of the oracle system ( NCP + OR ) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output .,"[('of', (2, 3)), ('show', (11, 12)), ('correlate with', (19, 21)), ('in', (31, 32)), ('result in', (36, 38))]","[('oracle system ( NCP + OR )', (4, 11)), ('content selection and ordering', (13, 17)), ('quality of', (22, 24)), ('content plan', (25, 27)), ('any improvements', (29, 31)), ('our planning component', (32, 35)), ('better output', (38, 40))]",[],[],[],[]
results,"As far as the template - based system is concerned , we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics .","[('observe', (12, 13)), ('obtains', (15, 16)), ('scores high', (22, 24)), ('on', (24, 25))]","[('template - based system', (4, 8)), ('low BLEU and CS precision', (16, 21)), ('CS recall and RG metrics', (25, 30))]",[],[],[],[]
results,84.5 % of the records in NCP + CC are non-duplicates compared to who obtain 72.9 % showing that our model is less repetitive .,"[('of', (2, 3)), ('in', (5, 6)), ('are', (9, 10)), ('compared to', (11, 13)), ('showing', (17, 18))]","[('84.5 %', (0, 2)), ('records', (4, 5)), ('NCP + CC', (6, 9)), ('non-duplicates', (10, 11)), ('obtain', (14, 15)), ('72.9 %', (15, 17))]",[],[],[],[]
results,"We see in that content selection and planning individually contribute to performance improvements over the baseline ( ED + CC ) , and accuracy further increases when both components are taken into account .","[('individually', (8, 9)), ('contribute to', (9, 11)), ('over', (13, 14)), ('further', (24, 25)), ('when', (26, 27)), ('taken', (30, 31))]","[('content selection and planning', (4, 8)), ('performance improvements', (11, 13)), ('baseline ( ED + CC )', (15, 21)), ('accuracy', (23, 24)), ('increases', (25, 26)), ('both components', (27, 29))]",[],[],[],[]
results,"Compared to the full system ( NCP + CC ) , content selection precision and recall are higher ( by 4.5 % and 2 % , respectively ) as well as content ordering ( by 1.8 % ) .","[('Compared to', (0, 2)), ('by', (19, 20)), ('as well as', (28, 31)), ('by', (34, 35))]","[('full system ( NCP + CC )', (3, 10)), ('content selection precision and recall', (11, 16)), ('higher', (17, 18)), ('4.5 % and 2 %', (20, 25)), ('content ordering', (31, 33)), ('1.8 %', (35, 37))]",[],[],[],[]
results,"CS precision is higher than 85 % , CS recall is higher than 93 % , and CO higher than 84 % .","[('higher than', (3, 5)), ('higher than', (11, 13)), ('higher than', (18, 20))]","[('CS precision', (0, 2)), ('85 %', (5, 7)), ('CS recall', (8, 10)), ('93 %', (13, 15)), ('CO', (17, 18)), ('84 %', (20, 22))]",[],[],[],[]
results,"NCP achieves higher accuracy in all metrics including relation generation , content selection , content ordering , and BLEU compared to .","[('achieves', (1, 2)), ('in', (4, 5)), ('including', (7, 8))]","[('NCP', (0, 1)), ('higher accuracy', (2, 4)), ('all metrics', (5, 7)), ('relation generation', (8, 10)), ('content selection', (11, 13)), ('content ordering', (14, 16)), ('BLEU', (18, 19))]",[],[],[],[]
results,"We find that NCP + CC over all performs best , however there is a significant gap between automatically generated summaries and human - authored ones .","[('find', (1, 2)), ('performs', (8, 9))]","[('NCP + CC over all', (3, 8)), ('best', (9, 10))]",[],[],[],[]
research-problem,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation,[],[],[],[],[],[]
model,The system is given a set of RDF triplets describing facts ( entities and relations between them ) and has to produce a fluent text that is faithful to the facts .,"[('given', (3, 4)), ('describing', (9, 10)), ('faithful to', (27, 29))]","[('set of RDF triplets', (5, 9)), ('facts ( entities and relations', (10, 15)), ('fluent text', (23, 25)), ('facts', (30, 31))]",[],[],[],[]
model,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .","[('propose', (2, 3)), ('whose', (12, 13)), ('fed into', (15, 17))]","[('explicit , symbolic , text planning stage', (4, 11)), ('output', (13, 14)), ('neural generation system', (18, 21))]",[],[],[],[]
model,The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,"[('determines', (3, 4)), ('expresses it', (8, 10))]","[('text planner', (1, 3)), ('information structure', (5, 7)), ('unambiguously', (10, 11)), ('sequence of ordered trees', (17, 21))]",[],[],[],[]
model,This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts .,"[('performed', (3, 4)), ('guaranteed to', (7, 9)), ('with regards to', (13, 16))]","[('symbolically', (4, 5)), ('remain', (9, 10)), ('input facts', (17, 19))]",[],[],[],[]
code,We release our code and the corpus extended with matching plans in https://github.com/AmitMY/ chimera .,[],"[('https://github.com/AmitMY/ chimera', (12, 14))]",[],[],[],[]
hyperparameters,We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase .,"[('map', (1, 2)), ('of', (6, 7)), ('by splitting on', (8, 11))]","[('DBPedia relations to sequences', (2, 6)), ('tokens', (7, 8)), ('underscores', (11, 12)), ('CamelCase', (13, 14))]",[],[],[],[]
hyperparameters,"Concretely , we use the Open NMT toolkit with the copy attn flag .","[('use', (3, 4)), ('with', (8, 9))]","[('Open NMT toolkit', (5, 8)), ('copy attn flag', (10, 13))]",[],[],[],[]
hyperparameters,"The pretrained embeddings are used to initialize the relation tokens in the plans , as well as the tokens in the reference texts .","[('in', (10, 11)), ('as well as', (14, 17)), ('in', (19, 20))]","[('pretrained embeddings', (1, 3)), ('initialize', (6, 7)), ('relation tokens', (8, 10)), ('plans', (12, 13)), ('tokens', (18, 19)), ('reference texts', (21, 23))]",[],[],[],[]
baselines,"We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .","[('compare', (1, 2)), ('in', (6, 7)), ('on', (23, 24))]","[('best submissions', (4, 6)), ('WebNLG challenge', (8, 10)), ('Melbourne', (11, 12)), ('end - to - end system', (14, 20)), ('best', (22, 23)), ('all categories', (24, 26))]",[],[],[],[]
baselines,"It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .","[('uses', (1, 2)), ('with', (9, 10))]","[('set encoder', (3, 5)), ('LSTM decoder', (7, 9)), ('attention', (10, 11)), ('copy - attention mechanism', (13, 17)), ('neural checklist model', (19, 22)), ('entity dropout', (27, 29))]",[],[],[],[]
baselines,6 Experiments and Results,[],[],[],[],[],[]
results,"BestPlan reduces all error types compared to StrongNeural , by 85 % , 56 % and 90 % respectively .","[('reduces', (1, 2)), ('compared to', (5, 7)), ('by', (9, 10))]","[('BestPlan', (0, 1)), ('all error types', (2, 5)), ('StrongNeural', (7, 8)), ('85 % , 56 % and 90 %', (10, 18))]",[],[],[],[]
results,"BestPlan performed on - par with StrongNeural , and surpassed the previous state - of - the - art UPF - FORGe .","[('performed', (1, 2)), ('with', (5, 6)), ('surpassed', (9, 10))]","[('BestPlan', (0, 1)), ('on - par', (2, 5)), ('StrongNeural', (6, 7)), ('previous state - of - the - art UPF - FORGe', (11, 22))]",[],[],[],[]
research-problem,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation,[],[],[],[],[],[]
research-problem,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .",[],"[('natural language generation', (19, 22))]",[],[],[],[]
research-problem,"Sequence - to - sequence frameworks have proved to be very effective in natural language generation ( NLG ) tasks , as well as in machine translation and in language modeling .",[],"[('Sequence - to - sequence frameworks', (0, 6)), ('natural language generation ( NLG )', (13, 19))]",[],[],[],[]
model,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .","[('present', (15, 16)), ('with', (26, 27)), ('results in', (30, 32))]","[('character - level sequence - to - sequence model', (17, 26)), ('attention mechanism', (27, 29)), ('completely neural end - to - end architecture', (33, 41))]",[],[],[],[]
model,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .","[('does not require', (10, 13))]","[('delexicalization', (13, 14)), ('tokenization', (15, 16)), ('lowercasing', (17, 18))]",[],[],[],[]
model,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .","[('shows', (5, 6)), ('with respect to', (10, 13)), ('consisting in', (33, 35)), ('between', (38, 39)), ('disengages', (45, 46)), ('to learn', (48, 50)), ('improves', (67, 68)), ('enhancing', (73, 74))]","[('two important features', (6, 9)), ('state - of - art architecture', (14, 20)), ('character - wise copy mechanism', (27, 32)), ('soft switch', (36, 38)), ('generation and copy mode', (39, 43)), ('model', (47, 48)), ('rare and unhelpful self - correspondences', (50, 56)), ('peculiar training procedure', (62, 65)), ('internal representation capabilities', (69, 72)), ('recall', (74, 75))]",[],[],[],[]
experimental-setup,"We developed our system using the PyTorch framework 2 , release 0.4.1 3 .","[('developed', (1, 2)), ('using', (4, 5))]","[('our system', (2, 4)), ('PyTorch framework', (6, 8))]",[],[],[],[]
experimental-setup,"The training has been carried out as described in subsection 2.3 : this training procedure needs the two GRUs to have the same dimensions , in terms of input size , hidden size , number of layers and presence of a bias term .","[('to have', (19, 21)), ('in terms of', (25, 28))]","[('same dimensions', (22, 24)), ('input size', (28, 30)), ('hidden size', (31, 33)), ('number of layers', (34, 37)), ('presence of', (38, 40)), ('bias term', (41, 43))]",[],[],[],[]
ablation-analysis,"Moreover , they both have to be bidirectional , even if the decoder ignores the backward part of its current GRU .","[('have to be', (4, 7))]","[('bidirectional', (7, 8))]",[],[],[],[]
experimental-setup,"We minimize the negative log - likelihood loss using teacher forcing and Adam , the latter being an optimizer that computes individual adaptive learning rates .","[('minimize', (1, 2)), ('using', (8, 9)), ('computes', (20, 21))]","[('negative log - likelihood loss', (3, 8)), ('teacher forcing and Adam', (9, 13)), ('individual adaptive learning rates', (21, 25))]",[],[],[],[]
baselines,We also propose a new formulation of P ( c ) that helps the model to learn when it is necessary to start a copying phase :,"[('propose', (2, 3)), ('of', (6, 7))]","[('new formulation', (4, 6)), ('P ( c )', (7, 11)), ('model', (14, 15)), ('necessary', (20, 21)), ('copying phase', (24, 26))]",[],[],[],[]
baselines,"The second one is TGen , a word - based model , still derived from , but integrating a beam search mechanism and a reranker over the top k outputs , in order to dis advantage utterances that do not verbalize all the information contained in the MR .","[('integrating', (17, 18)), ('over', (25, 26)), ('to dis advantage', (33, 36)), ('that do not verbalize', (37, 41)), ('contained in', (44, 46))]","[('TGen', (4, 5)), ('beam search mechanism and a reranker', (19, 25)), ('top k outputs', (27, 30)), ('utterances', (36, 37)), ('all the information', (41, 44)), ('MR', (47, 48))]",[],[],[],[]
ablation-analysis,"We used the official code provided in the E2E NLG Challenge website for TGen , and we developed our models and EDA in PyTorch , training them on NVIDIA GPUs .","[('used', (1, 2)), ('provided in', (5, 7)), ('for', (12, 13)), ('developed', (17, 18)), ('in', (22, 23)), ('training them on', (25, 28))]","[('official code', (3, 5)), ('E2E NLG Challenge website', (8, 12)), ('TGen', (13, 14)), ('our models and EDA', (18, 22)), ('PyTorch', (23, 24)), ('NVIDIA GPUs', (28, 30))]",[],[],[],[]
results,"A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .","[('is', (4, 5)), ('always obtains', (9, 11)), ('with respect to', (14, 17)), ('on', (18, 19)), ('on', (33, 34))]","[('our model EDA_CS', (6, 9)), ('higher metric values', (11, 14)), ('TGen', (17, 18)), ('Hotel and Restaurant datasets', (20, 24)), ('three out of five higher metrics values', (26, 33)), ('E2E dataset', (35, 37))]",[],[],[],[]
results,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .","[('in the case of', (2, 6)), ('achieves', (10, 11))]","[('E2E +', (6, 8)), ('TGen', (9, 10)), ('three out of five higher metrics values', (11, 18))]",[],[],[],[]
results,"A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .","[('allows to obtain', (10, 13)), ('with respect to', (15, 18)), ('in', (20, 21)), ('on', (24, 25))]","[('approach', (7, 8)), ('better performance', (13, 15)), ('training', (18, 19)), ('EDA_CS', (19, 20)), ('standard way', (22, 24)), ('Hotel and Restaurant datasets', (26, 30)), ('outperforms', (43, 44)), ('EDA_CS', (44, 45)), ('one case', (47, 49))]",[],[],[],[]
results,"Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .","[('shows', (4, 5)), ('of', (8, 9)), ('with respect to', (13, 16)), ('when compared to', (19, 22))]","[('EDA_CS TL', (2, 4)), ('bleu increment', (6, 8)), ('at least 14 %', (9, 13)), (""TGen 's score"", (16, 19)), ('Hotel and Restaurant datasets', (23, 27))]",[],[],[],[]
results,"Finally , the baseline model , EDA , is largely outperformed by all other examined methods .","[('by', (11, 12))]","[('baseline model', (3, 5)), ('EDA', (6, 7)), ('largely outperformed', (9, 11)), ('all other examined methods', (12, 16))]",[],[],[],[]
results,"We highlight that EDA_CS 's model 's good results are achieved even if it consists in a fully end - to - end model which does not benefit from the delexicalizationrelexicalization procedure , differently from TGen .","[('highlight', (1, 2)), ('are', (9, 10))]","[(""EDA_CS 's model 's"", (3, 7)), ('good results', (7, 9)), ('achieved', (10, 11))]",[],[],[],[]
research-problem,An improved neural network model for joint POS tagging and dependency parsing,"[('for', (5, 6))]",[],[],[],[],[]
code,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,[],"[('https://github.com/datquocnguyen/jPTDP', (11, 12))]",[],[],[],[]
model,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .","[('present', (5, 6)), ('for jointly', (13, 15))]","[('POS tagging and dependency paring', (16, 21))]",[],[],[],[]
results,"As mentioned in Section 4 , our model generally outperforms j PTDP v1.0 with 2.5 + % LAS improvements on universal dependencies ( UD ) treebanks .","[('with', (13, 14)), ('on', (19, 20))]","[('j PTDP v1.0', (10, 13)), ('2.5 + % LAS improvements', (14, 19)), ('universal dependencies ( UD ) treebanks', (20, 26))]",[],[],[],[]
code,"Our model is released as jPTDP v2.0 , available at https://github.com/datquocnguyen/",[],[],[],[],[],[]
hyperparameters,Our jPTDP v 2.0 is implemented using DYNET v2.0 with a fixed random seed .,"[('implemented using', (5, 7)), ('with', (9, 10))]","[('DYNET v2.0', (7, 9)), ('fixed random seed', (11, 14))]",[],[],[],[]
hyperparameters,"Word embeddings are initialized either randomly or by pre-trained word vectors , while character and POS tag embeddings are randomly initialized .","[('initialized', (3, 4)), ('are', (18, 19))]","[('Word embeddings', (0, 2)), ('randomly', (5, 6)), ('pre-trained word vectors', (8, 11)), ('character and POS tag embeddings', (13, 18)), ('randomly initialized', (19, 21))]",[],[],[],[]
hyperparameters,"For learning character - level word embeddings , we use one - layer BiLSTM seq , and set the size of LSTM hidden states to be equal to the vector size of character embeddings .","[('For', (0, 1)), ('use', (9, 10)), ('set', (17, 18)), ('of', (20, 21)), ('to be equal to', (24, 28))]","[('learning character - level word embeddings', (1, 7)), ('one - layer BiLSTM seq', (10, 15)), ('size', (19, 20)), ('LSTM hidden states', (21, 24)), ('vector size of', (29, 32)), ('character embeddings', (32, 34))]",[],[],[],[]
hyperparameters,We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,"[('apply', (1, 2)), ('with', (3, 4)), ('to', (9, 10))]","[('dropout', (2, 3)), ('67 % keep probability', (5, 9)), ('inputs of BiLSTMs and MLPs', (11, 16))]",[],[],[],[]
hyperparameters,"Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .","[('apply', (5, 6)), ('to learn', (8, 10)), ('for', (12, 13)), ('replace', (17, 18)), ('in', (28, 29)), ('with', (32, 33)), ('with', (39, 40))]","[('word dropout', (6, 8)), ('embedding', (11, 12)), ('unknown words', (13, 15)), ('each word token', (18, 21)), ('appearing # ( w ) times', (22, 28)), ('training set', (30, 32)), ('special "" unk "" symbol', (34, 39)), ('probability punk ( w )', (40, 45))]",[],[],[],[]
hyperparameters,"We optimize the objective loss using Adam ( Kingma and Ba , 2014 ) with an initial learning rate at 0.001 and no mini-batches .","[('optimize', (1, 2)), ('using', (5, 6)), ('with', (14, 15)), ('at', (19, 20))]","[('objective loss', (3, 5)), ('Adam ( Kingma and Ba , 2014 )', (6, 14)), ('initial learning rate', (16, 19)), ('0.001', (20, 21))]",[],[],[],[]
experiments,"For training , we run for 30 epochs , and restart the Adam optimizer and anneal its initial learning rate at a proportion of 0.5 every 10 epochs .","[('For', (0, 1)), ('run for', (4, 6)), ('restart', (10, 11)), ('anneal', (15, 16)), ('at', (20, 21)), ('of', (23, 24)), ('every', (25, 26))]","[('training', (1, 2)), ('30 epochs', (6, 8)), ('Adam optimizer', (12, 14)), ('initial learning rate', (17, 20)), ('proportion', (22, 23)), ('0.5', (24, 25)), ('10 epochs', (26, 28))]",[],[],[],[]
hyperparameters,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .","[('use', (9, 10))]","[('100 - dimensional word embeddings', (10, 15)), ('50 - dimensional character embeddings', (16, 21)), ('100 dimensional POS tag embeddings', (22, 27))]",[],[],[],[]
hyperparameters,We also fix the number of hidden nodes in MLPs at 100 .,"[('fix', (2, 3)), ('in', (8, 9)), ('at', (10, 11))]","[('number of hidden nodes', (4, 8)), ('MLPs', (9, 10)), ('100', (11, 12))]",[],[],[],[]
hyperparameters,"Due to limited computational resource , for experiments presented in Section 3 , we perform a minimal grid search of hyper - parameters to select the number of BiLSTM pos and BiLSTM dep layers from { 1 , 2 } and the size of LSTM hidden states in each layer from { 128 , 256 } .","[('perform', (14, 15)), ('of', (19, 20)), ('to select', (23, 25)), ('from', (34, 35)), ('in', (47, 48)), ('from', (50, 51))]","[('minimal grid search', (16, 19)), ('hyper - parameters', (20, 23)), ('number of BiLSTM pos and BiLSTM dep layers', (26, 34)), ('{ 1 , 2 }', (35, 40)), ('size of LSTM hidden states', (42, 47)), ('each layer', (48, 50)), ('{ 128 , 256 }', (51, 56))]",[],[],[],[]
hyperparameters,"For experiments presented in sections 4 and 5 , we fix the number of BiLSTM layers at 2 and the size of hidden states at 128 .","[('fix', (10, 11)), ('at', (16, 17)), ('at', (24, 25))]","[('number of BiLSTM layers', (12, 16)), ('2', (17, 18)), ('size of', (20, 22)), ('hidden states', (22, 24)), ('128', (25, 26))]",[],[],[],[]
experiments,Word embeddings are initialized by 100 dimensional Glo Ve word vectors pre-trained on Wikipedia and Gigaword .,"[('initialized by', (3, 5)), ('pre-trained on', (11, 13))]","[('Word embeddings', (0, 2)), ('100 dimensional Glo Ve word vectors', (5, 11)), ('Wikipedia and Gigaword', (13, 16))]",[],[],[],[]
experiments,"As mentioned in Section 2.5 , we perform a minimal grid search of hyper - parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256 - dimensional LSTM hidden states ( in , we present scores obtained on the development set when using 2 BiLSTM layers ) .","[('perform', (7, 8)), ('of', (12, 13)), ('find that', (17, 19)), ('on', (23, 24)), ('obtained when using', (28, 31))]","[('minimal', (9, 10)), ('highest mixed accuracy', (20, 23)), ('development set', (25, 27)), ('2 BiLSTM layers', (31, 34))]",[],[],[],[]
experiments,"Clearly , our model produces very competitive parsing results .","[('produces', (4, 5))]","[('our model', (2, 4)), ('very competitive parsing results', (5, 9))]",[],[],[],[]
experiments,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .","[('obtains', (5, 6)), ('at', (9, 10))]","[('UAS score', (7, 9)), ('94.51 %', (10, 12)), ('LAS score', (14, 16)), ('92.87 %', (17, 19))]",[],[],[],[]
experiments,"Our model also does better than the previous transition - based joint models in , and , while obtaining similar UAS and LAS scores to the joint model JMT proposed by .","[('does', (3, 4)), ('than', (5, 6)), ('to', (24, 25))]","[('Our model', (0, 2)), ('better', (4, 5)), ('previous transition - based joint models', (7, 13)), ('similar UAS and LAS scores', (19, 24)), ('joint model JMT', (26, 29))]",[],[],[],[]
experiments,We achieve 0.9 % lower parsing scores than the state - of - the - art dependency parser of .,"[('achieve', (1, 2)), ('than', (7, 8))]","[('0.9 % lower parsing scores', (2, 7)), ('state - of - the - art dependency parser', (9, 18))]",[],[],[],[]
experiments,"While also a BiLSTM - and graph - based model , it uses a more sophisticated attention mechanism "" biaffine "" for better decoding dependency arcs and relation types .","[('uses', (12, 13)), ('for better decoding', (21, 24))]","[('BiLSTM - and graph - based model', (3, 10)), ('more sophisticated attention mechanism', (14, 18)), ('biaffine', (19, 20)), ('dependency arcs and relation types', (24, 29))]",[],[],[],[]
experiments,"In future work , we will extend our model with the biaffine attention mechanism to investigate the benefit for our model .","[('extend', (6, 7)), ('with', (9, 10)), ('to investigate', (14, 16)), ('for', (18, 19))]","[('our model', (7, 9)), ('biaffine attention mechanism', (11, 14)), ('benefit', (17, 18)), ('our model', (19, 21))]",[],[],[],[]
experiments,"We also obtain a state - of - the - art POS tagging accuracy at 97.97 % on the test Section 23 , which is about 0.4 + % higher than those by , and .","[('obtain', (2, 3)), ('at', (14, 15)), ('on', (17, 18))]","[('state - of - the - art', (4, 11)), ('POS tagging accuracy', (11, 14)), ('97.97 %', (15, 17)), ('test Section', (19, 21))]",[],[],[],[]
experiments,4 4 UniMelb in the CoNLL 2018 shared task on UD parsing,"[('on', (9, 10))]",[],[],[],[],[]
experiments,"For each big or small treebank , we train a joint model for universal POS tagging and dependency parsing , using a fixed random seed and a fixed set .","[('For', (0, 1)), ('train', (8, 9)), ('for', (12, 13)), ('using', (20, 21))]","[('each big or small treebank', (1, 6)), ('joint model', (10, 12)), ('universal POS tagging and dependency parsing', (13, 19)), ('fixed random seed', (22, 25))]",[],[],[],[]
experiments,"Here , we utilize the tokenization , word and sentence segmentation predicted by UD - Pipe 1.2 .","[('utilize', (3, 4)), ('predicted by', (11, 13))]","[('tokenization , word and sentence segmentation', (5, 11)), ('UD - Pipe', (13, 16))]",[],[],[],[]
experiments,The final test runs are carried out on the TIRA platform .,"[('carried out on', (5, 8))]","[('final test runs', (1, 4)), ('TIRA platform', (9, 11))]",[],[],[],[]
experiments,presents our results in the CoNLL 2018 shared task on multilingual parsing from raw texts to universal dependencies .,[],"[('multilingual parsing', (10, 12))]",[],[],[],[]
experiments,"Over all 82 test sets , we outperform the baseline UDPipe 1.2 with 0.6 % absolute higher average UPOS F1 score and 2.5 + % higher average UAS and LAS F1 scores .","[('Over', (0, 1)), ('with', (12, 13))]","[('all 82 test sets', (1, 5)), ('outperform', (7, 8)), ('baseline UDPipe 1.2', (9, 12)), ('0.6 % absolute higher average UPOS F1 score', (13, 21)), ('2.5 + % higher average UAS and LAS F1 scores', (22, 32))]",[],[],[],[]
experiments,"In particular , for the "" big "" category consisting of 61 treebank test sets , we obtain 0.8 % higher UPOS and 3.1 % higher UAS and 3.6 % higher LAS than UDPipe 1.2 .","[('for', (3, 4)), ('consisting of', (9, 11)), ('obtain', (17, 18)), ('than', (32, 33))]","[('"" big "" category', (5, 9)), ('61 treebank test sets', (11, 15)), ('0.8 % higher', (18, 21)), ('UPOS', (21, 22)), ('3.1 % higher', (23, 26)), ('UAS', (26, 27)), ('3.6 % higher LAS', (28, 32)), ('UDPipe 1.2', (33, 35))]",[],[],[],[]
experiments,Our ( UniMelb ) official LAS - based rank is at 14 th place while the baseline UDPipe 1.2 is at 18 th place over total 26 participating systems .,"[('is at', (9, 11)), ('is at', (19, 21)), ('over', (24, 25))]","[('Our ( UniMelb ) official LAS - based rank', (0, 9)), ('14 th place', (11, 14)), ('baseline UDPipe 1.2', (16, 19)), ('18 th place', (21, 24)), ('total 26 participating systems', (25, 29))]",[],[],[],[]
experiments,"In , we also present our average UPOS , UAS and LAS accuracies with respect to ( w.r.t. ) gold - standard tokenization , word and sentence segmentation .","[('present', (4, 5)), ('with respect to', (13, 16))]","[('gold - standard tokenization', (19, 23)), ('word and sentence segmentation', (24, 28))]",[],[],[],[]
experiments,"In particular , we achieved the highest F 1 scores for both biomedical event extraction and opinion analysis .","[('achieved', (4, 5)), ('for', (10, 11))]","[('highest F 1 scores', (6, 10)), ('biomedical event extraction', (12, 15)), ('opinion analysis', (16, 18))]",[],[],[],[]
research-problem,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,[],"[('Dependency Parsing', (3, 5))]",[],[],[],[]
research-problem,We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) .,"[('based on', (12, 14))]","[('dependency parsing', (8, 10))]",[],[],[],[]
model,"The focus of this paper is on feature representation for dependency parsing , using recent techniques from the neural - networks ( "" deep learning "" ) literature .",[],"[('feature representation for', (7, 10)), ('dependency parsing', (10, 12))]",[],[],[],[]
model,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .","[('centered around', (7, 9)), ('which are', (16, 18))]","[('BiRNNs', (9, 10)), ('BiLSTMs', (14, 15)), ('strong and trainable sequence models', (18, 23))]",[],[],[],[]
model,"The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an "" infinite "" window around it .","[('excels at', (2, 4)), ('representing', (4, 5)), ('in', (6, 7)), ('capturing', (19, 20))]","[('BiLSTM', (1, 2)), ('elements', (5, 6)), ('sequence ( i.e. , words )', (8, 14)), ('contexts', (17, 18)), ('element', (21, 22)), ('"" infinite "" window', (24, 28))]",[],[],[],[]
model,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .","[('represent', (1, 2)), ('by', (4, 5)), ('use', (10, 11)), ('of', (13, 14)), ('of', (17, 18)), ('as', (21, 22)), ('passed to', (29, 31))]","[('each word', (2, 4)), ('concatenation', (12, 13)), ('minimal set', (15, 17)), ('our feature function', (22, 25)), ('non-linear scoring function ( multi - layer perceptron )', (32, 41))]",[],[],[],[]
model,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .","[('In', (0, 1)), ('jointly train', (6, 8)), ('on top of', (13, 16)), ('propagating', (19, 20)), ('from', (21, 22))]","[('graphbased parser', (2, 4)), ('structured - prediction model', (9, 13)), ('BiLSTM', (17, 18)), ('errors', (20, 21)), ('structured objective', (23, 25)), ('BiLSTM feature - encoder', (31, 35))]",[],[],[],[]
results,"For Chinese , we use the Penn Chinese Treebank 5.1 ( CTB5 ) , using the train / test / dev splits of with gold partof - speech tags , also following .","[('use', (4, 5)), ('using', (14, 15)), ('of with', (22, 24))]","[('Chinese', (1, 2)), ('Penn Chinese Treebank 5.1 ( CTB5 )', (6, 13)), ('train / test / dev splits', (16, 22)), ('gold partof - speech tags', (24, 29))]",[],[],[],[]
hyperparameters,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .","[('implemented in', (3, 5)), ('using', (7, 8)), ('for', (12, 13))]","[('parsers', (1, 2)), ('python', (5, 6)), ('neural network training', (13, 16))]",[],[],[],[]
code,The code is available at the github repository https://github.com/elikip / bist -parser .,[],"[('https://github.com/elikip / bist -parser', (8, 12))]",[],[],[],[]
hyperparameters,"We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .","[('use', (1, 2)), ('implemented in', (5, 7)), ('optimize using', (10, 12))]","[('LSTM variant', (3, 5)), ('PyCNN', (7, 8)), ('Adam optimizer', (13, 15))]",[],[],[],[]
code,11 https://github.com/clab/cnn/tree/,[],[],[],[],[],[]
hyperparameters,The word and POS embeddings e ( w i ) and e ( p i ) are initialized to random values and trained together with the rest of the parsers ' networks .,"[('initialized to', (17, 19)), ('trained together with', (22, 25))]","[('word and POS embeddings e ( w i ) and e ( p i )', (1, 16)), ('random values', (19, 21)), (""rest of the parsers ' networks"", (26, 32))]",[],[],[],[]
results,"We train the parsers for up to 30 iterations , and choose the best model according to the UAS accuracy on the development set .","[('train', (1, 2)), ('for', (4, 5)), ('choose', (11, 12)), ('according to', (15, 17)), ('on', (20, 21))]","[('parsers', (3, 4)), ('up to 30 iterations', (5, 9)), ('best model', (13, 15)), ('UAS accuracy', (18, 20)), ('development set', (22, 24))]",[],[],[],[]
results,"When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .","[('When not using', (0, 3)), ('with', (14, 15)), ('thatare', (21, 22)), ('not using', (22, 24)), ('including', (27, 28))]","[('external embeddings', (3, 5)), ('first - order graph - based parser', (7, 14)), ('2 features', (15, 17)), ('outperforms', (17, 18)), ('all other systems', (18, 21)), ('external resources', (24, 26)), ('third - order TurboParser', (29, 33))]",[],[],[],[]
results,Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,"[('Moving from', (0, 2)), ('to', (8, 9)), ('leads to', (17, 19)), ('for', (23, 24))]","[('simple ( 4 features )', (3, 8)), ('extended ( 11 features ) feature set', (10, 17)), ('some gains in accuracy', (19, 23))]",[],[],[],[]
results,Dynamic oracle training yields nice gains for both English and Chinese .,"[('yields', (3, 4)), ('for', (6, 7))]","[('Dynamic oracle training', (0, 3)), ('nice gains', (4, 6))]",[],[],[],[]
research-problem,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,[],"[('Ensemble of Greedy Dependency Parsers', (2, 7))]",[],[],[],[]
model,"In 3 , we apply this idea to build a firstorder graph - based ( FOG ) ensemble parser ) that seeks consensus among 20 randomly - initialized stack LSTM parsers , achieving nearly the best - reported performance on the standard Penn Treebank Stanford dependencies task ( 94.51 UAS , 92.70 LAS ) .","[('to build', (7, 9)), ('seeks', (21, 22)), ('among', (23, 24)), ('achieving', (32, 33)), ('on', (39, 40))]","[('firstorder graph - based ( FOG ) ensemble parser', (10, 19)), ('consensus', (22, 23)), ('20 randomly - initialized stack LSTM parsers', (24, 31)), ('nearly the best - reported performance', (33, 39)), ('standard Penn Treebank Stanford dependencies task', (41, 47))]",[],[],[],[]
model,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .","[('distilling', (7, 8)), ('into', (10, 11)), ('with', (15, 16)), ('by defining', (18, 20))]","[('ensemble', (9, 10)), ('single FOG parser', (12, 15)), ('discriminative training', (16, 18)), ('new cost function', (21, 24))]",[],[],[],[]
model,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .","[('derive', (5, 6)), ('of', (8, 9)), ('from', (12, 13)), ('use', (21, 22)), ('in', (24, 25))]","[('cost', (7, 8)), ('each possible attachment', (9, 12)), (""ensemble 's division of votes"", (14, 19)), ('cost', (23, 24)), ('discriminative learning', (25, 27))]",[],[],[],[]
model,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .",[],"[('graphbased dependency parsing', (9, 12))]",[],[],[],[]
experiments,"Our ensembles of greedy , locally normalized parsers perform comparably to the best previously reported , due to , which uses a beam ( width 32 ) for training and decoding .","[('perform', (8, 9)), ('to', (10, 11)), ('for', (27, 28))]","[('Our ensembles of greedy , locally normalized parsers', (0, 8)), ('comparably', (9, 10)), ('training and decoding', (28, 31))]",[],[],[],[]
experiments,"3 . When the ensemble is confident , cost for its choice ( s ) is lower than it would be under Hamming cost - even when the ensemble is wrong .","[('for', (9, 10)), ('under', (21, 22))]","[('ensemble is', (4, 6)), ('confident', (6, 7)), ('cost', (8, 9)), ('its', (10, 11)), ('choice ( s )', (11, 15)), ('lower', (16, 17)), ('Hamming cost', (22, 24))]",[],[],[],[]
experiments,"Second , we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer .","[('apply', (3, 4)), ('of', (9, 10)), ('to', (11, 12))]","[('per-epoch learning rate decay', (5, 9)), ('0.05', (10, 11)), ('Adam optimizer', (13, 15))]",[],[],[],[]
experiments,"While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes , we find that this additional per-epoch decay consistently improves performance across all settings and languages .","[('automatically adjusts', (4, 6)), ('according to', (10, 12)), ('find', (17, 18)), ('consistently improves', (23, 25)), ('across', (26, 27))]","[('Adam optimizer', (2, 4)), ('global learning rate', (7, 10)), ('past gradient magnitudes', (12, 15)), ('additional per-epoch decay', (20, 23)), ('performance', (25, 26)), ('all settings and languages', (27, 31))]",[],[],[],[]
hyperparameters,We used the standard splits for all languages .,"[('used', (1, 2)), ('for', (5, 6))]","[('standard splits', (3, 5)), ('all languages', (6, 8))]",[],[],[],[]
hyperparameters,For German we use the predicted tags provided by the CoNLL 2009 shared task organizers .,"[('For', (0, 1)), ('use', (3, 4)), ('provided by', (7, 9))]","[('German', (1, 2)), ('predicted tags', (5, 7)), ('CoNLL 2009 shared task organizers', (10, 15))]",[],[],[],[]
hyperparameters,"All models were augmented with pretrained structured - skipgram embeddings ; for English we used the Gigaword corpus and 100 dimensions , for Chinese Gigaword and 80 , and for German WMT 2010 monolingual data and 64 .","[('augmented with', (3, 5)), ('used', (14, 15))]","[('pretrained structured - skipgram embeddings', (5, 10)), ('English', (12, 13)), ('Gigaword corpus and 100 dimensions', (16, 21)), ('German WMT 2010 monolingual data', (30, 35))]",[],[],[],[]
hyperparameters,For the Adam optimizer we use the default settings in the CNN neural network library .,"[('For', (0, 1)), ('use', (5, 6)), ('in', (9, 10))]","[('Adam optimizer', (2, 4)), ('default settings', (7, 9)), ('CNN neural network library', (11, 15))]",[],[],[],[]
results,"Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .","[('training', (2, 3)), ('with', (6, 7)), ('gives', (9, 10)), ('for', (12, 13))]","[('same model', (4, 6)), ('distillation cost', (7, 9)), ('consistent improvements', (10, 12)), ('all languages', (13, 15))]",[],[],[],[]
results,"The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS , compared to 93.6 UAS and 91.1 LAS for the model with distillation cost .","[('trained with', (2, 4)), ('achieved', (6, 7)), ('compared to', (13, 15)), ('for', (20, 21))]","[('model', (1, 2)), ('Hamming cost', (4, 6)), ('93.1 UAS and 90.9 LAS', (7, 12)), ('93.6 UAS and 91.1 LAS', (15, 20)), ('model', (22, 23)), ('distillation cost', (24, 26))]",[],[],[],[]
research-problem,From POS tagging to dependency parsing for biomedical event extraction,[],[],[],[],[],[]
code,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.,[],[],[],[],[],[]
experimental-setup,"For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .","[('For', (0, 1)), ('utilizes', (20, 21)), ('employ', (26, 27))]","[('three BiLSTM - CRF - based models', (2, 9)), ('Stanford - NNdep', (10, 13)), ('jPTDP', (14, 15)), ('Stanford - Biaffine', (16, 19)), ('pre-trained word embeddings', (21, 24)), ('200 dimensional pre-trained word vectors', (27, 32))]",[],[],[],[]
experimental-setup,"We perform a grid search of hyperparameters to select the number of BiLSTM layers from { 1 , 2 } and the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .","[('perform', (1, 2)), ('of', (5, 6)), ('to select', (7, 9)), ('from', (14, 15)), ('in', (26, 27)), ('from', (29, 30))]","[('grid search', (3, 5)), ('hyperparameters', (6, 7)), ('number of BiLSTM layers', (10, 14)), ('{ 1 , 2 }', (15, 20)), ('number of LSTM units', (22, 26)), ('each layer', (27, 29)), ('{ 100 , 150 , 200 , 250 , 300 }', (30, 41))]",[],[],[],[]
experimental-setup,Early stopping is applied when no performance improvement on the development set is obtained after 10 contiguous epochs .,"[('applied when', (3, 5)), ('on', (8, 9)), ('obtained after', (13, 15))]","[('Early stopping', (0, 2)), ('no performance improvement', (5, 8)), ('development set', (10, 12)), ('10 contiguous epochs', (15, 18))]",[],[],[],[]
experimental-setup,"For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .","[('For', (0, 1)), ('select', (6, 7)), ('from', (10, 11)), ('from', (23, 24)), ('fix', (40, 41)), ('with', (43, 44))]","[('Stanford - NNdep', (1, 4)), ('word CutOff', (8, 10)), ('{ 1 , 2 }', (11, 16)), ('size of the', (18, 21)), ('hidden layer', (21, 23)), ('{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }', (24, 39)), ('other hyperparameters', (41, 43)), ('default values', (45, 47))]",[],[],[],[]
experimental-setup,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .","[('For', (0, 1)), ('use', (4, 5)), ('fix', (11, 12)), ('at', (16, 17))]","[('jPTDP', (1, 2)), ('50 - dimensional character embeddings', (5, 10)), ('initial learning rate', (13, 16)), ('0.0005', (17, 18))]",[],[],[],[]
experimental-setup,"We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .","[('fix', (2, 3)), ('at', (8, 9)), ('select', (11, 12)), ('in', (17, 18)), ('from', (20, 21))]","[('number of BiLSTM layers', (4, 8)), ('2', (9, 10)), ('number of LSTM units', (13, 17)), ('each layer', (18, 20)), ('{ 100 , 150 , 200 , 250 , 300 }', (21, 32))]",[],[],[],[]
code,https://github.com/tdozat/Parser-v2,[],[],[],[],[],[]
experiments,Corpus - level accuracy differences of at least 0.17 % in GENIA and 0.26 % in CRAFT between two POS tagging models are significant at p ? 0.05 .,"[('of', (5, 6)), ('in', (10, 11)), ('between', (17, 18))]","[('Corpus - level accuracy differences', (0, 5)), ('at least 0.17 %', (6, 10)), ('GENIA', (11, 12)), ('0.26 %', (13, 15)), ('CRAFT', (16, 17)), ('two POS tagging models', (18, 22))]",[],[],[],[]
results,POS tagging results,[],[],[],[],[],[]
results,"In general , we find that the six retrained models produce competitive results .","[('find', (4, 5)), ('produce', (10, 11))]","[('six retrained models', (7, 10)), ('competitive results', (11, 13))]",[],[],[],[]
results,BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .,"[('obtains', (3, 4)), ('of', (5, 6)), ('on', (8, 9)), ('on', (15, 16))]","[('BiLSTM - CRF', (0, 3)), ('accuracies', (4, 5)), ('98.44 %', (6, 8)), ('GE - NIA', (9, 12)), ('97.25 %', (13, 15)), ('CRAFT', (16, 17))]",[],[],[],[]
results,"Using character - level word embeddings helps to produce about 0.5 % and Trained on the PTB sections 0 - 18 , the accuracies for the GENIA tagger , Stanford tagger , MarMoT , NLP4J - POS , BiLSTM- CRF and BiLSTM - CRF + CNN - char on the benchmark test set of PTB sections 22 - 24 were reported at 97.05 % , 97.23 % , 97.28 % , 97.64 % , 97.45 % and 97.55 % , respectively .","[('Using', (0, 1)), ('helps to produce', (6, 9)), ('Trained on', (13, 15)), ('for', (24, 25)), ('on', (48, 49)), ('of', (53, 54)), ('reported at', (60, 62))]","[('character - level word embeddings', (1, 6)), ('about 0.5 %', (9, 12)), ('accuracies', (23, 24)), ('GENIA tagger', (26, 28)), ('Stanford tagger', (29, 31)), ('MarMoT', (32, 33)), ('NLP4J - POS', (34, 37)), ('BiLSTM- CRF', (38, 40)), ('benchmark test set', (50, 53)), ('97.05 %', (62, 64))]",[],[],[],[]
results,"Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .","[('Note', (0, 1)), ('for', (2, 3)), ('provided', (14, 15)), ('to', (19, 20))]","[('PTB', (3, 4)), ('CNN - based character - level word embeddings', (5, 13)), ('0.1 % improvement', (16, 19)), ('BiLSTM - CRF', (20, 23))]",[],[],[],[]
results,"On GENIA , among pre-trained models , BLLIP obtains highest results .","[('On', (0, 1)), ('among', (3, 4)), ('obtains', (8, 9))]","[('GENIA', (1, 2)), ('pre-trained models', (4, 6)), ('BLLIP', (7, 8)), ('highest results', (9, 11))]",[],[],[],[]
results,Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .,"[('Note', (0, 1)), ('result in', (8, 10)), ('irrespective of', (14, 16)), ('of', (18, 19)), ('i.e.', (22, 23))]","[('pre-trained NNdep and Biaffine models', (3, 8)), ('no significant performance differences', (10, 14)), ('source', (17, 18)), ('POS tags', (19, 21))]",[],[],[],[]
results,"Regarding the retrained parsing models , on both GENIA and CRAFT , Stanford - Biaffine achieves the","[('Regarding', (0, 1)), ('on', (6, 7)), ('achieves', (15, 16))]","[('retrained parsing models', (2, 5)), ('GENIA and CRAFT', (8, 11)), ('Stanford - Biaffine', (12, 15))]",[],[],[],[]
results,"As expected , all parsers produce better results for shorter sentences on both corpora ; longer sentences are likely to have longer dependencies which are typically harder to predict precisely .","[('produce', (5, 6)), ('for', (8, 9)), ('on', (11, 12)), ('likely to have', (18, 21))]","[('all parsers', (3, 5)), ('better results', (6, 8)), ('shorter sentences', (9, 11)), ('both corpora', (12, 14)), ('longer sentences', (15, 17)), ('longer dependencies', (21, 23))]",[],[],[],[]
results,Impact of parsing on event extraction,"[('on', (3, 4))]","[('Impact of parsing', (0, 3))]",[],[],[],[]
results,"The results for parsers trained with the GENIA treebank ( Rows 1 - 6 , ) are generally higher than http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi","[('for', (2, 3)), ('trained with', (4, 6))]","[('parsers', (3, 4)), ('GENIA treebank', (7, 9))]",[],[],[],[]
results,"Among the four dependency parsers trained on GENIA , Stanford - Biaffine , jPTDP and NLP4J - dep produce similar event extraction scores on the development set , while on the the test set jPTDP and NLP4 Jdep obtain the lowest and highest scores , respectively .","[('Among', (0, 1)), ('trained on', (5, 7)), ('produce', (18, 19)), ('on', (23, 24)), ('on', (29, 30)), ('obtain', (38, 39))]","[('four dependency parsers', (2, 5)), ('GENIA', (7, 8)), ('Stanford - Biaffine', (9, 12)), ('jPTDP', (13, 14)), ('similar event extraction scores', (19, 23)), ('development set', (25, 27)), ('test set', (32, 34)), ('jPTDP and NLP4 Jdep', (34, 38)), ('lowest and highest scores', (40, 44))]",[],[],[],[]
research-problem,Stack - Pointer Networks for Dependency Parsing,[],[],[],[],[],[]
research-problem,We introduce a novel architecture for dependency parsing : stack - pointer networks ( STACKPTR ) .,[],"[('dependency parsing', (6, 8)), ('stack - pointer networks ( STACKPTR )', (9, 16))]",[],[],[],[]
research-problem,"Dependency parsing , which predicts the existence and type of linguistic dependency relations between words , is a first step towards deep language understanding .",[],"[('Dependency parsing', (0, 2))]",[],[],[],[]
model,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .","[('propose', (5, 6)), ('for', (11, 12))]","[('novel neural network architecture', (7, 11)), ('dependency parsing', (12, 14)), ('stackpointer networks ( STACKPTR )', (15, 20))]",[],[],[],[]
model,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .","[('as', (7, 8)), ('equipped with', (13, 15)), ('to maintain', (18, 20)), ('in', (25, 26))]","[('STACKPTR parser', (1, 3)), ('pointer network', (5, 7)), ('backbone', (9, 10)), ('internal stack', (16, 18)), ('order of head words', (21, 25)), ('tree structures', (26, 28))]",[],[],[],[]
model,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .","[('performs', (3, 4)), ('in', (5, 6)), ('generates', (21, 22)), ('by assigning', (24, 26)), ('at the top of', (31, 35))]","[('STACKPTR parser', (1, 3)), ('parsing', (4, 5)), ('incremental , topdown , depth - first fashion', (7, 15)), ('child', (27, 28)), ('headword', (30, 31)), ('internal stack', (36, 38))]",[],[],[],[]
model,"This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees , while maintaining a number of parsing steps linear in the sentence length .","[('from', (8, 9)), ('maintaining', (20, 21)), ('in', (27, 28))]","[('capture', (6, 7)), ('information', (7, 8)), ('whole sentence and all the previously derived subtrees', (10, 18)), ('number of parsing steps', (22, 26)), ('linear', (26, 27)), ('sentence length', (29, 31))]",[],[],[],[]
hyperparameters,"For all the parsing models in different languages , we initialize word vectors with pretrained word embeddings .","[('For', (0, 1)), ('in', (5, 6)), ('initialize', (10, 11)), ('with', (13, 14))]","[('all the parsing models', (1, 5)), ('different languages', (6, 8)), ('word vectors', (11, 13)), ('pretrained word embeddings', (14, 17))]",[],[],[],[]
hyperparameters,"For Chinese , Dutch , English , German and Spanish , we use the structured - skipgram embeddings .","[('For', (0, 1)), ('use', (12, 13))]","[('Chinese , Dutch , English , German and Spanish', (1, 10)), ('structured - skipgram embeddings', (14, 18))]",[],[],[],[]
hyperparameters,For other languages we use Polyglot embeddings .,"[('use', (4, 5))]","[('other languages', (1, 3)), ('Polyglot embeddings', (5, 7))]",[],[],[],[]
hyperparameters,Parameter optimization is performed with the Adam optimizer with ? 1 = ? 2 = 0.9 . We choose an initial learning rate of ? 0 = 0.001 .,"[('performed with', (3, 5)), ('with', (8, 9)), ('choose', (18, 19)), ('of', (23, 24))]","[('Parameter optimization', (0, 2)), ('Adam optimizer', (6, 8)), ('? 1 = ? 2 = 0.9', (9, 16)), ('initial learning rate', (20, 23)), ('? 0 = 0.001', (24, 28))]",[],[],[],[]
hyperparameters,The learning rate ?,[],"[('learning rate', (1, 3))]",[],[],[],[]
hyperparameters,"To reduce the effects of "" gradient exploding "" , we use gradient clipping of 5.0 .","[('To reduce', (0, 2)), ('of', (4, 5)), ('use', (11, 12)), ('of', (14, 15))]","[('effects', (3, 4)), ('"" gradient exploding ""', (5, 9)), ('gradient clipping', (12, 14)), ('5.0', (15, 16))]",[],[],[],[]
hyperparameters,"To mitigate overfitting , we apply dropout .","[('To mitigate', (0, 2)), ('apply', (5, 6))]","[('overfitting', (2, 3)), ('dropout', (6, 7))]",[],[],[],[]
hyperparameters,"For BLSTM , we use recurrent dropout with a drop rate of 0.33 between hidden states and 0.33 between layers .","[('For', (0, 1)), ('use', (4, 5)), ('with', (7, 8)), ('of', (11, 12)), ('between', (13, 14)), ('between', (18, 19))]","[('BLSTM', (1, 2)), ('recurrent dropout', (5, 7)), ('drop rate', (9, 11)), ('0.33', (12, 13)), ('hidden states', (14, 16)), ('0.33', (17, 18)), ('layers', (19, 20))]",[],[],[],[]
hyperparameters,"Following , we also use embedding dropout with a rate of 0.33 on all word , character , and POS embeddings .","[('use', (4, 5)), ('with', (7, 8)), ('of', (10, 11)), ('on', (12, 13))]","[('embedding dropout', (5, 7)), ('rate', (9, 10)), ('0.33', (11, 12)), ('all word , character , and POS embeddings', (13, 21))]",[],[],[],[]
results,"We compare the performance of four variations of our model with different decoder inputs - Org , + gpar , + sib and Full - where the Org model utilizes only the encoder hidden states of head words , while the + gpar and + sib models augments the original one with grandparent and sibling information , respectively .","[('with', (10, 11)), ('where', (25, 26)), ('utilizes', (29, 30)), ('of', (35, 36)), ('augments', (47, 48)), ('with', (51, 52))]","[('different decoder inputs', (11, 14)), ('Org , + gpar , + sib and Full', (15, 24)), ('Org model', (27, 29)), ('encoder hidden states', (32, 35)), ('head words', (36, 38)), ('+ gpar and + sib models', (41, 47)), ('original one', (49, 51)), ('grandparent and sibling information', (52, 56))]",[],[],[],[]
results,"An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .","[('achieves', (8, 9)), ('on', (12, 13)), ('performs', (18, 19)), ('than', (21, 22)), ('on', (24, 25))]","[('Full model', (6, 8)), ('best accuracy', (10, 12)), ('English and Chinese', (13, 16)), ('slightly worse', (19, 21)), ('+ sib', (22, 24)), ('German', (25, 26))]",[],[],[],[]
results,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .","[('On', (0, 1)), ('on', (9, 10)), ('showing', (13, 14)), ('on', (19, 20))]","[('LCM and UCM', (1, 4)), ('STACKPTR', (5, 6)), ('significantly outperforms', (6, 8)), ('BIAF', (8, 9)), ('all languages', (10, 12)), ('superiority', (15, 16)), ('complete sentence parsing', (20, 23))]",[],[],[],[]
results,The results of our parser on RA are slightly worse than BIAF .,"[('of', (2, 3)), ('on', (5, 6)), ('slightly worse than', (8, 11))]","[('results', (1, 2)), ('our parser', (3, 5)), ('RA', (6, 7)), ('BIAF', (11, 12))]",[],[],[],[]
results,"Our Full model significantly outperforms all the transition - based parsers on all three languages , and achieves better results than most graph - based parsers .","[('on', (11, 12)), ('achieves', (17, 18)), ('than', (20, 21))]","[('Our Full model', (0, 3)), ('significantly outperforms', (3, 5)), ('all the transition - based parsers', (5, 11)), ('better results', (18, 20)), ('most graph - based parsers', (21, 26))]",[],[],[],[]
results,"re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .","[('of', (1, 2)), ('obtains', (3, 4)), ('than', (6, 7))]","[('re-implementation', (0, 1)), ('BIAF', (2, 3)), ('better performance', (4, 6)), ('original one', (8, 10))]",[],[],[],[]
results,"Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .","[('achieves', (2, 3)), ('on', (11, 12)), ('on', (16, 17))]","[('Our model', (0, 2)), ('state - of - the - art performance', (3, 11)), ('UAS and LAS', (13, 16)), ('Chinese', (17, 18)), ('best UAS', (20, 22))]",[],[],[],[]
results,"Consistent with the analysis in , STACKPTR tends to perform better on shorter sentences , which make fewer parsing decisions , significantly reducing the chance of error propagation .","[('tends to perform', (7, 10)), ('on', (11, 12)), ('make', (16, 17)), ('significantly reducing', (21, 23))]","[('STACKPTR', (6, 7)), ('better', (10, 11)), ('shorter sentences', (12, 14)), ('fewer parsing decisions', (17, 20)), ('error propagation', (26, 28))]",[],[],[],[]
results,CoNLL,[],[],[],[],[],[]
results,"First , both BIAF and STACKPTR parsers achieve relatively high parsing accuracies on all the 12 languages - all with UAS are higher than 90 % .","[('achieve', (7, 8)), ('on', (12, 13)), ('higher than', (22, 24))]","[('BIAF and STACKPTR parsers', (3, 7)), ('relatively high parsing accuracies', (8, 12)), ('all the 12 languages', (13, 17)), ('UAS', (20, 21)), ('90 %', (24, 26))]",[],[],[],[]
results,"On nine languages - Catalan , Czech , Dutch , English , French , German , Norwegian , Russian and Spanish - STACKPTR outperforms BIAF for both UAS and LAS .","[('On', (0, 1)), ('outperforms', (23, 24)), ('for both', (25, 27))]","[('nine languages', (1, 3)), ('Catalan', (4, 5)), ('English', (10, 11)), ('STACKPTR', (22, 23)), ('BIAF', (24, 25)), ('UAS and LAS', (27, 30))]",[],[],[],[]
results,"On Bulgarian , STACKPTR achieves slightly better UAS while LAS is slightly worse than BIAF .","[('On', (0, 1)), ('achieves', (4, 5)), ('slightly worse than', (11, 14))]","[('Bulgarian', (1, 2)), ('STACKPTR', (3, 4)), ('slightly better UAS', (5, 8)), ('LAS', (9, 10)), ('BIAF', (14, 15))]",[],[],[],[]
results,"On Italian and Romanian , BIAF obtains marginally better parsing performance than STACKPTR .","[('On', (0, 1)), ('obtains', (6, 7)), ('than', (11, 12))]","[('Italian and Romanian', (1, 4)), ('BIAF', (5, 6)), ('marginally better parsing performance', (7, 11)), ('STACKPTR', (12, 13))]",[],[],[],[]
research-problem,Structured Training for Neural Network Transition - Based Parsing,[],[],[],[],[],[]
research-problem,Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention .,[],"[('Syntactic analysis', (0, 2))]",[],[],[],[]
model,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .","[('In', (0, 1)), ('processed in', (8, 10)), ('choose', (25, 26)), ('defined by', (32, 34))]","[('transition - based parsing', (1, 5)), ('sentences', (6, 7)), ('linear left to right pass', (11, 16))]",[],[],[],[]
model,"Furthermore , because the neural network uses a distributed representation , it is able to model lexical , part - of - speech ( POS ) tag , and arc label similarities in a continuous space .","[('able to model', (13, 16)), ('in', (32, 33))]","[('lexical', (16, 17)), ('part - of - speech ( POS ) tag', (18, 27)), ('arc label similarities', (29, 32)), ('continuous space', (34, 36))]",[],[],[],[]
model,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .","[('combine', (5, 6)), ('of', (9, 10)), ('with', (12, 13)), ('enabled by', (16, 18)), ('making', (23, 24))]","[('representational power', (7, 9)), ('neural networks', (10, 12)), ('superior search', (14, 16)), ('structured training and inference', (18, 22)), ('our parser', (24, 26))]",[],[],[],[]
research-problem,"In addition , by incorporating unlabeled data into training , we further improve the accuracy of our model to 94.26 % UAS / 92.41 % LAS ( 93.46 % UAS / 91.49 % LAS for our greedy model ) .","[('incorporating', (4, 5)), ('into', (7, 8)), ('further improve', (11, 13)), ('of', (15, 16)), ('to', (18, 19))]","[('unlabeled data', (5, 7)), ('training', (8, 9)), ('accuracy', (14, 15)), ('our model', (16, 18)), ('94.26 % UAS / 92.41 % LAS', (19, 26))]",[],[],[],[]
model,"In our approach we start with the basic structure of , but with a deeper architecture and improvements to the optimization procedure .","[('start with', (4, 6)), ('with', (12, 13)), ('to', (18, 19))]","[('basic structure', (7, 9)), ('deeper architecture', (14, 16)), ('improvements', (17, 18)), ('optimization procedure', (20, 22))]",[],[],[],[]
model,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .","[('use', (3, 4)), ('from', (6, 7)), ('of', (9, 10)), ('as', (13, 14)), ('in', (16, 17)), ('trained with', (23, 25))]","[('activations', (5, 6)), ('all layers', (7, 9)), ('neural network', (11, 13)), ('representation', (15, 16)), ('structured perceptron model', (18, 21)), ('beam search and early updates', (25, 30))]",[],[],[],[]
model,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .","[('generate', (5, 6)), ('of', (8, 9)), ('by parsing', (14, 16)), ('with', (18, 19)), ('selecting', (23, 24)), ('produced', (32, 33))]","[('large quantities', (6, 8)), ('high - confidence parse trees', (9, 14)), ('unlabeled data', (16, 18)), ('two different parsers', (19, 22)), ('sentences', (26, 27)), ('two parsers', (30, 32)), ('same trees', (34, 36))]",[],[],[],[]
model,To this end we generate large quantities of high - confidence parse trees by parsing an unlabeled corpus and selecting only the sentences on which two different parsers produced the same parse trees .,"[('generate', (4, 5)), ('of', (7, 8)), ('by parsing', (13, 15)), ('selecting', (19, 20)), ('on', (23, 24)), ('produced', (28, 29))]","[('large quantities', (5, 7)), ('high - confidence parse trees', (8, 13)), ('unlabeled corpus', (16, 18)), ('sentences', (22, 23)), ('two different parsers', (25, 28)), ('same parse trees', (30, 33))]",[],[],[],[]
model,"This idea comes from tri-training and while applicable to other parsers as well , we show that it benefits neural network parsers more than models with discrete features .","[('comes from', (2, 4)), ('applicable to', (7, 9)), ('show', (15, 16)), ('benefits', (18, 19)), ('more than', (22, 24))]","[('tri-training', (4, 5)), ('neural network parsers', (19, 22)), ('models with', (24, 26)), ('discrete features', (26, 28))]",[],[],[],[]
results,"We use a CRF - based POS tagger to generate 5 fold jack - knifed POS tags on the training set and predicted tags on the dev , test and tune sets ; our tagger gets comparable accuracy to the Stanford POS tagger with 97 . 44 % on the test set .","[('use', (1, 2)), ('to generate', (8, 10)), ('on', (17, 18)), ('on', (24, 25)), ('gets', (35, 36)), ('with', (43, 44)), ('on', (48, 49))]","[('CRF - based POS tagger', (3, 8)), ('5 fold jack - knifed POS tags', (10, 17)), ('training set and predicted tags', (19, 24)), ('dev , test and tune sets', (26, 32)), ('our', (33, 34)), ('comparable accuracy', (36, 38)), ('Stanford POS tagger', (40, 43)), ('97 . 44 %', (44, 48))]",[],[],[],[]
baselines,We train on the union of each corpora 's training set and test on each domain separately .,"[('train on', (1, 3)), ('of', (5, 6)), ('test on', (12, 14))]","[('union', (4, 5)), (""each corpora 's training set"", (6, 11)), ('each domain', (14, 16))]",[],[],[],[]
baselines,"We process it with the Berkeley - Parser , a latent variable constituency parser , and a reimplementation of ZPar , a transition - based parser with beam search .","[('process it with', (1, 4)), ('reimplementation', (17, 18)), ('of', (18, 19)), ('with', (26, 27))]","[('Berkeley - Parser', (5, 8)), ('latent variable constituency parser', (10, 14)), ('ZPar', (19, 20)), ('transition - based parser', (22, 26)), ('beam search', (27, 29))]",[],[],[],[]
hyperparameters,randomly using a Gaussian distribution with variance 10 ?4 .,"[('randomly using', (0, 2)), ('with', (5, 6))]","[('Gaussian distribution', (3, 5)), ('variance 10 ?4', (6, 9))]",[],[],[],[]
hyperparameters,"We used fixed initialization with bi = 0.2 , to ensure that most Relu units are activated during the initial rounds of training .","[('used', (1, 2)), ('with', (4, 5)), ('to ensure', (9, 11)), ('during', (17, 18))]","[('fixed initialization', (2, 4)), ('bi = 0.2', (5, 8)), ('most Relu units', (12, 15)), ('activated', (16, 17)), ('initial rounds of training', (19, 23))]",[],[],[],[]
hyperparameters,"For the word embedding matrix E word , we initialized the parameters using pretrained word embeddings .","[('For', (0, 1)), ('initialized', (9, 10)), ('using', (12, 13))]","[('word embedding matrix E word', (2, 7)), ('parameters', (11, 12)), ('pretrained word embeddings', (13, 16))]",[],[],[],[]
hyperparameters,"For words not appearing in the unsupervised data and the special "" NULL "" etc. tokens , we used random initialization .","[('For', (0, 1)), ('not appearing in', (2, 5)), ('used', (18, 19))]","[('words', (1, 2)), ('unsupervised data', (6, 8)), ('random initialization', (19, 21))]",[],[],[],[]
hyperparameters,All hyperparameters ( including structure ) were tuned using Section 24 of the WSJ only .,"[('tuned using', (7, 9)), ('of', (11, 12))]","[('Section 24', (9, 11)), ('WSJ', (13, 14))]",[],[],[],[]
hyperparameters,"When not tri-training , we used hyperparameters of ? = 0.2 , ? 0 = 0.05 , = 0.9 , early stopping after roughly 16 hours of training time .","[('used', (5, 6)), ('of', (7, 8)), ('after', (22, 23)), ('of', (26, 27))]","[('not', (1, 2)), ('tri-training', (2, 3)), ('hyperparameters', (6, 7)), ('? = 0.2 , ? 0 = 0.05 , = 0.9', (8, 19)), ('early stopping', (20, 22)), ('roughly 16 hours', (23, 26)), ('training time', (27, 29))]",[],[],[],[]
hyperparameters,"For the Treebank Union setup , we set M 1 = M 2 = 1024 for the standard training set and for the tri-training setup .","[('For', (0, 1)), ('set', (7, 8)), ('for', (15, 16))]","[('Treebank Union setup', (2, 5)), ('M 1 = M 2 = 1024', (8, 15)), ('standard training set', (17, 20)), ('tri-training setup', (23, 25))]",[],[],[],[]
baselines,We compare to the best dependency parsers in the literature .,"[('compare to', (1, 3))]","[('best dependency parsers', (4, 7))]",[],[],[],[]
results,"On the WSJ and Web tasks , our parser outperforms all dependency parsers in our comparison by a substantial margin .","[('On', (0, 1)), ('outperforms', (9, 10)), ('in', (13, 14)), ('by', (16, 17))]","[('WSJ and Web tasks', (2, 6)), ('our parser', (7, 9)), ('all dependency parsers', (10, 13)), ('our comparison', (14, 16)), ('substantial margin', (18, 20))]",[],[],[],[]
results,"The Question ( QTB ) dataset is more sensitive to the smaller beam size we use in order to train the models in a reasonable time ; if we increase to B = 32 at inference time only , our perceptron performance goes up to 92.29 % LAS .","[('more sensitive to', (7, 10)), ('to train', (18, 20)), ('in', (22, 23)), ('increase to', (29, 31)), ('at', (34, 35)), ('goes up to', (42, 45))]","[('Question ( QTB ) dataset', (1, 6)), ('smaller beam size', (11, 14)), ('models', (21, 22)), ('reasonable time', (24, 26)), ('B = 32', (31, 34)), ('inference time only', (35, 38)), ('our perceptron performance', (39, 42)), ('92.29 % LAS', (45, 48))]",[],[],[],[]
results,"Although tritraining did help the baseline on the dev set , test set performance did not improve significantly .","[('did help', (2, 4)), ('on', (6, 7))]","[('tritraining', (1, 2)), ('baseline', (5, 6)), ('dev set', (8, 10)), ('test set performance', (11, 14)), ('did not improve significantly', (14, 18))]",[],[],[],[]
results,"As expected , tri-training helps most dramatically to increase accuracy on the Treebank Union setup with diverse domains , yielding 0.4 - 1.0 % absolute LAS improvement gains for our most accurate model .","[('helps', (4, 5)), ('to', (7, 8)), ('on', (10, 11)), ('with', (15, 16)), ('yielding', (19, 20)), ('for', (28, 29))]","[('tri-training', (3, 4)), ('most dramatically', (5, 7)), ('increase', (8, 9)), ('accuracy', (9, 10)), ('Treebank Union setup', (12, 15)), ('diverse domains', (16, 18)), ('0.4 - 1.0 % absolute LAS improvement gains', (20, 28)), ('our most accurate model', (29, 33))]",[],[],[],[]
results,"While adding a second hidden layer results in a large gain on the tune set , there is no gain on the dev set if pre-trained embeddings are not used .","[('adding', (1, 2)), ('results in', (6, 8)), ('on', (11, 12)), ('on', (20, 21)), ('if', (24, 25))]","[('second hidden layer', (3, 6)), ('large gain', (9, 11)), ('tune set', (13, 15)), ('dev set', (22, 24)), ('not used', (28, 30))]",[],[],[],[]
results,"For our neural network model , training on the output of the BerkeleyParser yields only modest gains , while training on the data where the two parsers agree produces significantly better results .","[('For', (0, 1)), ('training on', (6, 8)), ('of', (10, 11)), ('yields', (13, 14)), ('training on', (19, 21)), ('where', (23, 24)), ('produces', (28, 29))]","[('our neural network model', (1, 5)), ('output', (9, 10)), ('modest gains', (15, 17)), ('data', (22, 23)), ('two parsers agree', (25, 28)), ('significantly better results', (29, 32))]",[],[],[],[]
results,"This was especially pronounced for the greedy models : after tri-training , the greedy neural network model surpasses the BerkeleyParser in accuracy .","[('after', (9, 10)), ('surpasses', (17, 18)), ('in', (20, 21))]","[('greedy models', (6, 8)), ('tri-training', (10, 11)), ('greedy neural network model', (13, 17)), ('BerkeleyParser', (19, 20)), ('accuracy', (21, 22))]",[],[],[],[]
results,It is also interesting to note that up - training improved results far more than tri-training for the baseline .,"[('improved', (10, 11)), ('far more than', (12, 15)), ('for', (16, 17))]","[('up - training', (7, 10)), ('results', (11, 12)), ('tri-training', (15, 16)), ('baseline', (18, 19))]",[],[],[],[]
ablation-analysis,"Regardless of tri-training , using the structured perceptron improved error rates on some of the common and difficult labels : ROOT , ccomp , cc , conj , and nsubj all improved by > 1 % .","[('using', (4, 5)), ('improved', (8, 9)), ('on', (11, 12)), ('improved by', (31, 33))]","[('structured perceptron', (6, 8)), ('error rates', (9, 11)), ('some of the common and difficult labels', (12, 19)), ('ROOT', (20, 21)), ('> 1 %', (33, 36))]",[],[],[],[]
research-problem,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING,[],[],[],[],[],[]
model,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .","[('modify', (1, 2)), ('to achieve', (13, 15)), ('build', (19, 20)), ('uses', (26, 27)), ('replace', (31, 32)), ('affine', (40, 41)), ('with', (43, 44)), ('that reduce', (70, 72))]","[('neural graphbased approach', (3, 6)), ('few', (11, 12)), ('competitive performance', (15, 17)), ('network', (21, 22)), ('larger', (24, 25)), ('more regularization', (27, 29)), ('traditional MLP - based attention mechanism', (33, 39)), ('label classifier', (41, 43)), ('biaffine ones', (44, 46)), ('MLP operations', (68, 70)), ('dimensionality', (73, 74))]",[],[],[],[]
model,The resulting parser maintains most of the simplicity of neural graph - based approaches while approaching the performance of the SOTA transition - based one .,"[('maintains', (3, 4)), ('of', (8, 9)), ('of', (18, 19))]","[('resulting parser', (1, 3)), ('most of the simplicity', (4, 8)), ('neural graph - based approaches', (9, 14)), ('performance', (17, 18)), ('SOTA transition - based one', (20, 25))]",[],[],[],[]
model,"We call this a deep bilinear attention mechanism , as opposed to shallow bilinear attention , which uses the recurrent states directly .","[('call', (1, 2)), ('as', (9, 10)), ('opposed to', (10, 12)), ('uses', (17, 18))]","[('deep bilinear attention mechanism', (4, 8)), ('shallow bilinear attention', (12, 15)), ('recurrent states', (19, 21))]",[],[],[],[]
model,We use 100 - dimensional uncased word vectors 2 and POS tag vectors ; three BiLSTM layers ( 400 dimensions in each direction ) ; and 500 - and 100 - dimensional ReLU MLP layers .,"[('use', (1, 2))]","[('100 - dimensional uncased word vectors', (2, 8)), ('POS tag vectors', (10, 13)), ('three BiLSTM layers', (14, 17)), ('500 - and 100 - dimensional ReLU MLP layers', (26, 35))]",[],[],[],[]
model,"We also apply dropout at every stage of the model : we drop words and tags ( independently ) ; we drop nodes in the LSTM layers ( input and recurrent connections ) , applying the same dropout mask at every recurrent timestep ( cf. the Bayesian dropout of ) ; and we drop nodes in the MLP layers and classifiers , likewise applying the same dropout mask at every timestep .","[('apply', (2, 3)), ('at', (4, 5)), ('of', (7, 8)), ('drop', (21, 22)), ('in', (23, 24)), ('applying', (34, 35)), ('at', (39, 40)), ('in', (55, 56))]","[('dropout', (3, 4)), ('every stage', (5, 7)), ('nodes', (22, 23)), ('LSTM layers (', (25, 28)), ('same dropout mask', (36, 39)), ('nodes', (54, 55)), ('MLP layers and classifiers', (57, 61))]",[],[],[],[]
model,"We optimize the network with annealed Adam for about 50,000 steps , rounded up to the nearest epoch .","[('optimize', (1, 2)), ('with', (4, 5)), ('for', (7, 8)), ('rounded up to', (12, 15))]","[('network', (3, 4)), ('annealed Adam', (5, 7)), ('about 50,000 steps', (8, 11)), ('nearest epoch', (16, 18))]",[],[],[],[]
experiments,What we see is that the deep bilinear model outperforms the others with respect to both speed and accuracy .,"[('with respect to', (12, 15))]","[('deep bilinear model', (6, 9)), ('outperforms', (9, 10)), ('others', (11, 12)), ('both speed and accuracy', (15, 19))]",[],[],[],[]
experiments,"The model with shallow bilinear arc and label classifiers gets the same unlabeled performance as the deep model with the same settings , but because the label classifier is much larger ( ( 801 c 801 ) as opposed to ( 101 c 101 ) ) , it runs much slower and overfits .","[('with', (2, 3)), ('gets', (9, 10)), ('as', (14, 15)), ('with', (18, 19)), ('runs', (48, 49))]","[('model', (1, 2)), ('shallow bilinear arc and label classifiers', (3, 9)), ('same unlabeled performance', (11, 14)), ('deep model', (16, 18)), ('same settings', (20, 22)), ('much slower', (49, 51)), ('overfits', (52, 53))]",[],[],[],[]
experiments,"We find that using three or four layers gets significantly better performance than two layers , and increasing the LSTM sizes from 200 to 300 or 400 dimensions likewise signficantly improves performance .","[('find', (1, 2)), ('using', (3, 4)), ('gets', (8, 9)), ('than', (12, 13)), ('increasing', (17, 18)), ('from', (21, 22)), ('signficantly improves', (29, 31))]","[('three or four layers', (4, 8)), ('significantly better performance', (9, 12)), ('two layers', (13, 15)), ('LSTM sizes', (19, 21)), ('200 to', (22, 24)), ('300 or 400 dimensions', (24, 28)), ('performance', (31, 32))]",[],[],[],[]
experiments,"We also implemented the coupled input - forget gate LSTM cells ( Cif - LSTM ) suggested by , 6 finding that while the resulting model still slightly underperforms the more popular LSTM cells , the difference between the two is much smaller .","[('implemented', (2, 3))]","[('coupled input - forget gate LSTM cells ( Cif - LSTM )', (4, 16)), ('resulting', (24, 25)), ('more popular LSTM cells', (30, 34)), ('much smaller', (41, 43))]",[],[],[],[]
model,"In addition to using relatively extreme dropout in the recurrent and MLP layers mentioned in , we also regularize the input layer .","[('in', (7, 8)), ('regularize', (18, 19))]","[('recurrent and MLP layers', (9, 13)), ('input layer', (20, 22))]",[],[],[],[]
experiments,"Interestingly , not using any tags at all actually results in better performance than using tags without dropout .","[('not using', (2, 4)), ('results in', (9, 11)), ('than', (13, 14)), ('without', (16, 17))]","[('any tags at all', (4, 8)), ('better performance', (11, 13)), ('using', (14, 15)), ('tags', (15, 16))]",[],[],[],[]
results,"Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .","[('gets', (2, 3)), ('on', (8, 9)), ('as', (13, 14)), ('gets', (28, 29)), ('on', (32, 33)), ('as well', (36, 38)), ('on', (41, 42))]","[('Our model', (0, 2)), ('nearly the same UAS performance', (3, 8)), ('PTB - SD 3.3.0', (9, 13)), ('current SOTA model', (15, 18)), ('SOTA UAS performance', (29, 32)), ('CTB 5.1', (33, 35)), ('SOTA performance', (39, 41)), ('all CoNLL 09 languages', (42, 46))]",[],[],[],[]
research-problem,Training with Exploration Improves a Greedy Stack LSTM Parser,[],[],[],[],[],[]
model,"Coupled with a recursive tree composition function , the feature representation is able to capture information from the entirety of the state , without resorting to locality assumptions that were common in most other transition - based parsers .","[('Coupled with', (0, 2)), ('able to capture', (12, 15)), ('from', (16, 17)), ('without resorting to', (23, 26))]","[('recursive tree composition function', (3, 7)), ('feature representation', (9, 11)), ('information', (15, 16)), ('entirety of the state', (18, 22)), ('locality assumptions', (26, 28))]",[],[],[],[]
model,"The use of a novel stack LSTM data structure allows the parser to maintain a constant time per-state update , and retain an over all linear parsing time .","[('use of', (1, 3)), ('allows', (9, 10)), ('to maintain', (12, 14)), ('retain', (21, 22))]","[('novel stack LSTM data structure', (4, 9)), ('parser', (11, 12)), ('constant time per-state update', (15, 19)), ('over all linear parsing time', (23, 28))]",[],[],[],[]
model,"At test time , the parser makes greedy decisions according to the learned model .","[('At', (0, 1)), ('makes', (6, 7)), ('according to', (9, 11))]","[('test time', (1, 3)), ('parser', (5, 6)), ('greedy decisions', (7, 9)), ('learned model', (12, 14))]",[],[],[],[]
model,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .","[('adapt', (5, 6)), ('drawn', (15, 16))]","[('training criterion', (7, 9)), ('parser states', (13, 15)), ('training data', (20, 22)), ('model', (27, 28)), ('learned', (32, 33))]",[],[],[],[]
model,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .","[('interpolating', (1, 2)), ('sampled from', (5, 7)), ('sampled from', (11, 13)), ('at', (20, 21))]","[('algorithm states', (3, 5)), ('model', (8, 9)), ('training data', (14, 16)), ('more robust predictions', (17, 20)), ('test time', (21, 23))]",[],[],[],[]
results,The score achieved by the dynamic oracle for English is 93.56 UAS .,"[('achieved by', (2, 4)), ('for', (7, 8)), ('is', (9, 10))]","[('score', (1, 2)), ('dynamic oracle', (5, 7)), ('English', (8, 9)), ('93.56 UAS', (10, 12))]",[],[],[],[]
results,"The error - exploring dynamic - oracle training always improves over static oracle training controlling for the transition system , but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle .","[('always', (8, 9)), ('over', (10, 11)), ('for', (15, 16)), ('when trained with', (29, 32))]","[('error - exploring dynamic - oracle training', (1, 8)), ('static oracle training controlling', (11, 15)), ('transition system', (17, 19)), ('arc-hybrid system', (22, 24)), ('slightly under-performs', (24, 26)), ('arc-standard system', (27, 29)), ('static oracle', (32, 34))]",[],[],[],[]
hyperparameters,Flattening the sampling distribution ( ? = 0.75 ) is especially beneficial when training with pretrained word embeddings .,[],"[('Flattening', (0, 1)), ('sampling distribution ( ? = 0.75 )', (2, 9)), ('training', (13, 14)), ('pretrained word embeddings', (15, 18))]",[],[],[],[]
model,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .","[('demonstrate', (4, 5)), ('without', (11, 12)), ('can achieve', (14, 16)), ('than', (20, 21)), ('as', (23, 24)), ('long as', (24, 26)), ('are', (27, 28))]","[('simple feed - forward networks', (6, 11)), ('any recurrence', (12, 14)), ('comparable or better accuracies', (16, 20)), ('LSTMs', (21, 22)), ('globally normalized', (28, 30))]",[],[],[],[]
model,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .","[('use', (3, 4)), ('perform', (8, 9)), ('for', (11, 12)), ('introduce', (16, 17)), ('with', (19, 20)), ('to overcome', (28, 30))]","[('any recurrence', (4, 6)), ('beam search', (9, 11)), ('maintaining', (12, 13)), ('multiple hypotheses', (13, 15)), ('global normalization', (17, 19)), ('conditional random field ( CRF ) objective', (21, 28)), ('label bias problem', (31, 34)), ('locally normalized models', (35, 38))]",[],[],[],[]
model,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,"[('compute', (1, 2)), ('based on', (3, 5)), ('perform', (10, 11)), ('of', (14, 15)), ('based on', (19, 21))]","[('gradients', (2, 3)), ('approximate global normalization', (6, 9)), ('full backpropagation training', (11, 14)), ('all neural network parameters', (15, 19)), ('CRF loss', (22, 24))]",[],[],[],[]
model,"As discussed in more detail in Section 5 , we also outperform previous structured training approaches used for neural network transitionbased parsing .","[('used for', (16, 18))]","[('outperform', (11, 12)), ('previous structured training approaches', (12, 16)), ('neural network transitionbased parsing', (18, 22))]",[],[],[],[]
model,"We also provide a pre-trained , state - of - the art English dependency parser called "" Parsey McParseface , "" which we tuned for a balance of speed , simplicity , and accuracy .","[('provide', (2, 3)), ('called', (15, 16))]","[('pre-trained , state - of - the art English dependency parser', (4, 15)), ('Parsey McParseface', (17, 19))]",[],[],[],[]
experiments,We use stochastic gradient descent on the negative log - likelihood of the data under the model .,"[('use', (1, 2)), ('on', (5, 6)), ('of', (11, 12)), ('under', (14, 15))]","[('stochastic gradient descent', (2, 5)), ('negative log - likelihood', (7, 11)), ('data', (13, 14)), ('model', (16, 17))]",[],[],[],[]
baselines,"We apply our approach to POS tagging , syntactic dependency parsing , and sentence compression .","[('apply', (1, 2)), ('to', (4, 5))]","[('POS tagging', (5, 7)), ('syntactic dependency parsing', (8, 11)), ('sentence compression', (13, 15))]",[],[],[],[]
ablation-analysis,"While directly optimizing the global model defined by Eq. ( 5 ) works well , we found that training the model in two steps achieves the same precision much faster : we first pretrain the network using the local objective given in Eq. ( 4 ) , and then perform additional training steps using the global objective given in Eq. ( 6 ) .","[('works', (12, 13)), ('in', (21, 22)), ('achieves', (24, 25)), ('pretrain', (33, 34)), ('using', (36, 37)), ('perform', (49, 50)), ('using', (53, 54))]","[('directly optimizing', (1, 3)), ('global model', (4, 6)), ('well', (13, 14)), ('training', (18, 19)), ('model', (20, 21)), ('two steps', (22, 24)), ('same precision', (26, 28)), ('network', (35, 36)), ('local objective', (38, 40)), ('additional training steps', (50, 53)), ('global objective', (55, 57))]",[],[],[],[]
hyperparameters,"Specifically , we use averaged stochastic gradient descent with momentum , and we tune the learning rate , learning rate schedule , momentum , and early stopping time using a separate held - out corpus for each task .","[('use', (3, 4)), ('with', (8, 9)), ('tune', (13, 14)), ('using', (28, 29)), ('for', (35, 36))]","[('averaged stochastic gradient descent', (4, 8)), ('momentum', (9, 10)), ('learning rate', (15, 17)), ('learning rate schedule', (18, 21)), ('momentum', (22, 23)), ('early stopping time', (25, 28)), ('separate held - out corpus', (30, 35)), ('each task', (36, 38))]",[],[],[],[]
experiments,"Part of speech ( POS ) tagging is a classic NLP task , where modeling the structure of the output is important for achieving state - of - the - art performance .",[],"[('speech ( POS ) tagging', (2, 7))]",[],[],[],[]
ablation-analysis,"We extract features from words , POS tags , and dependency labels from a window of tokens centered on the in - put , as well as features from the history of predictions .","[('extract', (1, 2)), ('from', (3, 4)), ('from', (12, 13)), ('centered on', (17, 19)), ('as', (24, 25)), ('from', (28, 29))]","[('features', (2, 3)), ('words', (4, 5)), ('window of tokens', (14, 17)), ('in - put', (20, 23)), ('features', (27, 28)), ('history of predictions', (30, 33))]",[],[],[],[]
hyperparameters,We use a single hidden layer of size 400 .,"[('use', (1, 2)), ('of size', (6, 8))]","[('single hidden layer', (3, 6)), ('400', (8, 9))]",[],[],[],[]
experiments,Our globally normalized model again significantly outperforms the local model .,[],"[('Our globally normalized model', (0, 4)), ('significantly outperforms', (5, 7)), ('local model', (8, 10))]",[],[],[],[]
baselines,"We also compare to the sentence compression system from , a 3 - layer stacked LSTM which uses dependency label information .","[('compare to', (2, 4)), ('from', (8, 9)), ('uses', (17, 18))]","[('sentence compression system', (5, 8)), ('3 - layer stacked LSTM', (11, 16)), ('dependency label information', (18, 21))]",[],[],[],[]
experiments,"The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings , but our model is roughly 100 faster .","[('perform', (6, 7)), ('on both', (9, 11)), ('is', (24, 25))]","[('LSTM and our global model', (1, 6)), ('on par', (7, 9)), ('automatic evaluation', (12, 14)), ('human ratings', (18, 20)), ('our model', (22, 24)), ('roughly 100 faster', (25, 28))]",[],[],[],[]
experiments,All compressions kept approximately 42 % of the tokens on average and all the models are significantly better than the automatic extractions ( p < 0.05 ) .,"[('kept', (2, 3)), ('of', (6, 7))]","[('All compressions', (0, 2)), ('approximately 42 %', (3, 6)), ('tokens', (8, 9))]",[],[],[],[]
baselines,"Inspired by the integrated POS tagging and parsing transition system of , we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack .","[('employ', (13, 14)), ('uses', (19, 20)), ('predicts', (25, 26)), ('of', (29, 30)), ('on', (33, 34)), ('shifted to', (39, 41))]","[('simple transition system', (15, 18)), ('SHIFT action', (22, 24)), ('POS tag', (27, 29)), ('current word', (31, 33)), ('buffer', (35, 36)), ('stack', (42, 43))]",[],[],[],[]
hyperparameters,"We extract the following features on a window 3 tokens centered at the current focus token : word , cluster , character n- gram up to length 3 .","[('extract', (1, 2)), ('on', (5, 6)), ('centered at', (10, 12)), ('up to', (24, 26))]","[('window 3 tokens', (7, 10)), ('current focus token', (13, 16)), ('word , cluster', (17, 20)), ('length 3', (26, 28))]",[],[],[],[]
results,Our local model already compares favorably against these methods on average .,"[('compares', (4, 5))]","[('Our local model', (0, 3)), ('favorably', (5, 6))]",[],[],[],[]
results,"Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .","[('Using', (0, 1)), ('with', (3, 4)), ('does', (8, 9)), ('with', (13, 14)), ('leads to', (17, 19)), ('in', (23, 24))]","[('beam search', (1, 3)), ('locally normalized model', (5, 8)), ('help', (10, 11)), ('global normalization', (14, 16)), ('7 % reduction', (20, 23)), ('relative error', (24, 26))]",[],[],[],[]
experiments,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .","[('is', (6, 7)), ('increasing', (10, 11)), ('on', (13, 14)), ('by', (18, 19))]","[('set of character ngrams feature', (1, 6)), ('very important', (7, 9)), ('average accuracy', (11, 13)), (""CoNLL '09 datasets"", (15, 18)), ('about 0.5 % absolute', (19, 23))]",[],[],[],[]
experiments,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .","[('compares', (10, 11)), ('with', (23, 24))]","[('favorably', (11, 12)), ('94.26 % LAS and 92.41 % UAS', (14, 21))]",[],[],[],[]
experiments,Our results also significantly outperform the LSTM - based approaches of .,[],"[('significantly outperform', (3, 5)), ('LSTM - based approaches', (6, 10))]",[],[],[],[]
research-problem,Bag of Tricks for Efficient Text Classification,[],[],[],[],[],[]
research-problem,This paper explores a simple and efficient baseline for text classification .,[],"[('text classification', (9, 11))]",[],[],[],[]
research-problem,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",[],"[('Text classification', (0, 2))]",[],[],[],[]
experiments,Sentiment analysis,[],[],[],[],[],[]
tasks,"On this task , adding bigram information improves the performance by 1 - 4 % .","[('adding', (4, 5)), ('improves', (7, 8)), ('by', (10, 11))]","[('bigram information', (5, 7)), ('performance', (9, 10)), ('1 - 4 %', (11, 15))]",[],[],[],[]
tasks,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","[('slightly better than', (4, 7)), ('a', (16, 17)), ('bit worse than', (17, 20))]","[('our accuracy', (1, 3)), ('char - CNN and char - CRNN', (7, 14)), ('VDCNN', (20, 21))]",[],[],[],[]
tasks,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .","[('increase', (4, 5)), ('by using', (8, 10)), ('for example with', (13, 16)), ('performance on', (19, 21)), ('goes up to', (22, 25))]","[('accuracy slightly', (6, 8)), ('more n-grams', (10, 12)), ('trigrams', (16, 17)), ('Sogou', (21, 22)), ('97.1 %', (25, 27))]",[],[],[],[]
experiments,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,"[('focus on', (1, 3)), ('according to', (6, 8))]","[('predicting', (3, 4)), ('tags', (5, 6)), ('title and caption', (9, 12))]",[],[],[],[]
experiments,"The vocabulary size is 297,141 and there are 312,116 unique tags .","[('is', (3, 4))]","[('vocabulary size', (1, 3)), ('297,141', (4, 5)), ('312,116 unique tags', (8, 11))]",[],[],[],[]
baselines,We consider a frequency - based baseline which predicts the most frequent tag .,"[('consider', (1, 2)), ('predicts', (8, 9))]","[('frequency - based baseline', (3, 7)), ('most frequent tag', (10, 13))]",[],[],[],[]
baselines,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .","[('compare with', (2, 4)), ('based on', (24, 26))]","[('Tagspace ( Weston et al. , 2014 )', (4, 12)), ('tag', (16, 17)), ('Wsabie model', (27, 29))]",[],[],[],[]
results,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .","[('described using', (5, 7)), ('consider', (10, 11)), ('achieves', (16, 17)), ('is', (20, 21))]","[('Tagspace model', (2, 4)), ('convolutions', (7, 8)), ('linear version', (12, 14)), ('comparable performance', (17, 19)), ('much faster', (21, 23))]",[],[],[],[]
tasks,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .","[('achieve', (2, 3)), ('with', (6, 7)), ('adding', (13, 14)), ('gives', (15, 16)), ('in', (20, 21))]","[('Both', (0, 1)), ('similar performance', (4, 6)), ('small hidden layer', (8, 11)), ('bigrams', (14, 15)), ('significant boost', (18, 20)), ('accuracy', (21, 22))]",[],[],[],[]
tasks,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .","[('At', (0, 1)), ('needs to compute', (5, 8)), ('for', (10, 11)), ('makes it', (15, 17)), ('gives', (24, 25)), ('when', (30, 31))]","[('test time', (1, 3)), ('Tagspace', (4, 5)), ('scores', (9, 10)), ('all the classes', (11, 14)), ('relatively slow', (17, 19)), ('our fast inference', (21, 24)), ('significant speed - up', (26, 30)), ('number of classes is large ( more than 300 K here )', (32, 44))]",[],[],[],[]
research-problem,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION,[],[],[],[],[],[]
research-problem,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",[],"[('cross - lingual understanding ( XLU )', (3, 10))]",[],[],[],[]
approach,"In this paper , we propose to jointly tackle both language and domain transfer .",[],"[('jointly', (7, 8)), ('both language and domain transfer', (9, 14))]",[],[],[],[]
approach,"Using this unlabeled data , we combine the aforementioned cross - lingual methods with recently proposed unsupervised domain adaptation and weak supervision techniques on the task of cross - lingual document classification .",[],"[('cross - lingual document classification', (27, 32))]",[],[],[],[]
approach,"In particular , we focus on two approaches for domain adaptation .","[('focus on', (4, 6)), ('for', (8, 9))]","[('two approaches', (6, 8)), ('domain adaptation', (9, 11))]",[],[],[],[]
approach,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,"[('based on', (4, 6)), ('using', (17, 18))]","[('first method', (1, 3)), ('masked language model ( MLM ) pre-training', (6, 13)), ('unlabeled target language corpora', (18, 22))]",[],[],[],[]
approach,We propose to alleviate this issue by using self - training technique to do the domain adaptation from the source language into the target language .,"[('to do', (12, 14)), ('from', (17, 18)), ('into', (21, 22))]","[('self - training technique', (8, 12)), ('domain adaptation', (15, 17)), ('source language', (19, 21)), ('target language', (23, 25))]",[],[],[],[]
baselines,"In this paper , we adopt the second approach as the basic model , and utilize the XLM model ) as our base model , which has been pre-trained by large - scale parallel and monolingual data from various languages .","[('adopt', (5, 6)), ('as', (9, 10)), ('utilize', (15, 16)), ('as', (20, 21)), ('pre-trained by', (28, 30))]","[('second approach', (7, 9)), ('basic model', (11, 13)), ('XLM model', (17, 19)), ('our base model', (21, 24)), ('large - scale parallel and monolingual data', (30, 37))]",[],[],[],[]
baselines,SEMI - SUPERVISED XLU,[],[],[],[],[],[]
experiments,Masked Language,[],[],[],[],[],[]
experiments,"Alleviating the Train - Test Discrepancy of the UDA Method With the UDA algorithm , the classifier is able to learn some prior information on the target domain , however it still suffers from the train - test discrepancy .","[('Alleviating', (0, 1)), ('of', (6, 7)), ('With', (10, 11)), ('able to learn', (18, 21)), ('on', (24, 25)), ('suffers from', (32, 34))]","[('Train - Test Discrepancy', (2, 6)), ('UDA Method', (8, 10)), ('UDA algorithm', (12, 14)), ('target domain', (26, 28)), ('train - test', (35, 38))]",[],[],[],[]
baselines,"Follwing this process , we obtain a new classifier trained only based on the target domain , which does not suffer from the train - test mismatch problem .","[('obtain', (5, 6)), ('trained only based on', (9, 13))]","[('new classifier', (7, 9)), ('target domain', (14, 16)), ('train - test mismatch problem', (23, 28))]",[],[],[],[]
experiments,"In the sentiment classification task , because the size of the unlabeled corpus in each target domain is large enough , we fine - tune an XLM with MLM loss for each target domain respectively .","[('fine - tune', (22, 25)), ('with', (27, 28)), ('for', (30, 31))]","[('sentiment classification task', (2, 5)), ('XLM', (26, 27)), ('MLM loss', (28, 30)), ('each target domain', (31, 34))]",[],[],[],[]
baselines,Fine-tune ( Ft ) : Fine - tuning the pre-trained model with the source - domain training set .,"[('with', (11, 12))]","[('Fine-tune ( Ft )', (0, 4)), ('Fine - tuning', (5, 8)), ('pre-trained model', (9, 11)), ('source - domain training set', (13, 18))]",[],[],[],[]
results,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .","[('Looking at', (0, 2)), ('without the help of', (12, 16)), ('from', (18, 19)), ('between', (29, 30)), ('of', (33, 34)), ('when using', (44, 46))]","[('Ft ( XLM ) results', (2, 7)), ('unlabeled data', (16, 18)), ('target domain', (20, 22)), ('substantial gap', (27, 29)), ('model performance', (31, 33)), ('cross -lingual settings', (35, 38)), ('monolingual baselines', (40, 42)), ('state - of - the - art pre-trained cross -lingual representations', (46, 57))]",[],[],[],[]
results,"In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .","[('In', (0, 1)), ('where', (6, 7)), ('is', (11, 12)), ('consistently provides', (23, 25)), ('compared with', (27, 29))]","[('sentiment classification task', (2, 5)), ('unlabeled data size', (8, 11)), ('larger', (12, 13)), ('Ft ( XLM ft ) model usnig MLM pre-training', (14, 23)), ('larger improvements', (25, 27)), ('UDA method', (30, 32))]",[],[],[],[]
results,The combination of both methods - as in the UDA ( XLM ft ) model - consistently outperforms either method alone .,"[('combination of', (1, 3)), ('as in', (6, 8)), ('consistently outperforms', (16, 18))]","[('UDA ( XLM ft ) model', (9, 15)), ('either method alone', (18, 21))]",[],[],[],[]
results,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .","[('In', (0, 1)), ('observe', (7, 8)), ('over', (15, 16))]","[('sentiment classification task', (2, 5)), ('self - training technique', (9, 13)), ('consistently improves', (13, 15)), ('teacher model', (17, 19))]",[],[],[],[]
results,It offers best results in both XLM and XLM ft based classifiers .,"[('offers', (1, 2)), ('in', (4, 5))]","[('best results', (2, 4)), ('both XLM and XLM ft based classifiers', (5, 12))]",[],[],[],[]
results,"In the MLdoc dataset , self - training also achieves the best results over all , however the gains are less clear .","[('In', (0, 1)), ('achieves', (9, 10))]","[('MLdoc dataset', (2, 4)), ('self - training', (5, 8)), ('best results', (11, 13))]",[],[],[],[]
results,"Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .","[('comparing with', (2, 4)), ('able to', (19, 21)), ('by utilizing', (26, 28)), ('in', (30, 31))]","[('best cross - lingual results and monolingual fine - tune baseline', (5, 16)), ('completely close', (21, 23)), ('performance gap', (24, 26)), ('unlabeled data', (28, 30)), ('target language', (32, 34))]",[],[],[],[]
results,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .","[('reaches', (4, 5)), ('improving over', (15, 17)), ('by', (20, 21))]","[('our framework', (2, 4)), ('new state - of - the - art results', (5, 14)), ('vanilla XLM baselines', (17, 20)), ('44 %', (21, 23))]",[],[],[],[]
results,The experment results show that it lags behind the ones using unlabeled data from the target domain .,"[('show', (3, 4)), ('lags behind', (6, 8)), ('from', (13, 14))]","[('experment results', (1, 3)), ('ones using unlabeled data', (9, 13)), ('target domain', (15, 17))]",[],[],[],[]
results,"In contrast , the performance of the model improves consistently with more labeled data in the monolingual setting .","[('of', (5, 6)), ('with', (10, 11)), ('in', (14, 15))]","[('performance', (4, 5)), ('model', (7, 8)), ('improves', (8, 9)), ('consistently', (9, 10)), ('more labeled data', (11, 14)), ('monolingual setting', (16, 18))]",[],[],[],[]
ablation-analysis,"From the results , we conclude that t2t is the best performing approach , as it 's the best matched to the target domain .","[('conclude that', (5, 7)), ('is', (8, 9))]","[('t2t', (7, 8)), ('best performing approach', (10, 13)), ('target domain', (22, 24))]",[],[],[],[]
research-problem,Neural Attentive Bag - of - Entities Model for Text Classification,[],[],[],[],[],[]
research-problem,"As a result , our model achieved state - of - the - art results on all datasets .","[('achieved', (6, 7)), ('on', (15, 16))]","[('our model', (4, 6)), ('state - of - the - art results', (7, 15)), ('all datasets', (16, 18))]",[],[],[],[]
model,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .","[('proposes', (2, 3)), ('addresses', (21, 22)), ('by modeling', (26, 28)), ('in', (30, 31)), ('using', (34, 35))]","[('Neural Attentive Bagof - Entities ( NABoE ) model', (4, 13)), ('text classification problem', (23, 26)), ('semantics', (29, 30)), ('target documents', (32, 34)), ('entities in the', (35, 38)), ('KB', (38, 39))]",[],[],[],[]
model,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,"[('computed using', (3, 5)), ('that enables', (10, 12)), ('to focus on', (14, 17)), ('of', (20, 21)), ('that are', (23, 25)), ('in', (27, 28))]","[('weights', (1, 2)), ('novel neural attention mechanism', (6, 10)), ('model', (13, 14)), ('small subset', (18, 20)), ('entities', (22, 23)), ('less ambiguous', (25, 27)), ('meaning', (28, 29)), ('document', (34, 35))]",[],[],[],[]
model,"Furthermore , the attention mechanism improves the interpretability of the model because it enables us to inspect the small number of entities that strongly affect the classification decisions .","[('improves', (5, 6)), ('of', (8, 9))]","[('attention mechanism', (3, 5)), ('interpretability', (7, 8)), ('model', (10, 11))]",[],[],[],[]
code,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,[],[],[],[],[],[]
hyperparameters,We initialized the embeddings of words ( v w ) and entities ( v e ) using pretrained embeddings trained on KB .,"[('initialized', (1, 2)), ('of', (4, 5)), ('using', (16, 17)), ('trained on', (19, 21))]","[('embeddings', (3, 4)), ('words ( v w ) and entities ( v e )', (5, 16)), ('pretrained embeddings', (17, 19)), ('KB', (21, 22))]",[],[],[],[]
baselines,FTS- BRNN,[],[],[],[],[],[]
baselines,It uses the logistic regression classifier with the features derived by the RNN .,"[('uses', (1, 2)), ('with', (6, 7)), ('derived by', (9, 11))]","[('logistic regression classifier', (3, 6)), ('features', (8, 9)), ('RNN', (12, 13))]",[],[],[],[]
baselines,"Similar to our previous experiment , we also add SWEM - concat , and the variants of our NABoEentity and NABoE - full models based on Wikifier and TAGME ( see Section 4.2 ) .","[('add', (8, 9)), ('of', (16, 17)), ('based on', (24, 26))]","[('SWEM - concat', (9, 12)), ('variants', (15, 16)), ('our NABoEentity and NABoE - full models', (17, 24)), ('Wikifier and TAGME', (26, 29))]",[],[],[],[]
results,"Overall , our models achieved enhanced performance on this task .","[('achieved', (4, 5))]","[('our models', (2, 4)), ('enhanced performance', (5, 7))]",[],[],[],[]
results,"In particular , the NABoE - full model successfully outperformed all the baseline models , and the NABoE-entity model achieved competitive performance and outperformed all the baseline models in the literature category .","[('successfully', (8, 9)), ('achieved', (19, 20)), ('in', (28, 29))]","[('NABoE - full model', (4, 8)), ('all the baseline models', (10, 14)), ('NABoE-entity model', (17, 19)), ('competitive performance', (20, 22)), ('outperformed', (23, 24)), ('all the baseline models', (24, 28)), ('literature category', (30, 32))]",[],[],[],[]
results,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .","[('yielded', (7, 8)), ('on', (12, 13))]","[('our models', (5, 7)), ('enhanced over all performance', (8, 12)), ('both datasets', (13, 15))]",[],[],[],[]
results,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,"[('in terms of', (9, 12))]","[('NABoE - full model', (1, 5)), ('outperformed', (5, 6)), ('all baseline models', (6, 9)), ('both measures', (12, 14))]",[],[],[],[]
results,"Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .","[('in terms of', (10, 13)), ('on', (15, 16)), ('on', (25, 26))]","[('NABoE-entity model', (3, 5)), ('outperformed', (5, 6)), ('all the baseline models', (6, 10)), ('both measures', (13, 15)), ('20NG dataset', (17, 19)), ('F 1 score', (22, 25)), ('R8 dataset', (27, 29))]",[],[],[],[]
results,"Moreover , our attention mechanism consistently improved the performance .","[('consistently', (5, 6))]","[('our attention mechanism', (2, 5)), ('performance', (8, 9))]",[],[],[],[]
results,"Further , the models based on the dictionarybased entity detection ( see Section 2.1 ) generally outperformed the models based on the entity linking systems ( i.e. , Wikifier and TAGME ) .","[('based on', (4, 6)), ('generally outperformed', (15, 17)), ('based on', (19, 21)), ('i.e.', (26, 27))]","[('dictionarybased entity detection', (7, 10)), ('models', (18, 19)), ('entity linking systems', (22, 25)), ('Wikifier', (28, 29)), ('TAGME', (30, 31))]",[],[],[],[]
results,"Moreover , our attention mechanism consistently improved the performance for Wikifierand TAGME - based models because the attention mechanism enabled the model to focus on entities that were relevant to the document .","[('consistently improved', (5, 7)), ('for', (9, 10))]","[('our attention mechanism', (2, 5)), ('performance', (8, 9)), ('Wikifierand TAGME - based models', (10, 15))]",[],[],[],[]
experiments,Factoid Question Answering,[],[],[],[],[],[]
results,"Furthermore , similar to the previous text classification experiment , the attention mechanism and the pretrained embeddings consistently improved the performance .",[],"[('attention mechanism and the pretrained embeddings', (11, 17)), ('consistently improved', (17, 19)), ('performance', (20, 21))]",[],[],[],[]
results,"Moreover , the models based on dictionary - based entity detection outperformed the models based on the entity linking systems .","[('based on', (14, 16))]","[('models based on', (3, 6)), ('dictionary - based entity detection', (6, 11)), ('outperformed', (11, 12)), ('models', (13, 14)), ('entity linking systems', (17, 20))]",[],[],[],[]
research-problem,Task - oriented Word Embedding for Text Classification,[],[],[],[],[],[]
research-problem,Distributed word representation plays a pivotal role in various natural language processing tasks .,[],"[('Distributed word representation', (0, 3))]",[],[],[],[]
research-problem,AI : a combination of active learning and self learning for named entity recognition on twitter using conditional random fields learning :,"[('combination of', (3, 5)), ('for', (10, 11)), ('using', (16, 17))]","[('AI', (0, 1)), ('active learning and self learning', (5, 10)), ('named entity recognition on twitter', (11, 16)), ('conditional random fields learning', (17, 21))]",[],[],[],[]
research-problem,Learning word representation is a fundamental step in various natural language processing tasks .,[],"[('Learning word representation', (0, 3))]",[],[],[],[]
model,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .","[('propose', (5, 6)), ('denoted as', (14, 16)), ('to solve', (18, 20))]","[('task - oriented word embedding method', (7, 13)), ('ToWE', (16, 17))]",[],[],[],[]
model,"Specifically , we focus on text classification .","[('focus on', (3, 5))]","[('text classification', (5, 7))]",[],[],[],[]
model,"In the joint learning framework , the contextual information is captured following the context prediction task introduced by .","[('In', (0, 1)), ('captured following', (10, 12))]","[('joint learning framework', (2, 5)), ('contextual information', (7, 9)), ('context prediction task', (13, 16))]",[],[],[],[]
model,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .","[('regularize', (7, 8)), ('of', (10, 11)), ('to have', (14, 16)), ('adjust', (23, 24)), ('of', (26, 27)), ('in', (30, 31))]","[('task information', (3, 5)), ('distribution', (9, 10)), ('clear classification boundary', (17, 20)), ('distribution', (25, 26)), ('other words', (28, 30)), ('embedding space', (32, 34))]",[],[],[],[]
model,We propose a task - oriented word embedding method that is specially designed for text classification .,"[('propose', (1, 2)), ('specially designed for', (11, 14))]","[('task - oriented word embedding method', (3, 9)), ('text classification', (14, 16))]",[],[],[],[]
model,It introduces the function - aware component and highlights word 's functional attributes in the embedding space by regularizing the distribution of words to have a clear classification boundary .,"[('introduces', (1, 2)), ('highlights', (8, 9)), ('in', (13, 14)), ('by regularizing', (17, 19)), ('to have', (23, 25))]","[('function - aware component', (3, 7)), (""word 's functional attributes"", (9, 13)), ('embedding space', (15, 17)), ('distribution of words', (20, 23)), ('clear classification boundary', (26, 29))]",[],[],[],[]
baselines,It represents each document as a bag of words and the weighting scheme is TFIDF .,"[('represents', (1, 2)), ('as', (4, 5)), ('is', (13, 14))]","[('each document', (2, 4)), ('bag of words', (6, 9)), ('weighting scheme', (11, 13)), ('TFIDF', (14, 15))]",[],[],[],[]
baselines,"It comprises two models , i.e. , CBOW which predicts the target word using context information , and the Skip - gram ( denoted as SG ) which predicts each context word using the target word ; ( 3 ) the Glo Ve method is a state - of - the - art matrix factorization method .","[('comprises', (1, 2)), ('predicts', (9, 10)), ('using', (13, 14)), ('predicts', (28, 29)), ('using', (32, 33)), ('is', (44, 45))]","[('two', (2, 3)), ('CBOW', (7, 8)), ('target word', (11, 13)), ('context information', (14, 16)), ('Skip - gram', (19, 22)), ('each context word', (29, 32)), ('target word', (34, 36)), ('Glo Ve method', (41, 44)), ('state - of - the - art matrix factorization method', (46, 56))]",[],[],[],[]
ablation-analysis,"In this paper , we use the text classification task to evaluate the performance of word embeddings .","[('use', (5, 6)), ('to evaluate', (10, 12)), ('of', (14, 15))]","[('text classification task', (7, 10)), ('performance', (13, 14)), ('word embeddings', (15, 17))]",[],[],[],[]
experimental-setup,"We regard document embedding as a document feature and trained a linear classifier using Liblinear 7 , since the feature size is large , and Liblinear can quickly train a linear classifier with high dimension features .","[('regard', (1, 2)), ('as', (4, 5)), ('trained', (9, 10)), ('using', (13, 14))]","[('document embedding', (2, 4)), ('document feature', (6, 8)), ('linear classifier', (11, 13)), ('Liblinear', (14, 15))]",[],[],[],[]
experimental-setup,"We tokenized the corpus with the Stanford Tokenizer 8 and converted it to lowercase , then removed the stop words .","[('tokenized', (1, 2)), ('with', (4, 5)), ('converted it to', (10, 13)), ('removed', (16, 17))]","[('corpus', (3, 4)), ('Stanford Tokenizer', (6, 8)), ('lowercase', (13, 14)), ('stop words', (18, 20))]",[],[],[],[]
experimental-setup,"For a fair comparison , all word embeddings adhere to the following settings : the dimensionality of vectors is 300 , the size of the context window is 5 , the number of negative samples is 25 .","[('adhere', (8, 9)), ('of', (16, 17)), ('is', (18, 19)), ('is', (27, 28)), ('is', (35, 36))]","[('all word embeddings', (5, 8)), ('dimensionality', (15, 16)), ('vectors', (17, 18)), ('300', (19, 20)), ('size', (22, 23)), ('context window', (25, 27)), ('5', (28, 29)), ('number of negative samples', (31, 35)), ('25', (36, 37))]",[],[],[],[]
experiments,Group dataset .,[],"[('Group dataset', (0, 2))]",[],[],[],[]
experiments,Group dataset .,[],"[('Group dataset', (0, 2))]",[],[],[],[]
experimental-setup,The recommended N is 150 with the constraint that the total size of S is under 1200 based on practical experience .,"[('is', (3, 4)), ('with', (5, 6))]","[('recommended N', (1, 3)), ('150', (4, 5)), ('constraint', (7, 8)), ('total', (10, 11))]",[],[],[],[]
experimental-setup,"were tuned from 0 to 1 , with a step size of 0.1 .","[('tuned from', (1, 3)), ('with', (7, 8)), ('of', (11, 12))]","[('0 to 1', (3, 6)), ('step size', (9, 11)), ('0.1', (12, 13))]",[],[],[],[]
results,"The proposed method based on Skip - gram and CBOW reaches optimal performance when ? = 0.4 and ? = 0.3 , respectively .","[('based on', (3, 5)), ('reaches', (10, 11)), ('when', (13, 14))]","[('Skip - gram and CBOW', (5, 10)), ('optimal performance', (11, 13)), ('? = 0.4 and ? = 0.3', (14, 21))]",[],[],[],[]
results,"( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .","[('performs', (5, 6)), ('than', (7, 8)), ('proved to', (14, 16)), ('for', (19, 20))]","[('Our method', (3, 5)), ('better', (6, 7)), ('other methods', (9, 11)), ('highly reliable', (17, 19)), ('text classification task', (21, 24))]",[],[],[],[]
results,"In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .","[('on', (13, 14))]","[('ToWE - SG method', (4, 8)), ('significantly outperforms', (8, 10)), ('other baselines', (11, 13)), ('20 New s Group', (15, 19)), ('5 Abstract s Group', (20, 24))]",[],[],[],[]
results,"( 2 ) The word embedding methods outperform the basic bag - of - words methods in most cases , indicating the superiority of distributed word representation over the one - hot representation .","[('indicating', (20, 21))]","[('word embedding methods', (4, 7)), ('outperform', (7, 8)), ('basic bag - of - words methods', (9, 16))]",[],[],[],[]
baselines,( 3 ) The Retrofit method is the knowledge - base enhanced word embedding method .,"[('is', (6, 7))]","[('Retrofit method', (4, 6)), ('knowledge - base enhanced word embedding method', (8, 15))]",[],[],[],[]
results,"Our method achieves better performance over Retrofit method , indicating that the task - specific features could be more effective compared with general semantic relations constructed by humans in the knowledge bases .","[('achieves', (2, 3)), ('over', (5, 6))]","[('Our method', (0, 2)), ('better performance', (3, 5)), ('Retrofit method', (6, 8))]",[],[],[],[]
results,"( 4 ) In sentence classification , such as the MR and SST datasets , it is obvious that TWE achieves a relatively lower performance .","[('obvious', (17, 18)), ('achieves', (20, 21))]","[('TWE', (19, 20)), ('relatively lower performance', (22, 25))]",[],[],[],[]
results,"Our method outperforms the TWE method on both the document - level and sentence - level tasks , which shows the stability and reliability of modeling taskspecific features in real - world applications .","[('on', (6, 7))]","[('outperforms', (2, 3)), ('TWE method', (4, 6)), ('document - level and sentence - level tasks', (9, 17))]",[],[],[],[]
research-problem,Graph Convolutional Networks for Text Classification,[],[],[],[],[],[]
research-problem,Text classification is an important and classical problem in natural language processing .,[],"[('Text classification', (0, 2))]",[],[],[],[]
research-problem,Text classification is a fundamental problem in natural language processing ( NLP ) .,[],"[('Text classification', (0, 2))]",[],[],[],[]
model,"In this work , we propose a new graph neural networkbased method for text classification .","[('propose', (5, 6)), ('for', (12, 13))]","[('new graph neural networkbased method', (7, 12)), ('text classification', (13, 15))]",[],[],[],[]
model,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .","[('model', (1, 2)), ('with', (4, 5)), ('captures', (21, 22))]","[('Graph Convolutional Network ( GCN )', (6, 12)), ('high order neighborhoods information', (22, 26))]",[],[],[],[]
model,We then turn text classification problem into anode classification problem .,"[('turn', (2, 3)), ('into', (6, 7))]","[('text classification problem', (3, 6)), ('anode classification problem', (7, 10))]",[],[],[],[]
code,Our source code is available at https://github. com/yao8839836/text_gcn .,[],"[('https://github. com/yao8839836/text_gcn', (6, 8))]",[],[],[],[]
model,We propose a novel graph neural network method for text classification .,[],"[('text classification', (9, 11))]",[],[],[],[]
research-problem,Deep Learning for Text Classification,[],[],[],[],[],[]
experiments,Can our model learn predictive word and document embeddings ?,[],"[('Can our model learn', (0, 4))]",[],[],[],[]
baselines,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,"[('with', (12, 13))]","[('TF - IDF + LR', (0, 5)), ('bag - of - words model', (6, 12)), ('term frequencyinverse document frequency weighting', (13, 18))]",[],[],[],[]
baselines,Logistic Regression is used as the classifier .,"[('used as', (3, 5))]","[('Logistic Regression', (0, 2)), ('classifier', (6, 7))]",[],[],[],[]
baselines,CNN : Convolutional Neural Network ( Kim 2014 ) .,[],"[('CNN', (0, 1)), ('Convolutional Neural Network', (2, 5))]",[],[],[],[]
baselines,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,"[('uses', (8, 9)), ('as', (13, 14)), ('of', (16, 17))]","[('LSTM', (0, 1)), ('last hidden state', (10, 13)), ('representation', (15, 16)), ('whole text', (18, 20))]",[],[],[],[]
baselines,"Bi- LSTM : a bi-directional LSTM , commonly used in text classification .","[('commonly used', (7, 9))]","[('Bi- LSTM', (0, 2)), ('text classification', (10, 12))]",[],[],[],[]
baselines,We input pre-trained word embeddings to Bi - LSTM .,"[('input', (1, 2)), ('to', (5, 6))]","[('pre-trained word embeddings', (2, 5)), ('Bi - LSTM', (6, 9))]",[],[],[],[]
baselines,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .",[],"[('PV - DBOW', (0, 3)), ('paragraph vector model', (5, 8)), ('orders of words in text', (12, 17)), ('ignored', (18, 19))]",[],[],[],[]
baselines,We used Logistic Regression as the classifier .,"[('used', (1, 2)), ('as', (4, 5))]","[('Logistic Regression', (2, 4)), ('classifier', (6, 7))]",[],[],[],[]
baselines,"PV - DM : a paragraph vector model proposed by , which considers the word order .","[('considers', (12, 13))]","[('PV - DM', (0, 3)), ('paragraph vector model', (5, 8)), ('word order', (14, 16))]",[],[],[],[]
baselines,We used Logistic Regression as the classifier .,"[('used', (1, 2)), ('as', (4, 5))]","[('Logistic Regression', (2, 4)), ('classifier', (6, 7))]",[],[],[],[]
baselines,"PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .","[('firstly learns', (7, 9)), ('based on', (11, 13)), ('containing', (16, 17)), ('averages', (26, 27)), ('as', (29, 30)), ('for', (32, 33))]","[('PTE', (0, 1)), ('predictive text embedding', (2, 5)), ('word embedding', (9, 11)), ('heterogeneous text network', (13, 16)), ('words , documents and labels as', (17, 23)), ('nodes', (23, 24)), ('word embeddings', (27, 29)), ('document embeddings', (30, 32)), ('text classification', (33, 35))]",[],[],[],[]
baselines,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .","[('treats', (12, 13)), ('as', (21, 22)), ('feeds', (26, 27)), ('into', (29, 30))]","[('fast Text', (0, 2)), ('text classification', (7, 9)), ('average of word / n- grams embeddings', (14, 21)), ('document embeddings', (22, 24)), ('document embeddings', (27, 29)), ('linear classifier', (31, 33))]",[],[],[],[]
baselines,"SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .","[('employs', (8, 9)), ('operated over', (12, 14))]","[('SWEM', (0, 1)), ('simple word embedding models', (2, 6)), ('simple pooling strategies', (9, 12)), ('word embeddings', (14, 16))]",[],[],[],[]
baselines,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .","[('embeds', (9, 10)), ('in', (14, 15)), ('for', (19, 20))]","[('LEAM', (0, 1)), ('label - embedding attentive models', (2, 7)), ('words and labels', (11, 14)), ('same joint space', (16, 19)), ('text classification', (20, 22))]",[],[],[],[]
baselines,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .","[('operates', (11, 12)), ('over', (13, 14))]","[('Graph - CNN - C', (0, 5)), ('convolutions', (12, 13)), ('word embedding similarity graphs', (14, 18)), ('Chebyshev filter', (30, 32))]",[],[],[],[]
baselines,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) . ,[],"[('Graph - CNN - S', (0, 5))]",[],[],[],[]
baselines,Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,"[('using', (15, 16))]","[('Graph - CNN - F', (0, 5)), ('Fourier filter', (16, 18))]",[],[],[],[]
hyperparameters,"For Text GCN , we set the embedding size of the first convolution layer as 200 and set the window size as 20 .","[('For', (0, 1)), ('set', (5, 6)), ('of', (9, 10)), ('as', (14, 15)), ('set', (17, 18)), ('as', (21, 22))]","[('Text GCN', (1, 3)), ('embedding size', (7, 9)), ('first convolution layer', (11, 14)), ('200', (15, 16)), ('window size', (19, 21)), ('20', (22, 23))]",[],[],[],[]
hyperparameters,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )","[('For', (0, 1)), ('using', (3, 4)), ('used', (9, 10))]","[('pre-trained word embeddings', (4, 7)), ('300 dimensional Glo Ve word embeddings', (10, 16))]",[],[],[],[]
results,"Text GCN performs the best and significantly outperforms all baseline models ( p < 0.05 based on student t- test ) on four datasets , which showcases the effectiveness of the proposed method on long text datasets .","[('performs', (2, 3)), ('based on', (15, 17))]","[('Text GCN', (0, 2)), ('best and significantly outperforms', (4, 8)), ('all baseline models ( p < 0.05', (8, 15))]",[],[],[],[]
results,"For more in - depth performance analysis , we note that TF - IDF + LR performs well on long text datasets like 20 NG and can outperform CNN with randomly initialized word embeddings .","[('note', (9, 10)), ('performs', (16, 17)), ('on', (18, 19)), ('like', (22, 23)), ('with', (29, 30))]","[('TF - IDF + LR', (11, 16)), ('well', (17, 18)), ('long text datasets', (19, 22)), ('20 NG', (23, 25)), ('outperform', (27, 28)), ('CNN', (28, 29)), ('randomly initialized word embeddings', (30, 34))]",[],[],[],[]
results,"When pre-trained Glo Ve word embeddings are provided , CNN performs much better , especially on Ohsumed and 20 NG .","[('When', (0, 1)), ('provided', (7, 8)), ('performs', (10, 11)), ('especially on', (14, 16))]","[('pre-trained Glo Ve word embeddings', (1, 6)), ('CNN', (9, 10)), ('much better', (11, 13)), ('Ohsumed and 20 NG', (16, 20))]",[],[],[],[]
results,"CNN also achieves the best results on short text dataset MR with pre-trained word embeddings , which shows it can 7 http://nlp.stanford.edu/data/glove.6B.zip model consecutive and short - distance semantics well .","[('achieves', (2, 3)), ('on', (6, 7))]","[('CNN', (0, 1)), ('best results', (4, 6)), ('short text dataset MR', (7, 11))]",[],[],[],[]
results,"PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .","[('achieves', (3, 4)), ('to', (6, 7)), ('on', (9, 10))]","[('PV - DBOW', (0, 3)), ('comparable results', (4, 6)), ('strong baselines', (7, 9)), ('20 NG and Ohsumed', (10, 14))]",[],[],[],[]
results,"PV - DM performs worse than PV - DBOW , the only comparable results are on MR , where word orders are more essential .","[('performs', (3, 4)), ('than', (5, 6))]","[('PV - DM', (0, 3)), ('worse', (4, 5)), ('PV - DBOW', (6, 9))]",[],[],[],[]
results,The results of PV - DBOW and PV - DM indicate that unsupervised document embeddings are not very discriminative in text classification .,"[('of', (2, 3)), ('indicate', (10, 11)), ('are', (15, 16)), ('in', (19, 20))]","[('PV - DBOW and PV - DM', (3, 10)), ('unsupervised document embeddings', (12, 15)), ('not very discriminative', (16, 19)), ('text classification', (20, 22))]",[],[],[],[]
results,PTE and fast Text clearly outperform PV - DBOW and PV - DM because they learn document embeddings in a supervised manner so that label information can be utilized to learn more discriminative embeddings .,"[('clearly', (4, 5))]","[('PTE and fast Text', (0, 4)), ('PV - DBOW and PV - DM', (6, 13))]",[],[],[],[]
results,"The two recent methods SWEM and LEAM perform quite well , which demonstrates the effectiveness of simple pooling methods and label descriptions / embeddings .","[('perform', (7, 8))]","[('SWEM and LEAM', (4, 7)), ('quite well', (8, 10))]",[],[],[],[]
results,Graph - CNN models also show competitive performances .,"[('show', (5, 6))]","[('Graph - CNN models', (0, 4)), ('competitive performances', (6, 8))]",[],[],[],[]
baselines,"1 ) the text graph can capture both document - word relations and global word - word relations ; 2 ) the GCN model , as a special form of Laplacian smoothing , computes the new features of anode as the weighted average of itself and its second order neighbors .","[('can capture', (5, 7)), ('computes', (33, 34)), ('of', (37, 38)), ('as', (39, 40)), ('of', (43, 44))]","[('text graph', (3, 5)), ('document - word relations', (8, 12)), ('GCN model', (22, 24)), ('Laplacian smoothing', (30, 32)), ('new features', (35, 37)), ('anode', (38, 39)), ('weighted average', (41, 43)), ('itself', (44, 45)), ('second order neighbors', (47, 50))]",[],[],[],[]
baselines,"The label information of document nodes can be passed to their neighboring word nodes ( words within the documents ) , then relayed to other word nodes and document nodes thatare neighbor to the first step neighboring word nodes .","[('of', (3, 4)), ('passed to', (8, 10)), ('relayed to', (22, 24)), ('thatare neighbor to', (30, 33))]","[('label information', (1, 3)), ('document nodes', (4, 6)), ('neighboring word nodes ( words within', (11, 17)), ('documents )', (18, 20)), ('other word nodes and document nodes', (24, 30)), ('first step neighboring word nodes', (34, 39))]",[],[],[],[]
results,"However , we also observed that Text GCN did not outperform CNN and LSTM - based models on MR .","[('observed', (4, 5)), ('did not', (8, 10)), ('on', (17, 18))]","[('Text GCN', (6, 8)), ('CNN and LSTM - based models', (11, 17)), ('MR', (18, 19))]",[],[],[],[]
results,We note that Text GCN can achieve higher test accuracy with limited labeled documents .,"[('note', (1, 2)), ('can achieve', (5, 7)), ('with', (10, 11))]","[('Text GCN', (3, 5)), ('higher test accuracy', (7, 10)), ('limited labeled documents', (11, 14))]",[],[],[],[]
research-problem,Deep Pyramid Convolutional Neural Networks for Text Categorization,[],[],[],[],[],[]
research-problem,This paper proposes a low - complexity word - level deep convolutional neural network ( CNN ) architecture for text categorization that can efficiently represent longrange associations in text .,"[('proposes', (2, 3)), ('for', (18, 19)), ('efficiently represent', (23, 25))]","[('text categorization', (19, 21)), ('longrange associations in text', (25, 29))]",[],[],[],[]
model,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .","[('call', (1, 2)), ('as', (10, 11)), ('per', (14, 15)), ('in', (18, 19))]","[('deep pyramid CNN ( DPCNN )', (3, 9)), ('computation time', (12, 14)), ('layer', (15, 16)), ('decreases', (16, 17)), ('exponentially', (17, 18)), (""' pyramid shape"", (20, 23))]",[],[],[],[]
model,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .","[('converting', (1, 2)), ('to', (4, 5)), ('alternates', (12, 13)), ('over and', (20, 22)), ('leading to', (25, 27)), ('in which', (30, 32)), ('shrinks in', (42, 44))]","[('discrete text', (2, 4)), ('continuous representation', (5, 7)), ('DPCNN architecture', (9, 11)), ('convolution block and a downsampling layer', (14, 20)), ('1', (23, 24)), ('deep network', (28, 30)), ('internal data size ( as well as per-layer computation )', (32, 42)), ('pyramid shape', (45, 47))]",[],[],[],[]
model,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .","[('performs', (3, 4)), ('generalizes', (9, 10)), ('to', (14, 15)), ('of', (17, 18)), ('covering', (20, 21))]","[('first layer', (1, 3)), ('text region embedding', (4, 7)), ('commonly used word embedding', (10, 14)), ('embedding', (16, 17)), ('text regions', (18, 20)), ('one or more words', (21, 25))]",[],[],[],[]
model,We use max pooling for all pooling layers .,"[('use', (1, 2)), ('for', (4, 5))]","[('max pooling', (2, 4)), ('all pooling layers', (5, 8))]",[],[],[],[]
hyperparameters,"To minimize a log loss with softmax , minibatch SGD with momentum 0.9 was conducted for n epochs ( n was fixed to 50 for AG , 30 for Yelp.f / p and Dbpedia , and 15 for the rest ) while the learning rate was set to ?","[('To minimize', (0, 2)), ('with', (5, 6)), ('with', (10, 11)), ('conducted for', (14, 16)), ('set to', (46, 48))]","[('log loss', (3, 5)), ('softmax', (6, 7)), ('minibatch SGD', (8, 10)), ('momentum 0.9', (11, 13)), ('n epochs', (16, 18)), ('learning rate', (43, 45))]",[],[],[],[]
hyperparameters,The minibatch size was fixed to 100 .,"[('fixed to', (4, 6))]","[('minibatch size', (1, 3)), ('100', (6, 7))]",[],[],[],[]
hyperparameters,Regularization was done by weight decay with the parameter 0.0001 and by optional dropout with 0.5 applied to the input to the top layer .,"[('done by', (2, 4)), ('with', (6, 7)), ('by', (11, 12)), ('with', (14, 15)), ('applied to', (16, 18)), ('to', (20, 21))]","[('Regularization', (0, 1)), ('weight decay', (4, 6)), ('parameter 0.0001', (8, 10)), ('optional dropout', (12, 14)), ('0.5', (15, 16)), ('input', (19, 20))]",[],[],[],[]
hyperparameters,"In some cases overfitting was observed , and so we performed early stopping , based on the validation performance , after reducing the learning rate to 0.1 ?.","[('observed', (5, 6)), ('performed', (10, 11)), ('after reducing', (20, 22)), ('to', (25, 26))]","[('overfitting', (3, 4)), ('early stopping', (11, 13)), ('learning rate', (23, 25)), ('0.1', (26, 27))]",[],[],[],[]
hyperparameters,Weights were initialized by the Gaussian distribution with zero mean and standard deviation 0.01 .,"[('initialized by', (2, 4)), ('with', (7, 8))]","[('Weights', (0, 1)), ('Gaussian distribution', (5, 7)), ('zero mean', (8, 10)), ('standard deviation 0.01', (11, 14))]",[],[],[],[]
hyperparameters,"The discrete input to the region embedding layer was fixed to the bow input , and the region size was chosen from { 1 , 3 , 5 } , while fixing output dimensionality to 250 ( same as convolution layers ) .","[('to', (3, 4)), ('fixed to', (9, 11)), ('chosen from', (20, 22)), ('fixing', (31, 32)), ('to', (34, 35))]","[('discrete input', (1, 3)), ('region embedding layer', (5, 8)), ('bow input', (12, 14)), ('region size', (17, 19)), ('{ 1 , 3 , 5 }', (22, 29)), ('output dimensionality', (32, 34)), ('250', (35, 36))]",[],[],[],[]
experiments,The dimensionality of unsupervised embeddings was set to 300 unless otherwise specified .,"[('of', (2, 3)), ('set to', (6, 8))]","[('dimensionality', (1, 2)), ('unsupervised embeddings', (3, 5)), ('300', (8, 9))]",[],[],[],[]
ablation-analysis,"In the results below , the depth of DPCNN was fixed to 15 unless otherwise specified .","[('of', (7, 8)), ('fixed to', (10, 12))]","[('depth', (6, 7)), ('DPCNN', (8, 9)), ('15', (12, 13))]",[],[],[],[]
results,Making it deeper did not substantially improve or degrade accuracy .,"[('Making', (0, 1)), ('did not', (3, 5))]","[('deeper', (2, 3)), ('substantially improve', (5, 7)), ('accuracy', (9, 10))]",[],[],[],[]
experiments,Large data results,[],[],[],[],[],[]
results,"On all the five datasets , DPCNN outperforms all of the previous results , which validates the effectiveness of our approach .","[('On', (0, 1)), ('validates', (15, 16))]","[('DPCNN', (6, 7)), ('outperforms', (7, 8)), ('all of the previous results', (8, 13)), ('effectiveness', (17, 18))]",[],[],[],[]
experiments,Small data results,[],[],[],[],[],[]
results,One difference from the large dataset results is that the strength of shallow models stands out .,"[('from', (2, 3)), ('stands', (14, 15))]","[('large dataset results', (4, 7)), ('strength of', (10, 12)), ('shallow models', (12, 14))]",[],[],[],[]
results,"ShallowCNN ( row 2 ) rivals DPCNN ( row 1 ) , and Zhang et al. 's best linear model ( row 3 ) moved up from the worst performer to the third best performer .","[('rivals', (5, 6)), ('moved up from', (24, 27)), ('to', (30, 31))]","[('ShallowCNN', (0, 1)), ('DPCNN', (6, 7)), ('best linear model', (17, 20)), ('worst performer', (28, 30)), ('third best performer', (32, 35))]",[],[],[],[]
results,The error rate improves as the depth increases .,"[('as', (4, 5))]","[('error rate', (1, 3)), ('improves', (3, 4)), ('depth increases', (6, 8))]",[],[],[],[]
research-problem,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,[],"[('Supervised and Semi- Supervised Text Categorization', (0, 6))]",[],[],[],[]
approach,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .","[('In', (0, 1)), ('at', (19, 20)), ('converted to', (23, 25)), ('with', (28, 29)), ('relevant to', (30, 32)), ('loosely term', (39, 41))]","[('convolution layer', (2, 4)), ('small region of data ( e.g. , a', (6, 14)), ('every location', (20, 22)), ('low-dimensional vector', (26, 28)), ('information', (29, 30)), ('task being', (33, 35)), ('preserved', (35, 36)), ('embedding', (42, 43))]",[],[],[],[]
approach,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .","[('shared among', (4, 6))]","[('embedding function', (1, 3)), ('all the locations', (6, 9)), ('useful features', (12, 14))]",[],[],[],[]
approach,"A document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .","[('represented as', (3, 5)), ('of', (7, 8)), ('converts', (30, 31)), ('of', (33, 34)), ('to', (45, 46)), ('at', (48, 49)), ('aggregates', (61, 62)), ('to', (66, 67)), ('by taking', (70, 72))]","[('document', (1, 2)), ('sequence', (6, 7)), ('convolution layer', (28, 30)), ('small regions', (31, 33)), ('document', (35, 36)), ('low-dimensional vectors', (46, 48)), ('every location ( embedding of text regions )', (49, 57)), ('pooling layer', (59, 61)), ('region embedding results', (63, 66)), ('document vector', (68, 70)), ('componentwise maximum or average', (72, 76)), ('linear model', (87, 89))]",[],[],[],[]
approach,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .","[('build on', (5, 7)), ('of', (10, 11)), ('explore', (18, 19)), ('via', (24, 25)), ('seeking to overcome', (34, 37)), ('in', (41, 42))]","[('general framework', (8, 10)), ('more sophisticated region embedding', (20, 24)), ('Long Short - Term Memory ( LSTM )', (25, 33)), ('shortcomings', (38, 39)), ('supervised and semi-supervised settings', (43, 47))]",[],[],[],[]
approach,LSTM ) is a recurrent neural network .,"[('is', (2, 3))]","[('LSTM', (0, 1)), ('recurrent neural network', (4, 7))]",[],[],[],[]
experiments,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .","[('including', (15, 16))]","[('both our LSTM models and one - hot CNN', (2, 11)), ('strongly outperform', (11, 13)), ('other methods', (13, 15)), ('previous LSTM', (16, 18))]",[],[],[],[]
experiments,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .","[('obtained', (4, 5)), ('combining', (6, 7)), ('trained on', (22, 24))]","[('best results', (1, 3)), ('two types of region embeddings ( LSTM embed - dings and CNN embeddings )', (8, 22)), ('unlabeled data', (24, 26))]",[],[],[],[]
code,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,[],"[('http://riejohnson.com/cnn download.html', (8, 10))]",[],[],[],[]
research-problem,Supervised LSTM for text categorization,[],[],[],[],[],[]
research-problem,Pooling : simplifying sub - problems,[],[],[],[],[],[]
research-problem,Chopping for speeding up training,[],[],[],[],[],[]
hyperparameters,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .","[('reduced to', (8, 10)), ('of', (16, 17)), ('to reduce', (20, 22)), ('minimized with', (28, 30)), ('applied to', (31, 33)), ('initialized by', (42, 44))]","[('neural network experiments', (2, 5)), ('vocabulary', (6, 7)), ('most frequent 30 K words', (11, 16)), ('training data', (18, 20)), ('computational burden', (22, 24)), ('square loss', (25, 27)), ('dropout', (30, 31)), ('input to', (34, 36)), ('top layer', (37, 39)), ('weights', (40, 41))]",[],[],[],[]
experiments,"RCV1 ( second - level topics only ) and 20 NG are for topic categorization of Reuters news articles and newsgroup messages , respectively .","[('for', (12, 13))]","[('20 NG', (9, 11)), ('topic categorization of Reuters news articles and newsgroup messages', (13, 22))]",[],[],[],[]
experiments,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,"[('done with', (2, 4)), ('with', (5, 6)), ('with', (11, 12)), ('optionally', (14, 15)), ('for', (16, 17))]","[('Optimization', (0, 1)), ('SGD', (4, 5)), ('mini-batch size 50 or 100', (6, 11)), ('momentum', (12, 13)), ('rmsprop', (15, 16)), ('acceleration', (17, 18))]",[],[],[],[]
experiments,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .","[('Comparing', (0, 1)), ('see that', (9, 11)), ('with', (17, 18)), ('outperforms', (25, 26)), ('on', (35, 36))]","[('our one - hot bidirectional LSTM', (11, 17)), ('pooling ( oh - 2 LSTMp )', (18, 25)), ('word - vector LSTM ( wv - LSTM )', (26, 35)), ('all the datasets', (36, 39))]",[],[],[],[]
tasks,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .","[('obtained by', (2, 4)), ('with', (22, 23)), ('on', (26, 27)), ('with', (33, 34))]","[('bow - CNN', (4, 7)), ('region size 20', (23, 26)), ('RCV1', (27, 28)), ('seq -CNN', (30, 32)), ('regular concatenation input', (35, 38)), ('region size', (40, 42))]",[],[],[],[]
results,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .","[('on', (2, 3))]","[('three out of the four datasets', (3, 9)), ('oh - 2 LSTMp', (10, 14)), ('outperforms', (14, 15)), ('SVM and the CNN', (15, 19))]",[],[],[],[]
results,"However , on RCV1 , it underperforms both .","[('on', (2, 3))]","[('RCV1', (3, 4)), ('underperforms', (6, 7))]",[],[],[],[]
results,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .","[('is', (6, 7)), ('than', (9, 10))]","[('RCV1', (2, 3)), ('n-gram SVM', (4, 6)), ('no better', (7, 9)), ('bag - of - word SVM', (10, 16)), ('outperforms', (25, 26)), ('seq-CNN', (26, 27))]",[],[],[],[]
results,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .","[('on', (3, 4)), ('in a', (9, 11)), ('at', (14, 15)), ('are', (17, 18)), ('than', (20, 21)), ('in', (22, 23))]","[('RCV1', (4, 5)), ('bags of words', (6, 9)), ('window of 20', (11, 14)), ('every location', (15, 17)), ('more useful', (18, 20)), ('words', (21, 22)), ('strict order', (23, 25))]",[],[],[],[]
results,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .","[('does not have', (5, 8)), ('loses to', (16, 18))]","[('LSTM', (2, 3)), ('words', (12, 13)), ('bags', (14, 15)), ('bow - CNN', (18, 21))]",[],[],[],[]
results,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .","[('of', (5, 6)), ('to embed', (7, 9)), ('appears not to be', (11, 15))]","[('strength', (4, 5)), ('LSTM', (6, 7)), ('larger regions', (9, 11)), ('big contributor', (16, 18))]",[],[],[],[]
results,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .","[('works', (6, 7))]","[('one - hot CNN', (2, 6)), ('surprising well', (7, 9))]",[],[],[],[]
results,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .","[('on', (4, 5)), ('is', (6, 7)), ('of', (15, 16)), ('obtained by', (18, 20)), ('of', (24, 25)), ('with', (27, 28))]","[('previous best performance', (1, 4)), ('20NG', (5, 6)), ('15.3', (7, 8)), ('DL15', (16, 17)), ('pre-training wv - LSTM', (20, 24)), ('1024 units', (25, 27)), ('labeled training data', (28, 31))]",[],[],[],[]
results,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .","[('achieved', (5, 6))]","[('Our oh - 2 LSTMp', (0, 5)), ('13.32', (6, 7)), ('2 % better', (10, 13))]",[],[],[],[]
tasks,"The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one - hot CNN , resulting in higher accuracy .","[('to produce', (5, 7)), ('to', (9, 10)), ('of', (14, 15)), ('resulting in', (20, 22))]","[('obtained tv-embeddings', (1, 3)), ('additional input', (7, 9)), ('supervised region embedding', (11, 14)), ('one - hot CNN', (15, 19)), ('higher accuracy', (22, 24))]",[],[],[],[]
results,"Compared with the supervised oh - 2 LSTMp , clear performance improvements were obtained on all the datasets , thus , confirming the effectiveness of our approach .","[('Compared with', (0, 2)), ('obtained on', (13, 15))]","[('supervised oh - 2 LSTMp', (3, 8)), ('clear performance improvements', (9, 12)), ('all the datasets', (15, 18))]",[],[],[],[]
results,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .","[('with', (18, 19))]","[('pre-trained wv - LSTM', (2, 6)), ('clearly outperformed', (6, 8)), ('supervised wv - LSTM', (9, 13)), ('underperformed', (15, 16)), ('models', (17, 18)), ('region tv-embeddings', (19, 21))]",[],[],[],[]
results,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .","[('using', (8, 9)), ('performed', (18, 19))]","[('wv - 2 LSTMp', (4, 8)), ('Google News vectors', (10, 13)), ('relatively poorly', (19, 21))]",[],[],[],[]
results,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .","[('trained with', (3, 5)), ('observed after', (13, 15)), ('scaled', (16, 17))]","[('word2vec', (1, 2)), ('domain unlabeled data', (6, 9)), ('better results', (10, 12)), ('word vectors', (17, 19))]",[],[],[],[]
results,"Still , it underperformed the models with region tv - embeddings ( row # 4 , 5 ) , which used the same domain unlabeled data .","[('used', (20, 21))]","[('underperformed', (3, 4)), ('models', (5, 6)), ('region tv - embeddings', (7, 11)), ('same domain unlabeled data', (22, 26))]",[],[],[],[]
results,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,"[('rivals', (7, 8)), ('on', (17, 18)), ('on', (24, 25))]","[('LSTM', (1, 2)), ('CNN', (11, 12)), ('IMDB / Elec', (18, 21)), ('underperforms', (22, 23)), ('RCV1', (25, 26))]",[],[],[],[]
results,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .","[('Increasing', (0, 1)), ('of', (3, 4)), ('from', (6, 7)), ('on', (10, 11)), ('obtain', (14, 15))]","[('dimensionality', (2, 3)), ('LSTM tvembeddings', (4, 6)), ('100 to 300', (7, 10)), ('RCV1', (11, 12)), ('8.62', (15, 16))]",[],[],[],[]
results,"For example , adding the CNN tv-embeddings to the LSTM of row# 1 , the error rate on IMDB improved from 6.66 to 5.94 , and adding the LSTM tv-embeddings to the CNN of row # 2 , the error rate on RCV1 improved from 7.71 to 7.15 .","[('adding', (3, 4)), ('on', (17, 18)), ('improved from', (19, 21)), ('on', (41, 42)), ('improved from', (43, 45))]","[('CNN tv-embeddings', (5, 7)), ('error rate', (15, 17)), ('IMDB', (18, 19)), ('6.66 to', (21, 23)), ('5.94', (23, 24)), ('error rate', (39, 41)), ('RCV1', (42, 43)), ('7.71', (45, 46)), ('7.15', (47, 48))]",[],[],[],[]
results,"The results indicate that , as expected , LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined .","[('indicate', (2, 3)), ('when', (19, 20))]","[('LSTM tv-embeddings and CNN tv-embeddings', (8, 13)), ('complement', (13, 14)), ('each other', (14, 16)), ('improve', (17, 18)), ('performance', (18, 19)), ('combined', (20, 21))]",[],[],[],[]
results,"The best supervised results on IMDB / Elec of JZ15a are in the first row , obtained by integrating a document embedding layer into one - hot CNN .","[('on', (4, 5)), ('obtained by integrating', (16, 19)), ('into', (23, 24))]","[('best supervised results', (1, 4)), ('IMDB / Elec of JZ15a', (5, 10)), ('document embedding layer', (20, 23)), ('one - hot CNN', (24, 28))]",[],[],[],[]
results,"As shown in the last row of , our new model further improved it to 5.94 ; also on Elec and RCV1 , our best models exceeded the previous best results .","[('further improved it', (11, 14)), ('to', (14, 15)), ('on', (18, 19)), ('exceeded', (26, 27))]","[('our new model', (8, 11)), ('5.94', (15, 16)), ('Elec and RCV1', (19, 22)), ('our best models', (23, 26)), ('previous best results', (28, 31))]",[],[],[],[]
research-problem,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,[],[],[],[],[],[]
model,We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself .,"[('extend', (1, 2)), ('to', (7, 8)), ('by applying', (11, 13)), ('to', (14, 15)), ('in', (18, 19)), ('rather than to', (23, 26))]","[('adversarial and virtual adversarial training', (2, 7)), ('text domain', (9, 11)), ('perturbations', (13, 14)), ('word embeddings', (16, 18)), ('recurrent neural network', (20, 23)), ('original input itself', (27, 30))]",[],[],[],[]
model,"It improves not only robustness to adversarial examples , but also generalization performance for original examples .","[('improves', (1, 2)), ('to', (5, 6)), ('for', (13, 14))]","[('robustness', (4, 5)), ('adversarial examples', (6, 8)), ('generalization performance', (11, 13)), ('original examples', (14, 16))]",[],[],[],[]
model,"This is done by regularizing the model so that given an example , the model will produce the same output distribution as it produces on an adversarial perturbation of that example .","[('done by', (2, 4)), ('given', (9, 10)), ('as', (21, 22)), ('produces on', (23, 25)), ('of', (28, 29))]","[('regularizing', (4, 5)), ('model', (6, 7)), ('example', (11, 12)), ('model', (14, 15)), ('same output distribution', (18, 21)), ('adversarial perturbation', (26, 28)), ('example', (30, 31))]",[],[],[],[]
model,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .","[('define', (18, 19)), ('on', (21, 22)), ('instead of', (25, 27))]","[('perturbation', (20, 21)), ('continuous word embeddings', (22, 25)), ('discrete word inputs', (27, 30))]",[],[],[],[]
experimental-setup,All experiments used TensorFlow on GPUs .,"[('used', (2, 3)), ('on', (4, 5))]","[('TensorFlow', (3, 4)), ('GPUs', (5, 6))]",[],[],[],[]
code,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text.,[],[],[],[],[],[]
experimental-setup,We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,"[('applied', (1, 2)), ('with', (4, 5)), ('set to', (6, 8)), ('on', (9, 10)), ('except', (13, 14))]","[('gradient clipping', (2, 4)), ('norm', (5, 6)), ('1.0', (8, 9)), ('all the parameters', (10, 13)), ('word embeddings', (14, 16))]",[],[],[],[]
experimental-setup,"To reduce runtime on GPU , we used truncated backpropagation up to 400 words from each end of the sequence .","[('To reduce', (0, 2)), ('on', (3, 4)), ('used', (7, 8)), ('up to', (10, 12)), ('from', (14, 15))]","[('runtime', (2, 3)), ('GPU', (4, 5)), ('truncated backpropagation', (8, 10)), ('400 words', (12, 14)), ('each end of the', (15, 19)), ('sequence', (19, 20))]",[],[],[],[]
experimental-setup,"For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .","[('For', (0, 1)), ('of', (2, 3)), ('applied', (9, 10)), ('on', (11, 12)), ('with', (16, 17))]","[('regularization', (1, 2)), ('recurrent language model', (4, 7)), ('dropout', (10, 11)), ('word embedding layer', (13, 16)), ('0.5 dropout rate', (17, 20))]",[],[],[],[]
experimental-setup,"For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .","[('For', (0, 1)), ('used', (7, 8)), ('for', (12, 13)), ('used', (24, 25)), ('shared with', (31, 33))]","[('bidirectional LSTM model', (2, 5)), ('512 hidden units LSTM', (8, 12)), ('standard order and reversed order sequences', (15, 21)), ('256 dimensional word embeddings', (25, 29)), ('both of the LSTMs', (33, 37))]",[],[],[],[]
results,Pretraining with a recurrent language model was very effective on classification performance on all the datasets we tested on and so our results in Section 5 are with this pretraining .,"[('on', (9, 10))]","[('classification performance', (10, 12))]",[],[],[],[]
experimental-setup,"For optimization , we again used the Adam optimizer , with 0.0005 initial learning rate 0.9998 exponential decay .","[('used', (5, 6)), ('with', (10, 11))]","[('optimization', (1, 2)), ('Adam optimizer', (7, 9)), ('0.0005 initial learning rate', (11, 15))]",[],[],[],[]
experimental-setup,"Batch sizes are 64 on IMDB , Elec , RCV1 , and 128 on DBpedia .","[('are', (2, 3)), ('on', (4, 5)), ('on', (13, 14))]","[('Batch sizes', (0, 2)), ('64', (3, 4)), ('IMDB', (5, 6)), ('Elec', (7, 8)), ('RCV1', (9, 10)), ('128', (12, 13)), ('DBpedia', (14, 15))]",[],[],[],[]
experimental-setup,We again applied gradient clipping with the norm as 1.0 on all the parameters except the word embedding .,"[('applied', (2, 3)), ('with', (5, 6)), ('as', (8, 9)), ('on', (10, 11)), ('except', (14, 15))]","[('gradient clipping', (3, 5)), ('norm', (7, 8)), ('1.0', (9, 10)), ('all the parameters', (11, 14)), ('word embedding', (16, 18))]",[],[],[],[]
results,Every adversarial training method outperformed every random perturbation method .,[],"[('Every adversarial training method', (0, 4)), ('outperformed', (4, 5)), ('every random perturbation method', (5, 9))]",[],[],[],[]
results,"For the baseline and random perturbation method , the cosine distances were 0.361 and 0.377 , respectively .","[('For', (0, 1)), ('were', (11, 12))]","[('baseline and random perturbation method', (2, 7)), ('cosine distances', (9, 11)), ('0.361 and 0.377', (12, 15))]",[],[],[],[]
results,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .","[('on', (9, 10))]","[('our proposed method', (3, 6)), ('improved test performance', (6, 9)), ('baseline method', (11, 13))]",[],[],[],[]
results,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,"[('further improves', (19, 21)), ('on', (22, 23))]","[('Our unidirectional LSTM model', (0, 4)), ('improves', (4, 5)), ('state of the art method', (7, 12)), ('our method', (13, 15)), ('results', (21, 22)), ('RCV1', (23, 24))]",[],[],[],[]
results,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .","[('able to', (3, 5)), ('over', (6, 7)), ('with', (12, 13)), ('achieved', (20, 21)), ('as', (25, 26))]","[('Adversarial training', (0, 2)), ('improve', (5, 6)), ('baseline method', (8, 10)), ('almost the same performance', (21, 25)), ('current state of the art method', (27, 33))]",[],[],[],[]
results,"We can see that the baseline method has already achieved nearly the current state of the art performance , and our proposed method improves from the baseline method .","[('see that', (2, 4)), ('achieved', (9, 10)), ('improves from', (23, 25))]","[('baseline method', (5, 7)), ('nearly the current state of the art performance', (10, 18)), ('our proposed method', (20, 23)), ('baseline method', (26, 28))]",[],[],[],[]
research-problem,A C - LSTM Neural Network for Text Classification,[],[],[],[],[],[]
research-problem,Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling .,"[('demonstrated', (5, 6)), ('in', (13, 14))]","[('Neural network models', (0, 3)), ('remarkable performance', (11, 13)), ('sentence and document modeling', (14, 18))]",[],[],[],[]
research-problem,C - LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics .,"[('able to capture', (4, 7)), ('of', (10, 11)), ('as', (12, 13))]","[('C - LSTM', (0, 3)), ('both local features', (7, 10)), ('phrases', (11, 12)), ('global and temporal sentence semantics', (15, 20))]",[],[],[],[]
model,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .","[('introduce', (5, 6)), ('for', (10, 11)), ('by combining', (14, 16)), ('to', (19, 20))]","[('C - LSTM', (11, 14)), ('CNN and LSTM', (16, 19)), ('model sentences', (20, 22))]",[],[],[],[]
model,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .","[('To benefit from', (0, 3)), ('of', (5, 6)), ('design', (12, 13)), ('by feeding', (23, 25)), ('of', (27, 28)), ('into', (33, 34))]","[('advantages', (4, 5)), ('simple end - to - end , unified architecture', (14, 23)), ('output', (26, 27)), ('one - layer CNN', (29, 33)), ('LSTM', (34, 35))]",[],[],[],[]
model,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,"[('constructed on top of', (3, 7)), ('from', (11, 12)), ('to learn', (16, 18)), ('of', (22, 23))]","[('CNN', (1, 2)), ('pre-trained word vectors', (8, 11)), ('massive unlabeled text data', (12, 16)), ('higher - level representions', (18, 22)), ('n-grams', (23, 24))]",[],[],[],[]
model,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .","[('to learn', (1, 3)), ('from', (5, 6)), ('of', (15, 16)), ('organized as', (18, 20)), ('to serve', (23, 25)), ('of', (28, 29))]","[('sequential correlations', (3, 5)), ('higher - level suqence representations', (6, 11)), ('feature maps', (13, 15)), ('CNN', (16, 17)), ('sequential window features', (20, 23)), ('input', (27, 28)), ('LSTM', (29, 30))]",[],[],[],[]
model,"In this way , instead of constructing LSTM directly from the input sentence , we first transform each sentence into successive window ( n- gram ) features to help disentangle factors of variations within sentences .","[('directly from', (8, 10)), ('first transform', (15, 17)), ('into', (19, 20)), ('to help disentangle', (27, 30)), ('of', (31, 32)), ('within', (33, 34))]","[('constructing', (6, 7)), ('LSTM', (7, 8)), ('input sentence', (11, 13)), ('each sentence', (17, 19)), ('successive window ( n- gram ) features', (20, 27)), ('factors', (30, 31)), ('variations', (32, 33)), ('sentences', (34, 35))]",[],[],[],[]
experiments,"We also show that the combination of CNN and LSTM outperforms individual multi - layer CNN models and RNN models , which indicates that LSTM can learn longterm dependencies from sequences of higher - level representations better than the other models .","[('show', (2, 3))]","[('combination of', (5, 7)), ('CNN and LSTM', (7, 10)), ('outperforms', (10, 11)), ('individual multi - layer CNN models and RNN models', (11, 20))]",[],[],[],[]
baselines,"We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .","[('implement', (1, 2)), ('based on', (4, 6)), ('supports', (14, 15)), ('of', (21, 22))]","[('Theano', (6, 7)), ('python library', (10, 12)), ('efficient symbolic differentiation', (15, 18)), ('transparent use', (19, 21)), ('GPU', (23, 24))]",[],[],[],[]
experimental-setup,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .","[('To benefit from', (0, 3)), ('of', (8, 9)), ('train', (13, 14)), ('on', (16, 17))]","[('efficiency of parallel computation', (4, 8)), ('tensors', (10, 11)), ('model', (15, 16)), ('GPU', (18, 19))]",[],[],[],[]
experimental-setup,"For text preprocessing , we only convert all characters in the dataset to lowercase .","[('For', (0, 1)), ('convert', (6, 7)), ('in', (9, 10)), ('to', (12, 13))]","[('text preprocessing', (1, 3)), ('all characters', (7, 9)), ('dataset', (11, 12)), ('lowercase', (13, 14))]",[],[],[],[]
experimental-setup,"For SST , we conduct hyperparameter ( number of filters , filter length in CNN ; memory dimension in LSTM ; dropout rate and which layer to apply , etc. ) tuning on the validation data in the standard split .","[('For', (0, 1)), ('conduct', (4, 5)), ('on', (32, 33)), ('in', (36, 37))]","[('SST', (1, 2)), ('hyperparameter ( number of filters , filter length in CNN ;', (5, 16)), ('memory dimension in LSTM ;', (16, 21)), ('dropout rate and which layer to apply , etc. ) tuning', (21, 32)), ('validation data', (34, 36)), ('standard split', (38, 40))]",[],[],[],[]
experimental-setup,"For TREC , we holdout 1000 samples from the training dataset for hyperparameter search and train the model using the remaining data .","[('For', (0, 1)), ('holdout', (4, 5)), ('from', (7, 8)), ('for', (11, 12)), ('train', (15, 16)), ('using', (18, 19))]","[('TREC', (1, 2)), ('1000 samples', (5, 7)), ('training dataset', (9, 11)), ('hyperparameter search', (12, 14)), ('model', (17, 18)), ('remaining data', (20, 22))]",[],[],[],[]
experimental-setup,"In our final settings , we only use one convolutional layer and one LSTM layer for both tasks .","[('use', (7, 8)), ('for', (15, 16))]","[('one convolutional layer', (8, 11)), ('both tasks', (16, 18))]",[],[],[],[]
experimental-setup,"For the filter size , we investigated filter lengths of 2 , 3 and 4 in two cases : a ) single convolutional layer with the same filter length , and b ) multiple convolutional layers with different lengths of filters in parallel .","[('For', (0, 1)), ('investigated', (6, 7)), ('of', (9, 10)), ('in', (15, 16)), ('with', (24, 25)), ('with', (36, 37))]","[('filter size', (2, 4)), ('filter lengths', (7, 9)), ('2 , 3 and 4', (10, 15)), ('two cases', (16, 18)), ('single convolutional layer', (21, 24)), ('same filter length', (26, 29)), ('multiple convolutional layers', (33, 36)), ('different lengths of filters in parallel', (37, 43))]",[],[],[],[]
baselines,Binary is a 2 - classification task .,"[('is', (1, 2))]","[('Binary', (0, 1)), ('2 - classification task', (3, 7))]",[],[],[],[]
baselines,The third block are methods related to convolutional neural networks .,"[('related to', (5, 7))]","[('methods', (4, 5)), ('convolutional neural networks', (7, 10))]",[],[],[],[]
experimental-setup,The last block is our model .,[],"[('our model', (4, 6))]",[],[],[],[]
baselines,features after convolution and the sequence of window representations is fed into LSTM .,"[('of', (6, 7)), ('fed into', (10, 12))]","[('features after convolution and', (0, 4)), ('sequence', (5, 6)), ('window representations', (7, 9)), ('LSTM', (12, 13))]",[],[],[],[]
experimental-setup,We also exploit different combinations of different filter lengths .,"[('exploit', (2, 3)), ('of', (5, 6))]","[('different combinations', (3, 5)), ('different filter lengths', (6, 9))]",[],[],[],[]
experimental-setup,"According to the experiments , we choose a single convolutional layer with filter length","[('choose', (6, 7)), ('with', (11, 12))]","[('single convolutional layer', (8, 11))]",[],[],[],[]
experimental-setup,"3 . For SST , the number of filters of length 3 is set to be 150 and the memory dimension of LSTM is set to be 150 , too .","[('of', (9, 10)), ('set to', (13, 15)), ('of', (21, 22)), ('set to', (24, 26))]","[('SST', (3, 4)), ('number of filters', (6, 9)), ('length 3', (10, 12)), ('150', (16, 17)), ('memory dimension', (19, 21)), ('LSTM', (22, 23)), ('150', (27, 28))]",[],[],[],[]
experimental-setup,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,"[('of', (13, 14))]","[('word vector layer and the LSTM layer', (1, 8)), ('dropped outwith', (9, 11)), ('probability', (12, 13)), ('0.5', (14, 15))]",[],[],[],[]
experimental-setup,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .","[('For', (0, 1)), ('set to', (8, 10)), ('set to', (17, 19))]","[('TREC', (1, 2)), ('number of filters', (4, 7)), ('300', (11, 12)), ('memory dimension', (14, 16)), ('300', (20, 21))]",[],[],[],[]
experimental-setup,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,"[('of', (13, 14))]","[('word vector layer and the LSTM layer', (1, 8)), ('dropped outwith', (9, 11)), ('probability', (12, 13)), ('0.5', (14, 15))]",[],[],[],[]
experimental-setup,We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,"[('add', (2, 3)), ('with', (5, 6)), ('of', (8, 9)), ('to', (10, 11)), ('in', (13, 14))]","[('L2 regularization', (3, 5)), ('factor', (7, 8)), ('0.001', (9, 10)), ('weights', (12, 13)), ('softmax layer', (15, 17))]",[],[],[],[]
results,"For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .","[('For', (0, 1)), ('achieve', (7, 8)), ('with respect to', (10, 13))]","[('binary classification task', (2, 5)), ('comparable results', (8, 10)), ('state - of - the - art ones', (14, 22))]",[],[],[],[]
results,( 2 ) Comparing our results against single CNN and LSTM models shows that LSTM does learn long - term dependencies across sequences of higher - level representations better .,"[('Comparing', (3, 4)), ('against', (6, 7)), ('shows', (12, 13)), ('across', (21, 22)), ('of', (23, 24))]","[('single CNN and LSTM models', (7, 12)), ('LSTM', (14, 15)), ('long - term dependencies', (17, 21)), ('sequences', (22, 23)), ('higher - level representations better', (24, 29))]",[],[],[],[]
results,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .","[('consistently outperforms', (5, 7))]","[('Our result', (3, 5)), ('all published neural baseline models', (7, 12))]",[],[],[],[]
results,( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,"[('close to', (6, 8)), ('depends on', (20, 22))]","[('Our result', (3, 5)), ('state - of - the - art SVM', (11, 19)), ('highly engineered features', (22, 25))]",[],[],[],[]
ablation-analysis,Here we investigate the impact of different filter configurations in the convolutional layer on the model performance .,"[('investigate', (2, 3)), ('of', (5, 6)), ('in', (9, 10)), ('on', (13, 14))]","[('impact', (4, 5)), ('different filter configurations', (6, 9)), ('convolutional layer', (11, 13)), ('model performance', (15, 17))]",[],[],[],[]
ablation-analysis,"For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .","[('For the case of', (0, 4)), ('shown that', (12, 14)), ('with', (16, 17)), ('performs', (20, 21)), ('that', (22, 23))]","[('multiple convolutional layers', (4, 7)), ('filter configurations', (14, 16)), ('filter length 3', (17, 20)), ('better', (21, 22))]",[],[],[],[]
research-problem,Very Deep Convolutional Networks for Text Classification,[],[],[],[],[],[]
research-problem,We present a new architecture ( VD - CNN ) for text processing which operates directly at the character level and uses only small convolutions and pooling operations .,"[('operates directly at', (14, 17)), ('uses', (21, 22))]","[('text processing', (11, 13)), ('character level', (18, 20)), ('pooling operations', (26, 28))]",[],[],[],[]
model,The fundamental idea of is to consider feature extraction and classification as one jointly trained task .,"[('consider', (6, 7)), ('as', (11, 12))]","[('feature extraction and classification', (7, 11))]",[],[],[],[]
model,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .","[('of', (10, 11)), ('to approach', (14, 16)), ('using', (19, 20))]","[('deep architectures', (8, 10)), ('many convolutional layers', (11, 14)), ('up to 29 layers', (20, 24))]",[],[],[],[]
experiments,The proposed deep convolutional network shows significantly better results than previous ConvNets approach .,"[('shows', (5, 6)), ('than', (9, 10))]","[('proposed deep convolutional network', (1, 5)), ('significantly better results', (6, 9)), ('previous ConvNets approach', (10, 13))]",[],[],[],[]
results,Going from depth 9 to 17 and 29 for Amazon Full reduces the error rate by 1 % absolute .,"[('Going from', (0, 2)), ('for', (8, 9)), ('reduces', (11, 12)), ('by', (15, 16))]","[('depth 9 to 17 and 29', (2, 8)), ('Amazon Full', (9, 11)), ('error rate', (13, 15)), ('1 % absolute', (16, 19))]",[],[],[],[]
results,"Overall , compared to previous state - of - the - art , our best architecture with depth 29 and max - pooling has a test error of 37.0 compared to 40.43 % .","[('compared to', (2, 4)), ('with', (16, 17)), ('of', (27, 28)), ('compared to', (29, 31))]","[('previous state - of - the - art', (4, 12)), ('our best architecture', (13, 16)), ('depth 29 and max - pooling', (17, 23)), ('test error', (25, 27)), ('37.0', (28, 29)), ('40.43 %', (31, 33))]",[],[],[],[]
experiments,Max - pooling performs better than other pooling types .,"[('performs', (3, 4)), ('than', (5, 6))]","[('Max - pooling', (0, 3)), ('better', (4, 5)), ('other pooling types', (6, 9))]",[],[],[],[]
experiments,Our models outperform state - of - the - art Con -vNets .,[],"[('outperform', (2, 3)), ('state - of - the - art Con -vNets', (3, 12))]",[],[],[],[]
experiments,"When using shortcut connections , we observe improved results when the network has 49 layers : both the training and test errors go down and the network is less prone to underfitting than it was without shortcut connections .","[('using', (1, 2)), ('observe', (6, 7)), ('when', (9, 10)), ('go', (22, 23))]","[('shortcut connections', (2, 4)), ('improved results', (7, 9)), ('network', (11, 12)), ('49 layers', (13, 15)), ('training and test errors', (18, 22)), ('down', (23, 24)), ('underfitting', (31, 32))]",[],[],[],[]
baselines,"Following , all processing is done at the character level which is the atomic representation of a sentence , same as pixels for images .","[('done at', (5, 7)), ('which is', (10, 12)), ('of', (15, 16))]","[('all processing', (2, 4)), ('character level', (8, 10)), ('atomic representation', (13, 15)), ('sentence', (17, 18))]",[],[],[],[]
experimental-setup,The character embedding is of size 16 .,"[('of size', (4, 6))]","[('character embedding', (1, 3)), ('16', (6, 7))]",[],[],[],[]
experimental-setup,"Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .","[('performed with', (2, 4)), ('using', (6, 7)), ('of size', (9, 11))]","[('Training', (0, 1)), ('SGD', (4, 5)), ('mini-batch', (8, 9)), ('128', (11, 12)), ('initial learning rate', (14, 17)), ('0.01', (18, 19)), ('momentum', (20, 21)), ('0.9', (22, 23))]",[],[],[],[]
experimental-setup,We initialize our convolutional layers following .,"[('initialize', (1, 2))]","[('convolutional layers', (3, 5))]",[],[],[],[]
experimental-setup,The implementation is done using Torch 7 .,"[('done using', (3, 5))]","[('Torch', (5, 6))]",[],[],[],[]
experimental-setup,All experiments are performed on a single NVidia K40 GPU .,"[('performed on', (3, 5))]","[('single NVidia K40 GPU', (6, 10))]",[],[],[],[]
experimental-setup,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .","[('use', (13, 14)), ('without', (17, 18))]","[('temporal batch norm', (14, 17)), ('dropout', (18, 19))]",[],[],[],[]
results,"Our deep architecture works well on big data sets in particular , even for small depths .","[('works', (3, 4)), ('on', (5, 6)), ('for', (13, 14))]","[('Our deep architecture', (0, 3)), ('well', (4, 5)), ('big data sets', (6, 9)), ('small depths', (14, 16))]",[],[],[],[]
results,"For the smallest depth we use ( 9 convolutional layers ) , we see that our model already performs better than Zhang 's convolutional baselines ( which includes 6 convolutional layers and has a different architecture ) on the biggest data sets :","[('For', (0, 1)), ('see that', (13, 15)), ('performs', (18, 19)), ('than', (20, 21)), ('on', (37, 38))]","[('smallest depth', (2, 4)), ('our model', (15, 17)), ('better', (19, 20)), (""Zhang 's convolutional baselines"", (21, 25)), ('biggest data sets', (39, 42))]",[],[],[],[]
results,"Yelp Full , Yahoo Answers and Amazon Full and Polarity .",[],"[('Yelp Full', (0, 2)), ('Amazon Full and Polarity', (6, 10))]",[],[],[],[]
results,The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples . :,"[('in', (4, 5)), ('observed on', (9, 11)), ('which', (17, 18))]","[('most important decrease', (1, 4)), ('classification error', (5, 7)), ('largest data set Amazon Full', (12, 17)), ('more than 3 Million training samples', (19, 25))]",[],[],[],[]
results,"We also observe that for a small depth , temporal max - pooling works best on all data sets .","[('observe', (2, 3)), ('for', (4, 5)), ('works', (13, 14)), ('on', (15, 16))]","[('small depth', (6, 8)), ('temporal max - pooling', (9, 13)), ('best', (14, 15)), ('all data sets', (16, 19))]",[],[],[],[]
research-problem,Character - level Convolutional Networks for Text Classification,[],[],[],[],[],[]
research-problem,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",[],"[('Text classification', (0, 2))]",[],[],[],[]
model,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .","[('explore treating', (4, 6)), ('as', (7, 8)), ('at', (13, 14)), ('applying', (18, 19))]","[('raw signal', (11, 13)), ('character level', (14, 16)), ('temporal ( one-dimensional ) ConvNets', (19, 24))]",[],[],[],[]
model,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .","[('is', (2, 3)), ('where', (5, 6)), ('obtained by', (9, 11)), ('to perform', (14, 16))]","[('design', (1, 2)), ('modular', (3, 4)), ('gradients', (7, 8)), ('back - propagation', (11, 14)), ('optimization', (16, 17))]",[],[],[],[]
experiments,The dimension of the embedding is 300 .,"[('of', (2, 3)), ('is', (5, 6))]","[('dimension', (1, 2)), ('embedding', (4, 5)), ('300', (6, 7))]",[],[],[],[]
experiments,The number of means is 5000 .,"[('is', (4, 5))]","[('number of means', (1, 4)), ('5000', (5, 6))]",[],[],[],[]
research-problem,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling,[],"[('Text Classification', (0, 2))]",[],[],[],[]
model,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .","[('proposes', (5, 6)), ('to capture', (24, 26)), ('on both', (27, 29))]","[('Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )', (6, 24)), ('features', (26, 27)), ('time - step dimension', (30, 34)), ('feature vector dimension', (36, 39))]",[],[],[],[]
model,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,"[('utilizes', (2, 3)), ('to transform', (13, 15)), ('into', (17, 18))]","[('Bidirectional Long Short - Term Memory Networks ( BLSTM )', (3, 13)), ('text', (16, 17)), ('vectors', (18, 19))]",[],[],[],[]
model,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,"[('utilized to obtain', (7, 10))]","[('2D max pooling operation', (2, 6)), ('fixed - length vector', (11, 15))]",[],[],[],[]
model,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,"[('applies', (3, 4)), ('to capture', (11, 13)), ('to represent', (16, 18))]","[('2D convolution ( BLSTM - 2DCNN )', (4, 11)), ('more meaningful features', (13, 16)), ('input text', (19, 21))]",[],[],[],[]
model,"This paper proposes a combined framework , which utilizes BLSTM to capture long - term sentence dependencies , and extracts features by 2D convolution and 2D max pooling operation for sequence modeling tasks .","[('proposes', (2, 3)), ('utilizes', (8, 9)), ('to capture', (10, 12)), ('extracts', (19, 20)), ('for', (29, 30))]","[('combined framework', (4, 6)), ('BLSTM', (9, 10)), ('long - term sentence dependencies', (12, 17)), ('features', (20, 21)), ('2D convolution and 2D max pooling operation', (22, 29)), ('sequence modeling tasks', (30, 33))]",[],[],[],[]
model,"This work introduces two combined models BLSTM - 2DPooling and BLSTM - 2DCNN , and verifies them on six text classification tasks , including sentiment analysis , question classification , subjectivity classification , and newsgroups classification .","[('introduces', (2, 3)), ('including', (23, 24))]","[('two combined models', (3, 6)), ('BLSTM - 2DPooling', (6, 9)), ('six text classification tasks', (18, 22)), ('sentiment analysis', (24, 26)), ('question classification', (27, 29)), ('subjectivity classification', (30, 32)), ('newsgroups classification', (34, 36))]",[],[],[],[]
hyperparameters,"The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .","[('of', (2, 3)), ('is', (5, 6)), ('of', (11, 12)), ('is', (13, 14))]","[('dimension', (1, 2)), ('word embeddings', (3, 5)), ('300', (6, 7)), ('hidden units', (9, 11)), ('LSTM', (12, 13)), ('300', (14, 15))]",[],[],[],[]
hyperparameters,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .","[('use', (1, 2)), ('for', (6, 7))]","[('100 convolutional filters each', (2, 6)), ('window sizes', (7, 9)), ('( 3 , 3 )', (10, 15)), ('2D pooling size', (16, 19)), ('( 2 , 2 )', (20, 25))]",[],[],[],[]
hyperparameters,We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,"[('set', (1, 2)), ('as', (5, 6)), ('of', (11, 12)), ('as', (13, 14))]","[('mini-batch size', (3, 5)), ('10', (6, 7)), ('learning rate', (9, 11)), ('AdaDelta', (12, 13)), ('default value 1.0', (15, 18))]",[],[],[],[]
hyperparameters,"For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .","[('For', (0, 1)), ('employ', (4, 5)), ('with', (7, 8)), ('of', (10, 11)), ('for', (12, 13)), ('use', (31, 32)), ('with', (35, 36)), ('over', (40, 41))]","[('regularization', (1, 2)), ('Dropout operation', (5, 7)), ('dropout rate', (8, 10)), ('0.5', (11, 12)), ('word embeddings', (14, 16)), ('0.2', (17, 18)), ('BLSTM layer', (20, 22)), ('0.4', (23, 24)), ('penultimate layer', (26, 28)), ('l 2 penalty', (32, 35)), ('coefficient 10 ? 5', (36, 40)), ('parameters', (42, 43))]",[],[],[],[]
hyperparameters,These values are chosen via a grid search on the SST - 1 development set .,"[('chosen via', (3, 5)), ('on', (8, 9))]","[('grid search', (6, 8)), ('SST - 1 development set', (10, 15))]",[],[],[],[]
hyperparameters,"We only tune these hyperparameters , and more finer tuning , such as using different numbers of hidden units of LSTM layer , or using wide convolution , may further improve the performance .","[('tune', (2, 3)), ('such as using', (11, 14)), ('of', (19, 20))]","[('hyperparameters', (4, 5)), ('more finer tuning', (7, 10)), ('different numbers of hidden units', (14, 19)), ('LSTM layer', (20, 22)), ('wide convolution', (25, 27)), ('performance', (32, 33))]",[],[],[],[]
results,"This work implements four models , BLSTM , BLSTM - Att , BLSTM - 2DPooling , and BLSTM - 2DCNN . presents the performance of the four models and other state - of - the - art models on six classification tasks .","[('implements', (2, 3))]","[('four models', (3, 5)), ('BLSTM', (6, 7)), ('BLSTM - Att', (8, 11)), ('BLSTM - 2DPooling', (12, 15)), ('BLSTM - 2DCNN', (17, 20))]",[],[],[],[]
results,The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,"[('achieves', (5, 6)), ('on', (8, 9))]","[('BLSTM - 2DCNN model', (1, 5)), ('excellent performance', (6, 8)), ('4 out of 6 tasks', (9, 14))]",[],[],[],[]
results,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .","[('achieves', (3, 4)), ('on', (11, 12))]","[('52.4 % and 89.5 % test accuracies', (4, 11)), ('SST - 1 and SST - 2', (12, 19))]",[],[],[],[]
results,BLSTM - 2DPooling performs worse than the state - of - the - art models .,"[('performs', (3, 4)), ('than', (5, 6))]","[('BLSTM - 2DPooling', (0, 3)), ('worse', (4, 5)), ('state - of - the - art models', (7, 15))]",[],[],[],[]
results,"BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .","[('beats', (3, 4)), ('on', (6, 7))]","[('BLSTM - CNN', (0, 3)), ('all baselines', (4, 6)), ('SST - 1 ,', (7, 11)), ('SST - 2', (11, 14)), ('TREC datasets', (16, 18))]",[],[],[],[]
results,"As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .","[('As for', (0, 2)), ('gets', (10, 11))]","[('Subj and MR datasets', (2, 6)), ('BLSTM - 2DCNN', (7, 10)), ('second higher accuracies', (12, 15))]",[],[],[],[]
results,"Compared with RCNN , BLSTM - 2DCNN achieves a comparable result .","[('Compared with', (0, 2)), ('achieves', (7, 8))]","[('RCNN', (2, 3)), ('BLSTM - 2DCNN', (4, 7)), ('comparable result', (9, 11))]",[],[],[],[]
results,"BLSTM-2DCNN is an extension of BLSTM - 2DPooling , and the results show that BLSTM - 2DCNN can capture more dependencies in text .","[('extension of', (3, 5)), ('show', (12, 13)), ('capture', (18, 19)), ('in', (21, 22))]","[('BLSTM-2DCNN', (0, 1)), ('BLSTM - 2DPooling', (5, 8)), ('BLSTM -', (14, 16)), ('more dependencies', (19, 21)), ('text', (22, 23))]",[],[],[],[]
results,"Ada Sent utilizes a more complicated model to form a hierarchy of representations , and it outperforms BLSTM - 2DCNN on Subj and MR datasets .","[('utilizes', (2, 3)), ('to form', (7, 9)), ('of', (11, 12)), ('on', (20, 21))]","[('Ada Sent', (0, 2)), ('more complicated model', (4, 7)), ('hierarchy', (10, 11)), ('outperforms', (16, 17)), ('BLSTM - 2DCNN', (17, 20)), ('Subj and MR datasets', (21, 25))]",[],[],[],[]
results,DRNN : Deep recursive neural networks for compositionality in language .,"[('for', (6, 7))]","[('DRNN', (0, 1)), ('Deep recursive neural networks', (2, 6)), ('compositionality in language', (7, 10))]",[],[],[],[]
baselines,CNN -nonstatic / MC : Convolutional neural networks for sentence classification .,"[('for', (8, 9))]","[('CNN -nonstatic / MC', (0, 4)), ('Convolutional neural networks', (5, 8)), ('sentence classification', (9, 11))]",[],[],[],[]
baselines,"Molding - CNN : Molding CNNs for text : non-linear , non-consecutive convolutions .",[],"[('Molding - CNN', (0, 3)), ('Molding CNNs for', (4, 7)), ('text', (7, 8)), ('non-linear', (9, 10))]",[],[],[],[]
baselines,"MVCNN : Multichannel variable - size convolution for sentence classification ( Yin and Schtze , 2016 ) .","[('for', (7, 8))]","[('MVCNN', (0, 1)), ('Multichannel variable - size convolution', (2, 7)), ('sentence classification', (8, 10))]",[],[],[],[]
baselines,RCNN : Recurrent Convolutional Neural Networks for Text Classification .,[],"[('Text Classification', (7, 9))]",[],[],[],[]
baselines,S- LSTM : Long short - term memory over recursive structures .,"[('over', (8, 9))]","[('S- LSTM', (0, 2)), ('Long short - term memory', (3, 8)), ('recursive structures', (9, 11))]",[],[],[],[]
baselines,LSTM / BLSTM / Tree-LSTM :,[],"[('LSTM / BLSTM / Tree-LSTM', (0, 5))]",[],[],[],[]
baselines,LSTMN : Long short - term memory - networks for machine reading .,"[('for', (9, 10))]","[('LSTMN', (0, 1)), ('Long short - term memory - networks', (2, 9)), ('machine reading', (10, 12))]",[],[],[],[]
baselines,C - LSTM : A C - LSTM Neural Network for Text Classification .,[],"[('Text Classification', (11, 13))]",[],[],[],[]
results,Effect of Sentence Length,[],[],[],[],[],[]
results,It is found that both BLSTM - 2DPooling and BLSTM - 2DCNN outperform the other two models .,"[('found that', (2, 4))]","[('both BLSTM - 2DPooling and BLSTM - 2DCNN', (4, 12)), ('outperform', (12, 13)), ('other two models', (14, 17))]",[],[],[],[]
results,"The best accuracy is 52.6 with 2D filter size ( 5 , 5 ) and 2D max pooling size ( 5 , 5 ) , this shows that finer tuning can further improve the performance reported here .","[('is', (3, 4)), ('with', (5, 6))]","[('best accuracy', (1, 3)), ('52.6', (4, 5)), ('2D filter size ( 5 , 5 )', (6, 14)), ('2D max pooling size ( 5 , 5 )', (15, 24))]",[],[],[],[]
research-problem,Rethinking Complex Neural Network Architectures for Document Classification,[],[],[],[],[],[]
research-problem,"Surprisingly , our simple model is able to achieve these results without attention mechanisms .","[('able to achieve', (6, 9)), ('without', (11, 12))]","[('our simple model', (2, 5)), ('results', (10, 11)), ('attention mechanisms', (12, 14))]",[],[],[],[]
model,Our work provides an opensource platform and the foundation for future work in document classification .,[],"[('document classification', (13, 15))]",[],[],[],[]
experimental-setup,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .","[('performed on', (5, 7)), ('with', (16, 17)), ('as', (19, 20))]","[('Nvidia GTX 1080 and RTX 2080 Ti GPUs', (7, 15)), ('PyTorch 0.4.1', (17, 19)), ('backend framework', (21, 23))]",[],[],[],[]
experimental-setup,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,"[('use', (1, 2)), ('for computing', (4, 6)), ('implementing', (12, 13))]","[('Scikitlearn 0.19.2', (2, 4)), ('tf - idf vectors', (7, 11)), ('LR and SVMs', (13, 16))]",[],[],[],[]
experimental-setup,"For HAN , we use a batch size of 32 across all the datasets , with a learning rate of 0.01 for Reuters and 0.001 for the rest .","[('For', (0, 1)), ('use', (4, 5)), ('of', (8, 9)), ('across', (10, 11)), ('with', (15, 16))]","[('HAN', (1, 2)), ('batch size', (6, 8)), ('32', (9, 10)), ('all the datasets', (11, 14)), ('learning rate', (17, 19)), ('0.01', (20, 21)), ('0.001', (24, 25))]",[],[],[],[]
experimental-setup,"To train XML - CNN , we select a dynamic pooling window length of eight , a learning rate of 0.001 , and 128 output channels , with batch sizes of 32 and 64 for single - label and multilabel datasets , respectively .","[('train', (1, 2)), ('select', (7, 8)), ('with', (27, 28)), ('for', (34, 35))]","[('XML - CNN', (2, 5)), ('dynamic pooling window length', (9, 13)), ('eight', (14, 15)), ('learning rate', (17, 19)), ('0.001', (20, 21)), ('128 output channels', (23, 26)), ('batch sizes', (28, 30)), ('32 and 64', (31, 34)), ('single - label and multilabel datasets', (35, 41))]",[],[],[],[]
experimental-setup,"For KimCNN , we use a batch size of 64 with a learning rate of 0.01 .","[('For', (0, 1)), ('use', (4, 5)), ('of', (8, 9)), ('with', (10, 11)), ('of', (14, 15))]","[('KimCNN', (1, 2)), ('batch size', (6, 8)), ('64', (9, 10)), ('learning rate', (12, 14)), ('0.01', (15, 16))]",[],[],[],[]
experimental-setup,"For LSTM reg and LSTM base , we use the Adam optimizer with a learning rate of 0.01 on Reuters and 0.001 on the rest of the datasets , using batch sizes of 32 and 64 for multi-label and single - label tasks , respectively .","[('For', (0, 1)), ('use', (8, 9)), ('with', (12, 13)), ('of', (16, 17)), ('on', (18, 19)), ('using', (29, 30)), ('of', (32, 33)), ('for', (36, 37))]","[('LSTM reg and LSTM base', (1, 6)), ('Adam optimizer', (10, 12)), ('learning rate', (14, 16)), ('0.01', (17, 18)), ('Reuters', (19, 20)), ('0.001', (21, 22)), ('rest of the datasets', (24, 28)), ('batch sizes', (30, 32)), ('32 and 64', (33, 36)), ('multi-label and single - label tasks', (37, 43))]",[],[],[],[]
experimental-setup,"For LSTM reg , we also apply temporal averaging ( TA ) : as shown in , TA reduces both generalization error and stochastic noise in recent parameter estimates from stochastic approximation .","[('apply', (6, 7))]","[('LSTM reg', (1, 3)), ('temporal averaging ( TA )', (7, 12))]",[],[],[],[]
experimental-setup,We set the default TA exponential smoothing coefficient of ? EMA to 0.99 .,"[('set', (1, 2)), ('of', (8, 9)), ('to', (11, 12))]","[('default TA exponential smoothing coefficient', (3, 8)), ('? EMA', (9, 11)), ('0.99', (12, 13))]",[],[],[],[]
experimental-setup,"We choose 512 hidden units for the Bi - LSTM models , whose max - pooled output is regularized using a dropout rate of 0.5 .","[('choose', (1, 2)), ('for', (5, 6)), ('whose', (12, 13)), ('using', (19, 20)), ('of', (23, 24))]","[('512 hidden units', (2, 5)), ('Bi - LSTM models', (7, 11)), ('max - pooled output', (13, 17)), ('regularized', (18, 19)), ('dropout rate', (21, 23)), ('0.5', (24, 25))]",[],[],[],[]
experimental-setup,"We also regularize the input-hidden and hidden - hidden Bi - LSTM connections using embedding dropout and weight dropping , respectively , with dropout rates of 0.1 and 0.2 .","[('regularize', (2, 3)), ('using', (13, 14)), ('with', (22, 23)), ('of', (25, 26))]","[('input-hidden and hidden - hidden Bi - LSTM connections', (4, 13)), ('embedding dropout and weight dropping', (14, 19)), ('dropout rates', (23, 25)), ('0.1 and 0.2', (26, 29))]",[],[],[],[]
experimental-setup,"For our optimization objective , we use crossentropy and binary cross - entropy loss for singlelabel and multi-label tasks , respectively .","[('use', (6, 7)), ('for', (14, 15))]","[('optimization objective', (2, 4)), ('crossentropy and binary cross - entropy loss', (7, 14)), ('singlelabel and multi-label tasks', (15, 19))]",[],[],[],[]
experimental-setup,"On all datasets and models , we use 300 - dimensional word vectors pre-trained on Google News .","[('use', (7, 8)), ('pre-trained on', (13, 15))]","[('300 - dimensional word vectors', (8, 13)), ('Google News', (15, 17))]",[],[],[],[]
experimental-setup,"We train all neural models for 30 epochs with five random seeds , reporting the mean validation set scores and their corresponding test set results .","[('train', (1, 2)), ('for', (5, 6)), ('with', (8, 9)), ('reporting', (13, 14))]","[('all neural models', (2, 5)), ('30 epochs', (6, 8)), ('five random seeds', (9, 12)), ('mean', (15, 16))]",[],[],[],[]
results,"We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .","[('see that', (1, 3)), ('achieves', (8, 9)), ('on', (13, 14)), ('establishing', (26, 27)), ('for', (33, 34)), ('on', (39, 40)), ('of', (43, 44))]","[('our simple LSTM reg model', (3, 8)), ('state of the art', (9, 13)), ('Reuters and IMDB', (14, 17)), ('mean scores', (27, 29)), ('87.0 and 52.8', (30, 33)), ('F 1 score', (34, 37)), ('test sets', (41, 43))]",[],[],[],[]
results,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .","[('observe', (1, 2)), ('upon', (7, 8)), ('of', (10, 11)), ('yields', (31, 32)), ('of', (33, 34)), ('for', (38, 39))]","[('LSTM reg', (3, 5)), ('consistently improves', (5, 7)), ('performance', (9, 10)), ('LSTM base', (11, 13)), ('regularization', (30, 31)), ('increases', (32, 33)), ('1.5 and 0.5 points', (34, 38)), ('F 1 score', (39, 42))]",[],[],[],[]
results,"We also find the accuracy of LSTM reg and our reimplemented version of HAN on Yelp 2014 to be almost two points lower than the copied result of HAN ( rows 6 , 7 , and 10 ) from .","[('find', (2, 3)), ('of', (5, 6)), ('to be', (17, 19)), ('than', (23, 24)), ('of', (27, 28))]","[('accuracy', (4, 5)), ('LSTM reg and our reimplemented version of HAN', (6, 14)), ('almost two points lower', (19, 23)), ('copied result', (25, 27)), ('HAN', (28, 29))]",[],[],[],[]
results,"On the other hand , both of the models surpass the original result by nearly two points for the IMDB dataset .","[('surpass', (9, 10)), ('by', (13, 14)), ('for', (17, 18))]","[('original result', (11, 13)), ('nearly two points', (14, 17)), ('IMDB dataset', (19, 21))]",[],[],[],[]
results,"Interestingly , the non-neural LR and SVM baselines perform remarkably well .","[('perform', (8, 9))]","[('non-neural LR and SVM baselines', (3, 8)), ('remarkably well', (9, 11))]",[],[],[],[]
results,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .","[('On', (0, 1)), ('beats', (8, 9)), ('including', (13, 14))]","[('Reuters', (1, 2)), ('SVM', (7, 8)), ('many neural baselines', (9, 12)), ('our non-regularized LSTM base', (14, 18))]",[],[],[],[]
results,"On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .","[('On', (0, 1)), ('losing only to', (13, 16))]","[('AAPD', (1, 2)), ('SVM', (4, 5)), ('ties or beats', (6, 9)), ('other models', (10, 12)), ('SGM', (16, 17))]",[],[],[],[]
research-problem,Practical Text Classification With Large Pre-Trained Language Models,[],"[('Practical Text Classification', (0, 3))]",[],[],[],[]
research-problem,Multi-emotion sentiment classification is a natural language processing ( NLP ) problem with valuable use cases on realworld data .,[],"[('Multi-emotion sentiment classification', (0, 3))]",[],[],[],[]
research-problem,"By training an attention - based Transformer network ( Vaswani et al. 2017 ) on 40 GB of text ( Amazon reviews ) ( McAuley et al. 2015 ) and fine - tuning on the training set , our model achieves a 0.69 F1 score on the SemEval Task 1:E - c multidimensional emotion classification problem ( Mohammad et al. 2018 ) , based on the Plutchik wheel of emotions ( Plutchik 1979 ) .","[('training', (1, 2)), ('on', (14, 15)), ('achieves', (40, 41)), ('on', (45, 46))]","[('attention - based Transformer network', (3, 8)), ('40 GB of text', (15, 19)), ('0.69 F1 score', (42, 45)), ('SemEval Task 1:E - c multidimensional emotion classification problem', (47, 56))]",[],[],[],[]
model,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .","[('train', (5, 6)), ('on', (12, 13)), ('transfer', (21, 22)), ('to', (24, 25)), ('including', (33, 34)), ('based on', (42, 44))]","[('large 40 GB text dataset', (14, 19)), ('two text classification problems', (25, 29)), ('binary sentiment', (30, 32)), ('multidimensional emotion classification', (39, 42))]",[],[],[],[]
model,"By training a language model across a large text dataset , we expose our model to many contexts .","[('training', (1, 2)), ('across', (5, 6)), ('expose', (12, 13))]","[('language model', (3, 5)), ('large text dataset', (7, 10))]",[],[],[],[]
baselines,"We also compare our language models to ELMo ) , a contextualized word representation based on a deep bidirectional language model , trained on large text corpus .","[('based on', (14, 16)), ('trained on', (22, 24))]","[('ELMo', (7, 8)), ('contextualized word representation', (11, 14)), ('deep bidirectional language model', (17, 21)), ('large text corpus', (24, 27))]",[],[],[],[]
results,"We find that the inclusion of easier , more balanced label categories improves performance on harder ones in .","[('find', (1, 2)), ('of', (5, 6)), ('improves', (12, 13)), ('on', (14, 15))]","[('inclusion', (4, 5)), ('easier , more balanced label categories', (6, 12)), ('performance', (13, 14)), ('harder ones', (15, 17))]",[],[],[],[]
results,"For both the multihead MLP and the single linear layer instantiating off d , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as t * = 0.5 .","[('For', (0, 1)), ('instantiating off', (10, 12)), ('found that', (15, 17)), ('produced', (19, 20)), ('than using', (23, 25)), ('such as', (29, 31))]","[('multihead MLP and the single linear layer', (3, 10)), ('thresholding predictions', (17, 19)), ('noticeably better results', (20, 23)), ('fixed threshold value', (26, 29)), ('t * = 0.5', (31, 35))]",[],[],[],[]
results,We find that our models outperform Watson on every emotion category .,"[('find', (1, 2)), ('on', (7, 8))]","[('our models', (3, 5)), ('outperform', (5, 6)), ('Watson', (6, 7)), ('every emotion category', (8, 11))]",[],[],[],[]
results,"We submitted our finetuned Transformer model to the SemEval Task1:E - C challenge , as seen in Table 6 .","[('to', (6, 7))]","[('SemEval Task1:E - C challenge', (8, 13))]",[],[],[],[]
results,"Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .","[('achieved', (2, 3)), ('among', (8, 9)), ('with', (12, 13)), ('for', (17, 18))]","[('Our model', (0, 2)), ('top macro-averaged F1 score', (4, 8)), ('all submission', (9, 11)), ('competitive', (13, 14)), ('micro -average F1', (19, 22)), ('Jaccard Index accuracy', (24, 27))]",[],[],[],[]
results,We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .,"[('compare', (2, 3)), ('of', (7, 8)), ('find', (18, 19)), ('across', (26, 27))]","[('deep learning architectures', (4, 7)), ('Transformer and m LSTM', (9, 13)), ('Transformer', (21, 22)), ('outperforms', (22, 23)), ('m LSTM', (24, 26)), ('Plutchik categories', (27, 29))]",[],[],[],[]
results,"As with the SemEval challenge tweets , the Transformer outperformed the mLSTM .",[],"[('SemEval', (3, 4)), ('Transformer', (8, 9)), ('outperformed', (9, 10)), ('mLSTM', (11, 12))]",[],[],[],[]
results,Both models performed significantly better than the Watson API on all categories for which Watson supplies predictions .,"[('performed', (2, 3)), ('than', (5, 6)), ('on', (9, 10)), ('for', (12, 13)), ('supplies', (15, 16))]","[('significantly better', (3, 5)), ('Watson API', (7, 9)), ('all categories', (10, 12)), ('Watson', (14, 15)), ('predictions', (16, 17))]",[],[],[],[]
results,"Applying the SemEval - trained Transformer directly to our company tweets dataset gets reasonably good results ( 0.338 macro average ) , also validating that our labeling technique is similar to that of SemEval .","[('Applying', (0, 1)), ('directly to', (6, 8)), ('gets', (12, 13)), ('similar to', (29, 31))]","[('SemEval - trained Transformer', (2, 6)), ('our company tweets dataset', (8, 12)), ('reasonably good results', (13, 16))]",[],[],[],[]
results,"Looking at rater agreement by dataset , we see that Plutchik category labels contain large rater dis agreement , even among vetted raters who passed the golden set test .","[('Looking at', (0, 2)), ('see that', (8, 10)), ('contain', (13, 14)), ('among', (20, 21)), ('who passed', (23, 25))]","[('rater agreement by dataset', (2, 6)), ('Plutchik category labels', (10, 13)), ('large rater dis agreement', (14, 18)), ('vetted raters', (21, 23)), ('golden set test', (26, 29))]",[],[],[],[]
research-problem,Squeezed Very Deep Convolutional Neural Networks for Text Classification,[],[],[],[],[],[]
research-problem,VDCNN accuracy increases with depth .,"[('with', (3, 4))]","[('VDCNN accuracy', (0, 2)), ('increases', (2, 3)), ('depth', (4, 5))]",[],[],[],[]
model,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .","[('propose', (7, 8)), ('requires', (24, 25)), ('compared to', (28, 30))]","[('Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )', (9, 18)), ('text classification model', (20, 23)), ('significantly fewer parameters', (25, 28)), ('stateof - the - art CNNs', (31, 37))]",[],[],[],[]
model,a) Temporal Depthwise Separable Convolutions ( TD - SCs ) :,[],"[('Temporal Depthwise Separable Convolutions ( TD - SCs )', (1, 10))]",[],[],[],[]
model,b) Global Average Pooling ( GAP ) :,[],"[('Global Average Pooling ( GAP )', (1, 7))]",[],[],[],[]
experimental-setup,"The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .","[('performed with', (4, 6)), ('utilizing', (8, 9)), ('with', (14, 15))]","[('training', (1, 2)), ('SGD', (6, 7)), ('size batch of 64', (9, 13)), ('maximum of 100 epochs', (16, 20))]",[],[],[],[]
experiments,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .","[('use', (1, 2)), ('of', (6, 7))]","[('initial learning rate', (3, 6)), ('0.01', (7, 8)), ('momentum', (10, 11)), ('0.9', (12, 13)), ('weight decay', (15, 17)), ('0.001', (18, 19))]",[],[],[],[]
experimental-setup,All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,"[('performed on', (4, 6))]","[('NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU', (7, 18))]",[],[],[],[]
results,The use of TDSCs promoted a significant reduction in convolutional parameters compared to VDCNN .,"[('use of', (1, 3)), ('promoted', (4, 5)), ('in', (8, 9)), ('compared to', (11, 13))]","[('TDSCs', (3, 4)), ('significant reduction', (6, 8)), ('convolutional parameters', (9, 11)), ('VDCNN', (13, 14))]",[],[],[],[]
results,The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks .,"[('obtained by', (3, 5)), ('is', (7, 8))]","[('network reduction', (1, 3)), ('GAP', (6, 7)), ('even more representative', (8, 11))]",[],[],[],[]
results,"Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .","[('between', (5, 6)), ('varies between', (10, 12))]","[('performance difference', (3, 5)), ('VDCNN and SVDCNN models', (6, 10)), ('0.4 and 1.3 %', (12, 16))]",[],[],[],[]
results,The base property of VDCNN model is preserved on its squeezed model : the performance still increasing up with the depth and b),"[('of', (3, 4)), ('preserved on', (7, 9))]","[('base property', (1, 3)), ('VDCNN model', (4, 6)), ('squeezed model', (10, 12))]",[],[],[],[]
results,"The performance evaluated for the most extensive dataset , i.e. , Yelp Review ( 62.30 % ) , still overcomes the accuracy of the Char - CNN model ( 62.05 % ) .","[('evaluated for', (2, 4)), ('i.e.', (9, 10)), ('of', (22, 23))]","[('performance', (1, 2)), ('most extensive dataset', (5, 8)), ('Yelp Review', (11, 13)), ('accuracy', (21, 22)), ('Char - CNN model', (24, 28))]",[],[],[],[]
research-problem,Joint Embedding of Words and Labels for Text Classification,[],[],[],[],[],[]
research-problem,Text classification is a fundamental problem in natural language processing ( NLP ) .,[],"[('Text classification', (0, 2))]",[],[],[],[]
results,All approaches are better than traditional bag - of - words method .,"[('better than', (3, 5))]","[('All approaches', (0, 2)), ('traditional bag - of - words method', (5, 12))]",[],[],[],[]
results,"Our proposed LEAM outperforms the state - of - the - art methods on two largest datasets , i.e. , Yahoo and DBPedia .","[('on', (13, 14))]","[('outperforms', (3, 4)), ('state - of - the - art methods', (5, 13)), ('two largest datasets', (14, 17)), ('Yahoo and DBPedia', (20, 23))]",[],[],[],[]
results,LEAM consistently outperforms other methods with different proportion of labeled data .,"[('consistently', (1, 2)), ('with', (5, 6))]","[('LEAM', (0, 1)), ('other methods', (3, 5)), ('different proportion of labeled data', (6, 11))]",[],[],[],[]
results,Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model .,"[('use', (2, 3)), ('as', (10, 11)), ('for', (12, 13)), ('in', (18, 19))]","[('300 - dimensional Glo Ve word embeddings', (3, 10)), ('initialization', (11, 12)), ('word embeddings and label embeddings', (13, 18)), ('our model', (19, 21))]",[],[],[],[]
ablation-analysis,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .","[('initialized from', (10, 12)), ('with', (15, 16))]","[('Out - Of - Vocabulary ( OOV ) words', (0, 9)), ('uniform distribution', (13, 15)), ('range [ ? 0.01 , 0.01 ]', (16, 23))]",[],[],[],[]
results,"We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .","[('train', (1, 2)), ('with', (6, 7))]","[(""our model 's parameters"", (2, 6)), ('Adam Optimizer', (8, 10)), ('initial learning rate', (20, 23)), ('0.001', (24, 25)), ('minibatch size', (28, 30)), ('100', (31, 32))]",[],[],[],[]
ablation-analysis,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .","[('employed on', (3, 5)), ('with', (10, 11))]","[('Dropout regularization', (0, 2)), ('final MLP layer', (6, 9)), ('dropout rate 0.5', (11, 14))]",[],[],[],[]
ablation-analysis,The model is implemented using Tensorflow and is trained on GPU Titan X.,"[('implemented using', (3, 5)), ('trained on', (8, 10))]","[('Tensorflow', (5, 6))]",[],[],[],[]
code,The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,[],"[('https://github.com/guoyinwang/LEAM', (9, 10))]",[],[],[],[]
experiments,"To demonstrate the practical value of label embeddings , we apply LEAM for a real healthcare scenario : medical code prediction on the Electronic Health Records dataset .","[('apply', (10, 11)), ('for', (12, 13)), ('on', (21, 22))]","[('LEAM', (11, 12)), ('medical code prediction', (18, 21)), ('Electronic Health Records dataset', (23, 27))]",[],[],[],[]
baselines,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .","[('including', (14, 15))]","[('multi-label classification of clinical text', (8, 13)), ('Condensed Memory Networks ( C - MemNN )', (15, 23)), ('Attentive LSTM', (24, 26)), ('Convolutional Attention ( CAML )', (27, 32))]",[],[],[],[]
results,"LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .","[('provides', (1, 2)), ('than', (13, 14)), ('except', (16, 17))]","[('LEAM', (0, 1)), ('best AUC score', (3, 6)), ('better F1 and P@5 values', (8, 13)), ('all methods', (14, 16)), ('CNN', (17, 18))]",[],[],[],[]
results,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .","[('consistently', (1, 2)), ('performs', (15, 16)), ('than', (17, 18))]","[('CNN', (0, 1)), ('basic Bi - GRU architecture', (4, 9)), ('logistic regression baseline', (12, 15)), ('worse', (16, 17)), ('all deep learning architectures', (18, 22))]",[],[],[],[]
research-problem,HDLTex : Hierarchical Deep Learning for Text Classification,[],[],[],[],[],[]
research-problem,"Increasingly large document collections require improved information processing methods for searching , retrieving , and organizing text .",[],"[('searching , retrieving , and organizing text', (10, 17))]",[],[],[],[]
research-problem,Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased .,[],[],[],[],[],[]
research-problem,"Much of the recent work on automatic document classification has involved supervised learning techniques such as classification trees , nave Bayes , support vector machines ( SVM ) , neural nets , and ensemble methods .",[],"[('automatic document classification', (6, 9))]",[],[],[],[]
model,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,"[('call', (12, 13))]","[('hierarchical', (7, 8)), ('Hierarchical Deep Learning for Text classification ( HDLTex )', (13, 22))]",[],[],[],[]
model,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,"[('combines', (1, 2)), ('to allow', (5, 7)), ('by', (13, 14))]","[('HDLTex', (0, 1)), ('deep learning architectures', (2, 5)), ('over all and specialized learning', (8, 13)), ('level of the document hierarchy', (14, 19))]",[],[],[],[]
research-problem,Researchers have studied and developed a variety of methods for document classification .,[],"[('document classification', (10, 12))]",[],[],[],[]
model,This paper uses newer methods of machine learning for document classification taken from deep learning .,"[('uses', (2, 3)), ('for', (8, 9)), ('taken from', (11, 13))]","[('document classification', (9, 11)), ('deep learning', (13, 15))]",[],[],[],[]
model,This paper describes the use of deep learning approaches to create a hierarchical document classification approach .,"[('to create', (9, 11))]","[('deep learning approaches', (6, 9)), ('hierarchical document classification approach', (12, 16))]",[],[],[],[]
model,This paper compares fifteen methods for performing document classification .,[],"[('document classification', (7, 9))]",[],[],[],[]
research-problem,Multi - Class SVM : Text classification using string kernels within SVMs has been successful in many research projects .,[],"[('Multi - Class SVM', (0, 4)), ('Text classification', (5, 7))]",[],[],[],[]
model,Stacking Support Vector Machines ( SVM ) :,[],"[('Stacking Support Vector Machines ( SVM )', (0, 7))]",[],[],[],[]
experiments,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .","[('done on', (3, 5)), ('with', (14, 15)), ('were', (25, 26))]","[('processing', (1, 2)), ('Xeon E5 ? 2640 ( 2.6 GHz )', (6, 14)), ('32 cores', (15, 17)), ('64GB memory', (18, 20)), ('GPU cards', (23, 25)), ('N vidia Quadro K620', (26, 30)), ('N vidia Tesla K20c', (31, 35))]",[],[],[],[]
results,CNN performs secondbest for three data sets .,"[('performs', (1, 2)), ('for', (3, 4))]","[('CNN', (0, 1)), ('secondbest', (2, 3)), ('three data sets', (4, 7))]",[],[],[],[]
results,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,"[('is', (4, 5))]","[('SVM with term weighting', (0, 4)), ('third', (5, 6))]",[],[],[],[]
results,"For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .","[('For', (0, 1)), ('obtained by', (12, 14)), ('for', (17, 18)), ('for', (25, 26))]","[('data set W OS ? 11967', (1, 7)), ('best accuracy', (9, 11)), ('combination RNN', (15, 17)), ('first level of classification', (19, 23)), ('DNN', (24, 25)), ('second level', (27, 29))]",[],[],[],[]
results,This is significantly better than all of the others except for the combination of CNN and DNN .,"[('than', (4, 5)), ('except for', (9, 11))]","[('significantly better', (2, 4)), ('all of the others', (5, 9)), ('combination of CNN and DNN', (12, 17))]",[],[],[],[]
results,For data set W OS ? 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2 .,"[('achieved by', (12, 14)), ('for', (15, 16))]","[('data set W OS ? 46985', (1, 7)), ('best scores', (8, 10)), ('RNN', (14, 15)), ('RNN', (22, 23))]",[],[],[],[]
results,"Document classification is an important problem to address , given the growing size of scientific literature and other document sets .",[],"[('Document classification', (0, 2))]",[],[],[],[]
baselines,"This paper introduces a new approach to hierarchical document classification , HDLTex , that combines multiple deep learning approaches to produce hierarchical classifications .","[('combines', (14, 15))]","[('hierarchical document classification', (7, 10)), ('HDLTex', (11, 12))]",[],[],[],[]
results,Testing on a data set of documents obtained from the Web of Science shows that combinations of RNN at the higher level and DNN or CNN at the lower level produced accuracies consistently higher than those obtainable by conventional approaches using nave Bayes or SVM .,"[('Testing on', (0, 2)), ('shows', (13, 14)), ('of', (16, 17)), ('at', (18, 19)), ('at', (26, 27)), ('produced', (30, 31)), ('than', (34, 35)), ('obtainable by', (36, 38)), ('using', (40, 41))]","[('combinations', (15, 16)), ('RNN', (17, 18)), ('higher level', (20, 22)), ('DNN or CNN', (23, 26)), ('lower level', (28, 30)), ('accuracies', (31, 32)), ('consistently higher', (32, 34)), ('conventional approaches', (38, 40)), ('nave Bayes or SVM', (41, 45))]",[],[],[],[]
research-problem,Explicit Interaction Model towards Text Classification,[],[],[],[],[],[]
research-problem,Text classification is one of the fundamental tasks in natural language processing .,[],"[('Text classification', (0, 2))]",[],[],[],[]
model,"Thereafter , a fullyconnected ( FC ) layer at the topmost of the network is appended to make the final decision .","[('at', (8, 9)), ('appended', (15, 16)), ('to make', (16, 18))]","[('fullyconnected ( FC ) layer', (3, 8)), ('topmost of the', (10, 13)), ('network', (13, 14)), ('final decision', (19, 21))]",[],[],[],[]
model,"Mathematically , it interprets the parameter matrix of the FC layer as a set of class representations ( each column is associated with a class ) .","[('interprets', (3, 4)), ('of', (7, 8)), ('as', (11, 12))]","[('parameter matrix', (5, 7)), ('FC layer', (9, 11)), ('set', (13, 14)), ('class representations', (15, 17))]",[],[],[],[]
model,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .","[('introduce', (7, 8)), ('capable of incorporating', (21, 24)), ('for', (30, 31))]","[('interaction mechanism', (9, 11)), ('word - level matching signals', (25, 30)), ('text classification', (31, 33))]",[],[],[],[]
model,"From the word - level representation , it computes an interaction matrix , in which each entry is the matching score between a word and a class ( dot -product between their representations ) , illustrating the word - level matching signals .","[('From', (0, 1)), ('computes', (8, 9)), ('in which', (13, 15)), ('is', (17, 18)), ('between', (21, 22)), ('between', (30, 31)), ('illustrating', (35, 36))]","[('word - level representation', (2, 6)), ('interaction matrix', (10, 12)), ('each entry', (15, 17)), ('matching score', (19, 21)), ('word and a class ( dot -product', (23, 30)), ('word - level matching signals', (37, 42))]",[],[],[],[]
model,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .","[('Based upon', (0, 2)), ('devise', (7, 8)), ('as', (14, 15))]","[('EXplicit interAction Model ( dubbed', (9, 14))]",[],[],[],[]
model,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .","[('consists of', (5, 7))]","[('three main components', (7, 10)), ('word - level encoder', (11, 15)), ('interaction layer', (16, 18)), ('aggregation layer', (20, 22))]",[],[],[],[]
model,The word - level encoder projects the textual contents into the word - level representations .,"[('projects', (5, 6)), ('into', (9, 10))]","[('word - level encoder', (1, 5)), ('textual contents', (7, 9)), ('word - level representations', (11, 15))]",[],[],[],[]
model,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .","[('calculates', (5, 6)), ('between', (9, 10)), ('constructs', (17, 18))]","[('interaction layer', (3, 5)), ('matching scores', (7, 9)), ('words and classes', (11, 14)), ('interaction matrix', (19, 21))]",[],[],[],[]
model,"In summary , the contributions of this work are threefold : We present a novel framework , EXAM , which leverages the interaction mechanism to explicitly compute the wordlevel interaction signals for the text classification .","[('present', (12, 13)), ('leverages', (20, 21)), ('to explicitly compute', (24, 27)), ('for', (31, 32))]","[('threefold', (9, 10)), ('novel framework', (14, 16)), ('EXAM', (17, 18)), ('interaction mechanism', (22, 24)), ('wordlevel interaction signals', (28, 31)), ('text classification', (33, 35))]",[],[],[],[]
experimental-setup,"For the multi -class task , we chose region embedding as the Encoder in EXAM .","[('For', (0, 1)), ('chose', (7, 8)), ('as', (10, 11))]","[('multi -class task', (2, 5)), ('region embedding', (8, 10)), ('Encoder in EXAM', (12, 15))]",[],[],[],[]
experimental-setup,The region size is 7 and embedding size is 128 .,"[('is', (3, 4)), ('is', (8, 9))]","[('region size', (1, 3)), ('7', (4, 5)), ('embedding size', (6, 8)), ('128', (9, 10))]",[],[],[],[]
experimental-setup,We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,"[('used', (1, 2)), ('as', (9, 10)), ('with', (12, 13)), ('set to', (23, 25))]","[('adam ( Kingma and Ba 2014 )', (2, 9)), ('optimizer', (11, 12)), ('initial learning rate 0.0001', (14, 18)), ('batch size', (20, 22)), ('16', (25, 26))]",[],[],[],[]
experimental-setup,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .","[('set', (7, 8)), ('of', (10, 11)), ('as', (14, 15))]","[('aggregation MLP', (3, 5)), ('size', (9, 10)), ('hidden layer', (12, 14)), ('2 times interaction feature length', (15, 20))]",[],[],[],[]
experimental-setup,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,"[('implemented and trained by', (3, 7)), ('with', (13, 14))]","[('MXNet ( Chen et', (7, 11)), ('single NVIDIA TITAN Xp', (15, 19))]",[],[],[],[]
baselines,1 ) models based on feature engineering ;,"[('based on', (3, 5))]","[('feature engineering', (5, 7))]",[],[],[],[]
baselines,"2 ) Char - based deep models , and 3 ) Word - based deep models .",[],"[('Char - based deep models', (2, 7)), ('Word - based deep models', (11, 16))]",[],[],[],[]
results,Models based on feature engineering get the worst results on all the five datasets compared to the other methods .,"[('based on', (1, 3)), ('get', (5, 6)), ('on', (9, 10)), ('compared to', (14, 16))]","[('Models', (0, 1)), ('feature engineering', (3, 5)), ('worst results', (7, 9)), ('all the five datasets', (10, 14)), ('other methods', (17, 19))]",[],[],[],[]
results,Char - based models get the highest over all scores on the two Amazon datasets .,"[('get', (4, 5)), ('on', (10, 11))]","[('Char - based models', (0, 4)), ('highest over all scores', (6, 10)), ('two Amazon datasets', (12, 15))]",[],[],[],[]
results,"For the three char - based baselines , VDCNN gets the best performance on almost all the datasets because it has 29 convolutional layers allowing the model to learn more combinations of characters .","[('For', (0, 1)), ('gets', (9, 10)), ('on', (13, 14))]","[('three char - based baselines', (2, 7)), ('VDCNN', (8, 9)), ('best performance', (11, 13)), ('almost all the datasets', (14, 18))]",[],[],[],[]
results,Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .,"[('exceed', (4, 5)), ('on', (8, 9)), ('lose on', (12, 14))]","[('Word - based baselines', (0, 4)), ('other variants', (6, 8)), ('three datasets', (9, 11)), ('two Amazon datasets', (15, 18))]",[],[],[],[]
results,"For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .","[('performs', (8, 9))]","[('W.C Region Emb', (5, 8)), ('best', (10, 11))]",[],[],[],[]
results,"It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .","[('achieves', (7, 8)), ('over', (11, 12))]","[('EXAM', (6, 7)), ('best performance', (9, 11)), ('three datasets', (13, 15)), ('AG', (16, 17)), ('Yah. A.', (18, 20)), ('DBP', (21, 22))]",[],[],[],[]
results,"For the Yah.A. , EXAM improves the best performance by 1.1 % .","[('For', (0, 1)), ('improves', (5, 6)), ('by', (9, 10))]","[('Yah.A.', (2, 3)), ('EXAM', (4, 5)), ('best performance', (7, 9)), ('1.1 %', (10, 12))]",[],[],[],[]
results,"Additionally , as a word - based model , EXAM beats all the word - based baselines on the other two Amazon datasets with a performance gain of 1.0 % on the Amazon Full , because our EXAM considers more fine - grained interaction features between classes and words , which is quite helpful in this task .","[('as', (2, 3)), ('beats', (10, 11)), ('on', (17, 18)), ('with', (23, 24)), ('of', (27, 28)), ('on', (30, 31))]","[('word - based model', (4, 8)), ('EXAM', (9, 10)), ('all the word - based baselines', (11, 17)), ('other two Amazon datasets', (19, 23)), ('performance gain', (25, 27)), ('1.0 %', (28, 30)), ('Amazon Full', (32, 34))]",[],[],[],[]
baselines,We built a model called EXAM Encoder to preserve only the Encoder component with a max pooling layer and FC layer to derive the final probabilities .,"[('built', (1, 2)), ('called', (4, 5)), ('to preserve', (7, 9)), ('with', (13, 14)), ('to derive', (21, 23))]","[('EXAM Encoder', (5, 7)), ('only the Encoder component', (9, 13)), ('max pooling layer and FC layer', (15, 21)), ('final probabilities', (24, 26))]",[],[],[],[]
experimental-setup,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .","[('used', (1, 2)), ('trained by', (4, 6)), ('to initialize', (7, 9)), ('is', (17, 18))]","[('matrix', (3, 4)), ('word2vec', (6, 7)), ('embedding layer', (10, 12)), ('embedding size', (15, 17)), ('256', (18, 19))]",[],[],[],[]
experimental-setup,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .","[('adopted', (1, 2)), ('as', (3, 4))]","[('GRU', (2, 3)), ('Encoder', (5, 6)), ('1,024', (12, 13))]",[],[],[],[]
experimental-setup,The accumulated MLP has 60 hidden units .,[],"[('accumulated MLP', (1, 3)), ('60 hidden units', (4, 7))]",[],[],[],[]
experimental-setup,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,"[('applied', (1, 2)), ('to optimize', (3, 5)), ('on', (6, 7)), ('with', (11, 12)), ('is', (22, 23))]","[('Adam', (2, 3)), ('models', (5, 6)), ('one NVIDIA TITAN Xp', (7, 11)), ('batch size of 1000', (13, 17)), ('initial learning rate', (19, 22)), ('0.001', (23, 24))]",[],[],[],[]
experimental-setup,The validation set is applied for early - stopping to avoid overfitting .,"[('applied for', (4, 6)), ('to avoid', (9, 11))]","[('validation set', (1, 3)), ('early - stopping', (6, 9)), ('overfitting', (11, 12))]",[],[],[],[]
experimental-setup,All hyperparameters are chosen empirically .,"[('chosen', (3, 4))]","[('hyperparameters', (1, 2)), ('empirically', (4, 5))]",[],[],[],[]
results,Word - based models are better than char - based models in Kanshan - Cup dataset .,"[('better than', (5, 7)), ('in', (11, 12))]","[('Word - based models', (0, 4)), ('char - based models', (7, 11)), ('Kanshan - Cup dataset', (12, 16))]",[],[],[],[]
results,"For word - based baseline models , all the baselines have similar performance which corroborates the conclusion in FastText ) that simple network is on par with deep learning classifiers in text classification .","[('For', (0, 1)), ('have', (10, 11)), ('on par with', (24, 27)), ('in', (30, 31))]","[('word - based baseline models', (1, 6)), ('all the baselines', (7, 10)), ('similar performance', (11, 13)), ('simple network', (21, 23)), ('deep learning classifiers', (27, 30)), ('text classification', (31, 33))]",[],[],[],[]
results,Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .,"[('achieve', (2, 3)), ('over', (12, 13))]","[('Our models', (0, 2)), ('state - of - the - art performance', (4, 12)), ('two different datasets', (13, 16))]",[],[],[],[]
research-problem,A Corpus for Multilingual Document Classification in Eight Languages,[],"[('Multilingual Document Classification', (3, 6))]",[],[],[],[]
research-problem,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .,[],"[('Cross - lingual document classification', (0, 5))]",[],[],[],[]
research-problem,There is a large body of research on approaches for document classification .,[],"[('document classification', (10, 12))]",[],[],[],[]
model,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .","[('extend', (1, 2)), ('use', (5, 6)), ('in', (8, 9)), ('to define', (14, 16)), ('for', (23, 24)), ('namely', (29, 30))]","[('previous works', (2, 4)), ('data', (7, 8)), ('Reuters Corpus Volume 2', (10, 14)), ('new cross - lingual document classification tasks', (16, 23)), ('eight very different languages', (24, 28)), ('English', (30, 31)), ('French', (32, 33)), ('Spanish', (34, 35)), ('Italian', (36, 37)), ('German', (38, 39)), ('Russian', (40, 41)), ('Chinese', (42, 43)), ('Japanese', (44, 45))]",[],[],[],[]
results,Since the initial work by many alternative approaches to cross -lingual document classification have been developed .,[],"[('cross -lingual document classification', (9, 13))]",[],[],[],[]
baselines,"In this paper , we propose initial strong baselines which represent two complementary directions of research : one based on the aggregation of multilingual word embeddings , and another one , which directly learns multilingual sentence representations .","[('propose', (5, 6)), ('represent', (10, 11)), ('of', (14, 15)), ('based on', (18, 20)), ('of', (22, 23))]","[('initial strong baselines', (6, 9)), ('two complementary directions', (11, 14)), ('research', (15, 16)), ('aggregation', (21, 22)), ('multilingual word embeddings', (23, 26)), ('multilingual sentence representations', (34, 37))]",[],[],[],[]
results,"We will name this case "" zero - shot cross - lingual document classification "" .",[],"[('zero - shot cross - lingual document classification', (6, 14))]",[],[],[],[]
results,This type of cross - lingual document classification needs a very strong multilingual representation since no knowledge on the target language was used during the development of the classifier .,[],"[('cross - lingual document classification', (3, 8))]",[],[],[],[]
baselines,"We train a simple one - layer convolutional neural network ( CNN ) on top of the word embeddings , which has shown to perform well on text classification tasks regardless of training data size .","[('train', (1, 2)), ('on top of', (13, 16)), ('shown to', (22, 24))]","[('simple one - layer convolutional neural network ( CNN )', (3, 13)), ('word embeddings', (17, 19)), ('perform', (24, 25)), ('well', (25, 26))]",[],[],[],[]
ablation-analysis,"Specifically , convolutional filters are applied to windows of word embeddings , with a max - over - time pooling on top of them .","[('applied to', (5, 7)), ('of', (8, 9)), ('with', (12, 13)), ('on top of', (20, 23))]","[('convolutional filters', (2, 4)), ('windows', (7, 8)), ('word embeddings', (9, 11)), ('max - over - time pooling', (14, 20))]",[],[],[],[]
hyperparameters,"Hyper- parameters such as convolutional output dimension , window sizes are done by grid search over the Dev set of the same language as the train set .","[('such as', (2, 4)), ('done by', (11, 13)), ('over', (15, 16)), ('as', (23, 24))]","[('Hyper- parameters', (0, 2)), ('convolutional output dimension', (4, 7)), ('grid search', (13, 15)), ('Dev set of the same language', (17, 23)), ('train set', (25, 27))]",[],[],[],[]
results,A second direction of research is to directly learn multilingual sentence representations .,[],"[('multilingual sentence representations', (9, 12))]",[],[],[],[]
baselines,"In this paper , we evaluate a recently proposed technique to learn joint multilingual sentence representations .",[],"[('joint multilingual sentence representations', (12, 16))]",[],[],[],[]
results,"We have developed two versions of the system : one trained on the Europarl corpus to cover the languages English , German , French , Spanish and Italian , and another one trained on the United Nations corpus which allows to learn a joint sentence embedding for English , French , Spanish , Russian and Chinese .","[('developed', (2, 3)), ('of', (5, 6)), ('trained on', (10, 12)), ('to cover', (15, 17)), ('trained on', (32, 34)), ('allows to learn', (39, 42)), ('for', (46, 47))]","[('two versions', (3, 5)), ('Europarl corpus', (13, 15)), ('languages', (18, 19)), ('English', (19, 20)), ('United Nations corpus', (35, 38)), ('joint sentence embedding', (43, 46)), ('English', (47, 48))]",[],[],[],[]
ablation-analysis,We use a one hidden - layer MLP as classifier .,"[('use', (1, 2)), ('as', (8, 9))]","[('one hidden - layer MLP', (3, 8)), ('classifier', (9, 10))]",[],[],[],[]
results,"For comparison , we have evaluated its performance on the original subset of RCV2 as used in previous publications on cross - lingual document classification : we are able to outperform the current state - of - the - art in three out of six transfer directions .","[('evaluated', (5, 6)), ('on', (8, 9)), ('able to', (28, 30)), ('in', (40, 41))]","[('performance', (7, 8)), ('original subset of RCV2', (10, 14)), ('cross - lingual document classification', (20, 25)), ('outperform', (30, 31)), ('three out of six transfer directions', (41, 47))]",[],[],[],[]
results,Zero - shot cross - lingual document classification,[],[],[],[],[],[]
results,The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,"[('based on', (2, 4)), ('perform', (7, 8)), ('on', (10, 11))]","[('classifiers', (1, 2)), ('MultiCCA embeddings', (5, 7)), ('very well', (8, 10)), ('development corpus', (12, 14)), ('accuracies', (15, 16)), ('close or exceeding 90 %', (16, 21))]",[],[],[],[]
results,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .","[('trained on', (2, 4)), ('achieves', (6, 7)), ('when transfered to', (9, 12)), ('scores', (17, 18)), ('for', (19, 20))]","[('system', (1, 2)), ('English', (4, 5)), ('excellent results', (7, 9)), ('different languages', (13, 15)), ('best', (18, 19)), ('three out of seven languages ( DE , IT and ZH )', (20, 32))]",[],[],[],[]
results,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,[],[],[],[],[],[]
results,Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,[],"[('Crosslingual transfer between very different languages', (0, 6))]",[],[],[],[]
results,Targeted cross - lingual document classification,[],[],[],[],[],[]
research-problem,Disconnected Recurrent Neural Networks for Text Categorization,[],[],[],[],[],[]
research-problem,Recurrent neural network ( RNN ) has achieved remarkable performance in text categorization .,[],"[('text categorization', (11, 13))]",[],[],[],[]
research-problem,"Text categorization is a fundamental and traditional task in natural language processing ( NLP ) , which can be applied in various applications such as sentiment analysis , question classification and topic classification .",[],"[('Text categorization', (0, 2))]",[],[],[],[]
model,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .","[('incorporate', (5, 6)), ('into', (7, 8)), ('propose', (10, 11)), ('named', (14, 15))]","[('positioninvariance', (6, 7)), ('RNN', (8, 9)), ('Disconnected Recurrent Neural Network ( DRNN )', (15, 22))]",[],[],[],[]
model,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .","[('To maintain', (0, 2)), ('utilize', (8, 9)), ('to extract', (11, 13))]","[('position - invariance', (3, 6)), ('max pooling', (9, 11)), ('important information', (14, 16))]",[],[],[],[]
model,We also find that there is a trade - off between position - invariance and long - term dependencies in the DRNN .,"[('between', (10, 11)), ('in', (19, 20))]","[('trade - off', (7, 10)), ('position - invariance and long - term dependencies', (11, 19)), ('DRNN', (21, 22))]",[],[],[],[]
model,1 . We propose a novel model to incorporate position - variance into RNN .,"[('propose', (3, 4)), ('to incorporate', (7, 9)), ('into', (12, 13))]","[('novel model', (5, 7)), ('position - variance', (9, 12)), ('RNN', (13, 14))]",[],[],[],[]
model,"Based on this , we propose an empirical method to find the optimal window size .","[('propose', (5, 6)), ('to find', (9, 11))]","[('empirical method', (7, 9)), ('optimal window size', (12, 15))]",[],[],[],[]
experimental-setup,We tokenize all the corpus with NLTK 's tokenizer .,"[('tokenize', (1, 2)), ('with', (5, 6))]","[('all the corpus', (2, 5)), (""NLTK 's tokenizer"", (6, 9))]",[],[],[],[]
experimental-setup,"We utilize the 300D Glo Ve 840B vectors ( Pennington et al. , 2014 ) as our pre-trained word embeddings .","[('utilize', (1, 2)), ('as', (15, 16))]","[('300D Glo Ve 840B vectors', (3, 8)), ('pre-trained word embeddings', (17, 20))]",[],[],[],[]
experimental-setup,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .","[('use', (1, 2)), ('to optimize', (8, 10))]","[('Adadelta ( Zeiler , 2012 )', (2, 8)), ('all the trainable parameters', (10, 14))]",[],[],[],[]
experimental-setup,The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,"[('of', (2, 3)), ('set as', (5, 7)), ('suggest', (11, 12)), ('is', (20, 21))]","[('hyperparameter', (1, 2)), ('Adadelta', (3, 4)), ('Zeiler ( 2012 )', (7, 11)), ('1 e ? 6', (14, 18)), ('0.95', (21, 22))]",[],[],[],[]
experimental-setup,"To avoid the gradient explosion problem , we apply gradient norm clipping .","[('To avoid', (0, 2)), ('apply', (8, 9))]","[('gradient explosion problem', (3, 6)), ('gradient norm clipping', (9, 12))]",[],[],[],[]
experimental-setup,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,"[('set to', (4, 6)), ('of', (11, 12)), ('shows', (16, 17)), ('in', (27, 28))]","[('batch size', (1, 3)), ('128', (6, 7)), ('all the dimensions', (8, 11)), ('input vectors and hidden', (12, 16)), ('our proposed model', (18, 21)), ('significantly outperforms', (21, 23)), ('all the other models', (23, 27)), ('7 datasets', (28, 30))]",[],[],[],[]
results,Fast - Text and region embedding methods achieve comparable performance with other CNN and RNN based models .,"[('achieve', (7, 8)), ('with', (10, 11))]","[('Fast - Text and region embedding methods', (0, 7)), ('comparable performance', (8, 10)), ('other CNN and RNN based models', (11, 17))]",[],[],[],[]
baselines,The D - LSTM is a discriminative LSTM model .,"[('is', (4, 5))]","[('D - LSTM', (1, 4)), ('discriminative LSTM model', (6, 9))]",[],[],[],[]
baselines,Hierarchical attention network ( HAN ) is a hierarchical GRU model with attentive pooling .,"[('is', (6, 7)), ('with', (11, 12))]","[('Hierarchical attention network ( HAN )', (0, 6)), ('hierarchical GRU model', (8, 11)), ('attentive pooling', (12, 14))]",[],[],[],[]
results,We can see that very deep CNN ( VDCNN ) performs well in large datasets .,"[('see', (2, 3)), ('performs', (10, 11)), ('in', (12, 13))]","[('very deep CNN ( VDCNN )', (4, 10)), ('well', (11, 12)), ('large datasets', (13, 15))]",[],[],[],[]
results,"By contrast , our proposed model can achieve : DGRU compared with CNN better performance in these datasets by simply setting a large window size .","[('achieve', (7, 8)), ('compared with', (10, 12)), ('by', (18, 19))]","[('CNN better performance', (12, 15)), ('large window size', (22, 25))]",[],[],[],[]
baselines,Char-CRNN in the fourth block is a model which combines positioninvariance of CNN and long - term dependencies of RNN .,"[('combines', (9, 10)), ('of', (11, 12)), ('of', (18, 19))]","[('Char-CRNN', (0, 1)), ('positioninvariance', (10, 11)), ('CNN', (12, 13)), ('long - term dependencies', (14, 18)), ('RNN', (19, 20))]",[],[],[],[]
results,shows that our model achieves 10 - 50 % relative error reduction compared with char - CRNN in these datasets .,"[('shows', (0, 1)), ('achieves', (4, 5)), ('compared with', (12, 14))]","[('our model', (2, 4)), ('10 - 50 % relative error reduction', (5, 12)), ('char - CRNN', (14, 17))]",[],[],[],[]
results,Comparison with RNN and CNN,[],[],[],[],[],[]
results,shows that DRNN performs far better than CNN .,"[('shows', (0, 1)), ('performs', (3, 4)), ('than', (6, 7))]","[('DRNN', (2, 3)), ('far better', (4, 6)), ('CNN', (7, 8))]",[],[],[],[]
results,Our model DRNN achieves much better performance than GRU and LSTM .,"[('achieves', (3, 4)), ('than', (7, 8))]","[('Our model DRNN', (0, 3)), ('much better performance', (4, 7)), ('GRU and LSTM', (8, 11))]",[],[],[],[]
results,We find that the disconnected naive RNN performs just a little worse than disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU ) when the window size is lower than 5 .,"[('find that', (1, 3)), ('performs', (7, 8)), ('than', (12, 13)), ('when', (24, 25)), ('is', (28, 29))]","[('disconnected naive RNN', (4, 7)), ('disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU )', (13, 24)), ('window size', (26, 28)), ('lower than 5', (29, 32))]",[],[],[],[]
results,"DGRU achieves the best performance when the window size is 15 , while the best window size for DLSTM is 5 .","[('achieves', (1, 2)), ('when', (5, 6)), ('is', (9, 10)), ('for', (17, 18)), ('is', (19, 20))]","[('DGRU', (0, 1)), ('best performance', (3, 5)), ('window size', (7, 9)), ('15', (10, 11)), ('best window size', (14, 17)), ('DLSTM', (18, 19)), ('5', (20, 21))]",[],[],[],[]
results,We still conduct the experiments on AG dataset .,"[('on', (5, 6))]","[('AG dataset', (6, 8))]",[],[],[],[]
results,"From ( b ) , we can see that the DRNN model with max pooling performs better than the others .","[('see that', (7, 9)), ('with', (12, 13)), ('performs', (15, 16)), ('than', (17, 18))]","[('DRNN model', (10, 12)), ('max pooling', (13, 15)), ('better', (16, 17)), ('others', (19, 20))]",[],[],[],[]
results,We find attentive pooling is not significantly affected by window sizes .,"[('find', (1, 2)), ('not', (5, 6))]","[('attentive pooling', (2, 4)), ('window sizes', (9, 11))]",[],[],[],[]
results,Window size analysis,[],[],[],[],[],[]
research-problem,Investigating Capsule Networks with Dynamic Routing for Text Classification,[],[],[],[],[],[]
code,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,[],"[('https : //github.com/andyweizhao/capsule_text_ classification', (7, 11))]",[],[],[],[]
research-problem,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,[],"[('Modeling articles or sentences computationally', (0, 5))]",[],[],[],[]
model,It then hierarchically builds such pattern extraction pipelines at multiple levels .,"[('hierarchically builds', (2, 4)), ('at', (8, 9))]","[('pattern extraction pipelines', (5, 8)), ('multiple levels', (9, 11))]",[],[],[],[]
hyperparameters,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .","[('use', (5, 6)), ('to initialize', (11, 13))]","[('300 - dimensional word2vec vectors', (6, 11)), ('embedding vectors', (13, 15))]",[],[],[],[]
hyperparameters,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,"[('conduct', (1, 2)), ('with', (3, 4)), ('for', (6, 7))]","[('mini-batch', (2, 3)), ('size 50', (4, 6)), (""AG 's news"", (7, 10)), ('size 25', (11, 13)), ('other datasets', (14, 16))]",[],[],[],[]
hyperparameters,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,"[('use', (1, 2)), ('with', (5, 6)), ('to train', (11, 13))]","[('Adam optimization algorithm', (2, 5)), ('1e - 3 learning rate', (6, 11)), ('model', (14, 15))]",[],[],[],[]
hyperparameters,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,"[('use', (1, 2)), ('of', (4, 5)), ('for', (6, 7)), ('optimizes', (11, 12)), ('converges to', (16, 18))]","[('3 iteration', (2, 4)), ('routing', (5, 6)), ('all datasets', (7, 9)), ('loss', (13, 14)), ('lower loss', (19, 21))]",[],[],[],[]
baselines,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .","[('evaluate', (5, 6)), ('including', (15, 16))]","[('several strong baseline methods', (11, 15)), ('LSTM / Bi - LSTM', (17, 22)), ('tree - structured LSTM ( Tree - LSTM )', (23, 32)), ('LSTM regularized by linguistic knowledge ( LR - LSTM )', (33, 43)), ('CNNrand / CNN - static / CNN - non-static', (44, 53)), ('very deep convolutional network ( VD - CNN )', (59, 68)), ('character - level convolutional network ( CL - CNN )', (70, 80))]",[],[],[],[]
results,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .","[('observe', (5, 6)), ('achieve', (10, 11)), ('on', (13, 14)), ('verifies', (21, 22))]","[('capsule networks', (8, 10)), ('best results', (11, 13)), ('4 out of 6 benchmarks', (14, 19)), ('effectiveness', (23, 24))]",[],[],[],[]
research-problem,Deep Joint Entity Disambiguation with Local Neural Attention,[],"[('Deep Joint Entity Disambiguation', (0, 4))]",[],[],[],[]
research-problem,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .,[],"[('Entity disambiguation ( ED )', (0, 5))]",[],[],[],[]
research-problem,"In recent years , many text and language understanding tasks have been advanced by neural network architectures .",[],[],[],[],[],[]
experimental-setup,All models are implemented in the Torch framework .,"[('implemented in', (3, 5))]","[('Torch framework', (6, 8))]",[],[],[],[]
tasks,We use Adagrad with a learning rate of 0.3 .,"[('use', (1, 2)), ('with', (3, 4)), ('of', (7, 8))]","[('Adagrad', (2, 3)), ('learning rate', (5, 7)), ('0.3', (8, 9))]",[],[],[],[]
tasks,"We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .","[('choose', (1, 2))]","[('embedding size d', (2, 5)), ('300', (6, 7)), ('pre-trained ( fixed ) Word2 Vec word vectors', (8, 16))]",[],[],[],[]
tasks,"Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .","[('trained on', (7, 9)), ('validated on', (17, 19)), ('tested on', (23, 25))]","[('AIDA - train ( multiple epochs )', (9, 16)), ('AIDA - A', (19, 22))]",[],[],[],[]
tasks,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .","[('use', (1, 2)), ('with', (3, 4)), ('of', (6, 7)), ('until', (10, 11)), ('exceeds', (13, 14)), ('setting it to', (18, 21))]","[('Adam', (2, 3)), ('learning rate', (4, 6)), ('1e - 4', (7, 10)), ('validation accuracy', (11, 13)), ('90 %', (14, 16)), ('1e - 5', (21, 24))]",[],[],[],[]
experimental-setup,Variable size mini-batches consisting of all mentions in a document are used during training .,"[('consisting of', (3, 5)), ('used during', (11, 13))]","[('Variable size mini-batches', (0, 3)), ('training', (13, 14))]",[],[],[],[]
tasks,"Hyper- parameters of the best validated global model are : ? = 0.01 , K = 100 , R = 25 , S = 7 , ? = 0.5 , T = 10 .","[('of', (2, 3)), ('are', (8, 9))]","[('Hyper- parameters', (0, 2)), ('best validated global model', (4, 8)), ('? = 0.01', (10, 13))]",[],[],[],[]
tasks,"For the local model , R = 50 was best .","[('For', (0, 1)), ('was', (8, 9))]","[('local model', (2, 4)), ('R = 50', (5, 8)), ('best', (9, 10))]",[],[],[],[]
tasks,"To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .","[('use', (4, 5)), ('after', (19, 20))]","[('early stopping', (5, 7)), ('validation accuracy', (14, 16)), ('does not increase', (16, 19)), ('500 epochs', (20, 22))]",[],[],[],[]
experimental-setup,"By using diagonal matrices A , B , C , we keep the number of parameters very low ( approx. 1.2 K parameters ) .","[('using', (1, 2)), ('keep', (11, 12)), ('approx.', (19, 20))]","[('diagonal matrices A , B , C', (2, 9)), ('number of parameters', (13, 16)), ('very low', (16, 18))]",[],[],[],[]
tasks,"We also experimented with diagonal plus low - rank matrices , but encountered quality degradation .","[('experimented with', (2, 4)), ('encountered', (12, 13))]","[('diagonal plus low - rank matrices', (4, 10)), ('quality degradation', (13, 15))]",[],[],[],[]
results,Our method outperforms the well established Wikipedia link measure and the method of using less information ( only word - entity statistics ) .,"[('of using', (12, 14))]","[('outperforms', (2, 3)), ('well established Wikipedia link measure', (4, 9)), ('less information ( only word - entity statistics', (14, 22))]",[],[],[],[]
results,"We emphasize that our global ED model outperforms Huang 's ED model , likely due to the power of our local and joint neural network architectures .","[('emphasize', (1, 2)), ('of', (18, 19))]","[('our global ED model', (3, 7)), ('outperforms', (7, 8)), (""Huang 's ED model"", (8, 12)), ('power', (17, 18)), ('our local and joint neural network architectures', (19, 26))]",[],[],[],[]
research-problem,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation,[],[],[],[],[],[]
research-problem,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .","[('propose', (5, 6)), ('for', (15, 16))]","[('named entity disambiguation ( NED )', (16, 22))]",[],[],[],[]
research-problem,Named entity disambiguation ( NED ) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base ( KB ) .,[],"[('Named entity disambiguation ( NED )', (0, 6))]",[],[],[],[]
model,"In this paper , we describe a new contextualized embedding model for words and entities for NED .","[('for', (15, 16))]","[('words', (12, 13)), ('NED', (16, 17))]",[],[],[],[]
model,"Following , the proposed model is based on the bidirectional transformer encoder .","[('based on', (6, 8))]","[('bidirectional transformer encoder', (9, 12))]",[],[],[],[]
model,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .","[('takes', (1, 2)), ('in', (8, 9)), ('produces', (14, 15)), ('for', (18, 19))]","[('sequence of words and entities', (3, 8)), ('input text', (10, 12)), ('contextualized embedding', (16, 18)), ('each word and entity', (19, 23))]",[],[],[],[]
model,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .","[('propose', (5, 6)), ('based on', (25, 27)), ('in', (31, 32))]","[('masked entity prediction', (6, 9)), ('randomly masked entities', (22, 25)), ('words and non-masked entities', (27, 31)), ('input text', (33, 35))]",[],[],[],[]
model,The NED model addresses the task by capturing word - based and entity - based contextual information using the trained contextualized embeddings .,"[('addresses', (3, 4)), ('by capturing', (6, 8)), ('using', (17, 18))]","[('NED model', (1, 3)), ('task', (5, 6)), ('word - based and entity - based contextual information', (8, 17)), ('trained contextualized embeddings', (19, 22))]",[],[],[],[]
experiments,"We also set the feed - forward / filter size to 4096 , the dropout probability applied to all layers was 0.1 , and the maximum word length in an input sequence was set to 512 .","[('set', (2, 3)), ('to', (10, 11)), ('applied to', (16, 18)), ('was', (20, 21)), ('in', (28, 29)), ('set to', (33, 35))]","[('feed - forward / filter size', (4, 10)), ('4096', (11, 12)), ('dropout probability', (14, 16)), ('all layers', (18, 20)), ('0.1', (21, 22)), ('maximum word length', (25, 28)), ('input sequence', (30, 32)), ('512', (35, 36))]",[],[],[],[]
hyperparameters,"Other parameters , namely the parameters in the MEP and the embeddings for entities , were initialized randomly .","[('namely', (3, 4)), ('in', (6, 7)), ('for', (12, 13))]","[('Other parameters', (0, 2)), ('parameters', (5, 6)), ('MEP and the embeddings', (8, 12)), ('entities', (13, 14)), ('initialized randomly', (16, 18))]",[],[],[],[]
experiments,"We used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .","[('used', (1, 2)), ('with', (5, 6)), ('of', (9, 10)), ('of', (27, 28))]","[('Adam optimizer', (3, 5)), ('learning rate', (7, 9)), ('2 e - 5', (10, 14)), ('L2 weight decay', (24, 27)), ('0.01', (28, 29))]",[],[],[],[]
hyperparameters,The batch size was set to 252 .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('252', (6, 7))]",[],[],[],[]
experiments,"We set the batch size to 32 , and used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .","[('set', (1, 2)), ('to', (5, 6)), ('used', (9, 10)), ('with', (13, 14)), ('of', (17, 18)), ('of', (35, 36))]","[('batch size', (3, 5)), ('32', (6, 7)), ('Adam optimizer', (11, 13)), ('learning rate', (15, 17)), ('2 e - 5', (18, 22)), ('?1 = 0.9 , ?2 = 0.999', (23, 30)), ('L2 weight decay', (32, 35)), ('0.01', (36, 37))]",[],[],[],[]
results,"As shown , our models outperformed all previously proposed models .",[],"[('outperformed', (5, 6))]",[],[],[],[]
results,"Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .","[('using', (2, 3)), ('boosted', (6, 7)), ('by', (9, 10))]","[('pseudo entity annotations', (3, 6)), ('accuracy', (8, 9)), ('0.3 %', (10, 12))]",[],[],[],[]
research-problem,Deep contextualized word representations,[],[],[],[],[],[]
model,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .","[('introduce', (5, 6)), ('in', (36, 37)), ('across', (40, 41))]","[('deep contextualized word representation', (10, 14)), ('existing models', (25, 27)), ('significantly improves', (29, 31)), ('state of the art', (32, 36))]",[],[],[],[]
model,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"[('differ from', (2, 4)), ('in', (8, 9)), ('assigned', (13, 14)), ('of', (20, 21))]","[('traditional word type embeddings', (4, 8)), ('each token', (10, 12)), ('representation', (15, 16)), ('function', (19, 20))]",[],[],[],[]
model,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,"[('use', (1, 2)), ('derived from', (3, 5)), ('trained with', (10, 12)), ('on', (22, 23))]","[('vectors', (2, 3)), ('bidirectional LSTM', (6, 8)), ('coupled lan - guage model ( LM ) objective', (13, 22)), ('large text corpus', (24, 27))]",[],[],[],[]
model,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","[('call', (5, 6))]","[('ELMo ( Embeddings from Language Models ) representations', (7, 15))]",[],[],[],[]
model,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .","[('are', (11, 12)), ('function of', (21, 23)), ('of', (28, 29))]","[('ELMo representations', (9, 11)), ('deep', (12, 13)), ('all of the internal layers', (23, 28)), ('biLM', (30, 31))]",[],[],[],[]
model,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .","[('learn', (4, 5)), ('stacked above', (11, 13)), ('for', (16, 17)), ('markedly', (22, 23)), ('over', (25, 26))]","[('linear combination of the vectors', (6, 11)), ('each input word', (13, 16)), ('each end task', (17, 20)), ('performance', (24, 25)), ('using the top LSTM layer', (27, 32))]",[],[],[],[]
model,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision that are most useful for each end task .","[('Simultaneously exposing', (0, 2)), ('is', (6, 7)), ('allowing', (10, 11)), ('for', (23, 24))]","[('all of these signals', (2, 6)), ('highly beneficial', (7, 9)), ('learned models', (12, 14)), ('types of semi-supervision', (16, 19)), ('most useful', (21, 23)), ('each end task', (24, 27))]",[],[],[],[]
model,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .","[('benefits from', (3, 5)), ('through the use of', (7, 11)), ('seamlessly incorporate', (16, 18)), ('into', (20, 21)), ('without explicitly', (23, 25))]","[('subword units', (5, 7)), ('character convolutions', (11, 13)), ('multi-sense information', (18, 20)), ('downstream tasks', (21, 23)), ('predefined sense classes', (28, 31))]",[],[],[],[]
model,context2vec uses a bidirectional Long Short Term Memory ( LSTM ; to encode the context around a pivot word .,"[('uses', (1, 2)), ('to encode', (11, 13)), ('around', (15, 16))]","[('context2vec', (0, 1)), ('bidirectional Long Short Term Memory ( LSTM ;', (3, 11)), ('context', (14, 15)), ('pivot word', (17, 19))]",[],[],[],[]
model,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .","[('show', (1, 2)), ('induced by', (7, 9)), ('of', (14, 15)), ('that mix', (31, 33))]","[('similar signals', (3, 5)), ('modified language model objective', (10, 14)), ('our ELMo representations', (15, 18)), ('downstream tasks', (29, 31))]",[],[],[],[]
model,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .","[('with', (7, 8)), ('fix', (12, 13)), ('add', (16, 17)), ('allowing us to leverage', (22, 26)), ('for cases', (33, 35)), ('dictates', (40, 41))]","[('biLM', (6, 7)), ('unlabeled data', (8, 10)), ('weights', (14, 15)), ('additional taskspecific model capacity', (17, 21)), ('large , rich and universal biLM representations', (26, 33)), ('downstream training data size', (36, 40)), ('smaller supervised model', (42, 45))]",[],[],[],[]
experiments,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .","[('adding', (6, 7)), ('establishes', (8, 9)), ('with', (20, 21)), ('ranging from', (24, 26)), ('over', (30, 31))]","[('ELMo', (7, 8)), ('relative error reductions', (21, 24)), ('6 - 20 %', (26, 30)), ('strong base models', (31, 34))]",[],[],[],[]
experiments,"After adding ELMo to the baseline model , test set F 1 improved by 4.7 % from 81.1 % to 85.8 % , a 24.9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1.4 % .","[('adding', (1, 2)), ('to', (3, 4)), ('improved by', (12, 14)), ('from', (16, 17)), ('to', (19, 20)), ('improving', (34, 35)), ('by', (46, 47))]","[('ELMo', (2, 3)), ('baseline model', (5, 7)), ('test set F 1', (8, 12)), ('4.7 %', (14, 16)), ('81.1 %', (17, 19)), ('85.8 %', (20, 22)), ('24.9 % relative error reduction', (24, 29)), ('overall single model state - of - the - art', (36, 46)), ('1.4 %', (47, 49))]",[],[],[],[]
experiments,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .","[('adding', (2, 3)), ('to', (4, 5)), ('improves', (8, 9)), ('by', (10, 11)), ('across', (16, 17))]","[('ELMo', (3, 4)), ('ESIM model', (6, 8)), ('accuracy', (9, 10)), ('average of 0.7 %', (12, 16)), ('five random seeds', (17, 20))]",[],[],[],[]
experiments,"As shown in , our ELMo enhanced biLSTM - CRF achieves 92. 22 % F 1 averaged over five runs .","[('achieves', (10, 11)), ('averaged over', (16, 18))]","[('our', (4, 5)), ('ELMo enhanced biLSTM - CRF', (5, 10)), ('92. 22 % F 1', (11, 16)), ('five runs', (18, 20))]",[],[],[],[]
experiments,"As shown in Sec. 5.1 , using all layers instead of just the last layer improves performance across multiple tasks .","[('using', (6, 7)), ('instead of', (9, 11)), ('improves', (15, 16)), ('across', (17, 18))]","[('all layers', (7, 9)), ('last layer', (13, 15)), ('performance', (16, 17)), ('multiple tasks', (18, 20))]",[],[],[],[]
experiments,Sentiment analysis,[],[],[],[],[],[]
experiments,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to = 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( = 1 vs. = 0.001 ) .","[('Averaging', (0, 1)), ('instead of using', (4, 7)), ('improves', (11, 12)), ('another', (14, 15)), ('allowing', (30, 31)), ('to learn', (34, 36)), ('improves', (39, 40)), ('another', (42, 43))]","[('all biLM layers', (1, 4)), ('last layer', (9, 11)), ('F 1', (12, 14)), ('0.3 %', (15, 17)), ('task model', (32, 34)), ('individual layer weights', (36, 39)), ('F 1', (40, 42)), ('0.2 %', (43, 45))]",[],[],[],[]
experiments,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves overall results for some tasks .","[('including', (5, 6)), ('at', (7, 8)), ('of', (10, 11)), ('in', (13, 14)), ('improves', (18, 19)), ('for', (21, 22))]","[('ELMo', (6, 7)), ('output', (9, 10)), ('biRNN', (12, 13)), ('task - specific architectures', (14, 18)), ('overall results', (19, 21)), ('some tasks', (22, 24))]",[],[],[],[]
experiments,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .","[('including', (4, 5)), ('for', (13, 14)), ('improves over', (17, 19)), ('for', (25, 26)), ('is', (36, 37)), ('included at', (41, 43))]","[('ELMo', (5, 6)), ('input and output layers', (9, 13)), ('SNLI and SQuAD', (14, 17)), ('input layer', (21, 23)), ('SRL', (26, 27)), ('highest', (37, 38)), ('input layer', (45, 47))]",[],[],[],[]
results,the task - specific context representations are likely more important than those from the biLM .,"[('likely more', (7, 9))]","[('task - specific context representations', (1, 6)), ('biLM', (14, 15))]",[],[],[],[]
experiments,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .","[('improves', (1, 2)), ('over', (4, 5))]","[('ELMo', (0, 1)), ('task performance', (2, 4)), ('word vectors alone', (5, 8))]",[],[],[],[]
experiments,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .","[('have', (7, 8)), ('of', (10, 11)), ('better at', (14, 16)), ('then', (17, 18))]","[('biLM top layer rep-resentations', (3, 7)), ('F 1', (8, 10)), ('69.0', (11, 12)), ('WSD', (16, 17)), ('first layer', (19, 21))]",[],[],[],[]
experiments,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .","[('using', (6, 7)), ('higher than', (12, 14)), ('consistent with', (18, 20)), ('in', (26, 27))]","[('accuracies', (5, 6)), ('first biLM layer', (8, 11)), ('top layer', (15, 17)), ('deep biL - STMs', (22, 26)), ('multi-task training and MT', (27, 31))]",[],[],[],[]
results,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .","[('with', (8, 9)), ('of', (11, 12)), ('as', (21, 22)), ('with', (25, 26)), ('of', (28, 29))]","[('SRL', (2, 3)), ('ELMo model', (6, 8)), ('1 %', (9, 11)), ('training set', (13, 15)), ('baseline model', (23, 25)), ('10 %', (26, 28))]",[],[],[],[]
ablation-analysis,"The output layer weights are relatively balanced , with a slight preference for the lower layers .","[('are', (4, 5)), ('with', (8, 9)), ('for', (12, 13))]","[('output layer weights', (1, 4)), ('relatively balanced', (5, 7)), ('slight preference', (10, 12)), ('lower layers', (14, 16))]",[],[],[],[]
results,"Replacing the Glo Ve vectors with the biLM character layer gives a slight improvement for all tasks ( e.g. from 80.8 to 81.4 F 1 for SQuAD ) , but overall the improvements are small compared to the full ELMo model .","[('Replacing', (0, 1)), ('with', (5, 6)), ('gives', (10, 11)), ('for', (14, 15)), ('are', (33, 34)), ('compared to', (35, 37))]","[('Glo Ve vectors', (2, 5)), ('biLM character layer', (7, 10)), ('slight improvement', (12, 14)), ('all tasks', (15, 17)), ('improvements', (32, 33)), ('small', (34, 35)), ('full ELMo model', (38, 41))]",[],[],[],[]
results,"From this , we conclude that most of the gains in the downstream tasks are due to the contextual information and not the sub-word information .","[('conclude', (4, 5)), ('in', (10, 11)), ('due to', (15, 17)), ('not', (21, 22))]","[('most of the gains', (6, 10)), ('downstream tasks', (12, 14)), ('contextual information', (18, 20)), ('sub-word information', (23, 25))]",[],[],[],[]
tasks,"As shown in the two right hand columns of , adding Glo Ve to models with ELMo generally provides a marginal improvement over ELMo only models ( e.g. 0.2 % F 1 improvement for SRL from 84.5 to 84.7 ) .","[('adding', (10, 11)), ('to', (13, 14)), ('with', (15, 16)), ('provides', (18, 19)), ('over', (22, 23))]","[('Glo Ve', (11, 13)), ('models', (14, 15)), ('ELMo', (16, 17)), ('marginal improvement', (20, 22)), ('ELMo only models', (23, 26))]",[],[],[],[]
research-problem,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,[],[],[],[],[],[]
research-problem,"In this article , we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation , by exploiting the semantic relationships between senses such as synonymy , hypernymy and hyponymy , in order to compress the sense vocabulary of Princeton WordNet , and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database .","[('tackle', (5, 6)), ('for', (17, 18)), ('of', (20, 21)), ('by exploiting', (25, 27)), ('between', (30, 31)), ('such as', (32, 34)), ('of', (47, 48)), ('reduce', (53, 54)), ('of', (68, 69))]","[('issue', (7, 8)), ('limited quantity of manually sense annotated corpora', (10, 17)), ('task', (19, 20)), ('word sense disambiguation', (21, 24)), ('semantic relationships', (28, 30)), ('senses', (31, 32)), ('synonymy', (34, 35)), ('sense vocabulary', (45, 47)), ('Princeton WordNet', (48, 50)), ('number of different sense tags', (55, 60)), ('all words', (66, 68)), ('lexical database', (70, 72))]",[],[],[],[]
research-problem,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .","[('present', (7, 8)), ('relies on', (12, 14)), ('to achieve', (20, 22)), ('that', (23, 24)), ('on', (31, 32))]","[('WSD system', (9, 11)), ('pre-trained BERT word vectors', (14, 18)), ('results', (22, 23)), ('significantly outperforms', (24, 26)), ('state of the art', (27, 31)), ('all WSD evaluation tasks', (32, 36))]",[],[],[],[]
research-problem,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
model,"Therefore , we propose two different methods for building this subset and we call them sense vocabulary compression methods .","[('propose', (3, 4)), ('for', (7, 8)), ('call them', (13, 15))]","[('two different methods', (4, 7)), ('building', (8, 9)), ('subset', (10, 11)), ('sense vocabulary compression methods', (15, 19))]",[],[],[],[]
research-problem,"Current state of the art supervised WSD systems such as , , and are all confronted to the following issues :",[],"[('supervised WSD', (5, 7))]",[],[],[],[]
model,"In order to overcome all these issues , we propose a method for grouping together multiple sense tags that refer in fact to the same concept .","[('propose', (9, 10)), ('for grouping together', (12, 15)), ('refer in fact to', (19, 23))]","[('method', (11, 12)), ('multiple sense tags', (15, 18)), ('same concept', (24, 26))]",[],[],[],[]
research-problem,From Senses to Synsets : A Vocabulary Compression Based on Synonymy,[],"[('Vocabulary Compression', (6, 8))]",[],[],[],[]
experimental-setup,"For BERT , we used the model named "" bert - largecased "" of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .","[('For', (0, 1)), ('used', (4, 5)), ('of', (13, 14)), ('consists of', (20, 22)), ('of', (23, 24)), ('trained on', (27, 29))]","[('BERT', (1, 2)), ('model named', (6, 8)), ('"" bert - largecased ""', (8, 13)), ('PyTorch implementation', (15, 17)), ('vectors', (22, 23)), ('dimension 1024', (24, 26)), ('Book s Corpus', (29, 32)), ('English Wikipedia', (33, 35))]",[],[],[],[]
experimental-setup,"For the Transformer encoder layers , we used the same parameters as the "" base "" model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .","[('For', (0, 1)), ('used', (7, 8)), ('as', (11, 12)), ('with', (23, 24))]","[('Transformer encoder layers', (2, 5)), ('same parameters', (9, 11)), ('"" base "" model', (13, 17)), ('6 layers', (21, 23)), ('8 attention heads', (24, 27)), ('hidden size', (29, 31)), ('2048', (32, 33)), ('dropout', (36, 37)), ('0.1', (38, 39))]",[],[],[],[]
experiments,We performed every training for 20 epochs .,"[('performed', (1, 2)), ('for', (4, 5))]","[('every training', (2, 4)), ('20 epochs', (5, 7))]",[],[],[],[]
baselines,"3 . A "" all relations "" system which applies our second vocabulary compression through all relations on the training corpus .","[('applies', (9, 10)), ('through', (14, 15)), ('on', (17, 18))]","[('"" all relations "" system', (3, 8)), ('our second vocabulary compression', (10, 14)), ('all relations', (15, 17)), ('training corpus', (19, 21))]",[],[],[],[]
experiments,"We trained with mini-batches of 100 sentences , truncated to 80 words , and we used Adam with a learning rate of 0.0001 as the optimization method .","[('trained with', (1, 3)), ('of', (4, 5)), ('truncated to', (8, 10)), ('used', (15, 16)), ('with', (17, 18)), ('of', (21, 22)), ('as', (23, 24))]","[('mini-batches', (3, 4)), ('100 sentences', (5, 7)), ('80 words', (10, 12)), ('Adam', (16, 17)), ('learning rate', (19, 21)), ('0.0001', (22, 23)), ('optimization method', (25, 27))]",[],[],[],[]
experimental-setup,All models have been trained on one Nvidia 's Titan X GPU .,"[('trained on', (4, 6))]","[(""one Nvidia 's Titan X GPU"", (6, 12))]",[],[],[],[]
results,"In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .","[('use', (12, 13)), ('through', (17, 18)), ('obtain', (23, 24)), ('that are', (25, 27)), ('to', (29, 30))]","[('our systems', (9, 11)), ('sense vocabulary compression', (14, 17)), ('hypernyms or', (18, 20)), ('through all relations', (20, 23)), ('scores', (24, 25)), ('overall equivalent', (27, 29)), ('systems that do not use it', (31, 37))]",[],[],[],[]
results,"In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .","[('thanks to', (7, 9)), ('added to', (14, 16)), ('use of', (21, 23)), ('as', (24, 25)), ('on', (36, 37))]","[('Princeton WordNet Gloss Corpus', (10, 14)), ('training data', (17, 19)), ('BERT', (23, 24)), ('input embeddings', (25, 27)), ('outperform', (29, 30)), ('systematically', (30, 31)), ('state of the art', (32, 36)), ('every task', (37, 39))]",[],[],[],[]
ablation-analysis,"As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .","[('the use of', (16, 19)), ('as', (20, 21)), ('have', (24, 25)), ('on', (28, 29)), ('lead to', (32, 34))]","[('additional training corpus ( WNGC )', (7, 13)), ('BERT', (19, 20)), ('input embeddings', (21, 23)), ('major impact', (26, 28)), ('our results', (29, 31)), ('scores above the state of the art', (34, 41))]",[],[],[],[]
ablation-analysis,"Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .","[('Using', (0, 1)), ('instead of', (2, 4)), ('improves', (8, 9)), ('by', (12, 13)), ('adding', (23, 24)), ('to', (26, 27)), ('improves', (30, 31)), ('by', (32, 33))]","[('BERT', (1, 2)), ('ELMo or Glo Ve', (4, 8)), ('score', (11, 12)), ('approximately 3 and 5 points', (13, 18)), ('WNGC', (25, 26)), ('training data', (28, 30)), ('approximately 2 points', (33, 36))]",[],[],[],[]
ablation-analysis,"Finally , through the scores obtained by invidual models ( without ensemble ) , we can observe on the standard deviations that the vocabulary compression method through hypernyms never impact significantly the final score .","[('through', (2, 3)), ('obtained by', (5, 7)), ('observe on', (16, 18)), ('through', (26, 27)), ('never', (28, 29))]","[('scores', (4, 5)), ('invidual models ( without ensemble )', (7, 13)), ('standard deviations', (19, 21)), ('vocabulary compression method', (23, 26)), ('hypernyms', (27, 28)), ('significantly', (30, 31)), ('final score', (32, 34))]",[],[],[],[]
ablation-analysis,"However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .","[('through', (5, 6)), ('seems to', (8, 10)), ('in', (14, 15))]","[('compression method', (3, 5)), ('all relations', (6, 8)), ('negatively impact', (10, 12)), ('results', (13, 14)), ('some cases', (15, 17)), ('ELMo or GloVe', (20, 23))]",[],[],[],[]
research-problem,Incorporating Glosses into Neural Word Sense Disambiguation,[],[],[],[],[],[]
research-problem,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
research-problem,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .","[('models', (1, 2)), ('between', (5, 6)), ('in', (11, 12)), ('of', (22, 23))]","[('GAS', (0, 1)), ('semantic relationship', (3, 5)), ('context and the gloss', (7, 11)), ('improved memory network framework', (13, 17)), ('previous supervised methods and knowledge - based methods', (24, 32))]",[],[],[],[]
research-problem,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
model,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","[('propose', (5, 6)), ('variant of', (21, 23))]","[('novel model GAS', (7, 10)), ('gloss - augmented WSD neural network', (12, 18)), ('memory network', (24, 26))]",[],[],[],[]
model,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"[('jointly encodes', (1, 3)), ('of', (7, 8)), ('models', (12, 13)), ('between', (16, 17)), ('in', (21, 22))]","[('GAS', (0, 1)), ('context and glosses', (4, 7)), ('target word', (9, 11)), ('semantic relationship', (14, 16)), ('context and glosses', (18, 21)), ('memory module', (23, 25))]",[],[],[],[]
model,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .","[('to measure', (2, 4)), ('between', (7, 8)), ('employ', (15, 16)), ('within', (19, 20)), ('as', (22, 23)), ('adopt', (27, 28))]","[('inner relationship', (5, 7)), ('glosses and context', (8, 11)), ('more accurately', (11, 13)), ('multiple passes operation', (16, 19)), ('memory', (21, 22)), ('re-reading process', (24, 26)), ('two memory updating mechanisms', (28, 32))]",[],[],[],[]
model,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .","[('to model', (2, 4)), ('of', (6, 7)), ('propose', (12, 13)), ('in', (20, 21))]","[('semantic relationship', (4, 6)), ('context', (7, 8)), ('glossaugmented neural network ( GAS )', (14, 20)), ('improved memory network paradigm', (22, 26))]",[],[],[],[]
model,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,"[('extend', (1, 2)), ('in', (5, 6)), ('to', (7, 8)), ('to mirror', (13, 15)), ('in', (20, 21))]","[('gloss module', (3, 5)), ('GAS', (6, 7)), ('hierarchical framework', (9, 11)), ('hierarchies of word senses', (16, 20)), ('WordNet', (21, 22))]",[],[],[],[]
hyperparameters,"We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .","[('use', (1, 2)), ('with', (5, 6)), ('keep', (11, 12)), ('during', (14, 15))]","[('pre-trained word embeddings', (2, 5)), ('300 dimensions', (6, 8)), ('fixed', (13, 14)), ('training process', (16, 18))]",[],[],[],[]
hyperparameters,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","[('employ', (1, 2)), ('in both', (5, 7))]","[('256 hidden units', (2, 5)), ('gloss module', (8, 10)), ('context module', (12, 14))]",[],[],[],[]
hyperparameters,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","[('used for', (3, 5)), ('in', (6, 7)), ('with', (12, 13)), ('used for', (21, 23))]","[('Orthogonal initialization', (0, 2)), ('weights', (5, 6)), ('LSTM', (7, 8)), ('random uniform initialization', (9, 12)), ('range [ - 0.1 , 0.1 ]', (13, 20)), ('others', (23, 24))]",[],[],[],[]
hyperparameters,We assign gloss expansion depth K the value of 4 .,"[('assign', (1, 2)), ('of', (8, 9))]","[('gloss expansion depth K', (2, 6)), ('value', (7, 8)), ('4', (9, 10))]",[],[],[],[]
experiments,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","[('experiment with', (2, 4)), ('from', (12, 13)), ('in', (16, 17)), ('finding', (20, 21)), ('performs', (27, 28))]","[('number of passes | T M |', (5, 12)), ('our framework', (17, 19)), ('best', (28, 29))]",[],[],[],[]
hyperparameters,We use Adam optimizer in the training process with 0.001 initial learning rate .,"[('use', (1, 2)), ('in', (4, 5)), ('with', (8, 9))]","[('Adam optimizer', (2, 4)), ('training process', (6, 8)), ('0.001 initial learning rate', (9, 13))]",[],[],[],[]
hyperparameters,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .","[('to avoid', (2, 4)), ('use', (7, 8)), ('set', (11, 12)), ('to', (14, 15))]","[('overfitting', (4, 5)), ('dropout regularization', (8, 10)), ('drop rate', (12, 14)), ('0.5', (15, 16))]",[],[],[],[]
experiments,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,"[('exploits', (2, 3)), ('from', (7, 8)), ('builds', (10, 11)), ('for', (17, 18))]","[('Babelfy', (0, 1)), ('semantic network structure', (4, 7)), ('BabelNet', (8, 9)), ('unified graph - based architecture', (12, 17)), ('WSD and Entity Linking', (18, 22))]",[],[],[],[]
baselines,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,"[('selects', (3, 4)), ('as', (5, 6)), ('makes use of', (10, 13)), ('as', (15, 16))]","[('IMS +emb', (0, 2)), ('IMS', (4, 5)), ('underlying framework', (7, 9)), ('word embeddings', (13, 15)), ('features', (16, 17))]",[],[],[],[]
baselines,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,"[('leverages', (3, 4)), ('which shares', (8, 10)), ('among', (12, 13))]","[('Bi- LSTM', (0, 2)), ('model parameters', (10, 12)), ('all words', (13, 15))]",[],[],[],[]
results,English all - words results,[],[],[],[],[],[]
results,"In this section , we show the performance of our proposed model in the English all - words task .","[('in', (12, 13))]","[('performance', (7, 8)), ('English all - words task', (14, 19))]",[],[],[],[]
results,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,"[('achieves', (4, 5)), ('on', (12, 13))]","[('GAS and GAS ext', (0, 4)), ('state - of - theart performance', (6, 12)), ('concatenation of all test datasets', (14, 19))]",[],[],[],[]
results,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .","[('find that', (18, 20)), ('with', (22, 23)), ('achieves', (27, 28)), ('on', (32, 33))]","[('GAS ext', (20, 22)), ('concatenation memory updating strategy', (23, 27)), ('best results 70.6', (29, 32)), ('concatenation of the four test datasets', (34, 40))]",[],[],[],[]
results,". fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .","[('find that', (6, 8)), ('on', (18, 19))]","[('our best model', (8, 11)), ('outperforms', (11, 12)), ('previous best neural network models', (13, 18)), ('every individual test set', (19, 23))]",[],[],[],[]
results,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .","[('on', (11, 12))]","[('best model', (3, 5)), ('IMS + emb', (8, 11)), ('SE3 , SE13 and SE15 test sets', (13, 20))]",[],[],[],[]
results,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,"[('Incorporating', (0, 1)), ('into', (2, 3)), ('extending', (11, 12))]","[('glosses', (1, 2)), ('neural WSD', (3, 5)), ('greatly improve', (6, 8)), ('performance', (9, 10)), ('original gloss', (13, 15)), ('results', (19, 20))]",[],[],[],[]
results,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .","[('Compared with', (0, 2)), ('greatly improves', (16, 18)), ('by', (21, 22)), ('with the help of', (27, 31))]","[('Bi - LSTM baseline', (3, 7)), ('labeled data', (10, 12)), ('our proposed model', (13, 16)), ('WSD task', (19, 21)), ('2.2 % F1 - score', (22, 27)), ('gloss knowledge', (31, 33))]",[],[],[],[]
results,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .","[('compared with', (2, 4)), ('which only uses', (6, 9)), ('as', (11, 12)), ('can further improve', (18, 21)), ('with the help of', (23, 27)), ('through', (30, 31))]","[('GAS', (5, 6)), ('original gloss', (9, 11)), ('background knowledge', (13, 15)), ('GAS ext', (16, 18)), ('performance', (22, 23)), ('extended glosses', (28, 30)), ('semantic relations', (32, 34))]",[],[],[],[]
results,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .","[('shows', (1, 2)), ('performs', (6, 7)), ('than', (8, 9))]","[('multiple passes operation', (3, 6)), ('better', (7, 8)), ('one pass', (9, 11))]",[],[],[],[]
research-problem,Word Sense Disambiguation using a Bidirectional LSTM,[],"[('Word Sense Disambiguation', (0, 3))]",[],[],[],[]
research-problem,"In this paper we present a clean , yet effective , model for word sense disambiguation .",[],"[('word sense disambiguation', (13, 16))]",[],[],[],[]
model,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .","[('modeling', (10, 11)), ('surrounding', (15, 16)), ('represent', (36, 37)), ('using', (39, 40))]","[('sequence of words', (12, 15)), ('target word', (17, 19)), ('words', (38, 39)), ('real valued vector representation', (40, 44))]",[],[],[],[]
experimental-setup,"The source code , implemented using TensorFlow , has been released as open source 1 .","[('implemented using', (4, 6)), ('released as', (10, 12))]","[('source code', (1, 3)), ('TensorFlow', (6, 7)), ('open source 1', (12, 15))]",[],[],[],[]
experimental-setup,The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,"[('initialized using', (3, 5)), ('of', (7, 8)), ('trained on', (14, 16))]","[('embeddings', (1, 2)), ('set', (6, 7)), ('freely available 2 Glo Ve vectors', (8, 14)), ('Wikipedia and Gigaword', (16, 19))]",[],[],[],[]
experimental-setup,"Words not included in this set are initialized from N ( 0 , 0.1 ) .","[('initialized from', (7, 9))]","[('Words', (0, 1)), ('N ( 0 , 0.1 )', (9, 15))]",[],[],[],[]
results,htsa 3 by was the winner of the SE3 lexical sample task with a F 1 score of 72.9 .,"[('of', (6, 7)), ('with', (12, 13)), ('of', (17, 18))]","[('htsa', (0, 1)), ('winner', (5, 6)), ('SE3 lexical sample task', (8, 12)), ('F 1 score', (14, 17)), ('72.9', (18, 19))]",[],[],[],[]
results,Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,"[('achieves', (3, 4)), ('on', (7, 8)), ('tied with', (11, 13))]","[('Our proposed model', (0, 3)), ('top score', (5, 7)), ('SE2', (8, 9)), ('IMS + adapted CW', (13, 17)), ('SE3', (18, 19))]",[],[],[],[]
results,"Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .","[('see', (3, 4)), ('consistently', (6, 7)), ('on', (10, 11))]","[('dropword', (5, 6)), ('results', (9, 10)), ('both', (11, 12)), ('SE2 and SE3', (12, 15))]",[],[],[],[]
results,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .","[('Randomizing', (0, 1)), ('of', (3, 4)), ('yields', (7, 8)), ('are', (25, 26))]","[('input words', (5, 7)), ('substantially worse result', (9, 12)), ('order of the words', (21, 25)), ('significant', (26, 27))]",[],[],[],[]
research-problem,Knowledge - based Word Sense Disambiguation using Topic Models,[],"[('Knowledge -', (0, 2))]",[],[],[],[]
research-problem,"In this paper , we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context .","[('leverage', (5, 6)), ('of', (8, 9)), ('to design', (11, 13)), ('scales', (17, 18)), ('with', (19, 20)), ('in', (24, 25))]","[('formalism', (7, 8)), ('topic model', (9, 11)), ('WSD system', (14, 16)), ('linearly', (18, 19)), ('number of words', (21, 24)), ('context', (26, 27))]",[],[],[],[]
research-problem,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
research-problem,"WSD , being AI - complete ( Navigli 2009 ) , is still an open problem after over two decades of research .",[],"[('WSD', (0, 1))]",[],[],[],[]
model,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .","[('propose', (5, 6)), ('for', (13, 14)), ('utilizes', (22, 23)), ('as', (26, 27)), ('for', (29, 30))]","[('novel knowledge - based WSD algorithm', (7, 13)), ('all - word WSD task', (15, 20)), ('whole document', (24, 26)), ('context', (28, 29)), ('word', (31, 32)), ('current sentence', (37, 39))]",[],[],[],[]
model,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .","[('leverage', (11, 12)), ('of', (14, 15)), ('especially', (18, 19))]","[('WSD', (8, 9)), ('formalism', (13, 14)), ('topic models', (15, 17)), ('Latent Dirichlet Allocation ( LDA )', (19, 25))]",[],[],[],[]
model,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,"[('use', (1, 2)), ('prior for', (4, 6)), ('over', (9, 10)), ('to model', (11, 13)), ('within', (17, 18))]","[('non-uniform', (3, 4)), ('synset distribution', (7, 9)), ('words', (10, 11)), ('frequency of words', (14, 17)), ('synset', (19, 20))]",[],[],[],[]
model,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .","[('model', (4, 5)), ('between', (7, 8)), ('by using', (9, 11)), ('prior for drawing', (13, 16)), ('of', (19, 20))]","[('relationships', (6, 7)), ('synsets', (8, 9)), ('logisticnormal', (12, 13)), ('synset proportions', (17, 19)), ('document', (21, 22))]",[],[],[],[]
results,"The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .","[('denoted by', (4, 6)), ('referring', (12, 13)), ('using', (15, 16)), ('by', (30, 31)), ('by achieving', (39, 41))]","[('WSD', (14, 15)), ('outperforms', (19, 20)), ('state - of - the - art WSD system', (21, 30)), ('significant margin', (32, 34)), ('pvalue < 0.01', (35, 38)), ('overall F1 - score', (42, 46)), ('66.9', (47, 48))]",[],[],[],[]
results,"We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .","[('observe', (2, 3)), ('of', (6, 7)), ('than', (14, 15))]","[('performance', (5, 6)), ('proposed model', (8, 10)), ('not much worse', (11, 14)), ('best supervised system', (16, 19)), ('Melamud16 ( 69.4 )', (20, 24))]",[],[],[],[]
results,The proposed system outperforms all previous knowledgebased systems overall parts of speech .,[],"[('proposed system', (1, 3)), ('outperforms', (3, 4)), ('all previous knowledgebased systems overall parts of speech', (4, 12))]",[],[],[],[]
research-problem,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,[],"[('Question Answering Data', (8, 11))]",[],[],[],[]
research-problem,"Our approach outperforms the previous state - of - the - art system on this data , resulting in an average 8 % improvement of the final score .","[('resulting in', (17, 19)), ('of', (24, 25))]","[('Our approach', (0, 2)), ('outperforms', (2, 3)), ('previous state - of - the - art system', (4, 13)), ('average 8 % improvement', (20, 24)), ('final score', (26, 28))]",[],[],[],[]
research-problem,Knowledge base question answering ( QA ) requires a precise modeling of the question semantics through the entities and relations available in the knowledge base ( KB ) in order to retrieve the correct answer .,[],"[('Knowledge base question answering ( QA )', (0, 7))]",[],[],[],[]
model,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .","[('perform', (16, 17)), ('in', (24, 25)), ('that makes', (29, 31))]","[('entity mention detection and entity disambiguation jointly', (17, 24)), ('single neural model', (26, 29)), ('whole process', (32, 34)), ('end - to - end differentiable', (34, 40))]",[],[],[],[]
model,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .","[('To overcome', (0, 2)), ('in', (4, 5)), ('automatically learn', (9, 11)), ('over', (12, 13)), ('of', (17, 18))]","[('noise', (3, 4)), ('data', (6, 7)), ('features', (11, 12)), ('set of contexts', (14, 17)), ('different granularity levels', (18, 21))]",[],[],[],[]
model,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .","[('extract', (3, 4)), ('from', (5, 6)), ('of', (10, 11)), ('extracted for', (20, 22)), ('produced based on', (31, 34)), ('surrounding', (36, 37)), ('in', (40, 41))]","[('features', (4, 5)), ('knowledge base context', (7, 10)), ('candidate entity', (12, 14)), ('character - level features', (15, 19)), ('entity label', (23, 25)), ('higher - level features', (26, 30)), ('entities', (35, 36)), ('candidate entity', (38, 40)), ('knowledge graph', (42, 44))]",[],[],[],[]
research-problem,"In the recent years , EL on Twitter data has emerged as a branch of entity linking research .",[],"[('EL on Twitter data', (5, 9))]",[],[],[],[]
results,We compile two new datasets for entity linking on questions that we derive from publicly available question answering data : WebQSP and GraphQuestions .,"[('derive from', (12, 14))]","[('entity linking on questions', (6, 10)), ('publicly available question answering data', (14, 19)), ('WebQSP', (20, 21)), ('GraphQuestions', (22, 23))]",[],[],[],[]
baselines,We also include a heuristics baseline that ranks candidate entities according to their frequency in Wikipedia .,"[('include', (2, 3)), ('ranks', (7, 8)), ('according to', (10, 12)), ('in', (14, 15))]","[('heuristics baseline', (4, 6)), ('candidate entities', (8, 10)), ('Wikipedia', (15, 16))]",[],[],[],[]
results,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,"[('shows', (3, 4)), ('better than', (11, 13)), ('by', (17, 18))]","[('VCG model', (1, 3)), ('overall F- score result', (5, 9)), ('DBPedia Spotlight baseline', (14, 17)), ('wide margin', (19, 21))]",[],[],[],[]
results,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,"[('achieves', (7, 8)), ('manages to keep', (17, 20))]","[('our model', (5, 7)), ('higher precision values', (8, 11)), ('other approaches', (14, 16)), ('satisfactory level of recall', (21, 25))]",[],[],[],[]
research-problem,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,[],[],[],[],[],[]
research-problem,"Word Sense Disambiguation ( WSD ) is an important problem in Natural Language Processing ( NLP ) , both in its own right and as a steppingstone to other advanced tasks in the NLP pipeline , applications such as machine translation and question answering .",[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
approach,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .","[('develop', (5, 6)), ('that leverages', (10, 12))]","[('our supervised WSD model', (6, 10)), ('Bidirectional Long Short - Term Memory ( BLSTM ) network', (13, 23))]",[],[],[],[]
approach,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .","[('works with', (2, 4)), ('learned during', (15, 17)), ('employs', (21, 22)), ('learned through', (33, 35)), ('called', (40, 41)), ('for', (49, 50))]","[('neural sense vectors ( i.e. sense embeddings )', (4, 12)), ('model training', (17, 19)), ('neural word vectors ( i.e. word embeddings )', (22, 30)), ('unsupervised deep learning approach', (36, 40)), ('GloVe ( Global Vectors for word representation )', (41, 49)), ('context words', (51, 53))]",[],[],[],[]
hyperparameters,This results in a vocabulary size of | V | = 29044 .,"[('results in', (1, 3)), ('of', (6, 7))]","[('vocabulary size', (4, 6)), ('| V | = 29044', (7, 12))]",[],[],[],[]
results,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .","[('show', (1, 2)), ('sits among', (5, 7))]","[('5 top - performing algorithms', (8, 13))]",[],[],[],[]
baselines,IMS + adapted CW is another WSD model that considers deep neural networks and also uses pre-trained word embeddings as inputs .,"[('considers', (9, 10)), ('uses', (15, 16)), ('as', (19, 20))]","[('IMS + adapted CW', (0, 4)), ('WSD', (6, 7)), ('deep neural networks', (10, 13)), ('pre-trained word embeddings', (16, 19)), ('inputs', (20, 21))]",[],[],[],[]
results,htsa 3 was the winner of the SensEval - 3 lexical sample .,"[('of', (5, 6))]","[('htsa 3', (0, 2)), ('winner', (4, 5)), ('SensEval - 3 lexical sample', (7, 12))]",[],[],[],[]
baselines,"IRST - Kernels utilizes kernel methods for pattern abstraction , paradigmatic and syntagmatic information and unsupervised term proximity on British National Corpus ( BNC ) , in SVM classifiers .","[('utilizes', (3, 4)), ('for', (6, 7)), ('on', (18, 19)), ('in', (26, 27))]","[('IRST', (0, 1)), ('kernel methods', (4, 6)), ('pattern abstraction', (7, 9)), ('paradigmatic and syntagmatic information', (10, 14)), ('unsupervised term proximity', (15, 18)), ('British National Corpus ( BNC )', (19, 25)), ('SVM classifiers', (27, 29))]",[],[],[],[]
research-problem,Neural Sequence Learning Models for Word Sense Disambiguation,[],[],[],[],[],[]
research-problem,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .",[],"[('Word Sense Disambiguation , WSD', (16, 21))]",[],[],[],[]
model,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .",[],"[('supervised WSD', (7, 9)), ('sequence labeling', (55, 57))]",[],[],[],[]
model,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .","[('design , analyze and compare', (6, 11)), ('of', (15, 16)), ('ranging from', (19, 21)), ('to', (29, 30))]","[('various neural architectures', (12, 15)), ('different complexities', (16, 18)), ('single bidirectional Long Short - Term Memory', (22, 29)), ('sequence - tosequence approach', (31, 35))]",[],[],[],[]
hyperparameters,LSTM Tagger,[],[],[],[],[],[]
results,"6 Finally , we carried out an experiment on multilingual WSD using the Italian , German , French and Spanish data of SE13 .","[('carried out', (4, 6)), ('on', (8, 9)), ('using', (11, 12)), ('of', (21, 22))]","[('experiment', (7, 8)), ('multilingual WSD', (9, 11)), ('Italian , German , French and Spanish data', (13, 21)), ('SE13', (22, 23))]",[],[],[],[]
hyperparameters,"All models were trained fora fixed number of epochs E = 40 using Adadelta ( Zeiler , 2012 ) with learning rate 1.0 and batch size 32 .","[('trained fora', (3, 5)), ('using', (12, 13)), ('with', (19, 20))]","[('fixed number of epochs E = 40', (5, 12)), ('Adadelta', (13, 14)), ('learning rate', (20, 22)), ('1.0', (22, 23)), ('batch size', (24, 26)), ('32', (26, 27))]",[],[],[],[]
results,"11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .","[('achieved', (7, 8)), ('to', (31, 32)), ('in', (36, 37)), ('performing', (40, 41)), ('with', (43, 44)), ('tuned over', (46, 48))]","[('both BLSTM and Seq2Seq', (3, 7)), ('results', (8, 9)), ('state - of - the - art', (12, 19)), ('best supervised system', (33, 36)), ('each benchmark', (37, 39)), ('on par', (41, 43)), ('word experts', (44, 46)), ('explicitly engineered features', (48, 51))]",[],[],[],[]
results,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .","[('tended consistently to', (5, 8)), ('suggesting', (13, 14)), ('for', (29, 30))]","[('BLSTM models', (3, 5)), ('outperform', (8, 9)), ('Seq2Seq counterparts', (10, 12)), ('encoder - decoder architecture', (16, 20)), ('suboptimal', (28, 29)), ('WSD', (30, 31))]",[],[],[],[]
results,"Furthermore , introducing LEX ( cf. Section 4 ) as auxiliary task was generally helpful ; on the other hand , POS did not seem to help , corroborating previous findings ( Alonso .","[('introducing', (2, 3)), ('as', (9, 10)), ('did not', (22, 24))]","[('LEX', (3, 4)), ('auxiliary task', (10, 12)), ('generally helpful', (13, 15)), ('POS', (21, 22)), ('help', (26, 27))]",[],[],[],[]
results,English All - words WSD,[],[],[],[],[],[]
results,"It is worth noting that RNN - based architectures outperformed classical supervised approaches when dealing with verbs , which are shown to be highly ambiguous .","[('worth', (2, 3)), ('when dealing with', (13, 16)), ('shown to be', (20, 23))]","[('RNN - based architectures', (5, 9)), ('outperformed', (9, 10)), ('classical supervised approaches', (10, 13)), ('verbs', (16, 17)), ('highly ambiguous', (23, 25))]",[],[],[],[]
results,Multilingual All - words WSD,[],[],[],[],[],[]
results,"F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .","[('show', (4, 5)), ('trained only on', (13, 16)), ('consistently outperformed', (19, 21)), ('achieved', (25, 26)), ('with', (30, 31))]","[('F - score figures', (0, 4)), ('bilingual and multilingual models', (6, 10)), ('English data', (16, 18)), ('MFS baseline', (22, 24)), ('results', (26, 27)), ('competitive', (29, 30)), ('best participating systems', (32, 35))]",[],[],[],[]
results,"We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .","[('note', (2, 3)), ('when', (18, 19)), ('from', (20, 21))]","[('overall F- score performance', (5, 9)), ('did not change substantially ( and slightly improved )', (9, 18)), ('moving', (19, 20)), ('bilingual to multilingual models', (21, 25))]",[],[],[],[]
research-problem,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings,[],"[('Interpretable Word Sense Disambiguation', (6, 10))]",[],[],[],[]
research-problem,Word Sense Disambiguation ( WSD ) is the task to identify the correct sense of the usage of a word from a ( usually ) fixed inventory of sense identifiers .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
results,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,"[('beats', (10, 11)), ('of', (16, 17))]","[('Simple k NN with ELMo as well', (0, 7)), ('state of the art', (12, 16)), ('lexical sample task', (18, 21))]",[],[],[],[]
code,We are using Version 2.1 : https://github.com/getalp/,[],[],[],[],[],[]
results,UFSAC 4 BERT performed best in experiment one .,"[('performed', (3, 4))]","[('UFSAC 4 BERT', (0, 3)), ('best', (4, 5))]",[],[],[],[]
results,shows that including the POS restriction increases the F 1 scores for S7 - T7 and S7 - T17 .,"[('including', (2, 3)), ('increases', (6, 7)), ('for', (11, 12))]","[('POS restriction', (4, 6)), ('F 1 scores', (8, 11)), ('S7 - T7 and S7 - T17', (12, 19))]",[],[],[],[]
research-problem,Learning Distributed Representations of Texts and Entities from Knowledge Base,[],"[('Learning Distributed Representations of Texts and Entities', (0, 7))]",[],[],[],[]
code,Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base ( KB ) such as Wikipedia and Freebase .,[],"[('learning distributed representations of entities', (4, 9)), ('https://github.com/studio-ousia/ntee base ( KB )', (13, 18))]",[],[],[],[]
model,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .","[('propose', (4, 5)), ('to jointly learn', (18, 21)), ('of', (23, 24))]","[('Neural Text - Entity Encoder ( NTEE )', (5, 13)), ('neural network model', (15, 18)), ('distributed representations', (21, 23)), ('texts ( i.e. , sentences and paragraphs )', (24, 32)), ('KB entities', (33, 35))]",[],[],[],[]
model,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .","[('For', (0, 1)), ('in', (3, 4)), ('aims to', (9, 11)), ('places', (17, 18)), ('close to', (24, 26)), ('in', (28, 29))]","[('every text', (1, 3)), ('KB', (5, 6)), ('our model', (7, 9)), ('predict', (11, 12)), ('relevant entities', (13, 15)), ('text and the relevant entities', (19, 24)), ('each other', (26, 28)), ('continuous vector space', (30, 33))]",[],[],[],[]
research-problem,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .","[('shows', (3, 4)), ('can be', (6, 8)), ('using', (10, 11)), ('of', (14, 15))]","[('ESA', (2, 3)), ('text', (5, 6)), ('accurately represented', (8, 10)), ('small set', (12, 14))]",[],[],[],[]
model,"In both tasks , we adopt a simple multi -layer perceptron ( MLP ) classifier with the learned representations as features .","[('adopt', (5, 6)), ('with', (15, 16))]","[('simple multi -layer perceptron ( MLP ) classifier', (7, 15)), ('learned representations', (17, 19))]",[],[],[],[]
model,"Our work differs from these works because we aim to map texts ( i.e. , sentences and paragraphs ) and entities into the same vector space .",[],[],[],[],[],[]
model,We train the model using a large amount of entity annotations extracted directly from Wikipedia .,"[('train', (1, 2)), ('using', (4, 5)), ('extracted directly from', (11, 14))]","[('model', (3, 4)), ('large amount of entity annotations', (6, 11)), ('Wikipedia', (14, 15))]",[],[],[],[]
baselines,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,"[('based on', (4, 6)), ('augmented with', (9, 11)), ('with', (14, 15))]","[('BOW - DT', (0, 3)), ('BOW baseline', (7, 9)), ('feature set', (12, 14)), ('dependency relation indicators', (15, 18))]",[],[],[],[]
baselines,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,"[('based on', (4, 6)), ('to derive', (10, 12)), ('of', (15, 16))]","[('QANTA', (0, 1)), ('recursive neural network', (7, 10)), ('distributed representations', (13, 15)), ('questions', (16, 17))]",[],[],[],[]
baselines,The method also uses the LR classifier with the derived representations as features .,"[('uses', (3, 4)), ('with', (7, 8)), ('as', (11, 12))]","[('LR classifier', (5, 7)), ('derived representations', (9, 11)), ('features', (12, 13))]",[],[],[],[]
baselines,FTS - BRNN is based on the bidirectional recurrent neural network ( RNN ) with gated recurrent units ( GRU ) .,"[('based on', (4, 6)), ('with', (14, 15))]","[('FTS - BRNN', (0, 3)), ('bidirectional recurrent neural network ( RNN )', (7, 14)), ('gated recurrent units ( GRU )', (15, 21))]",[],[],[],[]
results,The experimental results show that our NTEE model achieved the best performance compared to the other proposed models and all the baseline methods on both the history and the literature datasets . :,"[('show', (3, 4)), ('achieved', (8, 9)), ('compared to', (12, 14)), ('on', (23, 24))]","[('our NTEE model', (5, 8)), ('best performance', (10, 12)), ('other proposed models', (15, 18)), ('all the baseline methods', (19, 23)), ('history and the literature datasets', (26, 31))]",[],[],[],[]
results,"In particular , despite the simplicity of the neural network architecture of our method compared to the state - of - the - art methods ( i.e. , QANTA and FTS - BRNN ) , our method clearly outperformed these methods .","[('of', (11, 12)), ('compared to', (14, 16)), ('i.e.', (26, 27))]","[('our method', (12, 14)), ('state - of - the - art methods', (17, 25)), ('QANTA and FTS - BRNN )', (28, 34)), ('our method', (35, 37)), ('clearly outperformed', (37, 39)), ('methods', (40, 41))]",[],[],[],[]
research-problem,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,[],"[('Neural Word Sense Disambiguation', (8, 12))]",[],[],[],[]
research-problem,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
research-problem,"Various approaches have been proposed to achieve WSD , and they are generally ordered by the type and the quantity of resources they use :","[('ordered by', (13, 15))]","[('Various approaches', (0, 2)), ('WSD', (7, 8))]",[],[],[],[]
model,"Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing , but in this work , the idea is to solve this issue by taking advantage of one of the multiple semantic relationships between senses included in WordNet : the hypernymy and hyponymy relationships .","[('by creating', (7, 9)), ('through', (20, 21)), ('between', (45, 46)), ('included in', (47, 49))]","[('new sense annotated corpora', (9, 13)), ('automatically', (15, 16)), ('crowdsourcing', (21, 22)), ('one', (39, 40)), ('multiple semantic relationships', (42, 45)), ('senses', (46, 47)), ('WordNet', (49, 50)), ('hypernymy and hyponymy relationships', (52, 56))]",[],[],[],[]
code,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,[],[],[],[],[],[]
experiments,Language Model Based WSD,[],[],[],[],[],[]
experiments,We performed every training for 20 epochs .,"[('performed', (1, 2)), ('for', (4, 5))]","[('every training', (2, 4)), ('20 epochs', (5, 7))]",[],[],[],[]
experiments,"We trained with mini-batches of 100 sentences , truncated to 80 words , and padded with zero vectors from the end , and we used Adam ( Kingma and Ba , 2014 ) , with the same default parameters described in their article as the optimization method , except for the learning rate that we set to 0.0001 ( 10 times smaller than the default value ) .","[('trained with', (1, 3)), ('of', (4, 5)), ('truncated to', (8, 10)), ('padded with', (14, 16)), ('used', (24, 25)), ('except for', (48, 50)), ('set to', (55, 57))]","[('mini-batches', (3, 4)), ('100 sentences', (5, 7)), ('80 words', (10, 12)), ('zero vectors', (16, 18)), ('Adam', (25, 26)), ('learning rate', (51, 53))]",[],[],[],[]
experimental-setup,All models have been trained on Nvidia 's Titan X GPUs .,"[('trained on', (4, 6))]","[(""Nvidia 's Titan X GPUs"", (6, 11))]",[],[],[],[]
results,"Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .","[('of', (11, 12)), ('obtained by', (13, 15)), ('using', (17, 18)), ('is', (24, 25)), ('regarding', (29, 30))]","[('difference', (10, 11)), ('scores', (12, 13)), ('our system', (15, 17)), ('sense vocabulary reduction or', (19, 23)), ('overall', (25, 26)), ('not significant', (26, 28)), ('"" ALL "" column', (31, 35))]",[],[],[],[]
results,"In comparison with the other works , our systems trained on the SemCor alone expose results comparable with the best system of , which is trained on the same corpus and augmented with a semi-supervised method .","[('trained on', (9, 11)), ('expose', (14, 15)), ('comparable with', (16, 18)), ('trained on', (25, 27)), ('augmented with', (31, 33))]","[('our systems', (7, 9)), ('SemCor alone', (12, 14)), ('results', (15, 16)), ('best system', (19, 21)), ('same corpus', (28, 30)), ('semi-supervised method', (34, 36))]",[],[],[],[]
results,"When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .","[('add', (2, 3)), ('to', (7, 8)), ('obtain', (14, 15)), ('on', (21, 22)), ('except on', (24, 26))]","[('WordNet Gloss Tagged', (4, 7)), ('training data', (9, 11)), ('systematically', (15, 16)), ('state of the art results', (16, 21)), ('all tasks', (22, 24)), ('SensEval', (26, 27))]",[],[],[],[]
results,"Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .","[('does not', (7, 9)), ('on', (15, 16)), ('in', (20, 21)), ('is', (31, 32)), ('as', (35, 36))]","[('sense reduction method', (4, 7)), ('consistently improves or decreases', (9, 13)), ('score', (14, 15)), ('every task', (16, 18)), ('overall', (21, 22)), ('result', (30, 31)), ('roughly the same', (32, 35))]",[],[],[],[]
results,"As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .","[('is', (6, 7)), ('in', (11, 12)), ('improves', (15, 16))]","[('ensembling', (5, 6)), ('very efficient method', (8, 11)), ('WSD', (12, 13)), ('systematically', (16, 17)), ('all our results', (17, 20))]",[],[],[],[]
results,"Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .","[('with', (2, 3)), ('are', (7, 8)), ('when applying', (10, 12))]","[('ensembles', (3, 4)), ('scores', (6, 7)), ('significantly higher', (8, 10)), ('vocabulary reduction algorithm', (13, 16))]",[],[],[],[]
research-problem,Incorporating Glosses into Neural Word Sense Disambiguation,[],[],[],[],[],[]
research-problem,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
research-problem,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .","[('models', (1, 2)), ('between', (5, 6)), ('in', (11, 12)), ('of', (22, 23))]","[('GAS', (0, 1)), ('semantic relationship', (3, 5)), ('context and the gloss', (7, 11)), ('improved memory network framework', (13, 17)), ('previous supervised methods and knowledge - based methods', (24, 32))]",[],[],[],[]
research-problem,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[]
model,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","[('propose', (5, 6)), ('variant of', (21, 23))]","[('novel model GAS', (7, 10)), ('gloss - augmented WSD neural network', (12, 18)), ('memory network', (24, 26))]",[],[],[],[]
model,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"[('jointly encodes', (1, 3)), ('of', (7, 8)), ('models', (12, 13)), ('between', (16, 17)), ('in', (21, 22))]","[('GAS', (0, 1)), ('context and glosses', (4, 7)), ('target word', (9, 11)), ('semantic relationship', (14, 16)), ('context and glosses', (18, 21)), ('memory module', (23, 25))]",[],[],[],[]
model,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .","[('to measure', (2, 4)), ('between', (7, 8)), ('employ', (15, 16)), ('within', (19, 20)), ('as', (22, 23)), ('adopt', (27, 28))]","[('inner relationship', (5, 7)), ('glosses and context', (8, 11)), ('more accurately', (11, 13)), ('multiple passes operation', (16, 19)), ('memory', (21, 22)), ('re-reading process', (24, 26)), ('two memory updating mechanisms', (28, 32))]",[],[],[],[]
model,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .","[('to model', (2, 4)), ('of', (6, 7)), ('propose', (12, 13)), ('in', (20, 21))]","[('semantic relationship', (4, 6)), ('context', (7, 8)), ('glossaugmented neural network ( GAS )', (14, 20)), ('improved memory network paradigm', (22, 26))]",[],[],[],[]
model,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,"[('extend', (1, 2)), ('in', (5, 6)), ('to', (7, 8)), ('to mirror', (13, 15)), ('in', (20, 21))]","[('gloss module', (3, 5)), ('GAS', (6, 7)), ('hierarchical framework', (9, 11)), ('hierarchies of word senses', (16, 20)), ('WordNet', (21, 22))]",[],[],[],[]
hyperparameters,"We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .","[('use', (1, 2)), ('with', (5, 6)), ('keep', (11, 12)), ('during', (14, 15))]","[('pre-trained word embeddings', (2, 5)), ('300 dimensions', (6, 8)), ('fixed', (13, 14)), ('training process', (16, 18))]",[],[],[],[]
hyperparameters,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","[('employ', (1, 2)), ('in both', (5, 7))]","[('256 hidden units', (2, 5)), ('gloss module', (8, 10)), ('context module', (12, 14))]",[],[],[],[]
hyperparameters,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","[('used for', (3, 5)), ('in', (6, 7)), ('with', (12, 13)), ('used for', (21, 23))]","[('Orthogonal initialization', (0, 2)), ('weights', (5, 6)), ('LSTM', (7, 8)), ('random uniform initialization', (9, 12)), ('range [ - 0.1 , 0.1 ]', (13, 20)), ('others', (23, 24))]",[],[],[],[]
hyperparameters,We assign gloss expansion depth K the value of 4 .,"[('assign', (1, 2)), ('of', (8, 9))]","[('gloss expansion depth K', (2, 6)), ('value', (7, 8)), ('4', (9, 10))]",[],[],[],[]
experiments,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","[('experiment with', (2, 4)), ('from', (12, 13)), ('in', (16, 17)), ('finding', (20, 21)), ('performs', (27, 28))]","[('number of passes | T M |', (5, 12)), ('our framework', (17, 19)), ('best', (28, 29))]",[],[],[],[]
hyperparameters,We use Adam optimizer in the training process with 0.001 initial learning rate .,"[('use', (1, 2)), ('in', (4, 5)), ('with', (8, 9))]","[('Adam optimizer', (2, 4)), ('training process', (6, 8)), ('0.001 initial learning rate', (9, 13))]",[],[],[],[]
hyperparameters,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .","[('to avoid', (2, 4)), ('use', (7, 8)), ('set', (11, 12)), ('to', (14, 15))]","[('overfitting', (4, 5)), ('dropout regularization', (8, 10)), ('drop rate', (12, 14)), ('0.5', (15, 16))]",[],[],[],[]
experiments,This work shows that glosses are important to WSD and enriching gloss information via its semantic relations can help to WSD .,"[('shows', (2, 3)), ('important to', (6, 8)), ('enriching', (10, 11)), ('via', (13, 14)), ('help to', (18, 20))]","[('glosses', (4, 5)), ('WSD', (8, 9)), ('gloss information', (11, 13)), ('semantic relations', (15, 17)), ('WSD', (20, 21))]",[],[],[],[]
experiments,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,"[('exploits', (2, 3)), ('from', (7, 8)), ('builds', (10, 11)), ('for', (17, 18))]","[('Babelfy', (0, 1)), ('semantic network structure', (4, 7)), ('BabelNet', (8, 9)), ('unified graph - based architecture', (12, 17)), ('WSD and Entity Linking', (18, 22))]",[],[],[],[]
baselines,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,"[('selects', (3, 4)), ('as', (5, 6)), ('makes use of', (10, 13)), ('as', (15, 16))]","[('IMS +emb', (0, 2)), ('IMS', (4, 5)), ('underlying framework', (7, 9)), ('word embeddings', (13, 15)), ('features', (16, 17))]",[],[],[],[]
baselines,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,"[('leverages', (3, 4)), ('which shares', (8, 10)), ('among', (12, 13))]","[('Bi- LSTM', (0, 2)), ('model parameters', (10, 12)), ('all words', (13, 15))]",[],[],[],[]
results,English all - words results,[],[],[],[],[],[]
results,"In this section , we show the performance of our proposed model in the English all - words task .","[('in', (12, 13))]","[('performance', (7, 8)), ('English all - words task', (14, 19))]",[],[],[],[]
results,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,"[('achieves', (4, 5)), ('on', (12, 13))]","[('GAS and GAS ext', (0, 4)), ('state - of - theart performance', (6, 12)), ('concatenation of all test datasets', (14, 19))]",[],[],[],[]
results,". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .","[('performs', (2, 3)), ('on', (4, 5)), ('with', (17, 18)), ('achieves', (22, 23)), ('on', (27, 28))]","[('best', (3, 4)), ('all the test sets', (5, 9)), ('GAS ext', (15, 17)), ('concatenation memory updating strategy', (18, 22)), ('best results', (24, 26)), ('70.6', (26, 27)), ('concatenation of', (29, 31))]",[],[],[],[]
results,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .","[('Compared with', (0, 2)), ('find that', (15, 17)), ('outperforms', (20, 21)), ('on', (27, 28))]","[('our best model', (17, 20)), ('previous best neural network models', (22, 27)), ('every individual test set', (28, 32))]",[],[],[],[]
results,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .","[('on', (11, 12))]","[('best model', (3, 5)), ('IMS + emb', (8, 11)), ('SE3 , SE13 and SE15 test sets', (13, 20))]",[],[],[],[]
results,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,"[('Incorporating', (0, 1)), ('into', (2, 3)), ('extending', (11, 12))]","[('glosses', (1, 2)), ('neural WSD', (3, 5)), ('greatly improve', (6, 8)), ('performance', (9, 10)), ('original gloss', (13, 15)), ('results', (19, 20))]",[],[],[],[]
results,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .","[('Compared with', (0, 2)), ('greatly improves', (16, 18)), ('by', (21, 22)), ('with the help of', (27, 31))]","[('Bi - LSTM baseline', (3, 7)), ('labeled data', (10, 12)), ('our proposed model', (13, 16)), ('WSD task', (19, 21)), ('2.2 % F1 - score', (22, 27)), ('gloss knowledge', (31, 33))]",[],[],[],[]
results,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .","[('compared with', (2, 4)), ('which only uses', (6, 9)), ('as', (11, 12)), ('can further improve', (18, 21)), ('with the help of', (23, 27)), ('through', (30, 31))]","[('GAS', (5, 6)), ('original gloss', (9, 11)), ('background knowledge', (13, 15)), ('GAS ext', (16, 18)), ('performance', (22, 23)), ('extended glosses', (28, 30)), ('semantic relations', (32, 34))]",[],[],[],[]
research-problem,Semi-supervised Word Sense Disambiguation with Neural Models,[],"[('Semi-supervised Word Sense Disambiguation', (0, 4))]",[],[],[],[]
research-problem,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,[],"[('text - word sense disambiguation ( WSD )', (7, 15))]",[],[],[],[]
research-problem,Word sense disambiguation ( WSD ) is a long - standing problem in natural language processing ( NLP ) with broad applications .,[],"[('Word sense disambiguation ( WSD )', (0, 6))]",[],[],[],[]
model,"In this paper , we describe two novel WSD algorithms .","[('describe', (5, 6))]","[('two novel WSD algorithms', (6, 10))]",[],[],[],[]
model,The first is based on a Long Short Term Memory ( LSTM ) ) .,"[('based on', (3, 5))]","[('Long Short Term Memory ( LSTM ) )', (6, 14))]",[],[],[],[]
model,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,"[('present', (2, 3)), ('to label', (10, 12)), ('based on', (14, 16)), ('to', (18, 19))]","[('semi-supervised algorithm', (4, 6)), ('label propagation', (8, 10)), ('unlabeled sentences', (12, 14)), ('similarity', (17, 18)), ('labeled ones', (19, 21))]",[],[],[],[]
experiments,Sem Eval Tasks,[],[],[],[],[],[]
results,Our proposed algorithms achieve the highest all - words F 1 scores except for Sem - Eval 2013 .,"[('achieve', (3, 4)), ('except for', (12, 14))]","[('Our proposed algorithms', (0, 3)), ('highest all - words F 1 scores', (5, 12)), ('Sem - Eval 2013', (14, 18))]",[],[],[],[]
results,"Our self - trained word embeddings have similar performance to the pre-trained embeddings , as shown in .","[('have', (6, 7)), ('to', (9, 10))]","[('Our self - trained word embeddings', (0, 6)), ('similar performance', (7, 9)), ('pre-trained embeddings', (11, 13))]",[],[],[],[]
hyperparameters,The learning rate is 0.1 .,"[('is', (3, 4))]","[('learning rate', (1, 3)), ('0.1', (4, 5))]",[],[],[],[]
experiments,"We experimented with other learning rates , and observed no significant performance difference after the training converges .","[('experimented with', (1, 3)), ('observed', (8, 9)), ('after', (13, 14))]","[('other learning rates', (3, 6)), ('no significant performance difference', (9, 13)), ('training converges', (15, 17))]",[],[],[],[]
results,Word2 Vec vectors Vs. LSTM,[],[],[],[],[],[]
results,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,"[('shows', (0, 1))]","[('LSTM classifier', (3, 5)), ('outperforms', (5, 6)), ('Word2 Vec classifier', (7, 10))]",[],[],[],[]
results,Sem Cor Vs. OMSTI,[],[],[],[],[],[]
results,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .","[('trained with', (10, 12)), ('performs', (13, 14)), ('than', (15, 16)), ('trained with', (17, 19))]","[('LSTM classifier', (8, 10)), ('OMSTI', (12, 13)), ('worse', (14, 15)), ('SemCor', (19, 20))]",[],[],[],[]
tasks,The algorithm performs similarly on the different data sets .,"[('performs', (2, 3)), ('on', (4, 5))]","[('algorithm', (1, 2)), ('similarly', (3, 4)), ('different data sets', (6, 9))]",[],[],[],[]
baselines,"Word 2 Vec : a nearest - neighbor classifier with Word2 Vec word embedding , which has similar performance to cutting - edge algorithms studied in on SemEval tasks .","[('with', (9, 10))]","[('Word 2 Vec', (0, 3)), ('nearest', (5, 6)), ('Word2 Vec word embedding', (10, 14))]",[],[],[],[]
results,"Across all part of speech tags and datasets , F1 scores increase after adding more training data .","[('Across', (0, 1)), ('after', (12, 13))]","[('all part of speech tags and datasets', (1, 8)), ('F1 scores', (9, 11)), ('increase', (11, 12)), ('adding', (13, 14)), ('more training data', (14, 17))]",[],[],[],[]
results,The SemCor ( or MASC ) trained classifier is on a par with the NOAD trained classifier on F1 score .,"[('with', (12, 13)), ('on', (17, 18))]","[('SemCor ( or MASC ) trained classifier', (1, 8)), ('on a par', (9, 12)), ('NOAD trained classifier', (14, 17)), ('F1 score', (18, 20))]",[],[],[],[]
research-problem,LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS FROM TEXT,[],"[('LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS', (0, 8))]",[],[],[],[]
research-problem,Work done as a Google AI Resident,[],[],[],[],[],[]
model,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .","[('trained to match', (21, 24))]","[('RELIC', (2, 3))]",[],[],[],[]
model,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .","[('apply', (1, 2)), ('to', (3, 4))]","[('RELIC', (2, 3)), ('entity typing', (4, 6)), ('entity', (22, 23)), ('trivia question answering', (38, 41))]",[],[],[],[]
research-problem,RELIC accurately captures categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies .,"[('accurately captures', (1, 3)), ('encoded by', (5, 7)), ('in', (9, 10))]","[('RELIC', (0, 1)), ('categorical information', (3, 5)), ('human experts', (7, 9)), ('Freebase and Wikipedia category hierarchies', (11, 16))]",[],[],[],[]
model,We also show that given just a few exemplar entities of a given category such as Scottish footballers we can use RELIC to recover the remaining entities of that category with good precision .,"[('given', (4, 5)), ('of', (10, 11)), ('such as', (14, 16)), ('use', (20, 21)), ('to recover', (22, 24)), ('of', (27, 28)), ('with', (30, 31))]","[('just', (5, 6)), ('given category', (12, 14)), ('Scottish footballers', (16, 18)), ('RELIC', (21, 22)), ('remaining entities', (25, 27)), ('good precision', (31, 33))]",[],[],[],[]
research-problem,Using RELIC for entity linking can match state - of - the - art approaches that make use of non-local and non-linguistic information about entities .,"[('for', (2, 3)), ('can match', (5, 7)), ('that make use of', (15, 19))]","[('RELIC', (1, 2)), ('entity linking', (3, 5)), ('state - of - the - art approaches', (7, 15)), ('non-local and', (19, 21))]",[],[],[],[]
experiments,"RELIC learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned , and not the surface form of the mention itself .","[('learns', (1, 2)), ('of', (4, 5)), ('if', (7, 8)), ('trained to match', (10, 13)), ('not', (23, 24))]","[('RELIC', (0, 1)), ('better representations', (2, 4)), ('entity properties', (5, 7)), ('contexts', (15, 16)), ('entities', (18, 19)), ('surface form', (25, 27))]",[],[],[],[]
experiments,"We also apply RELIC 's context encoder and entity embeddings to the task of end - to - end trivia question answering , and we show that this approach can capture more than half of the answers identified by the best existing reading comprehension systems .","[('apply', (2, 3)), ('to', (10, 11)), ('of', (13, 14))]","[(""RELIC 's context encoder and entity embeddings"", (3, 10)), ('task', (12, 13)), ('end - to - end trivia question answering', (14, 22))]",[],[],[],[]
results,"RELIC outperforms prior work , even with only 5 % of the training data .","[('even with', (5, 7)), ('of', (10, 11))]","[('RELIC', (0, 1)), ('outperforms', (1, 2)), ('prior work', (2, 4)), ('only 5 %', (7, 10)), ('training data', (12, 14))]",[],[],[],[]
results,We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,"[('observe', (1, 2)), ('taken by', (10, 12)), ('taken by', (19, 21))]","[('retrieve - then - read approach', (4, 10)), ('ORQA', (12, 13)), ('outperforms', (13, 14)), ('direct answer retrieval approach', (15, 19)), ('RELIC', (21, 22))]",[],[],[],[]
results,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .","[('by', (11, 12))]","[('RELIC', (5, 6)), ('outperforms', (6, 7)), ('reading comprehension baseline', (8, 11)), ('20 points', (12, 14))]",[],[],[],[]
research-problem,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,[],[],[],[],[],[]
research-problem,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",[],"[('Named Entity Disambiguation ( NED )', (0, 6))]",[],[],[],[]
research-problem,"By combining contexts based on the proposed embedding with standard NED features , we achieved state - of - theart accuracy of 93.1 % on the standard CoNLL dataset and 85.2 % on the TAC 2010 dataset .","[('combining', (1, 2)), ('based on', (3, 5)), ('with', (8, 9)), ('achieved', (14, 15)), ('of', (21, 22)), ('on', (24, 25)), ('on', (32, 33))]","[('contexts', (2, 3)), ('proposed embedding', (6, 8)), ('standard NED features', (9, 12)), ('state - of - theart accuracy', (15, 21)), ('93.1 %', (22, 24)), ('standard CoNLL dataset', (26, 29)), ('85.2 %', (30, 32)), ('TAC 2010 dataset', (34, 37))]",[],[],[],[]
research-problem,"Named Entity Disambiguation ( NED ) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",[],"[('Named Entity Disambiguation ( NED )', (0, 6))]",[],[],[],[]
model,The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low - dimensional vector space .,"[('designed to capture', (3, 6)), ('of', (9, 10)), ('when', (11, 12)), ('placed near', (15, 17)), ('in', (19, 20))]","[('semantic similarity', (7, 9)), ('words', (10, 11)), ('similar words', (12, 14)), ('one another', (17, 19)), ('relatively low - dimensional vector space', (21, 27))]",[],[],[],[]
model,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .","[('propose', (5, 6)), ('to construct', (8, 10)), ('into', (19, 20))]","[('method', (7, 8)), ('novel embedding', (11, 13)), ('words and entities', (16, 19)), ('same continuous vector space', (21, 25))]",[],[],[],[]
model,Our model consists of the following three models based on the skip - gram model :,"[('consists of', (2, 4)), ('based on', (8, 10))]","[('skip - gram model', (11, 15))]",[],[],[],[]
model,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .","[('given', (14, 15)), ('in', (18, 19)), ('given', (34, 35)), ('in', (38, 39)), ('of', (42, 43)), ('using', (63, 64))]","[('conventional skip - gram model', (3, 8)), ('neighboring words', (12, 14)), ('target word', (16, 18)), ('text corpora', (19, 21)), ('KB graph model', (25, 28)), ('neighboring entities', (32, 34)), ('target entity', (36, 38)), ('link graph', (40, 42)), ('KB', (44, 45)), ('anchor context model', (50, 53)), ('anchors', (64, 65)), ('KB', (71, 72))]",[],[],[],[]
model,"By jointly optimizing these models , our method simultaneously learns the embedding of words and entities .","[('jointly optimizing', (1, 3)), ('simultaneously learns', (8, 10)), ('of', (12, 13))]","[('our method', (6, 8)), ('embedding', (11, 12)), ('words and entities', (13, 16))]",[],[],[],[]
model,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .","[('develop', (8, 9)), ('computes', (14, 15)), ('using', (17, 18))]","[('straightforward NED method', (10, 13)), ('two contexts', (15, 17)), ('proposed embedding', (19, 21)), ('textual context similarity', (22, 25)), ('coherence', (27, 28))]",[],[],[],[]
model,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .","[('combines', (3, 4)), ('with', (6, 7)), ('using', (16, 17))]","[('Our', (0, 1)), ('NED method', (1, 3)), ('contexts', (5, 6)), ('several standard features', (7, 10)), ('prior probability', (13, 15)), ('supervised machine learning', (17, 20))]",[],[],[],[]
research-problem,Skip- gram Model for Word Similarity,[],[],[],[],[],[]
hyperparameters,We use stochastic gradient descent ( SGD ) for the optimization .,"[('use', (1, 2)), ('for', (8, 9))]","[('stochastic gradient descent ( SGD )', (2, 8)), ('optimization', (10, 11))]",[],[],[],[]
experiments,"Following , we also used learning rate ? = 0.025 which linearly decreased with the iterations of the Wikipedia dump .","[('used', (4, 5)), ('of', (16, 17))]","[('learning rate ? = 0.025', (5, 10)), ('linearly decreased', (11, 13)), ('iterations', (15, 16)), ('Wikipedia dump', (18, 20))]",[],[],[],[]
results,"Surprisingly , we attained results comparable with those of some state - of - the - art methods on the both datasets by only using base features .","[('attained', (3, 4)), ('comparable with', (5, 7)), ('by', (22, 23))]","[('results', (4, 5)), ('some state - of - the - art methods', (9, 18)), ('base features', (25, 27))]",[],[],[],[]
results,Adding string similarity features slightly further improved performance .,"[('Adding', (0, 1))]","[('string similarity features', (1, 4)), ('slightly further improved performance', (4, 8))]",[],[],[],[]
results,Our method outperformed some state - of - the - art methods without using coherence .,"[('without using', (12, 14))]","[('Our method', (0, 2)), ('outperformed', (2, 3)), ('some state - of - the - art methods', (3, 12)), ('coherence', (14, 15))]",[],[],[],[]
research-problem,"3D Face Morphable Models "" In - the - Wild """,[],"[('3D Face Morphable Models', (0, 4))]",[],[],[],[]
research-problem,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,[],"[('3 D facial shape estimation from single images', (0, 8))]",[],[],[],[]
model,The method requires the construction of a 3 DMM which is a statistical model of facial texture and shape in a space where there are explicit correspondences .,"[('requires', (2, 3)), ('of', (5, 6)), ('is', (10, 11)), ('of', (14, 15)), ('in', (19, 20)), ('where there are', (22, 25))]","[('construction', (4, 5)), ('3 DMM', (7, 9)), ('statistical model', (12, 14)), ('facial texture and shape', (15, 19)), ('space', (21, 22)), ('explicit correspondences', (25, 27))]",[],[],[],[]
research-problem,"3 D facial shape recovery from a single image under "" inthe - wild "" conditions is still an open and challenging problem in computer vision mainly due to the fact that :",[],"[('3 D facial shape recovery from a single image', (0, 9))]",[],[],[],[]
research-problem,The general problem of extracting the 3D facial shape from a single image is an ill - posed problem which is notoriously difficult to be solved without the use of any statistical priors for the shape and texture of faces .,[],"[('extracting the 3D facial shape', (4, 9)), ('single image', (11, 13))]",[],[],[],[]
research-problem,"Learning statistical priors of the 3D facial shape and texture for "" in - the - wild "" images is currently very difficult by using modern acquisition devices .",[],"[('Learning statistical priors of', (0, 4)), ('3D facial shape and texture for "" in - the - wild "" images', (5, 19))]",[],[],[],[]
model,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .","[('from', (10, 11)), ('with', (25, 26))]","[('learning', (5, 6)), ('statistical texture model', (7, 10)), ('"" in - the -wild "" facial images', (11, 19)), ('in', (22, 23)), ('full correspondence', (23, 25)), ('statistical shape', (27, 29)), ('identity and expression variations', (33, 37))]",[],[],[],[]
model,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .","[('for', (30, 31))]","[('featurebased texture models', (27, 30)), ('3 DMMs', (31, 33))]",[],[],[],[]
model,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .","[('show', (1, 2)), ('of using', (5, 7)), ('is', (19, 20)), ('gets', (24, 25))]","[('advantage', (4, 5)), ('"" in - the -wild "" feature - based texture model', (8, 19)), ('fitting strategy', (22, 24)), ('simplified', (25, 26))]",[],[],[],[]
model,"By capitalising on the recent advancements in fitting statistical deformable models , we propose a novel and fast algorithm for fitting "" in - the -wild "" 3 DMMs .","[('propose', (13, 14)), ('for fitting', (19, 21))]","[('fitting statistical', (7, 9)), ('novel and fast algorithm', (15, 19)), ('"" in - the -wild "" 3 DMMs', (21, 29))]",[],[],[],[]
hyperparameters,Gauss - Newton Optimization,[],[],[],[],[],[]
hyperparameters,"To train our model , which we label as ITW , we use a variant of the Basel Face Model ( BFM ) that we trained to contain both identities drawn from the original BFM model along with expressions provided by .","[('train', (1, 2)), ('label as', (7, 9)), ('use', (12, 13)), ('of', (15, 16)), ('trained to contain', (25, 28)), ('drawn from', (30, 32))]","[('our model', (2, 4)), ('ITW', (9, 10)), ('variant', (14, 15)), ('both identities', (28, 30)), ('original BFM model', (33, 36))]",[],[],[],[]
experiments,3D Shape Recovery,[],[],[],[],[],[]
experiments,"Herein , we evaluate our "" in - the -wild "" 3 DMM ( ITW ) in terms of 3D shape estimation accuracy against two popular state - of - the - art alternative 3 DMM formulations .","[('evaluate', (3, 4)), ('in terms of', (16, 19)), ('against', (23, 24))]","[('"" in - the -wild "" 3 DMM ( ITW )', (5, 16)), ('3D shape estimation accuracy', (19, 23)), ('two popular state - of - the - art alternative 3 DMM formulations', (24, 37))]",[],[],[],[]
baselines,The first one is a classic 3 DMM with the original Basel laboratory texture model and full lighting equation which we term Classic .,"[('with', (8, 9)), ('term', (21, 22))]","[('classic 3 DMM', (5, 8)), ('original Basel laboratory texture model', (10, 15)), ('full lighting equation', (16, 19)), ('Classic', (22, 23))]",[],[],[],[]
baselines,The second is the texture - less linear model proposed in which we refer to as Linear .,"[('refer to', (13, 15))]","[('texture - less linear model', (4, 9)), ('Linear', (16, 17))]",[],[],[],[]
hyperparameters,"The mean mesh of each model under testis landmarked with the same 49 point markup used in the dataset , and is registered against the ground truth mesh by performing a Procrustes alignment using the sparse annotations followed by Non-Rigid Iterative Closest Point ( N - ICP ) to iteratively deform the two surfaces until they are brought into correspondence .","[('of', (3, 4)), ('under', (6, 7)), ('with', (9, 10)), ('registered against', (22, 24)), ('by performing', (28, 30)), ('using', (33, 34)), ('followed by', (37, 39)), ('to iteratively deform', (48, 51)), ('until', (54, 55)), ('brought into', (57, 59))]","[('mean mesh', (1, 3)), ('each model', (4, 6)), ('testis landmarked', (7, 9)), ('same 49 point markup', (11, 15)), ('ground truth mesh', (25, 28)), ('Procrustes alignment', (31, 33)), ('sparse annotations', (35, 37)), ('Non-Rigid Iterative Closest Point ( N - ICP )', (39, 48)), ('two surfaces', (52, 54)), ('correspondence', (59, 60))]",[],[],[],[]
results,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .","[('does', (6, 7)), ('most able to recover', (14, 18)), ('due to', (21, 23)), ('for', (27, 28))]","[('texture - free Linear model', (1, 6)), ('better', (7, 8)), ('ITW model', (11, 13)), ('facial shapes', (19, 21)), ('ideal feature basis', (24, 27)), ('"" in - the -wild "" conditions', (29, 36))]",[],[],[],[]
experiments,ITW slightly outperforms IMM even though both IMM and PS - NL use all four available images of each subject .,[],"[('ITW', (0, 1)), ('slightly outperforms', (1, 3)), ('IMM', (3, 4))]",[],[],[],[]
research-problem,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,[],"[('Large Pose 3D Face Reconstruction', (0, 5))]",[],[],[],[]
research-problem,"Our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry ( including the non-visible parts of the face ) bypassing the construction ( during training ) and fitting ( during testing ) of a 3D Morphable Model .","[('works with', (2, 4)), ('does not require', (12, 15)), ('works for', (24, 26)), ('including', (45, 46)), ('bypassing', (53, 54)), ('of', (66, 67))]","[('Our CNN', (0, 2)), ('just', (4, 5)), ('accurate', (15, 16)), ('arbitrary facial poses and expressions', (26, 31)), ('reconstruct', (37, 38)), ('whole 3 D facial geometry', (39, 44)), ('non-visible parts of', (47, 50)), ('construction', (55, 56)), ('fitting', (61, 62)), ('3D Morphable Model', (68, 71))]",[],[],[],[]
research-problem,3 D face reconstruction is the problem of recovering the 3D facial geometry from 2D images .,[],"[('3 D face reconstruction', (0, 4))]",[],[],[],[]
research-problem,This work is on 3D face reconstruction using only a single image .,"[('on', (3, 4)), ('using', (7, 8))]","[('3D face reconstruction', (4, 7))]",[],[],[],[]
model,"In this paper , we propose to approach it , for the first time to the best of our knowledge , by directly learning a mapping from pixels to 3D coordinates using a Convolutional Neural Network ( CNN ) .","[('by directly learning', (21, 24)), ('from', (26, 27)), ('using', (31, 32))]","[('mapping', (25, 26)), ('pixels to 3D coordinates', (27, 31)), ('Convolutional Neural Network ( CNN )', (33, 39))]",[],[],[],[]
model,"Besides its simplicity , our approach works with totally unconstrained images downloaded from the web , including facial images of arbitrary poses , facial expressions and occlusions , as shown in .","[('works with', (6, 8)), ('downloaded from', (11, 13)), ('including', (16, 17)), ('of', (19, 20))]","[('totally unconstrained images', (8, 11)), ('web', (14, 15)), ('facial images', (17, 19)), ('arbitrary poses', (20, 22)), ('facial expressions', (23, 25)), ('occlusions', (26, 27))]",[],[],[],[]
hyperparameters,"Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .","[('trained', (5, 6)), ('using', (11, 12)), ('with', (13, 14)), ('of', (18, 19)), ('lowered after', (25, 27)), ('to', (29, 30))]","[('Each of our architectures', (0, 4)), ('end - to - end', (6, 11)), ('RMSProp', (12, 13)), ('initial learning rate', (15, 18)), ('10 ? 4', (19, 22)), ('40 epochs', (27, 29)), ('10 ?5', (30, 32))]",[],[],[],[]
hyperparameters,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .","[('During', (0, 1)), ('applied to', (6, 8)), ('applied', (25, 26))]","[('training', (1, 2)), ('random augmentation', (3, 5)), ('each input sample ( face image )', (8, 15)), ('in - plane rotation r ? [ ?45 , ... , 45 ]', (26, 39)), ('translation t z', (40, 43))]",[],[],[],[]
results,"3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .","[('on', (3, 4))]","[('3DDFA and EOS', (0, 3))]",[],[],[],[]
results,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .","[('perform', (4, 5)), ('across', (6, 7)), ('of', (10, 11))]","[('All VRNs', (2, 4)), ('well', (5, 6)), ('whole spectrum', (8, 10)), ('facial poses , expressions and occlusions', (11, 17))]",[],[],[],[]
results,"Also , there are no significant performance discrepancies across different datasets ( ALFW2000 - 3D seems to be slightly more difficult ) .","[('there are', (2, 4)), ('across', (8, 9))]","[('no', (4, 5)), ('significant performance discrepancies', (5, 8)), ('different datasets', (9, 11))]",[],[],[],[]
baselines,VRN - Guided uses another stacked hourglass network for landmark localization .,"[('uses', (3, 4)), ('for', (8, 9))]","[('VRN - Guided', (0, 3)), ('another stacked hourglass network', (4, 8)), ('landmark localization', (9, 11))]",[],[],[],[]
results,"4 . VRN - Multitask does not always perform particularly better than the plain VRN ( in fact on BU - 4 DFE it performs worse ) , not justifying the increase of network complexity .","[('does not always', (5, 8)), ('than', (11, 12))]","[('VRN - Multitask', (2, 5)), ('particularly better', (9, 11)), ('plain VRN', (13, 15))]",[],[],[],[]
ablation-analysis,"For all experiments reported , we used the best performing VRN - Guided .","[('used', (6, 7))]","[('best performing', (8, 10)), ('VRN - Guided', (10, 13))]",[],[],[],[]
ablation-analysis,"The performance of the 3D reconstruction dropped by a negligible amount , suggesting that as long as the Gaussians are of a sensible size , guidance will always help .","[('of', (2, 3))]","[('performance', (1, 2)), ('3D reconstruction', (4, 6)), ('dropped', (6, 7)), ('negligible amount', (9, 11))]",[],[],[],[]
research-problem,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,[],"[('Robust Facial Landmark Localisation', (3, 7))]",[],[],[],[]
research-problem,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .",[],"[('Facial landmark localisation', (0, 3)), ('face alignment', (5, 7))]",[],[],[],[]
research-problem,"The existing challenge is to achieve robust and accurate landmark localisation of unconstrained faces that are impacted by a variety of appearance variations , e.g. in pose , expression , illumination , image blurring and occlusion .","[('of', (11, 12)), ('impacted by', (16, 18)), ('e.g. in', (24, 26))]","[('robust and accurate landmark localisation', (6, 11)), ('unconstrained faces', (12, 14)), ('variety of', (19, 21)), ('appearance variations', (21, 23)), ('pose', (26, 27)), ('expression', (28, 29)), ('illumination', (30, 31)), ('image blurring', (32, 34)), ('occlusion', (35, 36))]",[],[],[],[]
model,"To further address the issue , we propose a new loss function , namely Wing loss ( ) , for robust facial landmark localisation .","[('propose', (7, 8)), ('namely', (13, 14)), ('for', (19, 20))]","[('new loss function', (9, 12)), ('Wing loss ( )', (14, 18)), ('robust facial landmark localisation', (20, 24))]",[],[],[],[]
model,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .","[('namely', (5, 6)), ('designed to improve', (12, 15)), ('for', (21, 22))]","[('novel loss function', (1, 4)), ('Wing loss', (7, 9)), ('deep neural network training capability', (16, 21)), ('small and medium range errors', (22, 27))]",[],[],[],[]
experimental-setup,"In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .","[('used', (5, 6))]","[('Matlab 2017a', (6, 8))]",[],[],[],[]
experimental-setup,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .","[('of', (4, 5)), ('conducted on', (8, 10)), ('running', (12, 13)), ('with', (15, 16))]","[('server', (11, 12)), ('Ubuntu 16.04', (13, 15)), ('2 Intel Xeon E5-2667 v4 CPU', (16, 22)), ('256 GB RAM', (23, 26)), ('4 NVIDIA GeForce GTX Titan X ( Pascal ) cards', (27, 37))]",[],[],[],[]
experimental-setup,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .","[('set', (1, 2)), ('to', (5, 6)), ('for', (19, 20))]","[('weight decay', (3, 5)), ('5 10 ? 4', (6, 10)), ('momentum', (11, 12)), ('0.9', (13, 14)), ('batch size', (15, 17)), ('8', (18, 19)), ('network training', (20, 22))]",[],[],[],[]
experimental-setup,Each model was trained for 120 k iterations .,"[('trained for', (3, 5))]","[('120 k iterations', (5, 8))]",[],[],[],[]
experimental-setup,"The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .","[('used for', (5, 7)), ('with', (13, 14)), ('used to', (19, 21))]","[('standard ReLu function', (1, 4)), ('nonlinear activation', (7, 9)), ('Max pooling', (11, 13)), ('stride of 2', (15, 18)), ('downsize', (21, 22)), ('feature maps', (22, 24))]",[],[],[],[]
experimental-setup,"For the convolutional layer , we used 3 3 kernels with the stride of 1 .","[('For', (0, 1)), ('used', (6, 7)), ('with', (10, 11))]","[('convolutional layer', (2, 4)), ('3 3 kernels', (7, 10)), ('stride of 1', (12, 15))]",[],[],[],[]
experimental-setup,"For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .","[('set to', (12, 14)), ('for', (15, 16))]","[('PDB strategy', (3, 5)), ('number of bins K', (7, 11)), ('17', (14, 15)), ('AFLW', (16, 17)), ('9', (18, 19)), ('300W', (20, 21))]",[],[],[],[]
experimental-setup,"For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .","[('For', (0, 1)), ('is', (9, 10)), ('reduced', (15, 16)), ('from', (19, 20)), ('to', (24, 25)), ('for', (28, 29))]","[('CNN - 6', (1, 4)), ('input image size', (6, 9)), ('64 64 3', (10, 13)), ('learning rate', (17, 19)), ('3 10 ? 6', (20, 24)), ('3 10 ?8', (25, 28)), ('L2 loss', (30, 32)), ('3 10 ?5', (35, 38)), ('other loss functions', (45, 48))]",[],[],[],[]
experimental-setup,"The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .","[('of', (2, 3)), ('set tow', (7, 9)), ('For', (15, 16)), ('is', (24, 25)), ('reduced', (30, 31)), ('from', (34, 35)), ('to', (39, 40)), ('for', (43, 44)), ('from', (49, 50))]","[('parameters', (1, 2)), ('Wing loss', (4, 6)), ('CNN - 7', (16, 19)), ('input image size', (21, 24)), ('128 128', (25, 27)), ('learning rate', (32, 34)), ('1 10 ? 6', (35, 39)), ('1 10 ?8', (40, 43)), ('L2 loss', (45, 47)), ('1 10 ? 5 to 1 10 ? 7', (50, 59)), ('other loss functions', (61, 64))]",[],[],[],[]
experimental-setup,The parameters of the Wing loss were set tow = 15 and = 3 .,"[('of', (2, 3)), ('set tow', (7, 9))]","[('parameters', (1, 2)), ('Wing loss', (4, 6))]",[],[],[],[]
experimental-setup,"To perform data augmentation , we randomly rotated each training image between [ ? 30 , 30 ] degrees for CNN - 6 and between [ ? 10 , 10 ] degrees for CNN - 7 .","[('perform', (1, 2)), ('randomly rotated', (6, 8)), ('between', (11, 12)), ('for', (19, 20)), ('between', (24, 25)), ('for', (32, 33))]","[('data augmentation', (2, 4)), ('each training image', (8, 11)), ('[ ? 30 , 30 ] degrees', (12, 19)), ('CNN - 6', (20, 23)), ('[ ? 10 , 10 ] degrees', (25, 32)), ('CNN - 7', (33, 36))]",[],[],[],[]
experimental-setup,"In addition , we randomly flipped each training image with the probability of 50 % .","[('randomly flipped', (4, 6)), ('with', (9, 10))]","[('each training image', (6, 9)), ('probability of', (11, 13)), ('50 %', (13, 15))]",[],[],[],[]
experimental-setup,"For bounding box perturbation , we applied random translations to the upper-left and bottom - right corners of the face bounding box within 5 % of the bounding .","[('For', (0, 1)), ('applied', (6, 7)), ('to', (9, 10)), ('of', (17, 18)), ('within', (22, 23)), ('of', (25, 26))]","[('bounding box perturbation', (1, 4)), ('random translations', (7, 9)), ('upper-left and bottom - right corners', (11, 17)), ('face bounding box', (19, 22)), ('5 %', (23, 25)), ('bounding', (27, 28))]",[],[],[],[]
experimental-setup,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .","[('randomly injected', (3, 5)), ('to', (11, 12)), ('with', (15, 16))]","[('Gaussian blur (? = 1 )', (5, 11)), ('each training image', (12, 15)), ('probability', (17, 18)), ('50 %', (19, 21))]",[],[],[],[]
results,Comparison with state of the art 7.2.1 AFLW,[],"[('Comparison with state of the art', (0, 6))]",[],[],[],[]
experimental-setup,"In our experiments , we used our two - stage facial landmark localisation framework by stacking the CNN - 6 and CNN - 7 networks ( denoted by CNN - 6 / 7 ) , as introduced in Section 6 .","[('used', (5, 6)), ('by stacking', (14, 16))]","[('our two - stage facial landmark localisation framework', (6, 14)), ('CNN - 6 and CNN - 7 networks', (17, 25))]",[],[],[],[]
experimental-setup,"In addition , the proposed Pose - based Data Balancing ( PDB ) strategy was adopted , as presented in Section 5 .",[],"[('proposed Pose - based Data Balancing ( PDB ) strategy', (4, 14))]",[],[],[],[]
results,"As shown in , our CNN - 6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function ( magenta solid line ) .","[('trained with', (16, 18))]","[('our CNN - 6/7 network', (4, 9)), ('outperforms', (9, 10)), ('all the other approaches', (10, 14)), ('commonly used L2 loss function ( magenta solid line )', (19, 29))]",[],[],[],[]
experiments,"Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .","[('switching', (4, 5)), ('from', (8, 9)), ('of', (18, 19))]","[('loss function', (6, 8)), ('L2 to L1', (9, 12)), ('smooth L1', (13, 15)), ('performance', (17, 18)), ('our method', (19, 21)), ('improved', (23, 24)), ('significantly', (24, 25)), ('red solid and black dashed lines', (26, 32))]",[],[],[],[]
results,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .","[('use of', (3, 5)), ('further improves', (11, 13))]","[('our newly proposed Wing loss function', (5, 11)), ('accuracy', (14, 15))]",[],[],[],[]
experimental-setup,The face images involved in 300W have been semi-automatically annotated by 68 facial landmarks .,"[('involved in', (3, 5))]","[('face images', (1, 3)), ('300W', (5, 6)), ('semi-automatically', (8, 9)), ('68 facial landmarks', (11, 14))]",[],[],[],[]
experimental-setup,The final size of the test set is 689 .,"[('of', (3, 4)), ('is', (7, 8))]","[('final size', (1, 3)), ('test set', (5, 7)), ('689', (8, 9))]",[],[],[],[]
results,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .","[('with', (11, 12)), ('on', (32, 33))]","[('our two - stage landmark localisation framework', (4, 11)), ('PDB strategy', (13, 15)), ('Wing loss function', (19, 22)), ('outperforms', (22, 23)), ('all the other stateof - the - art algorithms', (23, 32)), ('300 W dataset inaccuracy', (34, 38))]",[],[],[],[]
results,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,"[('reduced by', (4, 6)), ('compared to', (10, 12)), ('reported by', (16, 18))]","[('error', (1, 2)), ('almost 20 %', (6, 9)), ('current best result', (13, 16)), ('RAR algorithm', (19, 21))]",[],[],[],[]
experiments,Run time and network architectures,[],[],[],[],[],[]
experiments,"Facial landmark localisation has been widely used in many real - time practical applications , hence the speed together with accuracy of an algorithm is crucial for the deployment of the algorithm in commercial use cases .",[],"[('Facial landmark localisation', (0, 3))]",[],[],[],[]
experimental-setup,The input for ResNet is a 224 224 3 colour image .,"[('input for', (1, 3)), ('is', (4, 5))]","[('ResNet', (3, 4)), ('224 224 3 colour image', (6, 11))]",[],[],[],[]
results,"For both AFLW and 300 W , by replacing the CNN - 6/7 network with ResNet - 50 , the performance has been further improved by around 10 % , as shown in .","[('For', (0, 1)), ('replacing', (8, 9)), ('with', (14, 15)), ('by', (25, 26))]","[('AFLW and 300 W', (2, 6)), ('CNN - 6/7 network', (10, 14)), ('ResNet - 50', (15, 18)), ('performance', (20, 21)), ('further improved', (23, 25)), ('around 10 %', (26, 29))]",[],[],[],[]
experiments,The speed of TR - DRN is 83 fps on an NVIDIA GeForce GTX Titan X card .,"[('of', (2, 3)), ('is', (6, 7)), ('on', (9, 10))]","[('speed', (1, 2)), ('TR - DRN', (3, 6)), ('83 fps', (7, 9)), ('NVIDIA GeForce GTX Titan X card', (11, 17))]",[],[],[],[]
results,"It should be noted that our CNN - 6 / 7 still outperforms the state - of - the - art approaches by a significant margin while running at 170 fps on a GPU card , as shown in .","[('noted', (3, 4)), ('by', (22, 23)), ('on', (31, 32))]","[('our CNN - 6 / 7', (5, 11)), ('outperforms', (12, 13)), ('state - of - the - art approaches', (14, 22)), ('significant margin', (24, 26)), ('running', (27, 28)), ('170 fps', (29, 31)), ('GPU card', (33, 35))]",[],[],[],[]
research-problem,Unsupervised Training for 3D Morphable Model Regression,[],[],[],[],[],[]
research-problem,"Finding the coordinates of a person in this space from a single image of that person is a common task for applications such as 3D avatar creation , facial animation transfer , and video editing ( e.g. ) .",[],"[('Finding the coordinates', (0, 3))]",[],[],[],[]
model,We map features from a facial recognition network into identity parameters for the Basel 2017 Morphable Face Model .,"[('map', (1, 2)), ('from', (3, 4)), ('into', (8, 9))]","[('facial recognition network', (5, 8)), ('identity parameters', (9, 11)), ('Basel 2017 Morphable Face Model', (13, 18))]",[],[],[],[]
model,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,"[('removes', (11, 12)), ('reliance on', (21, 23)), ('to reproduce', (25, 27))]","[('training', (6, 7)), ('regression network', (8, 10)), ('supervised training data', (16, 19)), ('inverse rendering', (23, 25)), ('image pixels', (27, 29))]",[],[],[],[]
model,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .","[('learns to', (4, 6)), ('based on', (9, 11)), ('produced by', (15, 17)), ('such as', (21, 23))]","[('minimize', (6, 7)), ('loss', (8, 9)), ('facial identity features', (12, 15)), ('face recognition network', (18, 21)), ('VGG - Face', (23, 26)), (""Google 's FaceNet"", (27, 30))]",[],[],[],[]
model,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,"[('apply', (5, 6)), ('that matches', (8, 10)), ('between', (13, 14)), ('of', (21, 22))]","[('loss', (7, 8)), ('identity features', (11, 13)), ('input photograph', (15, 17)), ('synthetic rendering', (19, 21)), ('predicted face', (23, 25))]",[],[],[],[]
model,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .","[('alleviate', (1, 2)), ('by', (5, 6)), ('applying', (6, 7)), ('to match', (15, 17)), ('of', (19, 20)), ('to', (23, 24)), ('of', (26, 27)), ('to ensure', (34, 36)), ('can correctly reinterpret', (39, 42)), ('that combines', (51, 53)), ('from', (54, 55)), ('of', (59, 60))]","[('fooling problem', (3, 5)), ('three novel losses', (7, 10)), ('batch distribution loss', (12, 15)), ('statistics', (18, 19)), ('each training batch', (20, 23)), ('statistics', (25, 26)), ('morphable model', (28, 30)), ('loopback loss', (32, 34)), ('regression network', (37, 39)), ('own output', (43, 45)), ('multi-view identity loss', (48, 51)), ('features', (53, 54)), ('multiple , independent views', (55, 59)), ('predicted shape', (61, 63))]",[],[],[],[]
model,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .","[('train', (5, 6)), ('using', (13, 14))]","[('3D shape and texture regression network', (7, 13)), ('only a face recognition network', (14, 19)), ('morphable face model', (21, 24)), ('dataset of unlabeled face images', (27, 32))]",[],[],[],[]
hyperparameters,We use the Phong reflection model for shading .,"[('use', (1, 2)), ('for', (6, 7))]","[('Phong reflection model', (3, 6)), ('shading', (7, 8))]",[],[],[],[]
results,Identity prediction can be further enhanced by using multiple poses for each face .,"[('by using', (6, 8)), ('for', (10, 11))]","[('Identity prediction', (0, 2)), ('further enhanced', (4, 6)), ('multiple poses', (8, 10)), ('each face', (11, 13))]",[],[],[],[]
hyperparameters,Batch Distribution,[],[],[],[],[],[]
results,"Our method shows improved likeness and color fidelity over competing methods , especially in the shape of the eyes , eyebrows , and nose .","[('shows', (2, 3)), ('over', (8, 9)), ('especially in the shape of', (12, 17))]","[('improved likeness and color fidelity', (3, 8)), ('competing methods', (9, 11)), ('eyes', (18, 19))]",[],[],[],[]
results,Neutral Pose Reconstruction on MICC,[],[],[],[],[],[]
results,We quantitatively evaluate the ground - truth accuracy of our models on the MICC Florence 3D Faces dataset ( MICC ) in .,"[('on', (11, 12))]","[('MICC Florence 3D Faces dataset ( MICC )', (13, 21))]",[],[],[],[]
experiments,We instead average our encoder embeddings before making a single reconstruction .,"[('average', (2, 3)), ('before making', (6, 8))]","[('single reconstruction', (9, 11))]",[],[],[],[]
results,"Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .","[('indicate', (2, 3)), ('improved', (6, 7)), ('to', (9, 10)), ('by', (13, 14)), ('with less', (30, 32))]","[('absolute error', (7, 9)), ('ground truth', (11, 13)), ('20 - 25 %', (14, 18)), ('more consistent', (23, 25))]",[],[],[],[]
results,"We are also more stable across changing environments , with similar results for all three test sets .","[('across', (5, 6))]","[('more stable', (3, 5)), ('changing environments', (6, 8))]",[],[],[],[]
results,Face Recognition Results,[],[],[],[],[],[]
results,Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .,"[('achieves', (2, 3)), ('between', (6, 7)), ('of', (10, 11)), ('on', (12, 13))]","[('Our method', (0, 2)), ('average similarity', (4, 6)), ('rendering and photo', (7, 10)), ('0.403', (11, 12)), ('MoFA test', (13, 15))]",[],[],[],[]
results,"Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .","[('closer to', (5, 7)), ('than', (12, 13)), ('closer to', (27, 29))]","[(""Our method 's results"", (0, 4)), ('same - person distribution', (8, 12)), ('differentperson distribution', (14, 16)), ('all cases', (17, 19)), ('other methods results', (22, 25)), ('different - person distribution', (30, 34))]",[],[],[],[]
results,"Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .","[('between', (4, 5)), ('is', (15, 16)), ('with', (19, 20))]","[('distance', (3, 4)), ('GT distribution and the same - person LFW distribution', (6, 15)), ('very low', (16, 18)), ('almost the same mean ( 0.51 vs 0.50 )', (20, 29))]",[],[],[],[]
research-problem,Facial features smoothly degrade as the necessary information is no longer present in the input image .,"[('as', (4, 5))]","[('Facial features', (0, 2)), ('smoothly degrade', (2, 4)), ('necessary information', (6, 8)), ('no longer present', (9, 12)), ('input image', (14, 16))]",[],[],[],[]
research-problem,Face alignment is a classic problem in the computer vision field .,[],"[('Face alignment', (0, 2))]",[],[],[],[]
research-problem,"In this paper , for the first time , we aim at providing a very dense 3D alignment for largepose face images .","[('for', (18, 19))]","[('very dense 3D alignment', (14, 18)), ('largepose face images', (19, 22))]",[],[],[],[]
approach,"Moreover , DeFA should offer dense correspondence not only between two face images , but also between the face image and the canonical 3 D face model .","[('offer', (4, 5)), ('between', (9, 10)), ('between', (16, 17))]","[('DeFA', (2, 3)), ('dense correspondence', (5, 7)), ('two face images', (10, 13)), ('face image and', (18, 21)), ('canonical 3 D face model', (22, 27))]",[],[],[],[]
approach,"In this work , we choose to develop the idea of fitting a dense 3 D face model to an image , where the model with thousands of vertexes makes it possible for face alignment to go very "" dense "" .","[('develop', (7, 8)), ('of', (10, 11)), ('to', (18, 19)), ('where', (22, 23)), ('makes', (29, 30)), ('to go', (35, 37))]","[('idea', (9, 10)), ('fitting', (11, 12)), ('image', (20, 21)), ('model with thousands of vertexes', (24, 29)), ('face alignment', (33, 35)), ('very "" dense ""', (37, 41))]",[],[],[],[]
approach,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .","[('learn', (9, 10)), ('to fit', (12, 14)), ('to', (19, 20))]","[('CNN', (11, 12)), ('3 D face model', (15, 19)), ('face image', (21, 23))]",[],[],[],[]
approach,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .","[('of', (4, 5)), ('propose', (10, 11)), ('employ', (12, 13))]","[('first challenge', (2, 4)), ('limited landmark labeling', (5, 8)), ('additional constraints', (13, 15))]",[],[],[],[]
approach,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .","[('include', (1, 2)), ('where', (4, 5)), ('of', (7, 8)), ('detected on', (28, 30)), ('of', (33, 34)), ('map to', (38, 40)), ('on', (43, 44))]","[('contour constraint', (2, 4)), ('contour', (6, 7)), ('predicted shape', (9, 11)), ('detected', (14, 15)), ('2 D face boundary', (15, 19)), ('SIFT constraint', (21, 23)), ('SIFT key points', (25, 28)), ('two face images', (30, 33)), ('same individual', (35, 37)), ('same vertexes', (41, 43)), ('3D face model', (45, 48))]",[],[],[],[]
approach,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .","[('integrated into', (3, 5)), ('as', (8, 9)), ('where', (14, 15)), ('results in', (22, 24)), ('for', (27, 28))]","[('Both constraints', (0, 2)), ('CNN training', (6, 8)), ('additional loss function terms', (9, 13)), ('end - to - end training', (16, 22)), ('enhanced CNN', (25, 27)), ('3 D face model fitting', (28, 33))]",[],[],[],[]
approach,"Generally , our main contributions can be summarized as : 1 . We identify and define anew problem of dense face alignment , which seeks alignment of face - region pixels beyond the sparse set of landmarks .","[('identify and define', (13, 16)), ('of', (18, 19)), ('which seeks', (23, 25)), ('of', (26, 27)), ('beyond', (31, 32))]","[('dense face alignment', (19, 22)), ('alignment', (25, 26)), ('face - region pixels', (27, 31)), ('sparse set of landmarks', (33, 37))]",[],[],[],[]
approach,"To achieve dense face alignment , we develop a novel 3 D face model fitting algorithm that adopts multiple constraints and leverages multiple datasets .","[('To achieve', (0, 2)), ('develop', (7, 8)), ('adopts', (17, 18)), ('leverages', (21, 22))]","[('dense face alignment', (2, 5)), ('novel 3 D face model fitting algorithm', (9, 16)), ('multiple constraints', (18, 20)), ('multiple datasets', (22, 24))]",[],[],[],[]
hyperparameters,"At stage 1 , we use 300W - LP to train our DeFA network with parameter constraint ( PL ) .","[('use', (5, 6)), ('to train', (9, 11)), ('with', (14, 15))]","[('300W - LP', (6, 9)), ('our DeFA network', (11, 14)), ('parameter constraint ( PL )', (15, 20))]",[],[],[],[]
hyperparameters,"At stage 2 , we additionally include samples from the Caltech10K , and COFW to continue the training of our network with the additional landmark fitting constraint ( LFC ) .","[('include', (6, 7)), ('from', (8, 9)), ('to continue', (14, 16)), ('of', (18, 19)), ('with', (21, 22))]","[('samples', (7, 8)), ('training', (17, 18)), ('our network', (19, 21)), ('additional landmark fitting constraint ( LFC )', (23, 30))]",[],[],[],[]
baselines,"At stage 3 , we fine - tune the model with SPC and CFC constraints .","[('fine - tune', (5, 8)), ('with', (10, 11))]","[('model', (9, 10)), ('SPC and CFC constraints', (11, 15))]",[],[],[],[]
experiments,"For large - pose face alignment , we fine - tune the model with AFLW - LFPA training set .","[('For', (0, 1)), ('fine - tune', (8, 11)), ('with', (13, 14))]","[('large - pose face alignment', (1, 6)), ('model', (12, 13)), ('AFLW - LFPA training set', (14, 19))]",[],[],[],[]
hyperparameters,"For near - frontal face alignment , we fine - tune the model with 300 W training set .","[('For', (0, 1)), ('fine - tune', (8, 11)), ('with', (13, 14))]","[('near - frontal face alignment', (1, 6)), ('model', (12, 13)), ('300 W training set', (14, 18))]",[],[],[],[]
hyperparameters,"All samples at the third stage are augmented 20 times with up to 20 random in - plain rotation and 15 % random noise on the center , width , and length of the initial bounding box .","[('at', (2, 3)), ('augmented', (7, 8)), ('with', (10, 11)), ('on', (24, 25))]","[('samples', (1, 2)), ('third stage', (4, 6)), ('20 times', (8, 10)), ('up to 20 random in - plain rotation', (11, 19)), ('15 % random noise', (20, 24)), ('center , width , and length of', (26, 33))]",[],[],[],[]
experiments,"To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .","[('use', (6, 7)), ('for', (14, 15))]","[('network', (3, 4)), ('20 , 10 , and 10 epochs', (7, 14)), ('stage 1 to 3', (15, 19))]",[],[],[],[]
hyperparameters,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .","[('set', (1, 2)), ('as', (7, 8)), ('reduce', (14, 15)), ('by', (18, 19)), ('when', (23, 24)), ('approaches', (27, 28))]","[('initial global learning rate', (3, 7)), ('1 e ? 3', (8, 12)), ('learning rate', (16, 18)), ('factor of 10', (20, 23)), ('training error', (25, 27)), ('plateau', (29, 30))]",[],[],[],[]
experiments,"The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .","[('is', (3, 4)), ('is', (8, 9)), ('for', (15, 16)), ('is', (18, 19))]","[('minibatch size', (1, 3)), ('32', (4, 5)), ('weight decay', (6, 8)), ('0.005', (9, 10)), ('leak factor', (13, 15)), ('Leaky ReLU', (16, 18)), ('0.01', (19, 20))]",[],[],[],[]
hyperparameters,"In stage 2 , the regularization weights ?",[],"[('regularization weights', (5, 7))]",[],[],[],[]
hyperparameters,"lm for LFC is 5 ; In stage 3 , the regularization weights ? lm , ? s , ?","[('for', (1, 2)), ('is', (3, 4))]","[('lm', (0, 1)), ('LFC', (2, 3)), ('5', (4, 5))]",[],[],[],[]
hyperparameters,"c for LFC , SPC and CFC are set as 5 , 1 and 1 , respectively .","[('set as', (8, 10))]","[('LFC', (2, 3)), ('SPC and CFC', (4, 7)), ('5 , 1 and 1', (10, 15))]",[],[],[],[]
experiments,"For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .","[('For', (0, 1)), ('with', (11, 12)), ('of', (15, 16))]","[('AFLW - LFPA', (1, 4)), ('our method', (5, 7)), ('outperforms', (7, 8)), ('best methods', (9, 11)), ('large margin', (13, 15)), ('17.8 % improvement', (16, 19))]",[],[],[],[]
experiments,"For AFLW2000 - 3D , our method also shows a large improvement .","[('For', (0, 1)), ('shows', (8, 9))]","[('AFLW2000 - 3D', (1, 4)), ('our method', (5, 7)), ('large improvement', (10, 12))]",[],[],[],[]
experiments,"Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .","[('for', (2, 3)), ('with', (4, 5)), ('in', (7, 8)), ('improves', (16, 17)), ('by', (19, 20))]","[('images', (3, 4)), ('yaw angle', (5, 7)), ('[ 60 , 90 ]', (8, 13)), ('our method', (14, 16)), ('performance', (18, 19)), ('28 % ( from 7.93 to 5.68 )', (20, 28))]",[],[],[],[]
experiments,"For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .","[('For', (0, 1))]","[('IJB - A dataset', (2, 6))]",[],[],[],[]
experiments,The consistently superior performance of our DeFA indicates that we have advanced the state of the art in large - pose face alignment .,"[('of', (4, 5)), ('indicates', (7, 8)), ('in', (17, 18))]","[('consistently superior performance', (1, 4)), ('our DeFA', (5, 7)), ('advanced', (11, 12)), ('state of the art', (13, 17)), ('large - pose face alignment', (18, 23))]",[],[],[],[]
results,"Even though the proposed method can handle largepose alignment , to show its performance on the near- frontal datasets , we evaluate our method on the 300W dataset .","[('evaluate', (21, 22)), ('on', (24, 25))]","[('300W dataset', (26, 28))]",[],[],[],[]
baselines,"4 . To find the corresponding landmarks on the cheek , we apply the landmark marching algorithm to move contour landmarks from self - occluded location to the silhouette .","[('To find', (2, 4)), ('on', (7, 8)), ('apply', (12, 13)), ('to move', (17, 19)), ('from', (21, 22)), ('to', (26, 27))]","[('corresponding landmarks', (5, 7)), ('cheek', (9, 10)), ('landmark marching algorithm', (14, 17)), ('contour landmarks', (19, 21)), ('self - occluded location', (22, 26)), ('silhouette', (28, 29))]",[],[],[],[]
results,Our method is the second best method on the challenging set .,"[('is', (2, 3)), ('on', (7, 8))]","[('Our method', (0, 2)), ('second best method', (4, 7)), ('challenging set', (9, 11))]",[],[],[],[]
results,"In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .","[('of', (5, 6)), ('comparable to', (9, 11)), ('that', (13, 14))]","[('performance', (4, 5)), ('our method', (6, 8)), ('other methods', (11, 13)), ('near - frontal datasets', (17, 21))]",[],[],[],[]
results,"It is a strong testimony of our model in that DeFA , without further finetuning , outperforms both 3DDFA and its fine tuned version with SDM .","[('of', (5, 6)), ('without', (12, 13)), ('with', (24, 25))]","[('strong testimony', (3, 5)), ('DeFA', (10, 11)), ('outperforms', (16, 17)), ('3DDFA', (18, 19)), ('its fine tuned version', (20, 24)), ('SDM', (25, 26))]",[],[],[],[]
ablation-analysis,The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,"[('of', (2, 3)), ('on', (5, 6)), ('by adding', (12, 14))]","[('accuracy', (1, 2)), ('our method', (3, 5)), ('AFLW2000 - 3D', (7, 10)), ('consistently improves', (10, 12)), ('more datasets', (14, 16))]",[],[],[],[]
ablation-analysis,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .","[('For', (0, 1)), ('achieves', (9, 10)), ('by utilizing', (17, 19)), ('in', (21, 22)), ('over', (28, 29))]","[('AFLW - PIFA dataset', (2, 6)), ('our method', (7, 9)), ('9.5 % and 20 % relative improvement', (10, 17)), ('datasets', (20, 21)), ('stage 2 and stage 3', (23, 28)), ('first stage', (30, 32))]",[],[],[],[]
ablation-analysis,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .","[('including', (1, 2)), ('from', (4, 5)), ('can have', (13, 15)), ('achieve', (20, 21)), ('of', (22, 23))]","[('datasets', (3, 4)), ('both the second and third stages', (5, 11)), ('26 % relative improvement', (15, 19)), ('NME', (21, 22)), ('3.86 %', (23, 25))]",[],[],[],[]
ablation-analysis,5 shows that the effectiveness of CFC and SPC is more than LFC .,"[('shows', (1, 2)), ('of', (5, 6)), ('more than', (10, 12))]","[('effectiveness', (4, 5)), ('CFC and SPC', (6, 9)), ('LFC', (12, 13))]",[],[],[],[]
ablation-analysis,Comparing LFC + SPC and LFC + CFC performances shows that the CFC is more helpful than the SPC .,"[('Comparing', (0, 1)), ('shows', (9, 10)), ('than', (16, 17))]","[('LFC + SPC and LFC + CFC performances', (1, 9)), ('CFC', (12, 13)), ('more helpful', (14, 16)), ('SPC', (18, 19))]",[],[],[],[]
ablation-analysis,Using all constraints achieves the best performance .,"[('Using', (0, 1)), ('achieves', (3, 4))]","[('all constraints', (1, 3)), ('best performance', (5, 7))]",[],[],[],[]
ablation-analysis,This result shows that for the images with NME - lp between 5 % and 15 % the SPC is helpful .,"[('shows', (2, 3)), ('for', (4, 5)), ('with', (7, 8)), ('between', (11, 12)), ('is', (19, 20))]","[('images', (6, 7)), ('NME - lp', (8, 11)), ('5 % and 15 %', (12, 17)), ('SPC', (18, 19)), ('helpful', (20, 21))]",[],[],[],[]
ablation-analysis,"As shown in , SPC utilizes SIFT points to cover the whole 3D shape and the points in the highly textured areas are substantially used .","[('utilizes', (5, 6)), ('to cover', (8, 10)), ('in', (17, 18)), ('are', (22, 23))]","[('SPC', (4, 5)), ('SIFT points', (6, 8)), ('whole 3D shape', (11, 14)), ('points', (16, 17)), ('highly textured areas', (19, 22)), ('substantially used', (23, 25))]",[],[],[],[]
research-problem,Nonlinear 3D Face Morphable Model,[],[],[],[],[],[]
research-problem,"As a classic statistical model of 3D facial shape and texture , 3D Morphable Model ( 3 DMM ) is widely used in facial analysis , e.g. , model fitting , image synthesis .","[('of', (5, 6)), ('widely used in', (20, 23)), ('e.g.', (26, 27))]","[('3D facial', (6, 8)), ('3D Morphable Model ( 3 DMM )', (12, 19)), ('facial analysis', (23, 25)), ('model fitting', (28, 30)), ('image synthesis', (31, 33))]",[],[],[],[]
model,The entire network is end - to - end trainable with only weak supervision .,"[('is', (3, 4)), ('with', (10, 11))]","[('entire network', (1, 3)), ('end - to - end trainable', (4, 10)), ('only weak supervision', (11, 14))]",[],[],[],[]
model,"The morphable model framework provides two key benefits : first , a point - to - point correspondence between the reconstruction and all other models , enabling morphing , and second , modeling underlying transformations between types of faces ( male to female , neutral to smile , etc . ) .","[('provides', (4, 5)), ('between', (18, 19)), ('enabling', (26, 27)), ('between', (35, 36))]","[('morphable model framework', (1, 4)), ('two', (5, 6)), ('point - to - point correspondence', (12, 18)), ('reconstruction and all other models', (20, 25)), ('modeling', (32, 33)), ('underlying transformations', (33, 35)), ('types of faces ( male to female , neutral to smile , etc . )', (36, 51))]",[],[],[],[]
research-problem,"3 DMM has been widely applied in numerous areas , such as computer vision , graphics , human behavioral analysis and craniofacial surgery .",[],"[('3 DMM', (0, 2))]",[],[],[],[]
model,We propose a nonlinear 3 DMM to model shape / texture via deep neural networks ( DNNs ) .,"[('propose', (1, 2)), ('to model', (6, 8)), ('via', (11, 12))]","[('nonlinear 3 DMM', (3, 6)), ('shape / texture', (8, 11)), ('deep neural networks ( DNNs )', (12, 18))]",[],[],[],[]
model,"It can be trained from in - the - wild face images without 3 D scans , and also better reconstructs the original images due to the inherent nonlinearity .","[('trained from', (3, 5)), ('without', (12, 13)), ('better', (19, 20)), ('due to', (24, 26))]","[('in - the - wild face images', (5, 12)), ('3 D scans', (13, 16)), ('original images', (22, 24)), ('inherent nonlinearity', (27, 29))]",[],[],[],[]
research-problem,"To model highly variable 3 D face shapes , a large amount of high - quality 3 D face scans is required .","[('To model', (0, 2))]","[('highly variable 3 D face shapes', (2, 8)), ('large amount of high - quality 3 D face scans', (10, 20))]",[],[],[],[]
model,"Hence , it is fragile to large variances in the face identity .","[('is', (3, 4)), ('to', (5, 6)), ('in', (8, 9))]","[('fragile', (4, 5)), ('large variances', (6, 8)), ('face identity', (10, 12))]",[],[],[],[]
model,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .","[('utilize', (3, 4)), ('instead of', (8, 10)), ('as', (14, 15))]","[('two network decoders', (4, 7)), ('two PCA spaces', (10, 13)), ('shape and texture model components', (16, 21))]",[],[],[],[]
model,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .","[('of', (3, 4)), ('design', (8, 9)), ('for', (11, 12)), ('for', (24, 25)), ('for', (33, 34))]","[('each component', (4, 6)), ('different networks', (9, 11)), ('shape and texture', (12, 15)), ('multi - layer perceptron ( MLP )', (17, 24)), ('shape', (25, 26)), ('convolutional neural network ( CNN )', (27, 33)), ('texture', (34, 35))]",[],[],[],[]
model,These two decoders are essentially the nonlinear 3 DMM .,"[('are', (3, 4))]","[('nonlinear 3 DMM', (6, 9))]",[],[],[],[]
model,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .","[('learn', (3, 4)), ('to', (7, 8)), ('formulated as', (15, 17))]","[('fitting algorithm', (5, 7)), ('our nonlinear 3 DMM', (8, 12)), ('CNN encoder', (18, 20))]",[],[],[],[]
model,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .","[('takes', (2, 3)), ('as', (8, 9)), ('generates', (11, 12)), ('estimate', (22, 23))]","[('encoder', (1, 2)), ('2 D face image', (4, 8)), ('input', (9, 10)), ('shape and texture parameters', (13, 17)), ('two', (20, 21)), ('3D face', (24, 26))]",[],[],[],[]
model,"Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .","[('design', (3, 4)), ('to generate', (8, 10)), ('by fusing', (13, 15)), ('estimated by', (26, 28))]","[('differentiable rendering layer', (5, 8)), ('reconstructed face', (11, 13)), ('3D face , texture', (16, 20)), ('encoder', (29, 30))]",[],[],[],[]
model,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .","[('constructed where', (9, 11)), ('learnt jointly', (17, 19)), ('to minimize', (19, 21)), ('between', (23, 24))]","[('endto - end learning scheme', (3, 8)), ('encoder and two decoders', (12, 16)), ('difference', (22, 23)), ('reconstructed face and the input face', (25, 31))]",[],[],[],[]
model,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,"[('Jointly learning', (0, 2)), ('without relying on', (21, 24))]","[('3 DMM and the model fitting encoder', (3, 10)), ('large collection of unconstrained 2D images', (15, 21)), ('3D scans', (24, 26))]",[],[],[],[]
model,1 ) We learn a nonlinear 3 DMM model that has greater representation power than its traditional linear counterpart .,"[('learn', (3, 4)), ('than', (14, 15))]","[('nonlinear 3 DMM model', (5, 9)), ('greater representation power', (11, 14)), ('traditional linear counterpart', (16, 19))]",[],[],[],[]
model,"2 ) We jointly learn the model and the model fitting algorithm via weak supervision , by leveraging a large collection of 2D images without 3D scans .","[('jointly learn', (3, 5)), ('via', (12, 13)), ('by leveraging', (16, 18)), ('without', (24, 25))]","[('model and the model fitting algorithm', (6, 12)), ('weak supervision', (13, 15)), ('large collection of 2D images', (19, 24)), ('3D scans', (25, 27))]",[],[],[],[]
model,The novel rendering layer enables the end - to - end training .,"[('enables', (4, 5))]","[('novel rendering layer', (1, 4)), ('end - to - end training', (6, 12))]",[],[],[],[]
results,"Using facial mesh triangle definition by Basel Face Model ( BFM ) , we train our 3 DMM using 300W - LP dataset .","[('Using', (0, 1)), ('by', (5, 6)), ('train', (14, 15)), ('using', (18, 19))]","[('facial mesh triangle definition', (1, 5)), ('Basel Face Model ( BFM )', (6, 12)), ('3 DMM', (16, 18)), ('300W - LP dataset', (19, 23))]",[],[],[],[]
ablation-analysis,"The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.","[('optimized using', (3, 5)), ('with', (7, 8)), ('of', (12, 13)), ('when minimizing', (14, 16)), ('when minimizing', (21, 23))]","[('Adam optimizer', (5, 7)), ('initial learning rate', (9, 12)), ('0.001', (13, 14)), ('L 0', (16, 18)), ('0.0002', (20, 21))]",[],[],[],[]
ablation-analysis,"We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .","[('set to make', (29, 32)), ('to have', (33, 35))]","[('Q', (6, 7)), ('losses', (32, 33)), ('similar magnitudes', (35, 37))]",[],[],[],[]
results,Expressiveness,[],[],[],[],[],[]
results,"Our nonlinear model has a significantly smaller reconstruction error than the linear model , 0.0196 vs. 0.0241 ( Tab. 3 ) .","[('than', (9, 10))]","[('Our nonlinear model', (0, 3)), ('significantly smaller reconstruction error', (5, 9)), ('linear model', (11, 13)), ('0.0196', (14, 15))]",[],[],[],[]
experiments,visualizes our 3 DMM fitting results on CelebA dataset .,"[('visualizes', (0, 1)), ('on', (6, 7))]","[('3 DMM fitting results', (2, 6)), ('CelebA dataset', (7, 9))]",[],[],[],[]
experiments,We can recover personal facial characteristic in both shape and texture .,"[('recover', (2, 3)), ('in', (6, 7))]","[('personal facial characteristic', (3, 6)), ('both shape and texture', (7, 11))]",[],[],[],[]
experiments,Face alignment is a critical step for any facial analysis task such as face recognition .,[],"[('Face alignment', (0, 2))]",[],[],[],[]
experiments,We obtain a low error that is comparable to optimization - based methods .,"[('obtain', (1, 2)), ('comparable to', (7, 9))]","[('low error', (3, 5)), ('optimization - based methods', (9, 13))]",[],[],[],[]
research-problem,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses,[],"[('Facial Alignment', (5, 7))]",[],[],[],[]
model,The non-visible regions of the face are determined by the estimated camera center and the estimated 3D shape .,"[('of', (3, 4)), ('determined by', (7, 9))]","[('non-visible regions', (1, 3)), ('face', (5, 6)), ('estimated camera center', (10, 13)), ('estimated 3D shape', (15, 18))]",[],[],[],[]
model,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .","[('observe', (9, 10)), ('can', (16, 17)), ('deformed to', (25, 27)), ('at', (30, 31)), ('compared to', (36, 38))]","[('fairly accurate 3 D models', (11, 16)), ('simple mean shape', (22, 25)), ('input image', (28, 30)), ('relatively low computational cost', (32, 36)), ('other approaches', (38, 40))]",[],[],[],[]
model,Landmark locations can be directly predicted by a regression from a learned feature space .,"[('directly predicted by', (4, 7)), ('from', (9, 10))]","[('Landmark locations', (0, 2)), ('regression', (8, 9)), ('learned feature space', (11, 14))]",[],[],[],[]
model,The objective function in GSDM is divided into multiple regions of similar gradient directions .,"[('in', (3, 4)), ('divided into', (6, 8)), ('of', (10, 11))]","[('objective function', (1, 3)), ('GSDM', (4, 5)), ('multiple regions', (8, 10)), ('similar gradient directions', (11, 14))]",[],[],[],[]
model,It then constructs a separate cascaded shape regressor for each region .,"[('constructs', (2, 3)), ('for', (8, 9))]","[('separate cascaded shape regressor', (4, 8)), ('each region', (9, 11))]",[],[],[],[]
baselines,3DDFA fits a dense 3 D face model to the image via CNN and DDN proposes a novel cascaded framework incorporating geometric constraints for localizing landmarks in faces and other non-rigid objects .,"[('fits', (1, 2)), ('to', (8, 9)), ('via', (11, 12)), ('proposes', (15, 16)), ('incorporating', (20, 21)), ('for localizing', (23, 25)), ('in', (26, 27))]","[('3DDFA', (0, 1)), ('dense 3 D face model', (3, 8)), ('image', (10, 11)), ('CNN', (12, 13)), ('novel cascaded framework', (17, 20)), ('geometric constraints', (21, 23)), ('landmarks', (25, 26)), ('faces and other non-rigid objects', (27, 32))]",[],[],[],[]
research-problem,Nonlinear statistical model approaches are impractical in real - time applications .,[],"[('Nonlinear statistical model approaches', (0, 4))]",[],[],[],[]
research-problem,CNNs for 3D Object Modeling,[],[],[],[],[],[]
hyperparameters,Our network is implemented in the Caffe framework .,"[('implemented in', (3, 5))]","[('Our network', (0, 2)), ('Caffe framework', (6, 8))]",[],[],[],[]
ablation-analysis,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .","[('created', (4, 5)), ('consisting of', (5, 7))]","[('new layer', (1, 3)), ('3D TPS transformation module', (8, 12)), ('camera projection module', (14, 17)), ('bilinear sampler module', (19, 22))]",[],[],[],[]
hyperparameters,All modules are differentiable so that the whole network can be trained end - to - end .,"[('differentiable', (3, 4)), ('can be trained', (9, 12))]","[('whole network', (7, 9)), ('end - to - end', (12, 17))]",[],[],[],[]
hyperparameters,"We adopt two architectures , AlexNet and VGG - 16 , as the pre-trained models for our shared feature extraction networks in , i.e. we use the convolution layers from the pre-trained models to initialize ours .","[('adopt', (1, 2)), ('as', (11, 12)), ('for', (15, 16)), ('use', (25, 26)), ('from', (29, 30)), ('to initialize', (33, 35))]","[('two architectures', (2, 4)), ('AlexNet and VGG - 16', (5, 10)), ('shared feature extraction networks', (17, 21)), ('convolution layers', (27, 29))]",[],[],[],[]
hyperparameters,"Since these networks already extract informative low - level features and we do not want to lose this information , we freeze some of the earlier convolution layers and finetune the rest .","[('extract', (4, 5)), ('freeze', (21, 22)), ('finetune', (29, 30))]","[('some of', (22, 24)), ('earlier convolution layers', (25, 28)), ('rest', (31, 32))]",[],[],[],[]
hyperparameters,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .","[('For', (0, 1)), ('freeze', (6, 7)), ('for', (11, 12)), ('are', (22, 23))]","[('AlexNet architecture', (2, 4)), ('first layer', (8, 10)), ('VGG - 16 architecture', (13, 17)), ('first 4 layers', (19, 22)), ('frozen', (23, 24))]",[],[],[],[]
hyperparameters,The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,"[('implemented by', (5, 7)), ('on top of', (10, 13))]","[('2D landmark regression', (1, 4)), ('attaching', (7, 8)), ('additional layers', (8, 10)), ('last convolution layer', (14, 17))]",[],[],[],[]
hyperparameters,"With N landmarks to regress , we need NFC layers to compute the offsets for each individual landmark .","[('With', (0, 1)), ('need', (7, 8)), ('to compute', (10, 12)), ('for', (14, 15))]","[('N landmarks to regress', (1, 5)), ('NFC layers', (8, 10)), ('offsets', (13, 14)), ('each individual landmark', (15, 18))]",[],[],[],[]
hyperparameters,"While it 's possible to setup N individual FC layers , here we implement this by adding one Scaling layer followed by a Reduction layer and Bias layer .","[('followed by', (20, 22))]","[('one Scaling layer', (17, 20)), ('Reduction layer and Bias layer', (23, 28))]",[],[],[],[]
hyperparameters,During training only the new layers are updated and all previous layers are frozen .,"[('During', (0, 1)), ('are', (6, 7))]","[('training', (1, 2)), ('new layers', (4, 6)), ('updated', (7, 8)), ('all previous layers', (9, 12)), ('frozen', (13, 14))]",[],[],[],[]
experiments,Training on 300W - LP,[],"[('Training on', (0, 2))]",[],[],[],[]
experiments,"For the AlexNet architecture , we train for 100,000 iterations with a batch size of 50 .","[('For', (0, 1)), ('train for', (6, 8)), ('with', (10, 11))]","[('AlexNet architecture', (2, 4)), ('100,000 iterations', (8, 10)), ('batch size of 50', (12, 16))]",[],[],[],[]
hyperparameters,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 50,000 iterations .","[('set to', (5, 7)), ('drops by', (9, 11)), ('after', (15, 16))]","[('initial learning rate', (1, 4)), ('0.001', (7, 8)), ('factor of 2', (12, 15)), ('50,000 iterations', (16, 18))]",[],[],[],[]
hyperparameters,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 40,000 iterations .","[('training', (1, 2)), ('is', (10, 11)), ('drops by', (13, 15)), ('every', (19, 20))]","[('landmark regression', (3, 5)), ('initial learning rate', (7, 10)), ('0.01', (11, 12)), ('factor of 10', (16, 19)), ('40,000 iterations', (20, 22))]",[],[],[],[]
experiments,"For the VGG - 16 architecture , we train for 200,000 iterations with a batch size of 25 .","[('For', (0, 1)), ('train for', (8, 10)), ('with', (12, 13)), ('of', (16, 17))]","[('VGG - 16 architecture', (2, 6)), ('200,000 iterations', (10, 12)), ('batch size', (14, 16)), ('25', (17, 18))]",[],[],[],[]
hyperparameters,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 100,000 iterations .","[('set to', (5, 7)), ('drops by', (9, 11)), ('after', (15, 16))]","[('initial learning rate', (1, 4)), ('0.001', (7, 8)), ('factor of 2', (12, 15)), ('100,000 iterations', (16, 18))]",[],[],[],[]
hyperparameters,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 70,000 iterations .","[('is', (10, 11)), ('drops by', (13, 15)), ('every', (19, 20))]","[('landmark regression', (3, 5)), ('initial learning rate', (7, 10)), ('0.01', (11, 12)), ('factor of 10', (16, 19)), ('70,000 iterations', (20, 22))]",[],[],[],[]
hyperparameters,The momentum for all experiments is set to 0.9 .,"[('for', (2, 3)), ('set to', (6, 8))]","[('momentum', (1, 2)), ('all experiments', (3, 5)), ('0.9', (8, 9))]",[],[],[],[]
ablation-analysis,The VGG - 16 model outperforms the AlexNet model in all three pose ranges on the AFLW detected set as shown in .,"[('in', (9, 10)), ('on', (14, 15))]","[('VGG - 16 model', (1, 5)), ('outperforms', (5, 6)), ('AlexNet model', (7, 9)), ('all three pose ranges', (10, 14)), ('AFLW detected set', (16, 19))]",[],[],[],[]
ablation-analysis,shows that the landmark regression step greatly helps to improve the accuracy .,"[('shows', (0, 1)), ('to improve', (8, 10))]","[('landmark regression step', (3, 6)), ('greatly helps', (6, 8)), ('accuracy', (11, 12))]",[],[],[],[]
baselines,"AFLW : Since the CMS - RCNN approach may only detect the easier to landmark faces , we use the provided bounding box anytime the face is not detected by the detector .","[('use', (18, 19)), ('anytime', (23, 24)), ('not detected by', (27, 30))]","[('AFLW', (0, 1)), ('provided bounding box', (20, 23)), ('face', (25, 26)), ('detector', (31, 32))]",[],[],[],[]
baselines,"We compare against baseline methods used by on the same dataset , namely Cascaded Deformable Shape Models ( CDM ) , Robust Cascaded Pose Regression ( RCPR ) , Explicit Shape Regression ( ESR ) , SDM and 3DDFA .","[('compare', (1, 2)), ('namely', (12, 13))]","[('Cascaded Deformable Shape Models ( CDM )', (13, 20)), ('Robust Cascaded Pose Regression ( RCPR )', (21, 28)), ('Explicit Shape Regression ( ESR )', (29, 35)), ('SDM', (36, 37)), ('3DDFA', (38, 39))]",[],[],[],[]
baselines,All methods except for CDM were retrained on the 300W - LP dataset .,"[('except for', (2, 4)), ('retrained on', (6, 8))]","[('methods', (1, 2)), ('CDM', (4, 5)), ('300W - LP dataset', (9, 13))]",[],[],[],[]
results,"clearly shows that our model using the VGG - 16 architecture has achieved better accuracy in all pose ranges , especially the ( 60 , 90 ] category , and has achieved a smaller standard deviation in the error .","[('shows', (1, 2)), ('using', (5, 6)), ('achieved', (12, 13)), ('in', (15, 16)), ('especially', (20, 21)), ('achieved', (31, 32)), ('in', (36, 37))]","[('our model', (3, 5)), ('VGG - 16 architecture', (7, 11)), ('better accuracy', (13, 15)), ('all pose ranges', (16, 19)), ('( 60 , 90 ] category', (22, 28)), ('smaller standard deviation', (33, 36)), ('error', (38, 39))]",[],[],[],[]
results,"This means that not only are the landmarks more accurate , they are more consistent than the other methods ..","[('than', (15, 16))]","[('landmarks', (7, 8)), ('more accurate', (8, 10)), ('more consistent', (13, 15)), ('other methods', (17, 19))]",[],[],[],[]
baselines,AFLW2000 - 3D :,[],"[('AFLW2000 - 3D', (0, 3))]",[],[],[],[]
results,"Here we see that though 3DDFA + SDM performs well , the VGG - 16 architecture of our model still performs best in both the [ 0 , 30 ] and ( 60 , 90 ] ranges .","[('performs', (8, 9)), ('of', (16, 17)), ('performs', (20, 21)), ('in both', (22, 24))]","[('3DDFA + SDM', (5, 8)), ('well', (9, 10)), ('VGG - 16 architecture', (12, 16)), ('our model', (17, 19)), ('best', (21, 22))]",[],[],[],[]
results,"While the VGG - 16 model is only second best in the ( 30 , 60 ] range by a small amount , the improvement in ( 60 , 90 ] means that , once again , our method generates more accurate and more consistent landmarks , even in a 3D sense .","[('is', (6, 7)), ('in', (10, 11)), ('by', (18, 19)), ('generates', (39, 40))]","[('VGG - 16 model', (2, 6)), ('second best', (8, 10)), ('( 30 , 60 ] range', (12, 18)), ('small amount', (20, 22)), ('our method', (37, 39)), ('more accurate and', (40, 43))]",[],[],[],[]
results,Running Speed,[],[],[],[],[],[]
results,The models are evaluated on a 3.40 GHz Intel Core i7-6700 CPU and an NVIDIA GeForce GTX TITAN X GPU .,"[('evaluated on', (3, 5))]","[('3.40 GHz Intel Core i7-6700 CPU', (6, 12)), ('NVIDIA GeForce GTX TITAN X GPU', (14, 20))]",[],[],[],[]
research-problem,Face Alignment Across Large Poses : A 3D Solution,[],"[('Face Alignment Across', (0, 3))]",[],[],[],[]
model,The latter extracts features around key points and regresses it to the ground truth landmarks .,"[('features around', (3, 5)), ('regresses it to', (8, 11))]","[('key points', (5, 7)), ('ground truth landmarks', (12, 15))]",[],[],[],[]
model,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .","[('rather than', (11, 13)), ('to', (18, 19))]","[('3D dense face model', (7, 11)), ('sparse landmark shape model', (14, 18)), ('image', (20, 21))]",[],[],[],[]
research-problem,We call this method 3D Dense Face Alignment ( 3DDFA ) .,[],"[('3D Dense Face Alignment ( 3DDFA )', (4, 11))]",[],[],[],[]
model,"To resolve the fitting process in 3 DDFA , we propose a cascaded convolutional neutral network ( CNN ) based regression method .","[('To resolve', (0, 2)), ('in', (5, 6)), ('propose', (10, 11))]","[('fitting process', (3, 5)), ('3 DDFA', (6, 8)), ('cascaded convolutional neutral network ( CNN ) based regression method', (12, 22))]",[],[],[],[]
model,"In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .","[('adopt', (5, 6)), ('to fit', (7, 9)), ('with', (13, 14)), ('namely', (19, 20))]","[('CNN', (6, 7)), ('3D face model', (10, 13)), ('Projected Normalized Coordinate Code ( PNCC )', (20, 27))]",[],[],[],[]
model,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .","[('proposed as', (10, 12))]","[('Weighted Parameter Distance Cost ( WPDC )', (2, 9)), ('cost function', (13, 15))]",[],[],[],[]
model,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,"[('further propose', (1, 3)), ('to synthesize', (7, 9)), ('across', (14, 15))]","[('face profiling algorithm', (4, 7)), ('60 k + training samples', (9, 14)), ('large poses', (15, 17))]",[],[],[],[]
code,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/.",[],[],[],[],[],[]
baselines,"In this paper , we test the performance of 3DDFA on three different tasks , including the large - pose face alignment on AFLW , 3 D face alignment on AFLW2000 - 3D and mediumpose face alignment on 300W .","[('test', (5, 6)), ('of', (8, 9)), ('on', (10, 11)), ('including', (15, 16)), ('on', (22, 23)), ('on', (29, 30)), ('on', (37, 38))]","[('performance', (7, 8)), ('3DDFA', (9, 10)), ('three', (11, 12)), ('large - pose face alignment', (17, 22)), ('AFLW', (23, 24)), ('3 D face alignment', (25, 29)), ('AFLW2000 - 3D', (30, 33)), ('mediumpose face alignment', (34, 37)), ('300W', (38, 39))]",[],[],[],[]
baselines,Large Pose Face Alignment in AFLW Protocol :,[],"[('Large Pose Face Alignment in', (0, 5)), ('AFLW Protocol', (5, 7))]",[],[],[],[]
hyperparameters,The bounding boxes provided by AFLW are used for initialization ( which are not the ground truth ) .,"[('provided by', (3, 5)), ('used for', (7, 9))]","[('bounding boxes', (1, 3)), ('AFLW', (5, 6)), ('initialization', (9, 10))]",[],[],[],[]
hyperparameters,"During training , for 2D methods we use the projected 3D landmarks as the ground truth and for 3DDFA we directly regress the 3 DMM parameters .","[('During', (0, 1)), ('for', (3, 4)), ('use', (7, 8)), ('as', (12, 13)), ('for', (17, 18)), ('directly regress', (20, 22))]","[('training', (1, 2)), ('2D methods', (4, 6)), ('projected 3D landmarks', (9, 12)), ('ground truth', (14, 16)), ('3DDFA', (18, 19)), ('3 DMM parameters', (23, 26))]",[],[],[],[]
baselines,CDM is the first one claimed to perform pose - free face alignment .,[],"[('CDM', (0, 1)), ('pose - free face alignment', (8, 13))]",[],[],[],[]
baselines,RCPR is a occlusion - robust method with the potential to deal with selfocclusion and we train it with landmark visibility labels computed by .,"[('is', (1, 2)), ('with', (7, 8)), ('to deal with', (10, 13)), ('train it with', (16, 19))]","[('RCPR', (0, 1)), ('occlusion - robust method', (3, 7)), ('potential', (9, 10)), ('selfocclusion', (13, 14)), ('landmark visibility labels', (19, 22))]",[],[],[],[]
results,"Firstly , the results indicate that all the methods benefits substantially from face profiling when dealing with large poses .","[('indicate', (4, 5)), ('benefits', (9, 10)), ('from', (11, 12)), ('when dealing with', (14, 17))]","[('all the methods', (6, 9)), ('substantially', (10, 11)), ('face profiling', (12, 14)), ('large poses', (17, 19))]",[],[],[],[]
results,"The improvements in [ 60 , 90 ] are 44.06 % for RCPR , 40.36 % for ESR and 42.10 % for SDM .","[('in', (2, 3)), ('are', (8, 9)), ('for', (11, 12))]","[('improvements', (1, 2)), ('[ 60 , 90 ]', (3, 8)), ('44.06 %', (9, 11)), ('RCPR', (12, 13)), ('40.36 %', (14, 16)), ('ESR', (17, 18)), ('42.10 %', (19, 21)), ('SDM', (22, 23))]",[],[],[],[]
results,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .","[('reaches', (3, 4)), ('above', (9, 10)), ('especially beyond', (14, 16))]","[('3DDFA', (2, 3)), ('state of the art', (5, 9)), ('all the 2D methods', (10, 14)), ('medium poses', (16, 18))]",[],[],[],[]
results,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,"[('of', (4, 5)), ('demonstrates', (7, 8)), ('to pose', (10, 12))]","[('minimum standard deviation', (1, 4)), ('3DDFA', (5, 6)), ('robustness', (9, 10)), ('variations', (12, 13))]",[],[],[],[]
baselines,3D Face Alignment in AFLW2000-3D,[],"[('3D Face Alignment', (0, 3))]",[],[],[],[]
results,"Compared with the results in AFLW , we can seethe defect of barely evaluating visible landmarks .","[('Compared with', (0, 2)), ('in', (4, 5)), ('seethe', (9, 10)), ('of', (11, 12))]","[('results', (3, 4)), ('AFLW', (5, 6)), ('defect', (10, 11)), ('barely evaluating visible landmarks', (12, 16))]",[],[],[],[]
results,"For all the methods , despite with ground truth bounding boxes the performance in [ 60 , 90 ] and the standard deviation are obviously reduced when considering all the landmarks .","[('despite with', (5, 7)), ('in', (13, 14)), ('when considering', (26, 28))]","[('ground truth bounding boxes', (7, 11)), ('performance', (12, 13)), ('[ 60 , 90 ]', (14, 19)), ('standard deviation', (21, 23)), ('reduced', (25, 26)), ('all the landmarks', (28, 31))]",[],[],[],[]
research-problem,Deep Multi- Center Learning for Face Alignment,[],[],[],[],[],[]
code,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,[],"[('https://github.com/ZhiwenShao/MCNet-Extension', (8, 9))]",[],[],[],[]
model,"However , it needs extra labels of facial attributes for training samples , which limits its universality .","[('needs', (3, 4)), ('of', (6, 7)), ('for', (9, 10)), ('limits', (14, 15))]","[('extra labels', (4, 6)), ('facial attributes', (7, 9)), ('training', (10, 11)), ('samples', (11, 12)), ('universality', (16, 17))]",[],[],[],[]
model,It is observed that the nose can be localized roughly with the locations of eyes and mouth .,"[('observed that', (2, 4)), ('can be', (6, 8)), ('with', (10, 11))]","[('nose', (5, 6)), ('localized', (8, 9)), ('roughly', (9, 10)), ('locations of eyes and mouth', (12, 17))]",[],[],[],[]
model,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .","[('propose', (6, 7)), ('named', (12, 13)), ('to exploit', (20, 22)), ('among', (25, 26))]","[('novel deep learning framework', (8, 12)), ('Multi - Center Learning ( MCL )', (13, 20)), ('strong correlations', (23, 25)), ('landmarks', (26, 27))]",[],[],[],[]
model,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .","[('uses', (5, 6)), ('to predict', (10, 12)), ('emphasizes on', (22, 24)), ('of', (26, 27))]","[('our', (3, 4)), ('multiple shape prediction layers', (6, 10)), ('locations of landmarks', (13, 16)), ('each shape prediction layer', (18, 22)), ('detection', (25, 26)), ('certain cluster of landmarks', (28, 32))]",[],[],[],[]
model,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .","[('weighting', (1, 2)), ('of', (4, 5)), ('is', (19, 20))]","[('loss', (3, 4)), ('each landmark', (5, 7)), ('challenging landmarks', (8, 10)), ('focused', (11, 12)), ('firstly', (12, 13)), ('each cluster of landmarks', (15, 19)), ('further optimized', (20, 22))]",[],[],[],[]
model,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .","[('to decrease', (2, 4)), ('propose', (9, 10)), ('to integrate', (14, 16)), ('into', (20, 21))]","[('model complexity', (5, 7)), ('model assembling method', (11, 14)), ('multiple shape prediction layers', (16, 20)), ('one shape prediction layer', (21, 25))]",[],[],[],[]
model,The entire framework reinforces the learning process of each landmark with a low model complexity .,"[('reinforces', (3, 4)), ('of', (7, 8)), ('with', (10, 11))]","[('learning process', (5, 7)), ('each landmark', (8, 10)), ('low model complexity', (12, 15))]",[],[],[],[]
model,We propose a novel multi-center learning framework for exploiting the strong correlations among landmarks .,"[('among', (12, 13))]","[('strong correlations', (10, 12)), ('landmarks', (13, 14))]",[],[],[],[]
model,We propose a model assembling method which ensures a low model complexity .,"[('propose', (1, 2)), ('ensures', (7, 8))]","[('model assembling method', (3, 6)), ('low model complexity', (9, 12))]",[],[],[],[]
research-problem,C. Face Alignment via Deep Learning,[],"[('C. Face Alignment', (0, 3))]",[],[],[],[]
hyperparameters,"We enhance the diversity of raw training data on account of their limited variation patterns , using five steps : rotation , uniform scaling , translation , horizontal flip , and JPEG compression .","[('enhance', (1, 2)), ('of', (4, 5)), ('on account of', (8, 11)), ('using', (16, 17))]","[('diversity', (3, 4)), ('raw training data', (5, 8)), ('limited variation patterns', (12, 15)), ('five steps', (17, 19)), ('rotation', (20, 21)), ('uniform scaling', (22, 24)), ('translation', (25, 26)), ('horizontal flip', (27, 29)), ('JPEG compression', (31, 33))]",[],[],[],[]
hyperparameters,We train our MCL using an open source deep learning framework Caffe .,"[('train', (1, 2)), ('using', (4, 5))]","[('our MCL', (2, 4)), ('open source deep learning framework Caffe', (6, 12))]",[],[],[],[]
hyperparameters,"The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .","[('of', (4, 5)), ('are', (10, 11)), ('of', (23, 24)), ('are', (31, 32))]","[('maximum learning iterations', (1, 4)), ('pre-training', (5, 6)), ('each finetuning step', (7, 10)), ('1810 4 and 610 4', (11, 16)), ('initial learning rates', (20, 23)), ('pre-training', (24, 25)), ('each fine - tuning step', (26, 31)), ('0.02 and 0.001', (32, 35))]",[],[],[],[]
baselines,"FLD + PDE performs facial landmark detection , pose and deformation estimation simultaneously , in which the training data of pose and deformation estimation are used .","[('performs', (3, 4)), ('in', (14, 15)), ('of', (19, 20))]","[('FLD + PDE', (0, 3)), ('facial landmark detection', (4, 7)), ('pose and deformation estimation', (8, 12)), ('training data', (17, 19)), ('pose and deformation estimation', (20, 24))]",[],[],[],[]
results,"Our method MCL outperforms most of the state - of - the - art methods , especially on AFLW dataset where a relative error reduction of 3.93 % is achieved compared to RecNet .","[('especially on', (16, 18))]","[('Our method MCL', (0, 3)), ('outperforms', (3, 4)), ('most of the state - of - the - art methods', (4, 15)), ('AFLW dataset', (18, 20))]",[],[],[],[]
ablation-analysis,1 ) Global Average Pooling vs. Full Connection :,[],"[('Global Average Pooling vs. Full Connection', (2, 8))]",[],[],[],[]
ablation-analysis,2 ) Robustness of Weighting :,[],"[('Robustness of Weighting', (2, 5))]",[],[],[],[]
ablation-analysis,"When ? is 0.4 , WM can still achieves good performance .","[('When', (0, 1)), ('is', (2, 3))]","[('?', (1, 2)), ('0.4', (3, 4)), ('WM', (5, 6)), ('good performance', (9, 11))]",[],[],[],[]
ablation-analysis,"Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .","[('Compared to', (0, 2)), ('reduce', (14, 15)), ('of', (18, 19))]","[('WM', (2, 3)), ('left eye model and the right eye model', (5, 13)), ('alignment errors', (16, 18)), ('corresponding clusters', (20, 22))]",[],[],[],[]
ablation-analysis,"Taking the left eye model as an example , it additionally reduces the errors of landmarks of right eye , mouth , and chin , which is due to the correlations among different facial parts .","[('Taking', (0, 1)), ('of', (14, 15)), ('of', (16, 17))]","[('left eye model', (2, 5)), ('errors', (13, 14)), ('landmarks', (15, 16)), ('right eye , mouth , and chin', (17, 24))]",[],[],[],[]
ablation-analysis,"Moreover , for the right eye cluster , the right eye model improves the accuracy more significantly than the left eye model .","[('for', (2, 3)), ('improves', (12, 13)), ('than', (17, 18))]","[('right eye cluster', (4, 7)), ('right eye model', (9, 12)), ('accuracy', (14, 15)), ('more significantly', (15, 17)), ('left eye model', (19, 22))]",[],[],[],[]
ablation-analysis,"Note that Simplified AM has already acquired good results , which verifies the effectiveness of the multicenter fine - tuning stage .","[('acquired', (6, 7))]","[('Simplified AM', (2, 4)), ('good results', (7, 9))]",[],[],[],[]
ablation-analysis,It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG .,"[('seen that', (3, 5)), ('improves', (8, 9)), ('on', (10, 11)), ('fails to search', (13, 16)), ('on', (19, 20))]","[('Weighting Simplified AM', (5, 8)), ('COFW', (11, 12)), ('better solution', (17, 19)), ('IBUG', (20, 21))]",[],[],[],[]
research-problem,Aggregation via Separation : Boosting Facial Landmark Detector with Semi-Supervised,[],"[('Aggregation via Separation', (0, 3)), ('Boosting Facial Landmark Detector', (4, 8))]",[],[],[],[]
research-problem,Style Translation,[],[],[],[],[],[]
research-problem,"With these augmented synthetic samples , our semi-supervised model surprisingly outperforms the fully - supervised one by a large margin .","[('With', (0, 1)), ('by', (16, 17))]","[('our semi-supervised model', (6, 9)), ('surprisingly outperforms', (9, 11)), ('fully - supervised one', (12, 16)), ('large margin', (18, 20))]",[],[],[],[]
code,The code is made publicly available at https://github.com/thesouthfrog/stylealign.,[],[],[],[],[],[]
research-problem,"Facial landmark detection is a fundamentally important step in many face applications , such as face recognition , 3 D face reconstruction , face tracking and face editing .",[],"[('Facial landmark detection', (0, 3))]",[],[],[],[]
model,"We instead utilize style transfer and disentangled representation learning to tackle the face alignment problem , since style transfer aims at altering style while preserving content .","[('utilize', (2, 3)), ('to tackle', (9, 11))]","[('style transfer and disentangled representation learning', (3, 9)), ('face alignment problem', (12, 15))]",[],[],[],[]
model,"Our idea is based on the purpose of facial landmark detection , which is to regress "" facial content "" - the principal component of facial geometry - by filtering unconstrained "" styles "" .","[('by filtering', (28, 30))]","[('facial landmark detection', (8, 11)), ('unconstrained "" styles ""', (30, 34))]",[],[],[],[]
model,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .","[('propose', (5, 6)), ('to', (9, 10)), ('for', (12, 13)), ('without using', (16, 18))]","[('new framework', (7, 9)), ('augment training', (10, 12)), ('facial landmark detection', (13, 16)), ('extra knowledge', (18, 20))]",[],[],[],[]
model,"Instead of directly generating images , we first map face images into the space of structure and style .","[('first map', (7, 9)), ('into', (11, 12))]","[('face images', (9, 11)), ('space of structure and style', (13, 18))]",[],[],[],[]
model,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .","[('To guarantee', (0, 2)), ('design', (10, 11)), ('in which', (19, 21)), ('incorporated for', (33, 35)), ('of', (37, 38))]","[('disentanglement', (3, 4)), ('conditional variational auto - encoder model', (12, 18)), ('Kullback - Leiber ( KL ) divergence loss and skip connections', (21, 32)), ('compact representation', (35, 37)), ('style and structure', (38, 41))]",[],[],[],[]
model,"By factoring these features , we perform visual style translation between existing facial geometry .","[('perform', (6, 7)), ('between', (10, 11))]","[('visual style translation', (7, 10)), ('existing facial geometry', (11, 14))]",[],[],[],[]
model,A novel semi-supervised framework based on conditional variational auto - encoder is built upon this new perspective .,"[('based on', (4, 6))]","[('novel semi-supervised framework', (1, 4)), ('conditional variational auto - encoder', (6, 11))]",[],[],[],[]
model,"By disentangling style and structure , our model generates style - augmented images via style translation , further boosting facial landmark detection .","[('generates', (8, 9)), ('via', (13, 14)), ('further boosting', (17, 19))]","[('style and structure', (2, 5)), ('our model', (6, 8)), ('style - augmented images', (9, 13)), ('style translation', (14, 16)), ('facial landmark detection', (19, 22))]",[],[],[],[]
experiments,The Res - 18 baseline receives strong enhancement using synthetic images .,"[('receives', (5, 6)), ('using', (8, 9))]","[('Res - 18 baseline', (1, 5)), ('strong enhancement', (6, 8)), ('synthetic images', (9, 11))]",[],[],[],[]
experiments,"By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .","[('utilizing', (1, 2)), ('achieves', (8, 9)), ('under', (12, 13)), ('by', (26, 27))]","[('stronger baseline', (3, 5)), ('our model', (6, 8)), ('4.39 % NME', (9, 12)), ('style - augmented training', (13, 17)), ('outperforms', (18, 19)), ('state - of the - art entries', (19, 26)), ('large margin', (28, 30))]",[],[],[],[]
experiments,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .","[('for', (3, 4)), ('brings', (11, 12)), ('to', (15, 16)), ('to', (23, 24)), ('from', (25, 26)), ('to', (29, 30))]","[('strong baselines', (5, 7)), ('our method', (8, 10)), ('15.9 % improvement', (12, 15)), ('SAN model', (16, 18)), ('9 % boost', (20, 23)), ('LAB', (24, 25)), ('5.27 % NME', (26, 29)), ('4.76 %', (30, 32))]",[],[],[],[]
ablation-analysis,"It shows when the data is limited , our separation component tends to capture weak style information , such as color and lighting .","[('shows when', (1, 3)), ('is', (5, 6)), ('tends to capture', (11, 14)), ('such as', (18, 20))]","[('data', (4, 5)), ('our separation component', (8, 11)), ('weak style information', (14, 17)), ('color and lighting', (20, 23))]",[],[],[],[]
research-problem,Deep Alignment Network : A convolutional neural network for robust face alignment,"[('for', (8, 9))]","[('Deep Alignment Network', (0, 3)), ('convolutional neural network', (5, 8))]",[],[],[],[]
research-problem,"Face alignment is an important component of many computer vision applications , such as face verification , facial emotion recognition , humancomputer interaction and facial motion capture .",[],"[('Face alignment', (0, 2))]",[],[],[],[]
model,The features are then used to iteratively refine the estimates of landmark locations .,"[('used to', (4, 6)), ('of', (10, 11))]","[('features', (1, 2)), ('iteratively refine', (6, 8)), ('estimates', (9, 10)), ('landmark locations', (11, 13))]",[],[],[],[]
model,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .","[('proposing', (10, 11)), ('dub', (18, 19))]","[('novel face alignment method', (12, 16)), ('Deep Alignment Network ( DAN )', (19, 25))]",[],[],[],[]
model,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .","[('based on', (2, 4)), ('where', (8, 9)), ('refines', (11, 12)), ('estimated at', (15, 17)), ('iteratively improving', (21, 23))]","[('multistage neural network', (5, 8)), ('each stage', (9, 11)), ('landmark positions', (13, 15)), ('previous stage', (18, 20)), ('landmark locations', (24, 26))]",[],[],[],[]
model,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .","[('To make use of', (0, 4)), ('during', (8, 9)), ('of', (11, 12)), ('input at', (17, 19)), ('of', (30, 31))]","[('entire face image', (5, 8)), ('process', (10, 11)), ('face alignment', (12, 14)), ('each stage', (19, 21)), ('landmark heatmap', (22, 24)), ('key element', (28, 30))]",[],[],[],[]
model,The convolutional neural network can use the heatmaps to infer the current estimates of landmark locations in the image and thus refine them .,"[('use', (5, 6)), ('to infer', (8, 10)), ('of', (13, 14)), ('in', (16, 17))]","[('convolutional neural network', (1, 4)), ('heatmaps', (7, 8)), ('current estimates', (11, 13)), ('landmark locations', (14, 16)), ('image', (18, 19))]",[],[],[],[]
model,We introduce landmark heatmaps which transfer the information about current landmark location estimates between the stages of our method .,"[('introduce', (1, 2)), ('transfer', (5, 6)), ('about', (8, 9)), ('between', (13, 14))]","[('landmark heatmaps', (2, 4)), ('information', (7, 8)), ('current landmark location estimates', (9, 13)), ('stages of our method', (15, 19))]",[],[],[],[]
model,"This improvement allows our method to make use of the entire image of a face , instead of local patches , and avoid falling into local minima .","[('allows', (2, 3)), ('to make use of', (5, 9)), ('of', (12, 13)), ('instead of', (16, 18)), ('avoid falling into', (22, 25))]","[('our method', (3, 5)), ('entire image', (10, 12)), ('face', (14, 15)), ('local patches', (18, 20)), ('local minima', (25, 27))]",[],[],[],[]
baselines,"We train two models , DAN which is trained on the training subset of the 300W competition data and DAN - Menpo which is trained on both the above mentioned dataset and the Menpo challenge training set .","[('train', (1, 2)), ('trained on', (8, 10)), ('of', (13, 14)), ('trained on', (24, 26))]","[('two models', (2, 4)), ('DAN', (5, 6)), ('training subset', (11, 13)), ('300W competition data', (15, 18)), ('DAN - Menpo', (19, 22)), ('Menpo challenge training set', (33, 37))]",[],[],[],[]
experimental-setup,"Data augmentation is performed by mirroring around the Y axis as well as random translation , rotation and scaling , all sampled from normal distributions .","[('performed by', (3, 5)), ('around', (6, 7)), ('as well as', (10, 13)), ('sampled from', (21, 23))]","[('Data augmentation', (0, 2)), ('mirroring', (5, 6)), ('Y axis', (8, 10)), ('random translation', (13, 15)), ('rotation and', (16, 18)), ('scaling', (18, 19)), ('normal distributions', (23, 25))]",[],[],[],[]
baselines,Both models ( DAN and DAN - Menpo ) consist of two stages .,"[('consist of', (9, 11))]","[('two stages', (11, 13))]",[],[],[],[]
experimental-setup,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,"[('performed using', (2, 4))]","[('Training', (0, 1)), ('Theano 0.9.0', (4, 6)), ('Lasagne 0.2', (7, 9))]",[],[],[],[]
experimental-setup,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,"[('For', (0, 1)), ('use', (3, 4)), ('with', (7, 8))]","[('optimization', (1, 2)), ('Adam stochastic optimization', (4, 7)), ('initial step size', (9, 12)), ('0.001', (13, 14)), ('mini batch size', (15, 18)), ('64', (19, 20))]",[],[],[],[]
experimental-setup,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,"[('runs at', (3, 5)), ('for', (7, 8)), ('processed in', (9, 11)), ('for', (16, 17)), ('processed', (18, 19)), ('on', (20, 21))]","[('Python implementation', (1, 3)), ('73 fps', (5, 7)), ('images', (8, 9)), ('45 fps', (14, 16)), ('images', (17, 18)), ('sequentially', (19, 20)), ('GeForce GTX 1070 GPU', (22, 26))]",[],[],[],[]
experimental-setup,For each test set we initialize our method using the face detector bounding boxes provided with the datasets .,"[('For', (0, 1)), ('initialize', (5, 6)), ('using', (8, 9)), ('provided with', (14, 16))]","[('each test set', (1, 4)), ('our method', (6, 8)), ('face detector bounding boxes', (10, 14)), ('datasets', (17, 18))]",[],[],[],[]
results,Results on the Menpo challenge test set,"[('on', (1, 2))]",[],[],[],[],[]
experimental-setup,The first step performs face alignment using a square initialization bounding box placed in the middle of the image with a size set to a percentage of image height .,"[('performs', (3, 4)), ('using', (6, 7)), ('placed in the', (12, 15)), ('with', (19, 20)), ('set to', (22, 24))]","[('first', (1, 2)), ('face alignment', (4, 6)), ('square initialization bounding box', (8, 12)), ('middle', (15, 16)), ('image', (18, 19)), ('size', (21, 22)), ('percentage of image height', (25, 29))]",[],[],[],[]
experimental-setup,The chosen bounding box size was 46 % of the image height .,"[('chosen', (1, 2)), ('was', (5, 6)), ('of', (8, 9))]","[('bounding box size', (2, 5)), ('46 %', (6, 8)), ('image height', (10, 12))]",[],[],[],[]
experimental-setup,For the AUC and the failure rate we have chosen a threshold of 0.03 of the bounding box diagonal as it is approximately equivalent to 0.08 of the interocular distance used in the previous chapter .,"[('For', (0, 1)), ('chosen', (9, 10)), ('of', (12, 13)), ('of', (14, 15))]","[('AUC and the failure rate', (2, 7)), ('threshold', (11, 12)), ('0.03', (13, 14)), ('bounding box diagonal', (16, 19))]",[],[],[],[]
results,The addition of the second stage increases the AUC 0.08 by 20 % while the mean error and failure rate are reduced by 14 % and 56 % respectively .,"[('addition of', (1, 3)), ('increases', (6, 7)), ('by', (10, 11)), ('reduced by', (21, 23))]","[('second stage', (4, 6)), ('AUC', (8, 9)), ('0.08', (9, 10)), ('20 %', (11, 13)), ('mean error and failure rate', (15, 20)), ('14 % and 56 %', (23, 28))]",[],[],[],[]
results,The addition of a third stage does not bring significant benefit in any of the metrics .,"[('addition of', (1, 3)), ('does not bring', (6, 9)), ('in', (11, 12))]","[('third stage', (4, 6)), ('significant benefit', (9, 11)), ('any of', (12, 14))]",[],[],[],[]
research-problem,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild,[],"[('Face Alignment In', (6, 9))]",[],[],[],[]
research-problem,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .",[],"[('Face Alignment', (0, 2))]",[],[],[],[]
research-problem,"In this paper , we introduce DeCaFA , an end - to - end deep convolutional cascade architecture for face alignment .","[('introduce', (5, 6)), ('for', (18, 19))]","[('DeCaFA', (6, 7)), ('end - to - end deep convolutional cascade architecture', (9, 18)), ('face alignment', (19, 21))]",[],[],[],[]
research-problem,"We show experimentally that DeCaFA significantly outperforms existing approaches on 300W , CelebA and WFLW databases .","[('show', (1, 2)), ('on', (9, 10))]","[('DeCaFA', (4, 5)), ('significantly outperforms', (5, 7)), ('existing approaches', (7, 9)), ('300W , CelebA and WFLW databases', (10, 16))]",[],[],[],[]
model,"This allows to robustly learn rigid transformations , such as translation and rotation , in the first cascade stages , while learning non-rigid deformation ( e.g. due to facial expression or non-planar rotation ) later on .","[('such as', (8, 10)), ('in', (14, 15)), ('due to', (26, 28))]","[('robustly learn', (3, 5)), ('rigid transformations', (5, 7)), ('translation and rotation', (10, 13)), ('first cascade stages', (16, 19)), ('learning', (21, 22)), ('non-rigid deformation', (22, 24)), ('facial expression', (28, 30)), ('non-planar rotation', (31, 33))]",[],[],[],[]
model,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .","[('introduce', (5, 6))]","[('Deep convolutional Cascade for Face Alignment ( DeCaFA )', (7, 16))]",[],[],[],[]
model,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .","[('composed of', (2, 4)), ('each produce', (7, 9)), ('relatively to', (15, 17))]","[('DeCaFA', (0, 1)), ('several stages', (4, 6)), ('landmark - wise attention maps', (9, 14)), ('heterogeneous annotation markups', (17, 20))]",[],[],[],[]
model,"We introduce a fully - convolutional Deep Cascade for Face Alignment ( DeCaFA ) that unifies cascaded regression and end - to - end deep approaches , by using landmark - wise attention maps fused to extract local information around a current landmark estimate .","[('introduce', (1, 2)), ('unifies', (15, 16)), ('by using', (27, 29)), ('fused to extract', (34, 37)), ('around', (39, 40))]","[('fully - convolutional Deep Cascade for', (3, 9)), ('Face Alignment ( DeCaFA )', (9, 14)), ('cascaded regression and', (16, 19)), ('end - to - end deep approaches', (19, 26)), ('landmark - wise attention maps', (29, 34)), ('local information', (37, 39)), ('current landmark estimate', (41, 44))]",[],[],[],[]
model,"We show that intermediate supervision with increasing weights helps DeCaFA to learn coarse attention maps in its early stages , that are refined in the later stages .","[('show', (1, 2)), ('with', (5, 6)), ('helps', (8, 9)), ('to learn', (10, 12)), ('in', (15, 16))]","[('intermediate supervision', (3, 5)), ('increasing weights', (6, 8)), ('DeCaFA', (9, 10)), ('coarse attention maps', (12, 15)), ('early stages', (17, 19))]",[],[],[],[]
model,"Through chaining multiple transfer layers , DeCaFA integrates heterogeneous data annotated with different numbers of landmarks and model the intrinsic relationship between these tasks .","[('Through chaining', (0, 2)), ('integrates', (7, 8)), ('annotated with', (10, 12)), ('model', (17, 18))]","[('multiple transfer layers', (2, 5)), ('DeCaFA', (6, 7)), ('heterogeneous data', (8, 10)), ('different numbers of landmarks', (12, 16)), ('intrinsic relationship', (19, 21)), ('tasks', (23, 24))]",[],[],[],[]
hyperparameters,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .","[('use', (8, 9)), ('each contains', (14, 16)), ('with', (21, 22)), ('for', (34, 35))]","[('DeCaFA models', (1, 3)), ('1 to 4 stages', (9, 13)), ('12 3 3 convolutional layers', (16, 21)), ('64 ? 64 ? 128 ? 128 ? 256 ? 256 channels', (22, 34)), ('downsampling portion', (36, 38)), ('upsampling portion', (45, 47))]",[],[],[],[]
hyperparameters,Each convolution is followed by a batch normalization layer with ReLU activation .,"[('followed by', (3, 5)), ('with', (9, 10))]","[('Each convolution', (0, 2)), ('batch normalization layer', (6, 9)), ('ReLU activation', (10, 12))]",[],[],[],[]
hyperparameters,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,"[('to generate', (2, 4)), ('do not use', (8, 11)), ('bilinear', (14, 15)), ('followed with', (17, 19))]","[('smooth feature maps', (4, 7)), ('transposed convolution', (11, 13)), ('image upsampling', (15, 17)), ('3 3 convolutional layers', (19, 23))]",[],[],[],[]
hyperparameters,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,"[('trained using', (4, 6)), ('with', (8, 9)), ('with', (15, 16)), ('annealing with', (21, 23))]","[('whole architecture', (1, 3)), ('ADAM optimizer', (6, 8)), ('5e ? 4 learning rate', (10, 15)), ('momentum 0.9', (16, 18)), ('learning rate', (19, 21)), ('power 0.9', (23, 25))]",[],[],[],[]
experiments,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .","[('apply', (1, 2)), ('with', (4, 5)), ('for', (8, 9))]","[('400000 updates', (2, 4)), ('batch size 8', (5, 8)), ('each database', (9, 11))]",[],[],[],[]
ablation-analysis,"The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .","[('as', (4, 5)), ('add', (6, 7)), ('after', (12, 13)), ('on', (15, 16))]","[('accuracy', (1, 2)), ('steadily increases', (2, 4)), ('more stages', (7, 9)), ('saturates', (11, 12)), ('third', (14, 15)), ('LFPW and HELEN', (16, 19))]",[],[],[],[]
ablation-analysis,"On IBUG , this difference is more conspicuous , thus there is for improvement by stacking more cascade stages .","[('On', (0, 1)), ('is', (5, 6)), ('by', (14, 15))]","[('IBUG', (1, 2)), ('difference', (4, 5)), ('more conspicuous', (6, 8)), ('improvement', (13, 14)), ('stacking', (15, 16)), ('more cascade stages', (16, 19))]",[],[],[],[]
ablation-analysis,"Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .","[('significantly helps', (7, 9))]","[('Coarsely annotated data ( 5 landmarks )', (0, 7)), ('fine - grained landmark localization', (10, 15))]",[],[],[],[]
ablation-analysis,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .","[('reinjecting', (2, 3)), ('significantly', (18, 19)), ('on', (22, 23)), ('such as', (25, 27))]","[('whole input image', (4, 7)), ('accuracy', (21, 22)), ('challenging data', (23, 25)), ('300 W - challenging', (27, 31)), ('WFLW - pose', (32, 35))]",[],[],[],[]
ablation-analysis,F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,"[('using', (15, 16)), ('rivals', (20, 21))]","[('local + global information', (16, 20)), ('basic deep approach', (22, 25))]",[],[],[],[]
ablation-analysis,"Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .","[('is', (14, 15)), ('by', (17, 18))]","[('F 5 - Equation fusion', (2, 7)), ('local and global cues', (10, 14)), ('best', (16, 17)), ('significant margin', (19, 21))]",[],[],[],[]
ablation-analysis,shows a comparison between DeCaFA and recent state - of - the - art approaches on 300W database .,"[('on', (15, 16))]","[('300W database', (16, 18))]",[],[],[],[]
ablation-analysis,"Our approach performs better than most existing approaches on the common subset , and performs very close to its best contenders on the challenging subset .","[('performs', (2, 3)), ('than', (4, 5)), ('on', (8, 9)), ('performs', (14, 15)), ('to', (17, 18)), ('on', (21, 22))]","[('Our approach', (0, 2)), ('better', (3, 4)), ('most existing approaches', (5, 8)), ('common subset', (10, 12)), ('very close', (15, 17)), ('its', (18, 19)), ('best contenders', (19, 21)), ('challenging subset', (23, 25))]",[],[],[],[]
ablation-analysis,"Note that DeCaFA trained only on 300 W trainset has a ME of 3.69 % and is already very competitive with recent approaches , thanks to its end - to - end cascade architecture .","[('trained only on', (3, 6)), ('of', (12, 13)), ('thanks to', (24, 26))]","[('DeCaFA', (2, 3)), ('300 W trainset', (6, 9)), ('ME', (11, 12)), ('3.69 %', (13, 15)), ('very competitive', (18, 20)), ('end - to - end cascade architecture', (27, 34))]",[],[],[],[]
ablation-analysis,"DeCaFA is competitive with the best approaches , LAB and DAN - MENPO as well as JMFA - MENPO , which also use external data .","[('with', (3, 4)), ('as well as', (13, 16)), ('use', (22, 23))]","[('DeCaFA', (0, 1)), ('competitive', (2, 3)), ('best approaches', (5, 7)), ('LAB and DAN - MENPO', (8, 13)), ('JMFA - MENPO', (16, 19)), ('external data', (23, 25))]",[],[],[],[]
ablation-analysis,DeCaFA performs better than LAB and Wing by a significant margin on every subset .,"[('performs', (1, 2)), ('than', (3, 4)), ('by', (7, 8)), ('on', (11, 12))]","[('DeCaFA', (0, 1)), ('better', (2, 3)), ('LAB and Wing', (4, 7)), ('significant margin', (9, 11)), ('every subset', (12, 14))]",[],[],[],[]
ablation-analysis,"Also , note that DeCaFA trained solely on WFLW already as a ME of 5.01 on the whole test set , which is still better that these two methods .","[('note', (2, 3)), ('trained solely on', (5, 8)), ('as', (10, 11)), ('of', (13, 14)), ('on', (15, 16))]","[('DeCaFA', (4, 5)), ('WFLW', (8, 9)), ('ME', (12, 13)), ('5.01', (14, 15)), ('whole test set', (17, 20)), ('still better', (23, 25))]",[],[],[],[]
ablation-analysis,"Method Mean error ( % ) SDM 4.35 CFSS 3,95 DSRN 3.08 AAN 2.99 DeCaFA 2.10 approach is the best by a significant margin .","[('is', (17, 18)), ('by', (20, 21))]","[('Method Mean error ( % )', (0, 6)), ('SDM', (6, 7)), ('best', (19, 20)), ('significant margin', (22, 24))]",[],[],[],[]
ablation-analysis,"Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .","[('sets', (3, 4)), ('on', (13, 14)), ('with', (17, 18))]","[('new state - of - the - art', (5, 13)), ('three databases', (15, 17)), ('several evaluation metrics', (18, 21))]",[],[],[],[]
ablation-analysis,"A allows to substantially improve the landmark localization on both datasets , most notably when the number of training images is very low .","[('on', (8, 9)), ('is', (20, 21))]","[('A', (0, 1)), ('substantially improve', (3, 5)), ('landmark localization', (6, 8)), ('both datasets', (9, 11)), ('number of training images', (16, 20)), ('very low', (21, 23))]",[],[],[],[]
ablation-analysis,"DeCaFA trained with 15 % of 300 W trainset and 6 % of WFLW trainset is on par with SAN on 300W ( , see ) , and is substantially better than DVLN on WFLW .","[('trained with', (1, 3)), ('with', (18, 19)), ('on', (20, 21)), ('than', (31, 32)), ('on', (33, 34))]","[('DeCaFA', (0, 1)), ('15 % of 300 W trainset', (3, 9)), ('6 % of WFLW trainset', (10, 15)), ('on par', (16, 18)), ('SAN', (19, 20)), ('300W', (21, 22)), ('substantially better', (29, 31)), ('DVLN', (32, 33)), ('WFLW', (34, 35))]",[],[],[],[]
results,"Also notice that the predicted landmarks are close to the corresponding ground truth , even in the presence of rotations and occlusions ( WFLW ) or facial expressions ( CelebA ) .","[('notice', (1, 2)), ('close to', (7, 9)), ('in the presence of', (15, 19))]","[('predicted landmarks', (4, 6)), ('corresponding ground truth', (10, 13)), ('rotations and occlusions ( WFLW )', (19, 25)), ('facial expressions ( CelebA )', (26, 31))]",[],[],[],[]
research-problem,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,[],"[('Robust Face Alignment', (4, 7))]",[],[],[],[]
code,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.,[],[],[],[],[],[]
research-problem,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .","[('known', (4, 5))]","[('Face alignment', (0, 2)), ('facial landmark localization', (6, 9))]",[],[],[],[]
research-problem,"Face alignment plays an essential role in many face related applications such as face recognition , face frontalization and 3D face reconstruction .",[],"[('Face alignment', (0, 2))]",[],[],[],[]
experiments,"As a result of i ) and ii ) , models trained with the MSE loss tend to predict a blurry and dilated heatmap with low intensity on foreground pixels compared to the ground truth ( .","[('trained with', (11, 13)), ('tend to predict', (16, 19)), ('with', (24, 25)), ('on', (27, 28)), ('compared to', (30, 32))]","[('models', (10, 11)), ('MSE loss', (14, 16)), ('blurry and dilated heatmap', (20, 24)), ('low intensity', (25, 27)), ('foreground pixels', (28, 30)), ('ground truth', (33, 35))]",[],[],[],[]
model,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .","[('propose', (2, 3)), ('name it', (8, 10)), ('able to', (18, 20)), ('of', (24, 25))]","[('new loss function', (4, 7)), ('Adaptive Wing loss', (10, 13)), ('significantly improve', (20, 22)), ('quality', (23, 24)), ('heatmap regression results', (25, 28))]",[],[],[],[]
model,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .","[('Due to', (0, 2)), ('of', (5, 6)), ('in', (9, 10)), ('such as', (19, 21)), ('for', (41, 42))]","[('translation invariance', (3, 5)), ('convolution operation', (7, 9)), ('bottom - up and top - down CNN structures', (10, 19)), ('stacked Hourglass ( HG )', (21, 26)), ('network', (28, 29)), ('coordinate information', (33, 35)), ('facial landmark localization', (42, 45))]",[],[],[],[]
model,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .","[('Inspired by', (0, 2)), ('encode into', (14, 16)), ('only', (25, 26)), ('on', (26, 27)), ('predicted from', (28, 30)), ('into', (34, 35))]","[('Coord - Conv layer', (3, 7)), ('our model', (16, 18)), ('full coordinate information', (19, 22)), ('information', (24, 25)), ('boundaries', (27, 28)), ('previous HG module', (31, 34)), ('our model', (35, 37))]",[],[],[],[]
model,The encoded coordinate information further improves the performance of our approach .,"[('further improves', (4, 6)), ('of', (8, 9))]","[('encoded coordinate information', (1, 4)), ('performance', (7, 8)), ('our approach', (9, 11))]",[],[],[],[]
model,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .","[('To encode', (0, 2)), ('add', (7, 8)), ('of', (10, 11)), ('by concatenating', (13, 15)), ('into', (19, 20)), ('jointly trained with', (26, 29))]","[('boundary coordinates', (2, 4)), ('sub-task', (9, 10)), ('boundary prediction', (11, 13)), ('additional boundary channel', (16, 19)), ('ground truth heatmap', (21, 24)), ('other channels', (29, 31))]",[],[],[],[]
model,With proposed Weighted Loss Map it is also able to focus on foreground pixels and difficult background pixels during training .,"[('With', (0, 1)), ('during', (18, 19))]","[('proposed Weighted Loss Map', (1, 5)), ('focus', (10, 11)), ('foreground pixels', (12, 14)), ('difficult background pixels', (15, 18)), ('training', (19, 20))]",[],[],[],[]
model,"Encode coordinate information , including coordinates on boundary , into the face alignment algorithm using CoordConv .","[('Encode', (0, 1)), ('including', (4, 5)), ('on', (6, 7)), ('into', (9, 10)), ('using', (14, 15))]","[('coordinate information', (1, 3)), ('coordinates', (5, 6)), ('boundary', (7, 8)), ('face alignment algorithm', (11, 14)), ('CoordConv', (15, 16))]",[],[],[],[]
experiments,"The reduced influence of correct estimations helps the network to stay converged , instead of oscillating like the L1 and the Wing loss .","[('of', (3, 4)), ('helps', (6, 7)), ('to stay', (9, 11)), ('instead of', (13, 15)), ('like', (16, 17))]","[('reduced influence', (1, 3)), ('correct estimations', (4, 6)), ('network', (8, 9)), ('converged', (11, 12)), ('oscillating', (15, 16)), ('L1 and the Wing loss', (18, 23))]",[],[],[],[]
ablation-analysis,"Difficult background pixels should also be focused on since these pixels are relatively difficult to regress , accurately regressing them could help narrow down the area of foreground pixels to improve localization accuracy . :","[('accurately', (17, 18)), ('to improve', (29, 31))]","[('Difficult background pixels', (0, 3)), ('focused', (6, 7)), ('area of', (25, 27)), ('foreground pixels', (27, 29)), ('localization accuracy', (31, 33))]",[],[],[],[]
hyperparameters,"For the WFLW dataset , the provided bounding boxes are not very accurate , to ensure all landmarks are preserved from cropping , we enlarge the bounding boxes by 10 % on both dimensions .","[('For', (0, 1)), ('are', (9, 10)), ('to ensure', (14, 16)), ('preserved from', (19, 21)), ('enlarge', (24, 25)), ('by', (28, 29)), ('on', (31, 32))]","[('WFLW dataset', (2, 4)), ('not very accurate', (10, 13)), ('all landmarks', (16, 18)), ('cropping', (21, 22)), ('bounding boxes', (26, 28)), ('10 %', (29, 31)), ('both dimensions', (32, 34))]",[],[],[],[]
hyperparameters,"The input of the network is 256 256 , the output of each stacked HG is 64 64 .","[('input of', (1, 3)), ('is', (5, 6)), ('output of', (10, 12)), ('is', (15, 16))]","[('network', (4, 5)), ('256 256', (6, 8)), ('each stacked HG', (12, 15)), ('64 64', (16, 18))]",[],[],[],[]
hyperparameters,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .","[('During', (0, 1)), ('use', (4, 5)), ('with', (8, 9)), ('of', (13, 14))]","[('training', (1, 2)), ('RM - SProp', (5, 8)), ('initial learning rate', (10, 13)), ('1 10 ?4', (14, 17))]",[],[],[],[]
hyperparameters,We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,"[('set', (1, 2)), ('to be', (4, 6)), ('to be', (15, 17))]","[('momentum', (3, 4)), ('0', (6, 7)), ('weight decay', (13, 15)), ('1 10 ?5', (17, 20))]",[],[],[],[]
hyperparameters,"We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .","[('train for', (1, 3)), ('reduced to', (11, 13)), ('after', (21, 22))]","[('240 epoches', (3, 5)), ('learning rate', (8, 10)), ('1 10 ?5 and 1 10 ? 6', (13, 21)), ('80 and 160 epoches', (22, 26))]",[],[],[],[]
hyperparameters,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .","[('performed with', (3, 5))]","[('Data augmentation', (0, 2)), ('random rotation ( 50 )', (5, 10)), ('translation ( 25 px )', (11, 16)), ('flipping ( 50 % )', (17, 22)), ('rescaling ( 15 % )', (24, 29))]",[],[],[],[]
hyperparameters,"Random Gaussian blur , noise and occlusion are also used .",[],"[('Random Gaussian blur', (0, 3)), ('noise and occlusion', (4, 7))]",[],[],[],[]
results,"14 . Our approach outperforms previous state - of - the - art by a significant margin , especially on the failure rate .","[('by', (13, 14)), ('especially on', (18, 20))]","[('Our approach', (2, 4)), ('outperforms', (4, 5)), ('previous state - of - the - art', (5, 13)), ('significant margin', (15, 17)), ('failure rate', (21, 23))]",[],[],[],[]
experiments,We are able to reduce the failure rate measured at 10 % NME from 3.73 % to 0.99 % .,"[('measured at', (8, 10)), ('from', (13, 14)), ('to', (16, 17))]","[('reduce', (4, 5)), ('failure rate', (6, 8)), ('10 % NME', (10, 13)), ('3.73 %', (14, 16)), ('0.99 %', (17, 19))]",[],[],[],[]
results,Our performance on the COFW shows the robustness of our approach against faces with large pose and heavy occlusion .,"[('on', (2, 3)), ('shows', (5, 6)), ('of', (8, 9)), ('against', (11, 12)), ('with', (13, 14))]","[('COFW', (4, 5)), ('robustness', (7, 8)), ('our approach', (9, 11)), ('faces', (12, 13)), ('large pose and heavy occlusion', (14, 19))]",[],[],[],[]
results,"Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .","[('able to achieve', (3, 6)), ('on', (15, 16))]","[('Our', (0, 1)), ('state - of - the - art performance', (7, 15)), ('300W testing dataset', (17, 20))]",[],[],[],[]
results,"For the challenge subset ( iBug dataset ) , we are able to outperform","[('For', (0, 1)), ('able to', (11, 13))]","[('challenge subset ( iBug dataset )', (2, 8))]",[],[],[],[]
results,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .","[('achieves', (3, 4)), ('on', (7, 8)), ('than', (18, 19))]","[('Our method', (0, 2)), ('best results', (5, 7)), ('WFLW dataset', (9, 11)), ('significantly more difficult', (15, 18)), ('COFW and 300W', (19, 22))]",[],[],[],[]
results,On every subset we outperform the previous state - of - the - art ap - :,"[('On', (0, 1))]","[('every subset', (1, 3)), ('outperform', (4, 5)), ('previous state - of - the - art', (6, 14))]",[],[],[],[]
results,We are also able to reduce the failure rate and increase the AUC dramatically and hence improving the overall localization quality significantly .,[],"[('reduce', (5, 6)), ('failure rate', (7, 9)), ('increase', (10, 11)), ('AUC', (12, 13)), ('dramatically', (13, 14)), ('improving', (16, 17)), ('overall localization quality', (18, 21)), ('significantly', (21, 22))]",[],[],[],[]
results,"All in all , our approach fails on only 2.84 % of all images , more than a two times improvement compared with 7.6 .","[('on', (7, 8)), ('of', (11, 12))]","[('our approach', (4, 6)), ('fails', (6, 7)), ('only 2.84 %', (8, 11)), ('all images', (12, 14))]",[],[],[],[]
results,Note the baseline model ( model trained with MSE ) underperforms the state - of - theart .,"[('Note', (0, 1))]","[('baseline model ( model trained', (2, 7)), ('MSE )', (8, 10)), ('underperforms', (10, 11)), ('state - of - theart', (12, 17))]",[],[],[],[]
results,"To compare with a naive weight mask without focus on hard negative pixels , we introduced a baseline weight map W M base =? W + 1 , where W = 10 . The major contribution comes from Adaptive Wing loss , which improves the benchmark by 0.74 % .","[('compare with', (1, 3)), ('without focus on', (7, 10)), ('introduced', (15, 16)), ('where', (28, 29)), ('improves', (43, 44)), ('by', (46, 47))]","[('naive weight mask', (4, 7)), ('hard negative pixels', (10, 13)), ('baseline weight map W M base =? W + 1', (17, 27)), ('Adaptive Wing loss', (38, 41)), ('benchmark', (45, 46)), ('0.74 %', (47, 49))]",[],[],[],[]
results,"All other modules contributed incrementally to the localization performance , our Weighted Loss Map improves 0.25 % , boundary prediction and coordinates encoding are able to contribute another 0.09 % .","[('contributed', (3, 4)), ('improves', (14, 15)), ('able to contribute', (24, 27))]","[('All other modules', (0, 3)), ('incrementally', (4, 5)), ('localization performance', (7, 9)), ('our Weighted Loss Map', (10, 14)), ('0.25 %', (15, 17)), ('boundary prediction and coordinates encoding', (18, 23)), ('another', (27, 28)), ('0.09 %', (28, 30))]",[],[],[],[]
results,"Our proposed Adaptive Wing loss significantly boosts performance compared with MSE , which proves the general applicability of the proposed Adaptive Wing loss on more heatmap regression tasks .","[('compared with', (8, 10))]","[('Our proposed Adaptive Wing loss', (0, 5)), ('significantly boosts', (5, 7)), ('performance', (7, 8)), ('MSE', (10, 11))]",[],[],[],[]
results,"Even with only one HG block , our approach still outperforms previous state - of - the - arts in all datasets except the common subset and the full dataset of 300W .","[('in', (19, 20)), ('except', (22, 23))]","[('our approach', (7, 9)), ('outperforms', (10, 11)), ('previous state - of - the - arts', (11, 19)), ('all datasets', (20, 22)), ('common subset', (24, 26)), ('full dataset of', (28, 31)), ('300W', (31, 32))]",[],[],[],[]
experiments,Runtime is evaluated on Nvidia GTX 1080 Ti graphics card with batch size of 1 .,"[('evaluated on', (2, 4)), ('with', (10, 11))]","[('Runtime', (0, 1)), ('Nvidia GTX 1080 Ti graphics card', (4, 10)), ('batch size of 1', (11, 15))]",[],[],[],[]
research-problem,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network,[],"[('Facial Landmarks Detection', (0, 3))]",[],[],[],[]
research-problem,"It aims to detect the facial landmarks such as eyes , nose and mouth , namely predicting the location parameters of landmarks .","[('aims to detect', (1, 4)), ('such as', (7, 9)), ('namely', (15, 16)), ('of', (20, 21))]","[('facial landmarks', (5, 7)), ('eyes , nose and mouth', (9, 14)), ('location parameters', (18, 20)), ('landmarks', (21, 22))]",[],[],[],[]
research-problem,Researchers usually regard this task as atypical non -linear least squares problem .,[],[],[],[],[],[]
model,( b ) Self - Iterative Regression . :,[],"[('Self - Iterative Regression', (3, 7))]",[],[],[],[]
model,"To predict the landmarks ' location parameters , the CR based methods require multiple regressors , while SIR just need one regressor and updates parameters iteratively .","[('To predict', (0, 2)), ('require', (12, 13)), ('need', (19, 20)), ('updates', (23, 24))]","[(""landmarks ' location parameters"", (3, 7)), ('CR based methods', (9, 12)), ('multiple regressors', (13, 15)), ('SIR', (17, 18)), ('one regressor', (20, 22)), ('parameters', (24, 25))]",[],[],[],[]
model,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .","[('develop', (5, 6))]","[('Self - Iterative Regression ( SIR ) framework', (7, 15))]",[],[],[],[]
model,"The training data is obtained by random sampling in the parameter space , and in the test - ing process , parameters are updated iteratively by calling the same regressor , which is dubbed Self - Iterative Regression .","[('obtained by', (4, 6)), ('in', (8, 9)), ('in', (14, 15)), ('updated', (23, 24)), ('by calling', (25, 27)), ('which is', (31, 33))]","[('training data', (1, 3)), ('random sampling', (6, 8)), ('parameter space', (10, 12)), ('test - ing process', (16, 20)), ('parameters', (21, 22)), ('iteratively', (24, 25)), ('same regressor', (28, 30)), ('dubbed Self - Iterative Regression', (33, 38))]",[],[],[],[]
model,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .","[('to obtain', (2, 4)), ('proposed', (9, 10)), ('focuses on', (20, 22))]","[('discriminative landmarks features', (4, 7)), ('Landmarks - Attention Network ( LAN )', (11, 18)), ('appearance around', (23, 25)), ('landmarks', (25, 26))]",[],[],[],[]
model,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .","[('concurrently extracts', (2, 4)), ('obtains', (10, 11)), ('of', (20, 21))]","[(""local landmarks ' features"", (4, 8)), ('holistic increment', (12, 14)), ('significantly reduces', (16, 18)), ('final feature', (22, 24))]",[],[],[],[]
model,"2 . The Landmarks - Attention Network ( LAN ) is developed to independently learn discriminative features around each landmarks , which significantly reduces the dimension of feature layer and the number of model parameters .","[('developed to independently', (11, 14)), ('around', (17, 18))]","[('Landmarks - Attention Network ( LAN )', (3, 10)), ('discriminative features', (15, 17)), ('each landmarks', (18, 20))]",[],[],[],[]
experiments,"As illustrated in , SIR is more robust than CR because the former can cover more training space and is n't affected by the optimization path .","[('than', (8, 9))]","[('SIR', (4, 5)), ('more robust', (6, 8)), ('CR', (9, 10))]",[],[],[],[]
experiments,"Once one regressor predicts the false direction , the final result is prone to drift away ; ( b ) SIR Descent Direction Map : the training space of SIR includes distribution from coarse stages to fine stages and all descent directions are pointed to ground truth .","[('predicts', (3, 4)), ('prone to', (12, 14)), ('pointed to', (43, 45))]","[('one regressor', (1, 3)), ('false direction', (5, 7)), ('final result', (9, 11)), ('drift away', (14, 16)), ('SIR Descent Direction Map', (20, 24))]",[],[],[],[]
results,The NME results shows that SIR performs comparatively with RAR ) and outperform other existing methods .,"[('shows', (3, 4)), ('performs', (6, 7)), ('with', (8, 9))]","[('NME results', (1, 3)), ('SIR', (5, 6)), ('comparatively', (7, 8)), ('RAR', (9, 10)), ('outperform', (12, 13)), ('other existing methods', (13, 16))]",[],[],[],[]
results,Comparison with Cascaded Regression,[],[],[],[],[],[]
results,"Different from them , our method obtains state - of - the - art performance by iterative call the same regressor rather than adding anymore regressors .","[('obtains', (6, 7)), ('by iterative', (15, 17)), ('rather than adding', (21, 24))]","[('our method', (4, 6)), ('state - of - the - art performance', (7, 15)), ('same regressor', (19, 21)), ('anymore regressors', (24, 26))]",[],[],[],[]
research-problem,Look at Boundary : A Boundary - Aware Face Alignment Algorithm,[],[],[],[],[],[]
research-problem,We present a novel boundary - aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation .,"[('present', (1, 2)), ('by utilising', (10, 12)), ('as', (14, 15)), ('of', (18, 19)), ('to help', (22, 24))]","[('novel', (3, 4)), ('boundary - aware face alignment algorithm', (4, 10)), ('boundary lines', (12, 14)), ('geometric structure', (16, 18)), ('human face', (20, 22)), ('facial landmark localisation', (24, 27))]",[],[],[],[]
research-problem,"By utilising boundary information of 300 - W dataset , our method achieves 3.92 % mean error with 0.39 % failure rate on COFW dataset , and 1.25 % mean error on AFLW - Full dataset .","[('utilising', (1, 2)), ('of', (4, 5)), ('achieves', (12, 13)), ('with', (17, 18)), ('on', (22, 23)), ('on', (31, 32))]","[('boundary information', (2, 4)), ('300 - W dataset', (5, 9)), ('our method', (10, 12)), ('3.92 % mean error', (13, 17)), ('0.39 % failure rate', (18, 22)), ('COFW dataset', (23, 25)), ('1.25 % mean error', (27, 31)), ('AFLW - Full dataset', (32, 36))]",[],[],[],[]
dataset,"In this work , we represent facial structure using 13 boundary lines .","[('represent', (5, 6)), ('using', (8, 9))]","[('facial structure', (6, 8)), ('13 boundary lines', (9, 12))]",[],[],[],[]
model,"Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets , which will not suffer from inconsistency of the annotation schemes .","[('interpolated from', (6, 8)), ('across', (14, 15)), ('of', (24, 25))]","[('Each facial boundary line', (0, 4)), ('sufficient number of facial landmarks', (9, 14)), ('multiple datasets', (15, 17)), ('inconsistency', (23, 24)), ('annotation schemes', (26, 28))]",[],[],[],[]
model,Our boundary - aware face alignment algorithm contains two stages .,"[('contains', (7, 8))]","[('boundary - aware face alignment algorithm', (1, 7)), ('two stages', (8, 10))]",[],[],[],[]
model,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,"[('estimate', (2, 3)), ('with the help of', (10, 14))]","[('facial boundary heatmaps', (3, 6)), ('regress', (8, 9)), ('landmarks', (9, 10)), ('boundary heatmaps', (14, 16))]",[],[],[],[]
model,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .","[('introduce', (11, 12)), ('by using', (15, 17))]","[('facial', (5, 6)), ('adversarial learning ideas', (12, 15)), ('landmark - based boundary effectiveness discriminator', (18, 24))]",[],[],[],[]
model,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .","[('can be', (12, 14)), ('in', (16, 17))]","[('boundary heatmap estimator', (1, 4)), ('landmark regressor', (5, 7)), ('boundary effectiveness discriminator', (9, 12)), ('jointly', (14, 15)), ('end - to - end manner', (18, 24))]",[],[],[],[]
model,"After generating facial boundary heatmaps , the next step is deriving facial landmarks using boundaries .","[('generating', (1, 2)), ('deriving', (10, 11)), ('using', (13, 14))]","[('facial boundary heatmaps', (2, 5)), ('next', (7, 8)), ('facial landmarks', (11, 13)), ('boundaries', (14, 15))]",[],[],[],[]
model,"To fully utilise the structure information , we apply boundary heatmaps at multiple stages in the landmark regression network .","[('To fully utilise', (0, 3)), ('apply', (8, 9)), ('at', (11, 12)), ('in', (14, 15))]","[('structure information', (4, 6)), ('boundary heatmaps', (9, 11)), ('multiple stages', (12, 14)), ('landmark regression network', (16, 19))]",[],[],[],[]
dataset,Each image is annotated with 98 landmarks and 6 attributes .,"[('annotated with', (3, 5))]","[('Each image', (0, 2)), ('98 landmarks', (5, 7)), ('6 attributes', (8, 10))]",[],[],[],[]
tasks,AFLW dataset : AFLW contains 24386 in - the - wild faces with large head pose up to 120 for yaw and 90 for pitch and roll .,"[('contains', (4, 5)), ('with', (12, 13)), ('up to', (16, 18)), ('for', (19, 20))]","[('AFLW dataset', (0, 2)), ('24386 in - the - wild faces', (5, 12)), ('large head pose', (13, 16)), ('120', (18, 19)), ('yaw', (20, 21)), ('90', (22, 23)), ('pitch and roll', (24, 27))]",[],[],[],[]
experimental-setup,All our models are trained with Caffe [ 24 ] on 4 Titan X GPUs .,"[('trained with', (4, 6)), ('on', (10, 11))]","[('Caffe [ 24 ]', (6, 10)), ('4 Titan X GPUs', (11, 15))]",[],[],[],[]
results,Our method performs best among all of the state - of - the - art methods .,"[('performs', (2, 3)), ('among', (4, 5))]","[('Our method', (0, 2)), ('best', (3, 4)), ('all of the state - of - the - art methods', (5, 16))]",[],[],[],[]
results,shows the CED curves of our method against state - of - the - art methods on the COFW - 68 dataset .,"[('shows', (0, 1)), ('of', (4, 5)), ('against', (7, 8)), ('on', (16, 17))]","[('CED curves', (2, 4)), ('our', (5, 6)), ('state - of - the - art methods', (8, 16)), ('COFW - 68 dataset', (18, 22))]",[],[],[],[]
results,Our model outperforms previous results with a large margin .,"[('with', (5, 6))]","[('Our model', (0, 2)), ('outperforms', (2, 3)), ('previous results', (3, 5)), ('large margin', (7, 9))]",[],[],[],[]
results,We achieve 4.62 % mean error with 2.17 % failure rate .,"[('achieve', (1, 2)), ('with', (6, 7))]","[('4.62 % mean error', (2, 6)), ('2.17 % failure rate', (7, 11))]",[],[],[],[]
results,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .","[('is', (3, 4)), ('by', (6, 7))]","[('failure rate', (1, 3)), ('significantly reduced', (4, 6)), ('3.75 %', (7, 9))]",[],[],[],[]
results,There is a clear boost between our method without and with using boundary information .,"[('between', (5, 6)), ('without and with using', (8, 12))]","[('clear boost', (3, 5)), ('our method', (6, 8)), ('boundary information', (12, 14))]",[],[],[],[]
results,"Moreover , our method uses boundary information achieves 29 % , 32 % and 29 % relative performance improve- ment over the baseline method ( "" LAB without boundary "" ) on COFW - 29 , AFLW - Full and AFLW - Frontal respectively .","[('uses', (4, 5)), ('achieves', (7, 8)), ('over', (20, 21)), ('on', (31, 32))]","[('our method', (2, 4)), ('boundary information', (5, 7)), ('29 % , 32 % and 29 % relative performance improve- ment', (8, 20)), ('baseline method ( "" LAB without boundary', (22, 29)), ('COFW - 29', (32, 35)), ('AFLW - Full', (36, 39)), ('AFLW - Frontal', (40, 43))]",[],[],[],[]
ablation-analysis,"Our framework consists of several pivotal components , i.e. , boundary information fusion , message passing and adversarial learning .","[('consists of', (2, 4)), ('i.e.', (8, 9))]","[('Our', (0, 1)), ('several pivotal components', (4, 7)), ('boundary information fusion', (10, 13)), ('message passing', (14, 16)), ('adversarial learning', (17, 19))]",[],[],[],[]
ablation-analysis,"As indicated in , our final model that fuses boundary information in all four levels improves mean error from 7.12 % to 6.13 % .","[('fuses', (8, 9)), ('in', (11, 12)), ('improves', (15, 16)), ('from', (18, 19)), ('to', (21, 22))]","[('our final model', (4, 7)), ('boundary information', (9, 11)), ('all four levels', (12, 15)), ('mean error', (16, 18)), ('7.12 %', (19, 21)), ('6.13 %', (22, 24))]",[],[],[],[]
model,"In this paper , we present a novel use of facial boundary to derive facial landmarks .","[('present', (5, 6)), ('of', (9, 10)), ('to derive', (12, 14))]","[('novel use', (7, 9)), ('facial boundary', (10, 12)), ('facial landmarks', (14, 16))]",[],[],[],[]
model,The runtime of our algorithm is 60 ms on TITAN X GPU .,"[('of', (2, 3)), ('is', (5, 6)), ('on', (8, 9))]","[('runtime', (1, 2)), ('our algorithm', (3, 5)), ('60 ms', (6, 8)), ('TITAN X GPU', (9, 12))]",[],[],[],[]
research-problem,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees,[],"[('Face Alignment', (0, 2))]",[],[],[],[]
research-problem,"In this paper we present 3DDE , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ensembles of regression trees .","[('present', (4, 5)), ('based on', (14, 16)), ('of', (23, 24)), ('of', (25, 26))]","[('3DDE', (5, 6)), ('robust', (8, 9)), ('coarse - to - fine cascade', (17, 23)), ('ensembles', (24, 25)), ('regression trees', (26, 28))]",[],[],[],[]
research-problem,"In the experiments performed , 3 DDE improves the state - of - the - art in 300W , COFW , AFLW and WFLW data sets .","[('improves', (7, 8)), ('in', (16, 17))]","[('3 DDE', (5, 7)), ('state - of - the - art', (9, 16)), ('300W , COFW , AFLW and WFLW data sets', (17, 26))]",[],[],[],[]
model,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .","[('present', (4, 5)), ('based on', (24, 26)), ('of', (33, 34))]","[('3 DDE ( 3D Deeply - initialized Ensemble ) regressor', (6, 16)), ('robust and efficient face alignment algorithm', (18, 24)), ('coarse - to - fine cascade', (27, 33))]",[],[],[],[]
model,"It is a hybrid approach that inherits good properties of ERT , such as the ability to impose a face shape prior , and the robustness of deep models .","[('is', (1, 2)), ('inherits', (6, 7)), ('of', (9, 10)), ('such as', (12, 14))]","[('hybrid approach', (3, 5)), ('good properties', (7, 9)), ('ERT', (10, 11)), ('ability', (15, 16)), ('face shape', (19, 21)), ('robustness of', (25, 27)), ('deep models', (27, 29))]",[],[],[],[]
model,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,"[('initialized by', (2, 4)), ('to', (11, 12)), ('produced by', (15, 17))]","[('robustly fitting', (4, 6)), ('3 D face model', (7, 11)), ('probability maps', (13, 15)), ('CNN', (18, 19))]",[],[],[],[]
model,"With this initialization we tackle one of the main drawbacks of ERT , namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations .","[('tackle', (4, 5)), ('of', (10, 11)), ('in the presence of', (20, 24))]","[('ERT', (11, 12)), ('difficulty', (15, 16)), ('initializing', (17, 18)), ('regressor', (19, 20)), ('occlusions and large face rotations', (24, 29))]",[],[],[],[]
model,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .","[('imposes', (8, 9)), ('on', (13, 14)), ('addressing', (17, 18)), ('of', (20, 21)), ('when', (23, 24)), ('are', (29, 30))]","[('ERT', (6, 7)), ('prior face shape', (10, 13)), ('solution', (15, 16)), ('shortcomings', (19, 20)), ('deep models', (21, 23)), ('occlusions and ambiguous face configurations', (24, 29)), ('present', (30, 31))]",[],[],[],[]
model,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .","[('tackles', (9, 10)), ('of', (13, 14)), ('of', (23, 24)), ('using', (25, 26))]","[('coarse - to - fine structure', (3, 9)), ('combinatorial explosion', (11, 13)), ('parts deformation', (14, 16)), ('key limitation', (21, 23)), ('approaches', (24, 25)), ('shape constraints', (26, 28))]",[],[],[],[]
model,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,"[('improve', (2, 3)), ('by using', (5, 7)), ('in the presence of', (16, 20))]","[('initialization', (4, 5)), ('RANSAC - like procedure', (8, 12)), ('robustness', (15, 16)), ('occlusions', (20, 21))]",[],[],[],[]
experimental-setup,"For each data set , we train from scratch the CNN selecting the model parameters with lowest validation error .","[('For', (0, 1)), ('train from', (6, 8)), ('selecting', (11, 12)), ('with', (15, 16))]","[('each data set', (1, 4)), ('CNN', (10, 11)), ('model parameters', (13, 15)), ('lowest validation error', (16, 19))]",[],[],[],[]
experimental-setup,We crop faces using the ground truth bounding boxes annotations enlarged by 30 % .,"[('crop', (1, 2)), ('using', (3, 4)), ('enlarged by', (10, 12))]","[('faces', (2, 3)), ('ground truth bounding boxes annotations', (5, 10)), ('30 %', (12, 14))]",[],[],[],[]
experimental-setup,"We generate different training samples in each epoch by applying random in plane rotations between 45 , scale changes by 15 % and translations by 5 % of bounding box size , randomly mirroring images horizontally and generating random rectangular occlusions .","[('generate', (1, 2)), ('in', (5, 6)), ('by applying', (8, 10)), ('in', (11, 12)), ('between', (14, 15)), ('by', (19, 20)), ('by', (24, 25)), ('of', (27, 28)), ('randomly mirroring', (32, 34)), ('generating', (37, 38))]","[('different training samples', (2, 5)), ('each epoch', (6, 8)), ('random', (10, 11)), ('plane rotations', (12, 14)), ('45', (15, 16)), ('scale changes', (17, 19)), ('15 %', (20, 22)), ('translations', (23, 24)), ('5 %', (25, 27)), ('bounding box size', (28, 31)), ('images', (34, 35)), ('horizontally', (35, 36)), ('random rectangular occlusions', (38, 41))]",[],[],[],[]
experimental-setup,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .","[('use', (1, 2)), ('with', (5, 6))]","[('Adam stochastic optimization', (2, 5)), ('? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters', (6, 22))]",[],[],[],[]
experimental-setup,We train until convergence with an initial learning rate ? = 0.001 .,"[('train', (1, 2)), ('until', (2, 3)), ('with', (4, 5))]","[('convergence', (3, 4)), ('initial learning rate ? = 0.001', (6, 12))]",[],[],[],[]
experimental-setup,"When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .","[('levels', (3, 4)), ('multiply', (10, 11)), ('by', (14, 15)), ('=', (16, 17))]","[('validation error', (1, 3)), ('out', (4, 5)), ('10 epochs', (6, 8)), ('learning rate', (12, 14)), ('decay', (15, 16)), ('0.05', (17, 18))]",[],[],[],[]
experimental-setup,In the CNN the cropped input face is reduced from 160160 to 11 pixels gradually dividing by half their size across B = 8 branches applying astride 2 convolution with kernel size 22 1 .,"[('In', (0, 1)), ('reduced from', (8, 10)), ('to', (11, 12)), ('gradually dividing by', (14, 17)), ('across', (20, 21)), ('applying', (25, 26)), ('with', (29, 30))]","[('CNN', (2, 3)), ('cropped input face', (4, 7)), ('160160', (10, 11)), ('half their size', (17, 20)), ('B = 8 branches', (21, 25)), ('astride 2 convolution', (26, 29)), ('kernel', (30, 31))]",[],[],[],[]
experimental-setup,We apply batch normalization after each convolution .,"[('apply', (1, 2)), ('after', (4, 5))]","[('batch normalization', (2, 4)), ('each convolution', (5, 7))]",[],[],[],[]
experimental-setup,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .","[('apply', (1, 2)), ('with', (5, 6)), ('to', (9, 10)), ('to stabilize', (14, 16))]","[('Gaussian filter', (3, 5)), ('? = 33', (6, 9)), ('output probability maps', (11, 14)), ('initialization', (17, 18))]",[],[],[],[]
experimental-setup,The depth of trees is set to 4 .,"[('set to', (5, 7))]","[('depth of', (1, 3)), ('trees', (3, 4)), ('4', (7, 8))]",[],[],[],[]
experimental-setup,"The number of tests to choose the best split parameters , ? , is set to 200 .","[('to choose', (4, 6)), ('set to', (14, 16))]","[('number of tests', (1, 4)), ('best split parameters', (7, 10)), ('200', (16, 17))]",[],[],[],[]
experimental-setup,We resize each image to set the face size to 160160 pixels .,"[('resize', (1, 2)), ('to set', (4, 6)), ('to', (9, 10))]","[('each image', (2, 4)), ('face size', (7, 9)), ('160160 pixels', (10, 12))]",[],[],[],[]
experimental-setup,"For feature extraction , the FREAK pattern diameter is reduced gradually in each stage ( i.e. , in the last stages the pixel pairs for each feature are closer ) .","[('For', (0, 1)), ('reduced', (9, 10)), ('in', (11, 12))]","[('feature extraction', (1, 3)), ('FREAK pattern diameter', (5, 8)), ('gradually', (10, 11)), ('each stage', (12, 14))]",[],[],[],[]
experimental-setup,We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,"[('generate', (1, 2)), ('in', (6, 7)), ('of', (12, 13))]","[('Z = 25 initializations', (2, 6)), ('robust soft POSIT scheme', (8, 12)), ('g 0', (13, 15))]",[],[],[],[]
experimental-setup,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,"[('To avoid', (0, 2)), ('use', (4, 5)), ('in', (17, 18))]","[('overfitting', (2, 3)), ('shrinkage factor ? = 0.1', (6, 11)), ('subsampling factor ? = 0.5', (12, 17)), ('ERT', (19, 20))]",[],[],[],[]
experimental-setup,"Training the CNN and the coarse - to - fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20 GHz ( 210 cores / 20 threads , 128 GB of RAM ) with a batch size of 32 images .","[('takes', (13, 14)), ('using', (16, 17)), ('at', (36, 37)), ('with', (51, 52))]","[('Training', (0, 1)), ('CNN and the coarse - to - fine ensemble of trees', (2, 13)), ('48 hours', (14, 16)), ('NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU', (18, 28)), ('dual Intel Xeon Silver 4114 CPU', (30, 36)), ('2.20 GHz', (37, 39)), ('batch size', (53, 55)), ('32', (56, 57))]",[],[],[],[]
baselines,"The selected algorithms are representative of the three main families of solutions : a ) ensembles of regression trees ( c GPRT , RCPR , ERT ) , b) CNN - based approaches ( LAB , DAN , RCN ) and c ) mixed approaches with deep nets and ensembles of regression trees ( 3DDE , DCFE ) .","[('representative of', (4, 6)), ('with', (45, 46))]","[('three main families of solutions', (7, 12)), ('ensembles of regression trees', (15, 19)), ('CNN - based approaches (', (29, 34)), ('LAB , DAN , RCN', (34, 39)), ('mixed approaches', (43, 45)), ('deep nets and ensembles of regression trees', (46, 53)), ('3DDE', (54, 55))]",[],[],[],[]
results,"Overall , 3 DDE is better than any other providing a public implementation in the literature .","[('is', (4, 5)), ('than', (6, 7)), ('providing', (9, 10))]","[('3 DDE', (2, 4)), ('better', (5, 6)), ('any', (7, 8)), ('public implementation', (11, 13))]",[],[],[],[]
results,"In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .","[('able to', (4, 6)), ('improve by', (6, 8)), ('as', (14, 15))]","[('large margin', (9, 11)), ('other ERT methods', (11, 14)), ('RCPR', (15, 16)), ('ERT or c GPRT', (17, 21))]",[],[],[],[]
results,"We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .","[('without', (5, 6))]","[('outperform', (2, 3)), ('RCN', (3, 4))]",[],[],[],[]
results,Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition ( see ) and in the full subset of the 300W public test set ( see ) .,"[('obtains', (2, 3)), ('in', (7, 8)), ('of', (13, 14)), ('of', (25, 26))]","[('Our approach', (0, 2)), ('best overall performance', (4, 7)), ('indoor and outdoor subsets', (9, 13)), ('private competition', (15, 17)), ('full subset', (23, 25)), ('300W public test set', (27, 31))]",[],[],[],[]
results,"In the challenging subset of the 300W public competition , SHN gets better results than 3DDE .","[('In', (0, 1)), ('of', (4, 5)), ('gets', (11, 12)), ('than', (14, 15))]","[('challenging subset', (2, 4)), ('300W public competition', (6, 9)), ('SHN', (10, 11)), ('better results', (12, 14)), ('3DDE', (15, 16))]",[],[],[],[]
results,"3 DDE obtains the best results , NME 5.11 , establishing anew state - of - the - art .","[('obtains', (2, 3)), ('establishing', (10, 11))]","[('DDE', (1, 2)), ('best results', (4, 6)), ('NME 5.11', (7, 9))]",[],[],[],[]
results,"In terms of landmark visibility estimation , we have obtained better precision with an overall better recall than the best previous approach , DCFE .","[('In terms of', (0, 3)), ('obtained', (9, 10)), ('with', (12, 13)), ('than', (17, 18))]","[('landmark visibility estimation', (3, 6)), ('better precision', (10, 12)), ('overall better recall', (14, 17)), ('best previous approach', (19, 22)), ('DCFE', (23, 24))]",[],[],[],[]
experiments,"Again , the regularization together with the new initialization contributes to improve DCFE .","[('together with', (4, 6)), ('contributes to', (9, 11))]","[('regularization', (3, 4)), ('new initialization', (7, 9)), ('improve DCFE', (11, 13))]",[],[],[],[]
results,"Although the results in are not strictly comparable , because each paper uses its own train and test subsets , we get an NME of 2.06 with the full 21 landmarks set .","[('get', (21, 22)), ('of', (24, 25)), ('with', (26, 27))]","[('NME', (23, 24)), ('2.06', (25, 26)), ('full 21 landmarks set', (28, 32))]",[],[],[],[]
results,3DDE outperforms its competitors in all the WFLW subsets by a large margin .,"[('in', (4, 5)), ('by', (9, 10))]","[('3DDE', (0, 1)), ('outperforms', (1, 2)), ('its competitors', (2, 4)), ('all the WFLW subsets', (5, 9)), ('large margin', (11, 13))]",[],[],[],[]
ablation-analysis,"3DDE is based on three key ideas : 3D initialization , a cascaded ERT regressor operating on probabilistic CNN features and a coarse - to - fine scheme .","[('based on', (2, 4)), ('operating on', (15, 17))]","[('3DDE', (0, 1)), ('3D initialization', (8, 10)), ('cascaded ERT regressor', (12, 15)), ('probabilistic CNN features', (17, 20)), ('coarse - to - fine scheme', (22, 28))]",[],[],[],[]
ablation-analysis,In we show the results obtained by different configurations of our framework when evaluated on WFLW .,"[('show', (2, 3)), ('obtained by', (5, 7)), ('of', (9, 10)), ('evaluated on', (13, 15))]","[('different configurations', (7, 9)), ('our framework', (10, 12)), ('WFLW', (15, 16))]",[],[],[],[]
ablation-analysis,"When combined with the cascaded ERT , the 3D initialization is key to achieve top overall performance , see CNN + MS + DE vs CNN + 3D + DE in the full subset .","[('combined with', (1, 3)), ('key to achieve', (11, 14)), ('see', (18, 19))]","[('cascaded ERT', (4, 6)), ('3D initialization', (8, 10)), ('top overall performance', (14, 17)), ('CNN + MS + DE vs CNN + 3D + DE', (19, 30))]",[],[],[],[]
ablation-analysis,"So , it provides the largest improvement in the pose subset .","[('provides', (3, 4)), ('in', (7, 8))]","[('largest improvement', (5, 7)), ('pose subset', (9, 11))]",[],[],[],[]
ablation-analysis,The use of CNN probability maps improves the NME in the full data set in about 20 % ( see CNN+ 3D + SE vs CNN + 3D + DE ) .,"[('use of', (1, 3)), ('improves', (6, 7)), ('in', (9, 10)), ('in', (14, 15))]","[('CNN probability maps', (3, 6)), ('NME', (8, 9)), ('full data set', (11, 14)), ('about 20 %', (15, 18))]",[],[],[],[]
ablation-analysis,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .","[('in', (7, 8)), ('provides', (11, 12)), ('in', (15, 16)), ('with', (19, 20))]","[('coarse - to - fine strategy', (1, 7)), ('our cascaded ERT', (8, 11)), ('significative local improvements', (12, 15)), ('difficult cases', (16, 18)), ('rare facial part combinations', (20, 24))]",[],[],[],[]
results,"The smallest database , COFW , has the worst cross - dataset results .",[],"[('smallest database', (1, 3)), ('COFW', (4, 5)), ('worst cross - dataset results', (8, 13))]",[],[],[],[]
results,"On the other hand , the data set with greatest diversity , WFLW , has the best results .","[('with', (8, 9))]","[('data set', (6, 8)), ('greatest diversity', (9, 11)), ('best results', (16, 18))]",[],[],[],[]
results,"Moreover , the model All , trained with the training sets of all data bases , is able to improve , in all cross - dataset experiments , the models trained in a single data set .","[('trained with', (6, 8)), ('able to', (17, 19)), ('in', (21, 22)), ('trained in', (30, 32))]","[('model All', (3, 5)), ('training sets of all data bases', (9, 15)), ('improve', (19, 20)), ('all cross - dataset experiments', (22, 27)), ('models', (29, 30)), ('single data set', (33, 36))]",[],[],[],[]
results,"The landmarks with highest NME are those related to the ears , the bottom of the mouth and the chin .","[('with', (2, 3)), ('are', (5, 6)), ('related to', (7, 9))]","[('landmarks', (1, 2)), ('highest NME', (3, 5)), ('ears', (10, 11)), ('bottom of the mouth', (13, 17)), ('chin', (19, 20))]",[],[],[],[]
research-problem,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,[],"[('Joint 3D Face Reconstruction and Dense Alignment', (0, 7))]",[],[],[],[]
code,Code is available at https://github.com/YadiraF/PRNet.,[],[],[],[],[],[]
research-problem,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .,[],"[('3 D face reconstruction and face alignment', (0, 7))]",[],[],[],[]
model,"trains a complex network to regress 68 facial landmarks with 2D coordinates from a single image , but needs an extra network to estimate the depth value .","[('trains', (0, 1)), ('to regress', (4, 6)), ('with', (9, 10)), ('from', (12, 13))]","[('complex network', (2, 4)), ('68 facial landmarks', (6, 9)), ('2D coordinates', (10, 12)), ('single image', (14, 16))]",[],[],[],[]
model,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .","[('propose', (5, 6)), ('called', (13, 14)), ('to jointly predict', (21, 24)), ('reconstruct', (27, 28))]","[('end - to - end method', (7, 13)), ('Position map Regression Network ( PRN )', (14, 21)), ('dense alignment', (24, 26)), ('3 D face shape', (28, 32))]",[],[],[],[]
model,"Meanwhile , our method is straightforward with a very light - weighted model which provides the result in one pass with 9.8 ms .","[('is', (4, 5)), ('with', (6, 7)), ('provides', (14, 15)), ('in', (17, 18)), ('with', (20, 21))]","[('our method', (2, 4)), ('straightforward', (5, 6)), ('very light - weighted model', (8, 13)), ('result', (16, 17)), ('one pass', (18, 20)), ('9.8 ms', (21, 23))]",[],[],[],[]
model,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .","[('design', (3, 4)), ('which is', (9, 11)), ('recording', (14, 15)), ('of', (18, 19)), ('keeping', (30, 31)), ('at', (34, 35))]","[('UV position map', (5, 8)), ('2D image', (12, 14)), ('3D coordinates', (16, 18)), ('complete facial point cloud', (20, 24)), ('semantic meaning', (32, 34)), ('each UV place', (35, 38))]",[],[],[],[]
model,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,"[('train', (2, 3)), ('with', (9, 10)), ('focuses more on', (14, 17)), ('to regress', (19, 21)), ('from', (25, 26))]","[('simple encoder - decoder network', (4, 9)), ('weighted loss', (11, 13)), ('discriminative region', (17, 19)), ('UV position map', (22, 25)), ('single 2 D facial image', (27, 32))]",[],[],[],[]
experiments,"Figure1 shows our method is robust to poses , illuminations and occlusions .","[('shows', (1, 2)), ('to', (6, 7))]","[('our method', (2, 4)), ('robust', (5, 6)), ('poses , illuminations and occlusions', (7, 12))]",[],[],[],[]
model,"- To directly regress the 3D facial structure and dense alignment , we develop a novel representation called UV position map , which records the position information of 3 D face and provides dense correspondence to the semantic meaning of each point on UV space .","[('To directly regress', (1, 4)), ('develop', (13, 14)), ('called', (17, 18)), ('records', (23, 24)), ('of', (27, 28)), ('provides', (32, 33)), ('to', (35, 36)), ('on', (42, 43))]","[('3D', (5, 6)), ('novel representation', (15, 17)), ('UV position map', (18, 21)), ('position information', (25, 27)), ('3 D face', (28, 31)), ('dense correspondence', (33, 35)), ('semantic meaning', (37, 39)), ('each point', (40, 42)), ('UV space', (43, 45))]",[],[],[],[]
model,"- For training , we proposed a weight mask which assigns different weight to each point on position map and compute a weighted loss .","[('proposed', (5, 6)), ('which', (9, 10)), ('assigns', (10, 11)), ('to', (13, 14)), ('on', (16, 17)), ('compute', (20, 21))]","[('training', (2, 3)), ('weight mask', (7, 9)), ('different weight', (11, 13)), ('each point', (14, 16)), ('position map', (17, 19)), ('weighted loss', (22, 24))]",[],[],[],[]
model,- We finally provide a light - weighted framework that runs at over 100 FPS to directly obtain 3 D face reconstruction and alignment result from a single 2 D facial image .,"[('provide', (3, 4)), ('runs at', (10, 12)), ('to directly obtain', (15, 18)), ('from', (25, 26))]","[('light - weighted framework', (5, 9)), ('over 100 FPS', (12, 15)), ('3 D face reconstruction and alignment result', (18, 25)), ('single 2 D facial image', (27, 32))]",[],[],[],[]
experimental-setup,"Like , we also augment our training data by scaling color channels with scale range from 0.6 to 1.4 .","[('augment', (4, 5)), ('by scaling', (8, 10)), ('with', (12, 13)), ('from', (15, 16)), ('to', (17, 18))]","[('training data', (6, 8)), ('color channels', (10, 12)), ('scale range', (13, 15)), ('0.6', (16, 17))]",[],[],[],[]
experimental-setup,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .","[('For', (0, 1)), ('use', (4, 5)), ('with', (7, 8)), ('begins at', (11, 13)), ('after', (17, 18))]","[('optimization', (1, 2)), ('Adam optimizer', (5, 7)), ('learning rate', (9, 11)), ('0.0001', (13, 14)), ('decays', (15, 16)), ('half', (16, 17)), ('each 5 epochs', (18, 21))]",[],[],[],[]
experimental-setup,The batch size is set as 16 .,"[('set as', (4, 6))]","[('batch size', (1, 3)), ('16', (6, 7))]",[],[],[],[]
experimental-setup,All of our training codes are implemented with TensorFlow .,"[('implemented with', (6, 8))]","[('TensorFlow', (8, 9))]",[],[],[],[]
ablation-analysis,Network trained without using weight mask has worst performance compared with other two settings .,"[('compared with', (9, 11))]","[('Network trained', (0, 2)), ('weight mask', (4, 6)), ('worst performance', (7, 9)), ('other two settings', (11, 14))]",[],[],[],[]
ablation-analysis,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .","[('adding', (1, 2)), ('to', (3, 4)), ('such as', (6, 8)), ('shows', (19, 20)), ('on', (22, 23)), ('over', (26, 27))]","[('weights', (2, 3)), ('specific regions', (4, 6)), ('68 facial landmarks or', (8, 12)), ('central face region', (12, 15)), ('weight ratio', (16, 18)), ('considerable improvement', (20, 22)), ('68 points datasets', (23, 26)), ('weight ratio 2', (27, 30))]",[],[],[],[]
research-problem,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning,[],"[('Joint 3D Face Reconstruction and Dense Face Alignment', (0, 8))]",[],[],[],[]
research-problem,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,[],"[('3 D face reconstruction from a single 2D image', (0, 9))]",[],[],[],[]
model,"Specifically , taking the sparse 2 D facial landmarks as additional information , 2 DSAL introduces four novel self - supervision schemes that view the 2D landmark and 3D landmark prediction as a self - mapping process , including the 2D and 3D landmark self - prediction consistency , cycle - consistency over the 2D landmark prediction and self - critic over the predicted 3 DMM coefficients based on landmark predictions .","[('taking', (2, 3)), ('introduces', (15, 16)), ('that view', (22, 24)), ('as', (31, 32)), ('including', (38, 39)), ('over', (52, 53)), ('over', (61, 62)), ('based on', (67, 69))]","[('sparse 2 D facial landmarks', (4, 9)), ('2', (13, 14)), ('four novel self - supervision schemes', (16, 22)), ('2D landmark and 3D landmark prediction', (25, 31)), ('self - mapping process', (33, 37)), ('2D and 3D landmark self - prediction consistency', (40, 48)), ('cycle - consistency', (49, 52)), ('2D landmark prediction', (54, 57)), ('self - critic', (58, 61)), ('predicted 3 DMM coefficients', (63, 67)), ('landmark predictions', (69, 71))]",[],[],[],[]
research-problem,"Using these four self - supervision schemes , the 2DASL method significantly relieves demands on the the conventional paired 2D - to - 3D annotations and gives much higher - quality 3 D face models without requiring any additional 3D annotations .","[('on', (14, 15)), ('gives', (26, 27)), ('without requiring', (35, 37))]","[('2DASL method', (9, 11)), ('significantly relieves', (11, 13)), ('demands', (13, 14)), ('conventional paired 2D - to - 3D annotations', (17, 25)), ('much higher - quality 3 D face models', (27, 35)), ('additional 3D annotations', (38, 41))]",[],[],[],[]
research-problem,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,[],"[('3 D face reconstruction from a single 2D image', (0, 9))]",[],[],[],[]
research-problem,3 D face reconstruction is an important task in the field of computer vision and graphics .,[],"[('3 D face reconstruction', (0, 4))]",[],[],[],[]
model,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .","[('to overcome', (2, 4)), ('propose', (16, 17)), ('leverages', (22, 23)), ('to effectively supervise and facilitate', (33, 38))]","[('intrinsic limitation', (5, 7)), ('novel learning method', (18, 21)), ('2D "" in - the - wild "" face images', (23, 33)), ('3D face model learning', (39, 43))]",[],[],[],[]
model,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,"[('design', (1, 2)), ('able to train', (11, 14)), ('with', (19, 20)), ('from', (22, 23))]","[('novel self - supervised learning method', (3, 9)), ('3 D face model', (15, 19)), ('weak supervision', (20, 22)), ('2D images', (23, 25))]",[],[],[],[]
model,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .","[('exploits', (6, 7)), ('over', (10, 11)), ('taking', (18, 19)), ('as', (23, 24)), ('that have', (43, 45)), ('with', (47, 48))]","[('our proposed method', (2, 5)), ('cycle - consistency', (7, 10)), ('2D landmark predictions', (12, 15)), ('recovered 2D landmarks', (20, 23)), ('2D', (33, 34)), ('small difference', (45, 47)), ('annotated ones', (49, 51))]",[],[],[],[]
model,"To facilitate the overall learning procedure , our method also exploits self - critic learning .","[('To facilitate', (0, 2)), ('exploits', (10, 11))]","[('overall learning procedure', (3, 6)), ('our', (7, 8)), ('self - critic learning', (11, 15))]",[],[],[],[]
model,"It takes as input both the latent representation and 3 DMM coefficients of an face image and learns a critic model to evaluate the intrinsic consistency between the predicted 3 DMM coefficients and the corresponding face image , offering another supervision for 3 D face model learning .","[('takes as input', (1, 4)), ('of', (12, 13)), ('learns', (17, 18)), ('to evaluate', (21, 23)), ('between', (26, 27)), ('offering', (38, 39)), ('for', (41, 42))]","[('both', (4, 5)), ('latent representation and 3 DMM coefficients', (6, 12)), ('face image', (14, 16)), ('critic model', (19, 21)), ('intrinsic consistency', (24, 26)), ('predicted 3 DMM coefficients and the corresponding face image', (28, 37)), ('another supervision', (39, 41)), ('3 D face model learning', (42, 47))]",[],[],[],[]
model,"We propose anew scheme that aims to fully utilize the abundant "" in - the - wild "" 2 D face images to assist 3 D face model learning .","[('aims to', (5, 7)), ('to assist', (22, 24))]","[('fully utilize', (7, 9)), ('abundant "" in - the - wild "" 2 D face images', (10, 22)), ('3 D face model learning', (24, 29))]",[],[],[],[]
model,We introduce anew method that is able to train 3 D face models with 2 D face images by self - supervised learning .,"[('able to train', (6, 9)), ('with', (13, 14)), ('by', (18, 19))]","[('3 D face models', (9, 13)), ('2 D face images', (14, 18)), ('self - supervised learning', (19, 23))]",[],[],[],[]
model,"We develop anew self - critic learning based approach which could effectively improve the 3D face model learning procedure and give a better model , even though the 2D landmark annotations are noisy .","[('develop', (1, 2)), ('could', (10, 11))]","[('self - critic learning based approach', (3, 9)), ('effectively improve', (11, 13)), ('3D face model learning procedure', (14, 19)), ('better model', (22, 24))]",[],[],[],[]
research-problem,2D assisted self - supervised learning,[],[],[],[],[],[]
experiments,"Since the contour landmarks of a 2 D face are inaccurate to represent the corresponding points of 3 D face , we discard them and sample 18 landmarks from the 68 2D facial landmarks .","[('of', (4, 5)), ('discard', (22, 23)), ('sample', (25, 26)), ('from', (28, 29))]","[('18 landmarks', (26, 28)), ('68 2D facial landmarks', (30, 34))]",[],[],[],[]
experimental-setup,Our proposed 2 DASL is implemented with Pytorch .,"[('implemented with', (5, 7))]","[('Pytorch', (7, 8))]",[],[],[],[]
experimental-setup,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .","[('use', (1, 2)), ('for', (4, 5)), ('with', (8, 9)), ('beginning at', (12, 14)), ('uses', (23, 24)), ('as', (26, 27)), ('with', (28, 29))]","[('SGD optimizer', (2, 4)), ('CNN regressor', (6, 8)), ('learning rate', (10, 12)), ('5 10 ?5', (14, 17)), ('decays', (18, 19)), ('Adam', (25, 26)), ('optimizer', (27, 28)), ('fixed learning rate 1 10 ?4', (30, 36))]",[],[],[],[]
experimental-setup,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .","[('fine - tune', (6, 9)), ('using', (11, 12))]","[('our model', (9, 11)), ('Vertex Distance Cost', (13, 16))]",[],[],[],[]
model,The 2D facial landmarks of all the face images are detected by an advanced 2 D facial landmarks detector .,"[('of', (4, 5)), ('detected by', (10, 12))]","[('2D facial landmarks', (1, 4)), ('all the face images', (5, 9)), ('advanced 2 D facial landmarks detector', (13, 19))]",[],[],[],[]
experimental-setup,Each face is annotated with its corresponding 3 DMM coefficients and the 68 3D facial landmarks .,"[('annotated with', (3, 5))]","[('Each face', (0, 2)), ('corresponding 3 DMM coefficients', (6, 10)), ('68 3D facial landmarks', (12, 16))]",[],[],[],[]
experimental-setup,Each image is annotated with 34 facial landmarks .,"[('annotated with', (3, 5))]","[('Each image', (0, 2)), ('34 facial landmarks', (5, 8))]",[],[],[],[]
experiments,"As can be seen , our results are more accurate than the ground truth in some cases .","[('are', (7, 8)), ('than', (10, 11)), ('in', (14, 15))]","[('our results', (5, 7)), ('more accurate', (8, 10)), ('ground truth', (12, 14)), ('some cases', (15, 17))]",[],[],[],[]
experiments,"The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .","[('see', (9, 10)), ('achieves', (13, 14)), ('on the evaluation of', (20, 24)), ('among', (30, 31))]","[('our 2 DASL', (10, 13)), ('lowest NME ( % )', (15, 20)), ('2 D and 3D coordinates', (25, 30))]",[],[],[],[]
experiments,"For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .","[('For', (0, 1)), ('by', (16, 17)), ('on both', (20, 22))]","[('3 DMM - based methods', (1, 6)), ('3 DDFA and DeFA', (7, 11)), ('our method', (12, 14)), ('outperforms', (14, 15)), ('large margin', (18, 20)), ('68 spare landmarks', (23, 26)), ('dense coordinates', (28, 30))]",[],[],[],[]
experiments,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .","[('performs', (3, 4)), ('than', (5, 6)), ('reducing', (8, 9)), ('by', (10, 11)), ('on', (14, 15))]","[('Our 2DASL', (0, 2)), ('better', (4, 5)), ('PRNet', (6, 7)), ('NME', (9, 10)), ('0.09 and 0.08', (11, 14)), ('AFLW2000 - 3D', (15, 18)), ('AFLW - LFPA', (19, 22))]",[],[],[],[]
experiments,"Following , we first employ the Iterative Closest Points ( ICP ) algorithm to find the corresponding nearest points between the reconstructed 3 D face and the ground truth point cloud .","[('employ', (4, 5)), ('to find', (13, 15)), ('between', (19, 20))]","[('Iterative Closest Points ( ICP ) algorithm', (6, 13)), ('corresponding nearest points', (16, 19)), ('reconstructed 3 D face', (21, 25)), ('ground truth point cloud', (27, 31))]",[],[],[],[]
experiments,"As can be seen , the 3D reconstruction results of 2 DASL outperforms 3 DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .","[('of', (9, 10)), ('outperforms', (12, 13)), ('by', (15, 16)), ('for', (20, 21))]","[('3D reconstruction results', (6, 9)), ('2 DASL', (10, 12)), ('3 DDFA', (13, 15)), ('0.39 , and 2.29', (16, 20)), ('DeFA', (21, 22))]",[],[],[],[]
experiments,"As can be seen , the reconstructed shape of our 2 DASL are more smooth , however , both PRNet and VRN - Guided introduce some artifacts into the reconstructed results , which makes the reconstructed faces look unnaturally .","[('of', (8, 9)), ('are', (12, 13)), ('introduce', (24, 25)), ('into', (27, 28)), ('makes', (33, 34)), ('look', (37, 38))]","[('reconstructed shape', (6, 8)), ('our 2 DASL', (9, 12)), ('more smooth', (13, 15)), ('PRNet and VRN - Guided', (19, 24)), ('some artifacts', (25, 27)), ('reconstructed results', (29, 31)), ('reconstructed faces', (35, 37)), ('unnaturally', (38, 39))]",[],[],[],[]
baselines,"( 2 ) 2DASL ( cyc ) , which takes as input the combination of RGB face images and the corresponding 2D FLMs with self - supervison , however without self - critic supervision ; ( 3 ) 2DASL ( sc ) , which takes as input the RGB face images only using self - critic learning .","[('takes as input', (9, 12)), ('with', (23, 24)), ('takes as input', (44, 47)), ('using', (52, 53))]","[('2DASL ( cyc )', (3, 7)), ('combination', (13, 14)), ('RGB face images', (15, 18)), ('self - supervison', (24, 27)), ('2DASL ( sc )', (38, 42)), ('RGB face images', (48, 51)), ('self - critic learning', (53, 57))]",[],[],[],[]
baselines,"( 4 ) 2DASL ( cyc+sc ) , which contains both self - supervision and self - critic supervision .","[('contains', (9, 10))]","[('2DASL ( cyc+sc )', (3, 7)), ('self - supervision', (11, 14)), ('self - critic supervision', (15, 19))]",[],[],[],[]
ablation-analysis,"2 . Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages , respectively .","[('Adding', (2, 3)), ('to', (4, 5)), ('of', (7, 8)), ('reduces', (11, 12)), ('by', (14, 15)), ('to', (16, 17)), ('on', (18, 19))]","[('weights', (3, 4)), ('central', (5, 6)), ('points', (6, 7)), ('facial landmarks', (9, 11)), ('NME', (13, 14)), ('0.09', (15, 16)), ('0.23', (17, 18)), ('two stages', (20, 22))]",[],[],[],[]
ablation-analysis,"If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .","[('If', (0, 1)), ('is', (6, 7)), ('increases', (12, 13)), ('by', (13, 14)), ('for', (15, 16))]","[('self - critic learning', (2, 6)), ('not used', (7, 9)), ('NME', (11, 12)), ('0.04/0.18', (14, 15)), ('with / without weight mask', (16, 21))]",[],[],[],[]
ablation-analysis,The best result is achieved when both these two modules are used .,"[('achieved when', (4, 6)), ('used', (11, 12))]","[('best result', (1, 3)), ('both these two modules', (6, 10))]",[],[],[],[]
research-problem,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection,[],[],[],[],[],[]
model,"In addition , to recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) to correct outliers by considering the global face shape as a constraint .","[('due to', (9, 11)), ('propose', (17, 18)), ('to correct', (26, 28)), ('by considering', (29, 31)), ('as', (35, 36))]","[('recover', (4, 5)), ('unconfidently predicted landmarks', (6, 9)), ('occlusion and low quality', (11, 15)), ('global heatmap correction unit ( GHCU )', (19, 26)), ('outliers', (28, 29)), ('global face shape', (32, 35)), ('constraint', (37, 38))]",[],[],[],[]
model,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .","[('propose', (9, 10)), ('maintains', (19, 20)), ('recovers', (26, 27)), ('caused by', (31, 33)), ('such as', (35, 37))]","[('global heatmap correction unit ( GHCU )', (11, 18)), ('global face shape constraint', (21, 25)), ('unconfidently predicted landmarks', (28, 31)), ('challenging factors', (33, 35)), ('occlusions', (37, 38)), ('low resolution of', (39, 42)), ('images', (42, 43))]",[],[],[],[]
experiments,"1 . The GHCU implicitly learns the whole face shape constraint from the training data and always gives facialshape landmarks , as shown in .","[('implicitly learns', (4, 6)), ('from', (11, 12)), ('always gives', (16, 18))]","[('GHCU', (3, 4)), ('whole face shape constraint', (7, 11)), ('training data', (13, 15)), ('facialshape landmarks', (18, 20))]",[],[],[],[]
experimental-setup,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .","[('To', (0, 1)), ('perform', (1, 2)), ('randomly sample', (6, 8)), ('from', (17, 18))]","[('data augmentation', (2, 4)), ('angle of rotation and the bounding box scale', (9, 17)), ('Gaussian distribution', (18, 20))]",[],[],[],[]
experimental-setup,We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,"[('use', (1, 2)), ('as', (9, 10)), ('trained by', (14, 16))]","[('four - stage stacked hourglass network', (3, 9)), ('our backbone', (10, 12)), ('optimizer RMSprop', (17, 19))]",[],[],[],[]
ablation-analysis,"As described in Section 4 , our algorithm comprises two parts : network training and real groundtruth searching , which are alternatively optimized .","[('comprises', (8, 9)), ('are', (20, 21))]","[('our algorithm', (6, 8)), ('two parts', (9, 11)), ('network training', (12, 14)), ('real groundtruth searching', (15, 18)), ('alternatively optimized', (21, 23))]",[],[],[],[]
ablation-analysis,"Specifically , at each epoch , we first search the real ground - trut ?","[('at', (2, 3)), ('first search', (7, 9))]","[('each epoch', (3, 5))]",[],[],[],[]
experimental-setup,"When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .","[('with', (6, 7)), ('is', (14, 15)), ('after', (26, 27))]","[('training', (1, 2)), ('roughly converged model', (3, 6)), ('human annotations', (7, 9)), ('initial learning rate', (11, 14)), ('2.5 10 ?4', (15, 18)), ('decayed', (20, 21)), ('2.5 10 ? 6', (22, 26)), ('120 epochs', (27, 29))]",[],[],[],[]
experimental-setup,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .","[('of', (8, 9)), ('is', (19, 20)), ('divided by', (26, 28)), ('at', (33, 34))]","[('training', (1, 2)), ('Semantic Alignment', (3, 5)), ('initial learning rate', (16, 19)), ('2.5 10 ? 6', (20, 24)), ('5 , 2 and 2', (28, 33)), ('epoch 30 , 60 and 90', (34, 40))]",[],[],[],[]
experimental-setup,We set batch size to 10 for network training .,"[('set', (1, 2)), ('to', (4, 5)), ('for', (6, 7))]","[('batch size', (2, 4)), ('10', (5, 6)), ('network training', (7, 9))]",[],[],[],[]
experimental-setup,1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .,"[('trained with', (6, 8)), ('on', (12, 13))]","[('PyTorch [ 18 ]', (8, 12)), ('2 Titan X GPUs', (13, 17))]",[],[],[],[]
baselines,"2 ) uses the hourglass architecture with human annotations , which is actually the traditional landmark detector training .","[('uses', (2, 3)), ('with', (6, 7)), ('is', (11, 12))]","[('hourglass architecture', (4, 6)), ('human annotations', (7, 9))]",[],[],[],[]
results,"2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .","[('see', (4, 5)), ('with', (7, 8)), ('greatly outperform', (16, 18)), ('in terms of', (29, 32)), ('on', (33, 34)), ('showing', (37, 38))]","[('HGs', (6, 7)), ('our Semantic Alignment ( HGs + SA )', (8, 16)), ('hourglass ( HGs )', (18, 22)), ('4.37 % vs 5.04 %', (24, 29)), ('NME', (32, 33)), ('Full set', (34, 36))]",[],[],[],[]
results,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .","[('adding', (1, 2)), ('see', (6, 7))]","[('GHCU', (2, 3)), ('HGs + SA + GHCU', (8, 13)), ('slightly outperforms', (13, 15)), ('HGs + SA', (16, 19))]",[],[],[],[]
results,"Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .","[('normalize', (3, 4)), ('by training', (10, 12)), ('conduct', (17, 18)), ('achieve', (30, 31)), ('on', (36, 37))]","[('in - plane - rotation', (5, 10)), ('preprocessing network', (13, 15)), ('normalization ( HGs + SA + GHCU + Norm )', (19, 29)), ('state of the art performance', (31, 36)), ('Challenge set and Full set', (37, 42)), ('6.38 % and 4.02 %', (43, 48))]",[],[],[],[]
results,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .","[('on', (3, 4))]","[('Challenge set', (4, 6)), ('significantly outperform', (8, 10)), ('state of the art method', (11, 16)), ('6.38 % ( HGs + SA +GHCU + Norm )', (17, 27)), ('6.98 % ( LAB )', (28, 33))]",[],[],[],[]
results,It is also observed that HGs + SA + GHCU works better than HGs + SA .,"[('observed', (3, 4)), ('works', (10, 11)), ('than', (12, 13))]","[('HGs + SA + GHCU', (5, 10)), ('better', (11, 12)), ('HGs + SA', (13, 16))]",[],[],[],[]
results,The subset Category 3 is the most challenging one .,"[('is', (4, 5))]","[('subset Category 3', (1, 4)), ('most challenging one', (6, 9))]",[],[],[],[]
results,"4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .","[('see that', (4, 6))]","[('HGs + SA', (6, 9)), ('greatly outperforms', (9, 11)), ('HGs', (11, 12))]",[],[],[],[]
results,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .","[('compared with', (2, 4)), ('reduce', (13, 14)), ('by', (20, 21)), ('on', (23, 24))]","[('HGs + SA , HGs + SA + GHCU', (4, 13)), ('error rate ( RMSE )', (15, 20)), ('18 %', (21, 23)), ('Category 3 test set', (24, 28))]",[],[],[],[]
results,Comparison with state of the art on AFLW dataset .,"[('on', (6, 7))]","[('AFLW dataset', (7, 9))]",[],[],[],[]
baselines,"GHCU considers the global face shape as constraint , being robust to such challenging factors .","[('considers', (1, 2)), ('as', (6, 7))]","[('GHCU', (0, 1)), ('global face shape', (3, 6)), ('constraint', (7, 8))]",[],[],[],[]
results,"As shown in Tab. 8 , our CNN based GHCU outperforms PCA based method in terms of both accuracy and efficiency .","[('outperforms', (10, 11)), ('in terms of', (14, 17))]","[('our CNN based GHCU', (6, 10)), ('PCA based method', (11, 14)), ('both accuracy and efficiency', (17, 21))]",[],[],[],[]
ablation-analysis,"9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .","[('on', (9, 10))]","[('Semantic alignment', (2, 4)), ('consistently improve', (5, 7)), ('performance', (8, 9)), ('all subset sets', (10, 13))]",[],[],[],[]
ablation-analysis,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .","[('is', (1, 2)), ('on', (4, 5))]","[('GHCU', (0, 1)), ('more effective', (2, 4)), ('challenge data set', (6, 9)), ('8.15 % vs 9.91 %', (14, 19))]",[],[],[],[]
research-problem,Accurate Face Detection for High Performance,[],"[('Face Detection', (1, 3))]",[],[],[],[]
research-problem,Face detection has witnessed significant progress due to the advances of deep convolutional neural networks ( CNNs ) .,[],"[('Face detection', (0, 2))]",[],[],[],[]
research-problem,"Face detection is a tremendously important field in computer vision needed for face recognition , sentiment analysis , video surveillance , and many other fields .",[],"[('Face detection', (0, 2))]",[],[],[],[]
model,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .","[('to perform', (14, 16)), ('as', (18, 19))]","[('popular one - stage RetinaNet method', (8, 14)), ('face detection', (16, 18)), ('our baseline model', (19, 22))]",[],[],[],[]
model,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,"[('applied on', (5, 7)), ('to develop', (9, 11)), ('namely', (16, 17))]","[('high performance face detector', (12, 16)), ('AInnoFace', (17, 18))]",[],[],[],[]
model,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,"[('Employing', (3, 4)), ('for', (11, 12)), ('Applying', (17, 18)), ('for', (27, 28)), ('Revisiting', (33, 34)), ('based on', (37, 39)), ('for', (44, 45)), ('Utilizing', (50, 51)), ('for', (56, 57)), ('Using', (63, 64)), ('for', (68, 69))]","[('two - step classification and regression', (5, 11)), ('detection', (12, 13)), ('Intersection over Union ( IoU ) loss function', (19, 27)), ('regression', (28, 29)), ('data augmentation', (35, 37)), ('data - anchor - sampling', (39, 44)), ('training', (45, 46)), ('max - out operation', (52, 56)), ('robuster classification', (57, 59)), ('multi-scale testing strategy', (65, 68)), ('inference', (69, 70))]",[],[],[],[]
baselines,The focal loss is the reshaping of cross entropy loss such that it down - weights the loss assigned to well - classified examples .,"[('is', (3, 4)), ('of', (6, 7)), ('such that', (10, 12)), ('down - weights', (13, 16)), ('assigned to', (18, 20))]","[('focal loss', (1, 3)), ('reshaping', (5, 6)), ('cross entropy loss', (7, 10)), ('loss', (17, 18)), ('well - classified examples', (20, 24))]",[],[],[],[]
baselines,STR performs is two - step regression on three high level detection layers to adjust anchors and provide better initialization for the subsequent regressor .,"[('performs', (1, 2)), ('on', (7, 8)), ('to adjust', (13, 15)), ('provide', (17, 18)), ('for', (20, 21))]","[('STR', (0, 1)), ('two - step regression', (3, 7)), ('three high level detection layers', (8, 13)), ('anchors', (15, 16)), ('better initialization', (18, 20)), ('subsequent regressor', (22, 24))]",[],[],[],[]
experimental-setup,The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset .,"[('in', (3, 4)), ('initialized by', (9, 11)), ('on', (14, 15))]","[('backbone network', (1, 3)), ('proposed AInnoFace detector', (5, 8)), ('pretrained model', (12, 14)), ('ImageNet dataset', (16, 18))]",[],[],[],[]
experimental-setup,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .","[('use', (1, 2)), ('to randomly initialize', (7, 10)), ('in', (12, 13))]","[('"" xavier "" method', (3, 7)), ('parameters', (11, 12)), ('newly added convolutional layers', (14, 18))]",[],[],[],[]
experimental-setup,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .","[('with', (16, 17))]","[('stochastic gradient descent ( SGD ) algorithm', (1, 8)), ('model', (15, 16)), ('0.9 momentum', (17, 19)), ('0.0001 weight decay', (20, 23)), ('batch size 32', (24, 27))]",[],[],[],[]
experimental-setup,The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,"[('applied to', (4, 6)), ('from', (12, 13)), ('to', (14, 15)), ('at', (16, 17))]","[('warmup strategy', (1, 3)), ('gradually ramp up', (6, 9)), ('learning rate', (10, 12)), ('0.0003125', (13, 14)), ('0.01', (15, 16)), ('first 5 epochs', (18, 21))]",[],[],[],[]
experiments,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .","[('switches to', (4, 6)), ('dividing by', (14, 16)), ('at', (17, 18)), ('ending at', (23, 25))]","[('regular learning rate schedule', (7, 11)), ('10', (16, 17)), ('100 and 120 epochs', (18, 22)), ('130 epochs', (25, 27))]",[],[],[],[]
experimental-setup,The full training and testing codes are built on the PyTorch library .,"[('built on', (7, 9))]","[('full training and testing codes', (1, 6)), ('PyTorch library', (10, 12))]",[],[],[],[]
results,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .","[('sets', (7, 8)), ('based on', (18, 20)), ('across', (23, 24)), ('on', (27, 28)), ('i.e.', (34, 35)), ('for', (53, 54)), ('for', (75, 76))]","[('our face detector', (4, 7)), ('some new state - of - the - art results', (8, 18)), ('AP score', (21, 23)), ('three subsets', (25, 27)), ('validation and testing subsets', (29, 33)), ('97.0 % ( Easy )', (36, 41)), ('96.1 % ( Medium )', (42, 47)), ('91.8 % ( Hard )', (48, 53)), ('validation subset', (54, 56)), ('96.5 %', (58, 60)), ('95.7 % ( Medium )', (64, 69)), ('91.2 % ( Hard )', (70, 75)), ('testing subset', (76, 78))]",[],[],[],[]
research-problem,EXTD : Extremely Tiny Face Detector via Iterative Filter Reuse,[],"[('EXTD', (0, 1)), ('Extremely Tiny Face Detector', (2, 6))]",[],[],[],[]
research-problem,"In this paper , we propose a new multi-scale face detector having an extremely tiny number of parameters ( EXTD ) , less than 0.1 million , as well as achieving comparable performance to deep heavy detectors .","[('propose', (5, 6)), ('having', (11, 12)), ('less than', (22, 24)), ('achieving', (30, 31)), ('to', (33, 34))]","[('new multi-scale face detector', (7, 11)), ('extremely tiny number of parameters ( EXTD )', (13, 21)), ('0.1 million', (24, 26)), ('comparable performance', (31, 33)), ('deep heavy detectors', (34, 37))]",[],[],[],[]
model,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .","[('propose', (5, 6))]","[('extremely', (12, 13))]",[],[],[],[]
model,"The main discovery is that we can share the network in generating each feature - map , as shown in .","[('share', (7, 8)), ('in generating', (10, 12))]","[('network', (9, 10)), ('each feature - map', (12, 16))]",[],[],[],[]
model,"We note that our model does not require any extra layer commonly defined as in , and is trained from scratch .","[('note', (1, 2)), ('does not require', (5, 8)), ('trained from', (18, 20))]","[('our model', (3, 5)), ('any extra layer', (8, 11)), ('scratch', (20, 21))]",[],[],[],[]
model,"We propose an iterative network sharing model for multi-stage face detection which can significantly reduce the parameter size , as well as provide abundant object semantic information to the lower stage feature maps .","[('propose', (1, 2)), ('for', (7, 8)), ('can', (12, 13)), ('to', (27, 28))]","[('multi-stage face detection', (8, 11)), ('significantly reduce', (13, 15)), ('parameter size', (16, 18)), ('abundant object semantic information', (23, 27)), ('lower stage feature maps', (29, 33))]",[],[],[],[]
experimental-setup,"Using the hard negative mining technique , we balance the ratio of positive and negative samples N neg / N pos to 3 and the balancing parameter ?","[('Using', (0, 1)), ('balance', (8, 9)), ('of', (11, 12)), ('to', (21, 22))]","[('hard negative mining technique', (2, 6)), ('ratio', (10, 11)), ('positive and negative samples N neg / N pos', (12, 21)), ('3', (22, 23)), ('balancing parameter', (25, 27))]",[],[],[],[]
experimental-setup,is set to 4 .,"[('set to', (1, 3))]","[('4', (3, 4))]",[],[],[],[]
experimental-setup,The proposed method is implemented with PyTorch and NAVER Smart Machine Learning ( NSML ) system .,"[('implemented with', (4, 6))]","[('PyTorch and NAVER Smart Machine Learning ( NSML ) system', (6, 16))]",[],[],[],[]
code,Code will be available at https ://github.com/clovaai.,[],[],[],[],[],[]
experimental-setup,"For the model , we designed three variations which have a different number of parameters , lighter one having 0.063 M parameters with 32 channels for each feature maps , intermediate one having 0.1 M parameters with 48 channels , and the heavier one with 64 channels and 0.16M parameters when designed as FPN .","[('designed', (5, 6)), ('have', (9, 10)), ('having', (18, 19)), ('with', (22, 23)), ('for', (25, 26)), ('with', (36, 37))]","[('three variations', (6, 8)), ('different number of parameters', (11, 15)), ('lighter one', (16, 18)), ('0.063 M parameters', (19, 22)), ('32 channels', (23, 25)), ('each feature maps', (26, 29)), ('intermediate one', (30, 32)), ('0.1 M parameters', (33, 36)), ('48 channels', (37, 39)), ('heavier one', (42, 44)), ('64 channels and 0.16M parameters', (45, 50)), ('FPN', (53, 54))]",[],[],[],[]
experimental-setup,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .","[('of', (3, 4)), ('set to', (9, 11)), ('identical to', (15, 17)), ('of', (21, 22))]","[('negative slope', (1, 3)), ('Leaky - ReLU', (5, 8)), ('0.25', (11, 12)), ('initial negative slope', (18, 21)), ('PReLU', (23, 24))]",[],[],[],[]
experiments,"For a fair comparison , all the inference processes of the models are implemented by PyTorch 1.0 .","[('of', (9, 10)), ('implemented by', (13, 15))]","[('all the inference processes', (5, 9)), ('models', (11, 12)), ('PyTorch 1.0', (15, 17))]",[],[],[],[]
experiments,Comparison to the Existing Methods :,[],"[('Comparison to the Existing Methods', (0, 5))]",[],[],[],[]
experiments,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .","[('compared to', (1, 3)), ('such as', (6, 8)), ('achieved', (24, 25))]","[('SOTA face detectors', (3, 6)), ('Pyra - midBox and DSFD', (8, 13)), ('our best model EXTD - FPN - 64 - PReLU', (14, 24)), ('lower results', (25, 27))]",[],[],[],[]
experiments,The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,"[('between', (2, 3)), ('on', (8, 9)), ('was', (13, 14))]","[('margin', (1, 2)), ('PyramidBox and the proposed model', (3, 8)), ('WIDER FACE hard case', (9, 13)), ('3.4 %', (14, 16))]",[],[],[],[]
experiments,"The m AP gap to DSFD , which is tremendously heavier , is about 5.0 % , but it would be safe to suggest that the proposed method offers more decent trade - off in that DSFD uses about 2860 times more parameters than the proposed method .","[('to', (4, 5)), ('is about', (12, 14)), ('uses about', (37, 39)), ('than', (43, 44))]","[('m AP gap', (1, 4)), ('DSFD', (5, 6)), ('tremendously heavier', (9, 11)), ('5.0 %', (14, 16)), ('2860 times more parameters', (39, 43)), ('proposed method', (45, 47))]",[],[],[],[]
experiments,"When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .","[('comes to', (2, 4)), ('got', (11, 12)), ('than', (15, 16))]","[('our SSD - based variations', (4, 9)), ('lower mAP results', (12, 15)), ('FPN - based variants', (16, 20))]",[],[],[],[]
experiments,"However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .","[('compared with', (3, 5)), ('trained with', (8, 10)), ('achieved', (21, 22))]","[('S3FD version', (6, 8)), ('Mo - bile FaceNet backbone network', (10, 16)), ('proposed SSD variants', (18, 21)), ('comparable or better detection performance', (22, 27))]",[],[],[],[]
experiments,Detection performance regarding the Face Scale :,"[('regarding', (2, 3))]","[('Detection performance', (0, 2)), ('Face Scale', (4, 6))]",[],[],[],[]
experiments,"From the table , we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases .","[('see', (6, 7)), ('achieved', (10, 11)), ('in', (13, 14)), ('than', (18, 19))]","[('our method', (8, 10)), ('higher performance', (11, 13)), ('WIDER FACE hard dataset', (14, 18)), ('other cases', (19, 21))]",[],[],[],[]
experiments,"First , for all the different channel width , FPN based architecture achieved better detection performance compared to SSD based architecture , especially for detecting small faces .","[('for', (2, 3)), ('achieved', (12, 13)), ('compared to', (16, 18)), ('especially for detecting', (22, 25))]","[('all the different channel width', (3, 8)), ('FPN based architecture', (9, 12)), ('better detection performance', (13, 16)), ('SSD based architecture', (18, 21)), ('small faces', (25, 27))]",[],[],[],[]
experiments,"As the channel width increased by 32 to 64 , we can see that the detection accuracy significantly enhanced for all the cases ; Easy , Medium , and Hard .","[('increased by', (4, 6)), ('see that', (12, 14)), ('for', (19, 20))]","[('channel width', (2, 4)), ('32 to 64', (6, 9)), ('detection accuracy', (15, 17)), ('significantly enhanced', (17, 19)), ('all the cases', (20, 23)), ('Easy , Medium , and Hard', (24, 30))]",[],[],[],[]
experiments,"In all the cases including FPN based and SSD based structures , PReLU was the most effective choice when it comes to mAP , but the gap between Leaky - ReLU was not that significant for the FPN variants .","[('including', (4, 5)), ('was', (13, 14)), ('when it comes to', (18, 22))]","[('FPN', (5, 6)), ('PReLU', (12, 13)), ('most effective choice', (15, 18)), ('mAP', (22, 23)), ('gap', (26, 27)), ('Leaky - ReLU', (28, 31)), ('not that significant', (32, 35))]",[],[],[],[]
experiments,"When tested with SSD based architecture , PReLU outperformed Leaky - ReLU with larger margin than those using FPN structure .","[('tested with', (1, 3)), ('with', (12, 13)), ('than those using', (15, 18))]","[('SSD based architecture', (3, 6)), ('PReLU', (7, 8)), ('outperformed', (8, 9)), ('Leaky - ReLU', (9, 12)), ('larger margin', (13, 15)), ('FPN structure', (18, 20))]",[],[],[],[]
experiments,It is worth noting that ReLU occurred notable performance decreases especially when the channel width was small for both SSD and FPN cases .,"[('worth noting', (2, 4)), ('occurred', (6, 7)), ('for', (17, 18))]","[('ReLU', (5, 6)), ('notable performance decreases', (7, 10)), ('channel', (13, 14)), ('small', (16, 17)), ('SSD and FPN cases', (19, 23))]",[],[],[],[]
experiments,"When the channel width was set to 32 , m AP for all the three cases were lower than 10 % to 20 % compared to those using other activation functions .","[('set to', (5, 7)), ('for', (11, 12)), ('lower than', (17, 19)), ('compared to', (24, 26)), ('using', (27, 28))]","[('channel width', (2, 4)), ('32', (7, 8)), ('m AP', (9, 11)), ('all the three cases', (12, 16)), ('10 % to 20 %', (19, 24)), ('other activation functions', (28, 31))]",[],[],[],[]
experimental-setup,"For training the proposed architecture , a stochastic gradient descent optimizer ( SGD ) with learning rate 1e ? 3 , with 0.9 momentum , 0.0005 weight decay , and batch size 16 is used .","[('For', (0, 1)), ('with', (14, 15)), ('with', (21, 22))]","[('stochastic gradient descent optimizer ( SGD )', (7, 14)), ('learning rate', (15, 17)), ('1e ? 3', (17, 20)), ('0.9 momentum', (22, 24)), ('0.0005 weight decay', (25, 28)), ('batch size 16', (30, 33))]",[],[],[],[]
experimental-setup,"The maximum iteration number is basically set to 240K , and we drop the learning rate to 1e ? 4 and 1e ? 5 at 120 K and 180K iterations .","[('set to', (6, 8)), ('drop', (12, 13)), ('to', (16, 17)), ('at', (24, 25))]","[('maximum iteration number', (1, 4)), ('240K', (8, 9)), ('learning rate', (14, 16)), ('1e ? 4 and 1e ? 5', (17, 24)), ('120 K', (25, 27)), ('180K iterations', (28, 30))]",[],[],[],[]
research-problem,DSFD : Dual Shot Face Detector,[],"[('DSFD', (0, 1))]",[],[],[],[]
research-problem,"Face detection is a fundamental step for various facial applications , like face alignment , parsing , recognition , and verification .",[],"[('Face detection', (0, 2))]",[],[],[],[]
model,The first one is mainly based on the Region Proposal Network ( RPN ) adopted in Faster RCNN and employs two stage detection schemes .,"[('mainly based on', (4, 7)), ('adopted in', (14, 16)), ('employs', (19, 20))]","[('Region Proposal Network ( RPN )', (8, 14)), ('Faster RCNN', (16, 18)), ('two stage detection schemes', (20, 24))]",[],[],[],[]
model,RPN is trained end - to - end and generates highquality region proposals which are further refined by Fast R - CNN detector .,"[('trained', (2, 3)), ('generates', (9, 10))]","[('RPN', (0, 1)), ('end - to - end', (3, 8)), ('highquality region proposals', (10, 13)), ('Fast R - CNN detector', (18, 23))]",[],[],[],[]
research-problem,"The other one is Single Shot Detector ( SSD ) based one - stage methods , which get rid of RPN , and directly predict the bounding boxes and confidence .","[('get rid of', (17, 20)), ('directly predict', (23, 25))]","[('Single Shot Detector ( SSD ) based one - stage methods', (4, 15)), ('RPN', (20, 21)), ('bounding boxes', (26, 28))]",[],[],[],[]
hyperparameters,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,"[('initialized by', (4, 6)), ('on', (11, 12))]","[('backbone networks', (1, 3)), ('pretrained VGG / ResNet', (7, 11)), ('Image Net', (12, 14))]",[],[],[],[]
hyperparameters,All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,"[('initialized by', (8, 10))]","[(""newly added convolution layers ' parameters"", (1, 7)), (""' xavier ' method"", (11, 15))]",[],[],[],[]
hyperparameters,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .","[('use', (1, 2)), ('with', (3, 4)), ('to fine - tune', (10, 14))]","[('SGD', (2, 3)), ('0.9 momentum', (4, 6)), ('0.0005 weight decay', (7, 10)), ('our DSFD model', (14, 17))]",[],[],[],[]
hyperparameters,The batch size is set to 16 .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('16', (6, 7))]",[],[],[],[]
hyperparameters,"The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .","[('set to', (4, 6)), ('for', (8, 9)), ('for', (27, 28))]","[('learning rate', (1, 3)), ('10 ?3', (6, 8)), ('first 40 k steps', (10, 14)), ('decay', (17, 18)), ('10 ? 4 and 10 ? 5', (20, 27)), ('two 10 k steps', (28, 32))]",[],[],[],[]
hyperparameters,Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,"[('applied with', (3, 5)), ('of', (7, 8)), ('to produce', (9, 11))]","[('Non-maximum suppression', (0, 2)), ('jaccard overlap', (5, 7)), ('0.3', (8, 9)), ('top 750 high confident bounding boxes per image', (11, 19))]",[],[],[],[]
code,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,[],"[('https://github.com/TencentYoutuResearch/FaceDetection-DSFD', (8, 9))]",[],[],[],[]
results,Analysis on DSFD,[],[],[],[],[],[]
experiments,"Finally , we can improve our DSFD to 96.6 % , 95.7 % , 90.4 % with ResNet 152 as the backbone . Besides , shows that our improved anchor matching strategy greatly increases the number of ground truth faces that are closed to the anchor , which can reduce the contradiction between the discrete anchor scales and continuous face scales .","[('improve', (4, 5)), ('to', (7, 8)), ('with', (16, 17)), ('greatly increases', (32, 34))]","[('our DSFD', (5, 7)), ('96.6 % , 95.7 % , 90.4 %', (8, 16)), ('ResNet 152', (17, 19)), ('our improved anchor matching strategy', (27, 32)), ('number of', (35, 37)), ('anchor', (45, 46))]",[],[],[],[]
results,Comparison with RFB,[],[],[],[],[],[]
experiments,"2 ) Auxiliary loss based on progressive anchor is used to train all 12 different scale detection feature maps , and it improves the performance on easy , medium and hard faces simultaneously .","[('based on', (4, 6)), ('improves', (22, 23)), ('on', (25, 26))]","[('Auxiliary loss', (2, 4)), ('progressive anchor', (6, 8)), ('all 12 different scale detection feature maps', (12, 19)), ('performance', (24, 25)), ('easy , medium and hard faces simultaneously', (26, 33))]",[],[],[],[]
results,"3 ) Our improved anchor matching provides better initial anchors and ground - truth faces to regress anchor from faces , which achieves the improvements of 0.3 % , 0.1 % , 0.3 % on three settings , respectively .","[('provides', (6, 7)), ('to', (15, 16)), ('from', (18, 19)), ('achieves', (22, 23)), ('of', (25, 26))]","[('Our improved anchor matching', (2, 6)), ('better initial anchors and ground - truth faces', (7, 15)), ('regress anchor', (16, 18)), ('faces', (19, 20)), ('improvements', (24, 25)), ('0.3 %', (26, 28))]",[],[],[],[]
results,"Additionally , when we enlarge the training batch size ( i.e. , Large BS ) , the result in hard setting can get 91.2 % AP .","[('enlarge', (4, 5)), ('i.e.', (10, 11)), ('result in', (17, 19)), ('can get', (21, 23))]","[('training batch size', (6, 9)), ('hard setting', (19, 21)), ('91.2 % AP', (23, 26))]",[],[],[],[]
results,"From , DSFD with SE - ResNeXt101 324d got 95.7 % , 94.8 % , 88.9 % , on easy , medium and hard settings respectively , which indicates that more complexity model and higher Top - 1 I ma - geNet classification accuracy may not benefit face detection AP .","[('with', (3, 4)), ('got', (8, 9)), ('on', (18, 19))]","[('DSFD', (2, 3)), ('SE - ResNeXt101 324d', (4, 8)), ('95.7 % , 94.8 % , 88.9 %', (9, 17)), ('easy , medium and hard settings', (19, 25))]",[],[],[],[]
results,Our DSFD enjoys high inference speed benefited from simply using the second shot detection results .,"[('enjoys', (2, 3)), ('benefited from', (6, 8))]","[('Our', (0, 1)), ('high inference speed', (3, 6)), ('second shot detection results', (11, 15))]",[],[],[],[]
results,"As shown in , our DSFD achieves the best performance among all of the state - of - the - art face detectors based on the average precision ( AP ) across the three subsets , i.e. , 96.6 % ( Easy ) , 95.7 % ( Medium ) and 90.4 % ( Hard ) on validation set , and 96.0 % ( Easy ) , 95.3 % ( Medium ) and 90.0 % ( Hard ) on test set .","[('achieves', (6, 7)), ('among', (10, 11)), ('based on', (23, 25)), ('across', (31, 32)), ('i.e.', (36, 37)), ('on', (55, 56)), ('on', (77, 78))]","[('our DSFD', (4, 6)), ('best performance', (8, 10)), ('all of the state - of - the - art face detectors', (11, 23)), ('average precision ( AP )', (26, 31)), ('three subsets', (33, 35)), ('96.6 % ( Easy )', (38, 43)), ('95.7 % ( Medium )', (44, 49)), ('90.4 % ( Hard )', (50, 55)), ('validation set', (56, 58)), ('96.0 % ( Easy )', (60, 65)), ('95.3 % ( Medium )', (66, 71)), ('90.0 % ( Hard )', (72, 77)), ('test set', (78, 80))]",[],[],[],[]
experiments,"Since WIDER FACE has bounding box annotation while faces in FDDB are represented by ellipses , we learn a post - hoc ellipses regressor to transform the final prediction results .","[('while', (7, 8)), ('in', (9, 10)), ('represented by', (12, 14)), ('learn', (17, 18)), ('to transform', (24, 26))]","[('bounding box annotation', (4, 7)), ('faces', (8, 9)), ('FDDB', (10, 11)), ('ellipses', (14, 15)), ('post - hoc ellipses regressor', (19, 24)), ('final prediction results', (27, 30))]",[],[],[],[]
experiments,"As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .","[('achieves', (6, 7)), ('on', (15, 16)), ('i.e.', (23, 24)), ('when', (29, 30)), ('equals to', (35, 37))]","[('our DSFD', (4, 6)), ('state - of - the - art performance', (7, 15)), ('99.1 % and 86.2 %', (24, 29)), ('number of false positives', (31, 35)), ('1 , 000', (37, 40))]",[],[],[],[]
experiments,"After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .","[('adding', (1, 2)), ('to', (4, 5)), ('of', (12, 13)), ('can be', (15, 17))]","[('additional annotations', (2, 4)), ('unlabeled faces', (6, 8)), ('false positives', (10, 12)), ('our model', (13, 15)), ('further reduced', (17, 19)), ('outperform', (20, 21)), ('all other methods', (21, 24))]",[],[],[],[]
research-problem,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,[],[],[],[],[],[]
model,These complementary scale - specific detectors are combined to produce a strong multi-scale object detector .,"[('combined to produce', (7, 10))]","[('complementary scale - specific detectors', (1, 6)), ('strong multi-scale object detector', (11, 15))]",[],[],[],[]
model,"The R - CNN samples object proposals at multiple scales , using a preliminary attention stage , and then warps these proposals to the size ( e.g. 224224 ) supported by the CNN .","[('samples', (4, 5)), ('at', (7, 8)), ('using', (11, 12)), ('warps', (19, 20)), ('to', (22, 23)), ('supported by', (29, 31))]","[('R - CNN', (1, 4)), ('object proposals', (5, 7)), ('multiple scales', (8, 10)), ('preliminary attention stage', (13, 16)), ('proposals', (21, 22)), ('size ( e.g. 224224 )', (24, 29)), ('CNN', (32, 33))]",[],[],[],[]
model,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .","[('proposes', (2, 3)), ('denoted', (9, 10)), ('for', (20, 21))]","[('unified multi-scale deep CNN', (4, 8)), ('multi -scale CNN ( MS - CNN )', (11, 19)), ('fast object detection', (21, 24))]",[],[],[],[]
model,Both of them are learned end - to - end and share computations .,"[('learned', (4, 5))]","[('end - to - end', (5, 10)), ('share computations', (11, 13))]",[],[],[],[]
model,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,"[('at', (3, 4)), ('combined to form', (8, 11))]","[('complimentary detectors', (1, 3)), ('different output layers', (4, 7)), ('strong multi-scale detector', (12, 15))]",[],[],[],[]
model,"This is shown to produce accurate object proposals on detection benchmarks with large variation of scale , such as KITTI , achieving a recall of over 95 % for only 100 proposals .","[('shown', (2, 3)), ('produce', (4, 5)), ('on', (8, 9)), ('with', (11, 12)), ('such as', (17, 19)), ('achieving', (21, 22)), ('of', (24, 25)), ('for', (28, 29))]","[('accurate', (5, 6)), ('object proposals', (6, 8)), ('detection benchmarks', (9, 11)), ('large variation of scale', (12, 16)), ('KITTI', (19, 20)), ('recall', (23, 24)), ('over 95 %', (25, 28)), ('only 100 proposals', (29, 32))]",[],[],[],[]
model,A second contribution of this work is the use of feature upsampling as an alternative to input upsampling .,"[('use of', (8, 10)), ('as', (12, 13))]","[('feature upsampling', (10, 12)), ('input upsampling', (16, 18))]",[],[],[],[]
experimental-setup,"Learning is initialized with the model generated by the first learning stage of the proposal network , described in Section 3.4 .","[('initialized with', (2, 4)), ('generated by', (6, 8)), ('of', (12, 13))]","[('Learning', (0, 1)), ('model', (5, 6)), ('first learning stage', (9, 12)), ('proposal network', (14, 16))]",[],[],[],[]
experimental-setup,"The learning rate is set to 0.0005 , and reduced by a factor of 10 times after every 10,000 iterations .","[('set to', (4, 6)), ('reduced by', (9, 11)), ('after', (16, 17))]","[('learning rate', (1, 3)), ('0.0005', (6, 7)), ('factor of 10 times', (12, 16)), ('every 10,000 iterations', (17, 20))]",[],[],[],[]
experimental-setup,"Learning stops after 25,000 iterations .","[('after', (2, 3))]","[('Learning', (0, 1)), ('stops', (1, 2)), ('25,000 iterations', (3, 5))]",[],[],[],[]
experimental-setup,The joint optimization of ( 6 ) is solved by back - propagation throughout the unified network .,"[('solved by', (8, 10)), ('throughout', (13, 14))]","[('joint optimization', (1, 3)), ('back - propagation', (10, 13)), ('unified network', (15, 17))]",[],[],[],[]
experimental-setup,"1 . Following , the parameters of layers "" conv 1 - 1 "" to "" conv2 - 2 "" are fixed during learning , for faster training .","[('of', (6, 7)), ('to', (14, 15)), ('fixed during', (21, 23)), ('for', (25, 26))]","[('parameters', (5, 6)), ('layers "" conv 1 - 1 ""', (7, 14)), ('learning', (23, 24)), ('faster training', (26, 28))]",[],[],[],[]
experiments,"Simply forwarding object patches , at the original scale , through the CNN impairs performance ( especially for small ones ) , since the pre-trained CNN models have a natural scale ( e.g. 224224 ) .","[('Simply forwarding', (0, 2)), ('at', (5, 6)), ('through', (10, 11))]","[('object patches', (2, 4)), ('original scale', (7, 9)), ('CNN impairs performance', (12, 15))]",[],[],[],[]
experiments,Context Embedding,[],[],[],[],[],[]
baselines,"Following , a model was trained for car detection and another for pedestrian / cyclist detection .","[('trained for', (5, 7)), ('another for', (10, 12))]","[('car detection', (7, 9)), ('pedestrian / cyclist detection', (12, 16))]",[],[],[],[]
code,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn.","[('implemented in', (3, 5)), ('within', (7, 8))]","[('C ++', (5, 7)), ('Caffe toolbox', (9, 11))]",[],[],[],[]
experimental-setup,An NVIDIA Titan GPU was used for CNN computations .,"[('used for', (5, 7))]","[('NVIDIA Titan GPU', (1, 4)), ('CNN computations', (7, 9))]",[],[],[],[]
results,"As expected , each layer has highest accuracy for the objects that match its scale .","[('for', (8, 9)), ('that match', (11, 13))]","[('each layer', (3, 5)), ('highest accuracy', (6, 8)), ('objects', (10, 11)), ('its scale', (13, 15))]",[],[],[],[]
results,"While the individual recall across scales is low , the combination of all detectors achieves high recall for all object scales .","[('across', (4, 5)), ('is', (6, 7)), ('combination', (10, 11)), ('achieves', (14, 15)), ('for', (17, 18))]","[('individual recall', (2, 4)), ('scales', (5, 6)), ('low', (7, 8)), ('all detectors', (12, 14)), ('high recall', (15, 17)), ('all object scales', (18, 21))]",[],[],[],[]
results,The effect of input size shows that the proposal network is fairly robust to the size of input images for cars and pedestrians .,"[('shows', (5, 6)), ('is', (10, 11)), ('to', (13, 14)), ('for', (19, 20))]","[('effect of input size', (1, 5)), ('proposal network', (8, 10)), ('fairly robust', (11, 13)), ('size of input images', (15, 19)), ('cars and pedestrians', (20, 23))]",[],[],[],[]
results,"shows that , for the MS - CNN , detection can substantially benefit proposal generation , especially for pedestrians .","[('for', (3, 4)), ('especially for', (16, 18))]","[('MS - CNN', (5, 8)), ('detection', (9, 10)), ('substantially benefit', (11, 13)), ('proposal generation', (13, 15)), ('pedestrians', (18, 19))]",[],[],[],[]
results,"Comparison with the state - of - the - art compares the proposal generation network to BING , Selective Search , EdgeBoxes , MCG , 3 DOP and RPN .","[('Comparison', (0, 1)), ('compares', (10, 11)), ('to', (15, 16))]","[('state - of - the - art', (3, 10)), ('proposal generation network', (12, 15)), ('BING', (16, 17)), ('Selective Search', (18, 20)), ('EdgeBoxes', (21, 22)), ('MCG', (23, 24)), ('3 DOP', (25, 27)), ('RPN', (28, 29))]",[],[],[],[]
results,The top row of the figure shows that the MS - CNN achieves a recall about 98 % with only 100 proposals .,"[('shows', (6, 7)), ('achieves', (12, 13)), ('with', (18, 19))]","[('MS - CNN', (9, 12)), ('recall', (14, 15)), ('about 98 %', (15, 18)), ('only 100 proposals', (19, 22))]",[],[],[],[]
results,"For pedestrian , bootstrapping and mixture are close , but random is much worse .","[('For', (0, 1)), ('are', (6, 7)), ('is', (11, 12))]","[('pedestrian', (1, 2)), ('bootstrapping and', (3, 5)), ('mixture', (5, 6)), ('close', (7, 8)), ('random', (10, 11)), ('much worse', (12, 14))]",[],[],[],[]
results,"As shown in , the deconvoltion layer helps inmost cases .","[('helps', (7, 8))]","[('deconvoltion layer', (5, 7)), ('inmost cases', (8, 10))]",[],[],[],[]
research-problem,"In WSMA - Seg , multimodal annotations are proposed to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .","[('In', (0, 1)), ('proposed to achieve', (8, 11)), ('using', (16, 17)), ('develop', (24, 25)), ('to trace', (31, 33)), ('of', (34, 35))]","[('WSMA - Seg', (1, 4)), ('multimodal annotations', (5, 7)), ('instance - aware segmentation', (12, 16)), ('weakly supervised bounding boxes', (17, 21)), ('run-data - based following algorithm', (26, 31)), ('contours', (33, 34)), ('objects', (35, 36))]",[],[],[],[]
research-problem,"In addition , we propose a multi-scale pooling segmentation ( MSP - Seg ) as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg. Experimental results on multiple datasets show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors .","[('propose', (4, 5)), ('as', (14, 15)), ('of', (19, 20)), ('to achieve', (23, 25)), ('to enhance', (30, 32)), ('of', (35, 36)), ('show', (44, 45))]","[('multi-scale pooling segmentation ( MSP - Seg )', (6, 14)), ('underlying segmentation model', (16, 19)), ('WSMA - Seg', (20, 23)), ('more accurate segmentation', (26, 29)), ('detection accuracy', (33, 35)), ('WSMA - Seg.', (36, 39)), ('outperforms', (52, 53)), ('state - of - the - art detectors', (54, 62))]",[],[],[],[]
research-problem,Object detection in images is one of the most widely explored tasks in computer vision .,[],"[('Object detection in images', (0, 4))]",[],[],[],[]
approach,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .","[('propose', (9, 10)), ('which uses', (23, 25)), ('to achieve', (27, 29)), ('without', (35, 36))]","[('weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', (11, 22)), ('segmentation models', (25, 27)), ('accurate and robust', (30, 33)), ('object detection', (33, 35)), ('NMS', (36, 37))]",[],[],[],[]
approach,"It consists of two phases , namely , a training and a testing phase .","[('consists of', (1, 3)), ('namely', (6, 7))]","[('two phases', (3, 5)), ('training', (9, 10)), ('testing phase', (12, 14))]",[],[],[],[]
approach,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .","[('In', (0, 1)), ('first converts', (8, 10)), ('in', (15, 16)), ('to', (18, 19)), ('like', (22, 23)), ('called', (25, 26)), ('trained using', (35, 37)), ('as', (39, 40)), ('to learn', (41, 43)), ('for', (45, 46))]","[('training phase', (2, 4)), ('weakly supervised bounding box annotations', (10, 15)), ('detection tasks', (16, 18)), ('multi-channel segmentation', (19, 21)), ('masks', (23, 24)), ('multimodal annotations', (26, 28)), ('segmentation model', (32, 34)), ('multimodal annotations', (37, 39)), ('labels', (40, 41)), ('multimodal heatmaps', (43, 45)), ('training images', (47, 49))]",[],[],[],[]
approach,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .","[('In', (0, 1)), ('of', (8, 9)), ('converted into', (14, 16)), ('based on', (22, 24)), ('conducted to generate', (38, 41)), ('for', (42, 43)), ('using', (44, 45)), ('created as', (56, 58)), ('of', (60, 61))]","[('testing phase', (2, 4)), ('resulting heatmaps', (6, 8)), ('given test image', (10, 13)), ('instance - aware segmentation map', (17, 22)), ('pixel - level logic operation', (25, 30)), ('contour tracing operation', (34, 37)), ('contours', (41, 42)), ('objects', (43, 44)), ('segmentation map', (46, 48)), ('bounding boxes of objects', (51, 55)), ('circumscribed quadrilaterals', (58, 60)), ('corresponding contours', (62, 64))]",[],[],[],[]
approach,"WSMA - Seg has the following advantages : ( i ) as an NMS - free solution , WSMA - Seg avoids all hyperparameters related to anchor boxes and NMS ; so , the above - mentioned threshold selection problem is also avoided ; ( ii ) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation - like multimodal annotations ; and ( iii ) multimodal annotations are pixel - level annotations ; so , they can describe the objects more accurately and overcome the above - mentioned environment noise problem .","[('avoids', (21, 22)), ('related to', (24, 26)), ('utilizing', (55, 56)), ('of', (59, 60)), ('are', (72, 73))]","[('WSMA', (0, 1)), ('all hyperparameters', (22, 24)), ('anchor boxes and NMS', (26, 30)), ('topological structure', (57, 59)), ('segmentation', (60, 61))]",[],[],[],[]
approach,"Therefore , in this work , we further propose a multi-scale pooling segmentation ( MSP - Seg ) model , which is used as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation ( especially for extreme cases , e.g. , very small objects ) , and consequently enhances the detection accuracy of WSMA - Seg .","[('further propose', (7, 9)), ('used as', (22, 24)), ('of', (28, 29)), ('to achieve', (32, 34)), ('enhances', (53, 54)), ('of', (57, 58))]","[('multi-scale pooling segmentation ( MSP - Seg ) model', (10, 19)), ('underlying segmentation model', (25, 28)), ('WSMA - Seg', (29, 32)), ('more accurate segmentation', (35, 38)), ('extreme cases', (41, 43)), ('detection accuracy', (55, 57)), ('WSMA - Seg', (58, 61))]",[],[],[],[]
approach,"We propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach to achieve an accurate and robust object detection without NMS , which is the first anchor-free and NMS - free object detection approach .","[('propose', (1, 2)), ('to achieve', (14, 16)), ('without', (22, 23))]","[('weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', (3, 14)), ('accurate and', (17, 19)), ('NMS', (23, 24))]",[],[],[],[]
approach,We propose multimodal annotations to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .,"[('propose', (1, 2)), ('to achieve', (4, 6)), ('using', (11, 12)), ('develop', (19, 20)), ('to trace', (26, 28))]","[('multimodal annotations', (2, 4)), ('instance - aware segmentation', (7, 11)), ('weakly supervised bounding boxes', (12, 16)), ('run-data - based following algorithm', (21, 26)), ('contours of objects', (28, 31))]",[],[],[],[]
approach,We propose a multi-scale pooling segmentation ( MSP - Seg ) model to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg .,"[('propose', (1, 2)), ('to achieve', (12, 14)), ('to enhance', (19, 21)), ('of', (24, 25))]","[('multi-scale pooling segmentation ( MSP - Seg ) model', (3, 12)), ('more accurate segmentation', (15, 18)), ('detection accuracy', (22, 24)), ('WSMA - Seg', (25, 28))]",[],[],[],[]
research-problem,Object Detection Using Segmentation Results and Contour Tracing,[],"[('Object Detection', (0, 2))]",[],[],[],[]
experiments,"As shown in , our proposed method with Stack = 2 , Base = 40 , Depth = 5 has achieved the best performance among all solutions in terms of F1 Score .","[('with', (7, 8)), ('achieved', (20, 21)), ('among', (24, 25)), ('in terms of', (27, 30))]","[('Stack = 2', (8, 11)), ('best performance', (22, 24)), ('all solutions', (25, 27)), ('F1 Score', (30, 32))]",[],[],[],[]
research-problem,RetinaFace : Single - stage Dense Face Localisation in the Wild,[],[],[],[],[],[]
research-problem,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .",[],"[('uncontrolled face detection', (7, 10)), ('accurate and efficient face localisation', (11, 16))]",[],[],[],[]
research-problem,"This paper presents a robust single - stage face detector , named RetinaFace , which performs pixel - wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self - supervised multi-task learning .","[('presents', (2, 3)), ('named', (11, 12)), ('performs', (15, 16)), ('on', (21, 22)), ('by taking advantages of', (26, 30))]","[('RetinaFace', (12, 13)), ('pixel - wise face localisation', (16, 21)), ('various scales of faces', (22, 26)), ('joint extra-supervised and self - supervised multi-task learning', (30, 38))]",[],[],[],[]
approach,( 2 ) We further add a selfsupervised mesh decoder branch for predicting a pixel - wise 3D shape face information in parallel with the existing supervised branches .,"[('add', (5, 6)), ('for predicting', (11, 13)), ('in parallel with', (21, 24))]","[('selfsupervised mesh decoder branch', (7, 11)), ('pixel - wise 3D shape face information', (14, 21)), ('existing supervised branches', (25, 28))]",[],[],[],[]
research-problem,Automatic face localisation is the prerequisite step of facial image analysis for many applications such as facial attribute ( e.g. expression and age ) and facial identity recognition .,[],"[('Automatic face localisation', (0, 3)), ('facial image analysis', (8, 11))]",[],[],[],[]
approach,The proposed single - stage pixel - wise face localisation method employs extra-supervised and self - supervised multi-task learning in parallel with the existing box classification and regression branches .,"[('employs', (11, 12)), ('in parallel with', (19, 22))]","[('single - stage pixel - wise face localisation method', (2, 11)), ('extra-supervised and self - supervised multi-task learning', (12, 19)), ('existing box classification and regression branches', (23, 29))]",[],[],[],[]
approach,"Each positive anchor outputs ( 1 ) a face score , ( 2 ) a face box , ( 3 ) five facial landmarks , and ( 4 ) dense 3 D face vertices projected on the image plane .","[('outputs', (3, 4)), ('projected on', (34, 36))]","[('Each positive anchor', (0, 3)), ('face score', (8, 10)), ('face box', (15, 17)), ('five facial landmarks', (21, 24)), ('dense 3 D face vertices', (29, 34)), ('image plane', (37, 39))]",[],[],[],[]
approach,"Following this route , we improve the single - stage face detection framework and propose a state - of - the - art dense face localisation method by exploiting multi-task losses coming from strongly supervised and self - supervised signals .","[('improve', (5, 6)), ('propose', (14, 15)), ('by exploiting', (27, 29)), ('coming from', (31, 33))]","[('single - stage face detection framework', (7, 13)), ('state - of - the - art dense face localisation method', (16, 27)), ('multi-task losses', (29, 31)), ('strongly supervised and self - supervised signals', (33, 40))]",[],[],[],[]
approach,"Inspired by , MTCNN and STN simultaneously detected faces and five facial landmarks .","[('Inspired', (0, 1)), ('simultaneously detected', (6, 8))]","[('MTCNN and STN', (3, 6)), ('faces', (8, 9)), ('five facial landmarks', (10, 13))]",[],[],[],[]
approach,"In this paper , we employ a mesh decoder branch through self - supervision learning for predicting a pixel - wise 3 D face shape in parallel with the existing supervised branches .","[('employ', (5, 6)), ('through', (10, 11)), ('for predicting', (15, 17)), ('in parallel with', (25, 28))]","[('mesh decoder branch', (7, 10)), ('self - supervision learning', (11, 15)), ('pixel - wise 3 D face shape', (18, 25)), ('existing supervised branches', (29, 32))]",[],[],[],[]
approach,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .","[('Based on', (0, 2)), ('propose', (9, 10)), ('named', (18, 19)), ('employs', (23, 24)), ('to simultaneously predict', (28, 31)), ('of', (46, 47))]","[('single - stage design', (3, 7)), ('novel pixel - wise face localisation method', (11, 18)), ('Reti- naFace', (19, 21)), ('multi-task learning strategy', (25, 28)), ('face score', (31, 33)), ('face box', (34, 36)), ('five facial landmarks', (37, 40)), ('3D position and', (42, 45)), ('correspondence', (45, 46)), ('each facial pixel', (47, 50))]",[],[],[],[]
experimental-setup,"For negative anchors , only classification loss is applied .","[('For', (0, 1)), ('applied', (8, 9))]","[('negative anchors', (1, 3)), ('classification loss', (5, 7))]",[],[],[],[]
experimental-setup,"We employ a shared loss head ( 1 1 conv ) across different feature maps H n W n 256 , n ? { 2 , . . . , 6 }.","[('employ', (1, 2)), ('across', (11, 12))]","[('shared loss head ( 1 1 conv )', (3, 11)), ('different feature maps', (12, 15))]",[],[],[],[]
experimental-setup,"We set the scale step at 2 1 / 3 and the aspect ratio at 1 : During training , anchors are matched to a ground - truth box when IoU is larger than 0.5 , and to the background when IoU is less than 0.3 .","[('set', (1, 2)), ('at', (5, 6)), ('matched to', (22, 24)), ('when', (29, 30)), ('larger than', (32, 34))]","[('scale step', (3, 5)), ('2 1 / 3', (6, 10)), ('aspect ratio', (12, 14)), ('ground - truth box', (25, 29)), ('IoU', (30, 31)), ('0.5', (34, 35))]",[],[],[],[]
experimental-setup,"Since most of the anchors ( > 99 % ) are negative after the matching step , we employ standard OHEM to alleviate significant imbalance between the positive and negative training examples .","[('are', (10, 11)), ('after', (12, 13)), ('employ', (18, 19)), ('to alleviate', (21, 23)), ('between', (25, 26))]","[('most of the anchors ( > 99 % )', (1, 10)), ('negative', (11, 12)), ('matching step', (14, 16)), ('standard OHEM', (19, 21)), ('significant imbalance', (23, 25)), ('positive and negative training examples', (27, 32))]",[],[],[],[]
experimental-setup,"More specifically , we sort negative anchors by the loss values and select the top ones so that the ratio between the negative and positive samples is at least 3:1 .","[('sort', (4, 5)), ('by', (7, 8)), ('select', (12, 13)), ('so that', (16, 18)), ('between', (20, 21)), ('is', (26, 27))]","[('negative anchors', (5, 7)), ('loss values', (9, 11)), ('top ones', (14, 16)), ('ratio', (19, 20)), ('negative and positive samples', (22, 26)), ('at least 3:1', (27, 30))]",[],[],[],[]
experimental-setup,"We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .","[('train', (1, 2)), ('using', (4, 5)), ('on', (23, 24))]","[('RetinaFace', (3, 4)), ('SGD optimiser', (5, 7)), ('momentum', (8, 9)), ('0.9', (10, 11)), ('four NVIDIA Tesla P40 ( 24GB ) GPUs', (24, 32))]",[],[],[],[]
experimental-setup,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .","[('starts from', (3, 5)), ('rising to', (9, 11)), ('after', (14, 15)), ('divided by', (19, 21)), ('at', (22, 23))]","[('learning rate', (1, 3)), ('10 ? 3', (5, 8)), ('10 ? 2', (11, 14)), ('5 epochs', (15, 17)), ('10', (21, 22))]",[],[],[],[]
experimental-setup,The training process terminates at 80 epochs .,"[('terminates at', (3, 5))]","[('training process', (1, 3)), ('80 epochs', (5, 7))]",[],[],[],[]
experimental-setup,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,"[('applied on', (6, 8)), ('using', (15, 16)), ('at', (19, 20))]","[('Box voting', (0, 2)), ('union set of predicted face boxes', (9, 15)), ('IoU threshold', (17, 19)), ('0.4', (20, 21))]",[],[],[],[]
ablation-analysis,"By applying the practices of state - of - the - art techniques ( i.e. FPN , context module , and deformable convolution ) , we setup a strong baseline ( 91.286 % ) , which is slightly better than ISRN ( 90.9 % ) .","[('applying', (1, 2)), ('i.e.', (14, 15)), ('setup', (26, 27)), ('slightly better than', (37, 40))]","[('practices', (3, 4)), ('state - of - the - art techniques', (5, 13)), ('context module', (17, 19)), ('deformable convolution', (21, 23)), ('strong baseline ( 91.286 % )', (28, 34)), ('ISRN ( 90.9 % )', (40, 45))]",[],[],[],[]
ablation-analysis,"Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .","[('Adding', (0, 1)), ('on', (24, 25))]","[('branch of five facial landmark regression', (2, 8)), ('significantly improves', (8, 10)), ('face box AP ( 0.408 % )', (11, 18)), ('mAP ( 0.775 % )', (19, 24)), ('Hard subset', (26, 28))]",[],[],[],[]
ablation-analysis,"By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .","[('adding', (3, 4)), ('increases', (8, 9)), ('on', (13, 14)), ('on', (23, 24))]","[('dense regression branch', (5, 8)), ('face box AP', (10, 13)), ('Easy and Medium subsets', (14, 18)), ('slightly deteriorates', (19, 21)), ('results', (22, 23)), ('Hard subset', (25, 27))]",[],[],[],[]
ablation-analysis,"Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .","[('jointly', (7, 8)), ('enables', (8, 9)), ('compared to', (12, 14))]","[('learning', (2, 3)), ('landmark and dense regression', (3, 7)), ('further improvement', (10, 12)), ('adding landmark regression only', (14, 18))]",[],[],[],[]
ablation-analysis,"This demonstrates that landmark regression does help dense regression , which in turn boosts face detection performance even further .","[('demonstrates', (1, 2)), ('does help', (5, 7)), ('boosts', (13, 14))]","[('landmark regression', (3, 5)), ('dense regression', (7, 9)), ('face detection performance', (14, 17))]",[],[],[],[]
results,Our approach outper - forms these state - of - the - art methods in terms of AP .,"[('in terms of', (14, 17))]","[('outper', (2, 3)), ('state - of - the - art methods', (6, 14)), ('AP', (17, 18))]",[],[],[],[]
results,"More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .","[('produces', (4, 5)), ('in', (8, 9)), ('of', (11, 12)), ('i.e.', (18, 19)), ('for', (37, 38)), ('for', (59, 60))]","[('RetinaFace', (3, 4)), ('best AP', (6, 8)), ('all subsets', (9, 11)), ('validation and test sets', (13, 17)), ('96.9 % ( Easy )', (20, 25)), ('96.1 % ( Medium )', (26, 31)), ('91.8 % ( Hard )', (32, 37)), ('validation set', (38, 40)), ('96.3 % ( Easy )', (42, 47)), ('95.6 % ( Medium )', (48, 53)), ('91.4 % ( Hard )', (54, 59)), ('test set', (60, 62))]",[],[],[],[]
results,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .","[('Compared to', (0, 2)), ('sets up', (12, 14)), ('on', (25, 26)), ('which contains', (29, 31))]","[('recent best performed method', (3, 7)), ('Reti - na Face', (8, 12)), ('new impressive record ( 91.4 % v.s. 90.3 % )', (15, 25)), ('Hard subset', (27, 29)), ('large number of tiny faces', (32, 37))]",[],[],[],[]
results,"Besides accurate bounding boxes , the five facial landmarks predicted by Retina Face are also very robust under the variations of pose , occlusion and resolution .","[('Besides', (0, 1)), ('predicted by', (9, 11)), ('are', (13, 14)), ('under', (17, 18))]","[('accurate bounding boxes', (1, 4)), ('five facial landmarks', (6, 9)), ('Retina Face', (11, 13)), ('very robust', (15, 17)), ('variations of pose , occlusion and resolution', (19, 26))]",[],[],[],[]
results,RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .,"[('from', (10, 11)), ('to', (13, 14)), ('compared to', (17, 19))]","[('RetinaFace', (0, 1)), ('significantly decreases', (1, 3)), ('normalised mean errors ( NME )', (4, 10)), ('2.72 %', (11, 13)), ('2.21 %', (14, 16)), ('MTCNN', (19, 20))]",[],[],[],[]
results,"Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .","[('Compared to', (0, 2)), ('from', (10, 11)), ('to', (13, 14))]","[('MTCNN', (2, 3)), ('RetinaFace', (4, 5)), ('significantly', (5, 6)), ('failure rate', (8, 10)), ('26.31 %', (11, 13)), ('9.37 %', (14, 16))]",[],[],[],[]
results,"In this paper , we demonstrate how our face detection method can boost the performance of a state - of - the - art publicly available face recognition method , i.e. ArcFace .","[('demonstrate', (5, 6)), ('of', (15, 16)), ('i.e.', (30, 31))]","[('our face detection method', (7, 11)), ('boost', (12, 13)), ('performance', (14, 15)), ('state - of - the - art publicly available face recognition method', (17, 29)), ('ArcFace', (31, 32))]",[],[],[],[]
results,"The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .","[('on', (2, 3)), ('demonstrate', (7, 8)), ('boost', (14, 15)), ('from', (19, 20)), ('to', (22, 23))]","[('CFP - FP', (3, 6)), ('Reti - na Face', (9, 13)), (""ArcFace 's verification accuracy"", (15, 19)), ('98.37 %', (20, 22)), ('99.49 %', (23, 25))]",[],[],[],[]
approach,We employ two tricks ( i.e. flip test and face detection score to weigh samples within templates ) to progressively improve the face verification accuracy .,"[('employ', (1, 2)), ('to', (12, 13)), ('within', (15, 16)), ('to progressively improve', (18, 21))]","[('two tricks', (2, 4)), ('flip test', (6, 8)), ('face detection score', (9, 12)), ('weigh samples', (13, 15)), ('templates', (16, 17)), ('face verification accuracy', (22, 25))]",[],[],[],[]
results,"Under fair comparison , TAR ( at FAR = 1 e ? 6 ) significantly improves from 88 . 29 % to 89.59 % simply by replacing MTCNN with RetinaFace .","[('Under', (0, 1)), ('at', (6, 7)), ('from', (16, 17)), ('to', (21, 22)), ('by replacing', (25, 27)), ('with', (28, 29))]","[('fair comparison', (1, 3)), ('TAR (', (4, 6)), ('FAR = 1 e ? 6 )', (7, 14)), ('significantly improves', (14, 16)), ('88 . 29 %', (17, 21)), ('89.59 %', (22, 24)), ('MTCNN', (27, 28)), ('RetinaFace', (29, 30))]",[],[],[],[]
results,Inference Efficiency,[],[],[],[],[],[]
research-problem,WIDER FACE : A Face Detection Benchmark,[],"[('Face Detection', (4, 6))]",[],[],[],[]
dataset,We introduce a large - scale face detection dataset called WIDER FACE .,"[('introduce', (1, 2)), ('called', (9, 10))]","[('large - scale face detection dataset', (3, 9)), ('WIDER FACE', (10, 12))]",[],[],[],[]
model,"We show an example of using WIDER FACE through proposing a multi-scale two - stage cascade framework , which uses divide and conquer strategy to deal with large scale variations .","[('through proposing', (8, 10)), ('uses', (19, 20)), ('to deal with', (24, 27))]","[('WIDER', (6, 7)), ('multi-scale two - stage cascade framework', (11, 17)), ('divide and conquer strategy', (20, 24)), ('large scale variations', (27, 30))]",[],[],[],[]
results,"Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .","[('on', (4, 5)), ('with', (8, 9)), ('as', (12, 13))]","[('Faceness', (0, 1)), ('outperforms', (1, 2)), ('other methods', (2, 4)), ('three subsets', (5, 7)), ('DPM and ACF', (9, 12)), ('marginal second and third', (13, 17))]",[],[],[],[]
results,The results of small scale are abysmal : none of the algorithms is able to achieve more than 12 % AP .,"[('of', (2, 3)), ('are', (5, 6))]","[('results', (1, 2)), ('small scale', (3, 5)), ('abysmal', (6, 7))]",[],[],[],[]
results,"In , we show the impact of occlusion on detecting faces with a height of at least 30 pixels .","[('show', (3, 4)), ('of', (6, 7)), ('on', (8, 9)), ('with', (11, 12)), ('of', (14, 15))]","[('impact', (5, 6)), ('occlusion', (7, 8)), ('detecting faces', (9, 11)), ('height', (13, 14)), ('at least 30 pixels', (15, 19))]",[],[],[],[]
results,The maximum AP is only 26.5 % achieved by Faceness .,"[('is', (3, 4)), ('achieved by', (7, 9))]","[('maximum AP', (1, 3)), ('only 26.5 %', (4, 7)), ('Faceness', (9, 10))]",[],[],[],[]
results,The best performance of baseline methods drops to 14.4 % .,"[('of', (3, 4))]","[('best performance', (1, 3)), ('baseline methods', (4, 6)), ('drops', (6, 7)), ('14.4 %', (8, 10))]",[],[],[],[]
results,"It is worth noting that Faceness and DPM , which are part based models , already perform relatively better than other methods on occlusion handling .","[('worth noting', (2, 4)), ('perform', (16, 17)), ('than', (19, 20)), ('on', (22, 23))]","[('Faceness and DPM', (5, 8)), ('part based models', (11, 14)), ('relatively better', (17, 19)), ('other methods', (20, 22)), ('occlusion handling', (23, 25))]",[],[],[],[]
results,"The best performance is achieved by Faceness , with a recall below 20 % .","[('achieved by', (4, 6)), ('with', (8, 9))]","[('best performance', (1, 3)), ('Faceness', (6, 7)), ('recall below 20 %', (10, 14))]",[],[],[],[]
results,"Among the four baseline methods , Faceness tends to outperform the other methods .","[('Among', (0, 1)), ('tends to', (7, 9))]","[('four baseline methods', (2, 5)), ('Faceness', (6, 7)), ('outperform', (9, 10)), ('other methods', (11, 13))]",[],[],[],[]
results,WIDER FACE as an Effective Training Source,[],"[('WIDER FACE', (0, 2))]",[],[],[],[]
results,"As shown in , the retrained models perform consistently better than the baseline models .","[('perform', (7, 8)), ('than', (10, 11))]","[('retrained models', (5, 7)), ('consistently better', (8, 10)), ('baseline models', (12, 14))]",[],[],[],[]
results,The average AP improvement of retrained ACF detector is 5.4 % in comparison to baseline ACF detector .,"[('of', (4, 5)), ('is', (8, 9)), ('in comparison to', (11, 14))]","[('average AP improvement', (1, 4)), ('retrained ACF detector', (5, 8)), ('5.4 %', (9, 11)), ('baseline ACF detector', (14, 17))]",[],[],[],[]
results,"For the Faceness , the retrained Faceness model obtain 4.2 % improvement on WIDER hard test set .","[('For', (0, 1)), ('obtain', (8, 9)), ('on', (12, 13))]","[('Faceness', (2, 3)), ('retrained Faceness model', (5, 8)), ('4.2 % improvement', (9, 12)), ('WIDER hard test set', (13, 17))]",[],[],[],[]
results,"The retrained ACF detector achieves a recall rate of 87.48 % , outperforms the baseline ACF by a considerable margin of 1.4 % .","[('achieves', (4, 5)), ('of', (8, 9)), ('by', (16, 17)), ('of', (20, 21))]","[('retrained ACF detector', (1, 4)), ('recall rate', (6, 8)), ('87.48 %', (9, 11)), ('outperforms', (12, 13)), ('baseline ACF', (14, 16)), ('considerable margin', (18, 20)), ('1.4 %', (21, 23))]",[],[],[],[]
results,The recall rate improvement of the retrained Faceness detector is 0.8 % in comparison to the baseline Faceness detector .,"[('of', (4, 5)), ('is', (9, 10)), ('in comparison to', (12, 15))]","[('recall rate improvement', (1, 4)), ('retrained Faceness detector', (6, 9)), ('0.8 %', (10, 12)), ('baseline Faceness detector', (16, 19))]",[],[],[],[]
results,"As shown in , the multi-scale cascade CNN obtains 8.5 % AP improvement on the WIDER Hard subset compared to the retrained Faceness , suggesting its superior capability in handling faces with different scales .","[('obtains', (8, 9)), ('on', (13, 14)), ('compared to', (18, 20))]","[('multi-scale cascade CNN', (5, 8)), ('8.5 % AP improvement', (9, 13)), ('WIDER Hard subset', (15, 18)), ('retrained Faceness', (21, 23))]",[],[],[],[]
results,"For the WIDER Medium subset , the multi-scale cascade CNN outperforms other baseline methods with a considerable margin .","[('For', (0, 1)), ('outperforms', (10, 11)), ('with', (14, 15))]","[('WIDER Medium subset', (2, 5)), ('multi-scale cascade CNN', (7, 10)), ('other baseline methods', (11, 14)), ('considerable margin', (16, 18))]",[],[],[],[]
research-problem,FaceBoxes : A CPU Real - time Face Detector with High Accuracy,[],"[('CPU Real - time Face Detector', (3, 9))]",[],[],[],[]
research-problem,"To address this challenge , we propose a novel face detector , named FaceBoxes , with superior performance on both speed and accuracy .","[('propose', (6, 7)), ('named', (12, 13)), ('with', (15, 16)), ('on', (18, 19))]","[('novel face detector', (8, 11)), ('FaceBoxes', (13, 14)), ('superior performance', (16, 18)), ('both speed and accuracy', (19, 23))]",[],[],[],[]
code,Code is available at https://github.com/sfzhang15/FaceBoxes .,[],"[('https://github.com/sfzhang15/FaceBoxes', (4, 5))]",[],[],[],[]
research-problem,Face detection is one of the fundamental problems in computer vision and pattern recognition .,[],"[('Face detection', (0, 2))]",[],[],[],[]
model,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .","[('inspired by', (4, 6)), ('develop', (21, 22)), ('with', (32, 33)), ('on', (37, 38))]","[('RPN', (7, 8)), ('state - of - the - art face detector', (23, 32)), ('real - time speed', (33, 37)), ('CPU', (39, 40))]",[],[],[],[]
model,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .","[('propose', (3, 4)), ('named', (8, 9)), ('contains', (13, 14)), ('trained', (23, 24))]","[('novel face detector', (5, 8)), ('FaceBoxes', (9, 10)), ('single fully convolutional neural network', (15, 20)), ('end - to - end', (24, 29))]",[],[],[],[]
model,We design the Rapidly Digested Convolutional Layers ( RDCL ) to enable face detection to achieve real - time speed on the CPU ; We introduce the Multiple Scale Convolutional Layers ( MSCL ) to handle various scales of face via enriching receptive fields and discretizing anchors over layers .,"[('design', (1, 2)), ('to enable', (10, 12)), ('to achieve', (14, 16)), ('on', (20, 21)), ('introduce', (25, 26)), ('to handle', (34, 36)), ('via', (40, 41))]","[('Rapidly Digested Convolutional Layers ( RDCL )', (3, 10)), ('face detection', (12, 14)), ('real - time speed', (16, 20)), ('CPU', (22, 23)), ('Multiple Scale Convolutional Layers ( MSCL )', (27, 34)), ('various scales of face', (36, 40)), ('enriching receptive fields', (41, 44)), ('discretizing anchors over', (45, 48)), ('layers', (48, 49))]",[],[],[],[]
hyperparameters,"All the parameters are randomly initialized with the "" xavier "" method .","[('with', (6, 7))]","[('parameters', (2, 3)), ('randomly initialized', (4, 6)), ('"" xavier "" method', (8, 12))]",[],[],[],[]
hyperparameters,"We finetune the resulting model using SGD with 0.9 momentum , 0.0005 weight decay and batch size 32 .","[('finetune', (1, 2)), ('using', (5, 6)), ('with', (7, 8))]","[('resulting model', (3, 5)), ('SGD', (6, 7)), ('0.9 momentum', (8, 10)), ('0.0005 weight decay', (11, 14)), ('batch size', (15, 17)), ('32', (17, 18))]",[],[],[],[]
hyperparameters,"The maximum number of iterations is 120 k and we use 10 ? 3 learning rate for the first 80 k iterations , then continue training for 20 k iterations with 10 ? 4 and 10 ? 5 , respectively .","[('is', (5, 6)), ('use', (10, 11)), ('for', (16, 17)), ('continue', (24, 25)), ('for', (26, 27)), ('with', (30, 31))]","[('maximum number of iterations', (1, 5)), ('120 k', (6, 8)), ('10 ? 3 learning rate', (11, 16)), ('first 80 k iterations', (18, 22)), ('training', (25, 26)), ('20 k iterations', (27, 30)), ('10 ? 4 and 10 ? 5', (31, 38))]",[],[],[],[]
hyperparameters,Our method is implemented in the Caffe library .,"[('implemented in', (3, 5))]","[('Caffe library', (6, 8))]",[],[],[],[]
results,"As listed in Tab. 1 , comparing with recent CNN - based methods , our FaceBoxes can run at 20 FPS on the CPU with state - of - the - art accuracy .","[('comparing', (6, 7)), ('run at', (17, 19)), ('on', (21, 22)), ('with', (24, 25))]","[('our FaceBoxes', (14, 16)), ('20 FPS', (19, 21)), ('CPU', (23, 24)), ('state - of - the - art accuracy', (25, 33))]",[],[],[],[]
ablation-analysis,"2 indicates that MSCL effectively increases the m AP by 1.0 % , owning to the diverse receptive fields and the multi -scale anchor tiling mechanism .","[('indicates', (1, 2)), ('effectively increases', (4, 6)), ('by', (9, 10)), ('owning to', (13, 15))]","[('MSCL', (3, 4)), ('m AP', (7, 9)), ('1.0 %', (10, 12))]",[],[],[],[]
ablation-analysis,RDCL is efficient and accuracy - preserving .,[],"[('RDCL', (0, 1))]",[],[],[],[]
results,"As illustrated in , our FaceBoxes outperforms all others by a large margin .","[('by', (9, 10))]","[('our FaceBoxes', (4, 6)), ('outperforms', (6, 7)), ('all others', (7, 9)), ('large margin', (11, 13))]",[],[],[],[]
results,shows some qualitative results on the AFW dataset .,"[('on', (4, 5))]","[('AFW dataset', (6, 8))]",[],[],[],[]
results,"Our method significantly outperforms all other methods and commercial face detectors ( e.g. , SkyBiometry , Face + + and Picasa ) .",[],"[('significantly outperforms', (2, 4)), ('all other methods and commercial face detectors', (4, 11))]",[],[],[],[]
results,shows some qualitative results on the PASCAL face dataset .,"[('on', (4, 5))]","[('PASCAL face dataset', (6, 9))]",[],[],[],[]
research-problem,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition",[],"[('Face Detection', (8, 10)), ('Landmark Localization', (11, 13)), ('Gender', (18, 19))]",[],[],[],[]
research-problem,"D ETECTION and analysis of faces is a challenging problem in computer vision , and has been actively researched for applications such as face verification , face tracking , person identification , etc .",[],"[('D ETECTION and analysis of faces', (0, 6))]",[],[],[],[]
model,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .","[('present', (5, 6)), ('based on', (9, 11)), ('for', (12, 13)), ('from', (27, 28))]","[('novel framework', (7, 9)), ('CNNs', (11, 12)), ('simultaneous face detection', (13, 16)), ('facial landmarks localization', (17, 20)), ('head pose estimation', (21, 24)), ('gender recognition', (25, 27)), ('given image', (29, 31))]",[],[],[],[]
model,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,"[('design', (1, 2)), ('to learn', (5, 7)), ('for', (9, 10)), ('exploit', (13, 14))]","[('CNN architecture', (3, 5)), ('common features', (7, 9)), ('tasks', (11, 12)), ('synergy', (15, 16))]",[],[],[],[]
model,We exploit the fact that information contained in features is hierarchically distributed throughout the network as demonstrated in .,"[('exploit', (1, 2)), ('contained in', (6, 8))]","[('information', (5, 6)), ('features', (8, 9)), ('hierarchically distributed', (10, 12)), ('network', (14, 15))]",[],[],[],[]
model,Features fusion aims to transform the features to a common subspace where they can be combined linearly or non-linearly .,[],"[('Features fusion', (0, 2))]",[],[],[],[]
model,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .","[('construct', (3, 4)), ('to fuse', (9, 11))]","[('separate fusion - CNN', (5, 9)), ('hyperfeatures', (12, 13))]",[],[],[],[]
model,Fusing the intermediate layer features provides additional performance boost .,"[('Fusing', (0, 1)), ('provides', (5, 6))]","[('intermediate layer features', (2, 5)), ('additional performance boost', (6, 9))]",[],[],[],[]
research-problem,"Since then , several approaches have adopted MTL for solving different problems in computer vision .",[],"[('MTL', (7, 8))]",[],[],[],[]
model,This method is based on a mixture of trees with a shared pool of parts in the sense that every facial landmark is modeled as apart and uses global mixtures to capture the topological changes due to viewpoint variations .,"[('based on', (3, 5)), ('with', (9, 10)), ('modeled as', (23, 25)), ('uses', (27, 28)), ('to capture', (30, 32)), ('due to', (35, 37))]","[('mixture of trees', (6, 9)), ('shared pool of parts', (11, 15)), ('every facial landmark', (19, 22)), ('apart', (25, 26)), ('global mixtures', (28, 30)), ('topological changes', (33, 35)), ('viewpoint variations', (37, 39))]",[],[],[],[]
model,It fuses all the intermediate layers of a CNN at three different scales of the image pyramid for multi-task training on diverse sets .,"[('fuses', (1, 2)), ('of', (6, 7)), ('at', (9, 10)), ('of', (13, 14)), ('for', (17, 18)), ('on', (20, 21))]","[('all the intermediate layers', (2, 6)), ('CNN', (8, 9)), ('three different scales', (10, 13)), ('image pyramid', (15, 17)), ('multi-task training', (18, 20)), ('diverse sets', (21, 23))]",[],[],[],[]
model,"Instead , we strategically design the network architecture such that the tasks exploit low level as well as high level features of the network .","[('strategically design', (3, 5)), ('such that', (8, 10)), ('exploit', (12, 13)), ('of', (21, 22))]","[('network architecture', (6, 8)), ('tasks', (11, 12)), ('low level as well as high level features', (13, 21)), ('network', (23, 24))]",[],[],[],[]
research-problem,We also jointly predict the task of face detection and landmark localization .,"[('jointly predict', (2, 4))]","[('task', (5, 6)), ('face detection and landmark localization', (7, 12))]",[],[],[],[]
model,Landmarks localization :,[],"[('Landmarks localization', (0, 2))]",[],[],[],[]
model,"While the former learns the shape increment given a mean initial shape , the latter trains an appearance model to predict the keypoint locations .","[('learns', (3, 4)), ('given', (7, 8)), ('trains', (15, 16)), ('to predict', (19, 21))]","[('former', (2, 3)), ('shape increment', (5, 7)), ('mean initial shape', (9, 12)), ('appearance model', (17, 19)), ('keypoint locations', (22, 24))]",[],[],[],[]
experiments,Gender recognition :,[],"[('Gender recognition', (0, 2))]",[],[],[],[]
hyperparameters,"It provides annotations for 21 landmark points per face , along with the face bounding - box , face pose ( yaw , pitch and roll ) and gender information .","[('provides', (1, 2)), ('for', (3, 4)), ('along with', (10, 12))]","[('annotations', (2, 3)), ('21 landmark points per face', (4, 9)), ('face bounding - box', (13, 17)), ('face pose ( yaw , pitch and roll )', (18, 27)), ('gender information', (28, 30))]",[],[],[],[]
experiments,We use the Selective Search algorithm in R - CNN to generate region proposals for faces in an image .,"[('use', (1, 2)), ('in', (6, 7)), ('to generate', (10, 12)), ('for', (14, 15)), ('in', (16, 17))]","[('Selective Search algorithm', (3, 6)), ('R - CNN', (7, 10)), ('region proposals', (12, 14)), ('faces', (15, 16)), ('image', (18, 19))]",[],[],[],[]
hyperparameters,We use 21 point markups for face landmarks locations as provided in the AFLW dataset .,"[('use', (1, 2)), ('for', (5, 6)), ('provided', (10, 11))]","[('21 point markups', (2, 5)), ('face landmarks locations', (6, 9)), ('AFLW dataset', (13, 15))]",[],[],[],[]
experiments,We also learn the visibility factor in order to test the presence of the predicted landmark .,"[('learn', (2, 3)), ('in order to test', (6, 10)), ('of', (12, 13))]","[('visibility factor', (4, 6)), ('presence', (11, 12)), ('predicted landmark', (14, 16))]",[],[],[],[]
experiments,Gender Recognition :,[],"[('Gender Recognition', (0, 2))]",[],[],[],[]
experiments,Predicting gender is a two class problem similar to face detection .,"[('Predicting', (0, 1)), ('is', (2, 3)), ('similar to', (7, 9))]","[('two class problem', (4, 7)), ('face detection', (9, 11))]",[],[],[],[]
experiments,IRP improves the recall by generating more candidate proposals by using the predicted landmarks information from the initial set of region proposals .,"[('improves', (1, 2)), ('by generating', (4, 6)), ('by using', (9, 11)), ('from', (15, 16))]","[('IRP', (0, 1)), ('recall', (3, 4)), ('more candidate proposals', (6, 9)), ('predicted landmarks information', (12, 15)), ('initial set of region proposals', (17, 22))]",[],[],[],[]
baselines,R- CNN,[],[],[],[],[],[]
experiments,We also perform a linear bounding box regression to localize the face co-ordinates .,"[('perform', (2, 3)), ('to localize', (8, 10))]","[('linear bounding box regression', (4, 8)), ('face co-ordinates', (11, 13))]",[],[],[],[]
research-problem,Fast - HyperFace,[],[],[],[],[],[]
experiments,The Hyperface method is tested on a machine with 8 cores and GTX TITAN - X GPU .,"[('tested on', (4, 6)), ('with', (8, 9))]","[('Hyperface method', (1, 3)), ('machine', (7, 8)), ('8 cores', (9, 11)), ('GTX TITAN - X GPU', (12, 17))]",[],[],[],[]
model,We also propose a fast version of HyperFace which uses a high recall fast face detector instead of Selective Search to generate candidate region proposals .,"[('propose', (2, 3)), ('of', (6, 7)), ('uses', (9, 10)), ('instead of', (16, 18)), ('to generate', (20, 22))]","[('fast version', (4, 6)), ('HyperFace', (7, 8)), ('high recall fast face detector', (11, 16)), ('Selective Search', (18, 20)), ('candidate region proposals', (22, 25))]",[],[],[],[]
model,We implement a face detector using Single Shot Detector ( SSD ) framework .,"[('implement', (1, 2)), ('using', (5, 6))]","[('face detector', (3, 5)), ('Single Shot Detector ( SSD ) framework', (6, 13))]",[],[],[],[]
experiments,"Fast - HyperFace consumes a total time of 0.15s ( 0.05 s for SSD face detector , and 0.1s for HyperFace ) on a GTX TITAN X GPU .","[('consumes', (3, 4)), ('of', (7, 8)), ('on', (22, 23))]","[('Fast - HyperFace', (0, 3)), ('total time', (5, 7)), ('0.15s', (8, 9)), ('GTX TITAN X GPU', (24, 28))]",[],[],[],[]
results,"The Fast - HyperFace achieves a m AP of 97.6 % on AFW face detection task , which is comparable to the HyperFace m AP of 97.9 % .","[('achieves', (4, 5)), ('of', (8, 9)), ('on', (11, 12))]","[('Fast - HyperFace', (1, 4)), ('m AP', (6, 8)), ('97.6 %', (9, 11)), ('AFW face detection task', (12, 16))]",[],[],[],[]
experiments,"Thus , Fast - HyperFace improves the speed by a factor of 12 with negligible degradation in performance .","[('improves', (5, 6)), ('by', (8, 9)), ('with', (13, 14)), ('in', (16, 17))]","[('Fast - HyperFace', (2, 5)), ('speed', (7, 8)), ('factor of 12', (10, 13)), ('negligible degradation', (14, 16)), ('performance', (17, 18))]",[],[],[],[]
research-problem,Pyramid Box : A Context - assisted Single Shot Face Detector,[],[],[],[],[],[]
code,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,[],[],[],[],[],[]
research-problem,Face detection is a fundamental and essential task in various face applications .,[],"[('Face detection', (0, 2))]",[],[],[],[]
model,Face R - FCN re-weights embedding responses on score maps and eliminates the effect of non-uniformed contribution in each facial part using a position - sensitive average pooling .,"[('re-weights', (4, 5)), ('on', (7, 8)), ('eliminates', (11, 12)), ('in', (17, 18)), ('using', (21, 22))]","[('Face R - FCN', (0, 4)), ('embedding responses', (5, 7)), ('score maps', (8, 10)), ('effect of non-uniformed contribution', (13, 17)), ('each facial part', (18, 21)), ('position - sensitive average pooling', (23, 28))]",[],[],[],[]
model,FAN proposes an anchor - level attention by highlighting the features from the face region to detect the occluded faces .,"[('proposes', (1, 2)), ('by highlighting', (7, 9)), ('from', (11, 12)), ('to detect', (15, 17))]","[('FAN', (0, 1)), ('anchor - level attention', (3, 7)), ('features', (10, 11)), ('face region', (13, 15)), ('occluded faces', (18, 20))]",[],[],[],[]
model,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .","[('use', (5, 6)), ('to generate', (9, 11)), ('for', (13, 14)), ('related to', (16, 18)), ('called', (24, 25)), ('invented to be easily added to', (27, 33))]","[('semi-supervised solution', (7, 9)), ('approximate labels', (11, 13)), ('contextual parts', (14, 16)), ('faces', (18, 19)), ('series of anchors', (21, 24)), ('PyramidAnchors', (25, 26)), ('general anchor - based architectures', (33, 38))]",[],[],[],[]
model,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,"[('introduce', (1, 2)), ('to incorporate', (11, 13)), ('around', (15, 16)), ('with', (19, 20))]","[('Context - sensitive prediction module ( CPM )', (3, 11)), ('context information', (13, 15)), ('target face', (17, 19)), ('wider and deeper network', (21, 25))]",[],[],[],[]
model,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .","[('propose', (3, 4)), ('for', (11, 12)), ('to further improve', (15, 18)), ('of', (20, 21))]","[('max - in - out layer', (5, 11)), ('prediction module', (13, 15)), ('capability', (19, 20)), ('classification network', (21, 23))]",[],[],[],[]
model,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .","[('propose', (4, 5)), ('named as', (8, 10)), ('to make', (15, 17)), ('on', (19, 20)), ('of', (22, 23))]","[('training strategy', (6, 8)), ('Data - anchor - sampling', (10, 15)), ('adjustment', (18, 19)), ('distribution', (21, 22)), ('training dataset', (24, 26))]",[],[],[],[]
model,"We propose an anchor-based context assisted method , called PyramidAnchors , to introduce supervised information on learning contextual features for small , blurred and partially occluded faces .","[('propose', (1, 2)), ('called', (8, 9)), ('to introduce', (11, 13)), ('on learning', (15, 17)), ('for', (19, 20))]","[('anchor-based context assisted method', (3, 7)), ('PyramidAnchors', (9, 10)), ('supervised information', (13, 15)), ('contextual features', (17, 19)), ('small , blurred and partially occluded faces', (20, 27))]",[],[],[],[]
model,2 . We design the Low - level Feature Pyramid Networks ( LFPN ) to merge contextual features and facial features better .,"[('design', (3, 4)), ('to merge', (14, 16))]","[('Low - level Feature Pyramid Networks ( LFPN )', (5, 14)), ('contextual features and facial features', (16, 21)), ('better', (21, 22))]",[],[],[],[]
model,"3 . We introduce a context - sensitive prediction module , consisting of a mixed network structure and max - in - out layer to learn accurate location and classification from the merged features .","[('introduce', (3, 4)), ('consisting of', (11, 13)), ('to learn', (24, 26)), ('from', (30, 31))]","[('context - sensitive prediction module', (5, 10)), ('mixed network structure', (14, 17)), ('max - in - out layer', (18, 24)), ('accurate location and classification', (26, 30)), ('merged features', (32, 34))]",[],[],[],[]
experiments,"Data sampling is a classical subject in statistics , machine learning and pattern recognition , it achieves great development in recent years .",[],"[('Data sampling', (0, 2))]",[],[],[],[]
experiments,"For the task of objection detection , Focus Loss address the class imbalance by reshaping the standard cross entropy loss .","[('of', (3, 4)), ('address', (9, 10)), ('by reshaping', (13, 15))]","[('objection detection', (4, 6)), ('Focus Loss', (7, 9)), ('class imbalance', (11, 13)), ('standard cross entropy loss', (16, 20))]",[],[],[],[]
baselines,Here we utilize a data augment sample method named Data - anchor - sampling .,"[('utilize', (2, 3)), ('named', (8, 9))]","[('data augment sample method', (4, 8)), ('Data - anchor - sampling', (9, 14))]",[],[],[],[]
experiments,2 ) generate smaller face samples through larger ones to increase the diversity of face samples of smaller scales .,"[('generate', (2, 3)), ('through', (6, 7)), ('to increase', (9, 11)), ('of', (16, 17))]","[('smaller face samples', (3, 6)), ('larger ones', (7, 9)), ('diversity of face samples', (12, 16)), ('smaller scales', (17, 19))]",[],[],[],[]
hyperparameters,"As for the parameter initialization , our PyramidBox use the pre-trained parameters from VGG16 .","[('use', (8, 9)), ('from', (12, 13))]","[('parameter initialization', (3, 5)), ('our PyramidBox', (6, 8)), ('pre-trained parameters', (10, 12)), ('VGG16', (13, 14))]",[],[],[],[]
hyperparameters,"The parameters of conv fc 67 and conv fc 7 are initialized by sub - sampling parameters from fc 6 and fc 7 of VGG16 and the other additional layers are randomly initialized with "" xavier "" in .","[('of', (2, 3)), ('initialized by', (11, 13)), ('from', (17, 18)), ('of', (23, 24)), ('randomly initialized with', (31, 34))]","[('parameters', (1, 2)), ('conv fc 67 and conv fc 7', (3, 10)), ('sub - sampling parameters', (13, 17)), ('fc 6 and fc 7', (18, 23)), ('VGG16', (24, 25)), ('other additional layers', (27, 30)), ('"" xavier ""', (34, 37))]",[],[],[],[]
hyperparameters,"We use a learning rate of 10 ? 3 for 80 k iterations , and 10 ? 4 for the next 20 k iterations , and 10 ? 5 for the last 20 k iterations on the WIDER FACE training set with batch size 16 .","[('use', (1, 2)), ('of', (5, 6)), ('for', (9, 10)), ('on', (35, 36)), ('with', (41, 42))]","[('learning rate', (3, 5)), ('10 ? 3', (6, 9)), ('80 k iterations', (10, 13)), ('next 20 k iterations', (20, 24)), ('last 20 k iterations', (31, 35)), ('WIDER FACE training set', (37, 41)), ('batch size 16', (42, 45))]",[],[],[],[]
hyperparameters,We also use a momentum of 0.9 and a weight decay of 0.0005 .,"[('use', (2, 3)), ('of', (5, 6))]","[('momentum', (4, 5)), ('0.9', (6, 7)), ('weight decay', (9, 11)), ('0.0005', (12, 13))]",[],[],[],[]
baselines,Our Pyramid,[],[],[],[],[],[]
results,"The results listed in prove that LFPN started from a middle layer , using conv fc7 in our Pyramid Box , is more powerful , which implies that features with large gap in scale may not help each other .","[('prove', (4, 5)), ('started from', (7, 9)), ('using', (13, 14)), ('in', (16, 17)), ('is', (21, 22)), ('implies', (26, 27))]","[('LFPN', (6, 7)), ('middle layer', (10, 12)), ('conv fc7', (14, 16)), ('our Pyramid Box', (17, 20)), ('more powerful', (22, 24))]",[],[],[],[]
results,The comparison between the first and forth column of indicates that LFPN increases the m AP by 1.9 % on hard subset .,"[('indicates', (9, 10)), ('increases', (12, 13)), ('by', (16, 17)), ('on', (19, 20))]","[('LFPN', (11, 12)), ('m AP', (14, 16)), ('1.9 %', (17, 19)), ('hard subset', (20, 22))]",[],[],[],[]
results,We employ Data - anchor - sampling based on LFPN network and the result shows that our data - anchor - sampling effectively improves the performance .,"[('employ', (1, 2)), ('based on', (7, 9)), ('shows', (14, 15))]","[('Data - anchor - sampling', (2, 7)), ('LFPN network', (9, 11)), ('our data - anchor - sampling', (16, 22)), ('effectively improves', (22, 24)), ('performance', (25, 26))]",[],[],[],[]
results,"The mAP is increased by 0.4 % , 0.4 % and 0.6 % on easy , medium and hard subset , respectively .","[('increased by', (3, 5)), ('on', (13, 14))]","[('mAP', (1, 2)), ('0.4 % , 0.4 % and 0.6 %', (5, 13)), ('easy , medium and hard subset', (14, 20))]",[],[],[],[]
results,"By comparing the first and last column in , one can see that PyamidAnchor effectively improves the performance , i.e. , 0.7 % , 0.6 % and 0.9 % on easy , medium and hard , respectively .","[('comparing', (1, 2)), ('see that', (11, 13)), ('effectively', (14, 15)), ('i.e.', (19, 20)), ('on', (29, 30))]","[('PyamidAnchor', (13, 14)), ('performance', (17, 18)), ('easy , medium and hard', (30, 35))]",[],[],[],[]
results,Wider and deeper context prediction module is better .,"[('is', (6, 7))]","[('Wider and deeper context prediction module', (0, 6)), ('better', (7, 8))]",[],[],[],[]
results,shows that the performance of CPM is better than both DSSD module and SSH context module .,"[('shows', (0, 1)), ('of', (4, 5)), ('better than', (7, 9))]","[('performance', (3, 4)), ('CPM', (5, 6)), ('both', (9, 10)), ('DSSD module and SSH context module', (10, 16))]",[],[],[],[]
results,"Notice that the combination of SSH and DSSD gains very little compared to SSH alone , which indicates that large receptive field is more important to predict the accurate location and classification .","[('Notice', (0, 1)), ('gains', (8, 9)), ('compared to', (11, 13))]","[('combination of', (3, 5)), ('SSH and DSSD', (5, 8)), ('very little', (9, 11)), ('SSH alone', (13, 15))]",[],[],[],[]
results,"In addition , by comparing the last two column of , one can find that the method of Max - in - out improves the mAP on WIDER FACE validation set about + 0.2 % ( Easy ) , + 0.3 % ( Medium ) and + 0.1 % ( Hard ) , respectively .","[('find that', (13, 15)), ('improves', (23, 24)), ('on', (26, 27)), ('about', (31, 32))]","[('method of Max - in - out', (16, 23)), ('mAP', (25, 26)), ('WIDER FACE validation set', (27, 31)), ('+ 0.2 % ( Easy )', (32, 38)), ('+ 0.3 % ( Medium )', (39, 45)), ('+ 0.1 % ( Hard )', (46, 52))]",[],[],[],[]
results,"To conclude this section , we summarize our results in , from which one can see that m AP increase 2.1 % , 2.3 % and 4.7 % on easy , medium and hard subset , respectively .","[('can', (14, 15)), ('see', (15, 16)), ('increase', (19, 20)), ('on', (28, 29))]","[('m AP', (17, 19)), ('2.1 % , 2.3 % and 4.7 %', (20, 28)), ('easy , medium and hard subset', (29, 35))]",[],[],[],[]
results,"Our PyramidBox outperforms others across all three subsets , i.e. 0.961 ( easy ) , 0.950 ( medium ) , 0.889 ( hard ) for validation set , and 0.956 ( easy ) , 0.946 ( medium ) , 0.887 ( hard ) for testing set .","[('i.e.', (9, 10))]","[('Our PyramidBox', (0, 2)), ('outperforms', (2, 3)), ('others', (3, 4)), ('validation', (25, 26))]",[],[],[],[]
research-problem,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection,[],[],[],[],[],[]
research-problem,"Detection and analysis on human subjects using facial feature based biometrics for access control , surveillance systems and other security applications have gained popularity over the past few years .",[],"[('Detection and analysis on human subjects using facial feature based biometrics', (0, 11))]",[],[],[],[]
model,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .",[],"[('Contextual Multi - Scale Region - based CNN ( CMS - RCNN )', (9, 22)), ('face detection in', (27, 30))]",[],[],[],[]
model,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .","[('allows', (7, 8)), ('to simultaneously look at', (10, 14)), ('to explicitly look', (20, 23)), ('as', (26, 27))]","[('network', (9, 10)), ('multi-scale features', (14, 16)), ('outside facial regions', (23, 26)), ('potential body regions', (28, 31))]",[],[],[],[]
model,"Therefore , it is able to robustly deal with the challenges in the problem of unconstrained face detection .","[('able to', (4, 6)), ('in', (11, 12))]","[('robustly deal', (6, 8)), ('challenges', (10, 11)), ('problem of unconstrained face detection', (13, 18))]",[],[],[],[]
model,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,"[('introduces', (5, 6)), ('to generate', (18, 20)), ('to do inference', (39, 42)), ('on', (42, 43)), ('of', (46, 47))]","[('CMS - RCNN method', (1, 5)), ('Multi - Scale Region Proposal Network ( MS - RPN )', (7, 18)), ('set', (21, 22)), ('region candidates', (23, 25)), ('region candidates', (44, 46)), ('facial regions', (47, 49))]",[],[],[],[]
model,"Inside the network , skip pooling is used to extract information at multiple scales and levels of abstraction .","[('Inside', (0, 1)), ('used to', (7, 9)), ('at', (11, 12)), ('of', (16, 17))]","[('network', (2, 3)), ('skip pooling', (4, 6)), ('extract', (9, 10)), ('information', (10, 11)), ('multiple scales and levels', (12, 16)), ('abstraction', (17, 18))]",[],[],[],[]
model,"Unlike all the previous approaches that select a feature extractor beforehand and incorporate a linear classifier with the depth descriptor beside RGB channels , our method solves the problem under a deep learning framework where the global and the local context features , i.e. multi scaling , are synchronized to Faster Region - based Convolutional Neural Networks in order to robustly achieve semantic detection .","[('incorporate', (12, 13)), ('with', (16, 17)), ('beside', (20, 21)), ('solves', (26, 27)), ('under', (29, 30)), ('where', (34, 35)), ('synchronized to', (48, 50)), ('to robustly achieve', (59, 62))]","[('depth descriptor', (18, 20)), ('RGB channels', (21, 23)), ('our', (24, 25)), ('problem', (28, 29)), ('deep learning framework', (31, 34)), ('global and the local context features', (36, 42)), ('Faster Region - based Convolutional Neural Networks', (50, 57)), ('semantic detection', (62, 64))]",[],[],[],[]
hyperparameters,Our CMS - RCNN is implemented in the Caffe deep learning framework .,"[('implemented in', (5, 7))]","[('CMS - RCNN', (1, 4)), ('Caffe deep learning framework', (8, 12))]",[],[],[],[]
hyperparameters,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .","[('of', (4, 5)), ('have', (7, 8)), ('as', (11, 12)), ('during', (20, 21)), ('initialized from', (25, 27))]","[('first 5 sets', (1, 4)), ('convolution layers', (5, 7)), ('same architecture', (9, 11)), ('deep VGG - 16 model', (13, 18)), ('training', (21, 22)), ('parameters', (23, 24)), ('pre-trained VGG - 16', (28, 32))]",[],[],[],[]
hyperparameters,"Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .","[('set', (2, 3)), ('of', (6, 7)), ('to be', (19, 21))]","[('initial scale', (4, 6)), (""' conv3 ' , ' conv4 '"", (7, 14)), ('66.84 , 94.52 , and 94.52', (21, 27))]",[],[],[],[]
hyperparameters,"Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .","[('pooled from', (3, 5)), ('initialized with', (18, 20)), ('to be', (21, 23)), ('for', (31, 32))]","[('features', (2, 3)), (""' conv3 ' , ' conv4 '"", (5, 12)), ('scale', (20, 21)), ('57.75 , 81.67 , and 81.67', (23, 29))]",[],[],[],[]
experiments,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .","[('share', (9, 10)), ('for', (13, 14)), ('so', (17, 18)), ('can be done', (20, 23)), ('resulting in', (25, 27))]","[('MS - RPN and the CMS - CNN', (1, 9)), ('same parameters', (11, 13)), ('all convolution layers', (14, 17)), ('computation', (19, 20)), ('once', (23, 24)), ('higher efficiency', (27, 29))]",[],[],[],[]
hyperparameters,"Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .","[('in order', (2, 4)), ('to shrink', (4, 6)), ('of', (9, 10)), ('employed', (21, 22))]","[('channel size', (7, 9)), ('concatenated feature map', (11, 14)), ('11 convolution layer', (16, 19))]",[],[],[],[]
hyperparameters,Therefore the channel size of final feature map is at the same size as the original fifth convolution layer in Faster R - CNN .,"[('of', (4, 5)), ('at the same', (9, 12)), ('as', (13, 14)), ('in', (19, 20))]","[('channel size', (2, 4)), ('final feature map', (5, 8)), ('original fifth convolution layer', (15, 19)), ('Faster R - CNN', (20, 24))]",[],[],[],[]
results,"Using this database , our proposed approach robustly outperforms strong baseline methods , including Two - stage CNN , Multi-scale Cascade CNN , Faceness and Aggregate Channel Features ( ACF ) , by a large margin .","[('including', (13, 14)), ('by', (32, 33))]","[('robustly outperforms', (7, 9)), ('strong baseline methods', (9, 12)), ('Two - stage CNN', (14, 18)), ('Multi-scale Cascade CNN', (19, 22)), ('Faceness', (23, 24)), ('Aggregate Channel Features ( ACF )', (25, 31)), ('large margin', (34, 36))]",[],[],[],[]
results,Experiments on WIDER FACE Dataset,"[('on', (1, 2))]",[],[],[],[],[]
results,Our method outperforms those strong baselines by a large margin .,"[('by', (6, 7))]","[('Our method', (0, 2)), ('outperforms', (2, 3)), ('strong baselines', (4, 6)), ('large margin', (8, 10))]",[],[],[],[]
results,"It achieves the best average precision in all level faces , i.e. AP = 0.902 ( Easy ) , 0.874 ( Medium ) and 0.643 ( Hard ) , and outperforms the second best baseline by 26.0 % ( Easy ) , 37.4 % ( Medium ) and 60.8 % ( Hard ) .","[('achieves', (1, 2)), ('in', (6, 7)), ('i.e.', (11, 12)), ('by', (35, 36))]","[('best average precision', (3, 6)), ('all level faces', (7, 10)), ('AP', (12, 13)), ('0.902', (14, 15)), ('0.874', (19, 20)), ('0.643', (24, 25)), ('outperforms', (30, 31)), ('second best baseline', (32, 35)), ('26.0 % ( Easy )', (36, 41)), ('37.4 % ( Medium )', (42, 47)), ('60.8 % ( Hard )', (48, 53))]",[],[],[],[]
results,"These results suggest that as the difficulty level goes up , CMS - RCNN can detect challenging faces better .","[('suggest', (2, 3)), ('goes', (8, 9)), ('can detect', (14, 16))]","[('difficulty level', (6, 8)), ('up', (9, 10)), ('CMS - RCNN', (11, 14)), ('challenging faces', (16, 18)), ('better', (18, 19))]",[],[],[],[]
results,With Context v.s. Without Context,[],[],[],[],[],[]
results,"Additionally , the context model produces a longer PR curve , which means that contextual reasoning can help finding more faces .","[('produces', (5, 6))]","[('context model', (3, 5)), ('longer PR curve', (7, 10))]",[],[],[],[]
results,Experiments on FDDB Face Database,"[('on', (1, 2))]",[],[],[],[],[]
results,Our method achieves the best recall rate on this database .,"[('achieves', (2, 3))]","[('Our method', (0, 2)), ('best recall rate', (4, 7))]",[],[],[],[]
results,The proposed CMS - RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods ( as shown ) .,"[('achieves', (15, 16)), ('comparing against', (21, 23))]","[('proposed', (1, 2)), ('CMS - RCNN approach', (2, 6)), ('outperforms', (6, 7)), ('most of the published face detection methods', (7, 14)), ('very high recall rate', (17, 21)), ('all other methods', (23, 26))]",[],[],[],[]
baselines,"This paper has presented our proposed CMS - RCNN approach to robustly detect human facial regions from images collected under various challenging conditions , e.g. highly occlusions , low resolutions , facial expressions , illumination variations , etc .","[('from', (16, 17)), ('collected under', (18, 20))]","[('robustly detect', (11, 13)), ('human facial regions', (13, 16)), ('images', (17, 18))]",[],[],[],[]
results,The experimental results show that our proposed approach outperforms strong baselines on the WIDER FACE and consistently achieves very competitive results against state - of - the - art methods on the FDDB .,"[('show', (3, 4)), ('on', (11, 12)), ('consistently achieves', (16, 18)), ('against', (21, 22)), ('on', (30, 31))]","[('our proposed approach', (5, 8)), ('outperforms', (8, 9)), ('strong baselines', (9, 11)), ('WIDER FACE', (13, 15)), ('very competitive results', (18, 21)), ('state - of - the - art methods', (22, 30)), ('FDDB', (32, 33))]",[],[],[],[]
research-problem,Face Detection Using Improved Faster RCNN,[],"[('Face Detection', (0, 2))]",[],[],[],[]
research-problem,"Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy set , medium set , hard set ) .","[('achieves', (2, 3)), ('over', (13, 14))]","[('Our method', (0, 2)), ('two 1th places', (3, 6)), ('one 2nd place', (7, 10)), ('WIDER FACE validation dataset', (14, 18))]",[],[],[],[]
research-problem,"( 3 ) Our framework achieves two 1st places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy , medium , hard ) , one illustrative example of our results in the crowd case can be found in .","[('achieves', (5, 6)), ('in', (13, 14)), ('over', (16, 17))]","[('Our framework', (3, 5)), ('two 1st places', (6, 9)), ('one 2nd place', (10, 13)), ('three tasks', (14, 16)), ('WIDER FACE validation dataset ( easy , medium , hard )', (17, 28))]",[],[],[],[]
research-problem,"Face detection is one of the most fundamental and challenging problems in computer vision , and has been extensively studied for decades .",[],"[('Face detection', (0, 2))]",[],[],[],[]
research-problem,Dense - Box employs a fully deep convolutional neural network to directly predict face confidence and corresponding bounding box .,"[('employs', (3, 4)), ('to directly predict', (10, 13))]","[('Dense - Box', (0, 3)), ('fully deep convolutional neural network', (5, 10)), ('face confidence', (13, 15)), ('corresponding bounding box', (16, 19))]",[],[],[],[]
research-problem,"UnitBox introduces a novel intersection - over - union ( IoU ) loss to predict bounding box , which regresses the four bounds of a predicted box as a whole unit .","[('introduces', (1, 2)), ('to predict', (13, 15)), ('regresses', (19, 20)), ('of', (23, 24))]","[('UnitBox', (0, 1)), ('novel intersection - over - union ( IoU ) loss', (3, 13)), ('bounding box', (15, 17)), ('four bounds', (21, 23)), ('predicted', (25, 26))]",[],[],[],[]
research-problem,S 3 FD presents a single shot scale - invariant face detector which achieves good result on WIDER FACE datasets .,"[('presents', (3, 4)), ('achieves', (13, 14)), ('on', (16, 17))]","[('S 3 FD', (0, 3)), ('single shot scale - invariant face detector', (5, 12)), ('good result', (14, 16)), ('WIDER FACE datasets', (17, 20))]",[],[],[],[]
experimental-setup,Single NVIDIA Tesla K80 is used for training and testing .,"[('used for', (5, 7))]","[('Single NVIDIA Tesla K80', (0, 4)), ('training and testing', (7, 10))]",[],[],[],[]
experimental-setup,Mini batch size is set to 1 considering memory consumption .,"[('set to', (4, 6)), ('considering', (7, 8))]","[('Mini batch size', (0, 3)), ('1', (6, 7)), ('memory consumption', (8, 10))]",[],[],[],[]
baselines,"A deformable layer is used to output a "" thin "" feature map with exploiting image context .","[('used to output', (4, 7)), ('with exploiting', (13, 15))]","[('deformable layer', (1, 3)), ('"" thin "" feature map', (8, 13)), ('image context', (15, 17))]",[],[],[],[]
experimental-setup,"Aspect ratios ( 1 , 1.5 , 2 ) and scales ( 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 ) are carefully designed to capture better locations of faces in the RPN stage , and the number of filters for the RPN layer is set as 512 .","[('carefully designed to capture', (31, 35)), ('of', (37, 38)), ('in', (39, 40)), ('for', (49, 50)), ('set as', (54, 56))]","[('Aspect ratios ( 1 , 1.5 , 2 )', (0, 9)), ('scales', (10, 11)), ('better locations', (35, 37)), ('faces', (38, 39)), ('RPN stage', (41, 43)), ('number of filters', (46, 49)), ('RPN layer', (51, 53)), ('512', (56, 57))]",[],[],[],[]
experimental-setup,"By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .","[('of', (7, 8)), ('assigned as', (15, 17))]","[('batch size', (5, 7)), ('RPN and R - CNN', (8, 13)), ('256 and 128', (17, 20))]",[],[],[],[]
experimental-setup,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .","[('set to', (5, 7)), ('decrease to', (12, 14)), ('after', (17, 18))]","[('initial learning rate', (1, 4)), ('1e - 3', (7, 10)), ('1e - 4', (14, 17)), ('20w iterations', (18, 20))]",[],[],[],[]
experimental-setup,Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,"[('set to', (6, 8))]","[('Weight decay', (0, 2)), ('momentum', (4, 5)), ('1e - 4 and 0.9', (8, 13))]",[],[],[],[]
experimental-setup,"In testing stage , multi-scale testing strategy is adapted to be robust to different scale faces .","[('In', (0, 1)), ('adapted to be', (8, 11))]","[('testing stage', (1, 3)), ('multi-scale testing strategy', (4, 7)), ('robust', (11, 12)), ('different scale faces', (13, 16))]",[],[],[],[]
results,"We also find top - ranked 6000 proposals are directly selected without NMS during testing can boost 0.1 % , 0.3 % and 0.6 % on easy set , medium set and hard set respectively .","[('find', (2, 3)), ('directly selected without', (9, 12)), ('during', (13, 14)), ('can', (15, 16)), ('on', (25, 26))]","[('top - ranked 6000 proposals', (3, 8)), ('NMS', (12, 13)), ('testing', (14, 15)), ('boost', (16, 17)), ('0.1 % , 0.3 % and 0.6 %', (17, 25)), ('easy set , medium set and hard set', (26, 34))]",[],[],[],[]
results,"Compared with the recently published top approaches , FDNet1.0 wins two 1st places ( easy set = 95.9 % , medium set = 94.5 % ) and one 2nd place ( hard set = 87.9 % ) on the validation set , as illustrated in .","[('Compared with', (0, 2)), ('wins', (9, 10)), ('on', (37, 38))]","[('FDNet1.0', (8, 9)), ('two 1st places', (10, 13)), ('easy set', (14, 16)), ('one 2nd place', (27, 30)), ('validation set', (39, 41))]",[],[],[],[]
research-problem,Selective Refinement Network for High Performance Face Detection,[],[],[],[],[],[]
research-problem,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .",[],"[('High performance face detection', (0, 4))]",[],[],[],[]
research-problem,"Face detection is a long - standing problem in computer vision with extensive applications including face alignment , face analysis , face recognition , etc .",[],"[('Face detection', (0, 2))]",[],[],[],[]
research-problem,To further improve the performance of face detection has become a challenging issue .,"[('of', (5, 6))]","[('performance', (4, 5)), ('face detection', (6, 8))]",[],[],[],[]
approach,R - CNN - like detectors ) address the class imbalance by a two - stage cascade and sampling heuristics .,"[('address', (7, 8)), ('by', (11, 12)), ('sampling', (18, 19))]","[('R - CNN - like detectors', (0, 6)), ('class imbalance', (9, 11)), ('two - stage cascade', (13, 17))]",[],[],[],[]
approach,"As for single - shot detectors , RetinaNet proposes the focal loss to focus training on a sparse set of hard examples and down - weight the loss assigned to well - classified examples .","[('proposes', (8, 9)), ('to', (12, 13)), ('on', (15, 16)), ('of', (19, 20)), ('assigned to', (28, 30))]","[('single - shot detectors', (2, 6)), ('RetinaNet', (7, 8)), ('focal loss', (10, 12)), ('focus', (13, 14)), ('sparse set', (17, 19)), ('hard examples', (20, 22)), ('down - weight the loss', (23, 28)), ('well - classified examples', (30, 34))]",[],[],[],[]
approach,"As shown in ( d ) , as the IoU threshold increases , the AP drops dramatically , indicating that the accuracy of the bounding box location needs to be improved .","[('as', (7, 8))]","[('IoU threshold', (9, 11)), ('increases', (11, 12)), ('AP', (14, 15)), ('drops', (15, 16)), ('dramatically', (16, 17))]",[],[],[],[]
approach,Cascade R - CNN addresses this issue by cascading R - CNN with different IoU thresholds .,"[('by', (7, 8)), ('with', (12, 13))]","[('Cascade', (0, 1)), ('cascading', (8, 9)), ('R - CNN', (9, 12)), ('different IoU thresholds', (13, 16))]",[],[],[],[]
approach,RefineDet ) applies two - step regression to single - shot detector .,"[('applies', (2, 3)), ('to', (7, 8))]","[('RefineDet', (0, 1)), ('two - step regression', (3, 7)), ('single - shot detector', (8, 12))]",[],[],[],[]
approach,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .","[('investigate', (5, 6)), ('of', (8, 9)), ('on', (15, 16)), ('propose', (22, 23)), ('named', (29, 30)), ('selectively applies', (38, 40)), ('to', (46, 47))]","[('two - step classification and regression', (9, 15)), ('different levels', (16, 18)), ('novel', (24, 25)), ('Selective Refinement Network ( SRN )', (30, 36)), ('two - step classification and regression', (40, 46)), ('specific levels of detection layers', (47, 52))]",[],[],[],[]
experiments,"As shown in , RetinaNet with STC improves the recall efficiency to a certain extent .","[('improves', (7, 8)), ('to', (11, 12))]","[('RetinaNet with STC', (4, 7)), ('recall efficiency', (9, 11)), ('certain extent', (13, 15))]",[],[],[],[]
approach,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .","[('design', (4, 5)), ('to provide', (12, 14)), ('to better capture', (18, 21))]","[('Receptive Field Enhancement ( RFE )', (6, 12)), ('more diverse receptive fields', (14, 18)), ('extreme - pose faces', (22, 26))]",[],[],[],[]
approach,We present a STC module to filter out most simple negative samples from low level layers to reduce the classification search space .,"[('present', (1, 2)), ('to filter out', (5, 8)), ('from', (12, 13)), ('to reduce', (16, 18))]","[('STC module', (3, 5)), ('most simple negative samples', (8, 12)), ('low level layers', (13, 16)), ('classification search space', (19, 22))]",[],[],[],[]
experimental-setup,"It consists of 393 , 703 annotated face bounding boxes in 32 , 203 images with variations in pose , scale , facial expression , occlusion , and lighting condition .","[('consists of', (1, 3)), ('in', (10, 11)), ('with', (15, 16)), ('in', (17, 18))]","[('393 , 703 annotated face bounding boxes', (3, 10)), ('32', (11, 12)), ('203 images', (13, 15)), ('variations', (16, 17)), ('pose', (18, 19)), ('scale', (20, 21)), ('facial expression', (22, 24)), ('occlusion', (25, 26)), ('lighting condition', (28, 30))]",[],[],[],[]
experimental-setup,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .","[('initialized by', (4, 6)), ('in', (16, 17)), ('initialized by', (23, 25))]","[('backbone network', (1, 3)), ('pretrained ResNet - 50 model', (7, 12)), ('all the parameters', (13, 16)), ('newly added convolution layers', (18, 22)), ('"" xavier "" method', (26, 30))]",[],[],[],[]
experimental-setup,"We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .","[('fine - tune', (1, 4)), ('using', (7, 8)), ('with', (9, 10))]","[('SRN model', (5, 7)), ('SGD', (8, 9)), ('0.9 momentum', (10, 12)), ('0.0001 weight decay', (13, 16)), ('batch size', (18, 20)), ('32', (20, 21))]",[],[],[],[]
experimental-setup,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .","[('set', (1, 2)), ('to', (5, 6)), ('for', (8, 9)), ('decay it to', (15, 18)), ('for', (25, 26))]","[('learning rate', (3, 5)), ('10 ?2', (6, 8)), ('first 100 epochs', (10, 13)), ('10 ? 3 and 10 ? 4', (18, 25)), ('20 and 10 epochs', (27, 31))]",[],[],[],[]
experimental-setup,We implement SRN using the Py - Torch library .,"[('implement', (1, 2)), ('using', (3, 4))]","[('SRN', (2, 3)), ('Py - Torch library', (5, 9))]",[],[],[],[]
experimental-setup,"In the inference phase , the STC first filters the regularly tiled anchors on the selected pyramid levels with the negative confidence scores larger than the threshold ? = 0.99 , and then STR adjusts the locations and sizes of selected anchors .","[('In', (0, 1)), ('first filters', (7, 9)), ('on', (13, 14)), ('with', (18, 19)), ('larger than', (23, 25)), ('adjusts', (34, 35)), ('of', (39, 40))]","[('inference phase', (2, 4)), ('STC', (6, 7)), ('regularly tiled anchors', (10, 13)), ('selected pyramid levels', (15, 18)), ('negative confidence scores', (20, 23)), ('threshold ? = 0.99', (26, 30)), ('STR', (33, 34)), ('locations and sizes', (36, 39)), ('selected anchors', (40, 42))]",[],[],[],[]
experiments,"Finally , we apply the non-maximum suppression ( NMS ) with jaccard overlap of 0.5 to generate the top 750 high confident detections per image as the final results .","[('apply', (3, 4)), ('with', (10, 11)), ('of', (13, 14)), ('to generate', (15, 17))]","[('non-maximum suppression ( NMS )', (5, 10)), ('jaccard overlap', (11, 13)), ('0.5', (14, 15)), ('top 750 high confident detections per image', (18, 25))]",[],[],[],[]
ablation-analysis,"Firstly , we use the ordinary prediction head in ) instead of the proposed RFE .","[('use', (3, 4)), ('instead of', (10, 12))]","[('ordinary prediction head', (5, 8)), ('proposed RFE', (13, 15))]",[],[],[],[]
ablation-analysis,"Experimental results of applying two - step classification to each pyramid level are shown in , indicating that applying two - step classification to the low pyramid levels improves the performance , especially on tiny faces .","[('applying', (3, 4)), ('to', (8, 9)), ('applying', (18, 19)), ('to', (23, 24)), ('improves', (28, 29))]","[('two - step classification', (4, 8)), ('each pyramid level', (9, 12)), ('two - step classification', (19, 23)), ('low pyramid levels', (25, 28)), ('performance', (30, 31))]",[],[],[],[]
ablation-analysis,"Therefore , the STC module selectively applies the two - step classification on the low pyramid levels ( i.e. , P2 , P3 , and P4 ) , since these levels are associated with lots of small anchors , which are the main source of false positives .","[('selectively applies', (5, 7)), ('on', (12, 13))]","[('STC module', (3, 5)), ('two - step classification', (8, 12)), ('low pyramid levels ( i.e. , P2 , P3 , and P4 )', (14, 27))]",[],[],[],[]
ablation-analysis,"As listed in , our STC effectively reduces the false positives across different recall rates , demonstrating the effectiveness of the STC module . Selective Two - step Regression .","[('effectively reduces', (6, 8)), ('across', (11, 12))]","[('our STC', (4, 6)), ('false positives', (9, 11)), ('different recall rates', (12, 15)), ('Selective Two - step Regression', (24, 29))]",[],[],[],[]
results,"As shown in , it produces much better results than the baseline , with 0.8 % , 0.9 % and 0.8 % AP improvements on the Easy , Medium , and Hard subsets .","[('produces', (5, 6)), ('than', (9, 10)), ('with', (13, 14)), ('on', (24, 25))]","[('much better results', (6, 9)), ('baseline', (11, 12)), ('0.8 % , 0.9 % and 0.8 % AP improvements', (14, 24)), ('Easy , Medium , and Hard subsets', (26, 33))]",[],[],[],[]
results,Experimental results of applying two - step regression to each pyramid level ( see ) confirm our previous analysis .,"[('to', (8, 9))]","[('two - step regression', (4, 8)), ('each pyramid level', (9, 12))]",[],[],[],[]
ablation-analysis,"As shown in , the STR module produces consistently accurate detection results than the baseline method .","[('produces', (7, 8)), ('than', (12, 13))]","[('STR module', (5, 7)), ('consistently accurate detection results', (8, 12)), ('baseline method', (14, 16))]",[],[],[],[]
ablation-analysis,"The gap between the AP across all three subsets increases as the IoU threshold increases , which indicate that the STR module is important to produce more accurate detections .","[('between', (2, 3)), ('across', (5, 6)), ('as', (10, 11))]","[('gap', (1, 2)), ('AP', (4, 5)), ('all three subsets', (6, 9)), ('increases', (9, 10)), ('IoU threshold', (12, 14)), ('increases', (14, 15))]",[],[],[],[]
results,"In addition , coupled with the STC module , the performance is further improved to 96.1 % , 95.0 % and 90.1 % on the Easy , Medium and Hard subsets , respectively .","[('coupled with', (3, 5)), ('to', (14, 15)), ('on', (23, 24))]","[('STC module', (6, 8)), ('performance', (10, 11)), ('further improved', (12, 14)), ('96.1 % , 95.0 % and 90.1 %', (15, 23)), ('Easy , Medium and Hard subsets', (25, 31))]",[],[],[],[]
ablation-analysis,The RFE is used to diversify the receptive fields of detection layers in order to capture faces with extreme poses .,"[('of', (9, 10)), ('with', (17, 18))]","[('RFE', (1, 2)), ('diversify', (5, 6)), ('receptive fields', (7, 9)), ('detection layers', (10, 12)), ('faces', (16, 17)), ('extreme poses', (18, 20))]",[],[],[],[]
ablation-analysis,"Comparing the detection results between fourth and fifth columns in , we notice that RFE consistently improves the AP scores in different subsets , i.e. , 0.3 % , 0.3 % , and 0.1 % APs on the Easy , Medium , and Hard categories .","[('Comparing', (0, 1)), ('notice', (12, 13)), ('consistently improves', (15, 17)), ('in', (20, 21)), ('i.e.', (24, 25)), ('on', (36, 37))]","[('detection results', (2, 4)), ('fourth', (5, 6)), ('RFE', (14, 15)), ('AP scores', (18, 20)), ('different subsets', (21, 23)), ('0.3 % , 0.3 % , and 0.1 % APs', (26, 36)), ('Easy , Medium , and Hard categories', (38, 45))]",[],[],[],[]
results,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .","[('with', (15, 16))]","[('SRN', (4, 5)), ('outperforms', (5, 6)), ('top AP score ( 99.87 % )', (17, 24))]",[],[],[],[]
results,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,"[('achieves', (1, 2)), ('by improving', (11, 13)), ('compared to', (17, 19))]","[('SRN', (0, 1)), ('state - of - the - art results', (3, 11)), ('4.99 % AP score', (13, 17)), ('second best method STN', (20, 24))]",[],[],[],[]
results,"As shown in ( c ) , our SRN sets a new state - of - the - art performance , i.e. , 98.8 % true positive rate when the number of false positives is equal to 1000 .","[('sets', (9, 10)), ('i.e.', (21, 22)), ('when', (28, 29)), ('equal to', (35, 37))]","[('our SRN', (7, 9)), ('new state - of - the - art performance', (11, 20)), ('98.8 % true positive rate', (23, 28)), ('number of false positives', (30, 34)), ('1000', (37, 38))]",[],[],[],[]
results,"As shown in , we find that SRN performs favourably against the state - of - the - art based on the average precision ( AP ) across the three subsets , especially on the Hard subset which contains a large amount of small faces .","[('find', (5, 6)), ('performs', (8, 9)), ('against', (10, 11)), ('based on', (19, 21)), ('across', (27, 28)), ('especially on', (32, 34)), ('contains', (38, 39))]","[('SRN', (7, 8)), ('favourably', (9, 10)), ('state - of - the - art', (12, 19)), ('average precision ( AP )', (22, 27)), ('three subsets', (29, 31)), ('Hard subset', (35, 37)), ('large amount', (40, 42))]",[],[],[],[]
results,"Specifically , it produces the best AP scores in all subsets of both validation and testing sets , i.e. , 96.4 % ( Easy ) , 95.3 % ( Medium ) and 90.2 % ( Hard ) for validation set , and 95.9 % ( Easy ) , 94.9 % ( Medium ) and 89.7 % ( Hard ) for testing set , surpassing all approaches , which demonstrates the superiority of the proposed detector .","[('produces', (3, 4)), ('in', (8, 9)), ('of', (11, 12)), ('i.e.', (18, 19)), ('for', (37, 38)), ('for', (59, 60))]","[('best AP scores', (5, 8)), ('all subsets', (9, 11)), ('validation and testing sets', (13, 17)), ('96.4 % ( Easy )', (20, 25)), ('95.3 % ( Medium )', (26, 31)), ('90.2 % ( Hard )', (32, 37)), ('validation set', (38, 40)), ('95.9 % ( Easy )', (42, 47)), ('94.9 % ( Medium )', (48, 53)), ('89.7 % ( Hard )', (54, 59)), ('testing set', (60, 62))]",[],[],[],[]
research-problem,Aggregate Channel Features for Multi-view Face Detection,[],[],[],[],[],[]
model,The classifier learning process follows the VJ framework pipeline .,"[('follows', (4, 5))]","[('classifier learning process', (1, 4)), ('VJ framework pipeline', (6, 9))]",[],[],[],[]
model,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .","[('adopt', (5, 6)), ('of', (8, 9)), ('called', (11, 12)), ('extracted directly as', (18, 21)), ('on', (23, 24))]","[('variant', (7, 8)), ('channel features', (9, 11)), ('aggregate channel features', (12, 15)), ('pixel values', (21, 23)), ('subsampled channels', (24, 26))]",[],[],[],[]
model,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .","[('proven to', (73, 75))]","[('multi-scaling', (11, 12)), ('feature representation', (13, 15)), ('representation capacity', (18, 20)), ('performance', (39, 40))]",[],[],[],[]
hyperparameters,"Summary : Based on observations above , we choose 2048 as the number of weak classifiers contained in the soft cascade .","[('choose', (8, 9)), ('as', (10, 11)), ('contained in', (16, 18))]","[('2048', (9, 10)), ('number of weak classifiers', (12, 16)), ('soft cascade', (19, 21))]",[],[],[],[]
results,Evaluation on benchmark face database,[],[],[],[],[],[]
results,"As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .","[('in', (4, 5)), ('achieves', (10, 11)), ('of', (14, 15)), ('by', (22, 23))]","[('AFW', (5, 6)), ('our multi-scale detector', (7, 10)), ('ap value', (12, 14)), ('96.8 %', (15, 17)), ('outperforming', (18, 19)), ('other academic methods', (19, 22)), ('large margin', (24, 26))]",[],[],[],[]
results,"In discrete score where evaluation metric is the same as in AFW , our detector achieves 83.7 % , which is a little better than Yan et al ..","[('In', (0, 1)), ('where', (3, 4)), ('achieves', (15, 16))]","[('discrete score', (1, 3)), ('our detector', (13, 15)), ('83.7 %', (16, 18))]",[],[],[],[]
results,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .","[('using', (1, 2)), ('which takes', (4, 6)), ('as', (9, 10)), ('gets', (15, 16)), ('at', (21, 22)), ('for', (24, 25)), ('surpassing', (28, 29)), ('by', (35, 36))]","[('continuous score', (2, 4)), ('overlap ratio', (7, 9)), ('score', (11, 12)), ('our method', (13, 15)), ('61.9 % true positive rate', (16, 21)), ('1 FPPI', (22, 24)), ('multiscale version', (25, 27)), ('other methods', (29, 31)), ('rectangular detections', (33, 35)), ('notable margin', (37, 39))]",[],[],[],[]
research-problem,Supervised Transformer Network for Efficient Face Detection,[],[],[],[],[],[]
model,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .","[('simultaneously proposes', (14, 16)), ('along with', (19, 21))]","[('multi-task Region Proposal Network ( RPN )', (5, 12)), ('candidate face regions', (16, 19)), ('associated facial landmarks', (21, 24))]",[],[],[],[]
model,It first uses a conventional boosting cascade to obtain a set of face candidate areas .,"[('uses', (2, 3)), ('to obtain', (7, 9))]","[('conventional boosting cascade', (4, 7)), ('set of face candidate areas', (10, 15))]",[],[],[],[]
model,"Instead , we use the ROI masks , so that different samples can share the feature in the overlapping area .","[('use', (3, 4)), ('so', (8, 9)), ('in', (16, 17))]","[('ROI masks', (5, 7)), ('different', (10, 11)), ('feature', (15, 16)), ('overlapping area', (18, 20))]",[],[],[],[]
hyperparameters,We use a Real - Boost algorithm for the cascade classification learning .,"[('use', (1, 2)), ('for', (7, 8))]","[('Real - Boost algorithm', (3, 7)), ('cascade classification learning', (9, 12))]",[],[],[],[]
experiments,We use GoogleNet in both the RPN and RCNN networks .,"[('use', (1, 2)), ('in both', (3, 5))]","[('GoogleNet', (2, 3)), ('RPN and RCNN networks', (6, 10))]",[],[],[],[]
results,"As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .","[('bring about', (14, 16))]","[('multi-task RPN , Supervised Transformer , and feature combination', (4, 13)), ('1 % , 1 % , and 2 % recall improvement', (16, 27))]",[],[],[],[]
hyperparameters,"In the training phase , in order to increase the variation of training samples , we randomly select K positive / negative samples from each image for the RCNN network .","[('In', (0, 1)), ('in', (5, 6)), ('to increase', (7, 9)), ('of', (11, 12)), ('randomly select', (16, 18)), ('from', (23, 24)), ('for', (26, 27))]","[('training phase', (2, 4)), ('variation', (10, 11)), ('training samples', (12, 14)), ('K positive / negative samples', (18, 23)), ('each image', (24, 26)), ('RCNN network', (28, 30))]",[],[],[],[]
experiments,We found that NMS tend to include too much noisy low confidence candidates .,"[('found', (1, 2)), ('tend to include', (4, 7))]","[('NMS', (3, 4)), ('too much noisy low confidence candidates', (7, 13))]",[],[],[],[]
experiments,"We also compare the PR curves of using all candidates , NMS , and non-top K suppression .","[('compare', (2, 3)), ('of using', (6, 8))]","[('PR curves', (4, 6)), ('all candidates , NMS , and non-top K suppression', (8, 17))]",[],[],[],[]
experiments,"Our non - top K suppression is very close to using all candidates , and achieved consistently better results than NMS under the same number of candidates .","[('achieved', (15, 16)), ('than', (19, 20)), ('under', (21, 22))]","[('Our non - top K suppression', (0, 6)), ('using', (10, 11)), ('all candidates', (11, 13)), ('consistently better results', (16, 19)), ('NMS', (20, 21)), ('same number of candidates', (23, 27))]",[],[],[],[]
tasks,We also compare with the standard network without ROI convolution .,"[('compare with', (2, 4))]","[('standard network without', (5, 8)), ('ROI convolution', (8, 10))]",[],[],[],[]
tasks,Non- top K ( K = 3 ) suppression is adopted in all settings to make RCNN network more efficiency .,"[('adopted in', (10, 12)), ('to make', (14, 16))]","[('Non- top K ( K = 3 ) suppression', (0, 9)), ('all settings', (12, 14)), ('RCNN network', (16, 18)), ('more efficiency', (18, 20))]",[],[],[],[]
tasks,The original DNN detector can run at 10 FPS on CPU fora VGA image .,"[('run at', (5, 7)), ('on', (9, 10))]","[('original', (1, 2)), ('DNN detector', (2, 4)), ('10 FPS', (7, 9)), ('CPU fora VGA image', (10, 14))]",[],[],[],[]
tasks,"BC D. Qualitative face detection results on ( a ) FDDB , ( b ) AFW , ( c ) PASCAL faces datasets .","[('on', (6, 7)), ('faces', (21, 22))]","[('Qualitative face detection results', (2, 6)), ('datasets', (22, 23))]",[],[],[],[]
results,"On the FDDB dataset , we compare with all public methods .","[('On', (0, 1))]","[('FDDB dataset', (2, 4))]",[],[],[],[]
baselines,"On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .","[('On', (0, 1)), ('compare with', (9, 11)), ('e.g.', (19, 20))]","[('AFW and PASCAL faces datasets', (2, 7)), ('deformable part based methods', (14, 18)), ('structure model', (20, 22)), ('Tree Parts Model ( TSM )', (23, 29)), ('cascade - based methods', (33, 37))]",[],[],[],[]
baselines,"We learn a global regression from 5 facial points to face rectangles to match the annotation for each dataset , and use toolbox from for the evaluation .","[('learn', (1, 2)), ('from', (5, 6)), ('to', (9, 10)), ('to match', (12, 14))]","[('global regression', (3, 5)), ('5 facial points', (6, 9)), ('face rectangles', (10, 12)), ('annotation', (15, 16))]",[],[],[],[]
research-problem,A Fast and Accurate Unconstrained Face Detector,[],[],[],[],[],[]
research-problem,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .","[('to address', (4, 6)), ('in', (7, 8)), ('such as', (12, 14))]","[('challenges', (6, 7)), ('unconstrained face detection', (8, 11)), ('arbitrary', (14, 15))]",[],[],[],[]
research-problem,The objective of face detection is to find and locate faces in an image .,[],"[('face detection', (3, 5))]",[],[],[],[]
research-problem,It is the first step in automatic face recognition applications .,"[('in', (5, 6))]","[('first step', (3, 5)), ('automatic face recognition applications', (6, 10))]",[],[],[],[]
research-problem,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .","[('refer to', (5, 7)), ('as', (13, 14))]","[('face detection with arbitrary facial variations', (7, 13))]",[],[],[],[]
model,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .","[('propose', (3, 4)), ('called', (11, 12))]","[('Normalized Pixel Difference ( NPD )', (13, 19))]",[],[],[],[]
model,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .","[('such as', (8, 10)), ('to reconstruct', (17, 19))]","[('NPD feature', (1, 3)), ('several desirable properties', (4, 7)), ('scale invariance', (10, 12)), ('boundedness', (13, 14)), ('ability', (16, 17)), ('original image', (20, 22))]",[],[],[],[]
model,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .","[('show', (2, 3)), ('obtained from', (8, 10)), ('easily scaled for', (22, 25))]","[('NPD features', (4, 6)), ('lookup table', (11, 13)), ('resulting face detection template', (16, 20)), ('multiscale face detection', (25, 28))]",[],[],[],[]
model,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .","[('propose', (3, 4)), ('construct', (11, 12)), ('to handle', (19, 21))]","[('deep quadratic tree learning method', (5, 10)), ('single soft - cascade AdaBoost classifier', (13, 19)), ('complex face manifolds', (21, 24)), ('arbitrary pose', (25, 27)), ('occlusion conditions', (28, 30))]",[],[],[],[]
model,"While individual NPD features may have "" weak "" discriminative ability , our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree .","[('may have', (4, 6)), ('indicates', (14, 15)), ('can be', (21, 23)), ('combined to construct', (26, 29)), ('in', (32, 33))]","[('individual NPD', (1, 3)), ('subset of NPD features', (17, 21)), ('optimally learned', (23, 25)), ('more discriminative features', (29, 32)), ('deep quadratic tree', (34, 37))]",[],[],[],[]
model,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .","[('of', (15, 16)), ('in', (25, 26)), ('can', (30, 31)), ('partitioned in', (32, 34))]","[('different types of faces', (4, 8)), ('different leaves', (13, 15)), ('tree classifier', (17, 19)), ('complex face manifold', (22, 25)), ('high dimensional space', (27, 30)), ('learning process', (35, 37))]",[],[],[],[]
model,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .","[('is', (1, 2)), ('to tackle', (9, 11)), ('in', (14, 15)), ('without pre-labeling', (19, 21)), ('in', (23, 24))]","[('"" divide and conquer "" strategy', (3, 9)), ('unconstrained face detection', (11, 14)), ('single classifier', (16, 18)), ('views', (22, 23)), ('training set of face images', (25, 30))]",[],[],[],[]
model,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .","[('robust to', (5, 7)), ('as', (16, 17))]","[('resulting face detector', (1, 4)), ('variations in', (7, 9)), ('pose , occlusion , and illumination', (9, 15)), ('blur and low image resolution', (20, 25))]",[],[],[],[]
model,"A new type of feature , called NPD is proposed , which is efficient to compute and has several desirable properties , including scale invariance , boundedness , and enabling reconstruction of the original image .","[('called', (6, 7)), ('including', (22, 23)), ('enabling', (29, 30)), ('of', (31, 32))]","[('NPD', (7, 8)), ('several desirable properties', (18, 21)), ('scale invariance', (23, 25)), ('boundedness', (26, 27)), ('original image', (33, 35))]",[],[],[],[]
model,A deep quadratic tree learner is proposed to learn and combine an optimal subset of NPD features to boost their discriminability .,"[('proposed to learn and combine', (6, 11)), ('of', (14, 15)), ('to boost', (17, 19))]","[('deep quadratic tree learner', (1, 5)), ('optimal subset', (12, 14)), ('discriminability', (20, 21))]",[],[],[],[]
model,"In this way , only a single soft - cascade AdaBoost classifier is needed to handle unconstrained faces with occlusions and arbitrary viewpoints , without pose labeling or clustering in the training stage .","[('to handle', (14, 16)), ('with', (18, 19)), ('without pose', (24, 26)), ('in', (29, 30))]","[('only a single soft - cascade AdaBoost classifier', (4, 12)), ('unconstrained faces', (16, 18)), ('occlusions and arbitrary viewpoints', (19, 23)), ('labeling', (26, 27)), ('clustering', (28, 29)), ('training stage', (31, 33))]",[],[],[],[]
model,The unconstrained face detector does not depend on pose specific cascade structure design ; pose labeling or clustering in the training stage is also not required .,"[('does not depend on', (4, 8)), ('in', (18, 19))]","[('unconstrained face detector', (1, 4)), ('pose specific cascade structure design', (8, 13)), ('pose labeling', (14, 16)), ('training stage', (20, 22)), ('not required', (24, 26))]",[],[],[],[]
code,The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / .,[],"[('http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface', (10, 14))]",[],[],[],[]
model,"In this paper , we show that the optimal ordinal / contrastive features and their combinations can be learned by integrating the proposed NPD features in a deep quadratic tree .","[('show', (5, 6)), ('by integrating', (19, 21)), ('in', (25, 26))]","[('optimal ordinal / contrastive features', (8, 13)), ('proposed NPD features', (22, 25)), ('deep quadratic tree', (27, 30))]",[],[],[],[]
model,"In this way , unconstrained face variations can be automatically partitioned into different leaves of the learned quadratic tree classifier .","[('of', (14, 15))]","[('unconstrained face variations', (4, 7)), ('different leaves', (12, 14)), ('learned quadratic tree classifier', (16, 20))]",[],[],[],[]
model,The NPD feature measures the relative difference between two pixel values .,"[('measures', (3, 4)), ('between', (7, 8))]","[('NPD feature', (1, 3)), ('relative difference', (5, 7)), ('two pixel values', (8, 11))]",[],[],[],[]
model,"Compared to the absolute difference | x ? y| , NPD is invariant to scale change of the pixel intensities .","[('Compared to', (0, 2)), ('invariant to', (12, 14)), ('of', (16, 17))]","[('absolute difference | x ? y|', (3, 9)), ('NPD', (10, 11)), ('scale change', (14, 16))]",[],[],[],[]
research-problem,NPD FOR FACE DETECTION,[],[],[],[],[],[]
experiments,"For bootstrapping nonface images , we also used the AFLW images , but masked the facial regions with random images containing no faces , as shown in .","[('For', (0, 1)), ('used', (7, 8)), ('masked', (13, 14)), ('with', (17, 18)), ('containing', (20, 21))]","[('bootstrapping nonface images', (1, 4)), ('AFLW images', (9, 11)), ('facial regions', (15, 17)), ('random images', (18, 20)), ('no faces', (21, 23))]",[],[],[],[]
hyperparameters,We used a detection template of 24 24 pixels .,"[('used', (1, 2)), ('of', (5, 6))]","[('detection template', (3, 5)), ('24 24 pixels', (6, 9))]",[],[],[],[]
hyperparameters,"We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .","[('set', (1, 2)), ('of', (5, 6)), ('to be learned as', (9, 13))]","[('maximum depth', (3, 5)), ('tree classifiers', (7, 9)), ('eight', (13, 14))]",[],[],[],[]
hyperparameters,"Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .","[('contains', (3, 4))]","[('final detector', (1, 3)), ('1,226 deep quadratic trees', (4, 8)), ('46,401 NPD features', (10, 13))]",[],[],[],[]
hyperparameters,The detection template is 20 20 pixels .,"[('is', (3, 4))]","[('detection template', (1, 3)), ('20 20 pixels', (4, 7))]",[],[],[],[]
hyperparameters,"The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .","[('contains', (3, 4)), ('was', (17, 18)), ('with', (20, 21)), ('of', (24, 25))]","[('detector cascade', (1, 3)), ('15 stages', (4, 6)), ('target false accept rate', (13, 17)), ('0.5', (18, 19)), ('detection rate', (22, 24)), ('0.999', (25, 26))]",[],[],[],[]
hyperparameters,"In the test stage , a scale factor of 1.2 was set for multiscale detection .","[('of', (8, 9)), ('set for', (11, 13))]","[('test stage', (2, 4)), ('scale factor', (6, 8)), ('1.2', (9, 10)), ('multiscale detection', (13, 15))]",[],[],[],[]
results,The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,"[('proposed', (1, 2)), ('is', (5, 6)), ('at', (10, 11)), ('for', (14, 15)), ('for', (23, 24))]","[('NPD face detector', (2, 5)), ('second best one', (7, 10)), ('FP = 0', (11, 14)), ('discrete metric', (16, 18)), ('third best one', (20, 23)), ('continuous metric', (25, 27))]",[],[],[],[]
results,"It can be observed that the proposed NPD detector is among the top performers for the discrete metric , though it is not as good as the four recent methods for the continuous metric .","[('observed that', (3, 5)), ('among', (10, 11)), ('for', (14, 15))]","[('proposed NPD detector', (6, 9)), ('top performers', (12, 14)), ('discrete metric', (16, 18))]",[],[],[],[]
results,"Compared to recent methods , the Joint Cascade algorithm is the most competitive one to us in terms of accuracy and speed ( see Sec. 5.6 ) .","[('is', (9, 10)), ('in terms of', (16, 19))]","[('Joint Cascade algorithm', (6, 9)), ('most competitive one', (11, 14)), ('accuracy and speed', (19, 22))]",[],[],[],[]
results,The performance of the Zhu-Ramanan model is quite impressive considering such a small training data .,"[('of', (2, 3)), ('is', (6, 7))]","[('performance', (1, 2)), ('Zhu-Ramanan model', (4, 6)), ('quite impressive', (7, 9))]",[],[],[],[]
results,"Many rotated , occluded , and out - of - focus faces can be successfully detected by the proposed method .",[],"[('Many rotated , occluded , and out - of - focus faces', (0, 12)), ('successfully', (14, 15)), ('proposed method', (18, 20))]",[],[],[],[]
results,The results show that the proposed NPD face detector significantly outperforms both the Viola - Jones and PittPatt face detectors .,"[('show', (2, 3))]","[('proposed NPD face detector', (5, 9)), ('significantly outperforms', (9, 11)), ('Viola - Jones and PittPatt face detectors', (13, 20))]",[],[],[],[]
results,Evaluation on CMU - MIT Database,[],[],[],[],[],[]
results,"detector is better when FP < 3 , but SURF cascade method outperforms NPD at higher FPs .","[('when', (3, 4)), ('outperforms', (12, 13)), ('at', (14, 15))]","[('detector', (0, 1)), ('better', (2, 3)), ('FP < 3', (4, 7)), ('SURF cascade method', (9, 12)), ('NPD', (13, 14)), ('higher FPs', (15, 17))]",[],[],[],[]
results,"In addition , the proposed NPD method is not as good as the Soft cascade , the state - of - the - art method on the CMU - MIT dataset .","[('on', (25, 26))]","[('proposed NPD method', (4, 7)), ('not as good', (8, 11)), ('Soft cascade', (13, 15)), ('state - of - the - art method', (17, 25)), ('CMU - MIT dataset', (27, 31))]",[],[],[],[]
results,"Still , the proposed NPD method can detect about 80 % of the frontal faces without any false positives , which is promising .","[('can detect', (6, 8)), ('of', (11, 12)), ('without', (15, 16))]","[('proposed NPD method', (3, 6)), ('about 80 %', (8, 11)), ('frontal faces', (13, 15)), ('false positives', (17, 19))]",[],[],[],[]
results,"The NPD detector performs better than the Haar , LBP , and POF detectors with the same CART based weak learners .","[('performs', (3, 4)), ('than', (5, 6)), ('with', (14, 15))]","[('NPD detector', (1, 3)), ('better', (4, 5)), ('Haar , LBP , and POF detectors', (7, 14)), ('same CART based weak learners', (16, 21))]",[],[],[],[]
results,"The performance improvements due to NPD features over Haar , LBP , and POF features are about 6 % , 19 % , and 15 % , respectively , for discrete metric , and about 4 % , 13 % , and 10 % , respectively , for continuous metric , at FP = 1 .","[('due to', (3, 5)), ('over', (7, 8)), ('are', (15, 16)), ('for', (29, 30)), ('at', (51, 52))]","[('performance improvements', (1, 3)), ('NPD features', (5, 7)), ('Haar , LBP , and POF features', (8, 15)), ('about 6 % , 19 % , and 15 %', (16, 26)), ('discrete metric', (30, 32)), ('4 % , 13 % , and 10 %', (35, 44)), ('continuous', (48, 49)), ('FP', (52, 53))]",[],[],[],[]
results,"NPD is better than POF , because with NPD features the regression tree learns optimal thresholds to form more robust ordinal rules .","[('better than', (2, 4))]","[('NPD', (0, 1)), ('POF', (4, 5))]",[],[],[],[]
results,"NPD performs better than Haar and LBP , especially at low false positives , indicating that combining optimal pixel - level features in regression trees provides better discrimination between faces and nonfaces .","[('performs', (1, 2)), ('than', (3, 4)), ('especially at', (8, 10))]","[('NPD', (0, 1)), ('better', (2, 3)), ('Haar and LBP', (4, 7)), ('low false positives', (10, 13))]",[],[],[],[]
results,"As illustrated , using CART instead of stump classifier improves the face detection performance by about 0 % - 17 % for discrete metric and 0 % - 11 % for continuous metric .","[('using', (3, 4)), ('instead of', (5, 7)), ('improves', (9, 10)), ('by', (14, 15)), ('for', (21, 22)), ('for', (30, 31))]","[('CART', (4, 5)), ('stump classifier', (7, 9)), ('face detection performance', (11, 14)), ('about 0 % - 17 %', (15, 21)), ('discrete metric', (22, 24)), ('0 % - 11 %', (25, 30)), ('continuous metric', (31, 33))]",[],[],[],[]
results,"Besides , the DQT based detector further improves the performance , due to its quadratic splitting capability compared to linear splitting .","[('further improves', (6, 8)), ('due to', (11, 13)), ('compared to', (17, 19))]","[('DQT based detector', (3, 6)), ('performance', (9, 10)), ('quadratic splitting capability', (14, 17)), ('linear splitting', (19, 21))]",[],[],[],[]
results,"shows that the NPD face detector performs the best on the pose and illumination subsets , thanks to the scale - invariant NPD features and the deep quadratic trees .","[('shows', (0, 1)), ('performs', (6, 7)), ('on', (9, 10)), ('thanks to', (16, 18))]","[('NPD face detector', (3, 6)), ('best', (8, 9)), ('pose and illumination subsets', (11, 15)), ('scale - invariant NPD features and the deep quadratic trees', (19, 29))]",[],[],[],[]
results,The original resolution is 1280 720 .,"[('is', (3, 4))]","[('original resolution', (1, 3)), ('1280 720', (4, 6))]",[],[],[],[]
results,The NPD detector achieves similar speed as that of Joint Cascade method .,"[('achieves', (3, 4)), ('as', (6, 7))]","[('NPD detector', (1, 3)), ('similar speed', (4, 6)), ('Joint Cascade method', (9, 12))]",[],[],[],[]
baselines,We have proposed a fast and accurate method for face detection in cluttered scenes .,[],"[('face detection in cluttered scenes', (9, 14))]",[],[],[],[]
research-problem,LFFD : A Light and Fast Face Detector for Edge Devices,[],[],[],[],[],[]
research-problem,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .",[],"[('Face detection', (0, 2))]",[],[],[],[]
research-problem,"Under the new schema , the proposed method can achieve superior accuracy ( WIDER FACE Val / Test - Easy : 0.910/0.896 , Medium : 0.881/0.865 , Hard : 0.780/0.770 ; FDDB - discontinuous : 0.973 , continuous : 0.724 ) .","[('can achieve', (8, 10))]","[('proposed method', (6, 8)), ('superior accuracy', (10, 12)), ('WIDER', (13, 14))]",[],[],[],[]
research-problem,Face detection is a long - standing problem in computer vision .,[],"[('Face detection', (0, 2))]",[],[],[],[]
research-problem,Face detection is a fast - growing branch of general object detection in the past decade .,[],"[('Face detection', (0, 2))]",[],[],[],[]
model,One of its well - known followers is aggregate channel features ( ACF ) which can take advantages of channel features effectively .,"[('can', (15, 16)), ('take advantages of', (16, 19))]","[('aggregate channel features ( ACF )', (8, 14)), ('channel features', (19, 21))]",[],[],[],[]
model,"Two - stage methods consist of proposal selection and localization regression , which are mainly originated from R - CNN series .","[('consist of', (4, 6)), ('mainly', (14, 15))]","[('Two - stage methods', (0, 4)), ('proposal selection', (6, 8))]",[],[],[],[]
model,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .","[('propose', (5, 6)), ('for', (15, 16)), ('considerably balancing', (19, 21))]","[('Light and Fast Face Detector ( LFFD )', (7, 15)), ('edge devices', (16, 18)), ('running efficiency', (24, 26))]",[],[],[],[]
experimental-setup,We flip the cropped image with probability of 0.5 .,"[('flip', (1, 2)), ('with', (5, 6)), ('of', (7, 8))]","[('cropped image', (3, 5)), ('probability', (6, 7)), ('0.5', (8, 9))]",[],[],[],[]
experimental-setup,"For face classification , we use softmax with cross - entropy loss over two classes .","[('For', (0, 1)), ('use', (5, 6)), ('with', (7, 8)), ('over', (12, 13))]","[('face classification', (1, 3)), ('softmax', (6, 7)), ('cross - entropy loss', (8, 12)), ('two classes', (13, 15))]",[],[],[],[]
experimental-setup,"For bbox regression , we adopt L2 loss directly .","[('For', (0, 1)), ('adopt', (5, 6))]","[('bbox regression', (1, 3)), ('L2 loss', (6, 8))]",[],[],[],[]
experimental-setup,We initialize all parameters with xavier method and train the network from scratch .,"[('initialize', (1, 2)), ('with', (4, 5)), ('train', (8, 9)), ('from', (11, 12))]","[('all parameters', (2, 4)), ('xavier method', (5, 7)), ('network', (10, 11))]",[],[],[],[]
experimental-setup,"The optimization method is SGD with 0.9 momentum , zero weight decay and batch size 32 .","[('is', (3, 4)), ('with', (5, 6))]","[('optimization method', (1, 3)), ('SGD', (4, 5)), ('0.9 momentum', (6, 8)), ('zero weight decay', (9, 12)), ('batch size', (13, 15)), ('32', (15, 16))]",[],[],[],[]
experimental-setup,The initial learning rate is 0.1 .,"[('is', (4, 5))]","[('initial learning rate', (1, 4)), ('0.1', (5, 6))]",[],[],[],[]
experimental-setup,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .","[('train', (1, 2)), ('reduce', (5, 6)), ('by multiplying', (9, 11)), ('at', (12, 13))]","[('1,500,000 iterations', (2, 4)), ('learning rate', (7, 9)), ('0.1', (11, 12)), ('iteration 600,000', (13, 15))]",[],[],[],[]
experimental-setup,The training time is about 5 days with two NVIDIA GTX 1080 TI .,"[('is', (3, 4)), ('with', (7, 8))]","[('training time', (1, 3)), ('about 5 days', (4, 7)), ('two NVIDIA GTX 1080 TI', (8, 13))]",[],[],[],[]
results,"DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .","[('can achieve', (8, 10)), ('with', (12, 13))]","[('DSFD , Pyramid Box , S3FD and SSH', (0, 8)), ('high accuracy', (10, 12)), ('marginal gaps', (13, 15))]",[],[],[],[]
results,"Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .","[('obtains', (4, 5)), ('on', (8, 9))]","[('Pyramid Box', (2, 4)), ('best results', (6, 8)), ('Hard parts', (9, 11)), ('SSH', (16, 17))]",[],[],[],[]
results,We can see that the results on Medium and Hard parts are improved remarkably .,"[('see', (2, 3)), ('on', (6, 7))]","[('results', (5, 6)), ('Medium and Hard parts', (7, 11)), ('improved', (12, 13)), ('remarkably', (13, 14))]",[],[],[],[]
results,"Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .","[('consistently', (6, 7))]","[('proposed method LFFD', (3, 6)), ('Face - Boxes', (8, 11)), ('state of the art methods', (16, 21))]",[],[],[],[]
results,"For fair comparison , FaceBoxes 3.2 is used here instead of FaceBoxes .","[('instead of', (9, 11))]","[('FaceBoxes 3.2', (4, 6)), ('FaceBoxes', (11, 12))]",[],[],[],[]
results,"The proposed LFFD runs the fastest at 38402160 , and FaceBoxes 3.2 obtains the highest speed at other three resolutions .","[('runs', (3, 4)), ('at', (6, 7)), ('obtains', (12, 13)), ('at', (16, 17))]","[('proposed LFFD', (1, 3)), ('fastest', (5, 6)), ('38402160', (7, 8)), ('FaceBoxes 3.2', (10, 12)), ('highest speed', (14, 16)), ('other three resolutions', (17, 20))]",[],[],[],[]
research-problem,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",[],"[('Abstract', (0, 1)), ('Face detection and alignment in', (2, 7)), ('unconstrained environment', (7, 9))]",[],[],[],[]
baselines,2 ) Bounding box regression :,[],"[('Bounding box regression', (2, 5))]",[],[],[],[]
baselines,4 ) Multi-source training :,[],"[('Multi-source training', (2, 4))]",[],[],[],[]
baselines,5 ) Online Hard sample mining :,[],"[('Online Hard sample mining', (2, 6))]",[],[],[],[]
baselines,"1 ) P- Net : We randomly crop several patches from WIDER FACE to collect positives , negatives and part face .","[('randomly crop', (6, 8)), ('from', (10, 11)), ('to collect', (13, 15))]","[('P- Net', (2, 4)), ('several patches', (8, 10)), ('WIDER FACE', (11, 13)), ('positives', (15, 16)), ('part face', (19, 21))]",[],[],[],[]
baselines,"2 ) R - Net : We use first stage of our framework to detect faces from WIDER FACE to collect positives , negatives and part face while landmark faces are detected from CelebA .","[('use', (7, 8)), ('of', (10, 11)), ('to detect', (13, 15)), ('from', (16, 17)), ('to collect', (19, 21)), ('detected from', (31, 33))]","[('R - Net', (2, 5)), ('first stage', (8, 10)), ('our framework', (11, 13)), ('faces', (15, 16)), ('WIDER FACE', (17, 19)), ('positives , negatives', (21, 24)), ('part face', (25, 27)), ('landmark faces', (28, 30)), ('CelebA', (33, 34))]",[],[],[],[]
baselines,3 ) O - Net : Similar to R - Net to collect data but we use first two stages of our framework to detect faces .,"[('use', (16, 17)), ('of', (20, 21)), ('to detect', (23, 25))]","[('O - Net', (2, 5)), ('first two stages', (17, 20)), ('our framework', (21, 23)), ('faces', (25, 26))]",[],[],[],[]
experiments,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,"[('compare', (2, 3)), ('of', (5, 6)), ('suggests', (15, 16)), ('for', (24, 25))]","[('bounding box regression', (6, 9)), ('joint landmarks localization task learning', (17, 22)), ('beneficial', (23, 24)), ('face classification', (26, 28)), ('bounding box regression tasks', (29, 33))]",[],[],[],[]
research-problem,Robust Face Detection via Learning Small Faces on Hard Images,[],"[('Face Detection', (1, 3))]",[],[],[],[]
research-problem,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .",[],"[('Face detection', (0, 2))]",[],[],[],[]
experiments,"In , we show that , even on the train set of WIDER FACE , the official pre-trained SSH 1 still fails on some of the images with extremely hard faces .","[('of', (11, 12)), ('on', (22, 23)), ('with', (27, 28))]","[('train set', (9, 11)), ('WIDER FACE', (12, 14)), ('official pre-trained SSH 1', (16, 20)), ('fails', (21, 22)), ('some of the images', (23, 27)), ('extremely hard faces', (28, 31))]",[],[],[],[]
model,"To address this issue , in this paper , we propose a robust face detector by putting more training focus on those hard images .","[('propose', (10, 11)), ('by putting', (15, 17)), ('on', (20, 21))]","[('robust face detector', (12, 15)), ('more training focus', (17, 20)), ('hard images', (22, 24))]",[],[],[],[]
model,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .","[('propose to', (6, 8)), ('at', (11, 12)), ('in parallel with', (14, 17))]","[('mine hard examples', (8, 11)), ('image level', (12, 14)), ('anchor level', (17, 19))]",[],[],[],[]
model,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .","[('to', (10, 11)), ('during', (13, 14))]","[('dynamically assign', (6, 8)), ('difficulty scores', (8, 10)), ('training images', (11, 13)), ('learning process', (15, 17)), ('well - detected', (26, 29))]",[],[],[],[]
model,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .","[('by exclusively exploiting', (15, 18))]","[('detection quality', (13, 15)), ('small faces', (18, 20))]",[],[],[],[]
model,"To conclude , in this paper , we propose a novel face detector with the following contributions :","[('propose', (8, 9))]","[('novel face detector', (10, 13))]",[],[],[],[]
model,"This is done without any extra modules , parameters or computation overhead added on the existing detector .","[('done without', (2, 4)), ('added on', (12, 14))]","[('any extra modules , parameters', (4, 9)), ('computation overhead', (10, 12)), ('existing detector', (15, 17))]",[],[],[],[]
model,"We design a single shot detector with only one detection feature map , which focuses on small faces with a specific range of sizes .","[('design', (1, 2)), ('with', (6, 7)), ('focuses on', (14, 16)), ('with', (18, 19))]","[('single shot detector', (3, 6)), ('only one detection feature map', (7, 12)), ('small faces', (16, 18)), ('specific range of sizes', (20, 24))]",[],[],[],[]
experimental-setup,"We flip all images horizontally , to double the size of our training dataset to 25760 .","[('flip', (1, 2)), ('to double', (6, 8)), ('of', (10, 11)), ('to', (14, 15))]","[('all images', (2, 4)), ('horizontally', (4, 5)), ('size', (9, 10)), ('our training dataset', (11, 14)), ('25760', (15, 16))]",[],[],[],[]
experimental-setup,"We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .","[('use', (1, 2)), ('to initialize', (7, 9))]","[('ImageNet pretrained VGG16 model', (3, 7)), ('our network backbone', (9, 12)), ('our newly introduced layers', (14, 18)), ('randomly initialized', (19, 21)), ('Gaussian initialization', (22, 24))]",[],[],[],[]
experimental-setup,"We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .","[('train', (1, 2)), ('with', (4, 5)), ('to be', (7, 9)), ('for', (11, 12)), ('with', (16, 17)), ('of', (20, 21)), ('with', (30, 31))]","[('model', (3, 4)), ('itersize', (6, 7)), ('2', (9, 10)), ('46 k iterations', (12, 15)), ('learning rate', (18, 20)), ('0.004', (21, 22)), ('14 k iterations', (27, 30)), ('smaller learning rate', (32, 35)), ('0.0004', (36, 37))]",[],[],[],[]
experimental-setup,"During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .","[('During', (0, 1)), ('use', (4, 5)), ('to simultaneously', (7, 9)), ('to compute', (9, 11)), ('update', (14, 15)), ('by', (17, 18)), ('with', (20, 21))]","[('training', (1, 2)), ('4 GPUs', (5, 7)), ('gradient', (12, 13)), ('weight', (16, 17)), ('synchronized SGD', (18, 20)), ('Momentum', (21, 22))]",[],[],[],[]
experimental-setup,"The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .","[('of', (4, 5)), ('during', (8, 9)), ('of', (16, 17)), ('set to have', (19, 22))]","[('first two blocks', (1, 4)), ('VGG16', (5, 6)), ('frozen', (7, 8)), ('training', (10, 11)), ('rest layers', (14, 16)), ('VGG16', (17, 18)), ('double learning rate', (23, 26))]",[],[],[],[]
results,"As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .","[('achieves', (7, 8)), ('on', (11, 12)), ('by', (27, 28))]","[('our method', (5, 7)), ('best performance', (9, 11)), ('hard subset', (13, 15)), ('outperforms', (17, 18)), ('current state - of - the - art', (19, 27)), ('large margin', (29, 31))]",[],[],[],[]
results,"Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - thearts .","[('on', (2, 3)), ('comparable to', (7, 9)), ('on', (22, 23)), ('is', (26, 27))]","[('Our performance', (0, 2)), ('medium subset', (4, 6)), ('most recent state - of - the - art', (10, 19)), ('performance', (21, 22)), ('easy subset', (24, 26)), ('bit worse', (28, 30))]",[],[],[],[]
results,"We show the PR curve at compared with , and our method achieves a new the state - of - the - art performance of AP = 99.0 .","[('show', (1, 2)), ('achieves', (12, 13)), ('of', (24, 25))]","[('PR curve', (3, 5)), ('our method', (10, 12)), ('new the state - of - the - art performance', (14, 24)), ('AP = 99.0', (25, 28))]",[],[],[],[]
results,"As shown in compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 .","[('achieves', (8, 9)), ('with', (21, 22)), ('of', (24, 25))]","[('our method', (6, 8)), ('state - of - the - art and', (9, 17)), ('almost perfect performance', (17, 20)), ('AP', (23, 24)), ('99.60', (25, 26))]",[],[],[],[]
results,"From , we can see that our single level baseline model can achieve performance comparable to the current : Ablation experiments .","[('see that', (4, 6)), ('comparable to', (14, 16))]","[('our single level baseline model', (6, 11)), ('performance', (13, 14)), ('current : Ablation experiments', (17, 21))]",[],[],[],[]
baselines,Baseline - Three is a face detector similar to SSH with three detection feature maps .,"[('similar to', (7, 9)), ('with', (10, 11))]","[('face detector', (5, 7)), ('SSH', (9, 10)), ('three detection feature maps', (11, 15))]",[],[],[],[]
baselines,Baseline - Single is our proposed detector with single detection feature map shown in .,"[('is', (3, 4)), ('with', (7, 8))]","[('Single', (2, 3)), ('our proposed detector', (4, 7)), ('single detection feature map', (8, 12))]",[],[],[],[]
results,"Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors .","[('performs', (7, 8)), ('than', (9, 10))]","[('Our model with single detection feature map', (0, 7)), ('better', (8, 9)), ('one with three detection feature maps', (11, 17))]",[],[],[],[]
results,Combining HIM and DH together can improve further towards the state - of - the - art performance .,"[('Combining', (0, 1))]","[('HIM and DH together', (1, 5)), ('improve', (6, 7)), ('state - of - the - art performance', (10, 18))]",[],[],[],[]
results,The ablation results evaluated on WIDER FACE val dataset are shown in .,"[('evaluated on', (3, 5))]","[('ablation results', (1, 3)), ('WIDER FACE val dataset', (5, 9))]",[],[],[],[]
results,"Our full evaluation resizes the image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels respectively , to build an image pyramid .","[('resizes', (3, 4)), ('contains', (11, 12)), ('to build', (24, 26))]","[('image', (5, 6)), ('short side', (9, 11)), ('100 , 300 , 600 , 1000 and 1400 pixels', (12, 22)), ('image pyramid', (27, 29))]",[],[],[],[]
results,"Without resizing the short side to contain 100 and 300 pixels , the performance on easy subset is only 78.2 , which is even lower than the performance on medium and hard which contain much harder faces .","[('Without resizing', (0, 2)), ('to contain', (5, 7)), ('on', (14, 15)), ('is', (17, 18))]","[('short side', (3, 5)), ('100 and 300 pixels', (7, 11)), ('performance', (13, 14)), ('easy subset', (15, 17)), ('only 78.2', (18, 20))]",[],[],[],[]
results,"For fair comparison , we run all methods on the same machine , with one Titan X ( Maxwell ) GPU , and Intel","[('run', (5, 6)), ('on', (8, 9)), ('with', (13, 14))]","[('all methods', (6, 8)), ('same machine', (10, 12)), ('one Titan X ( Maxwell ) GPU', (14, 21))]",[],[],[],[]
results,"All methods except for Pyramid Box are based on Caffe1 implementation , which is compiled with CUDA 9.0 and CUDNN 7 .","[('except for', (2, 4)), ('based on', (7, 9)), ('compiled with', (14, 16))]","[('Pyramid Box', (4, 6)), ('Caffe1 implementation', (9, 11)), ('CUDA 9.0 and CUDNN 7', (16, 21))]",[],[],[],[]
results,We use the officially built Pad - dlePaddle with CUDA 9.0 and CUDNN 7 .,"[('use', (1, 2)), ('with', (8, 9))]","[('officially built Pad - dlePaddle', (3, 8)), ('CUDA 9.0 and CUDNN 7', (9, 14))]",[],[],[],[]
results,"As shown in , our detector can outperform SSH , S 3 FD and PyramidBox significantly with a smaller inference time .","[('with', (16, 17))]","[('our detector', (4, 6)), ('outperform', (7, 8)), ('SSH , S 3 FD and PyramidBox', (8, 15)), ('significantly', (15, 16)), ('smaller inference time', (18, 21))]",[],[],[],[]
research-problem,Recurrent Scale Approximation for Object Detection in CNN,[],"[('Object Detection', (4, 6))]",[],[],[],[]
research-problem,Object detection is one of the most important tasks in computer vision .,[],"[('Object detection', (0, 2))]",[],[],[],[]
model,"Most of the appearance variations can now be handled in CNN , benefiting from the invariance property of convolution and pooling operations .","[('handled in', (8, 10)), ('benefiting from', (12, 14)), ('of', (17, 18))]","[('Most of the appearance variations', (0, 5)), ('CNN', (10, 11)), ('invariance property', (15, 17)), ('convolution', (18, 19)), ('pooling operations', (20, 22))]",[],[],[],[]
model,"The location variations can be naturally solved via sliding windows , which can be efficiently incorporated into CNN in a fully convolutional manner .","[('can be', (3, 5)), ('via', (7, 8)), ('can be efficiently incorporated into', (12, 17))]","[('location variations', (1, 3)), ('naturally', (5, 6)), ('sliding windows', (8, 10)), ('CNN', (17, 18)), ('fully convolutional manner', (20, 23))]",[],[],[],[]
model,"The first way , as shown in , handles objects of different scales independently by resizing the input into different scales and then forwarding the resized images multiple times for detection .","[('handles', (8, 9)), ('of', (10, 11)), ('by resizing', (14, 16)), ('into', (18, 19)), ('forwarding', (23, 24)), ('for', (29, 30))]","[('first', (1, 2)), ('objects', (9, 10)), ('different scales independently', (11, 14)), ('input', (17, 18)), ('different scales', (19, 21)), ('resized images', (25, 27)), ('multiple times', (27, 29)), ('detection', (30, 31))]",[],[],[],[]
model,"The second way , as depicted in , forwards the image only once and then directly regresses objects at multiple scales .","[('forwards', (8, 9)), ('directly', (15, 16)), ('at', (18, 19))]","[('image', (10, 11)), ('only once', (11, 13)), ('objects', (17, 18)), ('multiple scales', (19, 21))]",[],[],[],[]
model,"Our solution to the feature pyramid in CNN descends from the observations of modern CNN - based detectors , including Faster - RCNN , R - FCN , SSD , YOLO and STN , where feature maps are first computed and the detection results are decoded from the maps afterwards .","[('to', (2, 3)), ('in', (6, 7)), ('descends from', (8, 10)), ('of', (12, 13)), ('including', (19, 20))]","[('feature pyramid', (4, 6)), ('CNN', (7, 8)), ('observations', (11, 12)), ('modern CNN - based detectors', (13, 18)), ('Faster - RCNN', (20, 23)), ('R - FCN', (24, 27)), ('SSD', (28, 29)), ('YOLO', (30, 31)), ('STN', (32, 33)), ('feature', (35, 36))]",[],[],[],[]
model,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .","[('propose', (5, 6)), ('to achieve', (16, 18))]","[('recurrent scale approximation ( RSA , see ) unit', (7, 16))]",[],[],[],[]
model,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,"[('designed to be', (4, 7)), ('in', (12, 13)), ('to be fed with', (16, 20)), ('at', (24, 25))]","[('RSA unit', (1, 3)), ('plugged', (7, 8)), ('some specific depths', (9, 12)), ('network', (14, 15)), ('initial feature map', (21, 24)), ('largest scale', (26, 28))]",[],[],[],[]
model,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,"[('convolves', (2, 3)), ('in', (5, 6)), ('to generate', (9, 11)), ('of', (13, 14))]","[('input', (4, 5)), ('recurrent manner', (7, 9)), ('prediction', (12, 13)), ('feature map', (15, 17)), ('half the size', (19, 22)), ('input', (24, 25))]",[],[],[],[]
model,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,"[('to globally predict', (8, 11)), ('for', (13, 14)), ('compute', (19, 20)), ('for', (22, 23)), ('based on', (29, 31))]","[('scale - forecast network', (4, 8)), ('potential scales', (11, 13)), ('novel image', (15, 17)), ('feature pyramids', (20, 22)), ('prediction', (32, 33))]",[],[],[],[]
model,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,"[('that retraces', (7, 9)), ('of', (11, 12)), ('in', (15, 16)), ('generates', (20, 21)), ('for', (24, 25)), ('based on', (27, 29))]","[('landmark retracing network', (4, 7)), ('location', (10, 11)), ('regressed landmarks', (13, 15)), ('preceding layers', (17, 19)), ('confidence score', (22, 24)), ('each landmark', (25, 27)), ('landmark feature set', (30, 33))]",[],[],[],[]
model,The three components can be incorporated into a unified CNN framework and trained end - to - end .,"[('incorporated into', (5, 7)), ('trained', (12, 13))]","[('unified CNN framework', (8, 11))]",[],[],[],[]
model,"1 ) We prove that deep CNN features for an image can be approximated from different scales using a portable recurrent unit ( RSA ) , which fully leverages efficiency and accuracy .","[('prove', (3, 4)), ('for', (8, 9)), ('approximated from', (13, 15)), ('using', (17, 18)), ('fully leverages', (27, 29))]","[('deep CNN features', (5, 8)), ('image', (10, 11)), ('different scales', (15, 17)), ('portable recurrent unit ( RSA )', (19, 25)), ('efficiency', (29, 30))]",[],[],[],[]
baselines,We use this model in scale - forecast network and LRN .,"[('in', (4, 5))]","[('scale - forecast network and LRN', (5, 11))]",[],[],[],[]
hyperparameters,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .","[('set to', (5, 7)), ('of', (8, 9))]","[('numbers of channels', (1, 4)), ('half', (7, 8)), ('original ResNet model', (10, 13))]",[],[],[],[]
experiments,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,"[('train', (2, 3)), ('use', (10, 11)), ('of', (13, 14)), ('to launch', (16, 18))]","[('scale - forecast network', (4, 8)), ('output', (12, 13)), ('predicted scales', (14, 16)), ('RSA unit and LRN', (19, 23))]",[],[],[],[]
experiments,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .","[('is', (3, 4)), ('set to', (10, 12)), ('with', (13, 14))]","[('batch size', (1, 3)), ('4', (4, 5)), ('base learning rate', (6, 9)), ('0.001', (12, 13)), ('decrease', (15, 16))]",[],[],[],[]
hyperparameters,"The maximum training iteration is 1,000,000 .","[('is', (4, 5))]","[('maximum training iteration', (1, 4)), ('1,000,000', (5, 6))]",[],[],[],[]
hyperparameters,We use stochastic gradient descent as the optimizer .,"[('use', (1, 2)), ('as', (5, 6))]","[('stochastic gradient descent', (2, 5)), ('optimizer', (7, 8))]",[],[],[],[]
experiments,"We can observe from the results that our trained scale network recalls almost 99 % at x = 1 , indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales .","[('observe', (2, 3)), ('recalls', (11, 12)), ('at', (15, 16))]","[('our trained scale network', (7, 11)), ('almost 99 %', (12, 15)), ('x = 1', (16, 19))]",[],[],[],[]
hyperparameters,"knowledge , during inference , we set the threshold for predicting potential scales of the input so that it has approximately two predictions .","[('during', (2, 3)), ('set', (6, 7)), ('for predicting', (9, 11)), ('of', (13, 14))]","[('inference', (3, 4)), ('threshold', (8, 9)), ('potential scales', (11, 13)), ('input', (15, 16)), ('approximately two predictions', (20, 23))]",[],[],[],[]
results,Performance of Scale - forecast Network,[],[],[],[],[],[]
baselines,Ablative Evaluation on RSA Unit,[],[],[],[],[],[]
experiments,"The image is first resized to higher dimension being 2048 and the RSA unit predicts six scales defined in Section 3.1 ( 1024 , 512 , 256 , 128 and 64 ) .","[('first', (3, 4)), ('resized to', (4, 6)), ('being', (8, 9))]","[('image', (1, 2)), ('higher dimension', (6, 8)), ('2048', (9, 10)), ('RSA', (12, 13))]",[],[],[],[]
experiments,"However , results from the figure indicate that as we plug RSA at deeper layers , its performance decades .","[('indicate', (6, 7)), ('plug', (10, 11)), ('at', (12, 13))]","[('RSA', (11, 12)), ('deeper layers', (13, 15)), ('performance decades', (17, 19))]",[],[],[],[]
experiments,"For example , in case final feature which means RSA is plugged at the final convolution layer after res3c , the error rate is almost 100 % , indicating RSA 's incapability of handling the insufficient information in this layer .","[('in', (3, 4)), ('means', (8, 9)), ('plugged at', (11, 13)), ('after', (17, 18)), ('is', (23, 24))]","[('case final feature', (4, 7)), ('RSA', (9, 10)), ('final convolution layer', (14, 17)), ('res3c', (18, 19)), ('error rate', (21, 23)), ('almost 100 %', (24, 27))]",[],[],[],[]
experiments,The error rate decreases in shallower cases .,[],"[('error rate', (1, 3)), ('decreases', (3, 4)), ('shallower cases', (5, 7))]",[],[],[],[]
experiments,The path during one - time forward from image to the input map right before RSA is shorter ; and the rolling out time increases accordingly .,"[('during', (2, 3)), ('from', (7, 8)), ('to', (9, 10)), ('right before', (13, 15)), ('is', (16, 17))]","[('path', (1, 2)), ('one - time forward', (3, 7)), ('image', (8, 9)), ('input map', (11, 13)), ('RSA', (15, 16)), ('shorter', (17, 18)), ('rolling out time', (21, 24)), ('increases', (24, 25))]",[],[],[],[]
experiments,Most of the computation happens before layer res2 b and it has an acceptable error rate of 3.44 % .,"[('happens before', (4, 6)), ('of', (16, 17))]","[('computation', (3, 4)), ('layer res2 b', (6, 9)), ('acceptable error rate', (13, 16)), ('3.44 %', (17, 19))]",[],[],[],[]
experiments,Our Algorithm vs. Baseline RPN,[],[],[],[],[],[]
baselines,"We compare our model ( denoted as RSA + LRN ) , a combination of the RSA unit and a landmark retracing network , with the region proposal network ( RPN ) .","[('denoted', (5, 6)), ('combination of', (13, 15)), ('with', (24, 25))]","[('region proposal network ( RPN )', (26, 32))]",[],[],[],[]
experiments,"In the first setting , we use the original RPN with multiple anchors ( denoted as RPN m ) to detect faces of various scales .","[('use', (6, 7)), ('with', (10, 11)), ('denoted as', (14, 16)), ('to detect', (19, 21))]","[('original RPN', (8, 10)), ('multiple anchors', (11, 13)), ('RPN m', (16, 18)), ('faces of various scales', (21, 25))]",[],[],[],[]
experiments,"On AFW , our algorithm achieves an AP of 99.17 % using the original annotation and an AP of 99 . 96 % using the revised annotation 7 ( c ) .","[('On', (0, 1)), ('achieves', (5, 6)), ('of', (8, 9)), ('using', (11, 12)), ('using', (23, 24))]","[('AFW', (1, 2)), ('our algorithm', (3, 5)), ('AP', (7, 8)), ('99.17 %', (9, 11)), ('original annotation', (13, 15)), ('AP', (17, 18)), ('99 . 96 %', (19, 23)), ('revised annotation', (25, 27))]",[],[],[],[]
experiments,"On FDDB , RSA + LRN recalls 93.0 % faces with 50 false positives 7 ( a ) .","[('On', (0, 1)), ('recalls', (6, 7)), ('with', (10, 11))]","[('FDDB', (1, 2)), ('RSA + LRN', (3, 6)), ('93.0 % faces', (7, 10)), ('50 false positives', (11, 14))]",[],[],[],[]
experiments,"On MALF , our method recalls 82.4 % faces with zero false positive 7 ( d ) .","[('On', (0, 1)), ('recalls', (5, 6)), ('with', (9, 10))]","[('MALF', (1, 2)), ('our method', (3, 5)), ('82.4 % faces', (6, 9)), ('zero false positive', (10, 13))]",[],[],[],[]
experiments,"To address this , we learn a transformer to fit each annotation from the landmarks .","[('learn', (5, 6)), ('to fit', (8, 10)), ('from', (12, 13))]","[('transformer', (7, 8)), ('each annotation', (10, 12)), ('landmarks', (14, 15))]",[],[],[],[]
experiments,"Our proposed model can detect faces at various scales , including the green annotations provided in AFW as well as faces marked in red that are of small sizes and not labeled in the dataset ..","[('at', (6, 7)), ('including', (10, 11)), ('provided in', (14, 16)), ('as well as', (17, 20)), ('marked in', (21, 23)), ('of', (26, 27)), ('not', (30, 31))]","[('faces', (5, 6)), ('various scales', (7, 9)), ('green annotations', (12, 14)), ('AFW', (16, 17)), ('faces', (20, 21)), ('red', (23, 24)), ('small sizes', (27, 29))]",[],[],[],[]
experiments,"The proposed algorithm ( Scale - forecast network with RSA + LRN , tagged by LRN + RSA ) outperforms other methods by a large margin .","[('tagged by', (13, 15)), ('by', (22, 23))]","[('proposed algorithm ( Scale - forecast network with', (1, 9)), ('RSA + LRN', (9, 12)), ('LRN + RSA )', (15, 19)), ('outperforms', (19, 20)), ('other methods', (20, 22)), ('large margin', (24, 26))]",[],[],[],[]
experiments,RSA on Generic Object Proposal,[],[],[],[],[],[]
experiments,We now verify that the scale approximation learning by RSA unit also generalizes comparably well on the generic region proposal task .,"[('verify', (2, 3)), ('by', (8, 9)), ('on', (15, 16))]","[('scale approximation learning', (5, 8)), ('RSA unit', (9, 11)), ('generalizes', (12, 13)), ('comparably well', (13, 15)), ('generic region proposal task', (17, 21))]",[],[],[],[]
experiments,ILSVRC DET is a challenging dataset for generic object detection .,[],"[('ILSVRC DET', (0, 2)), ('generic object detection', (7, 10))]",[],[],[],[]
experiments,We choose the single anchor RPN with ResNet - 101 as the baseline .,"[('choose', (1, 2)), ('with', (6, 7))]","[('single anchor RPN', (3, 6)), ('ResNet - 101', (7, 10))]",[],[],[],[]
experiments,"The anchors are of size 128 ? 2 squared , 128256 and 256128 .","[('of size', (3, 5))]","[('anchors', (1, 2)), ('128 ? 2 squared', (5, 9)), ('128256', (10, 11)), ('256128', (12, 13))]",[],[],[],[]
experiments,Scale - forecast network is also employed to predict the higher dimension of objects in the image .,"[('in', (14, 15))]","[('Scale - forecast network', (0, 4)), ('higher dimension of objects', (10, 14)), ('image', (16, 17))]",[],[],[],[]
experiments,"Without loss of recall , RPN + RSA reduces around 61.05 % computation cost compared with the single - scale RPN , when the number of boxes is over 100 .","[('Without loss of', (0, 3)), ('reduces around', (8, 10)), ('compared with', (14, 16)), ('when', (22, 23)), ('is', (27, 28))]","[('recall', (3, 4)), ('RPN + RSA', (5, 8)), ('61.05 % computation cost', (10, 14)), ('single - scale RPN', (17, 21)), ('number of boxes', (24, 27)), ('over 100', (28, 30))]",[],[],[],[]
experiments,RPN + RSA is also more efficient and recalls more objects than original RPN .,"[('recalls', (8, 9)), ('than', (11, 12))]","[('RPN + RSA', (0, 3)), ('more efficient', (5, 7)), ('more objects', (9, 11)), ('original RPN', (12, 14))]",[],[],[],[]
experiments,Our model and the single - anchor RPN both perform better than the original RPN .,"[('perform', (9, 10)), ('than', (11, 12))]","[('Our model and the single - anchor RPN', (0, 8)), ('better', (10, 11)), ('original RPN', (13, 15))]",[],[],[],[]
experiments,"Overall , our scheme of using RSA plus LRN competes comparably with the standard RPN method in terms of computation efficiency and accuracy .","[('competes', (9, 10)), ('in terms of', (16, 19))]","[('our scheme of', (2, 5)), ('RSA plus LRN', (6, 9)), ('comparably', (10, 11)), ('standard RPN method', (13, 16)), ('computation efficiency and accuracy', (19, 23))]",[],[],[],[]
research-problem,Detecting Faces Using Region - based Fully Convolutional Networks,[],"[('Detecting Faces', (0, 2))]",[],[],[],[]
research-problem,"In this report , we propose a region - based face detector applying deep networks in a fully convolutional fashion , named Face R - FCN .","[('propose', (5, 6)), ('applying', (12, 13)), ('in', (15, 16)), ('named', (21, 22))]","[('region - based face detector', (7, 12)), ('deep networks', (13, 15)), ('fully convolutional fashion', (17, 20)), ('Face R - FCN', (22, 26))]",[],[],[],[]
research-problem,"Based on Region - based Fully Convolutional Networks ( R - FCN ) , our face detector is more accurate and computationally efficient compared with the previous R - CNN based face detectors .","[('Based on', (0, 2)), ('is', (17, 18)), ('compared with', (23, 25))]","[('Region - based Fully Convolutional Networks ( R - FCN )', (2, 13)), ('our face detector', (14, 17)), ('more accurate', (18, 20)), ('previous R - CNN based face detectors', (26, 33))]",[],[],[],[]
research-problem,Face detection plays an important role in the modern face - relevant applications .,[],"[('Face detection', (0, 2))]",[],[],[],[]
approach,"The ConvNet of R - FCN is built with the computations shared on the entire image , which leads to the improvement of training and testing efficiency .","[('built with', (7, 9)), ('shared on', (11, 13)), ('leads to', (18, 20)), ('of', (22, 23))]","[('ConvNet of R - FCN', (1, 6)), ('computations', (10, 11)), ('entire image', (14, 16)), ('improvement', (21, 22)), ('training and testing efficiency', (23, 27))]",[],[],[],[]
approach,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .","[('develop', (5, 6)), ('on the top of', (9, 13)), ('with', (16, 17)), ('of', (19, 20)), ('achieves', (24, 25)), ('than', (28, 29))]","[('face detector', (7, 9)), ('R - FCN', (13, 16)), ('elaborate design', (17, 19)), ('details', (21, 22)), ('more decent performance', (25, 28))]",[],[],[],[]
approach,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .","[('introduce', (12, 13)), ('to generate', (19, 21)), ('for enhancing', (23, 25)), ('eliminate', (28, 29)), ('in', (34, 35))]","[('position - sensitive average pooling', (14, 19)), ('embedding features', (21, 23)), ('discrimination', (25, 26)), ('effect of non-uniformed contribution', (30, 34)), ('each facial part', (35, 38))]",[],[],[],[]
approach,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,"[('integrated into', (12, 14)), ('for boosting', (18, 20)), ('on', (22, 23))]","[('on - line hard example mining ( OHEM ) technique', (1, 11)), ('our network', (14, 16)), ('learning', (21, 22)), ('hard examples', (23, 25))]",[],[],[],[]
approach,"The proposed approach is based on R - FCN and is well suited for face detection , thus we call it Face R - FCN .","[('based on', (4, 6)), ('well suited for', (11, 14)), ('call it', (19, 21))]","[('R - FCN', (6, 9)), ('face detection', (14, 16)), ('Face R - FCN', (21, 25))]",[],[],[],[]
hyperparameters,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .","[('initialize', (8, 9)), ('with', (11, 12)), ('of', (15, 16)), ('trained on', (20, 22))]","[('our network', (9, 11)), ('pre-trained weights', (13, 15)), ('101 - layer ResNet', (16, 20)), ('Image Net', (22, 24))]",[],[],[],[]
hyperparameters,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .","[('at', (12, 13)), ('of', (16, 17)), ('throughout', (20, 21)), ('trained on', (33, 35))]","[('freeze', (3, 4)), ('general kernels ( weights of', (5, 10)), ('few layers', (10, 12)), ('beginning )', (14, 16)), ('pre-trained model', (18, 20)), ('entire training process', (22, 25)), ('essential feature extractor', (30, 33)), ('ImageNet', (35, 36))]",[],[],[],[]
baselines,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .","[('In terms of', (0, 3)), ('enumerates', (11, 12)), ('of', (14, 15)), ('to accurately search for', (19, 23))]","[('RPN stage', (4, 6)), ('Face R - FCN', (7, 11)), ('multiple configurations', (12, 14)), ('anchor', (16, 17)), ('faces', (23, 24))]",[],[],[],[]
hyperparameters,We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,"[('combine', (1, 2)), ('of', (4, 5)), ('to construct', (11, 13))]","[('range', (3, 4)), ('multi-scale anchors', (13, 15))]",[],[],[],[]
baselines,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,"[('learned jointly with', (8, 11))]","[('RPN and R - FCN', (1, 6)), ('softmax loss', (12, 14)), ('smooth L1 loss', (16, 19))]",[],[],[],[]
hyperparameters,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,"[('with', (12, 13))]","[('Non- maximum suppression ( NMS )', (0, 6)), ('anchors', (11, 12)), ('certain IoU scores', (13, 16))]",[],[],[],[]
hyperparameters,The proposals are processed by OHEM to train with hard examples .,"[('processed by', (3, 5)), ('to train with', (6, 9))]","[('proposals', (1, 2)), ('OHEM', (5, 6)), ('hard examples', (9, 11))]",[],[],[],[]
hyperparameters,We set the 256 for the size of RPN mini-batch and 128 for R - FCN respectively .,"[('set', (1, 2)), ('for', (4, 5)), ('for', (12, 13))]","[('256', (3, 4)), ('size', (6, 7)), ('RPN mini-batch', (8, 10)), ('128', (11, 12)), ('R - FCN', (13, 16))]",[],[],[],[]
hyperparameters,"We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .","[('utilize', (1, 2)), ('where', (4, 5)), ('resized with', (9, 11)), ('to', (13, 14))]","[('multi-scale training', (2, 4)), ('input image', (6, 8)), ('bilinear interpolation', (11, 13)), ('various scales', (14, 16))]",[],[],[],[]
hyperparameters,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .","[('In', (0, 1)), ('performed by', (8, 10)), ('into', (12, 13)), ('for', (16, 17)), ('on', (19, 20))]","[('testing stage', (2, 4)), ('multi-scale testing', (5, 7)), ('scale image', (10, 12)), ('image pyramid', (14, 16)), ('better detecting', (17, 19)), ('both tiny and general faces', (20, 25))]",[],[],[],[]
results,"As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .","[('consistently wins', (7, 9)), ('across', (12, 13)), ('on', (16, 17)), ('of', (24, 25))]","[('our proposed approach', (4, 7)), ('1st place', (10, 12)), ('three subsets', (14, 16)), ('validation set and test set', (19, 24)), ('WIDER FACE', (25, 27)), ('significantly outperforms', (28, 30)), ('existing results', (31, 33))]",[],[],[],[]
results,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .","[('on', (3, 4)), ('superior to', (12, 14)), ('performing one by', (18, 21)), ('demonstrates', (26, 27))]","[('WIDER FACE hard subset', (4, 8)), ('our approach', (9, 11)), ('prior best', (15, 17)), ('clear margin', (22, 24))]",[],[],[],[]
experiments,We use the training set of the WIDER FACE dataset to train our model ( denoted as Model - A in ) and compare against the recently published top approaches on FDDB .,"[('use', (1, 2)), ('of', (5, 6)), ('to train', (10, 12)), ('on', (30, 31))]","[('training set', (3, 5)), ('WIDER FACE dataset', (7, 10)), ('our model', (12, 14)), ('FDDB', (31, 32))]",[],[],[],[]
experiments,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .","[('consistently achieves', (10, 12)), ('in terms of both', (15, 19))]","[('Face R - FCN', (6, 10)), ('impressive performance', (13, 15)), ('discrete ROC curve', (20, 23)), ('continuous ROC curve', (24, 27))]",[],[],[],[]
experiments,Our discrete ROC curve is superior to the prior best - performing method .,"[('is', (4, 5)), ('to', (6, 7))]","[('discrete ROC curve', (1, 4)), ('superior', (5, 6)), ('prior best - performing method', (8, 13))]",[],[],[],[]
experiments,We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,"[('obtain', (2, 3)), ('of', (8, 9)), ('at', (13, 14))]","[('best true positive rate', (4, 8)), ('discrete ROC curve', (10, 13)), ('1000/2000 false positives', (14, 17))]",[],[],[],[]
experiments,But the competitive result we achieved is still noticeable .,"[('achieved', (5, 6))]","[('competitive result', (2, 4)), ('noticeable', (8, 9))]",[],[],[],[]
experiments,"Face R - FCN shows the superior performance over the prior methods across the three subsets ( easy , medium and hard ) in both validation and test sets .","[('shows', (4, 5)), ('across', (12, 13)), ('in', (23, 24))]","[('Face R - FCN', (0, 4)), ('superior performance', (6, 8)), ('prior methods', (10, 12)), ('three subsets', (14, 16)), ('validation and test sets', (25, 29))]",[],[],[],[]
results,"As expected , the performance of Face R - FCN is further improved .","[('of', (5, 6))]","[('performance', (4, 5)), ('Face R - FCN', (6, 10)), ('further improved', (11, 13))]",[],[],[],[]
research-problem,Finding Tiny Faces,[],[],[],[],[],[]
research-problem,"We describe a detector that can find around 800 faces out of the reportedly 1000 present , by making use of novel characterizations of scale , resolution , and context to find small objects .","[('describe', (1, 2)), ('can find', (5, 7)), ('of', (11, 12)), ('by making use of', (17, 21)), ('of', (23, 24)), ('to find', (30, 32))]","[('detector', (3, 4)), ('around 800 faces out', (7, 11)), ('reportedly 1000 present', (13, 16)), ('novel characterizations', (21, 23)), ('scale , resolution , and context', (24, 30)), ('small objects', (32, 34))]",[],[],[],[]
approach,We make use of a coarse image pyramid to capture extreme scale challenges in ( c ) .,"[('use of', (2, 4)), ('to capture', (8, 10))]","[('coarse image pyramid', (5, 8)), ('extreme scale challenges', (10, 13))]",[],[],[],[]
approach,"Finally , to improve performance on small faces , we model additional context , which is efficiently implemented as a fixed - size receptive field across all scale - specific templates ( d ) .","[('to improve', (2, 4)), ('on', (5, 6)), ('model', (10, 11)), ('efficiently implemented as', (16, 19)), ('across', (25, 26))]","[('performance', (4, 5)), ('small faces', (6, 8)), ('additional context', (11, 13)), ('fixed - size receptive field', (20, 25)), ('all scale - specific templates', (26, 31))]",[],[],[],[]
research-problem,Multi - task modeling of scales :,[],"[('Multi - task modeling of scales', (0, 6))]",[],[],[],[]
approach,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .","[('Instead of', (0, 2)), ('train', (11, 12)), ('tuned for', (14, 16))]","[('"" one-size - fitsall "" approach', (3, 9)), ('separate detectors', (12, 14)), ('different scales ( and aspect ratios )', (16, 23))]",[],[],[],[]
approach,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .","[('train and run', (6, 9)), ('in', (13, 14)), ('make use of', (19, 22)), ('defined over', (23, 25)), ('of', (27, 28))]","[('scale - specific detectors', (9, 13)), ('multitask fashion', (15, 17)), ('features', (22, 23)), ('multiple layers', (25, 27)), ('single ( deep ) feature hierarchy', (28, 34))]",[],[],[],[]
approach,"In , we present a simple human experiment where users attempt to classify true and false positive faces ( as given by our detector ) .","[('attempt to', (10, 12))]","[('users', (9, 10)), ('classify', (12, 13)), ('true and false positive faces', (13, 18))]",[],[],[],[]
approach,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .","[('demonstrate', (1, 2)), ('extracted from', (6, 8)), ('known as', (12, 14)), ('are', (19, 20)), ('that capture', (25, 27)), ('across', (38, 39))]","[('convolutional deep features', (3, 6)), ('multiple layers', (8, 10)), ('hypercolumn "" features', (15, 18)), ('effective "" foveal "" descriptors', (20, 25)), ('high - resolution detail', (28, 32)), ('coarse low - resolution cues', (33, 38)), ('large receptive field ( )', (39, 44))]",[],[],[],[]
approach,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,"[('show', (1, 2)), ('of', (5, 6)), ('extracted from', (10, 12))]","[('highresolution components', (3, 5)), ('our foveal descriptors', (6, 9)), ('lower convolutional layers', (12, 15))]",[],[],[],[]
approach,"In particular , when compared to prior art on WIDER FACE , our results reduce error by a factor of 2 ( our models produce an AP of 82 % while prior art ranges from 29 - 64 % ) . :","[('compared to', (4, 6)), ('on', (8, 9)), ('reduce', (14, 15)), ('by', (16, 17))]","[('prior art', (6, 8)), ('WIDER FACE', (9, 11)), ('our results', (12, 14)), ('error', (15, 16)), ('factor of', (18, 20)), ('2', (20, 21))]",[],[],[],[]
experiments,Adding a fixed contextual window of 300 pixels dramatically reduces error on small faces by 20 % .,"[('Adding', (0, 1)), ('of', (5, 6)), ('dramatically reduces', (8, 10)), ('on', (11, 12)), ('by', (14, 15))]","[('fixed contextual window', (2, 5)), ('300 pixels', (6, 8)), ('error', (10, 11)), ('small faces', (12, 14)), ('20 %', (15, 17))]",[],[],[],[]
hyperparameters,"We use a fixed learning rate of 10 ? 4 , a weight decay of 0.0005 , and a momentum of 0.9 .","[('use', (1, 2)), ('of', (6, 7))]","[('fixed learning rate', (3, 6)), ('10 ? 4', (7, 10)), ('weight decay', (12, 14)), ('0.0005', (15, 16)), ('momentum', (19, 20)), ('0.9', (21, 22))]",[],[],[],[]
results,"As shows , our hybrid - resolution model ( HR ) achieves state - of - the - art performance on all difficulty levels , but most importantly , reduces error on the "" hard "" set by 2X .","[('achieves', (11, 12)), ('on', (20, 21)), ('reduces', (29, 30)), ('on', (31, 32)), ('by', (37, 38))]","[('our hybrid - resolution model ( HR )', (3, 11)), ('state - of - the - art performance', (12, 20)), ('all difficulty levels', (21, 24)), ('error', (30, 31)), ('"" hard "" set', (33, 37)), ('2X', (38, 39))]",[],[],[],[]
results,"Our out - of - the - box detector ( HR ) outperforms all published results on the discrete score , which uses a standard 50 % intersection - over - union threshold to define correctness .","[('outperforms', (12, 13)), ('on', (16, 17)), ('uses', (22, 23)), ('to define', (33, 35))]","[('Our out - of - the - box detector ( HR )', (0, 12)), ('all published results', (13, 16)), ('discrete score', (18, 20)), ('standard 50 % intersection - over - union threshold', (24, 33)), ('correctness', (35, 36))]",[],[],[],[]
hyperparameters,Our regressor is trained with 10 - fold cross validation .,"[('trained with', (3, 5))]","[('Our regressor', (0, 2)), ('10 - fold cross validation', (5, 10))]",[],[],[],[]
results,Our Resnet 101 - based detector runs at 1.4 FPS on 1080 p resolution and 3.1 FPS on 720 p resolution .,"[('runs at', (6, 8)), ('on', (10, 11)), ('on', (17, 18))]","[('Resnet 101 - based detector', (1, 6)), ('1.4 FPS', (8, 10)), ('1080 p resolution', (11, 14)), ('3.1 FPS', (15, 17)), ('720 p resolution', (18, 21))]",[],[],[],[]
baselines,"We propose a simple yet effective framework for finding small objects , demonstrating that both large context and scale - variant representations are crucial .",[],"[('finding small objects', (8, 11))]",[],[],[],[]
ablation-analysis,We specifically show that massively - large receptive fields can be effectively encoded as a foveal descriptor that captures both coarse context ( necessary for detecting small objects ) and high - resolution image features ( helpful for localizing small objects ) .,"[('show', (2, 3)), ('can be', (9, 11)), ('captures', (18, 19))]","[('massively - large receptive fields', (4, 9)), ('effectively encoded', (11, 13)), ('foveal descriptor', (15, 17)), ('coarse context', (20, 22)), ('high - resolution image features', (30, 35))]",[],[],[],[]
ablation-analysis,"We also explore the encoding of scale in existing pre-trained deep networks , suggesting a simple way to extrapolate networks tuned for limited scales to more extreme scenarios in a scale - variant fashion .","[('explore', (2, 3)), ('in', (7, 8)), ('tuned for', (20, 22))]","[('encoding of scale', (4, 7)), ('existing pre-trained deep networks', (8, 12))]",[],[],[],[]
results,"By learning a post - hoc regressor that converts bounding boxes to ellipses , our approach ( HR - ER ) produces state - of the - art continuous overlaps as well ( right ) .","[('learning', (1, 2)), ('that converts', (7, 9)), ('to', (11, 12)), ('produces', (21, 22))]","[('post - hoc regressor', (3, 7)), ('bounding boxes', (9, 11)), ('ellipses', (12, 13)), ('our approach ( HR - ER )', (14, 21)), ('state - of the - art continuous overlaps', (22, 30))]",[],[],[],[]
results,"Our proposed detector is able to detect faces at a continuous range of scales , while being robust to challenges such as expression , blur , illumination etc .","[('able to detect', (4, 7)), ('at', (8, 9)), ('such as', (20, 22))]","[('proposed detector', (1, 3)), ('faces', (7, 8)), ('continuous range of scales', (10, 14)), ('robust', (17, 18)), ('challenges', (19, 20)), ('expression', (22, 23)), ('blur', (24, 25)), ('illumination', (26, 27))]",[],[],[],[]
experiments,Online hard mining and balanced sampling,[],[],[],[],[],[]
experiments,We set a small threshold ( 0.03 ) on classification loss to filter out easy locations .,"[('set', (1, 2)), ('on', (8, 9)), ('to filter', (11, 13))]","[('small threshold ( 0.03 )', (3, 8)), ('classification loss', (9, 11)), ('out', (13, 14)), ('easy locations', (14, 16))]",[],[],[],[]
results,Our detector is mostly affected by object scale ( from 0.044 to 0.896 ) and blur ( from 0.259 to 0.798 ) .,"[('mostly affected by', (3, 6))]","[('Our detector', (0, 2)), ('object scale', (6, 8)), ('from 0.044 to 0.896', (9, 13)), ('blur', (15, 16))]",[],[],[],[]
research-problem,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,[],"[('Unsupervised Hypernym Discovery', (13, 16))]",[],[],[],[]
research-problem,This paper describes a simple but competitive unsupervised system for hypernym discovery .,[],"[('hypernym discovery', (10, 12))]",[],[],[],[]
research-problem,"This shared task differs from recent taxonomy evaluation tasks by concentrating on Hypernym Discovery : the task of predicting ( discovering ) n hypernym candidates for a given input word , within the vocabulary of a specific domain .",[],"[('Hypernym Discovery', (12, 14))]",[],[],[],[]
research-problem,There are several competing approaches for producing word embedding vectors .,"[('for', (5, 6))]","[('several competing approaches', (2, 5)), ('producing word embedding vectors', (6, 10))]",[],[],[],[]
results,Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 .,"[('ranked at', (3, 5)), ('on', (9, 10)), ('with', (14, 15)), ('of', (22, 23))]","[('eleven out of eighteen', (5, 9)), ('medical domain subtask', (11, 14)), ('Mean Average Precision ( MAP )', (16, 22)), ('8.13', (23, 24))]",[],[],[],[]
results,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .","[('On', (0, 1)), ('ranked', (9, 10)), ('with', (15, 16)), ('of', (18, 19)), ('ranking', (21, 22)), ('among', (23, 24))]","[('music industry domain subtask', (2, 6)), ('our system', (7, 9)), ('13th out', (10, 12)), ('16 places', (13, 15)), ('MAP', (17, 18)), ('1.88', (19, 20)), ('4th', (22, 23)), ('unsupervised systems', (25, 27))]",[],[],[],[]
research-problem,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings,[],"[('Neural Hypernym Discovery', (7, 10))]",[],[],[],[]
research-problem,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .",[],"[('hypernym discovery', (4, 6))]",[],[],[],[]
research-problem,"Various natural language processing ( NLP ) tasks , especially those semantically intensive ones aiming for inference and reasoning with generalization capability , such as question answering and textual entailment , can benefit from identifying semantic relations between words beyond synonymy .",[],[],[],[],[],[]
research-problem,The hypernym discovery task aims to discover the most appropriate hypernym ( s ) for input concepts or entities from a pre-defined corpus .,[],"[('hypernym discovery', (1, 3))]",[],[],[],[]
research-problem,"The other challenge is representation for terms , including words and phrases , where the phrase embedding could not be obtained byword embeddings directly .","[('including', (8, 9)), ('could not be obtained byword', (17, 22))]","[('representation for terms', (4, 7)), ('words and phrases', (9, 12)), ('phrase embedding', (15, 17))]",[],[],[],[]
model,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .","[('introduce', (5, 6)), ('for', (10, 11)), ('empirically study', (15, 17)), ('to model', (20, 22)), ('for', (25, 26))]","[('neural network architecture', (7, 10)), ('concerned task', (12, 14)), ('various neural networks', (17, 20)), ('distributed representations', (23, 25)), ('words and phrases', (26, 29))]",[],[],[],[]
experimental-setup,Our model was implemented using the Theano 1 .,"[('implemented using', (3, 5))]","[('Our model', (0, 2)), ('Theano', (6, 7))]",[],[],[],[]
experimental-setup,The diagonal variant of Ada - Grad is used for neural network training .,"[('of', (3, 4)), ('used for', (8, 10))]","[('diagonal variant', (1, 3)), ('Ada - Grad', (4, 7)), ('neural network training', (10, 13))]",[],[],[],[]
experimental-setup,The hidden dimension of all neural models are 200 .,"[('of', (3, 4)), ('are', (7, 8))]","[('hidden dimension', (1, 3)), ('all neural models', (4, 7)), ('200', (8, 9))]",[],[],[],[]
experimental-setup,The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,"[('set to', (4, 6)), ('set to', (16, 18))]","[('batch size', (1, 3)), ('20', (6, 7)), ('word embedding and sense embedding sizes', (9, 15)), ('300', (18, 19))]",[],[],[],[]
experimental-setup,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .","[('trained on', (5, 7)), ('with', (17, 18)), ('for', (20, 21)), ('for', (25, 26)), ('for', (36, 37))]","[('single GPU ( NVIDIA GTX 980 Ti )', (8, 16)), ('roughly 1.5h', (18, 20)), ('general - purpose subtask', (21, 25)), ('English', (26, 27)), ('medical and music', (37, 40))]",[],[],[],[]
results,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .","[('observe', (2, 3)), ('better than', (9, 11))]","[('CNN - based network performance', (3, 8)), ('RNN - based', (11, 14))]",[],[],[],[]
results,"All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .","[('outperform', (4, 5)), ('in terms of', (8, 11)), ('performs', (20, 21)), ('than', (22, 23)), ('in', (27, 28)), ('using', (32, 33))]","[('All the neural models', (0, 4)), ('term embedding averaging', (5, 8)), ('all the metrics', (11, 14)), ('better', (21, 22)), ('RNN - based ones', (23, 27)), ('most of the metrics', (28, 32)), ('word embedding', (33, 35))]",[],[],[],[]
results,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .","[('Compared with', (0, 2)), ('shows', (8, 9))]","[('word embedding', (2, 4)), ('sense embedding', (6, 8)), ('much poorer result', (10, 13))]",[],[],[],[]
research-problem,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection,[],[],[],[],[],[]
results,"The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .","[('show', (2, 3)), ('to', (4, 5))]","[('preference', (3, 4)), ('syntactic context - types ( dep and joint )', (6, 15))]",[],[],[],[]
results,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .","[('in', (17, 18)), ('shown to', (34, 36))]","[('raw frequency', (11, 13)), ('successful', (16, 17)), ('hypernymy detection', (18, 20)), ('PPMI', (32, 33)), ('outperform', (36, 37))]",[],[],[],[]
results,The inclusion hypothesis seems to be most effective in discriminating between hypernyms and meronyms under syntactic contexts .,"[('in', (8, 9)), ('under', (14, 15))]","[('inclusion hypothesis', (1, 3)), ('most effective', (6, 8)), ('discriminating', (9, 10)), ('hypernyms and meronyms', (11, 14)), ('syntactic contexts', (15, 17))]",[],[],[],[]
results,"For instance , on EVALution , SLQS performs worse ( ranked only as high as 13th ) , as this dataset has no such restriction on the basic level concepts , and may contain pairs like ( eye , animal ) .","[('on', (3, 4)), ('performs', (7, 8)), ('ranked only as', (10, 13))]","[('EVALution', (4, 5)), ('SLQS', (6, 7)), ('worse', (8, 9)), ('13th', (15, 16))]",[],[],[],[]
results,Hypernym vs. Attribute,[],[],[],[],[],[]
results,"Hypernym vs. Synonym SLQS performs well also in discriminating between hypernyms and synonyms , in which y is also not more general than x .","[('performs', (4, 5)), ('in discriminating between', (7, 10))]","[('Hypernym vs. Synonym SLQS', (0, 4)), ('well', (5, 6)), ('hypernyms and synonyms', (10, 13))]",[],[],[],[]
results,"We observed that in the joint context type , the difference in SLQS scores between synonyms and hypernyms was the largest .","[('observed', (1, 2)), ('in', (3, 4)), ('in', (11, 12)), ('between', (14, 15)), ('was', (18, 19))]","[('joint context type', (5, 8)), ('difference', (10, 11)), ('SLQS scores', (12, 14)), ('synonyms and hypernyms', (15, 18)), ('largest', (20, 21))]",[],[],[],[]
results,Hypernym vs. Coordination,[],[],[],[],[],[]
results,"On Weeds , inclusion - based measures ( ClarkeDE , invCL and Weeds ) showed the best results .","[('On', (0, 1)), ('showed', (14, 15))]","[('Weeds', (1, 2)), ('inclusion - based measures', (3, 7)), ('ClarkeDE', (8, 9)), ('invCL', (10, 11)), ('Weeds', (12, 13)), ('best results', (16, 18))]",[],[],[],[]
results,"The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .","[('of', (4, 5)), ('is', (8, 9)), ('achieved using', (19, 21)), ('with', (24, 25))]","[('over all performance', (1, 4)), ('embeddingbased classifiers', (6, 8)), ('almost perfect', (9, 11)), ('best performance', (16, 18)), ('concatenation method', (22, 24)), ('GloVe', (26, 27)), ('dependency - based embeddings', (29, 33))]",[],[],[],[]
results,"As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .","[('perform', (6, 7)), ('than', (8, 9))]","[('unsupervised measures', (4, 6)), ('worse', (7, 8)), ('embedding - based classifiers', (10, 14))]",[],[],[],[]
research-problem,Supervised Distributional Hypernym Discovery via Domain Adaptation,[],"[('Supervised Distributional Hypernym Discovery', (0, 4))]",[],[],[],[]
research-problem,"In this paper , we propose a supervised distributional framework for hypernym discovery which operates at the sense level , enabling large - scale automatic acquisition of dis ambiguated taxonomies .","[('propose', (5, 6)), ('for', (10, 11)), ('operates at', (14, 16)), ('enabling', (20, 21)), ('of', (26, 27))]","[('supervised distributional framework', (7, 10)), ('hypernym discovery', (11, 13)), ('large - scale automatic acquisition', (21, 26)), ('dis ambiguated taxonomies', (27, 30))]",[],[],[],[]
research-problem,"By exploiting semantic regularities between hyponyms and hypernyms in embeddings spaces , and integrating a domain clustering algorithm , our model becomes sensitive to the target data .","[('exploiting', (1, 2)), ('between', (4, 5)), ('in', (8, 9)), ('integrating', (13, 14)), ('becomes', (21, 22))]","[('semantic regularities', (2, 4)), ('hyponyms and hypernyms', (5, 8)), ('embeddings spaces', (9, 11)), ('domain clustering algorithm', (15, 18)), ('our model', (19, 21)), ('sensitive', (22, 23)), ('target data', (25, 27))]",[],[],[],[]
research-problem,"By embedding cues about how we perceive concepts , and how these concepts generalize in a domain of knowledge , these resources bear a capacity for generalization that lies at the core of human cognition and have become key in Natural Language Processing ( NLP ) tasks where inference and reasoning have proved to be essential .","[('perceive', (6, 7))]",[],[],[],[],[]
model,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .","[('propose', (4, 5)), ('based on', (12, 14)), ('easily applied to', (20, 23)), ('of', (25, 26))]","[('TAXOEMBED', (5, 6)), ('hypernym detection algorithm', (9, 12)), ('sense embeddings', (14, 16)), ('construction', (24, 25)), ('lexical taxonomies', (26, 28))]",[],[],[],[]
model,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .","[('designed to discover', (2, 5)), ('by exploiting', (7, 9)), ('in', (11, 12)), ('leverages', (20, 21)), ('to learn', (23, 25)), ('for', (30, 31))]","[('hypernymic relations', (5, 7)), ('linear transformations', (9, 11)), ('embedding spaces', (12, 14)), ('specific semanticallyaware transformation matrix', (26, 30)), ('each domain of knowledge', (31, 35))]",[],[],[],[]
research-problem,"Compared to word - level taxonomy learning , TAXO - EMBED results in more refined and unambiguous hypernymic relations at the sense level , with a direct application in tasks such as semantic search .","[('Compared to', (0, 2)), ('results in', (11, 13)), ('at', (19, 20))]","[('word - level taxonomy learning', (2, 7)), ('TAXO - EMBED', (8, 11)), ('more refined and unambiguous hypernymic relations', (13, 19)), ('sense level', (21, 23))]",[],[],[],[]
baselines,"KB - UNIFY 6 ( Delli Bovi et al. , 2015 a ) ( KB - U ) is a knowledge - based approach , based on BabelNet , for integrating the output of different OIE systems into a single unified and dis ambiguated knowledge repository .","[('based on', (25, 27)), ('for integrating', (29, 31)), ('of', (33, 34)), ('into', (37, 38))]","[('KB - UNIFY', (0, 3)), ('BabelNet', (27, 28)), ('different OIE systems', (34, 37)), ('single unified and dis ambiguated knowledge repository', (39, 46))]",[],[],[],[]
baselines,"The unification algorithm takes as input a set K of OIE - derived resources , each of which is modeled as a set of entity , relation , entity triples , and comprises two subsequent stages : in the first dis ambiguation stage , each KB in K is linked to the sense inventory of Babel Net by dis ambiguating its relation argument pairs ; in the following alignment stage , equivalent relations across different KB in K are merged together .","[('takes as input', (3, 6)), ('modeled as', (19, 21)), ('linked to', (49, 51)), ('of', (54, 55)), ('by dis', (57, 59))]","[('unification algorithm', (1, 3)), ('set K of OIE - derived resources', (7, 14)), ('set', (22, 23)), ('each', (44, 45)), ('sense inventory', (52, 54)), ('Babel Net', (55, 57)), ('relation argument pairs', (61, 64))]",[],[],[],[]
results,"As expected , Yago and WiBi achieve the best over all results .","[('achieve', (6, 7))]","[('Yago and WiBi', (3, 6)), ('best over all results', (8, 12))]",[],[],[],[]
results,Experiment 2 : Extra-Coverage,[],[],[],[],[],[]
research-problem,"This paper proposes a simple but effective method for the discovery of hypernym sets based on word embedding , which can be used to measure the contextual similarities between words .","[('based on', (14, 16)), ('to measure', (23, 25)), ('between', (28, 29))]","[('discovery of', (10, 12)), ('hypernym sets', (12, 14)), ('word embedding', (16, 18)), ('contextual similarities', (26, 28)), ('words', (29, 30))]",[],[],[],[]
research-problem,"In the SemEval 2018 Task 9 , the task has shifted to "" Hypernym Discovery "" , i.e. , given the search space of a domain 's vocabulary and an input hyponym , discover its best ( set of ) candidate hypernyms .",[],"[('Hypernym Discovery', (13, 15))]",[],[],[],[]
hyperparameters,Word2vec is used to produce the word embeddings .,"[('to produce', (3, 5))]","[('Word2vec', (0, 1)), ('word embeddings', (6, 8))]",[],[],[],[]
hyperparameters,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,"[('used with', (11, 13)), ('set to', (16, 18))]","[('skip - gram model ( - cbow 0 )', (1, 10)), ('embedding dimension', (14, 16)), ('300', (18, 19))]",[],[],[],[]
results,Results Based on Projection Learning,[],[],[],[],[],[]
results,"By using the same evaluating metrics as PRF in the cited paper , our best F - value on the validation set is 0.68 ( the paper result is 0.73 ) when the best cluster number is 2 and the threshold is ( 17.7 , 17.3 ) .","[('using', (1, 2)), ('as', (6, 7)), ('on', (18, 19)), ('is', (22, 23)), ('when', (31, 32)), ('is', (36, 37)), ('is', (41, 42))]","[('our best F - value', (13, 18)), ('validation set', (20, 22)), ('0.68', (23, 24)), ('best cluster number', (33, 36)), ('2', (37, 38)), ('threshold', (40, 41)), ('( 17.7 , 17.3 )', (42, 47))]",[],[],[],[]
research-problem,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery,[],[],[],[],[],[]
research-problem,The goal of the hypernym discovery task at Sem - Eval 2018 is to predict the hypernyms of a query given a large vocabulary of candidate hypernyms .,[],"[('hypernym discovery', (4, 6))]",[],[],[],[]
model,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .","[('for', (7, 8))]","[('task of hypernym discovery exploits', (9, 14)), ('two approaches', (17, 19)), ('unsupervised , pattern - based approach', (21, 27)), ('supervised , projection learning approach', (29, 34))]",[],[],[],[]
research-problem,Pattern - Based Hypernym Discovery,[],[],[],[],[],[]
research-problem,Learning Projections for Hypernym Discovery,[],[],[],[],[],[]
results,Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,"[('ranked', (4, 5)), ('on', (6, 7))]","[('Our hybrid system', (0, 3)), ('1st', (5, 6)), ('all three sub - tasks', (7, 12)), ('runs', (16, 17))]",[],[],[],[]
results,"If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .","[('of', (7, 8)), ('see that', (13, 15)), ('improved', (17, 18)), ('on', (21, 22)), ('increased them', (27, 29)), ('by', (29, 30)), ('on', (32, 33))]","[('runs', (3, 4)), ('data augmentation', (15, 17)), ('our scores', (18, 20)), ('slightly', (20, 21)), ('1A and 2B', (22, 25)), ('several points', (30, 32)), ('2A', (33, 34))]",[],[],[],[]
results,"Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .","[('are', (3, 4)), ('computed using', (9, 11))]","[('Our cross-evaluation results', (0, 3)), ('better', (4, 5)), ('supervised baseline', (7, 9)), ('normal evaluation setup', (12, 15))]",[],[],[],[]
results,"Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task , and even outperformed the supervised baseline on 2A .",[],"[('unsupervised system', (3, 5)), ('outperformed', (5, 6)), ('all other unsupervised systems', (6, 10)), ('outperformed', (17, 18)), ('supervised baseline', (19, 21))]",[],[],[],[]
results,"Combining the outputs of the 2 systems improves the best score of either system on all test sets , sometimes by as much as 10 points .","[('Combining', (0, 1)), ('of', (3, 4)), ('improves', (7, 8)), ('of', (11, 12)), ('on', (14, 15)), ('sometimes', (19, 20)), ('by', (20, 21))]","[('outputs', (2, 3)), ('2 systems', (5, 7)), ('best score', (9, 11)), ('either system', (12, 14)), ('all test sets', (15, 18)), ('as much as 10 points', (21, 26))]",[],[],[],[]
results,"Notice also that the results obtained using only the supervised system indicate that data augmentation had a positive effect on our 2A scores only ( compare runs 1 and 2 ) , although our tests on the trial set suggested it would also have a positive effect on our 1A scores .","[('using', (6, 7)), ('indicate', (11, 12)), ('had', (15, 16)), ('on', (19, 20))]","[('only the supervised system', (7, 11)), ('data augmentation', (13, 15)), ('positive effect', (17, 19)), ('our 2A scores only', (20, 24))]",[],[],[],[]
results,"Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .","[('is', (12, 13)), ('on', (15, 16)), ('when', (20, 21))]","[('run 1', (10, 12)), ('best', (14, 15)), ('all 3 test sets', (16, 20)), ('hybrid system', (24, 26))]",[],[],[],[]
ablation-analysis,No subsampling : we sample positive examples uniformly from the training set .,"[('sample', (4, 5)), ('from', (8, 9))]","[('No subsampling', (0, 2)), ('positive examples', (5, 7)), ('uniformly', (7, 8)), ('training set', (10, 12))]",[],[],[],[]
baselines,"No MTL : instead of multi - task learning ( MTL ) , we use a single classifier for both named entities and concepts .","[('use', (14, 15)), ('for', (18, 19))]","[('No', (0, 1)), ('MTL', (1, 2)), ('single classifier', (16, 18)), ('named entities and concepts', (20, 24))]",[],[],[],[]
ablation-analysis,Frozen embeddings : the word embeddings are not fine - tuned during training .,"[('not fine - tuned', (7, 11)), ('during', (11, 12))]","[('Frozen embeddings', (0, 2)), ('word', (4, 5)), ('training', (12, 13))]",[],[],[],[]
ablation-analysis,"It is worth noting that our supervised model outperforms the supervised baseline provided for this task ( see ) even when it exploits a single projection matrix , however the difference in scores between these 2 systems is only 2 or 3 points , depending on the evaluation metric .","[('worth noting', (2, 4))]","[('our supervised model', (5, 8)), ('outperforms', (8, 9)), ('supervised baseline', (10, 12)), ('scores', (32, 33))]",[],[],[],[]
research-problem,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery,[],[],[],[],[],[]
research-problem,"In this paper , we present our proposed system ( EXPR ) to participate in the hypernym discovery task of SemEval 2018 .",[],"[('hypernym discovery task', (16, 19))]",[],[],[],[]
research-problem,The task addresses the challenge of discovering hypernym relations from a text corpus .,[],"[('discovering hypernym relations from a text corpus', (6, 13))]",[],[],[],[]
research-problem,Hypernym detection focuses on deciding whether a hypernymic relation holds between a given pair of terms or not .,[],"[('Hypernym detection', (0, 2))]",[],[],[],[]
research-problem,Hypernym discovery focuses on discovering a set containing the best hypernyms for a given term from a given vocabulary search space .,[],"[('Hypernym discovery', (0, 2))]",[],[],[],[]
research-problem,The task is divided into two subtasks : General - Purpose Hypernym Discovery and Domain - Specific Hypernym Discovery .,"[('divided into', (3, 5))]","[('two', (5, 6)), ('General - Purpose Hypernym Discovery', (8, 13)), ('Domain - Specific Hypernym Discovery', (14, 19))]",[],[],[],[]
dataset,"The second consists of discovering hypernym in a domain - specific corpus , thus they provide the participants with data for two specific domains : Medical and Music .","[('in', (6, 7)), ('provide', (15, 16)), ('for', (20, 21))]","[('discovering', (4, 5)), ('hypernym', (5, 6)), ('domain - specific corpus', (8, 12)), ('participants', (17, 18)), ('data', (19, 20)), ('two specific domains', (21, 24)), ('Medical and Music', (25, 28))]",[],[],[],[]
model,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .","[('propose', (6, 7)), ('combines', (10, 11)), ('via concatenating', (19, 21)), ('constructed using', (28, 30)), ('obtained using', (37, 39))]","[('path - based technique and distributional technique', (12, 19)), ('two feature vectors', (21, 24)), ('feature vector', (26, 28)), ('dependency parser output', (30, 33)), ('feature vector', (35, 37)), ('term embeddings', (39, 41))]",[],[],[],[]
model,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .","[('create', (8, 9)), ('based on', (14, 16))]","[('concatenated vector', (5, 7)), ('binary supervised classifier model', (10, 14)), ('support vector machine ( SVM ) algorithm', (16, 23))]",[],[],[],[]
results,"For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .","[('For', (0, 1)), ('performs', (7, 8)), ('than', (9, 10)), ('performs', (15, 16)), ('than', (17, 18)), ('on', (21, 22))]","[('three corpora', (2, 4)), ('our system', (5, 7)), ('better', (8, 9)), ('STJU system', (10, 12)), ('better', (16, 17)), ('MFH system', (19, 21)), ('English corpora', (23, 25))]",[],[],[],[]
results,"As shown in the table 2 , the candidate hypernym extraction ( CHE ) coverage for English testing terms is 950 ( 63 % ) , that means our system is unable to extract any candidate hypernym for 550 ( 37 % ) terms ( 398 entities and 152 concepts ) .","[('for', (15, 16)), ('is', (19, 20)), ('unable', (31, 32)), ('for', (37, 38))]","[('candidate hypernym extraction ( CHE ) coverage', (8, 15)), ('English testing terms', (16, 19)), ('950 ( 63 % )', (20, 25)), ('our', (28, 29)), ('550 ( 37 % ) terms', (38, 44))]",[],[],[],[]
model,utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other .,"[('utilizes', (0, 1)), ('for', (4, 5)), ('by training', (7, 9)), ('for', (12, 13)), ('such that', (16, 18)), ('correspond to', (20, 22))]","[('non-negative sparse coding', (1, 4)), ('word translation', (5, 7)), ('sparse word vectors', (9, 12)), ('two languages', (14, 16)), ('coding bases', (18, 20)), ('each other', (22, 24))]",[],[],[],[]
model,Here we apply sparse feature pairs to hypernym extraction .,"[('apply', (2, 3)), ('to', (6, 7))]","[('sparse feature pairs', (3, 6)), ('hypernym extraction', (7, 9))]",[],[],[],[]
research-problem,The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis ( FCA ) is relatively new .,[],"[('Formal concept Analysis ( FCA )', (14, 20))]",[],[],[],[]
experiments,Formal concept analysis,[],[],[],[],[],[]
tasks,Generating more negative samples also provides some additional performance boost .,"[('provides', (5, 6))]","[('Generating', (0, 1)), ('more negative samples', (1, 4)), ('some additional performance boost', (6, 10))]",[],[],[],[]
research-problem,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies,[],"[('Detecting Hypernymy Relations', (6, 9))]",[],[],[],[]
research-problem,"This paper presents the participation of Apollo 's team in the SemEval - 2018 Task 9 "" Hypernym Discovery "" , Subtask 1 : "" General - Purpose Hypernym Discovery "" , which tries to produce a ranked list of hypernyms for a specific term .",[],"[('Hypernym Discovery', (17, 19))]",[],[],[],[]
research-problem,We propose a novel approach for automatic extraction of hypernymy relations from a corpus by using dependency patterns .,[],"[('automatic extraction of hypernymy relations from a corpus', (6, 14))]",[],[],[],[]
research-problem,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,"[('presents', (2, 3)), ('participated in', (12, 14)), ('based on', (19, 21))]","[('hypernym discovery', (9, 11)), ('unsupervised machine learning', (21, 24))]",[],[],[],[]
model,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,"[('is', (1, 2)), ('exploits', (8, 9)), ('that generalize', (12, 14))]","[('rule - based system', (3, 7)), ('syntactic dependency paths', (9, 12)), ('Hearst - style lexical patterns', (14, 19))]",[],[],[],[]
research-problem,"It is well known that in natural language processing ( NLP ) , one of the biggest challenges is to understand the meaning of words .",[],[],[],[],[],[]
research-problem,A new Approach to Detect Hypernymy Relation,[],[],[],[],[],[]
research-problem,Neural Models for Reasoning over Multiple Mentions using Coreference,[],"[('Reasoning over Multiple Mentions', (3, 7))]",[],[],[],[]
research-problem,One important form of reasoning for Question Answering ( QA ) models is the ability to aggregate information from multiple mentions of entities .,[],"[('Question Answering ( QA )', (6, 11))]",[],[],[],[]
model,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .","[('call', (1, 2)), ('tied together', (25, 27)), ('with', (27, 28)), ('of', (30, 31)), ('which denote', (33, 35))]","[('coreference - based reasoning', (3, 7)), ('help', (29, 30)), ('referring expressions', (31, 33)), ('same real - world entity', (36, 41))]",[],[],[],[]
model,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .","[('extracted from', (9, 11)), ('introduce', (16, 17)), ('in', (19, 20)), ('for', (23, 24)), ('depends on', (31, 33)), ('of', (36, 37)), ('of', (40, 41))]","[('term', (18, 19)), ('update equations', (21, 23)), ('Gated Recurrent Units ( GRU )', (24, 30)), ('hidden state', (34, 36)), ('coreferent antecedent', (38, 40)), ('current token', (42, 44))]",[],[],[],[]
experiments,In each case we see clear improvements of using C - GRU layers over GRU layers .,"[('see', (4, 5)), ('of using', (7, 9)), ('over', (13, 14))]","[('clear improvements', (5, 7)), ('C - GRU layers', (9, 13)), ('GRU layers', (14, 16))]",[],[],[],[]
experiments,"The Bi - C - GRU model significantly improves on this baseline , which shows that , with less data , coreference annotations can provide a useful bias for a memory network on how to read and write memories .",[],"[('Bi - C - GRU model', (1, 7)), ('significantly improves', (7, 9))]",[],[],[],[]
experiments,"A break - down of task - wise performance is given in Appendix C. Comparing C - GRU to the GRU based method , we find that the main gains are on tasks 2 ( two supporting facts ) , 3 ( three supporting facts ) and 16 ( basic induction ) .","[('of', (4, 5)), ('Comparing', (14, 15)), ('to', (18, 19)), ('find', (25, 26))]","[('task - wise performance', (5, 9)), ('C - GRU', (15, 18)), ('GRU based method', (20, 23)), ('main gains', (28, 30))]",[],[],[],[]
experiments,"Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .","[('Comparing to', (0, 2)), ('found that', (7, 9)), ('was', (12, 13)), ('on', (15, 16))]","[('QRN baseline', (3, 5)), ('C - GRU', (9, 12)), ('significantly worse', (13, 15)), ('task', (16, 17))]",[],[],[],[]
experiments,"On the other hand , C - GRU was significantly better than QRN on task 16 ( basic induction ) .","[('was', (8, 9)), ('than', (11, 12))]","[('C - GRU', (5, 8)), ('significantly better', (9, 11)), ('QRN', (12, 13))]",[],[],[],[]
model,We also include a baseline which uses coreference features as 1 - hot vectors appended to the input word vectors ( GA w/ GRU + 1 - hot ) .,"[('uses', (6, 7)), ('as', (9, 10)), ('appended to', (14, 16))]","[('coreference features', (7, 9)), ('1 - hot vectors', (10, 14)), ('input word vectors ( GA w/ GRU + 1 - hot )', (17, 29))]",[],[],[],[]
experiments,"In both cases there is a sharp drop in performance , showing that specifically using coreference for connecting mentions is important .","[('in', (8, 9))]","[('sharp drop', (6, 8)), ('performance', (9, 10))]",[],[],[],[]
research-problem,"We see higher performance for the C - GRU model in the low data regime , and better generalization throughout the training curve for all three settings .","[('see', (1, 2)), ('for', (4, 5)), ('in', (10, 11)), ('throughout', (19, 20))]","[('higher performance', (2, 4)), ('C - GRU model', (6, 10)), ('low data regime', (12, 15)), ('better generalization', (17, 19)), ('training curve', (21, 23))]",[],[],[],[]
experiments,"Lastly , we note that both models vastly outperform the best reported result of BiDAf from 1 . We believe this is because the GA models select answers from the list of candidatees , whereas BiDAF ignores those candidates .","[('note', (3, 4)), ('of', (13, 14)), ('from', (15, 16))]","[('both models', (5, 7)), ('vastly outperform', (7, 9)), ('best reported result', (10, 13)), ('BiDAf', (14, 15))]",[],[],[],[]
experiments,We see a significant gain in performance when using the layer with coreference bias .,"[('see', (1, 2)), ('in', (5, 6)), ('when using', (7, 9)), ('with', (11, 12))]","[('significant gain', (3, 5)), ('performance', (6, 7)), ('layer', (10, 11)), ('coreference bias', (12, 14))]",[],[],[],[]
experiments,"Furthermore , the 1 - hot baseline which uses the same coreference information , but with sequential recency bias fails to improve over the regular GRU layer .","[('uses', (8, 9)), ('with', (15, 16)), ('fails to', (19, 21)), ('over', (22, 23))]","[('1 - hot baseline', (3, 7)), ('same coreference information', (10, 13)), ('sequential recency bias', (16, 19)), ('improve', (21, 22)), ('regular GRU layer', (24, 27))]",[],[],[],[]
hyperparameters,The maximum number of coreference clusters across all tasks was C = 13 .,"[('across', (6, 7)), ('was', (9, 10))]","[('maximum number of coreference clusters', (1, 6)), ('all tasks', (7, 9)), ('C = 13', (10, 13))]",[],[],[],[]
hyperparameters,"We used dropout of 0.2 in between the intermediate layers , and initialized word embeddings with Glove .","[('used', (1, 2)), ('of', (3, 4)), ('in between', (5, 7)), ('with', (15, 16))]","[('dropout', (2, 3)), ('0.2', (4, 5)), ('intermediate layers', (8, 10)), ('initialized', (12, 13)), ('word embeddings', (13, 15)), ('Glove', (16, 17))]",[],[],[],[]
research-problem,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension,[],[],[],[],[],[]
research-problem,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .,[],"[('Reading Comprehension ( RC )', (12, 17))]",[],[],[],[]
research-problem,Building Artificial Intelligence ( AI ) algorithms to teach machines to read and to comprehend text is a long - standing challenge in Natural Language Processing ( NLP ) .,[],"[('Building Artificial Intelligence ( AI ) algorithms to', (0, 8))]",[],[],[],[]
model,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .","[('develop', (6, 7)), ('for', (17, 18)), ('can', (22, 23)), ('skip through', (23, 25)), ('of', (27, 28)), ('generate', (31, 32)), ('using', (34, 35))]","[('novel context zoom - in network ( ConZNet )', (8, 17)), ('RC tasks', (18, 20)), ('irrelevant parts', (25, 27)), ('document', (29, 30)), ('answer', (33, 34)), ('only the relevant regions of text', (35, 41))]",[],[],[],[]
model,The ConZNet architecture consists of two phases .,"[('consists of', (3, 5))]","[('ConZNet architecture', (1, 3)), ('two phases', (5, 7))]",[],[],[],[]
model,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,"[('identify', (5, 6)), ('by employing', (11, 13))]","[('relevant regions of text', (7, 11)), ('reinforcement learning algorithm', (14, 17))]",[],[],[],[]
model,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .","[('based on', (4, 6)), ('comprehends', (13, 14)), ('generates', (20, 21)), ('by using', (23, 25)), ('as', (31, 32)), ('along with', (38, 40)), ('as', (43, 44))]","[('encoder - decoder architecture', (7, 11)), ('identified regions of text', (15, 19)), ('answer', (22, 23)), ('residual self - attention network', (26, 31)), ('encoder', (32, 33)), ('RNNbased sequence generator', (35, 38)), ('pointer network', (41, 43)), ('decoder', (45, 46))]",[],[],[],[]
model,"Moreover , our decoder combines span prediction and sequence generation .","[('combines', (4, 5))]","[('our decoder', (2, 4)), ('span prediction and sequence generation', (5, 10))]",[],[],[],[]
baselines,In both baselines we replace the span prediction layer with an answer generation layer .,"[('replace', (4, 5)), ('with', (9, 10))]","[('span prediction layer', (6, 9)), ('answer generation layer', (11, 14))]",[],[],[],[]
baselines,In Baseline 1 we use an 1 please refer for more details attention based seq2seq layer without using copy mechanism in the answer generation unit similar to .,"[('use', (4, 5)), ('refer for', (8, 10)), ('without using', (16, 18)), ('in', (20, 21))]","[('1', (6, 7)), ('more details attention based seq2seq layer', (10, 16)), ('copy mechanism', (18, 20)), ('answer generation unit', (22, 25))]",[],[],[],[]
experimental-setup,We split each document into sentences using the sentence tokenizer of the NLTK toolkit .,"[('split', (1, 2)), ('into', (4, 5)), ('using', (6, 7)), ('of', (10, 11))]","[('each document', (2, 4)), ('sentences', (5, 6)), ('sentence tokenizer', (8, 10)), ('NLTK toolkit', (12, 14))]",[],[],[],[]
experimental-setup,"Similarly , we further tokenize each sentence , corresponding question and answer using the word tokenizer of NLTK .","[('further', (3, 4)), ('using', (12, 13)), ('of', (16, 17))]","[('tokenize', (4, 5)), ('each sentence', (5, 7)), ('word tokenizer', (14, 16)), ('NLTK', (17, 18))]",[],[],[],[]
experimental-setup,The model is implemented using Python and Tensorflow .,"[('implemented using', (3, 5))]","[('Python and Tensorflow', (5, 8))]",[],[],[],[]
experimental-setup,All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,"[('of', (3, 4)), ('initialized by', (7, 9)), ('initialized with', (14, 16))]","[('weights', (2, 3)), ('model', (5, 6)), ('Glorot Initialization', (9, 11)), ('biases', (12, 13)), ('zeros', (16, 17))]",[],[],[],[]
experimental-setup,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .","[('use', (1, 2)), ('from', (7, 8)), ('with', (10, 11)), ('to initialize', (16, 18))]","[('300 dimensional word vectors', (3, 7)), ('GloVe', (8, 9)), ('840 billion pre-trained vectors', (11, 15)), ('word embeddings', (19, 21)), ('training', (27, 28))]",[],[],[],[]
experimental-setup,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,"[('do not appear in', (4, 8)), ('from', (13, 14))]","[('Glove', (8, 9)), ('initialized', (10, 11)), ('sampling', (12, 13)), ('uniform random distribution', (15, 18))]",[],[],[],[]
experimental-setup,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,"[('apply', (1, 2)), ('between', (3, 4)), ('with', (6, 7)), ('of', (9, 10))]","[('dropout', (2, 3)), ('layers', (5, 6)), ('keep probability', (7, 9)), ('0.8', (10, 11))]",[],[],[],[]
experimental-setup,The number of hidden units are set to 100 .,"[('set to', (6, 8))]","[('number of hidden units', (1, 5)), ('100', (8, 9))]",[],[],[],[]
experiments,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .","[('trained', (1, 2)), ('with', (4, 5)), ('for', (13, 14))]","[('our model', (2, 4)), ('AdaDelta ( Zeiler , 2012 ) optimizer', (6, 13)), ('50 epochs', (14, 16)), ('initial learning rate', (18, 21)), ('0.1', (22, 23)), ('minibatch size', (26, 28)), ('32', (29, 30))]",[],[],[],[]
experimental-setup,The hyperparameter ' sample size ' ( number of relevant sentences ) is chosen based on the model performance on the devset .,"[('chosen', (13, 14)), ('based on', (14, 16)), ('on', (19, 20))]","[('hyperparameter', (1, 2)), (""' sample size ' ( number of relevant sentences )"", (2, 12)), ('model performance', (17, 19)), ('devset', (21, 22))]",[],[],[],[]
results,shows the performance of various models on Narrative QA .,"[('on', (6, 7))]","[('Narrative QA', (7, 9))]",[],[],[],[]
results,It can be noted that our model with sample size 5 ( choosing 5 relevant sentences ) outperforms the best ROUGE - L score available so far by 12.62 % compared to .,"[('noted', (3, 4)), ('with', (7, 8)), ('by', (27, 28))]","[('our model', (5, 7)), ('outperforms', (17, 18)), ('best ROUGE - L score', (19, 24)), ('12.62 %', (28, 30))]",[],[],[],[]
results,"The low performance of Baseline 1 shows that the hybrid approach ( ConZNet ) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models ( Seq2Seq , ASR , BiDAF , MRU ) .","[('of', (3, 4)), ('shows', (6, 7)), ('for', (14, 15)), ('from', (17, 18)), ('as well as copying', (21, 25)), ('from', (26, 27)), ('than', (32, 33))]","[('low performance', (1, 3)), ('hybrid approach ( ConZNet )', (9, 14)), ('generating words', (15, 17)), ('fixed vocabulary', (19, 21)), ('document', (28, 29)), ('better suited', (30, 32)), ('span prediction models ( Seq2Seq , ASR , BiDAF , MRU )', (33, 45))]",[],[],[],[]
research-problem,A Simple and Effective Approach to the Story Cloze Test,[],[],[],[],[],[]
research-problem,"Following this approach , we present a simpler fully - neural approach to the Story Cloze Test using skip - thought embeddings of the stories in a feed - forward network that achieves close to state - of - the - art performance on this task without any feature engineering .","[('present', (5, 6)), ('to', (12, 13)), ('using', (17, 18)), ('of', (22, 23)), ('in', (25, 26)), ('achieves close to', (32, 35)), ('without', (46, 47))]","[('simpler fully - neural approach', (7, 12)), ('Story Cloze Test', (14, 17)), ('skip - thought embeddings', (18, 22)), ('stories', (24, 25)), ('feed - forward network', (27, 31)), ('state - of - the - art performance', (35, 43)), ('feature engineering', (48, 50))]",[],[],[],[]
research-problem,We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach .,"[('of', (9, 10)), ('instead of', (12, 14)), ('yields', (17, 18)), ('with', (20, 21))]","[('considering', (4, 5)), ('just the last sentence', (5, 9)), ('prompt', (11, 12)), ('whole prompt', (15, 17)), ('higher accuracy', (18, 20)), ('our approach', (21, 23))]",[],[],[],[]
experiments,We use two layer and three layer fully connected networks with Rectified Linear ( ReLU ) non-linearities ( refer to Appendix A for model - specific architecture ) .,"[('use', (1, 2)), ('with', (10, 11))]","[('two layer and three layer fully connected networks', (2, 10)), ('Rectified Linear ( ReLU ) non-linearities', (11, 17))]",[],[],[],[]
baselines,"Full Context ( FC ) Here , we use a Gated Recurrent Unit ( GRU ) to encode the entire story prompt into a 4800 - dimensional vector , add it to the skipthought embedding of the story ending , and pass it as input to the neural network .","[('use', (8, 9)), ('to encode', (16, 18)), ('into', (22, 23)), ('add it to', (29, 32)), ('of', (35, 36)), ('pass it as', (41, 44)), ('to', (45, 46))]","[('Full Context ( FC )', (0, 5)), ('Gated Recurrent Unit ( GRU )', (10, 16)), ('entire story prompt', (19, 22)), ('4800 - dimensional vector', (24, 28)), ('skipthought embedding', (33, 35)), ('story ending', (37, 39)), ('input', (44, 45)), ('neural network', (47, 49))]",[],[],[],[]
hyperparameters,We use cross-entropy loss and SGD with learning rate of 0.01 .,"[('use', (1, 2)), ('with', (6, 7)), ('of', (9, 10))]","[('cross-entropy loss', (2, 4)), ('SGD', (5, 6)), ('learning rate', (7, 9)), ('0.01', (10, 11))]",[],[],[],[]
results,The 3 - layer feed - forward neural network trained on the validation set by summing the skip - thought embeddings of the last sentence ( LS ) of the story prompt and the ending gives the best accuracy ( 76.5 % ) .,"[('trained on', (9, 11)), ('by summing', (14, 16)), ('of', (21, 22)), ('of', (28, 29)), ('gives', (35, 36))]","[('3 - layer feed - forward neural network', (1, 9)), ('validation set', (12, 14)), ('skip - thought embeddings', (17, 21)), ('last sentence ( LS )', (23, 28)), ('story prompt', (30, 32)), ('ending', (34, 35)), ('best accuracy', (37, 39))]",[],[],[],[]
ablation-analysis,"This approach is far simpler than previous approaches in the literature ; it requires no feature engineering , nor intricate neural network architecture , and achieves close to state - of - the - art accuracy .","[('achieves', (25, 26))]","[('close to state - of - the - art accuracy', (26, 36))]",[],[],[],[]
results,"We note that the model trained using only the last sentence ( LS ) of the story context has higher accuracy compared to the model that uses a GRU to encode the full context ( FC ) , and even the model which encodes the entire context .","[('note', (1, 2)), ('trained using', (5, 7)), ('of', (14, 15)), ('compared to', (21, 23)), ('uses', (26, 27)), ('to encode', (29, 31))]","[('model', (4, 5)), ('only the last sentence ( LS )', (7, 14)), ('story context', (16, 18)), ('higher accuracy', (19, 21)), ('model', (24, 25)), ('GRU', (28, 29)), ('full context ( FC )', (32, 37)), ('entire context', (45, 47))]",[],[],[],[]
research-problem,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING,[],[],[],[],[],[]
research-problem,Several deep learning models have been proposed for question answering .,[],"[('question answering', (8, 10))]",[],[],[],[]
research-problem,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,[],"[('Question answering ( QA )', (0, 5))]",[],[],[],[]
model,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .","[('introduce', (1, 2)), ('for', (21, 22))]","[('Dynamic Coattention Network ( DCN )', (3, 9)), ('end - to - end neural network', (14, 21)), ('question answering', (22, 24))]",[],[],[],[]
model,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .","[('consists of', (2, 4)), ('captures', (8, 9)), ('between', (11, 12)), ('as', (18, 19)), ('well as', (19, 21)), ('alternates', (26, 27)), ('between', (27, 28))]","[('coattentive encoder', (5, 7)), ('interactions', (10, 11)), ('question and the document', (13, 17)), ('dynamic pointing decoder', (22, 25)), ('estimating', (28, 29)), ('start and end of', (30, 34)), ('answer span', (35, 37))]",[],[],[],[]
experimental-setup,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .","[('preprocess', (1, 2)), ('use', (6, 7)), ('from', (9, 10))]","[('corpus', (3, 4)), ('tokenizer', (8, 9)), ('Stanford CoreNLP', (10, 12))]",[],[],[],[]
experimental-setup,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,"[('use', (1, 2)), ('pretrained on', (7, 9))]","[('Glo Ve word vectors', (3, 7)), ('840B Common Crawl corpus', (10, 14))]",[],[],[],[]
experimental-setup,We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,"[('limit', (1, 2)), ('to', (4, 5)), ('present in', (8, 10)), ('set', (15, 16)), ('for', (17, 18)), ('to', (24, 25))]","[('vocabulary', (3, 4)), ('words', (5, 6)), ('Common Crawl corpus', (11, 14)), ('embeddings', (16, 17)), ('out - of - vocabulary words', (18, 24)), ('zero', (25, 26))]",[],[],[],[]
experiments,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .","[('use', (1, 2)), ('of', (6, 7)), ('during', (8, 9)), ('for', (17, 18))]","[('max sequence length', (3, 6)), ('600', (7, 8)), ('training', (9, 10)), ('hidden state size', (12, 15)), ('200', (16, 17)), ('all recurrent units', (18, 21)), ('maxout layers', (22, 24)), ('linear layers', (26, 28))]",[],[],[],[]
experimental-setup,All LSTMs have randomly initialized parameters and an initial state of zero .,"[('have', (2, 3))]","[('randomly initialized parameters', (3, 6)), ('initial state of zero', (8, 12))]",[],[],[],[]
experimental-setup,Sentinel vectors are randomly initialized and optimized during training .,"[('are', (2, 3)), ('during', (7, 8))]","[('Sentinel vectors', (0, 2)), ('randomly initialized and optimized', (3, 7)), ('training', (8, 9))]",[],[],[],[]
experimental-setup,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .","[('For', (0, 1)), ('set', (6, 7)), ('to', (12, 13)), ('use', (15, 16)), ('of', (20, 21))]","[('dynamic decoder', (2, 4)), ('maximum number of iterations', (8, 12)), ('4', (13, 14)), ('maxout pool size', (17, 20)), ('16', (21, 22))]",[],[],[],[]
experimental-setup,"We use dropout to regularize our network during training , and optimize the model using ADAM .","[('use', (1, 2)), ('to regularize', (3, 5)), ('during', (7, 8)), ('optimize', (11, 12)), ('using', (14, 15))]","[('dropout', (2, 3)), ('our network', (5, 7)), ('training', (8, 9)), ('model', (13, 14)), ('ADAM', (15, 16))]",[],[],[],[]
experimental-setup,All models are implemented and trained with Chainer .,"[('implemented and trained with', (3, 7))]","[('Chainer', (7, 8))]",[],[],[],[]
results,"The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .","[('of', (2, 3)), ('compared to', (12, 14)), ('to estimate', (30, 32)), ('of', (37, 38)), ('conditioned on', (46, 48))]","[('performance', (1, 2)), ('Dynamic Coattention Network', (4, 7)), ('DCN', (26, 27)), ('capability', (29, 30)), ('start and end points', (33, 37)), ('answer span', (39, 41)), ('multiple times', (41, 43)), ('previous estimates', (49, 51))]",[],[],[],[]
ablation-analysis,"On the decoder side , we experiment with various pool sizes for the HMN maxout layers , using a 2 - layer MLP instead of a HMN , and forcing the HMN decoder to a single iteration .","[('On', (0, 1)), ('experiment with', (6, 8)), ('for', (11, 12)), ('using', (17, 18)), ('instead of', (23, 25)), ('forcing', (29, 30)), ('to', (33, 34))]","[('decoder side', (2, 4)), ('various pool sizes', (8, 11)), ('HMN maxout layers', (13, 16)), ('2 - layer MLP', (19, 23)), ('HMN', (26, 27)), ('HMN decoder', (31, 33)), ('single iteration', (35, 37))]",[],[],[],[]
ablation-analysis,"Empirically , we achieve the best performance on the development set with an iterative HMN with pool size 16 , and find that the model consistently benefits from a deeper , iterative decoder network .","[('achieve', (3, 4)), ('on', (7, 8)), ('with', (11, 12)), ('with', (15, 16)), ('find', (21, 22)), ('consistently benefits from', (25, 28))]","[('best performance', (5, 7)), ('development set', (9, 11)), ('iterative HMN', (13, 15)), ('pool size 16', (16, 19)), ('model', (24, 25)), ('deeper , iterative decoder network', (29, 34))]",[],[],[],[]
ablation-analysis,"The performance improves as the number of maximum allowed iterations increases , with little improvement after 4 iterations .","[('as', (3, 4)), ('after', (15, 16))]","[('performance', (1, 2)), ('improves', (2, 3)), ('number of maximum allowed iterations', (5, 10)), ('increases', (10, 11)), ('little improvement', (13, 15))]",[],[],[],[]
ablation-analysis,"On the encoder side , replacing the coattention mechanism with an attention mechanism similar to Wang & Jiang ( 2016 b ) by setting CD to QA D in equation 3 results in a 1.9 point F1 drop .","[('On', (0, 1)), ('replacing', (5, 6)), ('with', (9, 10)), ('setting', (23, 24)), ('to', (25, 26)), ('results in', (31, 33))]","[('encoder side', (2, 4)), ('coattention mechanism', (7, 9)), ('attention mechanism', (11, 13)), ('CD', (24, 25)), ('QA D', (26, 28)), ('1.9 point F1 drop', (34, 38))]",[],[],[],[]
ablation-analysis,Performance across length,[],[],[],[],[],[]
ablation-analysis,"However , as in shown in , there is no notable performance degradation for longer documents and questions contrary to our expectations .","[('there', (7, 8)), ('for', (13, 14))]","[('notable performance degradation', (10, 13)), ('longer documents and questions', (14, 18))]",[],[],[],[]
ablation-analysis,"Namely , it becomes increasingly challenging to compute the correct word span as the number of words increases .","[('becomes', (3, 4)), ('to compute', (6, 8)), ('as', (12, 13))]","[('increasingly challenging', (4, 6)), ('correct word span', (9, 12)), ('number of words', (14, 17)), ('increases', (17, 18))]",[],[],[],[]
ablation-analysis,"In , we note that the mean F1 of DCN exceeds those of previous systems across all question types .","[('note', (3, 4)), ('of', (8, 9)), ('exceeds', (10, 11)), ('across', (15, 16))]","[('mean F1', (6, 8)), ('DCN', (9, 10)), ('previous systems', (13, 15)), ('all question types', (16, 19))]",[],[],[],[]
research-problem,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING,[],"[('FINDING', (0, 1))]",[],[],[],[]
research-problem,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .","[('find out', (26, 28))]","[('text - based question and answering', (3, 9)), ('relational reasoning', (12, 14)), ('question relevant information', (29, 32))]",[],[],[],[]
model,The external memory enables the model to deal with a knowledge base without loss of information .,"[('enables', (3, 4)), ('to deal with', (6, 9)), ('without loss of', (12, 15))]","[('external memory', (1, 3)), ('model', (5, 6)), ('knowledge base', (10, 12)), ('information', (15, 16))]",[],[],[],[]
model,Generalization updates old memories given the new input and output feature map finds relevant information from the memory .,[],"[('Generalization updates old memories given the new', (0, 7))]",[],[],[],[]
model,"Based on the memory network architecture , neural network based models like end - to - end memory network ( Mem N2N ) , gated end - to - end memory network ( GMe m N2N ) , dynamic memory network ( DMN ) , and dynamic memory network + ( DMN + ) are proposed .","[('Based on', (0, 2)), ('like', (11, 12))]","[('memory network architecture', (3, 6)), ('neural network based models', (7, 11)), ('end - to - end memory network ( Mem N2N )', (12, 23)), ('gated end - to - end memory network ( GMe m N2N )', (24, 37)), ('dynamic memory network ( DMN )', (38, 44)), ('dynamic memory network + ( DMN + )', (46, 54))]",[],[],[],[]
model,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .","[('able to find', (14, 17)), ('when', (20, 21)), ('is', (25, 26))]","[('"" Relation Memory Network "" ( RMN )', (4, 12)), ('complex relation', (17, 19)), ('lot of information', (22, 25))]",[],[],[],[]
model,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,"[('uses', (1, 2)), ('to find out', (3, 6)), ('with', (8, 9))]","[('MLP', (2, 3)), ('relevant information', (6, 8)), ('new generalization', (10, 12)), ('information', (16, 17))]",[],[],[],[]
model,"Relation Memory Network ( RMN ) is composed of four components - embedding , attention , updating , and reasoning .","[('composed of', (7, 9))]","[('Relation Memory Network ( RMN )', (0, 6)), ('four components', (9, 11)), ('embedding', (12, 13)), ('attention', (14, 15)), ('updating', (16, 17)), ('reasoning', (19, 20))]",[],[],[],[]
model,EMBEDDING COMPONENT,[],[],[],[],[],[]
model,"To constitute the attention component , we applied simple MLP represented as gt ? .","[('To constitute', (0, 2)), ('applied', (7, 8)), ('represented as', (10, 12))]","[('attention component', (3, 5)), ('simple MLP', (8, 10)), ('gt ?', (12, 14))]",[],[],[],[]
model,"1 ) to control the intensity of attention , inspired by the way Neural Turing Machine reads from the memory .","[('to control', (2, 4)), ('inspired by', (9, 11)), ('reads from', (16, 18))]","[('intensity of attention', (5, 8)), ('Neural Turing Machine', (13, 16)), ('memory', (19, 20))]",[],[],[],[]
model,"To forget the information already used , we use intuitive updating component to renew the memory .","[('To forget', (0, 2)), ('use', (8, 9)), ('to renew', (12, 14))]","[('information', (3, 4)), ('intuitive updating component', (9, 12)), ('memory', (15, 16))]",[],[],[],[]
experiments,REASONING COMPONENT,[],[],[],[],[],[]
model,"MemN2N first calculates the relatedness of sentences in the question and memory by taking the inner product , and the sentence with the highest relatedness is selected as the first supporting sentence for the given question .","[('in', (7, 8)), ('by taking', (12, 14)), ('with', (21, 22)), ('selected as', (26, 28)), ('for', (32, 33))]","[('MemN2N', (0, 1)), ('relatedness of sentences', (4, 7)), ('question and memory', (9, 12)), ('inner product', (15, 17)), ('sentence', (20, 21)), ('highest relatedness', (23, 25)), ('first supporting sentence', (29, 32)), ('given question', (34, 36))]",[],[],[],[]
research-problem,bAbI story - based QA dataset,[],[],[],[],[],[]
model,"For regularization , we use batch normalization for all MLPs .","[('For', (0, 1)), ('use', (4, 5)), ('for', (7, 8))]","[('regularization', (1, 2)), ('batch normalization', (5, 7)), ('all MLPs', (8, 10))]",[],[],[],[]
model,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,"[('optimized with', (4, 6)), ('using', (12, 13)), ('with', (16, 17)), ('of', (20, 21))]","[('softmax output', (1, 3)), ('cross - entropy loss function', (7, 12)), ('Adam optimizer', (14, 16)), ('learning rate', (18, 20)), ('2 e ?4', (21, 24))]",[],[],[],[]
model,"While trained jointly , RMN learns these different solutions for each task .","[('trained', (1, 2)), ('learns', (5, 6)), ('for', (9, 10))]","[('RMN', (4, 5)), ('different solutions', (7, 9)), ('each task', (10, 12))]",[],[],[],[]
experiments,"For the task 3 , the only failed task , attention component still functions well ; it focuses sequentially on the supporting sentences .","[('focuses', (17, 18))]","[('attention component', (10, 12)), ('well', (14, 15)), ('sequentially', (18, 19)), ('supporting sentences', (21, 23))]",[],[],[],[]
experiments,"With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .","[('With', (0, 1)), ('except for', (16, 18)), ('compared to', (20, 22))]","[('match type feature', (2, 5)), ('all models other', (6, 9)), ('RMN', (10, 11)), ('significantly improved', (12, 14)), ('performance', (15, 16)), ('plain condition', (23, 25))]",[],[],[],[]
experiments,"Different from other tasks , RMN yields the same error rate 25.1 % with MemN2N and GMe m N2N on the task 3 .","[('yields', (6, 7)), ('with', (13, 14))]","[('RMN', (5, 6)), ('same error rate 25.1 %', (8, 13)), ('MemN2N and GMe m N2N', (14, 19))]",[],[],[],[]
experiments,"Overall , the number of hops is correlated with the number of supporting sentences .","[('correlated with', (7, 9))]","[('number of hops', (3, 6)), ('number of supporting sentences', (10, 14))]",[],[],[],[]
research-problem,Natural Language Comprehension with the EpiReader,[],"[('Natural Language Comprehension', (0, 3))]",[],[],[],[]
research-problem,"We present the EpiReader , a novel model for machine comprehension of text .",[],"[('machine comprehension of text', (9, 13))]",[],[],[],[]
research-problem,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",[],"[('Machine comprehension', (0, 2))]",[],[],[],[]
model,"In this paper , we argue that the same principle can be applied to machine comprehension of natural language .","[('applied to', (12, 14)), ('of', (16, 17))]","[('same', (8, 9)), ('machine comprehension', (14, 16)), ('natural language', (17, 19))]",[],[],[],[]
model,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .","[('propose', (1, 2)), ('call', (16, 17))]","[('deep , end - to - end , neural comprehension model', (3, 14)), ('EpiReader', (18, 19))]",[],[],[],[]
research-problem,Machine comprehension ( MC ) has therefore garnered significant attention from the machine learning research community .,[],"[('Machine comprehension ( MC )', (0, 5))]",[],[],[],[]
model,The EpiReader factors into two components .,"[('factors', (2, 3)), ('into', (3, 4))]","[('EpiReader', (1, 2)), ('two components', (4, 6))]",[],[],[],[]
model,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .","[('combine', (5, 6)), ('with', (10, 11)), ('to produce', (16, 18)), ('of', (21, 22))]","[(""Reasoner 's evidence"", (7, 10)), (""Extractor 's probability estimates"", (12, 16)), ('final ranking', (19, 21)), ('answer candidates', (23, 25))]",[],[],[],[]
experiments,Children 's Book Test,[],[],[],[],[],[]
model,The Extractor is a Pointer Network .,"[('is', (2, 3))]","[('Extractor', (1, 2)), ('Pointer Network', (4, 6))]",[],[],[],[]
model,"At each step the biGRU outputs two d-dimensional encoding vectors , one for the forward direction and one for the backward direction .","[('outputs', (5, 6)), ('one for', (11, 13)), ('one for', (17, 19))]","[('biGRU', (4, 5)), ('two d-dimensional encoding vectors', (6, 10)), ('forward direction', (14, 16)), ('backward direction', (20, 22))]",[],[],[],[]
experimental-setup,"To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","[('used', (5, 6)), ('with', (9, 10)), ('with', (21, 22)), ('of', (26, 27))]","[('our model', (2, 4)), ('stochastic gradient descent', (6, 9)), ('ADAM optimizer', (11, 13)), ('initial learning rate', (23, 26)), ('0.001', (27, 28))]",[],[],[],[]
experimental-setup,"The word embeddings were initialized randomly , drawing from the uniform distribution over .","[('drawing from', (7, 9))]","[('word embeddings', (1, 3)), ('initialized randomly', (4, 6)), ('uniform distribution', (10, 12))]",[],[],[],[]
experiments,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .","[('used', (1, 2)), ('of', (3, 4)), ('with', (10, 11)), ('of', (13, 14))]","[('batches', (2, 3)), ('32 examples', (4, 6)), ('early stopping', (8, 10)), ('patience', (12, 13)), ('2 epochs', (14, 16))]",[],[],[],[]
experimental-setup,Our model was implement in Theano using the Keras framework .,"[('implement in', (3, 5)), ('using', (6, 7))]","[('Theano', (5, 6)), ('Keras framework', (8, 10))]",[],[],[],[]
experimental-setup,"All our models used 2 - regularization at 0.001 , ? = 50 , and ? = 0.04 .","[('used', (3, 4)), ('at', (7, 8))]","[('2 - regularization', (4, 7)), ('0.001', (8, 9))]",[],[],[],[]
results,The EpiReader achieves state - of - the - art performance across the board for both datasets .,"[('achieves', (2, 3)), ('across', (11, 12))]","[('EpiReader', (1, 2)), ('state - of - the - art performance', (3, 11)), ('board', (13, 14)), ('both datasets', (15, 17))]",[],[],[],[]
results,"On CNN , we score 2.2 % higher on test than the best previous model of .","[('score', (4, 5)), ('on', (8, 9)), ('than', (10, 11))]","[('CNN', (1, 2)), ('2.2 % higher', (5, 8)), ('test', (9, 10)), ('best previous model', (12, 15))]",[],[],[],[]
results,The improvement on CBT - NE is more modest at 1.1 % .,"[('on', (2, 3)), ('is', (6, 7)), ('at', (9, 10))]","[('improvement', (1, 2)), ('CBT - NE', (3, 6)), ('more modest', (7, 9)), ('1.1 %', (10, 12))]",[],[],[],[]
hyperparameters,"For each mini-batch update , the 2 norm of the whole gradient of all parameters is measured 5 and if larger than L = 50 , then it is scaled down to have norm L.","[('For', (0, 1)), ('of', (8, 9)), ('of', (12, 13)), ('measured', (16, 17)), ('scaled down', (29, 31)), ('to have', (31, 33))]","[('each mini-batch update', (1, 4)), ('2 norm', (6, 8)), ('whole gradient', (10, 12)), ('all parameters', (13, 15)), ('5', (17, 18))]",[],[],[],[]
hyperparameters,"We use the learning rate annealing schedule from , namely , if the validation cost has not decreased after one epoch , then the learning rate is scaled down by a factor 1.5 .","[('use', (1, 2)), ('after', (18, 19)), ('scaled down by', (27, 30))]","[('learning rate', (3, 5)), ('validation', (13, 14)), ('not decreased', (16, 18)), ('one epoch', (19, 21)), ('learning rate', (24, 26)), ('factor 1.5', (31, 33))]",[],[],[],[]
hyperparameters,"Training terminates when the learning rate drops below 10 ? 5 , i.e. after 50 epochs or so .","[('terminates when', (1, 3))]","[('Training', (0, 1)), ('learning rate', (4, 6)), ('drops', (6, 7)), ('below', (7, 8)), ('10 ? 5', (8, 11))]",[],[],[],[]
hyperparameters,"Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .","[('initialized using', (2, 4)), ('set to', (14, 16))]","[('Weights', (0, 1)), ('N ( 0 , 0.05 )', (4, 10)), ('batch size', (11, 13)), ('128', (16, 17))]",[],[],[],[]
hyperparameters,"On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .","[('On', (0, 1)), ('repeat', (7, 8)), ('with', (12, 13)), ('pick', (17, 18))]","[('Penn tree dataset', (2, 5)), ('each training', (8, 10)), ('10 times', (10, 12)), ('different random initializations', (13, 16)), ('smallest validation cost', (21, 24))]",[],[],[],[]
hyperparameters,Note that the baseline architectures were tuned in to give optimal perplexity 6 .,"[('Note', (0, 1)), ('tuned in to give', (6, 10))]","[('baseline architectures', (3, 5)), ('optimal perplexity', (10, 12))]",[],[],[],[]
experiments,"We also vary the number of hops and memory size of our MemN2N , showing the contribution of both to performance ; note in particular that increasing the number of hops helps .","[('vary', (2, 3)), ('of', (10, 11))]","[('number of hops and memory size', (4, 10)), ('our MemN2N', (11, 13))]",[],[],[],[]
experiments,"In , we show how Mem N2N operates on memory with multiple hops .","[('show', (3, 4)), ('operates', (7, 8)), ('with', (10, 11))]","[('Mem N2N', (5, 7)), ('on memory', (8, 10)), ('multiple hops', (11, 13))]",[],[],[],[]
baselines,"MemNN : The strongly supervised AM + NG + NL Memory Networks approach , proposed in .",[],"[('MemNN', (0, 1)), ('strongly supervised AM + NG + NL Memory Networks approach', (3, 13))]",[],[],[],[]
baselines,It uses a max operation ( rather than softmax ) at each layer which is trained directly with supporting facts ( strong supervision ) .,"[('uses', (1, 2)), ('at', (10, 11)), ('trained directly with', (15, 18))]","[('max operation ( rather than softmax )', (3, 10)), ('each layer', (11, 13)), ('supporting facts', (18, 20)), ('strong supervision', (21, 23))]",[],[],[],[]
baselines,"It employs n-gram modeling , nonlinear layers and an adaptive number of hops per query .","[('employs', (1, 2)), ('per', (13, 14))]","[('n-gram modeling', (2, 4)), ('nonlinear layers', (5, 7)), ('adaptive number of hops', (9, 13)), ('query', (14, 15))]",[],[],[],[]
baselines,"LSTM : A standard LSTM model , trained using question / answer pairs only ( i.e. also weakly supervised ) .","[('trained using', (7, 9))]","[('LSTM', (0, 1)), ('question / answer pairs only', (9, 14)), ('weakly supervised', (17, 19))]",[],[],[],[]
experiments,Language Modeling Experiments,[],[],[],[],[],[]
experiments,"To aid training , we apply ReLU operations to half of the units in each layer .","[('To aid', (0, 2)), ('apply', (5, 6)), ('to', (8, 9)), ('in', (13, 14))]","[('training', (2, 3)), ('ReLU operations', (6, 8)), ('half of the units', (9, 13)), ('each layer', (14, 16))]",[],[],[],[]
experiments,"We use layer - wise ( RNN - like ) weight sharing , i.e. the query weights of each layer are the same ; the output weights of each layer are the same .","[('use', (1, 2)), ('of', (17, 18)), ('are', (20, 21)), ('of', (27, 28)), ('are', (30, 31))]","[('layer - wise ( RNN - like ) weight sharing', (2, 12)), ('query weights', (15, 17)), ('each layer', (18, 20)), ('output weights', (25, 27)), ('each layer', (28, 30))]",[],[],[],[]
research-problem,Neural Natural Language Inference Models Enhanced with External Knowledge,[],"[('Neural Natural Language Inference Models', (0, 5))]",[],[],[],[]
research-problem,Modeling natural language inference is a very challenging task .,[],"[('Modeling natural language inference', (0, 4))]",[],[],[],[]
research-problem,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",[],"[('Natural language inference ( NLI )', (0, 6)), ('recognizing textual entailment ( RTE )', (10, 16))]",[],[],[],[]
model,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .","[('enrich', (4, 5)), ('with', (12, 13)), ('in', (15, 16))]","[('neural - network - based NLI models', (5, 12)), ('external knowledge', (13, 15)), ('coattention', (16, 17)), ('local inference collection', (18, 21)), ('inference composition components', (23, 26))]",[],[],[],[]
experiments,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .","[('of using', (2, 4)), ('is', (6, 7)), ('when', (9, 10)), ('is', (15, 16))]","[('The', (0, 1)), ('advantage', (1, 2)), ('external knowledge', (4, 6)), ('more significant', (7, 9)), ('size of training data', (11, 15)), ('restricted', (16, 17))]",[],[],[],[]
hyperparameters,The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,"[('of', (10, 11)), ('of', (14, 15)), ('are', (19, 20))]","[('dimension', (9, 10)), ('hidden states', (12, 14)), ('LSTMs', (15, 16)), ('word embeddings', (17, 19)), ('300', (20, 21))]",[],[],[],[]
hyperparameters,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .","[('initialized by', (4, 6)), ('among', (17, 18))]","[('word embeddings', (1, 3)), ('300D GloVe 840B', (6, 9)), ('out - of - vocabulary words', (11, 17)), ('initialized', (20, 21)), ('randomly', (21, 22))]",[],[],[],[]
hyperparameters,All word embeddings are updated during training .,"[('updated during', (4, 6))]","[('word embeddings', (1, 3)), ('training', (6, 7))]",[],[],[],[]
hyperparameters,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .","[('used for', (9, 11)), ('with', (12, 13)), ('of', (17, 18))]","[('Adam ( Kingma and Ba , 2014 )', (0, 8)), ('optimization', (11, 12)), ('initial learning rate', (14, 17)), ('0.0004', (18, 19))]",[],[],[],[]
hyperparameters,The mini - batch size is set to 32 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('32', (8, 9))]",[],[],[],[]
code,ESIM is a strong NLI baseline framework with the source code made available at https://github.com/lukecq1231/nli,"[('is', (1, 2))]","[('ESIM', (0, 1))]",[],[],[],[]
results,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .","[('namely', (4, 5)), ('enriches', (15, 16)), ('with', (17, 18)), ('obtains', (21, 22)), ('of', (24, 25)), ('reported on', (34, 36))]","[('Knowledge - based Inference Model ( KIM )', (5, 13)), ('ESIM', (16, 17)), ('external knowledge', (18, 20)), ('accuracy', (23, 24)), ('88.6 %', (25, 27)), ('best single - model performance', (29, 34)), ('SNLI dataset', (37, 39))]",[],[],[],[]
hyperparameters,"In addition to that , we also use 15 semantic relation features , which does not bring additional gains in performance .","[('use', (7, 8)), ('does not', (14, 16))]","[('15 semantic relation features', (8, 12)), ('performance', (20, 21))]",[],[],[],[]
experiments,"To further investigate external knowledge , we add TransE relation embedding , and again no further improvement is observed on both the development and test sets when TransE relation embedding is used ( concatenated ) with the semantic relation vectors .","[('add', (7, 8)), ('observed on', (18, 20)), ('used', (31, 32))]","[('external knowledge', (3, 5)), ('TransE relation embedding', (8, 11)), ('TransE relation embedding', (27, 30)), ('semantic relation vectors', (37, 40))]",[],[],[],[]
results,"The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .","[('achieves', (3, 4)), ('on', (9, 10))]","[('baseline ESIM', (1, 3)), ('76.8 % and 75.8 %', (4, 9)), ('in - domain and cross - domain test set', (10, 19))]",[],[],[],[]
results,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .","[('extend', (2, 3)), ('with', (5, 6)), ('achieve', (10, 11)), ('to', (13, 14))]","[('ESIM', (4, 5)), ('external knowledge', (6, 8)), ('significant gains', (11, 13)), ('77.2 % and 76.4 %', (14, 19))]",[],[],[],[]
results,"Especially under the condition of restricted training data ( 0.8 % ) , the model obtains a large gain when using more than half of external knowledge . :","[('under', (1, 2)), ('obtains', (15, 16)), ('when using', (19, 21))]","[('condition of restricted training data ( 0.8 % )', (3, 12)), ('model', (14, 15)), ('large gain', (17, 19)), ('more than half of external knowledge', (21, 27))]",[],[],[],[]
results,"Especially , for antonym category in cross - domain set , KIM outperform ESIM significantly (+ absolute 5.0 % ) as expected , because antonym feature captured by external knowledge would help unseen cross - domain samples .","[('for', (2, 3)), ('in', (5, 6))]","[('antonym category', (3, 5)), ('cross - domain set', (6, 10)), ('KIM', (11, 12)), ('outperform', (12, 13)), ('ESIM', (13, 14)), ('significantly', (14, 15)), ('(+ absolute 5.0 % )', (15, 20))]",[],[],[],[]
research-problem,Text Understanding with the Attention Sum Reader Network,[],"[('Text Understanding', (0, 2))]",[],[],[],[]
research-problem,Ensemble of our models sets new state of the art on all evaluated datasets .,"[('sets', (4, 5)), ('on', (10, 11))]","[('new state of the art', (5, 10)), ('all evaluated datasets', (11, 14))]",[],[],[],[]
research-problem,Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence .,[],"[('teaching machines', (4, 6))]",[],[],[],[]
experiments,Children 's Book Test,[],[],[],[],[],[]
model,We call this the contextual embedding .,"[('call', (1, 2))]","[('contextual embedding', (4, 6))]",[],[],[],[]
hyperparameters,To train the model we used stochastic gradient descent with the ADAM update rule and learning rate of 0.001 or 0.0005 .,"[('used', (5, 6)), ('with', (9, 10)), ('of', (17, 18))]","[('stochastic gradient descent', (6, 9)), ('ADAM update rule', (11, 14)), ('learning rate', (15, 17)), ('0.001 or 0.0005', (18, 21))]",[],[],[],[]
hyperparameters,"2 . The initial weights in the word embedding matrix were drawn randomly uniformly from the interval [ ? 0.1 , 0.1 ] .","[('in', (5, 6)), ('drawn randomly', (11, 13)), ('from', (14, 15))]","[('initial weights', (3, 5)), ('word embedding matrix', (7, 10)), ('interval [ ? 0.1 , 0.1 ]', (16, 23))]",[],[],[],[]
hyperparameters,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,"[('in', (1, 2)), ('initialized by', (6, 8)), ('initialized to', (14, 16))]","[('Weights', (0, 1)), ('GRU networks', (3, 5)), ('random orthogonal matrices', (8, 11)), ('biases', (12, 13)), ('zero', (16, 17))]",[],[],[],[]
hyperparameters,We also used a gradient clipping threshold of 10 and batches of size 32 .,"[('used', (2, 3)), ('of', (7, 8))]","[('gradient clipping threshold', (4, 7)), ('10', (8, 9)), ('batches', (10, 11)), ('32', (13, 14))]",[],[],[],[]
hyperparameters,During training we randomly shuffled all examples in each epoch .,"[('During', (0, 1)), ('randomly shuffled', (3, 5)), ('in', (7, 8))]","[('training', (1, 2)), ('all examples', (5, 7)), ('each epoch', (8, 10))]",[],[],[],[]
hyperparameters,For each batch of the CNN and Daily Mail datasets we randomly reshuffled the assignment of named entities to the corresponding word embedding vectors to match the procedure proposed in .,"[('For', (0, 1)), ('of', (3, 4)), ('randomly reshuffled', (11, 13)), ('of', (15, 16)), ('to', (18, 19)), ('to match', (24, 26))]","[('each batch', (1, 3)), ('CNN and Daily Mail datasets', (5, 10)), ('assignment', (14, 15)), ('named entities', (16, 18)), ('corresponding word embedding vectors', (20, 24))]",[],[],[],[]
results,Ensembles of our models set new state - of - the - art results on all evaluated datasets .,"[('set', (4, 5)), ('on', (14, 15))]","[('new state - of - the - art results', (5, 14)), ('all evaluated datasets', (15, 18))]",[],[],[],[]
results,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .",[],"[('outperforms', (6, 7))]",[],[],[],[]
results,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,"[('On', (0, 1)), ('with', (7, 8)), ('achieves', (11, 12)), ('of', (15, 16))]","[('CNN dataset', (2, 4)), ('our single model', (4, 7)), ('best validation accuracy', (8, 11)), ('test accuracy', (13, 15)), ('69.5 %', (16, 18))]",[],[],[],[]
results,The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,"[('of', (3, 4)), ('according to', (9, 11)), ('is', (13, 14)), ('is', (17, 18)), ('than', (22, 23))]","[('average performance', (1, 3)), ('top 20 % models', (5, 9)), ('validation accuracy', (11, 13)), ('69.9 %', (14, 16)), ('even 0.5 % better', (18, 22)), ('single best - validation model', (24, 29))]",[],[],[],[]
results,Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets ..,"[('Fusing', (0, 1)), ('gives', (4, 5)), ('in', (9, 10)), ('on', (11, 12))]","[('multiple models', (1, 3)), ('significant further increase', (6, 9)), ('accuracy', (10, 11))]",[],[],[],[]
results,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .","[('In', (0, 1)), ('with', (8, 9)), ('performs', (13, 14)), ('than', (18, 19)), ('with', (21, 22))]","[('named entity prediction', (1, 4)), ('our best single model', (4, 8)), ('accuracy of', (9, 11)), ('68.6 %', (11, 13)), ('2 % absolute better', (14, 18)), ('MemNN', (20, 21)), ('self supervision', (22, 24))]",[],[],[],[]
results,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,"[('In', (0, 1)), ('is', (7, 8)), ('than', (12, 13)), ('improves', (18, 19)), ('to', (21, 22))]","[('common noun prediction', (1, 4)), ('our single models', (4, 7)), ('0.4 % absolute better', (8, 12)), ('Mem NN', (13, 15)), ('performance', (20, 21)), ('69 %', (22, 24))]",[],[],[],[]
research-problem,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING,[],"[('NATURAL LANGUAGE', (11, 13))]",[],[],[],[]
research-problem,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .",[],"[('natural language understanding ( NLU )', (1, 7))]",[],[],[],[]
dataset,"( ii ) An online evaluation platform and leaderboard , based primarily on privately - held test data .","[('based primarily on', (10, 13))]","[('online evaluation platform and leaderboard', (4, 9)), ('privately - held test data', (13, 18))]",[],[],[],[]
model,"It is model - agnostic , allowing for any kind of representation or contextualization , including models that use no explicit vector or symbolic representations for sentences whatsoever .","[('is', (1, 2)), ('allowing for', (6, 8)), ('including', (15, 16)), ('for', (25, 26))]","[('model - agnostic', (2, 5)), ('any kind of', (8, 11)), ('representation', (11, 12)), ('models', (16, 17)), ('explicit', (20, 21)), ('sentences', (26, 27))]",[],[],[],[]
experiments,GLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite .,"[('diverges from', (2, 4)), ('in', (5, 6)), ('included in', (13, 15))]","[('GLUE', (0, 1)), ('SentEval', (4, 5)), ('selection of', (7, 9)), ('evaluation tasks', (9, 11)), ('suite', (16, 17))]",[],[],[],[]
results,"Interannotator agreement is high , with a Fleiss 's ? of 0.73 .","[('is', (2, 3)), ('with', (5, 6)), ('of', (10, 11))]","[('Interannotator agreement', (0, 2)), ('high', (3, 4)), (""Fleiss 's ?"", (7, 10)), ('0.73', (11, 12))]",[],[],[],[]
hyperparameters,We implement our models in the AllenNLP library .,"[('implement', (1, 2)), ('in', (4, 5))]","[('AllenNLP library', (6, 8))]",[],[],[],[]
code,Original code for the baselines is available at https://github.com/nyu-mll/GLUE-baselines and a newer version is available at https://github.com/jsalt18-sentence-repl/jiant.,[],"[('https://github.com/nyu-mll/GLUE-baselines', (8, 9))]",[],[],[],[]
hyperparameters,"We train our models with Adam ( Kingma & Ba , 2015 ) with initial learning rate 10 ? 4 and batch size 128 .","[('train', (1, 2)), ('with', (4, 5)), ('with', (13, 14))]","[('Adam ( Kingma & Ba , 2015 )', (5, 13)), ('initial learning rate', (14, 17)), ('10 ? 4', (17, 20)), ('batch size', (21, 23)), ('128', (23, 24))]",[],[],[],[]
research-problem,Parameter Re-Initialization through Cyclical Batch Size Schedules,[],"[('Parameter Re-Initialization', (0, 2))]",[],[],[],[]
model,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,"[('explores', (2, 3)), ('to', (10, 11)), ('of', (14, 15))]","[('weight initialization', (8, 10)), ('optimization dynamics', (12, 14)), ('specific learning task', (16, 19))]",[],[],[],[]
model,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .","[('incorporate', (6, 7)), ('for', (12, 13)), ('where', (24, 25)), ('use', (26, 27)), ('to control', (31, 33)), ('of', (39, 40))]","[('"" adaptive initialization ""', (8, 12)), ('neural network training', (13, 16)), ('cyclical batch size schedules', (27, 31)), ('noise ( or temperature )', (34, 39)), ('SGD', (40, 41))]",[],[],[],[]
model,"We explore different cyclical batch size ( CBS ) schedules for training neural networks inspired by Bayesian statistics , particularly adaptive MCMC methods .",[],"[('training neural networks', (11, 14))]",[],[],[],[]
model,We propose a simple but effective ensembling method that combines models saved during different cycles at no additional training cost .,"[('propose', (1, 2)), ('combines', (9, 10)), ('saved during', (11, 13))]","[('ensembling method', (6, 8)), ('models', (10, 11)), ('different cycles', (13, 15))]",[],[],[],[]
experiments,Language Results,[],[],[],[],[],[]
results,Language modeling is a challenging problem due to the complex and long - range interactions between distant words .,[],"[('Language modeling', (0, 2))]",[],[],[],[]
results,"As we can see , the best performing CBS schedules result in significant improvements in perplexity ( up to 7.91 ) over the baseline schedules and also offer reductions in the number of SGD training iterations ( up to 33 % ) .","[('result in', (10, 12)), ('in', (14, 15)), ('over', (21, 22)), ('offer', (27, 28)), ('in', (29, 30)), ('up to', (37, 39))]","[('best performing CBS schedules', (6, 10)), ('significant improvements', (12, 14)), ('perplexity ( up to 7.91 )', (15, 21)), ('baseline schedules', (23, 25)), ('reductions', (28, 29)), ('number of SGD training iterations', (31, 36)), ('33 %', (39, 41))]",[],[],[],[]
results,Notice that almost all CBS schedules outperform the baseline schedule .,"[('Notice', (0, 1))]","[('almost all CBS schedules', (2, 6)), ('outperform', (6, 7)), ('baseline schedule', (8, 10))]",[],[],[],[]
results,"We see that the CBS schedules match baseline performance , but the number of training iterations used in CBS schedules is up to 2 fewer .","[('see that', (1, 3)), ('match', (6, 7)), ('used in', (16, 18)), ('up to', (21, 23))]","[('CBS schedules', (4, 6)), ('baseline performance', (7, 9)), ('number of training iterations', (12, 16)), ('CBS schedules', (18, 20)), ('2 fewer', (23, 25))]",[],[],[],[]
results,We observe that CBS achieves similar performance to the baseline .,"[('observe', (1, 2)), ('achieves', (4, 5)), ('to', (7, 8))]","[('CBS', (3, 4)), ('similar performance', (5, 7)), ('baseline', (9, 10))]",[],[],[],[]
results,"With CBS - 15 , we see 90.71 % training accuracy and 56. 44 % testing accuracy , which is a larger improvement than that offered by CBS on convolutional models on Cifar - 10 .","[('With', (0, 1)), ('see', (6, 7))]","[('CBS - 15', (1, 4)), ('90.71 % training accuracy', (7, 11)), ('56. 44 % testing accuracy', (12, 17))]",[],[],[],[]
results,Combining CBS - 15 on C2 with this strategy improves accuracy to 94.82 % .,"[('Combining', (0, 1)), ('improves', (9, 10)), ('to', (11, 12))]","[('CBS - 15 on C2', (1, 6)), ('accuracy', (10, 11)), ('94.82 %', (12, 14))]",[],[],[],[]
results,Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % .,"[('Applying', (0, 1)), ('on', (3, 4)), ('trained with', (5, 7)), ('of', (16, 17)), ('compared to', (21, 23))]","[('snapshot ensembling', (1, 3)), ('C3', (4, 5)), ('CBS - 15 - 2 leads', (7, 13)), ('improved accuracy', (14, 16)), ('93. 56 %', (17, 20)), ('92.58 %', (23, 25))]",[],[],[],[]
results,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .","[('ensembling', (1, 2)), ('on', (3, 4)), ('with', (5, 6)), ('increases to', (15, 17)), ('from', (19, 20))]","[('ResNet50', (2, 3)), ('Imagenet', (4, 5)), ('snapshots', (6, 7)), ('last two cycles', (9, 12)), ('performance', (14, 15)), ('76.401 %', (17, 19)), ('75.336 %', (20, 22))]",[],[],[],[]
experiments,"During training , we crop the image to 224 224 . PTB ( language modeling ) .","[('During', (0, 1)), ('crop', (4, 5)), ('to', (7, 8))]","[('training', (1, 2)), ('image', (6, 7)), ('224 224', (8, 10))]",[],[],[],[]
experiments,"The total vocabulary size is 10 k , and all words outside the vocabulary are replaced by a placeholder token . WikiText 2 ( language modeling ) .","[('is', (4, 5)), ('outside', (11, 12)), ('replaced by', (15, 17))]","[('total vocabulary size', (1, 4)), ('10 k', (5, 7)), ('all words', (9, 11)), ('vocabulary', (13, 14)), ('placeholder token', (18, 20)), ('WikiText', (21, 22))]",[],[],[],[]
research-problem,Natural Language Inference by Tree - Based Convolution and Heuristic Matching,[],"[('Natural Language Inference', (0, 3))]",[],[],[],[]
research-problem,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .","[('propose', (5, 6)), ('to recognize', (9, 11)), ('between', (14, 15))]","[('TBCNNpair model', (7, 9)), ('entailment and contradiction', (11, 14)), ('two sentences', (15, 17))]",[],[],[],[]
research-problem,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .,[],"[('natural language inference ( NLI )', (18, 24))]",[],[],[],[]
model,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .","[('propose', (5, 6)), ('to recognize', (12, 14)), ('between', (17, 18))]","[('TBCNN - pair neural model', (7, 12)), ('entailment and contradiction', (14, 17)), ('two sentences', (18, 20))]",[],[],[],[]
hyperparameters,"All our neural layers , including embeddings , were set to 300 dimensions .","[('including', (5, 6)), ('set to', (9, 11))]","[('embeddings', (6, 7)), ('300 dimensions', (11, 13))]",[],[],[],[]
results,"The model is mostly robust when the dimension is large , e.g. , several hundred .","[('is', (2, 3)), ('when', (5, 6)), ('is', (8, 9))]","[('model', (1, 2)), ('mostly robust', (3, 5)), ('dimension', (7, 8)), ('large , e.g. , several hundred', (9, 15))]",[],[],[],[]
hyperparameters,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,"[('pretrained ourselves by', (3, 6)), ('on', (7, 8)), ('fined tuned during', (13, 16)), ('as', (17, 18))]","[('Word embeddings', (0, 2)), ('word2vec', (6, 7)), ('English Wikipedia corpus', (9, 12)), ('training', (16, 17)), ('apart', (18, 19)), ('model parameters', (20, 22))]",[],[],[],[]
hyperparameters,We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,"[('applied', (1, 2)), ('of', (4, 5)), ('chosen by', (11, 13)), ('with', (14, 15)), ('of', (17, 18))]","[('2 penalty', (2, 4)), ('310 ? 4', (5, 8)), ('dropout', (9, 10)), ('validation', (13, 14)), ('0.1', (18, 19))]",[],[],[],[]
hyperparameters,"Initial learning rate was set to 1 , and a power decay was applied .","[('set to', (4, 6)), ('applied', (13, 14))]","[('Initial learning rate', (0, 3)), ('1', (6, 7)), ('power decay', (10, 12))]",[],[],[],[]
hyperparameters,We used stochastic gradient descent with a batch size of 50 .,"[('used', (1, 2)), ('with', (5, 6)), ('of', (9, 10))]","[('stochastic gradient descent', (2, 5)), ('batch size', (7, 9)), ('50', (10, 11))]",[],[],[],[]
results,"As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of humanengineered features , long short term memory .","[('followed by', (9, 11)), ('without', (23, 24)), ('including', (27, 28)), ('using', (33, 34)), ('of', (36, 37))]","[('TBCNN sentence pair model', (4, 8)), ('simple concatenation alone', (11, 14)), ('outperforms', (15, 16)), ('existing sentence encoding - based approaches', (16, 22)), ('pretraining', (24, 25)), ('feature - rich method', (29, 33)), ('6 groups', (34, 36)), ('humanengineered features', (37, 39)), ('long short term memory', (40, 44))]",[],[],[],[]
experiments,Model Variant,[],[],[],[],[],[]
results,We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,"[('using', (7, 8)), ('is', (13, 14)), ('than', (16, 17))]","[('each heuristic', (3, 5)), ('element - wise product alone', (8, 13)), ('significantly worse', (14, 16)), ('concatenation or element - wise difference', (17, 23))]",[],[],[],[]
results,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .","[('Combining', (0, 1)), ('improves', (4, 5)), ('with', (13, 14)), ('yields', (22, 23)), ('of', (26, 27))]","[('different matching heuristics', (1, 4)), ('result', (6, 7)), ('TBCNN - pair model', (9, 13)), ('concatenation', (14, 15)), ('element - wise product', (16, 20)), ('highest performance', (24, 26)), ('82.1 %', (27, 29))]",[],[],[],[]
results,Further applying element - wise product improves the accuracy by another 0.5 % .,"[('applying', (1, 2)), ('improves', (6, 7)), ('by', (9, 10))]","[('element - wise product', (2, 6)), ('accuracy', (8, 9)), ('another 0.5 %', (10, 13))]",[],[],[],[]
results,"The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .","[('in - cluding', (15, 18)), ('with', (29, 30))]","[('full TBCNN - pair model', (1, 6)), ('outperforms', (6, 7)), ('all existing sentence encoding - based approaches', (7, 14)), ('1024d gated recurrent unit ( GRU ) - based RNN', (19, 29)), ('"" skip - thought "" pretraining', (30, 36))]",[],[],[],[]
research-problem,Stochastic Answer Networks for Natural Language Inference,[],[],[],[],[],[]
research-problem,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .",[],"[('natural language inference', (1, 4)), ('recognizing textual entailment ( RTE )', (9, 15))]",[],[],[],[]
research-problem,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .","[('on', (22, 23))]","[('Machine Reading Comprehension ( MRC )', (9, 15)), ('NLI', (23, 24))]",[],[],[],[]
experimental-setup,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .,"[('used to', (5, 7)), ('to implement', (15, 17))]","[('spaCy tool', (1, 3)), ('tokenize', (7, 8)), ('all the dataset', (8, 11)), ('our models', (17, 19))]",[],[],[],[]
experimental-setup,We fix word embedding with 300 - dimensional GloVe word vectors .,"[('fix', (1, 2)), ('with', (4, 5))]","[('word embedding', (2, 4)), ('300 - dimensional GloVe word vectors', (5, 11))]",[],[],[],[]
experimental-setup,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .","[('For', (0, 1)), ('use', (6, 7)), ('of', (9, 10)), ('with', (15, 16))]","[('character encoding', (2, 4)), ('concatenation', (8, 9)), ('multi-filter Convolutional Neural Nets', (11, 15)), ('windows 1 , 3 , 5', (16, 22)), ('hidden size', (24, 26)), ('50 , 100 , 150', (26, 31))]",[],[],[],[]
experimental-setup,So lexicon embeddings are d =600 - dimensions .,"[('are', (3, 4))]","[('lexicon embeddings', (1, 3)), ('d =600 - dimensions', (4, 8))]",[],[],[],[]
experimental-setup,The embedding for the out - of - vocabulary is zeroed .,"[('for', (2, 3)), ('is', (9, 10))]","[('embedding', (1, 2)), ('out - of - vocabulary', (4, 9)), ('zeroed', (10, 11))]",[],[],[],[]
experimental-setup,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .","[('of', (3, 4)), ('in', (5, 6)), ('set to', (15, 17)), ('of', (23, 24)), ('is', (26, 27))]","[('hidden size', (1, 3)), ('LSTM', (4, 5)), ('contextual encoding layer', (7, 10)), ('memory generation layer', (11, 14)), ('128', (17, 18)), ('input size', (21, 23)), ('output layer', (24, 26)), ('1024 ( 128 * 2 * 4 )', (27, 35))]",[],[],[],[]
experimental-setup,The projection size in the attention layer is set to 256 .,"[('in', (3, 4)), ('set to', (8, 10))]","[('projection size', (1, 3)), ('attention layer', (5, 7)), ('256', (10, 11))]",[],[],[],[]
experimental-setup,"To speedup training , we use weight normalization .","[('To speedup', (0, 2)), ('use', (5, 6))]","[('weight normalization', (6, 8))]",[],[],[],[]
experimental-setup,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .","[('is', (3, 4)), ('fixed through', (11, 13)), ('in', (15, 16))]","[('dropout rate', (1, 3)), ('0.2', (4, 5)), ('dropout mask', (8, 10)), ('time steps', (13, 15)), ('LSTM', (16, 17))]",[],[],[],[]
experimental-setup,The mini - batch size is set to 32 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('32', (8, 9))]",[],[],[],[]
experimental-setup,Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,"[('is', (2, 3)), ('initialized as', (9, 11)), ('decreased by', (13, 15)), ('after', (16, 17))]","[('Our optimizer', (0, 2)), ('Adamax', (3, 4)), ('learning rate', (6, 8)), ('0.002', (11, 12)), ('0.5', (15, 16)), ('each 10 epochs', (17, 20))]",[],[],[],[]
results,"Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .","[('in', (6, 7)), ('except on', (12, 14))]","[('Our model', (0, 2)), ('outperforms', (2, 3)), ('best system', (4, 6)), ('RepEval 2017', (7, 9)), ('"" Conditional "" and "" Tense Difference "" categories', (14, 23))]",[],[],[],[]
results,"We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .","[('find', (2, 3)), ('works', (5, 6)), ('on', (8, 9))]","[('SAN', (4, 5)), ('extremely well', (6, 8)), ('"" Active / Passive "" and "" Paraphrase "" categories', (9, 19))]",[],[],[],[]
results,"Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .","[('Comparing with', (0, 2)), ('of', (9, 10)), ('on', (23, 24))]","[(""Chen 's model"", (2, 5)), ('biggest improvement', (7, 9)), ('SAN', (10, 11)), ('Matched and Mismatched settings', (24, 28)), ('"" Antonym "" category', (33, 37))]",[],[],[],[]
research-problem,Neural Tree Indexers for Text Understanding,[],[],[],[],[],[]
research-problem,NTI constructs a full n-ary tree by processing the input text with its node function in a bottom - up fashion .,"[('constructs', (1, 2)), ('by processing', (6, 8)), ('with', (11, 12)), ('in', (15, 16))]","[('NTI', (0, 1)), ('full n-ary tree', (3, 6)), ('input text', (9, 11)), ('node function', (13, 15)), ('bottom - up fashion', (17, 21))]",[],[],[],[]
model,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .","[('introduce', (5, 6))]","[('Neural Tree Indexers ( NTI )', (6, 12))]",[],[],[],[]
model,"Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling .","[('for', (8, 9)), ('is', (10, 11)), ('to', (22, 23))]","[('tree structure', (6, 8)), ('NTI', (9, 10)), ('relaxed', (11, 12)), ('input', (20, 21))]",[],[],[],[]
model,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :","[('such as', (6, 8)), ('chosen', (10, 11)), ('forms', (15, 16)), ('taking advantage of', (22, 25)), ('of', (30, 31))]","[('sequential leaf node transformer', (2, 6)), ('LSTM', (8, 9)), ('NTI network', (13, 15)), ('sequence - tree hybrid model', (17, 22)), ('conditional and compositional powers', (26, 30)), ('sequential and recursive models', (31, 35))]",[],[],[],[]
model,( b ) NTI learns representations for the premise and hypothesis sentences and then attentively combines them for classification .,"[('learns', (4, 5)), ('for', (6, 7)), ('attentively', (14, 15)), ('combines them for', (15, 18))]","[('NTI', (3, 4)), ('representations', (5, 6)), ('premise and hypothesis sentences', (8, 12)), ('classification', (18, 19))]",[],[],[],[]
hyperparameters,We trained NTI using Adam with hyperparameters selected on development set .,"[('trained', (1, 2)), ('using', (3, 4))]","[('NTI', (2, 3)), ('Adam', (4, 5))]",[],[],[],[]
hyperparameters,The pre-trained 300 - D Glove 840B vectors were obtained for the word embeddings,"[('obtained for', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (1, 8))]",[],[],[],[]
hyperparameters,The word embeddings are fixed during training .,"[('fixed during', (4, 6))]","[('word embeddings', (1, 3)), ('training', (6, 7))]",[],[],[],[]
hyperparameters,The embeddings for out - ofvocabulary words were set to zero vector .,"[('for', (2, 3)), ('set to', (8, 10))]","[('embeddings', (1, 2)), ('out - ofvocabulary words', (3, 7)), ('zero vector', (10, 12))]",[],[],[],[]
hyperparameters,The size of hidden units of the NTI modules were set to 300 .,"[('of', (2, 3)), ('of', (5, 6)), ('set to', (10, 12))]","[('size', (1, 2)), ('hidden units', (3, 5)), ('NTI modules', (7, 9)), ('300', (12, 13))]",[],[],[],[]
hyperparameters,The models were regularized by using dropouts and an l 2 weight decay .,"[('regularized by', (3, 5))]","[('dropouts', (6, 7)), ('l 2 weight decay', (9, 13))]",[],[],[],[]
experiments,Natural Language Inference,[],[],[],[],[],[]
experiments,"For each model , we set the batch size to 32 .","[('set', (5, 6)), ('to', (9, 10))]","[('batch size', (7, 9)), ('32', (10, 11))]",[],[],[],[]
experiments,"The initial learning , the regularization strength and the number of epoch to be trained are varied for each model .","[('to', (12, 13))]","[('initial learning', (1, 3)), ('regularization strength', (5, 7)), ('number of epoch', (9, 12)), ('trained', (14, 15)), ('each model', (18, 20))]",[],[],[],[]
experiments,NTI - SLSTM : this model does not rely on f leaf transformer but uses the S - LSTM units for the non-leaf node function .,"[('uses', (14, 15)), ('for', (20, 21))]","[('NTI - SLSTM', (0, 3)), ('S - LSTM units', (16, 20)), ('non-leaf node function', (22, 25))]",[],[],[],[]
experiments,"We set the initial learning rate to 1e - 3 and l 2 regularizer strength to 3 e - 5 , and train the model for 90 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('1e - 3', (7, 10)), ('regularizer strength', (13, 15)), ('3 e - 5', (16, 20)), ('model', (24, 25)), ('90 epochs', (26, 28))]",[],[],[],[]
experiments,The neural net was regularized by 10 % input dropouts and the 20 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('10 % input dropouts', (6, 10)), ('20 % output dropouts', (12, 16))]",[],[],[],[]
experiments,NTI - SLSTM - LSTM : we use LSTM for the leaf node function f leaf .,"[('use', (7, 8)), ('for', (9, 10))]","[('NTI - SLSTM - LSTM', (0, 5)), ('leaf node function f leaf', (11, 16))]",[],[],[],[]
experiments,NTI - SLSTM node - by - node global attention :,[],"[('NTI - SLSTM node - by - node global attention', (0, 10))]",[],[],[],[]
experiments,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 40 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('3e - 4', (7, 10)), ('regularizer strength', (13, 15)), ('1 e - 5', (16, 20)), ('model', (24, 25)), ('40 epochs', (26, 28))]",[],[],[],[]
experiments,The neural net was regularized by 15 % input dropouts and the 15 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('15 % input dropouts', (6, 10)), ('15 % output dropouts', (12, 16))]",[],[],[],[]
experiments,NTI - SLSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,[],"[('NTI - SLSTM node - by - node tree attention', (0, 10))]",[],[],[],[]
experiments,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 10 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('3e - 4', (7, 10)), ('regularizer strength', (13, 15)), ('1 e - 5', (16, 20)), ('model', (24, 25)), ('10 epochs', (26, 28))]",[],[],[],[]
experiments,The neural net was regularized by 10 % input dropouts and the 15 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('10 % input dropouts', (6, 10)), ('15 % output dropouts', (12, 16))]",[],[],[],[]
experiments,NTI - SLSTM - LSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,[],"[('NTI - SLSTM - LSTM node - by - node tree attention', (0, 12))]",[],[],[],[]
experiments,Tree matching NTI - SLSTM - LSTM global attention : this model first constructs the premise and hypothesis trees simultaneously with the NTI - SLSTM - LSTM model and then computes their matching vector by using the global attention and an additional LSTM .,"[('first constructs', (12, 14)), ('with', (20, 21)), ('computes', (30, 31)), ('by using', (34, 36))]","[('Tree matching NTI - SLSTM - LSTM global attention', (0, 9)), ('premise and hypothesis', (15, 18)), ('NTI - SLSTM - LSTM model', (22, 28)), ('matching vector', (32, 34)), ('global attention and an additional LSTM', (37, 43))]",[],[],[],[]
experiments,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train the model for 20 epochs .","[('set', (1, 2)), ('to', (6, 7)), ('l 2', (11, 13)), ('to', (15, 16)), ('train', (22, 23)), ('for', (25, 26))]","[('initial learning rate', (3, 6)), ('3e - 4', (7, 10)), ('regularizer strength', (13, 15)), ('3 e - 5', (16, 20)), ('model', (24, 25)), ('20 epochs', (26, 28))]",[],[],[],[]
experiments,The neural net was regularized by 20 % input dropouts and the 20 % output dropouts .,"[('regularized by', (4, 6))]","[('neural net', (1, 3)), ('20 % input dropouts', (6, 10)), ('20 % output dropouts', (12, 16))]",[],[],[],[]
experiments,Tree matching NTI - SLSTM - LSTM tree attention : we replace the global attention with the tree attention .,"[('replace', (11, 12)), ('with', (15, 16))]","[('Tree matching NTI - SLSTM - LSTM tree attention', (0, 9)), ('global attention', (13, 15)), ('tree attention', (17, 19))]",[],[],[],[]
experiments,"Full tree matching NTI - SLSTM - LSTM global attention : this model produces two sets of the attention vectors , one by attending over the premise tree regarding each hypothesis tree node and another by attending over the hypothesis tree regarding each premise tree node .","[('attending', (23, 24)), ('attending over', (36, 38)), ('regarding', (41, 42))]","[('Full tree matching NTI - SLSTM - LSTM global attention', (0, 10)), ('each premise tree node', (42, 46))]",[],[],[],[]
experiments,Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model .,"[('is', (6, 7)), ('obtained with', (10, 12))]","[('best score', (1, 3)), ('87.3 % accuracy', (7, 10)), ('full tree matching NTI model', (13, 18))]",[],[],[],[]
experiments,Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % .,"[('show', (2, 3)), ('improved', (7, 8)), ('of', (10, 11)), ('by', (15, 16))]","[('NTI - SLSTM', (4, 7)), ('performance', (9, 10)), ('sequential LSTM encoder', (12, 15)), ('approximately 2 %', (16, 19))]",[],[],[],[]
experiments,"Not surprisingly , using LSTM as leaf node function helps in learning better representations .","[('using', (3, 4)), ('as', (5, 6)), ('helps in', (9, 11))]","[('LSTM', (4, 5)), ('leaf node function', (6, 9)), ('learning', (11, 12)), ('better representations', (12, 14))]",[],[],[],[]
experiments,Our NTI - SLSTM - LSTM is a hybrid model which encodes a sequence sequentially through its leaf node function and then hierarchically composes the output representations .,"[('is', (6, 7)), ('encodes', (11, 12)), ('through', (15, 16)), ('hierarchically composes', (22, 24))]","[('Our', (0, 1)), ('NTI - SLSTM - LSTM', (1, 6)), ('hybrid model', (8, 10)), ('sequence sequentially', (13, 15)), ('leaf node function', (17, 20)), ('output representations', (25, 27))]",[],[],[],[]
experiments,"The node - by - node attention models improve the performance , indicating that modeling inter-sentence interaction is an important element in NLI .","[('improve', (8, 9))]","[('node - by - node attention models', (1, 8)), ('performance', (10, 11))]",[],[],[],[]
experiments,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .","[('by', (12, 13))]","[('Deep LSTM and LSTM attention models', (1, 7)), ('outperform', (7, 8)), ('previous best result', (9, 12)), ('large margin', (14, 16)), ('nearly 5 - 6 %', (17, 22))]",[],[],[],[]
experiments,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,"[('improves', (1, 2)), ('sets', (6, 7)), ('by combining', (10, 12)), ('with', (14, 15))]","[('NASM', (0, 1)), ('result', (3, 4)), ('strong baseline', (8, 10)), ('variational autoencoder', (12, 14)), ('soft attention', (16, 18))]",[],[],[],[]
results,"Surprisingly , attention degree for the single word expression like "" stone "" , "" wall "" and "" leaves "" is lower to compare with multiword phrases .","[('for', (4, 5)), ('like', (9, 10)), ('lower', (22, 23)), ('to compare with', (23, 26))]","[('attention degree', (2, 4)), ('single word expression', (6, 9)), ('"" stone ""', (10, 13)), ('multiword phrases', (26, 28))]",[],[],[],[]
experiments,Overall the NTI model is robust to the length of the phrases being matched .,"[('to', (6, 7))]","[('NTI model', (2, 4)), ('robust', (5, 6)), ('length of the phrases', (8, 12))]",[],[],[],[]
results,"When the padding size is less ( up to 10 ) , the NTI - SLSTM - LSTM model performs better .","[('When', (0, 1)), ('is', (4, 5)), ('performs', (19, 20))]","[('padding size', (2, 4)), ('less ( up to 10 )', (5, 11)), ('NTI - SLSTM - LSTM model', (13, 19)), ('better', (20, 21))]",[],[],[],[]
results,Overall we do not observe any significant performance drop for both models as the padding size increases .,"[('do not observe', (2, 5)), ('for', (9, 10)), ('as', (12, 13))]","[('significant performance drop', (6, 9)), ('both models', (10, 12)), ('padding size', (14, 16)), ('increases', (16, 17))]",[],[],[],[]
research-problem,Attention - over - Attention Neural Networks for Reading Comprehension,[],[],[],[],[],[]
model,"In addition to the primary model , we also propose an N - best re-ranking strategy to double check the validity of the candidates and further improve the performance .","[('propose', (9, 10)), ('to double check', (16, 19)), ('of', (21, 22))]","[('N - best re-ranking strategy', (11, 16)), ('validity', (20, 21)), ('candidates', (23, 24)), ('performance', (28, 29))]",[],[],[],[]
model,"In this paper , we present a novel neural network architecture , called attention - over - attention model .","[('present', (5, 6)), ('called', (12, 13))]","[('novel neural network architecture', (7, 11)), ('attention - over - attention model', (13, 19))]",[],[],[],[]
model,We also propose an N - best re-ranking strategy to re-score the candidates in various aspects and further improve the performance .,"[('propose', (2, 3)), ('to re-score', (9, 11)), ('in', (13, 14))]","[('N - best re-ranking strategy', (4, 9)), ('candidates', (12, 13)), ('various aspects', (14, 16)), ('performance', (20, 21))]",[],[],[],[]
experiments,Children 's Book Test,[],[],[],[],[],[]
baselines,Embedding Layer :,[],"[('Embedding Layer', (0, 2))]",[],[],[],[]
experimental-setup,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ].","[('are', (3, 4)), ('in', (10, 11))]","[('embedding weights', (1, 3)), ('randomly initialized', (4, 6)), ('uniformed distribution', (8, 10))]",[],[],[],[]
experiments,CB Test NE CB Test CN Valid Test Valid Test Valid Test,[],"[('CB Test', (0, 2))]",[],[],[],[]
experimental-setup,Hidden Layer : Internal weights of GRUs are initialized with random orthogonal matrices .,"[('of', (5, 6)), ('initialized with', (8, 10))]","[('Hidden Layer', (0, 2)), ('Internal weights', (3, 5)), ('GRUs', (6, 7)), ('random orthogonal matrices', (10, 13))]",[],[],[],[]
experiments,"We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .","[('adopted', (1, 2)), ('for', (4, 5)), ('with', (8, 9)), ('of', (13, 14))]","[('ADAM optimizer', (2, 4)), ('weight updating', (5, 7)), ('initial learning rate', (10, 13)), ('0.001', (14, 15))]",[],[],[],[]
experiments,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .","[('suffer from', (5, 7)), ('set', (13, 14)), ('to', (18, 19))]","[('GRU units', (2, 4)), ('gradient exploding issues', (8, 11)), ('gradient clipping threshold', (15, 18)), ('5', (19, 20))]",[],[],[],[]
experiments,We used batched training strategy of 32 samples .,"[('used', (1, 2)), ('of', (5, 6))]","[('batched training strategy', (2, 5)), ('32 samples', (6, 8))]",[],[],[],[]
experiments,"In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .","[('In', (0, 1)), ('generate', (5, 6)), ('from', (10, 11))]","[('re-ranking step', (1, 3)), ('5 - best list', (6, 10)), ('baseline neural network model', (12, 16))]",[],[],[],[]
experimental-setup,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .","[('trained on', (5, 7)), ('of', (10, 11)), ('with', (14, 15)), ('trained by', (25, 27))]","[('language model features', (1, 4)), ('training proportion', (8, 10)), ('each dataset', (11, 13)), ('8 - gram wordbased setting', (15, 20)), ('Kneser - Ney smoothing', (21, 25)), ('SRILM toolkit', (27, 29))]",[],[],[],[]
experiments,"The ensemble model is made up of four best models , which are trained using different random seed .","[('made up of', (4, 7)), ('trained using', (13, 15))]","[('ensemble model', (1, 3)), ('four best models', (7, 10)), ('different random seed', (15, 18))]",[],[],[],[]
experimental-setup,"Implementation is done with Theano ( Theano Development Team , 2016 ) and Keras , and all models are trained on Tesla K40 GPU . :","[('done with', (2, 4)), ('trained on', (19, 21))]","[('Implementation', (0, 1)), ('Theano ( Theano Development Team , 2016 ) and Keras', (4, 14))]",[],[],[],[]
results,"As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .","[('by', (18, 19)), ('where', (23, 24)), ('over', (31, 32)), ('in', (33, 34)), ('demonstrate', (42, 43))]","[('our AoA Reader', (6, 9)), ('outperforms', (9, 10)), ('state - of - the - art systems', (10, 18)), ('large margin', (20, 22)), ('2.3 % and 2.0 % absolute improvements', (24, 31)), ('EpiReader', (32, 33)), ('CBTest NE and CN test sets', (34, 40))]",[],[],[],[]
results,"Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .","[('adding', (2, 3)), ('in', (5, 6)), ('there is', (10, 12)), ('over', (20, 21)), ('in', (24, 25))]","[('additional features', (3, 5)), ('re-ranking step', (7, 9)), ('another significant boost 2.0 % to 3.7 %', (12, 20)), ('Ao A Reader', (21, 24)), ('CBTest NE / CN test sets', (25, 31))]",[],[],[],[]
results,"We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .","[('found', (3, 4)), ('with', (12, 13)), ('of', (26, 27)), ('beyond', (29, 30)), ('in', (38, 39))]","[('our single model', (5, 8)), ('stay', (9, 10)), ('on par', (10, 12)), ('previous best ensemble system', (14, 18)), ('absolute improvement', (24, 26)), ('0.9 %', (27, 29)), ('best ensemble model ( Iterative Attention )', (31, 38)), ('CBTest NE validation set', (40, 44))]",[],[],[],[]
results,"When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .","[('shows', (11, 12)), ('over', (14, 15)), ('by', (19, 20)), ('setup', (24, 25))]","[('ensemble model', (4, 6)), ('our AoA Reader', (7, 10)), ('significant improvements', (12, 14)), ('previous best ensemble models', (15, 19)), ('large margin', (21, 23)), ('new state - of - the - art system', (26, 35))]",[],[],[],[]
results,"Instead of using pre-defined merging heuristics , and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance , where 4.1 % and 3.7 % improvements can be made in CNN validation and test set against CAS Reader .","[('letting', (8, 9)), ('explicitly learn', (11, 13)), ('between', (15, 16)), ('results in', (18, 20)), ('in', (23, 24)), ('against', (43, 44))]","[('pre-defined merging heuristics', (3, 6)), ('model', (10, 11)), ('weights', (14, 15)), ('individual attentions', (16, 18)), ('significant boost', (21, 23)), ('performance', (25, 26)), ('CNN', (38, 39)), ('CAS Reader', (44, 46))]",[],[],[],[]
research-problem,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems,[],"[('Question Answering', (9, 11))]",[],[],[],[]
research-problem,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,[],"[('Question Answering ( QA )', (0, 5))]",[],[],[],[]
research-problem,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,[],"[('Sentence matching', (0, 2)), ('QA', (8, 9)), ('Paraphrase Identification ( PI )', (16, 21))]",[],[],[],[]
research-problem,"In addition , we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor ( ANN ) technology , which enables us to find the most similar question from all available candidates very quickly during online serving .","[('implement', (4, 5)), ('that combines', (10, 12)), ('enables', (26, 27)), ('during', (40, 41))]","[('general semantic retrieval framework', (6, 10)), ('our proposed model and the Approximate Nearest Neighbor ( ANN ) technology', (12, 24)), ('most similar question from all available candidates', (31, 38)), ('online serving', (41, 43))]",[],[],[],[]
research-problem,Question answering systems have been widely studied in both the academic and industrial community and are widely applied to various scenarios .,[],"[('Question answering', (0, 2))]",[],[],[],[]
model,"In this work , we focus on building an IR - based QA system to answer the Frequently Asked Questions ( FAQ ) .","[('to answer', (14, 16))]","[('building', (7, 8)), ('IR - based QA system', (9, 14))]",[],[],[],[]
research-problem,"The critical part of IRbased QA system is to find the most similar question from a massive QA knowledge base , which could be further reformulated as a Paraphrase Identification ( PI ) problem , also known as sentence matching .","[('known as', (36, 38))]","[('IRbased QA', (4, 6)), ('Paraphrase Identification ( PI ) problem', (28, 34)), ('sentence matching', (38, 40))]",[],[],[],[]
model,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .","[('employ', (1, 2)), ('to depict', (5, 7)), ('between', (10, 11)), ('for', (12, 13)), ('propose', (18, 19)), ('solves', (27, 28))]","[('connected graph', (3, 5)), ('paraphrase relation', (8, 10)), ('sentences', (11, 12)), ('PI task', (14, 16)), ('multi-task sentence - encoding model', (20, 25)), ('paraphrase identification task', (29, 32)), ('sentence intent classification task', (34, 38))]",[],[],[],[]
model,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .","[('propose', (1, 2)), ('integrates', (7, 8)), ('with', (15, 16)), ('allows', (24, 25)), ('from', (34, 35))]","[('semantic retrieval framework', (3, 6)), ('encoding - based sentence matching model', (9, 15)), ('approximate nearest neighbor search technology', (17, 22)), ('most similar question', (29, 32)), ('all available questions', (35, 38)), ('QA knowledge base', (49, 52))]",[],[],[],[]
research-problem,Natural language sentence matching ( NLSM ) has gone through substantial developments in recent years .,[],"[('Natural language sentence matching ( NLSM )', (0, 7))]",[],[],[],[]
research-problem,"For the paraphrase identification ( PI ) task , NLSM is utilized to determine whether two sentences are paraphrases or not .",[],"[('paraphrase identification ( PI ) task', (2, 8)), ('NLSM', (9, 10))]",[],[],[],[]
experiments,Bank Question ( BQ ) dataset is a specific - domain Chinese dataset for sentence semantic equivalence identification ( SSEI ) .,[],"[('sentence semantic equivalence identification ( SSEI )', (14, 21))]",[],[],[],[]
hyperparameters,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .","[('For', (0, 1)), ('use', (5, 6)), ('as', (13, 14))]","[('Quora dataset', (1, 3)), ('Glove - 840B - 300D vector', (7, 13)), ('pre-trained word embedding', (15, 18))]",[],[],[],[]
hyperparameters,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,"[('with', (6, 7)), ('of', (13, 14)), ('set to', (16, 18))]","[('character embedding', (1, 3)), ('randomly initialized', (4, 6)), ('150 D', (7, 9)), ('hidden size', (11, 13)), ('BiGRU', (14, 15)), ('300', (18, 19))]",[],[],[],[]
hyperparameters,We set = 0.8 in the multi - task loss function .,"[('set', (1, 2)), ('in', (4, 5))]","[('= 0.8', (2, 4)), ('multi - task loss function', (6, 11))]",[],[],[],[]
hyperparameters,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .","[('applied to', (4, 6)), ('of', (8, 9)), ('with', (14, 15)), ('of', (18, 19))]","[('Dropout layer', (0, 2)), ('output', (7, 8)), ('attentive pooling layer', (10, 13)), ('dropout rate', (16, 18)), ('0.1', (19, 20))]",[],[],[],[]
hyperparameters,An Adam optimizer is used to optimize all the trainable weights .,"[('to optimize', (5, 7))]","[('Adam optimizer', (1, 3)), ('all the trainable weights', (7, 11))]",[],[],[],[]
hyperparameters,The learning rate is set to 4e - 4 and the batch size is set to 200 .,"[('set to', (4, 6)), ('set to', (14, 16))]","[('learning rate', (1, 3)), ('4e - 4', (6, 9)), ('batch size', (11, 13)), ('200', (16, 17))]",[],[],[],[]
hyperparameters,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .","[('When', (0, 1)), ('of', (3, 4)), ('is', (6, 7)), ('with', (14, 15)), ('of', (18, 19)), ('to find', (24, 26))]","[('performance', (2, 3)), ('model', (5, 6)), ('no longer improved', (7, 10)), ('SGD optimizer', (12, 14)), ('learning rate', (16, 18)), ('1e - 3', (19, 22)), ('better local optimum', (27, 30))]",[],[],[],[]
baselines,ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .,"[('is', (6, 7)), ('for', (12, 13))]","[('ESIM', (0, 1)), ('Enhanced Sequential Inference Model', (2, 6)), ('interaction - based model', (8, 12)), ('natural language inference', (13, 16))]",[],[],[],[]
baselines,It uses BiLSTM to encode sentence contexts and uses the attention mechanism to calculate the information between two sentences .,"[('uses', (1, 2)), ('to encode', (3, 5)), ('uses', (8, 9)), ('to calculate', (12, 14)), ('between', (16, 17))]","[('BiLSTM', (2, 3)), ('sentence contexts', (5, 7)), ('attention mechanism', (10, 12)), ('information', (15, 16)), ('two sentences', (17, 19))]",[],[],[],[]
results,ESIM has shown excellent performance on the SNLI dataset .,"[('shown', (2, 3)), ('on', (5, 6))]","[('ESIM', (0, 1)), ('excellent performance', (3, 5)), ('SNLI dataset', (7, 9))]",[],[],[],[]
baselines,BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .,"[('is', (7, 8)), ('with', (15, 16))]","[('BiMPM', (0, 1)), ('Bilateral Multi- Perspective Matching model', (2, 7)), ('interaction - based sentence matching model', (9, 15)), ('superior', (16, 17)), ('performance', (17, 18))]",[],[],[],[]
baselines,"The model uses a BiLSTM layer to learn the sentence representation , four different types of multiperspective matching layers to match two sentences , an additional BiLSTM layer to aggregate the matching results , and a two - layer feed - forward network for prediction .","[('uses', (2, 3)), ('to learn', (6, 8)), ('to match', (19, 21)), ('to aggregate', (28, 30)), ('for', (43, 44))]","[('BiLSTM layer', (4, 6)), ('sentence representation', (9, 11)), ('four different types of multiperspective matching layers', (12, 19)), ('two sentences', (21, 23)), ('additional BiLSTM layer', (25, 28)), ('matching results', (31, 33)), ('two - layer feed - forward network', (36, 43)), ('prediction', (44, 45))]",[],[],[],[]
baselines,"SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .","[('is', (7, 8)), ('enhances', (16, 17)), ('with', (21, 22))]","[('SSE', (0, 1)), ('Shortcut - Stacked Sentence Encoder', (2, 7)), ('encodingbased sentence - matching model', (9, 14)), ('multi - layer BiLSTM', (17, 21)), ('short - cut connections', (22, 26))]",[],[],[],[]
baselines,DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .,"[('is', (6, 7)), ('for', (12, 13))]","[('Densely Interactive Inference Network', (2, 6)), ('interaction - based model', (8, 12)), ('natural language inference ( NLI )', (13, 19))]",[],[],[],[]
baselines,It hierarchically extracts semantic features from interaction space to achieve a high - level understanding of the sentence pair .,"[('hierarchically extracts', (1, 3)), ('from', (5, 6)), ('to achieve', (8, 10)), ('of', (15, 16))]","[('semantic features', (3, 5)), ('interaction space', (6, 8)), ('high - level understanding', (11, 15)), ('sentence pair', (17, 19))]",[],[],[],[]
results,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,[],"[('LCQMC dataset', (0, 2))]",[],[],[],[]
results,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .","[('by', (15, 16)), ('reaching', (20, 21)), ('recording', (26, 27))]","[('our model', (4, 6)), ('outperforms', (6, 7)), ('state - of - the - art models', (7, 15)), ('large margin', (17, 19)), ('83 . 62 %', (21, 25)), ('state - of - the - art performance', (28, 36))]",[],[],[],[]
results,As shown in show that our MSEM model achieves the best performance .,"[('show', (3, 4)), ('achieves', (8, 9))]","[('our MSEM model', (5, 8)), ('best performance', (10, 12))]",[],[],[],[]
results,And the model with multi-task learning further improved performance ranging from 0.4 % to 1 % .,"[('with', (3, 4)), ('further improved', (6, 8)), ('ranging from', (9, 11)), ('to', (13, 14))]","[('model', (2, 3)), ('multi-task learning', (4, 6)), ('performance', (8, 9)), ('0.4 %', (11, 13))]",[],[],[],[]
results,"Compared with existing models , our model shows great advantages on datasets with low average overlap rate , which is known to be very common in realworld question answering scenarios .","[('shows', (7, 8)), ('on', (10, 11)), ('with', (12, 13))]","[('great advantages', (8, 10)), ('datasets', (11, 12)), ('low average overlap rate', (13, 17))]",[],[],[],[]
ablation-analysis,It turns out that the attentive pooling is better than max pooling .,"[('turns out that', (1, 4)), ('better than', (8, 10))]","[('attentive pooling', (5, 7)), ('max pooling', (10, 12))]",[],[],[],[]
ablation-analysis,"Then if we remove the highway network , the accuracy will drop to 88.36 % .","[('remove', (3, 4)), ('to', (12, 13))]","[('highway network', (5, 7)), ('accuracy', (9, 10)), ('drop', (11, 12)), ('88.36 %', (13, 15))]",[],[],[],[]
ablation-analysis,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .","[('remove', (3, 4)), ('to', (14, 15))]","[('character - level embedding', (5, 9)), ('accuracy', (11, 12)), ('drop', (13, 14)), ('88.26 %', (15, 17))]",[],[],[],[]
results,"As shown in , the F 1 score of the new system is 14 . 26 % higher than the baseline system .","[('of', (8, 9)), ('is', (12, 13)), ('higher than', (17, 19))]","[('F 1 score', (5, 8)), ('new system', (10, 12)), ('14 . 26 %', (13, 17)), ('baseline system', (20, 22))]",[],[],[],[]
research-problem,Deep Fusion LSTMs for Text Semantic Matching,[],[],[],[],[],[]
research-problem,"Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .",[],"[('modelling the interactions of text pair', (7, 13))]",[],[],[],[]
research-problem,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .",[],"[('modelling the relevance / similarity of a pair of texts', (25, 35)), ('text semantic matching', (40, 43))]",[],[],[],[]
model,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .","[('adopt', (5, 6)), ('to model', (10, 12)), ('of', (15, 16))]","[('deep fusion strategy', (7, 10)), ('strong interactions', (13, 15)), ('two sentences', (16, 18))]",[],[],[],[]
research-problem,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .","[('regarded as', (6, 8)), ('of', (11, 12)), ('in', (14, 15))]","[('text matching', (2, 4)), ('modelling', (8, 9)), ('interaction', (10, 11)), ('two texts', (12, 14)), ('recursive matching way', (16, 19))]",[],[],[],[]
model,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .","[('propose', (5, 6)), ('to model', (20, 22))]","[('deep fusion long short - term memory neural networks ( DF - LSTMs )', (6, 20)), ('interactions', (23, 24))]",[],[],[],[]
model,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .","[('consist of', (6, 8)), ('models', (16, 17)), ('under', (20, 21))]","[('DF - LSTMs', (3, 6)), ('two interconnected conditional LSTMs', (8, 12)), ('apiece of', (17, 19)), ('text', (19, 20)), ('influence', (22, 23)), ('another', (24, 25))]",[],[],[],[]
model,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,"[('of', (3, 4)), ('fed into', (8, 10)), ('to compute', (16, 18))]","[('output vector', (1, 3)), ('DF - LSTMs', (4, 7)), ('task - specific output layer', (11, 16)), ('match - ing score', (19, 23))]",[],[],[],[]
model,"Different with previous models , DF - LSTMs model the strong interactions of two texts in a recursive matching way , which consist of two inter -and intra-dependent LSTMs .","[('model', (8, 9)), ('of', (12, 13)), ('in', (15, 16)), ('consist of', (22, 24))]","[('strong interactions', (10, 12)), ('two texts', (13, 15)), ('recursive matching way', (17, 20)), ('two inter -and intra-dependent LSTMs', (24, 29))]",[],[],[],[]
research-problem,Recursively Text Semantic Matching,[],[],[],[],[],[]
model,Long Short - Term Memory Network,[],[],[],[],[],[]
research-problem,Deep Fusion LSTMs for Recursively Semantic Matching,[],[],[],[],[],[]
baselines,Neural bag - of - words ( NBOW ) :,[],"[('Neural bag - of - words ( NBOW )', (0, 9))]",[],[],[],[]
baselines,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .","[('represented as', (3, 5)), ('of', (7, 8)), ('of', (10, 11)), ('to', (22, 23))]","[('Each sequence', (0, 2)), ('sum', (6, 7)), ('embeddings', (9, 10)), ('words it contains', (12, 15)), ('concatenated and fed', (19, 22)), ('MLP', (24, 25))]",[],[],[],[]
baselines,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .","[('encoded by', (6, 8))]","[('Single LSTM', (0, 2))]",[],[],[],[]
baselines,"Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .","[('encoded by', (7, 9))]","[('Parallel LSTMs', (0, 2)), ('two LSTMs', (9, 11)), ('concatenated', (16, 17)), ('MLP', (21, 22))]",[],[],[],[]
baselines,"Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .","[('encoded by', (6, 8)), ('with', (9, 10))]","[('Attention LSTMs', (0, 2)), ('Two sequences', (3, 5)), ('LSTMs', (8, 9)), ('attention mechanism', (10, 12))]",[],[],[],[]
baselines,"Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :","[('of', (11, 12)), ('introduces', (16, 17))]","[('Word - by - word Attention LSTMs', (0, 7)), ('improved strategy', (9, 11)), ('word - by - word attention mechanism', (17, 24))]",[],[],[],[]
results,Experiment - I : Recognizing Textual Entailment,[],[],[],[],[],[]
results,"The results of DF - LSTMs outperform all the competitor models with the same number of hidden states while achieving comparable results to the state - of - the - art and using much fewer parameters , which indicate that it is effective to model the strong interactions of two texts in a recursive matching way .","[('results of', (1, 3)), ('with', (11, 12)), ('achieving', (19, 20)), ('to', (22, 23))]","[('DF - LSTMs', (3, 6)), ('outperform', (6, 7)), ('all the competitor models', (7, 11)), ('same number of hidden states', (13, 18)), ('comparable results', (20, 22)), ('state - of - the - art', (24, 31)), ('much fewer parameters', (33, 36))]",[],[],[],[]
results,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .","[('analyzing', (1, 2)), ('of', (5, 6)), ('can see', (11, 13)), ('consistently outperform', (25, 27)), ('with', (37, 38))]","[('evaluation results', (3, 5)), ('questionanswer matching', (6, 8)), ('strong interaction models ( attention LSTMs , our DF - LSTMs )', (13, 25)), ('weak interaction models ( NBOW , parallel LSTMs )', (28, 37)), ('large margin', (39, 41))]",[],[],[],[]
research-problem,Using Wikipedia articles as the knowledge source causes the task of question answering ( QA ) to combine the challenges of both large - scale open - domain QA and of machine comprehension of text .,[],"[('question answering ( QA )', (11, 16))]",[],[],[],[]
research-problem,"We term this setting , machine reading at scale ( MRS ) .",[],"[('machine reading at scale ( MRS )', (5, 12))]",[],[],[],[]
model,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,"[('treats', (2, 3)), ('as', (4, 5))]","[('Wikipedia', (3, 4)), ('collection of articles', (6, 9))]",[],[],[],[]
model,"Instead MRS is focused on simultaneously maintaining the challenge of machine comprehension , which requires the deep understanding of text , while keeping the realistic constraint of searching over a large open resource .","[('focused on', (3, 5))]","[('MRS', (1, 2)), ('simultaneously', (5, 6)), ('machine comprehension', (10, 12))]",[],[],[],[]
model,"In this paper , we show how multiple existing QA datasets can be used to evaluate MRS by requiring an open - domain system to perform well on all of them at once .","[('show', (5, 6)), ('by requiring', (17, 19)), ('to perform', (24, 26)), ('on', (27, 28))]","[('multiple existing QA datasets', (7, 11)), ('MRS', (16, 17)), ('open - domain system', (20, 24)), ('well', (26, 27)), ('all of them', (28, 31))]",[],[],[],[]
model,"We develop DrQA , a strong system for question answering from Wikipedia composed of : ( 1 ) Document Retriever , a module using bigram hashing and TF - IDF matching designed to , given a question , efficiently return a subset of relevant articles and ( 2 ) Document Reader , a multi - layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents .","[('develop', (1, 2)), ('composed of', (12, 14)), ('using', (23, 24)), ('designed to', (31, 33)), ('efficiently return', (38, 40)), ('of', (42, 43))]","[('DrQA', (2, 3)), ('question answering', (8, 10)), ('Document Retriever', (18, 20)), ('module', (22, 23)), ('bigram hashing and TF - IDF matching', (24, 31)), ('subset', (41, 42)), ('relevant articles', (43, 45)), ('Document Reader', (49, 51)), ('multi - layer', (53, 56)), ('answer spans', (65, 67))]",[],[],[],[]
experiments,Our System : DrQA,[],"[('Our System', (0, 2))]",[],[],[],[]
results,WikiMovies,[],[],[],[],[],[]
experimental-setup,We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,"[('use', (1, 2)), ('with', (7, 8)), ('for', (13, 14))]","[('3 - layer bidirectional LSTMs', (2, 7)), ('h = 128 hidden units', (8, 13)), ('both', (14, 15)), ('paragraph and question encoding', (15, 19))]",[],[],[],[]
experimental-setup,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .","[('apply', (1, 2)), ('for', (6, 7)), ('generating', (10, 11))]","[('Stanford CoreNLP toolkit', (3, 6)), ('tokenization', (7, 8)), ('lemma', (11, 12)), ('partof - speech', (13, 16)), ('named entity tags', (18, 21))]",[],[],[],[]
experimental-setup,We use Adamax for optimization as described in .,"[('use', (1, 2)), ('for', (3, 4))]","[('Adamax', (2, 3)), ('optimization', (4, 5))]",[],[],[],[]
experimental-setup,Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,"[('with', (1, 2)), ('applied to', (6, 8)), ('of', (15, 16))]","[('Dropout', (0, 1)), ('p = 0.3', (2, 5)), ('word embeddings and all the hidden units', (8, 15)), ('LSTMs', (16, 17))]",[],[],[],[]
results,"Our system ( single model ) can achieve 70.0 % exact match and 79.0 % F 1 scores on the test set , which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing .","[('achieve', (7, 8)), ('on', (18, 19))]","[('Our system ( single model )', (0, 6)), ('70.0 % exact match', (8, 12)), ('79.0 % F 1 scores', (13, 18)), ('test set', (20, 22))]",[],[],[],[]
results,As shown in all the features contribute to the performance of our final system .,"[('contribute to', (6, 8)), ('of', (10, 11))]","[('performance', (9, 10)), ('our final system', (11, 14))]",[],[],[],[]
results,"Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .","[('Without', (0, 1)), ('able to achieve', (21, 24)), ('over', (25, 26))]","[('aligned question embedding feature', (2, 6)), ('our system', (17, 19)), ('F1', (24, 25)), ('77 %', (26, 28))]",[],[],[],[]
baselines,SQuAD : A single Document Reader model is trained on the SQuAD training set only and used on all evaluation sets .,"[('trained on', (8, 10)), ('used on', (16, 18))]","[('SQuAD', (0, 1)), ('single Document Reader model', (3, 7)), ('SQuAD training set', (11, 14)), ('all evaluation sets', (18, 21))]",[],[],[],[]
baselines,Fine-tune ( DS ) : A Document Reader model is pre-trained on SQuAD and then fine - tuned for each dataset independently using its distant supervision ( DS ) training set .,"[('pre-trained on', (10, 12)), ('fine - tuned for', (15, 19)), ('using', (22, 23))]","[('Fine-tune ( DS )', (0, 4)), ('SQuAD', (12, 13)), ('each dataset independently', (19, 22)), ('distant supervision ( DS ) training set', (24, 31))]",[],[],[],[]
baselines,Multitask ( DS ) :,[],"[('Multitask ( DS )', (0, 4))]",[],[],[],[]
results,"Despite the difficulty of the task compared to machine comprehension ( where you are given the right paragraph ) and unconstrained QA ( using redundant resources ) , Dr QA still provides reasonable performance across all four datasets .",[],"[('unconstrained QA', (20, 22)), ('Dr QA', (28, 30)), ('reasonable performance', (32, 34))]",[],[],[],[]
research-problem,A Deep Cascade Model for Multi - Document Reading Comprehension,[],[],[],[],[],[]
research-problem,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .",[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[]
model,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .","[('propose', (7, 8)), ('combines', (13, 14)), ('in', (19, 20))]","[('deep cascade model', (9, 12)), ('advantages', (15, 16)), ('coarse - to - fine manner', (21, 27))]",[],[],[],[]
model,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,"[('passed to', (5, 7)), ('for extracting', (14, 16)), ('at', (20, 21))]","[('selected paragraphs', (2, 4)), ('attention - based deep MRC model', (8, 14)), ('actual answer span', (17, 20)), ('word level', (21, 23))]",[],[],[],[]
model,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .","[('introduce', (9, 10)), ('as', (16, 17))]","[('answer extraction', (4, 6)), ('document extraction and paragraph extraction', (11, 16)), ('two auxiliary tasks', (17, 20))]",[],[],[],[]
model,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .","[('jointly optimize', (1, 3)), ('in', (7, 8)), ('shares', (15, 16))]","[('all the three tasks', (3, 7)), ('unified deep MRC model', (9, 13)), ('some common bottom layers', (16, 20))]",[],[],[],[]
model,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .","[('enables', (3, 4)), ('to perform', (6, 8)), ('at', (15, 16)), ('can be learnt', (21, 24))]","[('models', (5, 6)), ('coarse - to - fine pruning', (9, 15)), ('different stages', (16, 18)), ('better models', (19, 21))]",[],[],[],[]
model,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .","[('consists of', (11, 13))]","[('three modules', (13, 15)), ('document retrieval', (16, 18)), ('paragraph retrieval', (19, 21)), ('answer extraction', (22, 24))]",[],[],[],[]
model,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .","[('at', (2, 3)), ('consumes', (6, 7)), ('from', (9, 10)), ('further prunes', (15, 17)), ('given', (24, 25))]","[('module', (1, 2)), ('each subsequent stage', (3, 6)), ('output', (8, 9)), ('previous stage', (11, 13)), ('documents', (18, 19)), ('paragraphs and', (20, 22)), ('answer spans', (22, 24)), ('question', (26, 27))]",[],[],[],[]
model,"For each of the first two modules , we define a ranking function and an extraction function .","[('define', (9, 10))]","[('ranking function', (11, 13)), ('extraction function', (15, 17))]",[],[],[],[]
model,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .","[('used as', (5, 7)), ('to discard', (10, 12)), ('to keep', (22, 24))]","[('ranking function', (1, 3)), ('preliminary filter', (8, 10)), ('most of the irrelevant documents or paragraphs', (12, 19)), ('our framework efficient', (24, 27))]",[],[],[],[]
model,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .","[('designed to deal with', (5, 9)), ('jointly optimized with', (19, 22)), ('for', (27, 28))]","[('extraction function', (1, 3)), ('auxiliary document and paragraph extraction tasks', (10, 16)), ('final answer extraction module', (23, 27)), ('better extraction performance', (28, 31))]",[],[],[],[]
model,"The main contributions can be summarized as follow : We propose a deep cascade learning framework to address the practical multi-document machine reading comprehension task , which considers both the effectiveness and efficiency in a coarse - to - fine manner .","[('propose', (10, 11)), ('to address', (16, 18)), ('considers', (27, 28))]","[('deep cascade learning framework', (12, 16))]",[],[],[],[]
model,"We incorporate the auxiliary document extraction and paragraph extraction tasks to the pure answer span prediction , which helps to narrow down the search space and improves the final extraction result in multi-document MRC scenario .","[('incorporate', (1, 2)), ('to', (10, 11))]","[('auxiliary document extraction and paragraph extraction tasks', (3, 10)), ('pure answer span prediction', (12, 16))]",[],[],[],[]
research-problem,Related Work Machine Reading Comprehension,[],[],[],[],[],[]
model,Cascade Learning,[],[],[],[],[],[]
experimental-setup,"Since the Trivia QA documents often contain many small paragraphs , we also restructure the documents by merging consecutive paragraphs to a maximum size of 600 words for each paragraph as in ( Clark and Gardner 2017 ) .","[('restructure', (13, 14)), ('by merging', (16, 18)), ('to', (20, 21)), ('of', (24, 25)), ('for', (27, 28))]","[('Trivia QA', (2, 4)), ('documents', (15, 16)), ('consecutive paragraphs', (18, 20)), ('maximum size', (22, 24)), ('600 words', (25, 27)), ('each paragraph', (28, 30))]",[],[],[],[]
experimental-setup,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .","[('For', (0, 1)), ('adopt', (8, 9)), ('for', (12, 13)), ('with', (15, 16))]","[('multi-task deep attention framework', (2, 6)), ('Adam optimizer', (10, 12)), ('training', (13, 14)), ('mini-batch size', (17, 19)), ('32', (20, 21)), ('initial learning rate', (22, 25)), ('0.0005', (26, 27))]",[],[],[],[]
experimental-setup,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,"[('use', (1, 2)), ('in', (8, 9)), ('train', (11, 12)), ('with', (17, 18)), ('for', (22, 23))]","[('GloVe 300 dimensional word embeddings', (3, 8)), ('TriviaQA', (9, 10)), ('word2 vec word embeddings', (13, 17)), ('whole DuReader corpus', (19, 22)), ('DuReader', (23, 24))]",[],[],[],[]
experimental-setup,The word embeddings are fixed during training .,"[('fixed during', (4, 6))]","[('word embeddings', (1, 3)), ('training', (6, 7))]",[],[],[],[]
experimental-setup,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,"[('of', (3, 4)), ('set as', (6, 8)), ('for', (9, 10))]","[('hidden size', (1, 3)), ('LSTM', (4, 5)), ('150', (8, 9)), ('TriviaQA', (10, 11)), ('128', (12, 13)), ('DuReader', (14, 15))]",[],[],[],[]
experimental-setup,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,"[('set as', (16, 18))]","[('Regularization parameter', (26, 28))]",[],[],[],[]
experimental-setup,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,"[('trained on', (3, 5)), ('with', (9, 10)), ('in', (13, 14))]","[('Nvidia Tesla M40 GPU', (5, 9)), ('Cudnn LSTM cell', (10, 13)), ('Tensorflow 1.3', (14, 16))]",[],[],[],[]
results,"We can see that by adopting the deep cascade learning framework , the proposed model outperforms the previous state - of - the - art methods by an evident margin on both datasets , which validates the effectiveness of the proposed method in addressing the challenging multi-document MRC task .","[('adopting', (5, 6)), ('by', (26, 27))]","[('deep cascade learning framework', (7, 11)), ('proposed model', (13, 15)), ('outperforms', (15, 16)), ('previous state - of - the - art methods', (17, 26)), ('evident margin', (28, 30)), ('both datasets', (31, 33)), ('proposed method', (40, 42))]",[],[],[],[]
ablation-analysis,"From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .","[('see', (6, 7)), ('in', (18, 19)), ('among', (21, 22)), ('from', (42, 43)), ('extracted from', (49, 51)), ('can keep', (61, 63)), ('from', (66, 67))]","[('shared LSTM', (12, 14)), ('important', (16, 17)), ('answer extraction', (19, 21)), ('multiple documents', (22, 24)), ('content probability score', (39, 42)), ('multiple documents', (43, 45)), ('ranking order', (64, 66)), ('document ranking component', (67, 70))]",[],[],[],[]
ablation-analysis,"2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .","[('serve as', (20, 22)), ('between', (27, 28))]","[('Both the preliminary cascade ranking and multi-task answer extraction strategy', (2, 12)), ('vital', (13, 14)), ('final performance', (16, 18)), ('good trade - off', (23, 27)), ('pure pipeline method and fully joint learning method', (29, 37))]",[],[],[],[]
ablation-analysis,"Jointly training the three extraction tasks can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .","[('Jointly training', (0, 2)), ('provide', (7, 8)), ('shows', (12, 13)), ('with', (26, 27)), ('at', (29, 30))]","[('three extraction tasks', (3, 6)), ('great benefits', (8, 10)), ('three tasks', (15, 17)), ('closely related', (19, 21)), ('boost', (23, 24)), ('each other', (24, 26)), ('shared representations', (27, 29)), ('bottom layers', (30, 32))]",[],[],[],[]
ablation-analysis,Effectiveness v.s. Efficiency Trade - off,[],[],[],[],[],[]
ablation-analysis,The result on DuReader development set is presented in .,"[('on', (2, 3))]","[('DuReader development set', (3, 6))]",[],[],[],[]
ablation-analysis,"We can see that : 1 ) By properly taking more documents or paragraphs into consideration , the performance of the model gradually increases when it reaches 4 documents and 2 paragraphs , and then the performance decreases slightly which maybe due to that much noisy data is introduced .","[('By properly', (7, 9)), ('into', (14, 15)), ('of', (19, 20)), ('when', (24, 25)), ('reaches', (26, 27))]","[('more documents or paragraphs', (10, 14)), ('consideration', (15, 16)), ('performance', (18, 19)), ('model', (21, 22)), ('gradually increases', (22, 24)), ('4 documents and 2 paragraphs', (27, 32)), ('performance', (36, 37)), ('decreases slightly', (37, 39))]",[],[],[],[]
ablation-analysis,"2 ) The time cost can be largely reduced by removing more irrelevant documents and paragraphs in the cascade ranking stage , while keeping the performance not change that much .","[('can be', (5, 7)), ('by removing', (9, 11)), ('in', (16, 17))]","[('time cost', (3, 5)), ('largely reduced', (7, 9)), ('more irrelevant documents and paragraphs', (11, 16)), ('cascade ranking stage', (18, 21)), ('performance', (25, 26)), ('not change that much', (26, 30))]",[],[],[],[]
ablation-analysis,The performance of jointly training the answer extraction module with different auxiliary tasks on DuReader development set is shown in Table 5 .,"[('of', (2, 3)), ('on', (13, 14))]","[('performance', (1, 2)), ('jointly training', (3, 5)), ('answer extraction module', (6, 9)), ('DuReader development set', (14, 17))]",[],[],[],[]
ablation-analysis,"We can see that by incorporating the auxiliary document extraction or paragraph extraction task in the joint learning framework , the performance can always improve which again shows the advantage of introducing auxiliary tasks for helping to learn shared bottom representations .","[('incorporating', (5, 6)), ('in', (14, 15)), ('can', (22, 23))]","[('auxiliary document extraction or paragraph extraction task', (7, 14)), ('joint learning framework', (16, 19)), ('performance', (21, 22)), ('always improve', (23, 25))]",[],[],[],[]
ablation-analysis,"Besides , the performance gain by adding document extraction task is larger , which maybe due to that it can better lay the foundation of the model with that information from different documents can be distinguished .","[('by adding', (5, 7)), ('is', (10, 11))]","[('performance gain', (3, 5)), ('document extraction task', (7, 10)), ('larger', (11, 12))]",[],[],[],[]
results,Results on E-commerce and Tax data,[],[],[],[],[],[]
results,"Besides , the performance with respect to F 1 score is also largely improved with the proposed multi-document MRC model , which demonstrates the effectiveness of our method for removing the rich irrelevant noisy content in our online scenario .","[('with respect to', (4, 7))]","[('performance', (3, 4)), ('F 1 score', (7, 10)), ('largely improved', (12, 14)), ('proposed multi-document MRC model', (16, 20))]",[],[],[],[]
results,Results on Different Document Lengths,[],[],[],[],[],[]
results,"We can see that without incorporating with the cascade ranking module , the answer extraction module performs rather poorly both in effectiveness and efficiency as the document length increases .","[('without incorporating with', (4, 7)), ('performs', (16, 17)), ('both in', (19, 21)), ('as', (24, 25))]","[('cascade ranking module', (8, 11)), ('answer extraction module', (13, 16)), ('rather poorly', (17, 19)), ('effectiveness and efficiency', (21, 24)), ('document length', (26, 28)), ('increases', (28, 29))]",[],[],[],[]
research-problem,U - Net : Machine Reading Comprehension with Unanswerable Questions,[],"[('Machine Reading Comprehension', (4, 7))]",[],[],[],[]
research-problem,Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing .,[],"[('Machine reading comprehension', (0, 3))]",[],[],[],[]
research-problem,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .",[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[]
experimental-setup,"Glove embedding ( Pennington , Socher , and Manning 2014 ) and Elmo embedding ) are used as basic embeddings .","[('used as', (16, 18))]","[('Glove embedding', (0, 2)), ('Elmo embedding', (12, 14)), ('basic embeddings', (18, 20))]",[],[],[],[]
experimental-setup,"We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .","[('use', (1, 2)), ('to process', (3, 5)), ('to obtain', (9, 11)), ('of', (21, 22))]","[('Spacy', (2, 3)), ('each question and passage', (5, 9)), ('tokens', (11, 12)), ('POS tags', (13, 15)), ('NER tags', (16, 18)), ('lemmas tags', (19, 21)), ('each text', (22, 24))]",[],[],[],[]
experimental-setup,"We use 12 dimensions to embed POS tags , 8 for NER tags .","[('use', (1, 2)), ('to embed', (4, 6)), ('for', (10, 11))]","[('12 dimensions', (2, 4)), ('POS tags', (6, 8)), ('8', (9, 10)), ('NER tags', (11, 13))]",[],[],[],[]
experimental-setup,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .","[('use', (1, 2)), ('between', (16, 17))]","[('3 binary features', (2, 5)), ('exact match', (6, 8)), ('lower - case match', (9, 13)), ('lemma match', (14, 16)), ('question and', (18, 20)), ('passage', (20, 21))]",[],[],[],[]
experimental-setup,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,"[('use', (1, 2))]","[('100 - dim Glove pretrained word embeddings', (2, 9)), ('1024 - dim Elmo embeddings', (10, 15))]",[],[],[],[]
experimental-setup,All the LSTM blocks are bi-directional with one single layer .,"[('are', (4, 5)), ('with', (6, 7))]","[('LSTM blocks', (2, 4)), ('bi-directional', (5, 6)), ('one single layer', (7, 10))]",[],[],[],[]
experimental-setup,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .","[('set', (1, 2)), ('as', (6, 7))]","[('hidden layer dimension', (3, 6)), ('125', (7, 8)), ('attention layer dimension', (9, 12)), ('250', (13, 14))]",[],[],[],[]
experimental-setup,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .","[('added', (1, 2)), ('overall', (5, 6)), ('including', (10, 11)), ('at', (15, 16)), ('of', (19, 20))]","[('dropout layer', (3, 5)), ('modeling layers', (7, 9)), ('embedding layer', (12, 14)), ('dropout rate', (17, 19)), ('0.3', (20, 21))]",[],[],[],[]
experimental-setup,We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,"[('use', (1, 2)), ('with', (4, 5)), ('of', (8, 9))]","[('Adam optimizer', (2, 4)), ('learning rate', (6, 8)), ('0.002', (9, 10))]",[],[],[],[]
results,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .","[('achieves', (2, 3))]","[('Our model', (0, 2)), ('F 1 score', (4, 7)), ('74.0', (8, 9)), ('EM score', (11, 13)), ('70.3', (14, 15)), ('development set', (17, 19)), ('F 1 score', (22, 25)), ('EM score', (29, 31))]",[],[],[],[]
results,Our model outperforms most of the previous approaches .,[],"[('Our model', (0, 2)), ('outperforms', (2, 3)), ('most of the previous approaches', (3, 8))]",[],[],[],[]
results,"Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .","[('Comparing to', (0, 2)), ('is', (15, 16))]","[('best - performing systems', (3, 7)), ('our model', (8, 10)), ('simple architecture', (12, 14)), ('end - to - end model', (17, 23))]",[],[],[],[]
results,"In fact , among all the end - to - end models , we achieve the best F1 scores .","[('among', (3, 4)), ('achieve', (14, 15))]","[('all the end - to - end models', (4, 12)), ('best F1 scores', (16, 19))]",[],[],[],[]
ablation-analysis,"Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .","[('show', (1, 2))]","[('performance', (4, 5)), ('dropped slightly', (5, 7))]",[],[],[],[]
ablation-analysis,"Compared to the bi-attention model , the F1 - score decreases 0.5 % .","[('Compared to', (0, 2))]","[('bi-attention model', (3, 5)), ('F1 - score', (7, 10)), ('decreases', (10, 11)), ('0.5 %', (11, 13))]",[],[],[],[]
ablation-analysis,Multi- task Study,[],[],[],[],[],[]
ablation-analysis,Results ( the first two rows in ) show that there is a large gain when using the multi - task model .,"[('show', (8, 9)), ('when using', (15, 17))]","[('large gain', (13, 15)), ('multi - task model', (18, 22))]",[],[],[],[]
ablation-analysis,"For the answer boundary detection task , we find that the multi -task setup ( i.e. , the classification layer participates in the training process ) does not help its performance .","[('For', (0, 1)), ('find that', (8, 10)), ('does not', (26, 28))]","[('answer boundary detection task', (2, 6)), ('multi -task setup', (11, 14)), ('help', (28, 29)), ('its', (29, 30)), ('performance', (30, 31))]",[],[],[],[]
ablation-analysis,"But as shown above , our model achieves a good score in SQuAD 2.0 test , which shows this model has the potential to achieve higher performance by making progress on both the answer detection and classification tasks .","[('achieves', (7, 8)), ('in', (11, 12))]","[('our model', (5, 7)), ('good score', (9, 11)), ('SQuAD 2.0 test', (12, 15))]",[],[],[],[]
ablation-analysis,"Overall , we can conclude that our multi-task model works well since the performance of unanswerability classification improves significantly when the answer pointer and answer verifier work simultaneously .","[('conclude that', (4, 6)), ('works', (9, 10)), ('of', (14, 15)), ('when', (19, 20)), ('work', (26, 27))]","[('our multi-task model', (6, 9)), ('well', (10, 11)), ('performance', (13, 14)), ('unanswerability classification', (15, 17)), ('improves', (17, 18)), ('significantly', (18, 19)), ('answer pointer and answer verifier', (21, 26)), ('simultaneously', (27, 28))]",[],[],[],[]
ablation-analysis,"As we can see , when the threshold is set to 0.5 , F1 score of answerable questions is similar to that of unanswerable questions .","[('set to', (9, 11)), ('of', (15, 16)), ('similar to', (19, 21))]","[('threshold', (7, 8)), ('0.5', (11, 12)), ('F1 score', (13, 15)), ('answerable questions', (16, 18)), ('unanswerable questions', (23, 25))]",[],[],[],[]
ablation-analysis,"When we increase the threshold ( i.e. , more likely to predict the question as unanswerable ) , performance for answerable questions degrades , and improves for unanswerable questions .","[('increase', (2, 3)), ('for', (19, 20))]","[('threshold', (4, 5)), ('performance', (18, 19)), ('answerable questions', (20, 22)), ('degrades', (22, 23)), ('improves', (25, 26)), ('unanswerable questions', (27, 29))]",[],[],[],[]
ablation-analysis,"We can see that the overall F 1 score is slightly better , which is consistent with the idea from SQ uAD 2.0 .","[('see that', (2, 4)), ('is', (9, 10)), ('consistent with', (15, 17))]","[('overall F 1 score', (5, 9)), ('slightly better', (10, 12))]",[],[],[],[]
ablation-analysis,"Finally , we set the threshold to be 0.7 for the submission system to SQuAD evaluation .","[('set', (3, 4)), ('to be', (6, 8)), ('for', (9, 10)), ('to', (13, 14))]","[('threshold', (5, 6)), ('0.7', (8, 9)), ('submission system', (11, 13)), ('SQuAD evaluation', (14, 16))]",[],[],[],[]
research-problem,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING,[],"[('CONVERSATIONAL QUESTION', (9, 11))]",[],[],[],[]
research-problem,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,[],"[('Conversational question answering ( CQA )', (0, 6))]",[],[],[],[]
research-problem,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",[],"[('machine reading comprehension ( MRC )', (6, 12)), ('CQA', (14, 15)), ('passage', (16, 17))]",[],[],[],[]
research-problem,Traditional machine reading comprehension ( MRC ) tasks share the single - turn setting of answering a single question related to a passage .,[],"[('machine reading comprehension ( MRC )', (1, 7))]",[],[],[],[]
model,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .","[('propose', (5, 6)), ('for', (16, 17))]","[('SDNet', (6, 7)), ('contextual attention - based deep neural network', (9, 16)), ('task', (18, 19)), ('conversational question answering', (20, 23))]",[],[],[],[]
model,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .","[('leverages', (3, 4)), ('in', (7, 8))]","[('SDNet', (2, 3)), ('latest breakthrough', (5, 7)), ('NLP', (8, 9)), ('BERT contextual embedding', (10, 13))]",[],[],[],[]
hyperparameters,"For training , we use all questions / answers for one passage as a batch .","[('For', (0, 1)), ('use', (4, 5)), ('for', (9, 10)), ('as', (12, 13))]","[('training', (1, 2)), ('all questions / answers', (5, 9)), ('one passage', (10, 12)), ('batch', (14, 15))]",[],[],[],[]
baselines,"We compare SDNet with the following baseline models : PGNet ( Seq2 Seq with copy mechanism ) , DrQA , DrQA + PGNet , BiDAF ++ and .","[('compare', (1, 2))]","[('SDNet', (2, 3)), ('PGNet ( Seq2 Seq with copy mechanism )', (9, 17)), ('DrQA', (18, 19)), ('DrQA + PGNet', (20, 23)), ('BiDAF ++', (24, 26))]",[],[],[],[]
experiments,"As shown , SDNet achieves significantly better results than baseline models .","[('achieves', (4, 5)), ('than', (8, 9))]","[('SDNet', (3, 4)), ('significantly better results', (5, 8)), ('baseline models', (9, 11))]",[],[],[],[]
experiments,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .","[('improves', (7, 8)), ('by', (11, 12)), ('compared with', (15, 17)), ('on', (24, 25))]","[('single SDNet model', (4, 7)), ('overall F 1', (8, 11)), ('1.6 %', (12, 14)), ('previous state - of - art model', (17, 24)), ('CoQA', (25, 26))]",[],[],[],[]
experiments,"Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .","[('further improves', (3, 5)), ('by', (9, 10)), ('on', (27, 28))]","[('Ensemble SDNet model', (0, 3)), ('overall F 1 score', (5, 9)), ('2.7 %', (10, 12)), ('over', (21, 22)), ('in - domain datasets', (28, 32))]",[],[],[],[]
experiments,"As seen , SDNet overpasses all but one baseline models after the second epoch , and achieves state - of - the - art results only after 8 epochs .","[('overpasses', (4, 5)), ('after', (10, 11)), ('achieves', (16, 17)), ('after', (26, 27))]","[('SDNet', (3, 4)), ('all but one baseline models', (5, 10)), ('second epoch', (12, 14)), ('state - of - the - art results', (17, 25)), ('8 epochs', (27, 29))]",[],[],[],[]
ablation-analysis,The results show that removing BERT can reduce the F 1 score on development set by 7.15 % .,"[('show', (2, 3)), ('removing', (4, 5)), ('on', (12, 13)), ('by', (15, 16))]","[('BERT', (5, 6)), ('F 1 score', (9, 12)), ('development set', (13, 15)), ('7.15 %', (16, 18))]",[],[],[],[]
ablation-analysis,"Our proposed weight sum of per-layer output from BERT is crucial , which can boost the performance by 1.75 % , compared with using only last layer 's output .","[('of', (4, 5)), ('from', (7, 8)), ('is', (9, 10)), ('can', (13, 14)), ('by', (17, 18)), ('compared with using', (21, 24))]","[('Our', (0, 1)), ('proposed weight sum', (1, 4)), ('per-layer output', (5, 7)), ('BERT', (8, 9)), ('crucial', (10, 11)), ('boost', (14, 15)), ('performance', (16, 17)), ('1.75 %', (18, 20)), (""only last layer 's output"", (24, 29))]",[],[],[],[]
ablation-analysis,"Using BERT - base instead of BERT - large pretrained model hurts the F 1 score by 2.61 % , which manifests the superiority of BERT - large model .","[('Using', (0, 1)), ('instead of', (4, 6)), ('hurts', (11, 12)), ('by', (16, 17))]","[('BERT - base', (1, 4)), ('BERT - large pretrained model', (6, 11)), ('F 1 score', (13, 16)), ('2.61 %', (17, 19))]",[],[],[],[]
ablation-analysis,"Variational dropout and self attention can each improve the performance by 0.24 % and 0.75 % , respectively .","[('by', (10, 11))]","[('Variational dropout and self attention', (0, 5)), ('performance', (9, 10)), ('0.24 % and 0.75 %', (11, 16))]",[],[],[],[]
research-problem,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS,[],"[('TRACKING THE WORLD STATE', (0, 4))]",[],[],[],[]
model,"It may also learn basic rules of approximate ( logical ) inference , such as the fact that objects belonging to the same category tend to have similar properties ( light objects can be carried over from rooms to rooms for instance ) .","[('of', (6, 7)), ('belonging to', (19, 21)), ('tend to have', (24, 27))]","[('basic rules', (4, 6)), ('approximate ( logical ) inference', (7, 12)), ('same category', (22, 24)), ('similar properties', (27, 29))]",[],[],[],[]
model,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,"[('with', (6, 7)), ('uses', (17, 18))]","[('new kind of memory - augmented neural network', (8, 16)), ('distributed memory and processor architecture', (19, 24)), ('Recurrent Entity Network ( EntNet )', (26, 32))]",[],[],[],[]
results,SYNTHETIC WORLD MODEL TASK,[],[],[],[],[],[]
hyperparameters,"For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .","[('For', (0, 1)), ('set', (5, 6)), ('equal to', (10, 12))]","[('MemN2N', (2, 3)), ('number of hops', (7, 10)), ('T ? 2', (12, 15)), ('embedding dimension', (17, 19)), ('d = 20', (20, 23))]",[],[],[],[]
hyperparameters,"All models were trained with ADAM with initial learning rates set by grid search over { 0.1 , 0.01 , 0.001 } and divided by 2 every 10,000 updates .","[('trained with', (3, 5)), ('with', (6, 7)), ('set by', (10, 12)), ('over', (14, 15)), ('divided by', (23, 25))]","[('ADAM', (5, 6)), ('initial learning rates', (7, 10)), ('grid search', (12, 14)), ('{ 0.1 , 0.01 , 0.001 }', (15, 22)), ('2 every 10,000 updates', (25, 29))]",[],[],[],[]
results,"The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .","[('as', (10, 11))]","[('MemN2N', (1, 2)), ('worst performance', (4, 6)), ('degrades', (8, 9)), ('quickly', (9, 10)), ('length of the sequence', (12, 16)), ('increases', (16, 17))]",[],[],[],[]
results,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .","[('performs', (2, 3)), ('as', (9, 10))]","[('LSTM', (1, 2)), ('better', (3, 4)), ('loses', (7, 8)), ('accuracy', (8, 9)), ('length of the sequence', (11, 15)), ('increases', (15, 16))]",[],[],[],[]
results,"In contrast , the EntNet is able to solve the task in all cases .","[('able to solve', (6, 9)), ('in', (11, 12))]","[('EntNet', (4, 5)), ('task', (10, 11)), ('all cases', (12, 14))]",[],[],[],[]
results,We see that the model is able to achieve good performance several times past its training horizon .,"[('see', (1, 2)), ('able to achieve', (6, 9))]","[('model', (4, 5)), ('good performance', (9, 11)), ('several times past', (11, 14)), ('its training horizon', (14, 17))]",[],[],[],[]
experiments,BABI TASKS,[],[],[],[],[],[]
hyperparameters,"All models were trained with ADAM using a learning rate of ? = 0.01 , which was divided by 2 every 25 epochs until 200 epochs were reached .","[('trained with', (3, 5)), ('using', (6, 7)), ('of', (10, 11)), ('divided by', (17, 19)), ('until', (23, 24))]","[('ADAM', (5, 6)), ('learning rate', (8, 10)), ('? = 0.01', (11, 14)), ('2 every 25 epochs', (19, 23)), ('200 epochs', (24, 26))]",[],[],[],[]
hyperparameters,"In all experiments , our model had embedding dimension size d = 100 and 20 memory slots .",[],"[('our model', (4, 6)), ('embedding dimension size d = 100', (7, 13)), ('20 memory slots', (14, 17))]",[],[],[],[]
experiments,"Our model is able to solve all the tasks , outperforming the other models in terms of both the number of solved tasks and the average error .","[('able to', (3, 5)), ('in terms of', (14, 17))]","[('Our model', (0, 2)), ('solve', (5, 6)), ('all the tasks', (6, 9)), ('outperforming', (10, 11)), ('other models', (12, 14)), ('number of solved tasks', (19, 23)), ('average error', (25, 27))]",[],[],[],[]
experiments,"Note that it does not store useful or correct information in the memory slots corresponding to locations , most likely because this task does not contain questions about locations ( such as "" who is in the kitchen ? "" ) .","[('does not store', (3, 6)), ('in', (10, 11)), ('corresponding to', (14, 16))]","[('useful or correct information', (6, 10)), ('memory slots', (12, 14)), ('locations', (16, 17))]",[],[],[],[]
experiments,CHILDRE N'S BOOK TEST ( CBT ),[],[],[],[],[],[]
hyperparameters,All models were trained using standard stochastic gradient descent ( SGD ) with a fixed learning rate of 0.001 .,"[('trained using', (3, 5)), ('with', (12, 13)), ('of', (17, 18))]","[('standard stochastic gradient descent ( SGD )', (5, 12)), ('fixed learning rate', (14, 17)), ('0.001', (18, 19))]",[],[],[],[]
hyperparameters,"We used separate input encodings for the update and gating functions , and applied a dropout rate of 0.5 to the word embedding dimensions .","[('used', (1, 2)), ('for', (5, 6)), ('applied', (13, 14)), ('of', (17, 18)), ('to', (19, 20))]","[('separate input encodings', (2, 5)), ('update and gating functions', (7, 11)), ('dropout rate', (15, 17)), ('0.5', (18, 19)), ('word embedding dimensions', (21, 24))]",[],[],[],[]
results,"The general EntNet performs better than the LSTMs and n-gram model on the Named Entities Task , but lags behind on the Common Nouns task .","[('performs', (3, 4)), ('than', (5, 6)), ('on', (11, 12)), ('lags behind', (18, 20))]","[('general EntNet', (1, 3)), ('better', (4, 5)), ('LSTMs and n-gram model', (7, 11)), ('Named Entities Task', (13, 16)), ('Common Nouns task', (22, 25))]",[],[],[],[]
results,"The simplified EntNet outperforms all other single - pass models on both tasks , and also performs better than the Memory Network which does not use the self - supervision heuristic .","[('performs', (16, 17)), ('than', (18, 19)), ('which does not use', (22, 26))]","[('simplified EntNet', (1, 3)), ('outperforms', (3, 4)), ('all other single - pass models', (4, 10)), ('better', (17, 18)), ('Memory Network', (20, 22)), ('self - supervision heuristic', (27, 31))]",[],[],[],[]
results,The fact that the simplified EntNet is able to obtain decent performance is encouraging since it indicates that the model is able to build an internal representation of the story which it can then use to answer a relatively diverse set of queries .,"[('able to obtain', (7, 10)), ('is', (12, 13))]","[('simplified EntNet', (4, 6)), ('decent performance', (10, 12)), ('encouraging', (13, 14))]",[],[],[],[]
model,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .","[('introduce', (12, 13)), ('for the use of', (17, 21))]","[('general framework PhaseCond', (14, 17)), ('multiple attention layers', (21, 24))]",[],[],[],[]
model,"This perspective leads to a different attention - based architecture containing two sequential phases , question - aware passage representation phase and evidence propagation phase. , RNET , MReader , and PhaseCond ( our proposed model ) .","[('leads to', (2, 4)), ('containing', (10, 11))]","[('different attention - based architecture', (5, 10)), ('two sequential phases', (11, 14)), ('question - aware passage representation phase', (15, 21)), ('evidence propagation phase.', (22, 25)), ('RNET', (26, 27)), ('MReader', (28, 29)), ('PhaseCond', (31, 32))]",[],[],[],[]
model,"Moreover , we observe several meaningful trends : a ) during the questionpassage attention phase , repeatedly attending the passage with the same question representation "" forces "" each passage word to become increasingly closer to the original question representation , and therefore increasing the number of layers has a risk of degrading the network performance , b ) during the self - attention phase , the self - attention 's alignment weights of the second layer become noticeably "" sharper "" than the first layer , suggesting the importance of fully propagating evidence through the passage itself .","[('observe', (3, 4)), ('during', (10, 11)), ('repeatedly attending', (16, 18)), ('with', (20, 21)), ('forces', (26, 27)), ('to become', (31, 33)), ('during', (59, 60)), ('of', (73, 74)), ('become', (77, 78)), ('than', (82, 83))]","[('several meaningful trends', (4, 7)), ('questionpassage attention phase', (12, 15)), ('passage', (19, 20)), ('same question representation', (22, 25)), ('each passage word', (28, 31)), ('increasingly closer', (33, 35)), ('original question representation', (37, 40)), ('self - attention phase', (61, 65)), (""self - attention 's alignment weights"", (67, 73)), ('second layer', (75, 77)), ('noticeably "" sharper ""', (78, 82)), ('first layer', (84, 86))]",[],[],[],[]
hyperparameters,"We use pre-trained GloVe 100 - dimensional word vectors , parts - of - speech tag features , named - entity tag feature , and binary features of exact matching which indicate if a passage word can be exactly matched to any question word and vice versa .","[('use', (1, 2)), ('of', (27, 28))]","[('pre-trained GloVe 100 - dimensional word vectors', (2, 9)), ('parts - of - speech tag features', (10, 17)), ('named - entity tag feature', (18, 23)), ('binary features', (25, 27)), ('exact matching', (28, 30))]",[],[],[],[]
hyperparameters,"Following , we also use question type ( what , how , who , when , which , where , why , be , and other ) features where each type is represented by a trainable embedding .","[('use', (4, 5)), ('where', (28, 29)), ('represented by', (32, 34))]","[('question type ( what , how , who , when , which , where , why , be , and other ) features', (5, 28)), ('each type', (29, 31)), ('trainable embedding', (35, 37))]",[],[],[],[]
hyperparameters,We use CNN with 100 one - dimensional filters with width 5 to encode character level embedding .,"[('use', (1, 2)), ('with', (3, 4)), ('with', (9, 10)), ('to encode', (12, 14))]","[('CNN', (2, 3)), ('100 one - dimensional filters', (4, 9)), ('width 5', (10, 12)), ('character level embedding', (14, 17))]",[],[],[],[]
hyperparameters,The hidden size is set as 128 for all the LSTM layers .,"[('set as', (4, 6)), ('for', (7, 8))]","[('hidden size', (1, 3)), ('128', (6, 7)), ('all the LSTM layers', (8, 12))]",[],[],[],[]
hyperparameters,Dropout are used for all the learnable parameters with a ratio as 0.2 .,"[('used for', (2, 4)), ('with', (8, 9)), ('as', (11, 12))]","[('Dropout', (0, 1)), ('all the learnable parameters', (4, 8)), ('ratio', (10, 11)), ('0.2', (12, 13))]",[],[],[],[]
hyperparameters,"We use the Adam optimizer ( Kingma & Ba , 2014 ) with an initial learning rate of 0.0006 , which is halved when a bad checkpoint is met .","[('use', (1, 2)), ('with', (12, 13)), ('of', (17, 18))]","[('Adam optimizer ( Kingma & Ba , 2014 )', (3, 12)), ('initial learning rate', (14, 17)), ('0.0006', (18, 19)), ('bad', (25, 26))]",[],[],[],[]
results,Single Model Ensemble Models Dev Set,[],[],[],[],[],[]
results,"The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .","[('of', (3, 4)), ('lower than', (9, 11))]","[('EM result', (1, 3)), ('our', (4, 5)), ('baseline Iterative Aligner', (5, 8)), ('RNET', (11, 12))]",[],[],[],[]
results,"For the question - passage attention phase , using single layer does n't degrade the performance significantly from the default setting of two layers , resulting in a different conclusion from ; .","[('For', (0, 1)), ('using', (8, 9)), (""does n't degrade"", (11, 14)), ('from', (17, 18)), ('of', (21, 22)), ('resulting in', (25, 27))]","[('question - passage attention phase', (2, 7)), ('single layer', (9, 11)), ('performance', (15, 16)), ('significantly', (16, 17)), ('default setting', (19, 21)), ('two layers', (22, 24))]",[],[],[],[]
experiments,ANALYSIS ON ATTENTION LAYERS,[],[],[],[],[],[]
experiments,"First , the first layer of the question - passage attention phase can successfully align question keywords with the corresponding passage keywords , as shown in .","[('of', (5, 6)), ('successfully align', (13, 15)), ('with', (17, 18))]","[('first layer', (3, 5)), ('question - passage attention phase', (7, 12)), ('question keywords', (15, 17)), ('corresponding passage keywords', (19, 22))]",[],[],[],[]
research-problem,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering,[],[],[],[],[],[]
research-problem,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .,[],"[('machine comprehension ( MC )', (15, 20)), ('question answering ( QA )', (21, 26))]",[],[],[],[]
dataset,The question - answer pairs are annotated through crowdsourcing .,"[('annotated through', (6, 8))]","[('question - answer pairs', (1, 5)), ('crowdsourcing', (8, 9))]",[],[],[],[]
model,The bi-directional attention flow ( BIDAF ) used the bi-directional attention to obtain a question - aware context representation .,"[('used', (7, 8)), ('to obtain', (11, 13))]","[('bi-directional attention flow ( BIDAF )', (1, 7)), ('bi-directional attention', (9, 11)), ('question - aware context representation', (14, 19))]",[],[],[],[]
model,"In this paper , we introduce syntactic information to encode questions with a specific form of recursive neural networks .","[('introduce', (5, 6)), ('to encode', (8, 10)), ('with', (11, 12))]","[('syntactic information', (6, 8)), ('questions', (10, 11)), ('specific form of recursive neural networks', (13, 19))]",[],[],[],[]
model,"More specifically , we explore a tree - structured LSTM which extends the linear - chain long short - term memory ( LSTM ) ] to a recursive structure , which has the potential to capture long - distance interactions over the structures .","[('explore', (4, 5)), ('extends', (11, 12)), ('to', (25, 26)), ('over', (40, 41))]","[('tree - structured LSTM', (6, 10)), ('linear - chain long short - term memory ( LSTM )', (13, 24)), ('recursive structure', (27, 29)), ('long - distance interactions', (36, 40)), ('structures', (42, 43))]",[],[],[],[]
experiments,Word embedding,[],[],[],[],[],[]
results,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,"[('use', (1, 2)), ('to initialize', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (2, 9)), ('our word embeddings', (11, 14))]",[],[],[],[]
hyperparameters,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"[('with', (12, 13))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('initialized randomly', (10, 12)), ('Gaussian samples', (13, 15))]",[],[],[],[]
hyperparameters,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .","[('is', (3, 4))]","[('CharCNN filter length', (0, 3)), ('1 , 3 , 5', (4, 9)), ('50 dimensions', (12, 14))]",[],[],[],[]
results,The cluster number K in discriminative block is 100 .,"[('in', (4, 5)), ('is', (7, 8))]","[('cluster number K', (1, 4)), ('discriminative block', (5, 7)), ('100', (8, 9))]",[],[],[],[]
hyperparameters,The Adam method is used for optimization .,"[('used for', (4, 6))]","[('Adam method', (1, 3)), ('optimization', (6, 7))]",[],[],[],[]
results,The initial learning rate is 0.0004 and the batch size is 32 .,"[('is', (4, 5))]","[('initial learning rate', (1, 4)), ('0.0004', (5, 6)), ('batch size', (8, 10)), ('32', (11, 12))]",[],[],[],[]
hyperparameters,"All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .","[('of', (3, 4)), ('are', (8, 9)), ('is', (19, 20))]","[('hidden states', (1, 3)), ('GRUs , and TreeLSTMs', (4, 8)), ('500 dimensions', (9, 11)), ('word - level embedding d w', (13, 19)), ('300 dimensions', (20, 22))]",[],[],[],[]
results,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .","[('set', (1, 2)), ('of', (4, 5)), ('to', (6, 7)), ('drop', (10, 11)), ('on', (18, 19))]","[('max length', (2, 4)), ('document', (5, 6)), ('500', (7, 8)), ('question - document pairs', (12, 16)), ('training set', (19, 21))]",[],[],[],[]
results,We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,"[('apply', (1, 2)), ('to', (3, 4)), ('with', (10, 11)), ('of', (14, 15))]","[('dropout', (2, 3)), ('Encoder layer and aggregation layer', (5, 10)), ('dropout rate', (12, 14)), ('0.5', (15, 16))]",[],[],[],[]
results,"Our model achieves a 68.73 % EM score and 77.39 % F1 score , which is ranked among the state of the art single models ( without model ensembling shows the ablation performances of various Q- code on the development set .","[('achieves', (2, 3)), ('ranked among', (16, 18))]","[('Our model', (0, 2)), ('68.73 % EM score', (4, 8)), ('77.39 % F1 score', (9, 13)), ('state of the art single models', (19, 25))]",[],[],[],[]
results,"Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .","[('using', (3, 4)), ('achieved', (7, 8))]","[('Our baseline model', (0, 3)), ('no Q- code', (4, 7)), ('68.00 % and 77.36 % EM and F 1 scores', (9, 19))]",[],[],[],[]
results,"When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .","[('added', (2, 3)), ('into', (10, 11)), ('improved slightly to', (18, 21))]","[('explicit question type T - code', (4, 10)), ('baseline model', (12, 14)), ('performance', (16, 17)), ('68.16 % ( EM )', (21, 26)), ('77.58 % ( F1 )', (27, 32))]",[],[],[],[]
results,"We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .","[('used', (2, 3)), ('introduce', (4, 5)), ('for', (7, 8)), ('consistently shows', (24, 26))]","[('TreeLSTM', (3, 4)), ('syntactic parses', (5, 7)), ('question representation and understanding', (8, 12))]",[],[],[],[]
results,"When letting the number of hidden question types ( K ) to be 20 , the performance improves to 68.73%/77.74 % on EM and F1 , respectively , which corresponds to the results of our model reported in .","[('letting', (1, 2)), ('to be', (11, 13)), ('on', (21, 22))]","[('number of hidden question types ( K )', (3, 11)), ('20', (13, 14)), ('performance', (16, 17)), ('improves', (17, 18)), ('68.73%/77.74 %', (19, 21)), ('EM and F1', (22, 25))]",[],[],[],[]
research-problem,Convolutional Neural Network Architectures for Matching Natural Language Sentences,[],[],[],[],[],[]
research-problem,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",[],"[('Semantic matching', (0, 2))]",[],[],[],[]
research-problem,Matching two potentially heterogenous language objects is central to many natural language applications .,[],"[('Matching two potentially heterogenous language objects', (0, 6)), ('central', (7, 8))]",[],[],[],[]
model,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .","[('generalizes', (1, 2)), ('of', (5, 6))]","[('conventional notion', (3, 5)), ('similarity', (6, 7))]",[],[],[],[]
model,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .","[('propose', (5, 6)), ('adapt', (12, 13)), ('to', (24, 25))]","[('deep neural network models', (6, 10)), ('convolutional strategy', (14, 16)), ('natural language', (25, 27))]",[],[],[],[]
model,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .","[('devise', (13, 14)), ('can naturally host', (18, 21)), ('for', (25, 26)), ('of', (35, 36)), ('with', (38, 39))]","[('novel model', (15, 17)), ('sentences', (26, 27)), ('simple - to - comprehensive fusion', (29, 35)), ('matching patterns', (36, 38)), ('same convolutional architecture', (40, 43))]",[],[],[],[]
experiments,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,"[('perform', (4, 5)), ('with', (6, 7)), ('can be easily', (16, 19)), ('with', (23, 24))]","[('better', (5, 6)), ('mini-batch ( 100 ? 200 in sizes )', (7, 15)), ('parallelized', (19, 20)), ('single machine', (21, 23)), ('multi-cores', (24, 25))]",[],[],[],[]
experiments,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .","[('For', (0, 1)), ('find that for', (4, 7)), ('is enough for', (12, 15)), ('with', (16, 17))]","[('regularization', (1, 2)), ('both architectures', (7, 9)), ('early stopping', (10, 12)), ('models', (15, 16)), ('medium size and large training sets ( with', (17, 25)), ('over 500K instances )', (25, 29))]",[],[],[],[]
hyperparameters,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .","[('use', (1, 2)), ('trained with', (7, 9))]","[('50 - dimensional word embedding', (2, 7)), ('Word2 Vec', (10, 12))]",[],[],[],[]
experiments,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .","[('use', (1, 2)), ('as', (3, 4)), ('for', (7, 8)), ('yields', (18, 19)), ('to', (23, 24))]","[('ReLu', (2, 3)), ('activation function', (5, 7)), ('all of models ( convolution and MLP )', (8, 16)), ('comparable or better results', (19, 23)), ('sigmoid - like functions', (24, 28)), ('converges', (30, 31)), ('faster', (31, 32))]",[],[],[],[]
baselines,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,"[('as', (9, 10)), ('of', (15, 16))]","[('WORDEMBED', (0, 1)), ('each short - text', (5, 9)), ('sum', (11, 12)), ('embedding', (14, 15)), ('words it contains', (17, 20))]",[],[],[],[]
baselines,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :","[('of', (3, 4)), ('calculated with', (9, 11)), ('with', (13, 14)), ('of', (16, 17)), ('train it on', (32, 35)), ('with', (37, 38)), ('in', (45, 46))]","[('matching score', (1, 3)), ('two short - texts', (4, 8)), ('MLP', (12, 13)), ('embedding', (15, 16)), ('two documents', (18, 20)), ('DEEPMATCH', (23, 24)), ('our datasets', (35, 37)), ('3 hidden layers', (38, 41)), ('1,000 hidden nodes', (42, 45)), ('first hidden layer', (47, 50)), ('URAE+ MLP', (51, 53))]",[],[],[],[]
baselines,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :","[('use', (1, 2)), ('to get', (6, 8)), ('of', (13, 14)), ('put', (18, 19)), ('on', (21, 22)), ('as in', (24, 26))]","[('Unfolding Recursive Autoencoder', (3, 6)), ('100 dimensional vector representation', (9, 13)), ('each sentence', (14, 16)), ('MLP', (20, 21)), ('top', (23, 24)), ('WORDEMBED', (26, 27)), ('SENNA + MLP / SIM', (28, 33))]",[],[],[],[]
baselines,We use the SENNA - type sentence model for sentence representation ;,"[('use', (1, 2)), ('for', (8, 9))]","[('SENNA - type sentence model', (3, 8)), ('sentence representation', (9, 11))]",[],[],[],[]
baselines,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .","[('take', (3, 4)), ('as', (7, 8)), ('with', (10, 11)), ('use', (18, 19)), ('to obtain', (21, 23))]","[('SENMLP', (0, 1)), ('whole sentence', (5, 7)), ('input', (8, 9)), ('word embedding aligned sequentially', (11, 15)), ('MLP', (20, 21)), ('score of coherence', (24, 27))]",[],[],[],[]
results,Experiment I : Sentence Completion,[],[],[],[],[],[]
results,"The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .","[('get', (4, 5)), ('of', (7, 8)), ('with', (13, 14)), ('over', (16, 17)), ('without', (22, 23))]","[('two proposed', (1, 3)), ('nearly half', (5, 7)), ('cases right', (9, 11)), ('large margin', (14, 16)), ('other sentence models', (17, 20)), ('explicit sequence modeling', (23, 26))]",[],[],[],[]
results,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .","[('showing', (9, 10))]","[('ARC - II', (0, 3)), ('outperforms', (3, 4)), ('ARC - I', (4, 7)), ('significantly', (7, 8))]",[],[],[],[]
results,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .","[('performs', (8, 9))]","[('SENNA + MLP', (5, 8)), ('fairly well', (9, 11))]",[],[],[],[]
results,"Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .","[('beats', (4, 5)), ('with', (7, 8))]","[('ARC', (1, 2)), ('other models', (5, 7)), ('large margins', (8, 10)), ('two convolutional', (12, 14))]",[],[],[],[]
research-problem,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes,[],"[('Scaling Memory - Augmented Neural Networks', (0, 6))]",[],[],[],[]
approach,"External memory allows MANNs to learn algorithmic solutions to problems that have eluded the capabilities of traditional LSTMs , and to generalize to longer sequence lengths .","[('allows', (2, 3)), ('to learn', (4, 6)), ('to', (8, 9)), ('to generalize', (20, 22)), ('to', (22, 23))]","[('External memory', (0, 2)), ('MANNs', (3, 4)), ('algorithmic solutions', (6, 8)), ('problems', (9, 10)), ('capabilities', (14, 15)), ('longer sequence lengths', (23, 26))]",[],[],[],[]
approach,"In this paper , we present a MANN named SAM ( sparse access memory ) .","[('present', (5, 6)), ('named', (8, 9))]","[('MANN', (7, 8)), ('SAM ( sparse access memory )', (9, 15))]",[],[],[],[]
approach,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .","[('thresholding', (1, 2)), ('to', (4, 5)), ('using', (10, 11)), ('for', (14, 15)), ('is', (23, 24)), ('in', (25, 26)), ('with respect to', (29, 32)), ('retaining', (36, 37))]","[('memory modifications', (2, 4)), ('sparse subset', (6, 8)), ('efficient data structures', (11, 14)), ('content - based read operations', (15, 20)), ('our model', (21, 23)), ('optimal', (24, 25)), ('space and time', (26, 29)), ('memory size', (32, 34)), ('end - to - end gradient based optimization', (37, 45))]",[],[],[],[]
approach,"This Sparse Differentiable Neural Computer ( SDNC ) is over 400 faster than the canonical dense variant fora memory size of 2,000 slots , and achieves the best reported result in the Babi tasks without supervising the memory access .","[('is', (8, 9)), ('than', (12, 13)), ('fora', (17, 18)), ('of', (20, 21)), ('achieves', (25, 26)), ('in', (30, 31)), ('without supervising', (34, 36))]","[('Sparse Differentiable Neural Computer ( SDNC )', (1, 8)), ('over 400 faster', (9, 12)), ('canonical dense variant', (14, 17)), ('memory size', (18, 20)), ('2,000 slots', (21, 23)), ('best reported result', (27, 30)), ('Babi tasks', (32, 34)), ('memory access', (37, 39))]",[],[],[],[]
results,Speed and memory benchmarks,[],[],[],[],[],[]
results,Question answering on the Babi tasks,[],[],[],[],[],[]
results,"The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .","[('except', (3, 4)), ('able to learn', (8, 11)), ('comparable to', (12, 14))]","[('MANNs', (1, 2)), ('NTM', (5, 6)), ('solutions', (11, 12)), ('previous best results', (15, 18)), ('failing', (19, 20)), ('only 2 of the tasks', (21, 26))]",[],[],[],[]
results,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .","[('manages to', (2, 4))]","[('SDNC', (1, 2)), ('solve', (4, 5)), ('all but 1 of the tasks', (5, 11))]",[],[],[],[]
results,"More directly comparable previous work with end - to - end memory networks , which did not use supervision , fails at 6 of the tasks .","[('with', (5, 6)), ('which did not use', (14, 18)), ('at', (21, 22))]","[('end - to - end memory networks', (6, 13)), ('supervision', (18, 19)), ('fails', (20, 21)), ('6 of the tasks', (22, 26))]",[],[],[],[]
results,"Both the sparse and dense perform comparably at this task , again indicating the sparse approximations do not impair learning .","[('perform', (5, 6))]","[('Both the sparse and dense', (0, 5)), ('comparably', (6, 7))]",[],[],[],[]
results,"SAM outperformed other models , presumably due to its much larger memory capacity .",[],"[('SAM', (0, 1)), ('outperformed', (1, 2)), ('other models', (2, 4))]",[],[],[],[]
results,"SAM is able to outperform other approaches , presumably because it can utilize a much larger memory .","[('able to', (2, 4))]","[('SAM', (0, 1)), ('outperform', (4, 5)), ('other approaches', (5, 7))]",[],[],[],[]
results,G Babi results,[],[],[],[],[],[]
results,"SDNC achieves the best reported result on this task with unsupervised memory access , solving all but 1 task .","[('achieves', (1, 2)), ('with', (9, 10)), ('solving', (14, 15))]","[('SDNC', (0, 1)), ('best reported result', (3, 6)), ('unsupervised memory access', (10, 13)), ('all but 1 task', (15, 19))]",[],[],[],[]
research-problem,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller,[],"[('Large - Scale Reading Comprehension', (2, 7))]",[],[],[],[]
research-problem,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .","[('propose', (5, 6)), ('to handle', (12, 14)), ('in', (19, 20))]","[('novel deep neural network architecture', (7, 12)), ('long - range dependency', (15, 19)), ('RC tasks', (20, 22))]",[],[],[],[]
research-problem,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,[],"[('Reading comprehension ( RC )', (0, 5))]",[],[],[],[]
model,"Inspired by these approaches , we develop a customized memory controller along with an external memory augmentation for complicated RC tasks .","[('develop', (6, 7)), ('along with', (11, 13)), ('for', (17, 18))]","[('customized memory controller', (8, 11)), ('external memory augmentation', (14, 17)), ('complicated RC tasks', (18, 21))]",[],[],[],[]
model,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,"[('extend', (1, 2)), ('with', (5, 6)), ('to alleviate', (9, 11))]","[('memory controller', (3, 5)), ('residual connection', (7, 9)), ('information distortion', (12, 14))]",[],[],[],[]
model,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,"[('expand', (2, 3)), ('with', (10, 11)), ('conveys', (15, 16)), ('to', (18, 19)), ('containing', (22, 23)), ('as well as', (25, 28))]","[('gated recurrent unit ( GRU )', (4, 10)), ('dense connection', (12, 14)), ('enriched features', (16, 18)), ('next layer', (20, 22)), ('original', (24, 25)), ('transformed information', (29, 31))]",[],[],[],[]
model,( 1 ) We propose an extended memory controller module for RC tasks .,"[('propose', (4, 5)), ('for', (10, 11))]","[('extended memory controller module', (6, 10)), ('RC tasks', (11, 13))]",[],[],[],[]
experiments,NLTK is used for tokenizing words .,"[('used for', (2, 4))]","[('NLTK', (0, 1)), ('tokenizing words', (4, 6))]",[],[],[],[]
experiments,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .","[('In', (0, 1)), ('use', (6, 7)), ('set to', (20, 22)), ('initialized as', (27, 29))]","[('memory controller', (2, 4)), ('four read heads', (7, 10)), ('one write head', (11, 14)), ('memory size', (17, 19)), ('100 36', (22, 24)), ('0', (29, 30))]",[],[],[],[]
experiments,The hidden vector dimension l is set to 200 .,"[('set to', (6, 8))]","[('hidden vector dimension l', (1, 5)), ('200', (8, 9))]",[],[],[],[]
experiments,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .","[('use', (1, 2)), ('as', (8, 9)), ('with', (11, 12)), ('of', (15, 16))]","[('AdaDelta ( Zeiler , 2012 )', (2, 8)), ('optimizer', (10, 11)), ('learning rate', (13, 15)), ('0.5', (16, 17))]",[],[],[],[]
experiments,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,"[('set to', (4, 6)), ('for', (7, 8))]","[('batch size', (1, 3)), ('20', (6, 7)), ('TriviaQA', (8, 9)), ('30', (10, 11)), ('SQuAD', (12, 13)), ('QUASAR', (14, 15))]",[],[],[],[]
experiments,We use an exponential moving average of weights with a decaying factor of 0.001 .,"[('use', (1, 2)), ('of', (6, 7)), ('with', (8, 9)), ('of', (12, 13))]","[('exponential moving average', (3, 6)), ('weights', (7, 8)), ('decaying factor', (10, 12)), ('0.001', (13, 14))]",[],[],[],[]
results,"For our quantitative comparisons , we use BiDAF with self attention ) as a baseline , which maintains the best results published on both TriviaQA and SQuAD datasets .","[('use', (6, 7)), ('as', (12, 13)), ('maintains', (17, 18)), ('published on', (21, 23))]","[('BiDAF with self attention', (7, 11)), ('best results', (19, 21)), ('TriviaQA and SQuAD datasets', (24, 28))]",[],[],[],[]
results,"In TriviaQA and QUASAR - T dataset , we compare our model with BiDAF as well as its variant called ' BiDAF + DNC , ' which is augmented with an existing external memory architecture just before the answer prediction layer in the BiDAF .","[('compare', (9, 10)), ('called', (19, 20)), ('augmented with', (28, 30)), ('just before', (35, 37)), ('in', (41, 42))]","[('TriviaQA and QUASAR - T dataset', (1, 7)), ('BiDAF', (13, 14)), ('BiDAF + DNC', (21, 24)), ('existing external memory architecture', (31, 35)), ('answer prediction layer', (38, 41)), ('BiDAF', (43, 44))]",[],[],[],[]
results,"Overall , in lengthy - document cases such as Trivi aQA and QUASAR - T , our model outperforms all the published results , as seen in Tables 2 and 3 , while in the short - document case such as SQuAD , we mostly achieve the best results , as seen in .","[('in', (2, 3)), ('such as', (7, 9)), ('achieve', (45, 46))]","[('lengthy - document cases', (3, 7)), ('Trivi aQA', (9, 11)), ('our model', (16, 18)), ('outperforms', (18, 19)), ('all the published results', (19, 23)), ('short - document case', (35, 39)), ('best results', (47, 49))]",[],[],[],[]
results,TriviaQA .,[],"[('TriviaQA', (0, 1))]",[],[],[],[]
results,"As shown in , our model , even without DEBS , outperforms the existing state - of - the - art method such as ' BiDAF + SA + SN ' by a large margin in all the cases .","[('such as', (22, 24)), ('by', (31, 32))]","[('our model', (4, 6)), ('outperforms', (11, 12)), ('existing state - of - the - art method', (13, 22)), (""BiDAF + SA + SN '"", (25, 31)), ('large margin', (33, 35))]",[],[],[],[]
results,"Our model with DEBS , which replaces BiGRU encoder blocks , performs even better than that without it in all the cases except for the combination of the ' full ' and ' Wikipedia ' case , which involves documents containing no relevant information for the answer .","[('replaces', (6, 7)), ('performs', (11, 12)), ('than', (14, 15)), ('except for', (22, 24))]","[('Our model with', (0, 3)), ('DEBS', (3, 4)), ('BiGRU encoder blocks', (7, 10)), ('even better', (12, 14)), ('all the cases', (19, 22)), ('combination of', (25, 27)), (""' full ' and ' Wikipedia ' case"", (28, 36))]",[],[],[],[]
results,"We note that our method achieves these outstanding results without any additional features. , our simple baseline ' BiDAF + DNC , ' which involves an existing memory architecture , improves performance over BiDAF , indicating the efficacy of incorporating an external memory .","[('note', (1, 2)), ('achieves', (5, 6)), ('involves', (24, 25)), ('over', (32, 33))]","[('our method', (3, 5)), ('outstanding results', (7, 9)), ('BiDAF + DNC', (18, 21)), ('existing memory architecture', (26, 29)), ('improves', (30, 31)), ('performance', (31, 32)), ('BiDAF', (33, 34))]",[],[],[],[]
results,"Moreover , our model with the proposed memory controller achieves significantly better results compared to other models .","[('with', (4, 5)), ('achieves', (9, 10)), ('compared to', (13, 15))]","[('our model', (2, 4)), ('proposed memory controller', (6, 9)), ('significantly better results', (10, 13)), ('other models', (15, 17))]",[],[],[],[]
ablation-analysis,"First , we adopt ELMo to our model ( without DEBS ) , which uses word embedding as the weighted sum of the hidden layers of a language model with regularization as an additional feature to our word embeddings .","[('adopt', (3, 4)), ('to', (5, 6)), ('uses', (14, 15)), ('as', (17, 18)), ('with', (29, 30))]","[('ELMo', (4, 5)), ('our model ( without DEBS )', (6, 12)), ('word embedding', (15, 17)), ('weighted', (19, 20)), ('regularization', (30, 31)), ('our word embeddings', (36, 39))]",[],[],[],[]
results,"This improves the F 1 score of our model up to 85.13 and EM to 77. 44 , showing the highest performances among all the methods without using self attention .","[('improves', (1, 2)), ('of', (6, 7)), ('up to', (9, 11))]","[('F 1 score', (3, 6)), ('our model', (7, 9)), ('85.13', (11, 12)), ('EM', (13, 14)), ('77. 44', (15, 17))]",[],[],[],[]
results,"Due to the relatively short document length in SQuAD compared to TriviaQA and QUASAR - T , our model without DEBS performs worse than the baseline ' BiDAF + Self Attention + ELMo. '","[('without', (19, 20)), ('performs', (21, 22)), ('than', (23, 24))]","[('our model', (17, 19)), ('DEBS', (20, 21)), ('worse', (22, 23)), ('baseline', (25, 26)), ('BiDAF + Self Attention + ELMo.', (27, 33))]",[],[],[],[]
results,QUASAR - T .,[],[],[],[],[],[]
results,"As can be seen in , using DEBS in all the places improves the performance most , and furthermore , the memory controller with DEBS gives the largest performance margin .","[('using', (6, 7)), ('in', (8, 9)), ('improves', (12, 13)), ('with', (23, 24)), ('gives', (25, 26))]","[('DEBS', (7, 8)), ('all the places', (9, 12)), ('performance', (14, 15)), ('most', (15, 16)), ('memory controller', (21, 23)), ('DEBS', (24, 25)), ('largest performance margin', (27, 30))]",[],[],[],[]
research-problem,Sentence Similarity Learning by Lexical Decomposition and Composition,[],"[('Sentence Similarity Learning', (0, 3))]",[],[],[],[]
research-problem,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .","[('to determine', (12, 14)), ('are', (17, 18))]","[('paraphrase identification', (4, 6)), ('sentence similarity', (8, 10)), ('two sentences', (15, 17)), ('paraphrases', (18, 19))]",[],[],[],[]
research-problem,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,[],"[('paraphrase identification', (1, 3))]",[],[],[],[]
hyperparameters,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .","[('set', (5, 6)), ('of', (8, 9)), ('as', (12, 13)), ('pre-train', (18, 19)), ('with', (21, 22)), ('on', (26, 27))]","[('size', (7, 8)), ('word vector dimension', (9, 12)), ('d = 300', (13, 16)), ('vectors', (20, 21)), ('word2 vec toolkit', (23, 26)), ('English Gigaword ( LDC2011T07 )', (28, 33))]",[],[],[],[]
research-problem,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding,[],[],[],[],[],[]
model,"The vector is then used for various downstream tasks , e.g. , sentiment analysis , natural language inference , etc .","[('used for', (4, 6)), ('e.g.', (10, 11))]","[('vector', (1, 2)), ('various downstream tasks', (6, 9)), ('sentiment analysis', (12, 14)), ('natural language inference', (15, 18))]",[],[],[],[]
model,Self - attention computes attention weights by the inner product between words and the learnable weight vector .,"[('computes', (3, 4)), ('by', (6, 7)), ('between', (10, 11))]","[('Self - attention', (0, 3)), ('attention weights', (4, 6)), ('inner product', (8, 10)), ('words and the learnable weight vector', (11, 17))]",[],[],[],[]
model,"The weight vector is important in that it detects informative words , yet it is static during inference .","[('detects', (8, 9)), ('is', (14, 15)), ('during', (16, 17))]","[('weight vector', (1, 3)), ('informative words', (9, 11)), ('static', (15, 16)), ('inference', (17, 18))]",[],[],[],[]
model,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .","[('propose', (7, 8)), ('for', (14, 15)), ('namely', (18, 19))]","[('new self - attention mechanism', (9, 14)), ('sentence embedding', (15, 17)), ('Dynamic Self - Attention ( DSA )', (19, 26))]",[],[],[],[]
model,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .","[('modify', (5, 6)), ('such that', (8, 10)), ('functions as', (11, 13)), ('with', (16, 17))]","[('dynamic routing', (6, 8)), ('self - attention', (13, 16)), ('dynamic weight vector', (18, 21))]",[],[],[],[]
model,"We design and implement Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .","[('design and implement', (1, 4)), ('for', (18, 19))]","[('Dynamic Self - Attention ( DSA )', (4, 11)), ('sentence embedding', (19, 21))]",[],[],[],[]
model,We devise the dynamic weight vector with which DSA computes attention weights .,"[('devise', (1, 2)), ('with', (6, 7)), ('computes', (9, 10))]","[('dynamic weight vector', (3, 6)), ('DSA', (8, 9)), ('attention weights', (10, 12))]",[],[],[],[]
baselines,"We implement single DSA , multiple DSA and self - attention in Eq. 1 as a baseline .","[('implement', (1, 2))]","[('single DSA', (2, 4)), ('multiple DSA', (5, 7)), ('self - attention', (8, 11))]",[],[],[],[]
baselines,Both DSA and self - attention are stacked on CNN with Dense Connection for fair comparison .,"[('stacked on', (7, 9)), ('for', (13, 14))]","[('DSA and self - attention', (1, 6)), ('CNN with Dense Connection', (9, 13))]",[],[],[],[]
hyperparameters,"For our implementations , we initialize word embeddings by 300D Glo Ve 840B pretrained vectors , and fix them during training .","[('initialize', (5, 6)), ('by', (8, 9)), ('fix them during', (17, 20))]","[('word embeddings', (6, 8)), ('300D Glo Ve 840B pretrained vectors', (9, 15)), ('training', (20, 21))]",[],[],[],[]
hyperparameters,We use cross-entropy loss as an objective function for both tasks .,"[('use', (1, 2)), ('as', (4, 5))]","[('cross-entropy loss', (2, 4)), ('objective function', (6, 8)), ('both tasks', (9, 11))]",[],[],[],[]
experiments,"We set do = 600 , m = 1 for single DSA and do = 300 , m = 8 for multiple DSA .","[('set', (1, 2)), ('for', (9, 10))]","[('do = 600', (2, 5)), ('single DSA', (10, 12))]",[],[],[],[]
experiments,Natural Language Inference Results,[],[],[],[],[],[]
results,"With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .","[('With', (0, 1)), ('in terms of', (2, 5)), ('outperforms', (14, 15)), ('by', (17, 18))]","[('tradeoffs', (1, 2)), ('parameters and learning time per epoch', (5, 11)), ('multiple DSA', (12, 14)), ('other models', (15, 17)), ('large margin ( + 1.1 % )', (19, 26))]",[],[],[],[]
results,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .","[('In comparison to', (0, 3)), ('shows', (8, 9)), ('than', (11, 12))]","[('single DSA', (6, 8)), ('better performance', (9, 11)), ('self - attention', (12, 15))]",[],[],[],[]
results,"Note that our implementation of the baseline , selfattention stacked on CNN with Dense Connection , shows better performance ( + 0.4 % ) than the one stacked on BiLSTM .","[('of', (4, 5)), ('stacked on', (9, 11)), ('with', (12, 13)), ('shows', (16, 17)), ('than', (24, 25)), ('stacked on', (27, 29))]","[('our implementation', (2, 4)), ('baseline', (6, 7)), ('selfattention', (8, 9)), ('CNN', (11, 12)), ('Dense Connection', (13, 15)), ('better performance ( + 0.4 % )', (17, 24)), ('BiLSTM', (29, 30))]",[],[],[],[]
results,Sentiment Analysis Results,[],[],[],[],[],[]
results,"Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .","[('in', (7, 8)), ('achieves', (14, 15)), ('in', (17, 18))]","[('Single DSA', (0, 2)), ('outperforms', (2, 3)), ('all the baseline models', (3, 7)), ('SST - 2 dataset', (8, 12)), ('comparative results', (15, 17)), ('SST - 5', (18, 21))]",[],[],[],[]
results,"In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .","[('in', (6, 7)), ('in', (15, 16)), ('in', (22, 23)), ('between', (25, 26))]","[('SNLI dataset', (7, 9)), ('SST dataset', (16, 18)), ('performance', (24, 25)), ('DSA and the previous self - attentive models', (26, 34))]",[],[],[],[]
research-problem,Teaching Machines to Read and Comprehend,[],[],[],[],[],[]
research-problem,Teaching machines to read natural language documents remains an elusive challenge .,[],"[('Teaching machines to read natural language documents', (0, 7))]",[],[],[],[]
research-problem,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",[],"[('Machine reading', (0, 2))]",[],[],[],[]
model,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,"[('directly', (6, 7)), ('by introducing', (16, 18)), ('to building', (21, 23))]","[('lack of real natural language training data', (9, 16)), ('novel approach', (19, 21)), ('supervised reading comprehension data set', (24, 29))]",[],[],[],[]
dataset,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,"[('collected', (5, 6)), ('of', (9, 10)), ('with', (15, 16)), ('from', (18, 19))]","[('two new corpora', (6, 9)), ('roughly a million news stories', (10, 15)), ('associated queries', (16, 18)), ('CNN and Daily Mail websites', (20, 25))]",[],[],[],[]
model,"Here we propose a methodology for creating real - world , large scale supervised training data for learning reading comprehension models .",[],"[('learning', (17, 18)), ('reading comprehension models', (18, 21))]",[],[],[],[]
model,"Inspired by work in summarisation , we create two machine reading corpora by exploiting online newspaper articles and their matching summaries .","[('create', (7, 8)), ('by exploiting', (12, 14))]","[('two machine reading corpora', (8, 12)), ('online newspaper articles', (14, 17)), ('matching summaries', (19, 21))]",[],[],[],[]
experiments,We tune the maximum penalty per word ( m = 8 ) on the validation data .,"[('tune', (1, 2)), ('on', (12, 13))]","[('maximum penalty per word ( m = 8 )', (3, 12)), ('validation data', (14, 16))]",[],[],[],[]
results,Word distance benchmark,[],[],[],[],[],[]
research-problem,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention,[],"[('Natural Language Inference', (1, 4))]",[],[],[],[]
research-problem,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .","[('proposed', (5, 6)), ('for recognizing', (12, 14))]","[('sentence encoding - based model', (7, 12)), ('text entailment', (14, 16))]",[],[],[],[]
research-problem,"With less number of parameters , our model outperformed the existing best sentence encoding - based approach by a large margin .","[('With', (0, 1)), ('by', (17, 18))]","[('less number of parameters', (1, 5)), ('our model', (6, 8)), ('outperformed', (8, 9)), ('existing best sentence encoding - based approach', (10, 17)), ('large margin', (19, 21))]",[],[],[],[]
research-problem,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",[],"[('recognizing text entailment ( RTE )', (9, 15))]",[],[],[],[]
approach,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .","[('proposed', (5, 6)), ('for recognizing', (11, 13))]","[('unified deep learning framework', (7, 11)), ('textual entailment', (13, 15))]",[],[],[],[]
approach,The basic model is based on building biL - STM models on both premises and hypothesis .,"[('based on', (4, 6))]","[('building biL - STM models', (6, 11))]",[],[],[],[]
hyperparameters,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .","[('is', (6, 7)), ('use', (14, 15)), ('with', (17, 18)), ('for', (27, 28))]","[('training objective', (1, 3)), ('cross - entropy loss', (7, 11)), ('minibatch SGD', (15, 17)), ('Rmsprop', (19, 20)), ('optimization', (28, 29))]",[],[],[],[]
hyperparameters,The batch size is 128 .,"[('is', (3, 4))]","[('batch size', (1, 3)), ('128', (4, 5))]",[],[],[],[]
hyperparameters,A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,"[('applied in', (4, 6)), ('of', (8, 9)), ('with', (11, 12)), ('set to', (15, 17))]","[('dropout layer', (1, 3)), ('output', (7, 8)), ('network', (10, 11)), ('dropout rate', (13, 15)), ('0.25', (17, 18))]",[],[],[],[]
hyperparameters,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .","[('used', (5, 6)), ('to initialize', (11, 13))]","[('pretrained 300D Glove 840B vectors', (6, 11)), ('word embedding', (14, 16))]",[],[],[],[]
hyperparameters,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .","[('in', (6, 7)), ('by sampling', (13, 15)), ('from', (17, 18))]","[('Out - of - vocabulary words', (0, 6)), ('training set', (8, 10)), ('randomly initialized', (11, 13)), ('values', (15, 16)), ('uniformly', (16, 17)), ('( 0.05 , 0.05 )', (18, 23))]",[],[],[],[]
ablation-analysis,"2 . Keep their representation stays close to unseen similar words in inference time , which improved the model 's generation ability .","[('Keep', (2, 3)), ('stays close to', (5, 8)), ('in', (11, 12)), ('improved', (16, 17))]","[('representation', (4, 5)), ('unseen similar words', (8, 11)), ('inference time', (12, 14)), ('model', (18, 19))]",[],[],[],[]
code,"This baseline has been submitted to the official NQ leaderboard . Code , preprocessed data and pretrained model are available . https://ai.google.com/research/NaturalQuestions https://github.com/google-research/language/tree/",[],[],[],[],[],[]
approach,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .","[('in', (16, 17)), ('into', (33, 34)), ('by using', (37, 39)), ('of', (41, 42)), ('at', (69, 70)), ('to create', (72, 74)), ('at', (90, 91)), ('to predict', (93, 95))]","[('short and long answers', (12, 16)), ('single model', (18, 20)), ('pipeline approach', (24, 26)), ('each document', (31, 33)), ('multiple training instances', (34, 37)), ('overlapping windows', (39, 41)), ('tokens', (42, 43)), ('aggressively downsample', (58, 60)), ('null instances ( i.e. instances without an answer )', (60, 69)), ('training time', (70, 72)), ('balanced training set', (75, 78)), ('training time', (91, 93))]",[],[],[],[]
approach,We refer to our model as BERT joint to emphasize the fact that we are modeling short and long answers in a single model rather than in a pipeline of two models .,"[('as', (5, 6)), ('in', (20, 21)), ('rather than', (24, 26))]","[('BERT joint', (6, 8)), ('single model', (22, 24))]",[],[],[],[]
hyperparameters,We initialized our model from a BERT model already finetuned on SQ u AD 1.1 .,"[('initialized', (1, 2)), ('from', (4, 5)), ('finetuned on', (9, 11))]","[('our model', (2, 4)), ('BERT model', (6, 8)), ('SQ u AD 1.1', (11, 15))]",[],[],[],[]
hyperparameters,"We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .","[('trained', (1, 2)), ('by', (4, 5)), ('with', (11, 12)), ('with', (22, 23)), ('of', (26, 27))]","[('model', (3, 4)), ('minimizing', (5, 6)), ('loss L', (6, 8)), ('Adam optimizer', (13, 15)), ('batch size', (24, 26)), ('8', (27, 28))]",[],[],[],[]
results,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,"[('for', (3, 4)), ('performs', (5, 6)), ('than', (8, 9))]","[('Our BERT model', (0, 3)), ('NQ', (4, 5)), ('dramatically better', (6, 8)), ('original NQ paper', (14, 17))]",[],[],[],[]
