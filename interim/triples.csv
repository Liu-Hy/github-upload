info_unit,text,phrases,triple_A,triple_B,triple_C,triple_D,triple_E,SPEC_1,SPEC_2,SPEC_3,SPEC_4
research-problem,Learning Phrase Representations using RNN Encoder - Decoder for Statistical Machine Translation,Statistical Machine Translation,,,,,"Contribution||has research problem||Statistical Machine Translation
",,,,
research-problem,"Along this line of research on using neural networks for SMT , this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase - based SMT system .","SMT
phrase - based SMT",,,,,"Contribution||has research problem||SMT
Contribution||has research problem||phrase - based SMT
",,,,
model,"The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .","neural network architecture
refer to as
RNN Encoder - Decoder
consists of
two recurrent neural networks ( RNN )
act as
encoder
decoder","neural network architecture||refer to as||RNN Encoder - Decoder
neural network architecture||consists of||two recurrent neural networks ( RNN )
two recurrent neural networks ( RNN )||act as||encoder
two recurrent neural networks ( RNN )||act as||decoder
",,,"Model||has||neural network architecture
",,,,,
model,"The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .","maps
a variable - length source sequence to a fixed - length vector
maps
the vector representation back to a variable - length target sequence",,,,,,"encoder||maps||a variable - length source sequence to a fixed - length vector
decoder||maps||the vector representation back to a variable - length target sequence
",,,
model,The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .,"trained
jointly
to maximize
conditional probability
of
the target sequence given a source sequence","jointly||to maximize||conditional probability
conditional probability||of||the target sequence given a source sequence
",,,,,"two recurrent neural networks ( RNN )||trained||jointly
",,,
model,"Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .","propose
sophisticated hidden unit
to improve
both the memory capacity and the ease of training","sophisticated hidden unit||to improve||both the memory capacity and the ease of training
",,"Model||propose||sophisticated hidden unit
",,,,,,
baselines,The baseline phrase - based SMT system was built using Moses with default settings .,"built using
Moses
with
default settings","Moses||with||default settings
",,"Baselines||built using||Moses
",,,,,,
results,"As expected , adding features computed by neural networks consistently improves the performance over the baseline performance .","adding features
computed by
neural networks
improves the performance
over the baseline performance","adding features||computed by||neural networks
",,"Results||improves the performance||adding features
Results||improves the performance||over the baseline performance
",,,,,,
results,The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder - Decoder .,"best performance was achieved
used both CSLM and the phrase scores
from
RNN Encoder - Decoder","used both CSLM and the phrase scores||from||RNN Encoder - Decoder
",,"Results||best performance was achieved||used both CSLM and the phrase scores
",,,,,,
research-problem,Neural Machine Translation in Linear Time,,,,,,,,,,
research-problem,The ByteNet decoder attains state - of - the - art performance on character - level language modelling and outperforms the previous best results obtained with recurrent networks .,character - level language modelling,,,,,"Contribution||has research problem||character - level language modelling
",,,,
research-problem,"The ByteNet also achieves state - of - the - art performance on character - to - character machine translation on the English - to - German WMT translation task , surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time .",character - to - character machine translation,,,,,"Contribution||has research problem||character - to - character machine translation
",,,,
research-problem,"In neural language modelling , a neural network estimates a distribution over sequences of words or characters that belong to a given language .",neural language modelling,,,,,"Contribution||has research problem||neural language modelling
",,,,
research-problem,"In neural machine translation , the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language .",neural machine translation,,,,,"Contribution||has research problem||neural machine translation
",,,,
,The ByteNet is the instance within this family of models that uses one - dimensional convolutional neural networks ( CNN ) of fixed depth for both the encoder and the decoder ) .,"ByteNet
of
uses
one - dimensional convolutional neural networks ( CNN )
fixed depth
for
both the encoder and the decoder",,,,,,,,,
,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,"two CNNs
use
increasing factors of dilation
rapidly grow
receptive fields",,,,,,,,,
,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .,"convolutions
in
decoder CNN
masked
prevent the network from seeing future tokens
target sequence",,,,,,,,,
model,The network has beneficial computational and learning properties .,beneficial computational and learning properties,,,,"Model||has||beneficial computational and learning properties
",,,,,
model,"From a computational perspective , the network has a running time that is linear in the length of the source and target sequences ( up to a constant c ?","From
computational perspective
running time
linear in the length of the source and target sequences","computational perspective||running time||linear in the length of the source and target sequences
",,"Model||From||computational perspective
",,,,,,
model,"From a learning perspective , the representation of the source sequence in the ByteNet is resolution preserving ; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder .","learning perspective
representation of the source sequence
resolution preserving","learning perspective||representation of the source sequence||resolution preserving
",,,"Model||From||learning perspective
",,,,,
experiments,Character Prediction,Character Prediction,,,,,,,,"Tasks||has||Character Prediction
","Character Prediction||has||Hyperparameters
"
experiments,"The ByteNet Decoder that we use for the result has 30 residual blocks split into six sets of five blocks each ; for the five blocks in each set the dilation rates are , respectively , 1 , 2 , 4 , 8 and 16 .","ByteNet Decoder
for
30 residual blocks
split into
six sets of five blocks
five blocks
dilation rates
1 , 2 , 4 , 8 and 16","ByteNet Decoder||for||five blocks
five blocks||dilation rates||1 , 2 , 4 , 8 and 16
30 residual blocks||split into||six sets of five blocks
","ByteNet Decoder||has||30 residual blocks
",,,,,,"Hyperparameters||has||ByteNet Decoder
",
experiments,The masked kernel has size 3 .,"masked kernel
size 3",,,,,,"Hyperparameters||masked kernel||size 3
",,,
experiments,The number of hidden units dis 512 .,"number of hidden units
dis 512",,,,,,"Hyperparameters||number of hidden units||dis 512
",,,
experiments,For the optimization we use Adam with a learning rate of 0.0003 and a weight decay term of 0.0001 .,"optimization
Adam
learning rate
0.0003
weight decay
0.0001","Adam||learning rate||0.0003
Adam||weight decay||0.0001
",,,,,"Hyperparameters||optimization||Adam
",,,
experiments,We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 .,"apply
dropout
to
ReLU layer before the softmax dropping units
with
probability of 0.1","dropout||to||ReLU layer before the softmax dropping units
ReLU layer before the softmax dropping units||with||probability of 0.1
",,,,,"Hyperparameters||apply||dropout
",,,
experiments,"At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters .","sample
batch of sequences
of
500 characters each
use
first 100 characters
as
the minimum context
predict
latter 400 characters","batch of sequences||of||500 characters each
500 characters each||use||first 100 characters
first 100 characters||as||the minimum context
500 characters each||predict||latter 400 characters
",,,,,"Hyperparameters||sample||batch of sequences
",,,
experiments,The ByteNet decoder achieves 1.31 bits / character on the test set .,"achieves
1.31 bits / character
on
test set","test set||achieves||1.31 bits / character
",,,,,"Results||on||test set
",,,
experiments,Character - Level Machine Translation,Character - Level Machine Translation,,,,,,,,"Tasks||has||Character - Level Machine Translation
","Character - Level Machine Translation||has||Hyperparameters
"
experiments,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder .,"ByteNet
in
30 residual blocks
in the encoder
in the decoder","30 residual blocks||in||in the encoder
30 residual blocks||in||in the decoder
","ByteNet||has||30 residual blocks
",,,,,,"Hyperparameters||has||ByteNet
",
experiments,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .","residual blocks
arranged in sets of five
with
dilation rates
of
1 , 2 , 4 , 8 and 16","arranged in sets of five||with||dilation rates
dilation rates||of||1 , 2 , 4 , 8 and 16
",,,,,"Hyperparameters||residual blocks||arranged in sets of five
",,,
experiments,For this task we use the residual blocks with ReLUs ( .,"use
residual blocks
with
ReLUs","residual blocks||with||ReLUs
",,,,,"Hyperparameters||use||residual blocks
",,,
experiments,The number of hidden units dis 800 .,"hidden units
dis 800",,,,,,"Hyperparameters||hidden units||dis 800
",,,
experiments,"The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 .","size of the kernel
in
source network
3
size of the masked kernel
target network
3","target network||size of the masked kernel||3
source network||size of the kernel||3
",,,,,"Hyperparameters||in||target network
Hyperparameters||in||source network
",,,
experiments,For the optimization we use Adam with a learning rate of 0.0003 .,"optimization
use
Adam
with
learning rate of 0.0003","optimization||use||Adam
Adam||with||learning rate of 0.0003
",,,,,,,"Hyperparameters||has||optimization
",
experiments,Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) .,"sentence
padded with
special characters
nearest greater multiple
50","sentence||padded with||special characters
special characters||nearest greater multiple||50
",,,,,,,"Hyperparameters||has||sentence
",
experiments,Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training .,"pair of sentences
mapped
to
bucket
based on
pair of padded lengths
for
efficient batching during training","pair of sentences||to||bucket
bucket||based on||pair of padded lengths
pair of padded lengths||for||efficient batching during training
",,,,,"Hyperparameters||mapped||pair of sentences
",,,
experiments,We use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end -of - sentence token .,"vanilla beam search
according to
total likelihood of the generated candidate
accept
candidates which end in a end -of - sentence token","vanilla beam search||according to||total likelihood of the generated candidate
vanilla beam search||accept||candidates which end in a end -of - sentence token
",,,,,,,"Hyperparameters||use||vanilla beam search
",
experiments,We use a beam of size 12 .,"beam
of
size 12","beam||of||size 12
",,,,,,,"Hyperparameters||use||beam
",
experiments,"On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces .","On
NewsTest 2014
achieves
highest performance
in
character - level and subword - level neural machine translation
compared to
word - level systems
is
second
to
version of GNMT
uses
word - pieces","NewsTest 2014||achieves||highest performance
highest performance||in||character - level and subword - level neural machine translation
NewsTest 2014||is||second
second||compared to||word - level systems
word - level systems||to||version of GNMT
version of GNMT||uses||word - pieces
",,,,,"Results||On||NewsTest 2014
",,,
experiments,"On NewsTest 2015 , to our knowledge , ByteNet achieves the best published results to date .","NewsTest 2015
achieves
best published results to date","NewsTest 2015||achieves||best published results to date
",,,,,,,"Results||On||NewsTest 2015
",
research-problem,"We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .",attention mechanisms,,,,,"Contribution||has research problem||attention mechanisms
",,,,
research-problem,Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .,push the boundaries of recurrent language models and encoder - decoder architectures,,,,,"Contribution||has research problem||push the boundaries of recurrent language models and encoder - decoder architectures
",,,,
research-problem,"In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .",model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output,,,,,"Contribution||has research problem||model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output
",,,,
model,Encoder and Decoder Stacks,Encoder and Decoder Stacks,,,,"Model||has||Encoder and Decoder Stacks
",,,,,"Encoder and Decoder Stacks||has||Encoder
"
model,Encoder :,Encoder,,,,,,,,,
model,The encoder is composed of a stack of N = 6 identical layers .,"composed of
stack of N = 6 identical layers",,,,,,"Encoder||composed of||stack of N = 6 identical layers
",,,"stack of N = 6 identical layers||has||two sub-layers
"
model,Each layer has two sub-layers .,two sub-layers,,,,,,,,,
model,"The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .","first
multi-head self - attention mechanism
second
simple , positionwise fully connected feed - forward network",,,,,,"two sub-layers||first||multi-head self - attention mechanism
two sub-layers||second||simple , positionwise fully connected feed - forward network
",,,
model,"We employ a residual connection around each of the two sub-layers , followed by layer normalization .","employ
residual connection
around
two sub-layers
followed by
layer normalization","residual connection||around||two sub-layers
two sub-layers||followed by||layer normalization
",,,,,"Encoder||employ||residual connection
",,,
model,Decoder :,Decoder,,,,,,,,"Encoder and Decoder Stacks||has||Decoder
",
model,The decoder is also composed of a stack of N = 6 identical layers .,"composed of
stack of N = 6 identical layers",,,,,,"Decoder||composed of||stack of N = 6 identical layers
",,,
model,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .","inserts
third sub - layer
performs
multi-head attention
over
output of the encoder stack","third sub - layer||performs||multi-head attention
multi-head attention||over||output of the encoder stack
",,,,,"Decoder||inserts||third sub - layer
",,,
model,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .","employ
residual connections
around
each of the sub-layers
followed by
layer normalization","residual connections||around||each of the sub-layers
each of the sub-layers||followed by||layer normalization
",,,,,"Decoder||employ||residual connections
",,,
model,We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .,"modify
self - attention sub - layer
in
decoder stack
prevent
positions from attending to subsequent positions","self - attention sub - layer||in||decoder stack
decoder stack||prevent||positions from attending to subsequent positions
",,,,,"Decoder||modify||self - attention sub - layer
",,,
model,Attention,Attention,,,,"Model||has||Attention
",,,,,
model,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .","described as
mapping a query and a set of key - value pairs
to
output
where","mapping a query and a set of key - value pairs||to||output
",,,,,"Attention||described as||mapping a query and a set of key - value pairs
",,,
model,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .","computed as
weighted sum of the values
weight assigned to each value
computed by
compatibility function of the query
with
corresponding key","weight assigned to each value||computed by||compatibility function of the query
compatibility function of the query||with||corresponding key
","weighted sum of the values||where||weight assigned to each value
",,,,"output||computed as||weighted sum of the values
",,,
model,Scaled Dot - Product Attention,Scaled Dot - Product Attention,,,,"Model||has||Scaled Dot - Product Attention
",,,,,
model,Multi - Head Attention,Multi - Head Attention,,,,"Model||has||Multi - Head Attention
",,,,,
model,Position - wise Feed - Forward Networks,Position - wise Feed - Forward Networks,,,,"Model||has||Position - wise Feed - Forward Networks
",,,,,
model,"In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .","contains
fully connected feed - forward network
applied to
each position separately and identically","fully connected feed - forward network||applied to||each position separately and identically
",,,,,"Position - wise Feed - Forward Networks||contains||fully connected feed - forward network
",,,
model,This consists of two linear transformations with a ReLU activation in between .,"consists
two linear transformations
with
ReLU activation","two linear transformations||with||ReLU activation
",,,,,"each position separately and identically||consists||two linear transformations
",,,
model,Embeddings and Softmax,Embeddings and Softmax,,,,"Model||has||Embeddings and Softmax
",,,,,
model,"Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .","to
use
learned embeddings
to convert
input tokens and output tokens
vectors of dimension d model","learned embeddings||to convert||input tokens and output tokens
input tokens and output tokens||to||vectors of dimension d model
",,,,,"Embeddings and Softmax||use||learned embeddings
",,,
model,We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next - token probabilities .,"usual learned linear transformation and softmax function
to convert
decoder output
to
predicted next - token probabilities","usual learned linear transformation and softmax function||to convert||decoder output
decoder output||to||predicted next - token probabilities
",,,,,,,"Embeddings and Softmax||use||usual learned linear transformation and softmax function
",
model,Positional Encoding,Positional Encoding,,,,"Model||has||Positional Encoding
",,,,,
model,"Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .","of
inject
some information
about
relative or absolute position","some information||about||relative or absolute position
",,,,,"Positional Encoding||inject||some information
","relative or absolute position||of||tokens in the sequence
",,
model,tokens in the sequence .,tokens in the sequence,,,,,,,,,
model,"To this end , we add "" positional encodings "" to the input embeddings at the bottoms of the encoder and decoder stacks .","add
positional encodings
to
input embeddings
at the bottoms of
encoder and decoder stacks","positional encodings||to||input embeddings
input embeddings||at the bottoms of||encoder and decoder stacks
",,,,,"Positional Encoding||add||positional encodings
",,,
experimental-setup,Hardware and Schedule,Hardware and Schedule,,,,"Experimental setup||has||Hardware and Schedule
",,,,,
experimental-setup,We trained our models on one machine with 8 NVIDIA P100 GPUs .,"trained
models
on
one machine
with
8 NVIDIA P100 GPUs","models||on||one machine
one machine||with||8 NVIDIA P100 GPUs
",,,,,"Hardware and Schedule||trained||models
",,,
experimental-setup,"We trained the base models for a total of 100,000 steps or 12 hours .","base models
for
total of 100,000 steps or 12 hours","base models||for||total of 100,000 steps or 12 hours
",,,,,,,"Hardware and Schedule||trained||base models
",
experimental-setup,"The big models were trained for 300,000 steps ( 3.5 days ) .","big models
for
300,000 steps ( 3.5 days )","big models||for||300,000 steps ( 3.5 days )
",,,,,,,"Hardware and Schedule||trained||big models
",
experimental-setup,Optimizer,Optimizer,,,,"Experimental setup||has||Optimizer
",,,,,
experimental-setup,"We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .","used
Adam optimizer
with
? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9","Adam optimizer||with||? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9
",,,,,"Optimizer||used||Adam optimizer
",,,
experimental-setup,We used warmup_steps = 4000 .,"warmup_steps
=
4000","warmup_steps||=||4000
",,,,,,,"Optimizer||used||warmup_steps
",
experimental-setup,Regularization,Regularization,,,,"Experimental setup||has||Regularization
",,,,,"Regularization||has||Residual Dropout
"
experimental-setup,Residual Dropout,Residual Dropout,,,,,,,,,
experimental-setup,"We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .","apply
dropout
to
output of each sub - layer","dropout||to||output of each sub - layer
",,,,,"Residual Dropout||apply||dropout
","dropout||to||sums of the embeddings and the positional encodings
",,
experimental-setup,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .","sums of the embeddings and the positional encodings
in
both the encoder and decoder stacks","sums of the embeddings and the positional encodings||in||both the encoder and decoder stacks
",,,,,,,,
experimental-setup,Label Smoothing,Label Smoothing,,,,,,,,"Regularization||has||Label Smoothing
",
experimental-setup,"During training , we employed label smoothing of value ls = 0.1 .","employed
value ls = 0.1",,,,,,"Label Smoothing||employed||value ls = 0.1
",,,
results,Machine Translation,Machine Translation,,,,,,,,"Tasks||has||Machine Translation
",
results,"On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing a new state - of - the - art BLEU score of 28.4 .","On
WMT 2014 English - to - German translation task
outperforms
the best previously reported models ( including ensembles )
by
more than 2.0 BLEU
establishing
state - of - the - art
BLEU score
28.4","WMT 2014 English - to - German translation task||outperforms||the best previously reported models ( including ensembles )
the best previously reported models ( including ensembles )||by||more than 2.0 BLEU
the best previously reported models ( including ensembles )||establishing||state - of - the - art
state - of - the - art||BLEU score||28.4
",,,,,"Machine Translation||On||WMT 2014 English - to - German translation task
",,,
results,"On the WMT 2014 English - to - French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 / 4 the training cost of the previous state - of - the - art model .","WMT 2014 English - to - French translation task
achieves
BLEU score of 41.0
outperforming
all of the previously published single models","WMT 2014 English - to - French translation task||achieves||BLEU score of 41.0
BLEU score of 41.0||outperforming||all of the previously published single models
",,,,,,,"Machine Translation||On||WMT 2014 English - to - French translation task
",
results,English Constituency Parsing,English Constituency Parsing,,,,,,,,"Tasks||has||English Constituency Parsing
",
results,"Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .","performs
surprisingly well
yielding
better results
than
all previously reported models
with the exception
Recurrent Neural Network Grammar","surprisingly well||yielding||better results
better results||than||all previously reported models
all previously reported models||with the exception||Recurrent Neural Network Grammar
",,,,,"English Constituency Parsing||performs||surprisingly well
",,,
research-problem,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,"Neural machine translation
NMT
machine translation
MT",,,,,"Contribution||has research problem||Neural machine translation
Contribution||has research problem||NMT
Contribution||has research problem||machine translation
Contribution||has research problem||MT
",,,,
model,"In this work , we introduce a new type of linear connections for multi - layer recurrent networks .","introduce
a new type of linear connections
for
multi - layer recurrent networks","a new type of linear connections||for||multi - layer recurrent networks
",,"Model||introduce||a new type of linear connections
",,,,,,
model,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .","called
fast - forward connections
play an essential role
building a deep topology
with
depth of 16","fast - forward connections||play an essential role||building a deep topology
building a deep topology||with||depth of 16
",,,,,"a new type of linear connections||called||fast - forward connections
",,,
model,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .","interleaved bi-directional architecture
to stack
LSTM layers in the encoder","interleaved bi-directional architecture||to stack||LSTM layers in the encoder
",,,"Model||introduce||interleaved bi-directional architecture
",,,,,
experimental-setup,We use 256 dimensional word embeddings for both the source and target languages .,"use
256 dimensional
word embeddings
for
both the source and target languages","word embeddings||for||both the source and target languages
","word embeddings||is||256 dimensional
","Experimental setup||use||word embeddings
",,,,,,
experimental-setup,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .","LSTM layers
including
2n e layers in the encoder and then d layers in the decoder
have
512 memory cells","LSTM layers||including||2n e layers in the encoder and then d layers in the decoder
","512 memory cells||has||LSTM layers
","Experimental setup||have||512 memory cells
",,,,,,
experimental-setup,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .","For
each LSTM layer
activation functions for gates , inputs and outputs
sigmoid , tanh , and tanh","each LSTM layer||activation functions for gates , inputs and outputs||sigmoid , tanh , and tanh
",,"Experimental setup||For||each LSTM layer
",,,,,,
experimental-setup,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .","in
recurrent computation
larger learning rate
l r
5 10 ? 4
is
used
for
feed - forward computation
smaller learning rate
l f
4 10 ? 5
used","recurrent computation||used||larger learning rate
larger learning rate||l r||5 10 ? 4
feed - forward computation||used||smaller learning rate
smaller learning rate||l f||4 10 ? 5
",,"Experimental setup||in||recurrent computation
Experimental setup||for||feed - forward computation
",,,,,,
experimental-setup,The dropout ratio pd is 0.1 .,"dropout ratio
0.1",,,"Experimental setup||dropout ratio||0.1
",,,,,,
experimental-setup,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,"4 ? 8 GPU machines
running for
10 days
to train
full model with parallelization at the data batch level","4 ? 8 GPU machines||running for||10 days
4 ? 8 GPU machines||to train||full model with parallelization at the data batch level
",,,"Experimental setup||use||4 ? 8 GPU machines
",,,,,
results,Single models,Single models,,,,"Results||has||Single models
",,,,,"Single models||has||English - to - French
"
results,English - to - French : First we list our single model results on the English - to - French task in Tab .,English - to - French,,,,,,,,,
results,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .","From
Deep - ED
obtain
BLEU score
of
36.3
outperforms
Enc - Dec model
by
4.8 BLEU points","Deep - ED||obtain||BLEU score
BLEU score||of||36.3
36.3||outperforms||Enc - Dec model
Enc - Dec model||by||4.8 BLEU points
",,,,,"English - to - French||From||Deep - ED
",,,
results,"For Deep - Att , the performance is further improved to 37.7 .","For
Deep - Att
performance is further improved
37.7","Deep - Att||performance is further improved||37.7
",,,,,"English - to - French||For||Deep - Att
",,,
research-problem,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,"Unsupervised neural machine translation
NMT
machine translation",,,,,"Contribution||has research problem||Unsupervised neural machine translation
Contribution||has research problem||NMT
Contribution||has research problem||machine translation
",,,,
research-problem,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared - latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .",unsupervised NMT,,,,,"Contribution||has research problem||unsupervised NMT
",,,,
model,"In order to address this issue , we extend the encoder - shared model , i.e. , the model with one shared encoder , by leveraging two independent encoders with each for one language .","extend
encoder - shared model
with
by
two independent encoders
each for one language","encoder - shared model||by||two independent encoders
two independent encoders||with||each for one language
",,"Model||extend||encoder - shared model
",,,,,,
model,"Similarly , two independent decoders are utilized .","two independent decoders
utilized",,,"Model||utilized||two independent decoders
",,,,,,
model,"For each language , the encoder and its corresponding decoder perform an AE , where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations .","For
each language
perform
AE
encoder generates
latent representations
from
perturbed input sentences
decoder reconstructs
sentences
from
latent representations","each language||perform||AE
AE||encoder generates||latent representations
latent representations||from||perturbed input sentences
AE||decoder reconstructs||sentences
sentences||from||latent representations
",,"Model||For||each language
",,,,,,
model,"To map the latent representations from different languages to a shared - latent space , we propose the weightsharing constraint to the two AEs .","To map
latent representations
from
different languages
to
shared - latent space
propose
weightsharing constraint
to
two AEs","latent representations||from||different languages
different languages||to||shared - latent space
shared - latent space||propose||weightsharing constraint
weightsharing constraint||to||two AEs
",,"Model||To map||latent representations
",,,,,,
model,"For cross - language translation , we utilize the backtranslation following .","cross - language translation
utilize
backtranslation","cross - language translation||utilize||backtranslation
",,,"Model||For||cross - language translation
",,,,,
model,"Additionally , two different generative adversarial networks ( GAN ) , namely the local and global GAN , are proposed to further improve the cross - language translation .","two different generative adversarial networks ( GAN )
namely
local and global GAN
are proposed
to further improve
cross - language translation","two different generative adversarial networks ( GAN )||namely||local and global GAN
two different generative adversarial networks ( GAN )||to further improve||cross - language translation
",,"Model||are proposed||two different generative adversarial networks ( GAN )
",,,,,,
model,"We utilize the local GAN to constrain the source and target latent representations to have the same distribution , whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation .","utilize
local GAN
to constrain
source and target latent representations
to have
same distribution","local GAN||to constrain||source and target latent representations
source and target latent representations||to have||same distribution
",,"Model||utilize||local GAN
",,,,,,
model,"We apply the global GAN to finetune the corresponding generator , i.e. , the composition of the encoder and decoder of the other language , where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 .","apply
global GAN
to finetune
corresponding generator
i.e.
composition of the encoder and decoder of the other language","global GAN||to finetune||corresponding generator
corresponding generator||i.e.||composition of the encoder and decoder of the other language
",,"Model||apply||global GAN
",,,,,,
hyperparameters,"Following the base model in , we set the dimension of word embedding as 512 , dropout rate as 0.1 and the head number as 8 .","set
dimension of word embedding
as
512
dropout rate
as
0.1
head number
as
8","dimension of word embedding||as||512
dropout rate||as||0.1
head number||as||8
",,"Hyperparameters||set||dimension of word embedding
Hyperparameters||set||dropout rate
Hyperparameters||set||head number
",,,,,,
hyperparameters,We use beam search with a beam size of 4 and length penalty ? = 0.6 .,"use
beam search
with
beam size
of
4
length penalty
? =
0.6","beam search||with||length penalty
length penalty||? =||0.6
beam search||with||beam size
beam size||of||4
",,"Hyperparameters||use||beam search
",,,,,,
hyperparameters,The model is implemented in TensorFlow and trained on up to four K80 GPUs synchronously in a multi - GPU setup on a single machine .,"implemented in
TensorFlow
trained on
up to four K80 GPUs
in
multi - GPU setup
on
single machine","up to four K80 GPUs||in||multi - GPU setup
multi - GPU setup||on||single machine
",,"Hyperparameters||implemented in||TensorFlow
Hyperparameters||trained on||up to four K80 GPUs
",,,,,,
baselines,Word - by - word translation ( WBW ) The first baseline we consider is a system that performs word - by - word translations using the inferred bilingual dictionary .,,,,,,,,,,
baselines,Lample et al .,Lample et al .,,,,"Baselines||has||Lample et al .
",,,,,
baselines,Supervised training,Supervised training,,,,"Baselines||has||Supervised training
",,,,,
results,Number of weight - sharing layers,Number of weight - sharing layers,,,,"Results||has||Number of weight - sharing layers
",,,,,
results,And the best translation performance is achieved when only one layer is shared in our system .,"best translation performance
achieved
when
only one layer is shared in our system","best translation performance||when||only one layer is shared in our system
",,,,,"Number of weight - sharing layers||achieved||best translation performance
",,,
results,"When all of the four layers are shared , i.e. , only one shared encoder is utilized , we get poor translation performance in all of the three translation tasks .","When
all of the four layers are shared
get
poor translation performance
in
all of the three translation tasks","all of the four layers are shared||get||poor translation performance
poor translation performance||in||all of the three translation tasks
",,,,,"Number of weight - sharing layers||When||all of the four layers are shared
",,,
ablation-analysis,"The most critical component is the weight - sharing constraint , which is vital to map sentences of different languages to the shared - latent space .","most critical component
weight - sharing constraint
is vital to map
sentences of different languages to the shared - latent space","weight - sharing constraint||is vital to map||sentences of different languages to the shared - latent space
",,"Ablation analysis||most critical component||weight - sharing constraint
",,,,,,
ablation-analysis,The embedding - reinforced encoder also brings some improvement on all of the translation tasks .,"embedding - reinforced encoder
brings some improvement
on
all of the translation tasks","embedding - reinforced encoder||on||all of the translation tasks
",,"Ablation analysis||brings some improvement||embedding - reinforced encoder
",,,,,,
ablation-analysis,"When we remove the directional self - attention , we getup to - 0.3 BLEU points decline .","remove
directional self - attention
- 0.3 BLEU points decline",,"directional self - attention||get||- 0.3 BLEU points decline
","Ablation analysis||remove||directional self - attention
",,,,,,
ablation-analysis,The GANs also significantly improve the translation performance of our system .,"The GANs
significantly improve
translation performance
our system","The GANs||translation performance||our system
",,"Ablation analysis||significantly improve||The GANs
",,,,,,
research-problem,Tilde 's Machine Translation Systems for WMT 2018,Machine Translation,,,,,"Contribution||has research problem||Machine Translation
",,,,
research-problem,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,"NMT
news translation",,,,,"Contribution||has research problem||NMT
Contribution||has research problem||news translation
",,,,
research-problem,Neural machine translation ( NMT ) is a rapidly changing research area .,Neural machine translation,,,,,"Contribution||has research problem||Neural machine translation
",,,,
model,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .","submitted
both constrained and unconstrained NMT systems",,,"Model||submitted||both constrained and unconstrained NMT systems
",,,,,,
,The following is a list of the five MT systems submitted :,list of the five MT systems submitted,,,,,,,,,
,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,Constrained English - Estonian and Estonian - English NMT systems,,,,,,,,,
,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,"trained using
parallel data and back - translated data in a 1 - to - 1 proportion",,,,,,,,,
model,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,Unconstrained English - Estonian and Estonian - English NMT systems,,,,"Model||list of the five MT systems submitted||Unconstrained English - Estonian and Estonian - English NMT systems
",,,,,
model,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .","trained using
back - translated data
were not
factored","back - translated data||were not||factored
",,,,,"Unconstrained English - Estonian and Estonian - English NMT systems||trained using||back - translated data
",,,
model,A constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,"constrained Estonian - English NMT system
is a
system combination of six factored data NMT systems","constrained Estonian - English NMT system||is a||system combination of six factored data NMT systems
",,,"Model||list of the five MT systems submitted||constrained Estonian - English NMT system
",,,,,
model,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,Constrained English - Estonian and Estonian - English NMT systems,,,,"Model||list of the five MT systems submitted||Constrained English - Estonian and Estonian - English NMT systems
",,,,,
model,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,"trained using
two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data",,,,,,"Constrained English - Estonian and Estonian - English NMT systems||trained using||two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data
",,,
results,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,"achieved
better results
than
MLSTM - based models","better results||than||MLSTM - based models
",,"Results||achieved||better results
",,,,,,
results,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .","For
constrained scenarios
both ensembles of averaged models
achieved
higher scores
than
each individual averaged model","constrained scenarios||achieved||both ensembles of averaged models
constrained scenarios||achieved||higher scores
higher scores||than||each individual averaged model
",,"Results||For||constrained scenarios
",,,,,,
results,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,"evident
unconstrained models
best results","best results||evident||unconstrained models
",,,"Results||achieved||best results
",,,,,
research-problem,FRAGE : Frequency - Agnostic Word Representation,Frequency - Agnostic Word Representation,,,,,"Contribution||has research problem||Frequency - Agnostic Word Representation
",,,,
research-problem,"Although it is widely accepted that words with similar semantics should be close to each other in the embedding space , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of highfrequency and low - frequency words lie in different subregions of the embedding space , and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar .",word embeddings learned in several tasks are biased towards word frequency,,,,,"Contribution||has research problem||word embeddings learned in several tasks are biased towards word frequency
",,,,
research-problem,"Interestingly , the learned embeddings of rare words and popular words behave differently .",learned embeddings of rare words and popular words behave differently,,,,,"Contribution||has research problem||learned embeddings of rare words and popular words behave differently
",,,,
research-problem,"As shown in ( a ) and 1 ( b ) , the embeddings of rare words and popular words actually lie in different subregions of the space .",embeddings of rare words and popular words actually lie in different subregions of the space,,,,,"Contribution||has research problem||embeddings of rare words and popular words actually lie in different subregions of the space
",,,,
research-problem,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,different behaviors of the embeddings of popular words and rare words are problematic,,,,,"Contribution||has research problem||different behaviors of the embeddings of popular words and rare words are problematic
",,,,
approach,"To address this problem , in this paper , we propose an adversarial training method to learn FRequency - AGnostic word Embedding ( FRAGE ) .","to learn
FRequency - AGnostic word Embedding ( FRAGE )",,,"Approach||to learn||FRequency - AGnostic word Embedding ( FRAGE )
",,,,,,
approach,"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .","minimize
task - specific loss
by optimizing
task - specific parameters
together with
word embeddings
introduce
another discriminator
a word embedding
input
classifies
popular / rare word","task - specific loss||by optimizing||task - specific parameters
task - specific parameters||together with||word embeddings
another discriminator||input||a word embedding
a word embedding||classifies||popular / rare word
",,"Approach||minimize||task - specific loss
Approach||introduce||another discriminator
",,,,,,
approach,"The discriminator optimizes its parameters to maximize its classification accuracy , while word embeddings are optimized towards a low task - dependent loss as well as fooling the discriminator to mis-classify the popular and rare words .","discriminator
optimizes
to maximize
classification accuracy
word embeddings
optimized
towards
low task - dependent loss
fooling
discriminator
to mis-classify
popular and rare words","word embeddings||fooling||discriminator
discriminator||to mis-classify||popular and rare words
word embeddings||towards||low task - dependent loss
discriminator||to maximize||classification accuracy
",,"Approach||optimized||word embeddings
Approach||optimizes||discriminator
",,,,,,
approach,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .","When
the whole training process converges
achieves
equilibrium
can not well differentiate
popular words
from
rare words","equilibrium||When||the whole training process converges
equilibrium||can not well differentiate||popular words
popular words||from||rare words
",,"Approach||achieves||equilibrium
",,,,,,
experiments,"Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation .",Word Similarity,,,,"Experiments||Tasks||Word Similarity
",,,,,
experiments,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .","use
skip - gram model
as
baseline model","skip - gram model||as||baseline model
",,,,,"Word Similarity||use||skip - gram model
",,,
experiments,"We test the baseline and our method on three datasets : RG65 , WS and RW .","test
baseline and our method
on
three datasets : RG65 , WS and RW","baseline and our method||on||three datasets : RG65 , WS and RW
",,,,,"Word Similarity||test||baseline and our method
",,,
experiments,Language Modeling is a basic task in natural language processing .,Language Modeling,,,,"Experiments||Tasks||Language Modeling
",,,,,
experiments,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,"goal
predict the next word
conditioned on
previous words
evaluated by
perplexity","predict the next word||conditioned on||previous words
",,,,,"Language Modeling||goal||predict the next word
Language Modeling||evaluated by||perplexity
",,,
experiments,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .","experiments on
two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 )",,,,,,"Language Modeling||experiments on||two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 )
",,,
experiments,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .","choose
two recent works
as
our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model","two recent works||as||our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model
",,,,,"Language Modeling||choose||two recent works
",,,
experiments,Machine Translation is a popular task in both deep learning and natural language processing .,Machine Translation,,,,"Experiments||Tasks||Machine Translation
",,,,,
experiments,"We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .","choose
two datasets : WMT14 English - German and IWSLT14 German - English datasets
evaluated in terms of
score","two datasets : WMT14 English - German and IWSLT14 German - English datasets||evaluated in terms of||score
",,,,,"Machine Translation||choose||two datasets : WMT14 English - German and IWSLT14 German - English datasets
",,,
experiments,"We use Transformer as the baseline model , which achieves state - of - the - art accuracy on multiple translation datasets .","use
Transformer
as
baseline model","Transformer||as||baseline model
",,,,,"Machine Translation||use||Transformer
",,,
experiments,Text Classification is a conventional machine learning task and is evaluated by accuracy .,Text Classification,,,,"Experiments||Tasks||Text Classification
",,,,,
experiments,"Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) .","implement
Recurrent CNN - based model
test it on
AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG )",,,,,,"Text Classification||implement||Recurrent CNN - based model
Text Classification||test it on||AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG )
",,,
results,Language Modeling,Language Modeling,,,,"Results||Tasks||Language Modeling
",,,,,
results,"In all these settings , our method outperforms the two baselines .","outperforms
two baselines",,,,,,"Language Modeling||outperforms||two baselines
",,,
results,"On PTB dataset , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .","PTB dataset
improves
AWD - LSTM and AWD - LSTM - MoS baseline
by
0.8/1.2/1.0 and 0.76/1.13/1.15 points","PTB dataset||improves||AWD - LSTM and AWD - LSTM - MoS baseline
AWD - LSTM and AWD - LSTM - MoS baseline||by||0.8/1.2/1.0 and 0.76/1.13/1.15 points
",,,,,,,"Language Modeling||on||PTB dataset
",
results,"On WT2 dataset , which contains more rare words , our method achieves larger improvements .","WT2 dataset
achieves
larger improvements","WT2 dataset||achieves||larger improvements
",,,,,,,"Language Modeling||on||WT2 dataset
",
results,"We improve the results of AWD - LSTM and AWD - LSTM - MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity , respectively .","improve the results of
AWD - LSTM and AWD - LSTM - MoS
by
2.3/2.4/2.7 and 1.15/1.72/1.54
in terms of
test perplexity","AWD - LSTM and AWD - LSTM - MoS||by||2.3/2.4/2.7 and 1.15/1.72/1.54
2.3/2.4/2.7 and 1.15/1.72/1.54||in terms of||test perplexity
",,,,,"Language Modeling||improve the results of||AWD - LSTM and AWD - LSTM - MoS
",,,
results,Machine Translation,Machine Translation,,,,"Results||Tasks||Machine Translation
",,,,,
results,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,"outperform
baselines
for
1.06/0.71
in the term of
BLEU
on","baselines||for||1.06/0.71
baselines||in the term of||BLEU
",,,,,"Machine Translation||outperform||baselines
",,,
results,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,"model
learned from
adversarial training
outperforms
original one in IWSLT14 German - English task
by
0.85","model||learned from||adversarial training
original one in IWSLT14 German - English task||by||0.85
",,,,,"Machine Translation||outperforms||model
Machine Translation||outperforms||original one in IWSLT14 German - English task
",,,
results,Text Classification,Text Classification,,,,"Results||Tasks||Text Classification
",,,,,
results,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,"outperforms
baseline
for
1.26%/0.66%/0.44 %
on
three different datasets","baseline||for||1.26%/0.66%/0.44 %
baseline||on||three different datasets
",,,,,"Text Classification||outperforms||baseline
",,,
research-problem,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .","Conditional computation
increasing model capacity without a proportional increase in computation",,,,,"Contribution||has research problem||Conditional computation
Contribution||has research problem||increasing model capacity without a proportional increase in computation
",,,,
research-problem,Exploiting scale in both training data and model size has been central to the success of deep learning .,Exploiting scale in both training data and model size,,,,,"Contribution||has research problem||Exploiting scale in both training data and model size
",,,,
approach,Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,"to
conditional computation
to introduce
new type of general purpose neural network component
:
Sparsely - Gated Mixture - of - Experts Layer ( MoE )","conditional computation||to introduce||new type of general purpose neural network component
new type of general purpose neural network component||:||Sparsely - Gated Mixture - of - Experts Layer ( MoE )
",,"Approach||to||conditional computation
",,,,,,
approach,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .","consists of
number of experts
each
simple feed - forward neural network
trainable gating network
selects
sparse combination of the experts to process each input","trainable gating network||selects||sparse combination of the experts to process each input
number of experts||each||simple feed - forward neural network
",,"Approach||consists of||trainable gating network
Approach||consists of||number of experts
",,,,,,
approach,All parts of the network are trained jointly by back - propagation .,"All parts of
network
trained
jointly
by
back - propagation","jointly||by||back - propagation
back - propagation||All parts of||network
",,"Approach||trained||jointly
",,,,,,
tasks,100 BILLION WORD GOOGLE NEWS CORPUS,100 BILLION WORD GOOGLE NEWS CORPUS,,,,"Tasks||has||100 BILLION WORD GOOGLE NEWS CORPUS
",,,,,"100 BILLION WORD GOOGLE NEWS CORPUS||has||Hyperparameters
"
tasks,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .","MoE layers
32 , experts",,,,,,"Hyperparameters||MoE layers||32 , experts
",,,
tasks,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .","When
training over the full 100 billion words
test perplexity
improves significantly
up to
65536 experts ( 68 billion parameters )","training over the full 100 billion words||test perplexity||improves significantly
improves significantly||up to||65536 experts ( 68 billion parameters )
",,,,,"Results||When||training over the full 100 billion words
",,,
tasks,MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),,,,"Tasks||has||MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )
",,,,,"MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )||has||Hyperparameters
"
tasks,Our model was a modified version of the GNMT model described in .,"model
was a
modified version
GNMT model","GNMT model||was a||modified version
",,,,,"Hyperparameters||model||GNMT model
",,,
tasks,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .","To reduce
computation
decreased
LSTM layers
in
encoder and decoder
from
9 and 8 to 3 and 2 respectively","LSTM layers||in||encoder and decoder
encoder and decoder||from||9 and 8 to 3 and 2 respectively
LSTM layers||To reduce||computation
",,,,,"Hyperparameters||decreased||LSTM layers
",,,
tasks,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,"inserted
MoE layers
in both
encoder
between
layers 2 and 3
decoder
between
layers 1 and 2","MoE layers||in both||decoder
decoder||between||layers 1 and 2
MoE layers||in both||encoder
encoder||between||layers 2 and 3
",,,,,"Hyperparameters||inserted||MoE layers
",,,
tasks,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .","Each
MoE layer
contained up to
2048 experts each
with
about two million parameters","MoE layer||contained up to||2048 experts each
2048 experts each||with||about two million parameters
",,,,,"Hyperparameters||Each||MoE layer
",,,
tasks,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,"BLEU scores
40.56 and 26.03
on
WMT ' 14 En?Fr and En ? De benchmarks","40.56 and 26.03||on||WMT ' 14 En?Fr and En ? De benchmarks
",,,,,"Results||BLEU scores||40.56 and 26.03
",,,
tasks,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .","On
Google Production dataset
1.01 higher
test BLEU score","1.01 higher||On||Google Production dataset
",,,,,"Results||test BLEU score||1.01 higher
",,,
tasks,MULTILINGUAL MACHINE TRANSLATION,MULTILINGUAL MACHINE TRANSLATION,,,,"Tasks||has||MULTILINGUAL MACHINE TRANSLATION
",,,,,"MULTILINGUAL MACHINE TRANSLATION||has||Results
"
tasks,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,"achieves
19 % lower perplexity
on
dev set
than
multilingual GNMT model","dev set||achieves||19 % lower perplexity
19 % lower perplexity||than||multilingual GNMT model
",,,,,"Results||on||dev set
",,,
tasks,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .","On
BLEU score
significantly beats
multilingual GNMT model
on
11 of the 12 language pairs
by as much as
5.84 points
even beats
monolingual GNMT models
on
8 of 12 language pairs","BLEU score||significantly beats||multilingual GNMT model
multilingual GNMT model||on||11 of the 12 language pairs
11 of the 12 language pairs||by as much as||5.84 points
BLEU score||even beats||monolingual GNMT models
monolingual GNMT models||on||8 of 12 language pairs
",,,,,"Results||On||BLEU score
",,,
research-problem,Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,NEURAL MACHINE TRANSLATION,,,,,"Contribution||has research problem||NEURAL MACHINE TRANSLATION
",,,,
research-problem,"In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .",use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture,,,,,"Contribution||has research problem||use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture
",,,,
research-problem,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .",machine translation,,,,,"Contribution||has research problem||machine translation
",,,,
model,"In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .","introduce
extension to the encoder - decoder model
learns to
align and translate jointly","extension to the encoder - decoder model||learns to||align and translate jointly
",,"Model||introduce||extension to the encoder - decoder model
",,,,,,
model,"Each time the proposed model generates a word in a translation , it ( soft - ) searches for a set of positions in a source sentence where the most relevant information is concentrated .","Each time
generates
word
in
translation
( soft - ) searches
set of positions
in
source sentence
where
most relevant information is concentrated","Each time||generates||word
word||in||translation
set of positions||in||source sentence
source sentence||where||most relevant information is concentrated
",,"Model||( soft - ) searches||Each time
Model||( soft - ) searches||set of positions
",,,,,,
model,The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .,"predicts
target word
based on
context vectors
associated with
source positions
previous generated target words","target word||based on||context vectors
context vectors||associated with||source positions
target word||based on||previous generated target words
",,"Model||predicts||target word
",,,,,,
hyperparameters,We train two types of models .,"train
two types of models",,,"Hyperparameters||train||two types of models
",,,,,,
hyperparameters,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .","first one
RNN Encoder - Decoder
other
RNNsearch",,,,,,"two types of models||first one||RNN Encoder - Decoder
two types of models||other||RNNsearch
",,,
hyperparameters,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .","each model twice
with
sentences
of length
up to 30 words
up to 50 word","each model twice||with||sentences
sentences||of length||up to 30 words
sentences||of length||up to 50 word
",,,"Hyperparameters||train||each model twice
",,,,,
hyperparameters,The encoder and decoder of the RNNencdec have 1000 hidden units each .,"encoder and decoder
of
RNNencdec
have
1000
hidden units each","RNNencdec||encoder and decoder||hidden units each
hidden units each||have||1000
",,"Hyperparameters||of||RNNencdec
",,,,,,
hyperparameters,The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .,"encoder of
RNNsearch
consists of
forward and backward recurrent neural networks ( RNN )
each having
1000 hidden units","RNNsearch||consists of||forward and backward recurrent neural networks ( RNN )
forward and backward recurrent neural networks ( RNN )||each having||1000 hidden units
",,"Hyperparameters||encoder of||RNNsearch
",,,,,,
hyperparameters,It s decoder has 1000 hidden units .,"decoder
1000
hidden units","decoder||hidden units||1000
",,,"Hyperparameters||has||decoder
",,,,,
hyperparameters,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .","use
multilayer network
with
single maxout hidden layer
to compute
conditional probability
of
each target word","multilayer network||with||single maxout hidden layer
multilayer network||to compute||conditional probability
conditional probability||of||each target word
",,"Hyperparameters||use||multilayer network
",,,,,,
results,"In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .","of
RNNencdec
dramatically drops as
length of the sentences increases","RNNencdec||dramatically drops as||length of the sentences increases
",,"Results||of||RNNencdec
",,,,,,
results,"On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .","both RNNsearch - 30 and RNNsearch - 50
more robust
to
length of the sentences","both RNNsearch - 30 and RNNsearch - 50||to||length of the sentences
",,"Results||more robust||both RNNsearch - 30 and RNNsearch - 50
",,,,,,
results,"RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .","RNNsearch - 50
no performance deterioration
with
sentences
of length
50 or more","RNNsearch - 50||with||sentences
sentences||of length||50 or more
",,"Results||no performance deterioration||RNNsearch - 50
",,,,,,
results,This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .,"RNNsearch - 30
even
outperforms
RNNencdec - 50","RNNsearch - 30||even||RNNencdec - 50
",,"Results||outperforms||RNNsearch - 30
",,,,,,
research-problem,Under review as a conference paper at ICLR 2018 COMPRESSING WORD EMBEDDINGS VIA DEEP COMPOSITIONAL CODE LEARNING,COMPRESSING WORD EMBEDDINGS,,,,,"Contribution||has research problem||COMPRESSING WORD EMBEDDINGS
",,,,
research-problem,Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance .,compressing the word embeddings without any significant sacrifices in performance,,,,,"Contribution||has research problem||compressing the word embeddings without any significant sacrifices in performance
",,,,
research-problem,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .",compress the size of NLP models,,,,,"Contribution||has research problem||compress the size of NLP models
",,,,
approach,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .","represent
each word w
with
code C","each word w||with||code C
",,"Approach||represent||each word w
",,,,,,
approach,"Once we have obtained such compact codes for all words in the vocabulary , we use embedding vectors to represent the codes rather than the unique words .","codes
use
embedding vectors
to represent
rather than
unique words","embedding vectors||to represent||codes
codes||rather than||unique words
",,"Approach||use||embedding vectors
",,,,,,
approach,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .","create
M codebooks
each containing
K codeword vectors","M codebooks||each containing||K codeword vectors
",,"Approach||create||M codebooks
",,,,,,
approach,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,"embedding of a word
computed
by summing up
codewords
corresponding to
all the components in the code","embedding of a word||by summing up||codewords
codewords||corresponding to||all the components in the code
",,"Approach||computed||embedding of a word
",,,,,,
research-problem,"In this work , we utilize such codes for a different purpose , that is , constructing word embeddings with drastically fewer parameters .",constructing word embeddings with drastically fewer parameters,,,,,"Contribution||has research problem||constructing word embeddings with drastically fewer parameters
",,,,
approach,We utilize the Gumbel - softmax trick to find the best discrete codes that minimize the loss .,"utilize
Gumbel - softmax trick
to find
best discrete codes
minimize
loss","Gumbel - softmax trick||to find||best discrete codes
best discrete codes||minimize||loss
",,"Approach||utilize||Gumbel - softmax trick
",,,,,,
experiments,CODE LEARNING,CODE LEARNING,,,,"Experiments||has||CODE LEARNING
",,,,,"CODE LEARNING||has||Experimental setup
"
experiments,"In each iteration , a small batch of the embeddings is sampled uniformly from the baseline embedding matrix .","small batch of the embeddings
sampled uniformly
from
baseline embedding matrix","sampled uniformly||from||baseline embedding matrix
",,,,,"Experimental setup||small batch of the embeddings||sampled uniformly
",,,
experiments,"In our experiments , the batch size is set to 128 .","batch size
128",,,,,,"Experimental setup||batch size||128
",,,
experiments,We use Adam optimizer with a fixed learning rate of 0.0001 .,"use
Adam optimizer
with
fixed learning rate
of
0.0001","Adam optimizer||with||fixed learning rate
fixed learning rate||of||0.0001
Adam optimizer||with||fixed learning rate
fixed learning rate||of||0.0001
",,,,,"Experimental setup||use||Adam optimizer
",,,
experiments,The training is run for 200K iterations .,"training
200K iterations",,,,,,"Experimental setup||training||200K iterations
",,,
experiments,"We evenly distribute the model training to 4 GPUs using the nccl package , so that one round of code learning takes around 15 minutes to complete .","distribute
model training
4
GPUs
using
nccl package","model training||using||nccl package
model training||GPUs||4
",,,,,"Experimental setup||distribute||model training
",,,
experiments,SENTIMENT ANALYSIS,SENTIMENT ANALYSIS,,,,"Experiments||has||SENTIMENT ANALYSIS
",,,,,"SENTIMENT ANALYSIS||has||Hyperparameters
"
experiments,The models are trained with Adam optimizer for 15 epochs with a fixed learning rate of 0.0001 .,"trained with
Adam optimizer
for
15 epochs
with
fixed learning rate
of
0.0001","Adam optimizer||for||15 epochs
",,,,,"Hyperparameters||trained with||Adam optimizer
",,,
experiments,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .","maximum loss - free
compression rate
achieved by
16 32 coding scheme","16 32 coding scheme||maximum loss - free||compression rate
",,,,,"Results||achieved by||16 32 coding scheme
",,,
experiments,We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,"classification accuracy
substantially improved
with
slightly lower compression rate","classification accuracy||with||slightly lower compression rate
",,,,,"Results||substantially improved||classification accuracy
",,,
experiments,MACHINE TRANSLATION,MACHINE TRANSLATION,,,,"Experiments||has||MACHINE TRANSLATION
",,,,,"MACHINE TRANSLATION||has||Experimental setup
"
experiments,All models are trained by Nesterov 's accelerated gradient with an initial learning rate of 0.25 .,"trained by
Nesterov 's accelerated gradient
with
initial learning rate
of
0.25","Nesterov 's accelerated gradient||with||initial learning rate
initial learning rate||of||0.25
",,,,,"Experimental setup||trained by||Nesterov 's accelerated gradient
",,,
experiments,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .","training
distributed
4
GPUs
computes
mini-batch
16 samples","training||GPUs||4
training||GPUs||computes
computes||mini-batch||16 samples
",,,,,"Experimental setup||distributed||training
",,,
experiments,The loss - free compression rate reaches 92 % on ASPEC dataset by pruning 90 % of the connections .,"loss - free compression rate
reaches 92 %
on
ASPEC dataset
by pruning
90 % of the connections","ASPEC dataset||by pruning||90 % of the connections
90 % of the connections||loss - free compression rate||reaches 92 %
",,,,,"Results||on||ASPEC dataset
",,,
experiments,"However , with the same pruning ratio , a modest performance loss is observed in IWSLT14 dataset .","modest performance loss
observed
in
IWSLT14 dataset","IWSLT14 dataset||observed||modest performance loss
",,,,,"Results||in||IWSLT14 dataset
",,,
experiments,"For the models using compositional coding , the loss - free compression rate is 94 % for the IWSLT14 dataset and 99 % for the ASPEC dataset .","using
compositional coding
loss - free compression rate
94 %
for
IWSLT14 dataset
99 %
for
ASPEC dataset","compositional coding||loss - free compression rate||94 %
94 %||for||IWSLT14 dataset
compositional coding||loss - free compression rate||99 %
99 %||for||ASPEC dataset
",,,,,"Results||using||compositional coding
",,,
research-problem,"0,000 + Times Accelerated Robust Subset Selection ( ARSS )",Accelerated Robust Subset Selection,,,,,"Contribution||has research problem||Accelerated Robust Subset Selection
",,,,
research-problem,Subset selection from massive data with noised information is increasingly popular for various applications .,Subset selection from massive data,,,,,"Contribution||has research problem||Subset selection from massive data
",,,,
research-problem,"Specifically in the subset selection area , this is the first attempt to employ the p ( 0 < p ? 1 ) - norm based measure for the representation loss , preventing large errors from dominating our objective .",subset selection,,,,,"Contribution||has research problem||subset selection
",,,,
approach,"In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .","propose
accelerated robust subset selection",,,"Approach||propose||accelerated robust subset selection
",,,,,,
approach,"To this end , we use the p ( 0 < p ? 1 ) - norm based robust measure for the representation loss , preventing large errors from dominating our objective .","use
p ( 0 < p ? 1 ) - norm based robust measure
for
representation loss","p ( 0 < p ? 1 ) - norm based robust measure||for||representation loss
",,"Approach||use||p ( 0 < p ? 1 ) - norm based robust measure
",,,,,,
approach,"Then , based on the observation that data size is generally much larger than feature length , i.e. N L , we propose a speedup solver .","based on
observation
that
data size is generally much larger than feature length
propose
speedup solver","observation||that||data size is generally much larger than feature length
data size is generally much larger than feature length||propose||speedup solver
",,"Approach||based on||observation
",,,,,,
approach,The main acceleration is owing to the Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation .,"main acceleration
is
owing to
Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation","Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation||is||main acceleration
",,"Approach||owing to||Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation
",,,,,,
experiments,"All experiments are conducted on a server with 64 - core Intel Xeon E7-4820 @ 2.00 GHz , 18 Mb Cache and 0.986 TB RAM , using Matlab 2012 .","server
64 - core Intel Xeon E7-4820 @ 2.00 GHz
18 Mb
Cache
0.986 TB
RAM
using
Matlab 2012",,,,,,,,,
experiments,Speed Comparisons,Speed Comparisons,,,,,,,,,"Results||Speed Comparisons||Speed vs. increasing N
"
experiments,"Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .",Speed vs. increasing N,,,,,,,,,
experiments,"Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .","curve
surprisingly lower and highly stable
almost no rise of selection time
over growing N",,,,,,"Speed vs. increasing N||curve||surprisingly lower and highly stable
Speed vs. increasing N||over growing N||almost no rise of selection time
",,,
experiments,"Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .","Speed with fixed N
is",,,,,,,"Speed with fixed N||is||fastest algorithm
","Results||Speed Comparisons||Speed with fixed N
",
experiments,"First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .",fastest algorithm,,,,,,,,,
experiments,Prediction Accuracy,Prediction Accuracy,,,,,,,,,"Results||Prediction Accuracy||Accuracy comparison
"
experiments,Accuracy comparison,Accuracy comparison,,,,,,,,,
experiments,"Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .","achieve
appreciable advantage",,,,,,"Accuracy comparison||achieve||appreciable advantage
",,,
experiments,"Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .",Prediction accuracies vs. increasing K,,,,,,,,"Results||Prediction Accuracy||Prediction accuracies vs. increasing K
",
experiments,"As we shall see , the prediction accuracies generally increase as K increases .","prediction accuracies generally increase
K increases",,,,,,"Prediction accuracies vs. increasing K||K increases||prediction accuracies generally increase
",,,
experiments,"For each sub-figure , ARSS is generally among the best .","ARSS
among the best",,,,,,"Prediction accuracies vs. increasing K||among the best||ARSS
",,,
research-problem,Neural Architectures for Named Entity Recognition,Named Entity Recognition,,,,,"Contribution||has research problem||Named Entity Recognition
",,,,
research-problem,Our models obtain state - of - the - art performance in NER in four languages without resorting to any language - specific knowledge or resources such as gazetteers .,NER,,,,,"Contribution||has research problem||NER
",,,,
model,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .","bidirectional LSTM
sequential conditional random layer
above it
LSTM - CRF
constructs and labels
chunks of input sentences
using
algorithm inspired by transition - based parsing with states
represented by
stack LSTMs
S - LSTM","bidirectional LSTM||above it||sequential conditional random layer
S - LSTM||constructs and labels||chunks of input sentences
chunks of input sentences||using||algorithm inspired by transition - based parsing with states
algorithm inspired by transition - based parsing with states||represented by||stack LSTMs
","LSTM - CRF||has||bidirectional LSTM
",,"Model||has||LSTM - CRF
Model||has||S - LSTM
",,,,,
model,"To capture orthographic sensitivity , we use character - based word representation model to capture distributional sensitivity , we combine these representations with distributional representations .","To capture
orthographic sensitivity
use
character - based word representation model
with
distributional representations","orthographic sensitivity||use||character - based word representation model
character - based word representation model||with||distributional representations
",,"Model||To capture||orthographic sensitivity
",,,,,,
hyperparameters,"For both models presented , we train our networks using the back - propagation algorithm updating our parameters on every training example , one at a time , using stochastic gradient descent ( SGD ) with a learning rate of 0.01 and a gradient clipping of 5.0 .","train
networks
using
back - propagation algorithm
updating
parameters
using
stochastic gradient descent ( SGD )
learning rate
0.01
gradient clipping
5.0","networks||using||back - propagation algorithm
networks||updating||parameters
parameters||using||stochastic gradient descent ( SGD )
stochastic gradient descent ( SGD )||gradient clipping||5.0
stochastic gradient descent ( SGD )||learning rate||0.01
",,"Hyperparameters||train||networks
",,,,,,
hyperparameters,Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .,"LSTM - CRF model
uses
single layer for the forward and backward LSTMs
dimensions
100","LSTM - CRF model||uses||single layer for the forward and backward LSTMs
single layer for the forward and backward LSTMs||dimensions||100
",,,"Hyperparameters||has||LSTM - CRF model
",,,,,
hyperparameters,We set the dropout rate to 0.5 .,"dropout rate
0.5",,,,,,"LSTM - CRF model||dropout rate||0.5
",,,
hyperparameters,The stack - LSTM model uses two layers each of dimension 100 for each stack .,"stack - LSTM model
uses
two layers
dimension
100
for
each stack","stack - LSTM model||uses||two layers
two layers||dimension||100
100||for||each stack
",,,"Hyperparameters||has||stack - LSTM model
",,,,,
hyperparameters,"The embeddings of the actions used in the composition functions have 16 dimensions each , and the output embedding is of dimension 20 .","embeddings of the actions
used in
composition functions
have
16 dimensions each
output embedding
dimension
20","embeddings of the actions||have||16 dimensions each
embeddings of the actions||used in||composition functions
output embedding||dimension||20
",,,,,,,"stack - LSTM model||has||embeddings of the actions
stack - LSTM model||has||output embedding
",
results,"Our LSTM - CRF model outperforms all other systems , including the ones using external labeled data like gazetteers .","LSTM - CRF model
outperforms
all other systems
including
ones using external labeled data like gazetteers","LSTM - CRF model||outperforms||all other systems
all other systems||including||ones using external labeled data like gazetteers
",,,"Results||has||LSTM - CRF model
",,,,,
results,"Our Stack - LSTM model also outperforms all previous models that do not incorporate external features , apart from the one presented by .","Stack - LSTM model
outperforms
all previous models
do not incorporate
external features","Stack - LSTM model||outperforms||all previous models
all previous models||do not incorporate||external features
",,,"Results||has||Stack - LSTM model
",,,,,
results,"and 4 present our results on NER for German , Dutch and Spanish respectively in comparison to other models .","for
German , Dutch and Spanish",,,"Results||for||German , Dutch and Spanish
",,,,,,"German , Dutch and Spanish||has||LSTM - CRF model
"
results,"On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .","LSTM - CRF model
significantly outperforms
all previous methods","LSTM - CRF model||significantly outperforms||all previous methods
",,,,,,,,
results,"The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .","exception
Dutch",,,,,,"all previous methods||exception||Dutch
",,,
results,The Stack - LSTM also consistently presents statethe - art ( or close to ) results compared to systems that do not use external data .,"presents
statethe - art ( or close to ) results
compared to
systems that do not use external data","systems that do not use external data||presents||statethe - art ( or close to ) results
",,"Results||compared to||systems that do not use external data
",,,,,,
research-problem,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,Fast and Accurate Entity Recognition,,,,,"Contribution||has research problem||Fast and Accurate Entity Recognition
",,,,
research-problem,"This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .",faster alternative to Bi - LSTMs for NER,,,,,"Contribution||has research problem||faster alternative to Bi - LSTMs for NER
",,,,
research-problem,"In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .",democratize large - scale NLP and information extraction while minimizing our environmental footprint,,,,,"Contribution||has research problem||democratize large - scale NLP and information extraction while minimizing our environmental footprint
",,,,
approach,"In response , this paper presents an application of dilated convolutions for sequence labeling ) .","application of
dilated convolutions
for
sequence labeling","dilated convolutions||for||sequence labeling
",,"Approach||application of||dilated convolutions
",,,,,,
approach,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .","operate on
sliding window of context
over
sequence","sliding window of context||over||sequence
",,"Approach||operate on||sliding window of context
",,,,,,
approach,"By stacking layers of dilated convolutions of exponentially increasing dilation width , we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers :","stacking
layers
of
dilated convolutions
of
exponentially increasing dilation width
expand the size
effective input width
to cover
entire length of most sequences","layers||of||dilated convolutions
dilated convolutions||of||exponentially increasing dilation width
dilated convolutions||expand the size||effective input width
effective input width||to cover||entire length of most sequences
",,"Approach||stacking||layers
",,,,,,
approach,Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .,"iterated dilated CNN architecture ( ID - CNN )
applies
same block of dilated convolutions
to
token - wise representations","iterated dilated CNN architecture ( ID - CNN )||applies||same block of dilated convolutions
same block of dilated convolutions||to||token - wise representations
",,,"Approach||has||iterated dilated CNN architecture ( ID - CNN )
",,,,,
approach,This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .,"prevents
overfitting
provides opportunities to
inject supervision on intermediate activations of the network",,,,,,"token - wise representations||prevents||overfitting
token - wise representations||provides opportunities to||inject supervision on intermediate activations of the network
",,,
approach,"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .","two methods
for
performing prediction
predict
each token 's label independently
by running
Viterbi inference
in
chain structured graphical model","two methods||for||performing prediction
performing prediction||predict||each token 's label independently
performing prediction||by running||Viterbi inference
Viterbi inference||in||chain structured graphical model
",,,"Approach||has||two methods
",,,,,
baselines,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .","strong LSTM and CNN baselines
Bi - LSTM with local decoding , and one with CRF decoding",,,"Baselines||strong LSTM and CNN baselines||Bi - LSTM with local decoding , and one with CRF decoding
",,,,,,
baselines,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,"non-dilated CNN architecture
4 - layer CNN
5 - layer CNN",,,"Baselines||non-dilated CNN architecture||4 - layer CNN
","Baselines||has||5 - layer CNN
",,,,,
results,6.3 CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .,"CoNLL - 2003 English NER
Sentence - level prediction",,"CoNLL - 2003 English NER||has||Sentence - level prediction
",,"Results||has||CoNLL - 2003 English NER
",,,,,"Sentence - level prediction||has||Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN
"
results,"The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .","Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN
obtain
highest average scores","Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN||obtain||highest average scores
",,,,,,,,
results,"Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .","greedy ID - CNN
outperforms
Bi - LSTM and the 4 - layer CNN","greedy ID - CNN||outperforms||Bi - LSTM and the 4 - layer CNN
",,,,,,,"Sentence - level prediction||has||greedy ID - CNN
",
results,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .","When paired with
Viterbi decoding
ID - CNN
performs on par with
Bi - LSTM
showing
ID - CNN is also an effective token encoder
for
structured inference","ID - CNN||performs on par with||Bi - LSTM
Bi - LSTM||showing||ID - CNN is also an effective token encoder
ID - CNN is also an effective token encoder||for||structured inference
ID - CNN||When paired with||Viterbi decoding
",,,,,,,"Sentence - level prediction||has||ID - CNN
",
results,Document - level prediction,Document - level prediction,,,,,,,,"CoNLL - 2003 English NER||has||Document - level prediction
",
results,In we show that adding document - level context improves every model on CoNLL - 2003 .,"adding
document - level context
improves
every model
on
CoNLL - 2003","CoNLL - 2003||adding||document - level context
document - level context||improves||every model
",,,,,"Document - level prediction||on||CoNLL - 2003
",,,
results,OntoNotes 5.0 English NER,OntoNotes 5.0 English NER,,,,"Results||has||OntoNotes 5.0 English NER
",,,,,"OntoNotes 5.0 English NER||has||greedy model
"
results,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .","greedy model
out - performed by
Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 )","greedy model||out - performed by||Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 )
",,,,,,,,
research-problem,Semi-supervised sequence tagging with bidirectional language models,Semi-supervised sequence tagging,,,,,"Contribution||has research problem||Semi-supervised sequence tagging
",,,,
research-problem,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems,,,,,"Contribution||has research problem||general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems
",,,,
approach,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .","semisupervised approach
not require
additional labeled data","semisupervised approach||not require||additional labeled data
",,,"Approach||has||semisupervised approach
",,,,,
approach,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .","use
neural language model ( LM )
pre-trained on
large , unlabeled corpus
to compute
encoding of the context at each position in the sequence
LM embedding
use it in
supervised sequence tagging model","neural language model ( LM )||pre-trained on||large , unlabeled corpus
large , unlabeled corpus||to compute||encoding of the context at each position in the sequence
large , unlabeled corpus||use it in||supervised sequence tagging model
","encoding of the context at each position in the sequence||name||LM embedding
","Approach||use||neural language model ( LM )
",,,,,,
tasks,CoNLL 2003 NER .,CoNLL 2003 NER,,,,"Tasks||has||CoNLL 2003 NER
",,,,,"CoNLL 2003 NER||Hyperparameters||two bidirectional GRUs
"
tasks,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,"two bidirectional GRUs
with
80 hidden units
25 dimensional character embeddings
for
token character encoder","two bidirectional GRUs||for||token character encoder
token character encoder||with||80 hidden units
token character encoder||with||25 dimensional character embeddings
",,,,,,,,
tasks,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,"sequence layer
uses
bidirectional GRUs
with
300 hidden units each","sequence layer||uses||bidirectional GRUs
bidirectional GRUs||with||300 hidden units each
",,,,,,,"CoNLL 2003 NER||Hyperparameters||sequence layer
",
tasks,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .","regularization
add
25 % dropout
to the input of
each GRU","regularization||add||25 % dropout
25 % dropout||to the input of||each GRU
",,,,,,,"CoNLL 2003 NER||Hyperparameters||regularization
",
tasks,CoNLL 2000 chunking .,CoNLL 2000 chunking,,,,"Tasks||has||CoNLL 2000 chunking
",,,,,"CoNLL 2000 chunking||Hyperparameters||baseline sequence tagger
"
tasks,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,"baseline sequence tagger
uses
30 dimensional character embeddings
CNN
with
30 filters of width 3 characters
followed by
tanh non-linearity for the token character encoder","baseline sequence tagger||uses||30 dimensional character embeddings
baseline sequence tagger||uses||CNN
CNN||with||30 filters of width 3 characters
30 filters of width 3 characters||followed by||tanh non-linearity for the token character encoder
",,,,,,,,
tasks,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,"sequence layer
uses
two bidirectional LSTMs
200
hidden units","sequence layer||uses||two bidirectional LSTMs
two bidirectional LSTMs||hidden units||200
",,,,,,,"CoNLL 2000 chunking||Hyperparameters||sequence layer
",
tasks,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .","50 % dropout
to
character embeddings
input to each LSTM layer
output of the final LSTM layer","50 % dropout||to||character embeddings
50 % dropout||to||input to each LSTM layer
50 % dropout||to||output of the final LSTM layer
",,,,,,,"CoNLL 2000 chunking||Hyperparameters||50 % dropout
",
hyperparameters,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .","use
Adam optimizer
with
gradient norms
clipped at
5.0","Adam optimizer||with||gradient norms
gradient norms||clipped at||5.0
",,"Hyperparameters||use||Adam optimizer
",,,,,,
hyperparameters,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .","fine tune
pre-trained Senna word embeddings
fix
all weights
in
pre-trained language models","all weights||in||pre-trained language models
",,"Hyperparameters||fine tune||pre-trained Senna word embeddings
Hyperparameters||fix||all weights
",,,,,,
hyperparameters,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .","early stopping
to prevent
over-fitting","early stopping||to prevent||over-fitting
",,,"Hyperparameters||use||early stopping
",,,,,
hyperparameters,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,"train with
constant learning rate ? = 0.001",,,"Hyperparameters||train with||constant learning rate ? = 0.001
",,,,,,
results,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .","In
CoNLL 2003 NER task
scores
91.93 mean F 1","CoNLL 2003 NER task||scores||91.93 mean F 1
",,"Results||In||CoNLL 2003 NER task
",,,,,,
results,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .","CoNLL 2000 Chunking task
achieves
96.37 mean F 1","CoNLL 2000 Chunking task||achieves||96.37 mean F 1
",,,"Results||In||CoNLL 2000 Chunking task
",,,,,
research-problem,Deep contextualized word representations,Deep contextualized word representations,,,,,"Contribution||has research problem||Deep contextualized word representations
",,,,
approach,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"differ
traditional word type embeddings
each token
assigned
representation
function of the entire input sentence","each token||representation||function of the entire input sentence
",,"Approach||assigned||each token
Approach||differ||traditional word type embeddings
",,,,,,
approach,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,"use
vectors
derived from
bidirectional LSTM
trained with
coupled lan - guage model ( LM ) objective
on
large text corpus","vectors||derived from||bidirectional LSTM
vectors||trained with||coupled lan - guage model ( LM ) objective
coupled lan - guage model ( LM ) objective||on||large text corpus
",,"Approach||use||vectors
",,,,,,
approach,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","call
ELMo ( Embeddings from Language Models ) representations",,,"Approach||call||ELMo ( Embeddings from Language Models ) representations
",,,,,,
approach,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .","learn
linear combination of the vectors
stacked above
each input word
for
each end task
markedly improves
performance
over
using the top LSTM layer","linear combination of the vectors||stacked above||each input word
each input word||for||each end task
each end task||markedly improves||performance
performance||over||using the top LSTM layer
",,"Approach||learn||linear combination of the vectors
",,,,,,
tasks,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",Textual entailment,,,,"Tasks||has||Textual entailment
",,,,,"Textual entailment||has||Stanford Natural Language Inference ( SNLI )
"
tasks,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,Stanford Natural Language Inference ( SNLI ),,,,,,,,,
tasks,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .","adding
ELMo
to
ESIM model
improves
accuracy
by
average of 0.7 %
across
five random seeds","accuracy||by||average of 0.7 %
average of 0.7 %||across||five random seeds
accuracy||adding||ELMo
ELMo||to||ESIM model
",,,,,"Stanford Natural Language Inference ( SNLI )||improves||accuracy
",,,
tasks,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,Coreference resolution,,,,"Tasks||has||Coreference resolution
",,,,,
tasks,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .","with
OntoNotes coreference annotations
from
CoNLL 2012 shared task
adding
ELMo
improved
average F 1 by 3.2 %
from
67.2 to 70.4","OntoNotes coreference annotations||improved||average F 1 by 3.2 %
average F 1 by 3.2 %||adding||ELMo
average F 1 by 3.2 %||from||67.2 to 70.4
OntoNotes coreference annotations||from||CoNLL 2012 shared task
",,,,,"Coreference resolution||with||OntoNotes coreference annotations
",,,
research-problem,Sentence - State LSTM for Text Representation,Sentence - State LSTM for Text Representation,,,,,"Contribution||has research problem||Sentence - State LSTM for Text Representation
",,,,
research-problem,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .",alternative LSTM structure for encoding text,,,,,"Contribution||has research problem||alternative LSTM structure for encoding text
",,,,
research-problem,We investigate an alternative recurrent neural network structure for addressing these issues .,alternative recurrent neural network structure,,,,,"Contribution||has research problem||alternative recurrent neural network structure
",,,,
model,"As shown in , the main idea is to model the hidden states of all words simultaneously at each recurrent step , rather than one word at a time .","to model
hidden states of all words simultaneously
at
each recurrent step","hidden states of all words simultaneously||at||each recurrent step
",,"Model||to model||hidden states of all words simultaneously
",,,,,,
model,"In particular , we view the whole sentence as a single state , which consists of sub-states for individual words and an over all sentence - level state .","view
whole sentence
as
single state
consists of
sub-states
for
individual words
over all sentence - level state","whole sentence||as||single state
single state||consists of||sub-states
sub-states||for||individual words
single state||consists of||over all sentence - level state
",,"Model||view||whole sentence
",,,,,,
model,"To capture local and non-local contexts , states are updated recurrently by exchanging information between each other .","To capture
local and non-local contexts
states
updated recurrently
by
exchanging information between each other","local and non-local contexts||updated recurrently||states
states||by||exchanging information between each other
",,"Model||To capture||local and non-local contexts
",,,,,,
model,"At each recurrent step , information exchange is conducted between consecutive words in the sentence , and between the sentence - level state and each word .","At each recurrent step
information exchange
conducted between
consecutive words in the sentence
sentence - level state and each word","information exchange||conducted between||consecutive words in the sentence
information exchange||conducted between||sentence - level state and each word
",,"Model||At each recurrent step||information exchange
",,,,,,
model,"In particular , each word receives information from its predecessor and successor simultaneously .","each word
receives
information
from
predecessor and successor simultaneously","each word||receives||information
information||from||predecessor and successor simultaneously
",,,"Model||has||each word
",,,,,
code,"We release our code and models at https://github.com/ leuchine /S - LSTM , which include all baselines and the final model .",https://github.com/ leuchine /S - LSTM,,,,,"Contribution||Code||https://github.com/ leuchine /S - LSTM
",,,,
experimental-setup,All experiments are conducted using a GeForce GTX 1080 GPU with 8 GB memory .,"experiments
conducted using
GeForce GTX 1080 GPU
with
8 GB memory","experiments||conducted using||GeForce GTX 1080 GPU
GeForce GTX 1080 GPU||with||8 GB memory
",,,"Experimental setup||has||experiments
",,,,,
results,Final Results for Classification,"for
Classification",,,"Results||for||Classification
",,,,,,
results,"As shown in , the final results on the movie review dataset are consistent with the development results , where S - LSTM outperforms BiL - STM significantly , with a faster speed .","on
movie review dataset
with
S - LSTM
outperforms
BiL - STM
faster speed","S - LSTM||outperforms||BiL - STM
BiL - STM||with||faster speed
","movie review dataset||has||S - LSTM
",,,,"Classification||on||movie review dataset
",,,
results,"As shown in , among the 16 datasets of , S - LSTM gives the best results on 12 , compared with BiLSTM and 2 layered BiL - STM models .","among
16 datasets
gives
best results
on
12","16 datasets||gives||best results
best results||on||12
",,,,,"Classification||among||16 datasets
",,,
results,"The average accuracy of S - LSTM is 85.6 % , significantly higher compared with 84.9 % by 2 - layer stacked BiLSTM .","average accuracy
of
S - LSTM
85.6 %","S - LSTM||average accuracy||85.6 %
",,,,,"Classification||of||S - LSTM
",,,
results,Final Results for Sequence Labelling,Sequence Labelling,,,,"Results||for||Sequence Labelling
",,,,,
results,"For NER , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .","For
NER
S - LSTM
gives
F1 - score
91.57 %
on
CoNLL test set","NER||on||CoNLL test set
CoNLL test set||gives||S - LSTM
S - LSTM||F1 - score||91.57 %
",,,,,"Sequence Labelling||For||NER
",,,
research-problem,Robust Lexical Features for Improved Neural Network Named - Entity Recognition,Neural Network Named - Entity Recognition,,,,,"Contribution||has research problem||Neural Network Named - Entity Recognition
",,,,
research-problem,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,Neural network approaches to Named - Entity Recognition,,,,,"Contribution||has research problem||Neural network approaches to Named - Entity Recognition
",,,,
research-problem,Named - Entity Recognition ( NER ) is the task of identifying textual mentions and classifying them into a predefined set of types .,"Named - Entity Recognition
NER",,,,,"Contribution||has research problem||Named - Entity Recognition
Contribution||has research problem||NER
",,,,
model,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .","propose
alternative lexical representation
trained
offline
added to
any neural NER system","alternative lexical representation||trained||offline
alternative lexical representation||added to||any neural NER system
",,"Model||propose||alternative lexical representation
",,,,,,
model,"In a nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .","embed
words and entity types
into
joint vector space
by leveraging
WiFiNE","words and entity types||into||joint vector space
joint vector space||by leveraging||WiFiNE
",,"Model||embed||words and entity types
",,,,,,
experimental-setup,Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a gradient clipping of 5.0 .,"Training
carried out by
mini-batch stochastic gradient descent ( SGD )
with
momentum of 0.9
gradient clipping of 5.0","Training||carried out by||mini-batch stochastic gradient descent ( SGD )
mini-batch stochastic gradient descent ( SGD )||with||momentum of 0.9
mini-batch stochastic gradient descent ( SGD )||with||gradient clipping of 5.0
",,,"Experimental setup||has||Training
",,,,,
experimental-setup,"The mini-batch is 10 for both datasets , and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively .","mini-batch
is
10
for
both datasets
learning rates
are
0.009 and 0.013
for
CONLL and ONTONOTES","mini-batch||is||10
10||for||both datasets
learning rates||are||0.009 and 0.013
0.009 and 0.013||for||CONLL and ONTONOTES
",,,"Experimental setup||has||mini-batch
Experimental setup||has||learning rates
",,,,,
experimental-setup,"We varied dropout . 65 ] ) , hidden units ) , capitalization ) and char ) embedding dimensions , learning rate ( [ 0.001 , 0.015 ] by step 0.002 ) , and optimization algorithms and fixed the other hyper - parameters .",,,,,,,,,,
experimental-setup,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .","implemented
system
using
Tensorflow library
ran
models
on
GeForce GTX TITAN Xp GPU","system||using||Tensorflow library
models||on||GeForce GTX TITAN Xp GPU
",,"Experimental setup||implemented||system
Experimental setup||ran||models
",,,,,,
results,"First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .","significantly outperforms
models
use
extensive sets of handcrafted features","models||use||extensive sets of handcrafted features
",,,,,"CONLL||significantly outperforms||models
",,,
results,"Second , our model outperforms as well other NN models that only use standard word embeddings , which indicates that our lexical feature vector is complementary to standard word embeddings .","outperforms
other NN models
that only use
standard word embeddings
indicates
our lexical feature vector is complementary to standard word embeddings","other NN models||that only use||standard word embeddings
standard word embeddings||indicates||our lexical feature vector is complementary to standard word embeddings
",,,,,"CONLL||outperforms||other NN models
",,,
results,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .","matches
state - of - the - art performances
of
models
that use
more complex architectures
more elaborate features","state - of - the - art performances||of||models
models||that use||more complex architectures
models||that use||more elaborate features
",,,,,"CONLL||matches||state - of - the - art performances
",,,
results,"In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .","significantly outperforms
Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 )
by
an absolute gain
of
1.68 and 0.96 points respectively","Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 )||by||an absolute gain
an absolute gain||of||1.68 and 0.96 points respectively
",,,,,"ONTONOTES||significantly outperforms||Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 )
",,,
results,"Less surprisingly , it surpasses systems with hand - crafted features , including that use gazetteers , and the system of which uses coreference annotation in ONTONOTES to jointly model NER , entity linking , and coreference resolution tasks .","surpasses
systems with hand - crafted features
including
use gazetteers
system of which uses coreference annotation
in
ONTONOTES
to jointly model
NER , entity linking , and coreference resolution tasks","ONTONOTES||surpasses||system of which uses coreference annotation
system of which uses coreference annotation||in||ONTONOTES
ONTONOTES||to jointly model||NER , entity linking , and coreference resolution tasks
ONTONOTES||surpasses||systems with hand - crafted features
systems with hand - crafted features||including||use gazetteers
",,,,,,,,
results,Results on CONLL,"on
CONLL",,,"Results||on||CONLL
",,,,,,
results,Results on ONTONOTES,ONTONOTES,,,,"Results||on||ONTONOTES
",,,,,
ablation-analysis,We also observe that models that use both feature sets significantly outperform other configurations .,"observe
models that use both feature sets
significantly outperform
other configurations","models that use both feature sets||significantly outperform||other configurations
",,"Ablation analysis||observe||models that use both feature sets
",,,,,,
ablation-analysis,"We observe that on both CONLL and ONTONOTES , the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average .","observe
on
both CONLL and ONTONOTES
SSKIP model
outperforms
our feature vector approach
by
0.65 F1 points on average","both CONLL and ONTONOTES||observe||SSKIP model
SSKIP model||outperforms||our feature vector approach
our feature vector approach||by||0.65 F1 points on average
",,"Ablation analysis||on||both CONLL and ONTONOTES
",,,,,,
research-problem,A Neural Transition - based Model for Nested Mention Recognition,Nested Mention Recognition,,,,,"Contribution||has research problem||Nested Mention Recognition
",,,,
research-problem,This paper introduces a scalable transition - based method to model the nested structure of mentions .,model the nested structure of mentions,,,,,"Contribution||has research problem||model the nested structure of mentions
",,,,
research-problem,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .",nested hierarchical structure of entity mentions should betaken into account,,,,,"Contribution||has research problem||nested hierarchical structure of entity mentions should betaken into account
",,,,
model,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .","To achieve
scalable and effective solution
for recognizing
nested mentions
design
transition - based system","transition - based system||To achieve||scalable and effective solution
scalable and effective solution||for recognizing||nested mentions
",,"Model||design||transition - based system
",,,,,,
model,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .","each sentence with nested mentions
mapped
to
forest where each outermost mention forms a tree consisting of its inner mentions","each sentence with nested mentions||to||forest where each outermost mention forms a tree consisting of its inner mentions
",,"Model||mapped||each sentence with nested mentions
",,,,,,
model,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,"learns to construct
this forest
through
sequence of shift - reduce actions","this forest||through||sequence of shift - reduce actions
",,"Model||learns to construct||this forest
",,,,,,
model,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .","employ
Stack - LSTM
to represent
system 's state
consists of
states of input , stack and action history
in
continuous space incrementally","Stack - LSTM||to represent||system 's state
system 's state||consists of||states of input , stack and action history
states of input , stack and action history||in||continuous space incrementally
",,"Model||employ||Stack - LSTM
",,,,,,
model,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,"( partially ) processed nested mentions
encoded with
recursive neural networks
where
composition functions
are used to capture
dependencies
between
nested mentions","( partially ) processed nested mentions||encoded with||recursive neural networks
recursive neural networks||where||composition functions
composition functions||are used to capture||dependencies
dependencies||between||nested mentions
",,,,,,,"Stack - LSTM||has||( partially ) processed nested mentions
",
model,"Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .","incorporate
characterlevel LSTM
to capture
morphological information","characterlevel LSTM||to capture||morphological information
",,,,,"Stack - LSTM||incorporate||characterlevel LSTM
",,,
model,"Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .","help deal with
out - of - vocabulary problem
of
neural models","out - of - vocabulary problem||of||neural models
",,,,,"characterlevel LSTM||help deal with||out - of - vocabulary problem
",,,
hyperparameters,Pre-trained embeddings,Pre-trained embeddings,,,,,,,,,"Hyperparameters||Pre-trained embeddings||Glo Ve
"
hyperparameters,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,"Glo Ve
of
dimension 100
used to initialize
word vectors
for
all three datasets","Glo Ve||of||dimension 100
Glo Ve||used to initialize||word vectors
word vectors||for||all three datasets
",,,,,,,,
hyperparameters,9 The embeddings of POS tags are initialized randomly with dimension 32 .,"embeddings of POS tags
initialized randomly
with
dimension 32","initialized randomly||with||dimension 32
",,"Hyperparameters||embeddings of POS tags||initialized randomly
",,,,,,
hyperparameters,The model is trained using Adam and a gradient clipping of 3.0 .,"trained using
Adam
gradient clipping of 3.0",,,"Hyperparameters||trained using||Adam
Hyperparameters||trained using||gradient clipping of 3.0
",,,,,,
results,Our neural transition - based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F 1 measure .,"Our neural transition - based model
achieves
best results
in
ACE datasets
comparable results
in
GENIA dataset
in terms of
F 1 measure","Our neural transition - based model||achieves||best results
best results||in||ACE datasets
Our neural transition - based model||achieves||comparable results
comparable results||in||GENIA dataset
","F 1 measure||has||Our neural transition - based model
","Results||in terms of||F 1 measure
",,,,,,
results,"We can observe that the margin of improvement is more significant in the portion of nested mentions , revealing our model 's effectiveness in handling nested mentions .","margin of improvement
more significant
in
portion of nested mentions
revealing
model 's effectiveness
in handling
nested mentions","margin of improvement||in||portion of nested mentions
portion of nested mentions||revealing||model 's effectiveness
model 's effectiveness||in handling||nested mentions
",,"Results||more significant||margin of improvement
",,,,,,
ablation-analysis,"To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .","evaluate
contribution of neural components
including
pre-trained embeddings , the characterlevel LSTM and dropout layers","contribution of neural components||including||pre-trained embeddings , the characterlevel LSTM and dropout layers
",,"Ablation analysis||evaluate||contribution of neural components
",,,,,,
ablation-analysis,"From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .","contribute significantly to
effectiveness of our model
in
all three datasets","effectiveness of our model||in||all three datasets
",,,,,"pre-trained embeddings , the characterlevel LSTM and dropout layers||contribute significantly to||effectiveness of our model
",,,
research-problem,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,Language Understanding,,,,,"Contribution||has research problem||Language Understanding
",,,,
research-problem,"We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",language representation model,,,,,"Contribution||has research problem||language representation model
",,,,
research-problem,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,Language model pre-training,,,,,"Contribution||has research problem||Language model pre-training
",,,,
approach,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .","proposing
BERT : Bidirectional Encoder Representations from Transformers",,,"Approach||proposing||BERT : Bidirectional Encoder Representations from Transformers
",,,,,,
approach,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .","BERT
alleviates
unidirectionality constraint
using
"" masked language model "" ( MLM ) pre-training objective","BERT||alleviates||unidirectionality constraint
unidirectionality constraint||using||"" masked language model "" ( MLM ) pre-training objective
",,,"Approach||name||BERT
",,,,,
approach,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .","masked language model
randomly masks
some of the tokens
from
input
objective
predict the original vocabulary id
of
masked word
based only on
context","masked language model||randomly masks||some of the tokens
some of the tokens||from||input
masked language model||objective||predict the original vocabulary id
predict the original vocabulary id||of||masked word
masked word||based only on||context
",,,"Approach||has||masked language model
",,,,,
approach,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .","MLM objective
enables
representation to fuse the left and the right context
allows us to pretrain
deep bidirectional Transformer","MLM objective||enables||representation to fuse the left and the right context
representation to fuse the left and the right context||allows us to pretrain||deep bidirectional Transformer
",,,"Approach||has||MLM objective
",,,,,
approach,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .","use
"" next sentence prediction "" task
jointly pretrains
text - pair representations",""" next sentence prediction "" task||jointly pretrains||text - pair representations
",,"Approach||use||"" next sentence prediction "" task
",,,,,,
tasks,GLUE,GLUE,,,,"Tasks||has||GLUE
",,,,,"GLUE||name||General Language Understanding Evaluation ( GLUE )
"
tasks,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",General Language Understanding Evaluation ( GLUE ),,,,,,,,,
tasks,GLUE,,,,,,,,,,
tasks,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,"use
batch size
of
32
fine - tune
for 3 epochs
over
data for all GLUE tasks","batch size||of||32
for 3 epochs||over||data for all GLUE tasks
",,,,,"Hyperparameters||use||batch size
Hyperparameters||fine - tune||for 3 epochs
",,,
tasks,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .","selected
best fine - tuning learning rate
among
5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5
on
Dev set","best fine - tuning learning rate||among||5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5
best fine - tuning learning rate||on||Dev set
",,,,,"Hyperparameters||selected||best fine - tuning learning rate
",,,
tasks,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .","for
BERT LARGE
ran
several random restarts
selected
best model on the Dev set","BERT LARGE||ran||several random restarts
several random restarts||selected||best model on the Dev set
",,,,,"Hyperparameters||for||BERT LARGE
",,,
tasks,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .","BERT BASE and BERT LARGE
outperform
all systems on all tasks
by
substantial margin
obtaining
4.5 % and 7.0 % respective average accuracy improvement
over
prior state of the art","BERT BASE and BERT LARGE||outperform||all systems on all tasks
all systems on all tasks||by||substantial margin
substantial margin||obtaining||4.5 % and 7.0 % respective average accuracy improvement
4.5 % and 7.0 % respective average accuracy improvement||over||prior state of the art
",,,,,,,"Results||has||BERT BASE and BERT LARGE
",
tasks,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .","BERT LARGE
significantly outperforms
BERT BASE
across
all tasks","BERT LARGE||significantly outperforms||BERT BASE
BERT BASE||across||all tasks
",,,,,,,"Results||has||BERT LARGE
Results||has||BERT LARGE
",
tasks,SQuAD v 1.1,SQuAD v 1.1,,,,"Tasks||has||SQuAD v 1.1
",,,,,"SQuAD v 1.1||name||Stanford Question Answering Dataset ( SQuAD v1.1 )
"
tasks,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,Stanford Question Answering Dataset ( SQuAD v1.1 ),,,,,,,,,
tasks,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,"fine - tune
3 epochs
learning rate
5 e - 5
batch size
32","3 epochs||learning rate||5 e - 5
3 epochs||batch size||32
",,,,,"Hyperparameters||fine - tune||3 epochs
Hyperparameters||fine - tune||3 epochs
",,,
tasks,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,"best performing system
outperforms
top leaderboard system
by
+ 1.5 F1
in
ensembling
+ 1.3 F1
as
single system","best performing system||outperforms||top leaderboard system
top leaderboard system||by||+ 1.5 F1
+ 1.5 F1||in||ensembling
top leaderboard system||by||+ 1.3 F1
+ 1.3 F1||as||single system
",,,,,,,"Results||has||best performing system
",
tasks,SQuAD v 2.0,SQuAD v 2.0,,,,"Tasks||has||SQuAD v 2.0
",,,,,"SQuAD v 2.0||has||Hyperparameters
"
tasks,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,"fine - tuned
2 epochs
learning rate
5 e - 5
batch size
48","2 epochs||learning rate||5 e - 5
2 epochs||batch size||48
",,,,,"Hyperparameters||fine - tuned||2 epochs
",,,
tasks,We observe a + 5.1 F1 improvement over the previous best system .,"observe
+ 5.1 F1 improvement
over
previous best system","+ 5.1 F1 improvement||over||previous best system
",,,,,"Results||observe||+ 5.1 F1 improvement
",,,
tasks,SWAG,SWAG,,,,"Tasks||has||SWAG
",,,,,"SWAG||name||Situations With Adversarial Generations
"
tasks,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,Situations With Adversarial Generations,,,,,,,,,
tasks,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,"fine - tune
3 epochs
learning rate
2 e - 5
batch size
16","3 epochs||learning rate||2 e - 5
3 epochs||batch size||16
",,,,,,,,
tasks,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,"BERT LARGE
outperforms
authors ' baseline ESIM + ELMo system
by
+ 27.1 %
OpenAI GPT
by
8.3 %","BERT LARGE||outperforms||OpenAI GPT
OpenAI GPT||by||8.3 %
BERT LARGE||outperforms||authors ' baseline ESIM + ELMo system
authors ' baseline ESIM + ELMo system||by||+ 27.1 %
",,,,,,,,
ablation-analysis,Effect of Model Size,Effect of Model Size,,,,"Ablation analysis||has||Effect of Model Size
",,,,,
ablation-analysis,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .","is
first work
to demonstrate
scaling to extreme model
leads to
large improvements
on
very small scale tasks
provided that
model has been sufficiently pre-trained","first work||to demonstrate||scaling to extreme model
scaling to extreme model||leads to||large improvements
large improvements||on||very small scale tasks
very small scale tasks||provided that||model has been sufficiently pre-trained
",,,,,"Effect of Model Size||is||first work
",,,
ablation-analysis,Feature - based Approach with BERT,Feature - based Approach with BERT,,,,"Ablation analysis||has||Feature - based Approach with BERT
",,,,,"Feature - based Approach with BERT||has||BERT LARGE
"
ablation-analysis,BERT LARGE performs competitively with state - of - the - art methods .,"BERT LARGE
performs competitively
state - of - the - art methods","BERT LARGE||performs competitively||state - of - the - art methods
",,,,,,,,
ablation-analysis,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,"demonstrates
BERT is effective
for both
finetuning
feature - based approaches","BERT is effective||for both||finetuning
BERT is effective||for both||feature - based approaches
",,,,,"Feature - based Approach with BERT||demonstrates||BERT is effective
",,,
research-problem,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,pre-trained biomedical language representation,,,,,"Contribution||has research problem||pre-trained biomedical language representation
",,,,
research-problem,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",extracting valuable information from biomedical literature,,,,,"Contribution||has research problem||extracting valuable information from biomedical literature
",,,,
research-problem,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora,,,,,"Contribution||has research problem||biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora
",,,,
research-problem,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",pre-trained language model BERT can be adapted for biomedical corpora,,,,,"Contribution||has research problem||pre-trained language model BERT can be adapted for biomedical corpora
",,,,
code,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",https://github. com/naver/biobert-pretrained,,,,,"Contribution||Code||https://github. com/naver/biobert-pretrained
",,,,
research-problem,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",word distributions of general and biomedical corpora are quite different,,,,,"Contribution||has research problem||word distributions of general and biomedical corpora are quite different
",,,,
approach,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .","introduce
BioBERT
is
pre-trained language representation model
for
biomedical domain","BioBERT||is||pre-trained language representation model
pre-trained language representation model||for||biomedical domain
",,"Approach||introduce||BioBERT
",,,,,,
approach,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .","initialize
BioBERT
with
weights
from
BERT
pretrained on
general domain corpora ( English Wikipedia and Books Corpus )","BioBERT||with||weights
weights||from||BERT
BERT||pretrained on||general domain corpora ( English Wikipedia and Books Corpus )
",,"Approach||initialize||BioBERT
",,,,,,
approach,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .","BioBERT
pre-trained on
biomedical domain corpora ( PubMed abstracts and PMC full - text articles )","BioBERT||pre-trained on||biomedical domain corpora ( PubMed abstracts and PMC full - text articles )
",,,"Approach||has||BioBERT
",,,,,
approach,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .","fine - tuned and evaluated on
three popular biomedical text mining tasks",,,,,,"BioBERT||fine - tuned and evaluated on||three popular biomedical text mining tasks
",,,
experimental-setup,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,"used
BERT BASE model
pre-trained on
English Wikipedia and Books Corpus
for
1 M steps","BERT BASE model||pre-trained on||English Wikipedia and Books Corpus
English Wikipedia and Books Corpus||for||1 M steps
",,"Experimental setup||used||BERT BASE model
",,,,,,
experimental-setup,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,"BioBERT v1.0 ( PubMed PMC )
version of
BioBERT ( PubMed PMC )
trained for
470 K steps","BioBERT v1.0 ( PubMed PMC )||version of||BioBERT ( PubMed PMC )
BioBERT ( PubMed PMC )||trained for||470 K steps
",,,"Experimental setup||has||BioBERT v1.0 ( PubMed PMC )
",,,,,
experimental-setup,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,"eight NVIDIA V100 ( 32GB ) GPUs
for
pre-training","eight NVIDIA V100 ( 32GB ) GPUs||for||pre-training
",,,"Experimental setup||used||eight NVIDIA V100 ( 32GB ) GPUs
",,,,,
experimental-setup,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .","maximum sequence length
fixed to
512
mini-batch size
set to
192","mini-batch size||set to||192
maximum sequence length||fixed to||512
",,,"Experimental setup||has||mini-batch size
Experimental setup||has||maximum sequence length
",,,,,
experimental-setup,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,"single NVIDIA Titan Xp ( 12GB ) GPU
to fine - tune
BioBERT
on
each task","single NVIDIA Titan Xp ( 12GB ) GPU||to fine - tune||BioBERT
BioBERT||on||each task
",,,"Experimental setup||used||single NVIDIA Titan Xp ( 12GB ) GPU
",,,,,
results,The results of NER are shown in .,"of
NER",,,"Results||of||NER
",,,,,,"NER||has||BioBERT
"
results,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .","BioBERT
achieves
higher scores
than
BERT","BioBERT||achieves||higher scores
higher scores||than||BERT
",,,,,,,,
results,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .","outperformed
state - of - the - art models
on
six out of nine datasets
BioBERT v 1.1 ( PubMed )
outperformed
state - of - the - art models
by
0.62
in terms of
micro averaged F1 score","BioBERT v 1.1 ( PubMed )||outperformed||state - of - the - art models
state - of - the - art models||by||0.62
0.62||in terms of||micro averaged F1 score
state - of - the - art models||on||six out of nine datasets
",,,,,"BioBERT||outperformed||state - of - the - art models
",,"NER||has||BioBERT v 1.1 ( PubMed )
",
results,The RE results of each model are shown in .,RE,,,,"Results||of||RE
",,,,,"RE||has||BioBERT v1.0 ( PubMed )
"
results,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .","BioBERT v1.0 ( PubMed )
obtained
higher F1 score ( 2.80 higher )
than
state - of - the - art models","BioBERT v1.0 ( PubMed )||obtained||higher F1 score ( 2.80 higher )
higher F1 score ( 2.80 higher )||than||state - of - the - art models
",,,,,,,,
results,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .","BioBERT
achieved
highest F 1 scores
on
2 out of 3 biomedical datasets","BioBERT||achieved||highest F 1 scores
highest F 1 scores||on||2 out of 3 biomedical datasets
",,,,,,,"RE||has||BioBERT
",
results,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .","of
BioBERT
significantly outperformed
BERT
state - of - the - art models
BioBERT v1.1 ( PubMed )
obtained
strict accuracy
of
38.77
lenient accuracy
of
53.81
mean reciprocal rank score
44.77","BioBERT v1.1 ( PubMed )||obtained||mean reciprocal rank score
mean reciprocal rank score||of||44.77
BioBERT v1.1 ( PubMed )||obtained||lenient accuracy
lenient accuracy||of||53.81
BioBERT v1.1 ( PubMed )||obtained||strict accuracy
strict accuracy||of||38.77
BioBERT||significantly outperformed||BERT
BioBERT||significantly outperformed||state - of - the - art models
",,,,,,,"QA||has||BioBERT v1.1 ( PubMed )
QA||has||BioBERT
","Results||of||QA
"
results,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .","biomedical QA datasets
BioBERT
achieved
new state - of - the - art performance
in terms of
MRR","BioBERT||achieved||new state - of - the - art performance
new state - of - the - art performance||in terms of||MRR
","biomedical QA datasets||has||BioBERT
",,,,,,"QA||has||biomedical QA datasets
",
research-problem,Gated - Attention Readers for Text Comprehension,Text Comprehension,,,,,"Contribution||has research problem||Text Comprehension
",,,,
code,Source code is available on github : https:// github.com/bdhingra/ga-reader,https:// github.com/bdhingra/ga-reader,,,,,"Contribution||Code||https:// github.com/bdhingra/ga-reader
",,,,
research-problem,A recent trend to measure progress towards machine reading is to test a system 's ability to answer questions about a document it has to comprehend .,machine reading,,,,,"Contribution||has research problem||machine reading
",,,,
model,"More specifically , unlike existing models where the query attention is applied either token - wise or sentence - wise to allow weighted aggregation , the Gated - Attention ( GA ) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic - level , and is applied layer - wise as information filters during the multi-hop representation learning process .","query
applied
Gated - Attention ( GA ) module
allows
to directly interact with
each dimension
of
token embeddings
at
semantic - level
layer - wise
as
information filters
during
multi-hop representation learning process","Gated - Attention ( GA ) module||allows||query
query||to directly interact with||each dimension
each dimension||of||token embeddings
token embeddings||at||semantic - level
Gated - Attention ( GA ) module||applied||layer - wise
layer - wise||as||information filters
information filters||during||multi-hop representation learning process
",,,"Model||has||Gated - Attention ( GA ) module
",,,,,
model,"Such a fine - grained attention enables our model to learn conditional token representations w.r.t. the given question , leading to accurate answer selections .","fine - grained attention
enables
model
to learn
conditional token representations
w.r.t.
given question","fine - grained attention||enables||model
model||to learn||conditional token representations
conditional token representations||w.r.t.||given question
",,,"Model||has||fine - grained attention
",,,,,
results,"Interestingly , we observe that feature engineering leads to significant improvements for WDW and CBT datasets , but not for CNN and Daily Mail datasets .","observe that
feature engineering
leads to
significant improvements
for
WDW and CBT datasets
not for
CNN and Daily Mail datasets","feature engineering||leads to||significant improvements
significant improvements||not for||CNN and Daily Mail datasets
significant improvements||for||WDW and CBT datasets
",,"Results||observe that||feature engineering
",,,,,,
results,"Similarly , fixing the word embeddings provides an improvement for the WDW and CBT , but not for CNN and Daily Mail .","fixing
word embeddings
provides
improvement
for
WDW and CBT
not for
CNN and Daily Mail","word embeddings||provides||improvement
improvement||not for||CNN and Daily Mail
improvement||for||WDW and CBT
",,"Results||fixing||word embeddings
",,,,,,
results,"Comparing with prior work , on the WDW dataset the basic version of the GA Reader outperforms all previously published models when trained on the Strict setting .","on
WDW dataset
basic version of the GA Reader
outperforms
all previously published models
when trained on
Strict setting","basic version of the GA Reader||when trained on||Strict setting
basic version of the GA Reader||outperforms||all previously published models
","WDW dataset||has||basic version of the GA Reader
","Results||on||WDW dataset
",,,,,,
results,By adding the qecomm feature the performance increases by 3.2 % and 3.5 % on the Strict and Relaxed settings respectively to set a new state of the art on this dataset .,"adding
qecomm feature
performance
increases
by
3.2 % and 3.5 %
on
Strict and Relaxed settings","increases||by||3.2 % and 3.5 %
increases||on||Strict and Relaxed settings
","qecomm feature||has||performance
performance||has||increases
","Results||adding||qecomm feature
",,,,,,
results,On the CNN and Daily Mail datasets the GA Reader leads to an improvement of 3.2 % and 4.3 % respectively over the best previous single models .,"On
CNN and Daily Mail datasets
GA Reader
leads to
improvement
of
3.2 % and 4.3 %
over
best previous single models","GA Reader||leads to||improvement
improvement||of||3.2 % and 4.3 %
3.2 % and 4.3 %||over||best previous single models
","CNN and Daily Mail datasets||has||GA Reader
","Results||On||CNN and Daily Mail datasets
",,,,,,
results,"For CBT - NE , GA Reader with the qecomm feature outperforms all previous single and ensemble models except the AS Reader trained on the much larger BookTest Corpus .","For
CBT - NE
GA Reader
with
qecomm feature
outperforms
all previous single and ensemble models
except
AS Reader
trained on
much larger BookTest Corpus","GA Reader||with||qecomm feature
GA Reader||outperforms||all previous single and ensemble models
all previous single and ensemble models||except||AS Reader
AS Reader||trained on||much larger BookTest Corpus
","CBT - NE||has||GA Reader
","Results||For||CBT - NE
",,,,,,
results,"Lastly , on CBT - CN the GA Reader with the qe-comm feature outperforms all previously published single models except the NSE , and AS Reader trained on a larger corpus .","CBT - CN
GA Reader
with
qe-comm feature
outperforms
all previously published single models
except
NSE
AS Reader
trained on
larger corpus","GA Reader||with||qe-comm feature
GA Reader||outperforms||all previously published single models
all previously published single models||except||NSE
all previously published single models||except||AS Reader
AS Reader||trained on||larger corpus
","CBT - CN||has||GA Reader
",,"Results||on||CBT - CN
",,,,,
ablation-analysis,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .","observe
substantial drop
when removing
tokenspecific attentions
over
query in the GA module
allow gating
individual tokens
in
document
by
parts of the query
relevant to
token
rather than
over all query representation","substantial drop||when removing||tokenspecific attentions
tokenspecific attentions||over||query in the GA module
tokenspecific attentions||allow gating||individual tokens
individual tokens||in||document
individual tokens||by||parts of the query
parts of the query||relevant to||token
token||rather than||over all query representation
",,"Ablation analysis||observe||substantial drop
",,,,,,
ablation-analysis,"Finally , removing the character embeddings , which were only used for WDW and CBT , leads to a reduction of about 1 % in the performance .","removing
character embeddings
used for
WDW and CBT
leads to
reduction
of
about 1 % in the performance","character embeddings||leads to||reduction
reduction||of||about 1 % in the performance
character embeddings||used for||WDW and CBT
",,"Ablation analysis||removing||character embeddings
",,,,,,
research-problem,Large - scale Simple Question Answering with Memory Networks,Large - scale Simple Question Answering,,,,,"Contribution||has research problem||Large - scale Simple Question Answering
",,,,
research-problem,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,large - scale question answering,,,,,"Contribution||has research problem||large - scale question answering
",,,,
research-problem,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",simple question answering,,,,,"Contribution||has research problem||simple question answering
",,,,
research-problem,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",Simple Question Answering,,,,,"Contribution||has research problem||Simple Question Answering
",,,,
dataset,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .","collected
first large - scale dataset of questions and answers
based on
KB
called
SimpleQuestions","first large - scale dataset of questions and answers||called||SimpleQuestions
first large - scale dataset of questions and answers||based on||KB
",,"Dataset||collected||first large - scale dataset of questions and answers
",,,,,,
dataset,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?","contains
more than 100 k questions",,,"Dataset||contains||more than 100 k questions
",,,,,,
model,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .","present
embedding - based QA system
developed under
framework of Memory Networks ( Mem NNs )","embedding - based QA system||developed under||framework of Memory Networks ( Mem NNs )
",,"Model||present||embedding - based QA system
",,,,,,
model,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .","Memory Networks
are
learning systems
centered around
memory component
can be
read and written to
focus on cases where
relationship
between
input and response languages ( here natural language )
storage language ( here , the facts from KBs )
performed by
embedding
in
same vector space","Memory Networks||are||learning systems
learning systems||focus on cases where||relationship
relationship||performed by||embedding
embedding||in||same vector space
embedding||between||input and response languages ( here natural language )
embedding||between||storage language ( here , the facts from KBs )
learning systems||centered around||memory component
memory component||can be||read and written to
",,,"Model||has||Memory Networks
",,,,,
model,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,"setting of
simple QA
corresponds to
elementary operation
of performing
single lookup
in
memory","simple QA||corresponds to||elementary operation
elementary operation||of performing||single lookup
single lookup||in||memory
",,"Model||setting of||simple QA
",,,,,,
hyperparameters,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?","embedding dimension and the learning rate
chosen among
{ 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 }
margin","embedding dimension and the learning rate||chosen among||{ 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 }
",,,"Hyperparameters||has||margin
Hyperparameters||has||embedding dimension and the learning rate
",,,,,
hyperparameters,was set to 0.1 .,"set to
0.1",,,,,,"margin||set to||0.1
",,,
results,"On WebQuestions , not specifically designed as a simple QA dataset , 86 % of the questions can now be answered with a single supporting fact , and performance increases significantly ( from 36.2 % to 41.0 % F1-score ) .","On
WebQuestions
86 % of the questions
answered with
single supporting fact
performance
increases significantly
from
36.2 %
to
41.0 % F1-score","performance||from||36.2 %
36.2 %||to||41.0 % F1-score
86 % of the questions||answered with||single supporting fact
","WebQuestions||has||86 % of the questions
86 % of the questions||has||performance
performance||has||increases significantly
","Results||On||WebQuestions
",,,,,,
results,"Using the bigger FB5M as KB does not change performance on SimpleQuestions because it was based on FB2M , but the results show that our model is robust to the addition of more entities than necessary .","Using
bigger FB5M as KB
not change
performance
on
SimpleQuestions","bigger FB5M as KB||not change||performance
performance||on||SimpleQuestions
",,"Results||Using||bigger FB5M as KB
",,,,,,
results,Transfer learning on Reverb,Transfer learning on Reverb,,,,"Results||has||Transfer learning on Reverb
",,,,,"Transfer learning on Reverb||has||best results
"
results,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .","best results
are
67 % accuracy
68 %
for
ensemble of 5 models","best results||are||67 % accuracy
best results||are||68 %
68 %||for||ensemble of 5 models
",,,,,,,,
results,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .","notice
models
trained on
single QA dataset
perform
poorly
on
other datasets","models||trained on||single QA dataset
single QA dataset||perform||poorly
poorly||on||other datasets
",,,,,"Transfer learning on Reverb||notice||models
",,,
results,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .","training on
both datasets
improves
performance","both datasets||improves||performance
",,,,,"Transfer learning on Reverb||training on||both datasets
",,,
results,Importance of data sources,Importance of data sources,,,,"Results||has||Importance of data sources
",,,,,"Importance of data sources||has||paraphrases
"
results,"While paraphrases do not seem to help much on WebQuestions and SimpleQuestions , except when training only with synthetic questions , they have a dramatic impact on the performance on Reverb .","paraphrases
not seem to
help much
on
WebQuestions and SimpleQuestions
except when
training
only with
synthetic questions
have
dramatic impact
on
performance
on
Reverb","paraphrases||not seem to||help much
help much||except when||training
training||only with||synthetic questions
synthetic questions||have||dramatic impact
dramatic impact||on||performance
performance||on||Reverb
help much||on||WebQuestions and SimpleQuestions
",,,,,,,,
research-problem,Learning to Compose Task - Specific Tree Structures,Task - Specific Tree Structures,,,,,"Contribution||has research problem||Task - Specific Tree Structures
",,,,
research-problem,"In this paper , we propose Gumbel Tree - LSTM , a novel tree - structured long short - term memory architecture that learns how to compose task - specific tree structures only from plain text data efficiently .",task - specific tree structures only from plain text data,,,,,"Contribution||has research problem||task - specific tree structures only from plain text data
",,,,
model,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without explicit guidance .","propose
Gumbel Tree - LSTM
is
novel RvNN architecture
not require
structured data
learns to compose
task - specific tree structures
without
explicit guidance","Gumbel Tree - LSTM||is||novel RvNN architecture
novel RvNN architecture||learns to compose||task - specific tree structures
task - specific tree structures||without||explicit guidance
novel RvNN architecture||not require||structured data
",,"Model||propose||Gumbel Tree - LSTM
",,,,,,
model,"Our Gumbel Tree - LSTM model is based on tree - structured long short - term memory ( Tree - LSTM ) architecture , which is one of the most renowned variants of RvNN .","Our Gumbel Tree - LSTM model
based on
tree - structured long short - term memory ( Tree - LSTM ) architecture","Our Gumbel Tree - LSTM model||based on||tree - structured long short - term memory ( Tree - LSTM ) architecture
",,,"Model||has||Our Gumbel Tree - LSTM model
",,,,,
model,"To learn how to compose task - specific tree structures without depending on structured input , our model introduces composition query vector that measures validity of a composition .","our model
introduces
composition query vector
measures
validity
of
composition","our model||introduces||composition query vector
composition query vector||measures||validity
validity||of||composition
",,,"Model||has||our model
",,,,,
model,"Using validity scores computed by the composition query vector , our model recursively selects compositions until only a single representation remains .","Using
validity scores
computed by
composition query vector
our model
recursively selects
compositions
until
only a single representation remains","validity scores||computed by||composition query vector
our model||recursively selects||compositions
compositions||until||only a single representation remains
","validity scores||has||our model
","Model||Using||validity scores
",,,,,,
model,We use Straight - Through ( ST ) Gumbel - Softmax estimator to sample compositions in the training phase .,"use
Straight - Through ( ST ) Gumbel - Softmax estimator
to sample
compositions
in
training phase","Straight - Through ( ST ) Gumbel - Softmax estimator||to sample||compositions
compositions||in||training phase
",,"Model||use||Straight - Through ( ST ) Gumbel - Softmax estimator
",,,,,,
model,"ST Gumbel - Softmax estimator relaxes the discrete sampling operation to be continuous in the backward pass , thus our model can be trained via the standard backpropagation .","relaxes
discrete sampling operation
to be
continuous
in
backward pass","discrete sampling operation||to be||continuous
continuous||in||backward pass
",,,,,"Straight - Through ( ST ) Gumbel - Softmax estimator||relaxes||discrete sampling operation
",,,
experiments,Natural Language Inference,Natural Language Inference,,,,,,,,"Tasks||has||Natural Language Inference
","Natural Language Inference||has||Experimental setup
"
experiments,"Similar to 100D experiments , we initialize the word embedding matrix with GloVe 300D pretrained vectors 4 , however we do not update the word representations during training .","initialize
word embedding matrix
with
GloVe 300D pretrained vectors","word embedding matrix||with||GloVe 300D pretrained vectors
",,,,,"Experimental setup||initialize||word embedding matrix
",,,
experiments,The dropout probability is set to 0.2 and word embeddings are not updated during training .,"dropout probability
set to
0.2","dropout probability||set to||0.2
",,,,,,,"Experimental setup||has||dropout probability
",
experiments,"The size of mini-batches is set to 128 in all experiments , and hyperparameters are tuned using the validation split .","size
of
mini-batches
set to
128","size||of||mini-batches
mini-batches||set to||128
size||of||mini-batches
",,,,,,,"Experimental setup||has||size
",
experiments,"The temperature parameter ? of Gumbel - Softmax is set to 1.0 , and we did not find that temperature annealing improves performance .","temperature parameter
of
Gumbel - Softmax
set to
1.0","temperature parameter||of||Gumbel - Softmax
Gumbel - Softmax||set to||1.0
",,,,,,,"Experimental setup||has||temperature parameter
",
experiments,"For training models , Adam optimizer is used .","For training
models
Adam optimizer
used","models||used||Adam optimizer
",,,,,"Experimental setup||For training||models
",,,
experiments,"First , we can see that LSTM - based leaf transformation has a clear advantage over the affine - transformation - based one .","see that
LSTM - based leaf transformation
clear advantage
over
affine - transformation - based one","clear advantage||over||affine - transformation - based one
","LSTM - based leaf transformation||has||clear advantage
",,,,"Results||see that||LSTM - based leaf transformation
",,,
experiments,"Secondly , comparing ours with other models , we find that our 100D and 300D model outperform all other models of similar numbers of parameters .","find that
our 100D and 300D model
outperform
all other models
of
similar numbers of parameters","our 100D and 300D model||outperform||all other models
all other models||of||similar numbers of parameters
",,,,,"Results||find that||our 100D and 300D model
",,,
experiments,"Our 600D model achieves the accuracy of 86.0 % , which is comparable to that of the state - of - the - art model , while using far less parameters .","Our 600D model
achieves
accuracy
of
86.0 %
comparable to
state - of - the - art model","Our 600D model||achieves||accuracy
accuracy||of||86.0 %
86.0 %||comparable to||state - of - the - art model
",,,,,,,"Results||has||Our 600D model
",
experiments,All of our models converged within a few hours on a machine with NVIDIA Titan Xp GPU .,"on
machine
with
NVIDIA Titan Xp GPU","machine||with||NVIDIA Titan Xp GPU
",,,,,"Experimental setup||on||machine
",,,
experiments,Sentiment Analysis,Sentiment Analysis,,,,,,,,"Tasks||has||Sentiment Analysis
","Sentiment Analysis||has||Hyperparameters
"
experiments,is a single - hidden layer MLP with the ReLU activation function .,"is
single - hidden layer MLP
with
ReLU activation function","single - hidden layer MLP||with||ReLU activation function
",,,,,"Hyperparameters||is||single - hidden layer MLP
",,,
experiments,"We trained our SST - 2 model with hyperparameters D x = 300 , D h = 300 , D c = 300 .","trained
SST - 2 model
with hyperparameters
D x = 300 , D h = 300 , D c = 300","SST - 2 model||with hyperparameters||D x = 300 , D h = 300 , D c = 300
",,,,,"Hyperparameters||trained||SST - 2 model
",,,
experiments,The word vectors are initialized with GloVe 300D pretrained vectors and fine - tuned during training .,"word vectors
initialized
with
GloVe 300D pretrained vectors
fine - tuned
during
training","fine - tuned||during||training
initialized||with||GloVe 300D pretrained vectors
","word vectors||has||fine - tuned
word vectors||has||initialized
",,,,,,"SST - 2 model||has||word vectors
",
experiments,We apply dropout ( p = 0.5 ) on the output of the word embedding layer and the input and the output of the MLP layer .,"apply
dropout ( p = 0.5 )
on
output
of
word embedding layer
input and the output
of
MLP layer","dropout ( p = 0.5 )||on||output
output||of||word embedding layer
dropout ( p = 0.5 )||on||input and the output
input and the output||of||MLP layer
",,,,,"SST - 2 model||apply||dropout ( p = 0.5 )
",,,
experiments,The size of mini-batches is set to 32 and Adadelta optimizer is used for optimization .,"size
of
mini-batches
set to
32
Adadelta optimizer
used for
optimization","Adadelta optimizer||used for||optimization
mini-batches||set to||32
",,,,,,,"SST - 2 model||has||Adadelta optimizer
SST - 2 model||has||size
",
experiments,"For our SST - 5 model , hyperparameters are set to D x = 300 , D h = 300 , D c = 1024 . Similar to the SST - 2 model , we optimize the model using Adadelta optimizer with batch size 64 and apply dropout with p = 0.5 .","For
SST - 5 model
hyperparameters
set to
D x = 300 , D h = 300 , D c = 1024
model
optimize
using
Adadelta optimizer
with
batch size 64
apply
dropout
with
p = 0.5","SST - 5 model||optimize||model
model||using||Adadelta optimizer
Adadelta optimizer||with||batch size 64
Adadelta optimizer||apply||dropout
dropout||with||p = 0.5
hyperparameters||set to||D x = 300 , D h = 300 , D c = 1024
","SST - 5 model||has||hyperparameters
",,,,"Hyperparameters||For||SST - 5 model
",,,
experiments,"Our SST - 2 model outperforms all other models substantially except byte - m LSTM , where a byte - level language model trained on the large product review dataset is used to obtain sentence representations .","SST - 2 model
outperforms
all other models
substantially
except
byte - m LSTM","SST - 2 model||outperforms||all other models
all other models||except||byte - m LSTM
","all other models||has||substantially
",,,,,,"Results||has||SST - 2 model
",
experiments,"We also see that the performance of our SST - 5 model is on par with that of the current state - of - the - art model , which is pretrained on large parallel datasets and uses character n-gram embeddings alongside word embeddings , even though our model does not utilize external resources other than GloVe vectors and only uses wordlevel representations .","see that
performance
of
our SST - 5 model
on par with
current state - of - the - art model","performance||of||our SST - 5 model
our SST - 5 model||on par with||current state - of - the - art model
",,,,,"Results||see that||performance
",,,
experiments,The authors of stated that utilizing pretraining and character n-gram embeddings improves validation accuracy by 2.8 % ( SST - 2 ) or 1.7 % ( SST - 5 ) .,"utilizing
pretraining and character n-gram embeddings
improves
validation accuracy
by
2.8 % ( SST - 2 )
1.7 % ( SST - 5 )","pretraining and character n-gram embeddings||improves||validation accuracy
validation accuracy||by||2.8 % ( SST - 2 )
validation accuracy||by||1.7 % ( SST - 5 )
",,,,,"Results||utilizing||pretraining and character n-gram embeddings
",,,
research-problem,"As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers .",question answering,,,,,"Contribution||has research problem||question answering
",,,,
research-problem,"Question answering ( QA ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search .",Question answering ( QA ),,,,,"Contribution||has research problem||Question answering ( QA )
",,,,
research-problem,"Many of current QA systems use a learning to rank approach that encodes question / answer pairs with complex linguistic features including lexical , syntactic and semantic features .",QA,,,,,"Contribution||has research problem||QA
",,,,
model,"To handle these issues in the existing deep learning architectures for ranking answers , we propose an attention based neural matching model ( a NMM ) .","propose
attention based neural matching model ( a NMM )",,,"Model||propose||attention based neural matching model ( a NMM )
",,,,,,
model,Deep neural network with value - shared weights :,Deep neural network with value - shared weights,,,,"Model||has||Deep neural network with value - shared weights
",,,,,
model,"We introduce a novel value - shared weighting scheme in deep neural networks as a counterpart of the position - shared weighting scheme in CNNs , based on the idea that semantic matching between a question and answer is mainly about the ( semantic similarity ) value regularities rather than spatial regularities .","introduce
novel value - shared weighting scheme
in
deep neural networks","novel value - shared weighting scheme||in||deep neural networks
",,,,,"Deep neural network with value - shared weights||introduce||novel value - shared weighting scheme
",,,
model,Incorporate attention scheme over question terms :,Incorporate attention scheme over question terms,,,,"Model||has||Incorporate attention scheme over question terms
",,,,,
model,"We incorporate the attention scheme over the question terms using a gating function , so that we can explicitly discriminate the question term importance .","incorporate
attention scheme
over
question terms
using
gating function","attention scheme||over||question terms
question terms||using||gating function
",,,,,"Incorporate attention scheme over question terms||incorporate||attention scheme
",,,
hyperparameters,"For the setting of hyper - parameters , we set the number of bins as 600 , word embedding dimension as 700 for a NNM - 1 , the number of bins as 200 , word embedding dimension as 700 for a NNM - 2 after we tune hyper - parameters on the provided DEV set of TREC QA data .","set
number of bins
as
600
word embedding dimension
as
700
NNM - 1
number of bins
as
200
word embedding dimension
as
700
NNM - 2","word embedding dimension||as||700
number of bins||as||200
word embedding dimension||as||700
number of bins||as||600
","NNM - 2||has||word embedding dimension
NNM - 2||has||number of bins
NNM - 1||has||word embedding dimension
NNM - 1||has||number of bins
","Hyperparameters||set||NNM - 2
Hyperparameters||set||NNM - 1
",,,,,,
results,We can see a NMM trained with TRAIN - ALL set beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models .,"see
NMM
trained with
TRAIN - ALL set
beats
all the previous state - of - the art systems
including
both methods
using
feature engineering and deep learning models","NMM||beats||all the previous state - of - the art systems
all the previous state - of - the art systems||including||both methods
both methods||using||feature engineering and deep learning models
NMM||trained with||TRAIN - ALL set
",,"Results||see||NMM
",,,,,,
results,"Furthermore , even without combining additional features , a NMM still performs well for answer ranking , showing significant improvements over previous deep learning model with no additional features and linguistic feature engineering methods .","without combining
additional features
NMM
performs
well
for
answer ranking
showing
significant improvements
over
previous deep learning model
with no
additional features
linguistic feature engineering methods","NMM||performs||well
well||for||answer ranking
well||showing||significant improvements
significant improvements||over||previous deep learning model
previous deep learning model||with no||additional features
previous deep learning model||with no||linguistic feature engineering methods
","additional features||has||NMM
","Results||without combining||additional features
",,,,,,
research-problem,Dynamic Integration of Background Knowledge in Neural NLU Systems,Neural NLU,,,,,"Contribution||has research problem||Neural NLU
",,,,
research-problem,"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time .",neural natural language understanding ( NLU ),,,,,"Contribution||has research problem||neural natural language understanding ( NLU )
",,,,
model,"In this paper , we develop a new architecture for dynamically incorporating external background knowledge in NLU models .","develop
new architecture
for dynamically incorporating
external background knowledge
in
NLU models","new architecture||for dynamically incorporating||external background knowledge
external background knowledge||in||NLU models
",,"Model||develop||new architecture
",,,,,,
model,"Rather than relying only on static knowledge implicitly present in the training data , supplementary knowledge is retrieved from external knowledge sources ( in this paper , ConceptNet and Wikipedia ) to assist with understanding text inputs .","supplementary knowledge
retrieved from
external knowledge sources
ConceptNet
Wikipedia
assist with understanding
text inputs","supplementary knowledge||retrieved from||external knowledge sources
external knowledge sources||assist with understanding||text inputs
","external knowledge sources||name||ConceptNet
external knowledge sources||name||Wikipedia
",,"Model||has||supplementary knowledge
",,,,,
model,The retrieved supplementary texts are read together with the task inputs by an initial reading module whose outputs are contextually refined word embeddings ( 3 ) .,"retrieved supplementary texts
read together with
task inputs
by
initial reading module
whose
outputs
contextually refined word embeddings",,,,,,,,,
model,These refined embeddings are then used as input to a task - specific NLU architecture ( any architecture that reads text as a sequence of word embeddings can be used here ) .,"are
used as
input
to
task - specific NLU architecture","input||to||task - specific NLU architecture
",,,,,"contextually refined word embeddings||used as||input
",,,"outputs||are||contextually refined word embeddings
"
model,"The initial reading module and the task module are learnt jointly , end - to - end .","initial reading module and the task module
are learnt
jointly , end - to - end","initial reading module and the task module||are learnt||jointly , end - to - end
",,,"Model||has||initial reading module and the task module
",,,,,
hyperparameters,All models are trained end - to - end jointly with the refinement module using a dimensionality of n = 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints .,"All models
trained
end - to - end
jointly
with
refinement module
using a dimensionality
of
n = 300
for
all but the TriviaQA experiments
had to reduce
n to 150
due to
memory constraints","All models||trained||end - to - end
All models||trained||jointly
jointly||with||refinement module
All models||trained||using a dimensionality
using a dimensionality||of||n = 300
n = 300||for||all but the TriviaQA experiments
all but the TriviaQA experiments||had to reduce||n to 150
n to 150||due to||memory constraints
",,,"Hyperparameters||has||All models
",,,,,
hyperparameters,All baselines operate on the unrefined word embeddings E 0 described in 3.1 .,"All baselines
operate on
unrefined word embeddings","All baselines||operate on||unrefined word embeddings
",,,"Hyperparameters||has||All baselines
",,,,,
hyperparameters,For the DQA baseline system we add the lemma in - question feature ( liq ) suggested in .,"For
DQA baseline system
add
lemma
in
question feature ( liq )","DQA baseline system||add||lemma
lemma||in||question feature ( liq )
",,"Hyperparameters||For||DQA baseline system
",,,,,,
results,"Wikipedia ( W ) yields further , significant improvements on TriviaQA , slightly outperforming the current state of the art model .","Wikipedia ( W )
yields
further , significant improvements
on
TriviaQA
slightly outperforming
current state of the art model","Wikipedia ( W )||yields||further , significant improvements
further , significant improvements||on||TriviaQA
","further , significant improvements||has||slightly outperforming
slightly outperforming||has||current state of the art model
",,"Results||has||Wikipedia ( W )
",,,,,
results,shows the results of our RTE experiments .,RTE experiments,,,,"Results||has||RTE experiments
",,,,,
results,"In general , the introduction of our refinement strategy almost always helps , both with and without external knowledge .","introduction of
our refinement strategy
almost always helps
with and without
external knowledge","almost always helps||with and without||external knowledge
","our refinement strategy||has||almost always helps
",,,,"RTE experiments||introduction of||our refinement strategy
",,,
results,"When providing additional background knowledge from ConceptNet , our BiLSTM based models improve substantially , while the ESIM - based models improve only on the more difficult MultiNLI dataset .","When providing
additional background knowledge
from
ConceptNet
our BiLSTM based models
improve substantially
ESIM - based models
improve
only on
more difficult MultiNLI dataset","additional background knowledge||from||ConceptNet
improve||only on||more difficult MultiNLI dataset
","additional background knowledge||has||ESIM - based models
ESIM - based models||has||improve
additional background knowledge||has||our BiLSTM based models
our BiLSTM based models||has||improve substantially
",,,,"RTE experiments||When providing||additional background knowledge
",,,
results,"Compared to previously published state of the art systems , our models acquit themselves quite well on the MultiNLI benchmark , and competitively on the SNLI benchmark .","our models
acquit
quite well
on
MultiNLI benchmark
competitively
on
SNLI benchmark","our models||acquit||competitively
competitively||on||SNLI benchmark
our models||acquit||quite well
quite well||on||MultiNLI benchmark
",,,,,,,"RTE experiments||has||our models
",
results,"We do find that there is little impact of using external knowledge on the RTE task with ESIM , although the refinement strategy helps using just p + q.","find that
little impact
of using
external knowledge
on
RTE task
with
ESIM","little impact||of using||external knowledge
external knowledge||on||RTE task
RTE task||with||ESIM
",,,,,"RTE experiments||find that||little impact
",,,
results,"Nevertheless , both ESIM and our BiL - STM models when trained with knowledge from ConceptNet are sensitive to the semantics of the provided assertions as demonstrated in our analysis in 5.3 .","both ESIM and our BiL - STM models
trained with
knowledge
from
ConceptNet
sensitive to
semantics
of
provided assertions","both ESIM and our BiL - STM models||trained with||knowledge
knowledge||sensitive to||semantics
semantics||of||provided assertions
knowledge||from||ConceptNet
",,,,,,,"RTE experiments||has||both ESIM and our BiL - STM models
",
results,"Furthermore , increasing the coverage of assertions in ConceptNet would most likely yield improved performance even without retraining our models .","increasing
coverage
of
assertions
in
ConceptNet
most likely yield
improved performance
without retraining
our models","coverage||of||assertions
assertions||in||ConceptNet
assertions||most likely yield||improved performance
improved performance||without retraining||our models
",,,,,"RTE experiments||increasing||coverage
",,,
research-problem,Shortcut - Stacked Sentence Encoders for Multi- Domain Inference,Multi- Domain Inference,,,,,"Contribution||has research problem||Multi- Domain Inference
",,,,
research-problem,We present a simple sequential sentence encoder for multi-domain natural language inference .,multi-domain natural language inference,,,,,"Contribution||has research problem||multi-domain natural language inference
",,,,
research-problem,Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .,"Natural language inference ( NLI )
recognizing textual entailment ( RTE )",,,,,"Contribution||has research problem||Natural language inference ( NLI )
Contribution||has research problem||recognizing textual entailment ( RTE )
",,,,
model,"In this paper , we follow the former approach of encoding - based models , and propose a novel yet simple sequential sentence encoder for the Multi - NLI problem .","follow
former approach
of
encoding - based models
propose
novel yet simple sequential sentence encoder
for
Multi - NLI problem","former approach||of||encoding - based models
novel yet simple sequential sentence encoder||for||Multi - NLI problem
",,"Model||follow||former approach
Model||propose||novel yet simple sequential sentence encoder
",,,,,,
model,It is basically a stacked ( multi-layered ) bidirectional LSTM - RNN with shortcut connections ( feeding all previous layers ' outputs and word embeddings to each layer ) and word embedding fine - tuning .,"stacked ( multi-layered ) bidirectional LSTM - RNN
with
shortcut connections
word embedding fine - tuning","stacked ( multi-layered ) bidirectional LSTM - RNN||with||shortcut connections
stacked ( multi-layered ) bidirectional LSTM - RNN||with||word embedding fine - tuning
",,,"Model||has||stacked ( multi-layered ) bidirectional LSTM - RNN
",,,,,
model,"The over all supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors , and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural ( similar to the classifier setup of and ) .","over all supervised model
uses
shortcutstacked encoders
to encode
two input sentences
into
two vectors
use
classifier
over
vector combination
to label
relationship
between
two sentences
as
entailment
contradiction
neural","over all supervised model||use||classifier
classifier||over||vector combination
vector combination||to label||relationship
relationship||as||entailment
relationship||as||contradiction
relationship||as||neural
relationship||between||two sentences
over all supervised model||uses||shortcutstacked encoders
shortcutstacked encoders||to encode||two input sentences
two input sentences||into||two vectors
",,,"Model||has||over all supervised model
",,,,,
code,Github Code Link : https://github.com/ easonnie/multiNLI_encoder,https://github.com/ easonnie/multiNLI_encoder,,,,,"Contribution||Code||https://github.com/ easonnie/multiNLI_encoder
",,,,
hyperparameters,We use cross - entropy loss as the training objective with Adam - based opti-Model Accuracy SNLI Multi - NLI Matched Multi - NLI Mismatched CBOW 80.6 65.2 64.6 biLSTM Encoder 81.5 67.5 67.1 300D Tree - CNN Encoder 82.1 --300D SPINN - PI Encoder 83.2 --300D NSE Encoder 84.6 --biLSTM -Max Encoder 84 . mization with 32 batch size .,"use
cross - entropy loss
as
training objective
with
Adam
with
32 batch size","cross - entropy loss||as||training objective
training objective||with||Adam
Adam||with||32 batch size
",,"Hyperparameters||use||cross - entropy loss
",,,,,,
hyperparameters,The starting learning rate is 0.0002 with half decay every two epochs .,"starting learning rate
is
0.0002
with
half decay
every
two epochs","starting learning rate||is||0.0002
0.0002||with||half decay
half decay||every||two epochs
",,,"Hyperparameters||has||starting learning rate
",,,,,
hyperparameters,The number of hidden units for MLP in classifier is 1600 .,"number of hidden units
for
MLP
in
classifier
is
1600","number of hidden units||in||classifier
number of hidden units||for||MLP
number of hidden units||is||1600
",,,"Hyperparameters||has||number of hidden units
",,,,,
hyperparameters,"Dropout layer is also applied on the output of each layer of MLP , with dropout rate set to 0.1 .","Dropout layer
applied on
output
of
each layer of MLP
with
dropout rate
set to
0.1","Dropout layer||applied on||output
output||with||dropout rate
dropout rate||set to||0.1
output||of||each layer of MLP
",,,"Hyperparameters||has||Dropout layer
",,,,,
hyperparameters,We used pre-trained 300D Glove 840B vectors to initialize the word embeddings .,"used
pre-trained 300D Glove 840B vectors
to initialize
word embeddings","pre-trained 300D Glove 840B vectors||to initialize||word embeddings
",,"Hyperparameters||used||pre-trained 300D Glove 840B vectors
",,,,,,
ablation-analysis,"These ablation results are shown in and 4 , all based on the Multi - NLI development sets .","based on
Multi - NLI development sets",,,"Ablation analysis||based on||Multi - NLI development sets
",,,,,,
ablation-analysis,"As shown , each added layer model improves the accuracy and we achieve a substantial improvement in accuracy ( around 2 % ) on both matched and mismatched settings , compared to the single - layer biLSTM in .","each added layer model
improves
accuracy
achieve
substantial improvement
in
accuracy ( around 2 % )
on
matched and mismatched settings
compared to
single - layer biLSTM","each added layer model||achieve||substantial improvement
substantial improvement||in||accuracy ( around 2 % )
substantial improvement||compared to||single - layer biLSTM
substantial improvement||on||matched and mismatched settings
each added layer model||improves||accuracy
",,,"Ablation analysis||has||each added layer model
",,,,,
ablation-analysis,"Next , in , we show that the shortcut connections among the biLSTM layers is also an important contributor to accuracy improvement ( around 1.5 % on top of the full 3 - layered stacked - RNN model ) .","show
shortcut connections
among
biLSTM layers
is
important contributor
to
accuracy improvement
around
1.5 %
on top of
full 3 - layered stacked - RNN model","shortcut connections||among||biLSTM layers
shortcut connections||is||important contributor
important contributor||to||accuracy improvement
accuracy improvement||around||1.5 %
1.5 %||on top of||full 3 - layered stacked - RNN model
",,"Ablation analysis||show||shortcut connections
",,,,,,
ablation-analysis,"Next , in , we show that fine - tuning the word embeddings also improves results , again for both the in - domain task and cross - domain tasks ( the ablation results are based on a smaller model with a 128 +256 2 - layer biLSTM ) .","show that
fine - tuning
word embeddings
improves
results
for both
in - domain task and cross - domain tasks","fine - tuning||improves||results
results||for both||in - domain task and cross - domain tasks
","fine - tuning||has||word embeddings
","Ablation analysis||show that||fine - tuning
",,,,,,
ablation-analysis,The last ablation in shows that a classifier with two layers of relu is preferable than other options .,"last ablation
shows that
classifier
with
two layers of relu
is
preferable","last ablation||shows that||classifier
classifier||with||two layers of relu
two layers of relu||is||preferable
",,,"Ablation analysis||has||last ablation
",,,,,
results,"First for Multi - NLI , we improve substantially over the CBOW and biL - STM Encoder baselines reported in the dataset paper .","for
Multi - NLI
improve
substantially
over
CBOW and biL - STM Encoder baselines","Multi - NLI||improve||substantially
substantially||over||CBOW and biL - STM Encoder baselines
",,"Results||for||Multi - NLI
",,,,,,
results,We also show that our final shortcut - based stacked encoder achieves around 3 % improvement as compared to the 1 layer biLSTM - Max Encoder in the second last row ( using the exact same classifier and optimizer settings ) .,"show that
our final shortcut - based stacked encoder
achieves
around 3 % improvement
compared to
1 layer biLSTM - Max Encoder","our final shortcut - based stacked encoder||achieves||around 3 % improvement
around 3 % improvement||compared to||1 layer biLSTM - Max Encoder
",,,,,"Multi - NLI||show that||our final shortcut - based stacked encoder
",,,
results,Our shortcut - encoder was also the top singe - model ( non-ensemble ) result on the EMNLP RepEval Shared Task leaderboard .,"shortcut - encoder
was
top singe - model ( non-ensemble ) result
on
EMNLP RepEval Shared Task leaderboard","shortcut - encoder||was||top singe - model ( non-ensemble ) result
top singe - model ( non-ensemble ) result||on||EMNLP RepEval Shared Task leaderboard
",,,,,,,"Multi - NLI||has||shortcut - encoder
",
results,"Next , for SNLI , we compare our shortcutstacked encoder with the current state - of - the - art encoders from the SNLI leaderboard ( https :// nlp.stanford.edu/projects/snli/ ) .","SNLI
compare
shortcutstacked encoder
with
current state - of - the - art encoders
from
SNLI leaderboard","SNLI||compare||shortcutstacked encoder
shortcutstacked encoder||with||current state - of - the - art encoders
current state - of - the - art encoders||from||SNLI leaderboard
",,,"Results||for||SNLI
",,,,,
results,"We also compare to the recent biLSTM - Max Encoder of , which served as our model 's 1 - layer starting point .","compare to
recent biLSTM - Max Encoder",,,,,,"SNLI||compare to||recent biLSTM - Max Encoder
",,,"recent biLSTM - Max Encoder||has||results
"
results,"The results indicate that ' Our Shortcut - Stacked Encoder ' sur-passes all the previous state - of - the - art encoders , and achieves the new best encoding - based result on SNLI , suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders .","results
indicate
Our Shortcut - Stacked Encoder
sur-passes
all the previous state - of - the - art encoders
achieves
new best encoding - based result
on
SNLI","results||indicate||Our Shortcut - Stacked Encoder
Our Shortcut - Stacked Encoder||sur-passes||all the previous state - of - the - art encoders
Our Shortcut - Stacked Encoder||achieves||new best encoding - based result
new best encoding - based result||on||SNLI
",,,,,,,,
research-problem,Multi-range Reasoning for Machine Comprehension,Machine Comprehension,,,,,"Contribution||has research problem||Machine Comprehension
",,,,
research-problem,"We propose MRU ( Multi - Range Reasoning Units ) , a new fast compositional encoder for machine comprehension ( MC ) .",machine comprehension ( MC ),,,,,"Contribution||has research problem||machine comprehension ( MC )
",,,,
research-problem,"While the usage of recurrent encoder is often regarded as indispensable in highly complex MC tasks , there are still several challenges and problems pertaining to it 's usage in modern MC tasks .",MC,,,,,"Contribution||has research problem||MC
",,,,
model,"To this end , we propose a new compositional encoder that can either be used in - place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .","propose
new compositional encoder
used in - place of
standard RNN encoders
serve as
new module
complementary to
existing neural architectures","new compositional encoder||serve as||new module
new module||complementary to||existing neural architectures
new compositional encoder||used in - place of||standard RNN encoders
",,"Model||propose||new compositional encoder
",,,,,,
model,Our proposed MRU encoders learns gating vectors via multiple contract - and - expand layers at multiple dilated resolutions .,"Our proposed MRU encoders
learns
gating vectors
via
multiple contract - and - expand layers
at
multiple dilated resolutions","Our proposed MRU encoders||learns||gating vectors
gating vectors||via||multiple contract - and - expand layers
multiple contract - and - expand layers||at||multiple dilated resolutions
",,,"Model||has||Our proposed MRU encoders
",,,,,
model,"Specifically , we compress the input document an arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 ) into a neural bag - of - words ( summed ) representation .","compress
input document
arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 )
into
neural bag - of - words ( summed ) representation","input document||into||neural bag - of - words ( summed ) representation
","input document||has||arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 )
","Model||compress||input document
",,,,,,
model,The compact sequence is then passed through affine transformation layers and then re-expanded to the original sequence length .,"passed through
affine transformation layers
re-expanded to
original sequence length",,,,,,"neural bag - of - words ( summed ) representation||re-expanded to||original sequence length
neural bag - of - words ( summed ) representation||passed through||affine transformation layers
",,,
model,The k document representations ( at multiple ranges and n-gram blocks ) are then combined and modeled with fully connected layers to form the final compositional gate which are applied onto the original input document .,"k document representations
at
multiple ranges and n-gram blocks
combined and modeled with
fully connected layers
to form
final compositional gate
applied onto
original input document","k document representations||combined and modeled with||fully connected layers
fully connected layers||to form||final compositional gate
final compositional gate||applied onto||original input document
k document representations||at||multiple ranges and n-gram blocks
",,,"Model||has||k document representations
",,,,,
baselines,"RACE - the key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .","RACE
key competitors
are
Stanford Attention Reader ( Stanford AR )
Gated Attention Reader ( GA )
Dynamic Fusion Networks ( DFN )","key competitors||are||Stanford Attention Reader ( Stanford AR )
key competitors||are||Gated Attention Reader ( GA )
key competitors||are||Dynamic Fusion Networks ( DFN )
","RACE||has||key competitors
",,"Baselines||has||RACE
",,,,,
baselines,Search QA - the main competitor baseline is the AMANDA model proposed by .,"Search QA
main competitor baseline
is
AMANDA model","main competitor baseline||is||AMANDA model
","Search QA||has||main competitor baseline
",,"Baselines||has||Search QA
",,,,,
baselines,NarrativeQA,NarrativeQA,,,,"Baselines||has||NarrativeQA
",,,,,"NarrativeQA||has||baselines
"
baselines,"We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .","baselines
are
context - less sequence to sequence ( seq2seq ) model
ASR
BiDAF","baselines||are||context - less sequence to sequence ( seq2seq ) model
baselines||are||ASR
baselines||are||BiDAF
",,,,,,,,
experimental-setup,We implement all models in TensorFlow .,"implement
all models
in
TensorFlow","all models||in||TensorFlow
",,,,,"Experimental Setup||implement||all models
",,,
experimental-setup,Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,"Word embeddings
initialized with
300d Glo Ve vectors
not fine - tuned during
training","Word embeddings||initialized with||300d Glo Ve vectors
Word embeddings||not fine - tuned during||training
",,,,,,,"Experimental Setup||has||Word embeddings
",
experimental-setup,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .","Dropout rate
tuned amongst
{ 0.1 , 0.2 , 0.3 }
on
all layers
including
embedding layer","Dropout rate||tuned amongst||{ 0.1 , 0.2 , 0.3 }
{ 0.1 , 0.2 , 0.3 }||on||all layers
all layers||including||embedding layer
",,,,,,,"Experimental Setup||has||Dropout rate
",
experimental-setup,We adopt the Adam optimizer with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / NarrativeQA respectively .,"adopt
Adam optimizer
with
learning rate
of
0.0003/ 0.001/0.001
for
RACE / SearchQA / NarrativeQA","Adam optimizer||with||learning rate
learning rate||of||0.0003/ 0.001/0.001
0.0003/ 0.001/0.001||for||RACE / SearchQA / NarrativeQA
",,,,,"Experimental Setup||adopt||Adam optimizer
",,,
experimental-setup,The batch size is set to 64/256/32 accordingly .,"batch size
set to
64/256/32","batch size||set to||64/256/32
",,,,,,,"Experimental Setup||has||batch size
",
experimental-setup,The maximum sequence lengths are 500/200/1100 respectively .,"maximum sequence lengths
are
500/200/1100","maximum sequence lengths||are||500/200/1100
",,,,,,,"Experimental Setup||has||maximum sequence lengths
",
experimental-setup,All models are trained and all runtime benchmarks are based on a TitanXP GPU .,"All models
trained
all runtime benchmarks
based on
TitanXP GPU","TitanXP GPU||trained||All models
","TitanXP GPU||has||all runtime benchmarks
",,,,"Experimental Setup||based on||TitanXP GPU
",,,
results,"Overall , there is a 6 % improvement on the RACE - H dataset and 1.8 % improvement on the RACE - M dataset .","6 % improvement
on
RACE - H dataset
1.8 % improvement
on
RACE - M dataset","1.8 % improvement||on||RACE - M dataset
",,,,,,,"RACE||has||1.8 % improvement
",
results,Experimental Results on RACE,"on
RACE",,,"Results||on||RACE
",,,,,,"RACE||has||6 % improvement
6 % improvement||on||RACE - H dataset
"
results,"In general , we also found that the usage of a recurrent cell is not really crucial on this dataset since ( 1 ) Sim . MRU and MRU can achieve comparable performance to each other , ( 2 ) GRU and LSTM models do not have a competitive edge and ( 3 ) Using no encoder already achieves comparable 1 performance to DFN .","Sim . MRU and MRU
achieve
comparable performance
to
each other
GRU and LSTM models
do not have
competitive edge
no encoder
achieves
comparable 1 performance
to
DFN","Sim . MRU and MRU||achieve||comparable performance
comparable performance||to||each other
GRU and LSTM models||do not have||competitive edge
no encoder||achieves||comparable 1 performance
comparable 1 performance||to||DFN
",,,,,,,"RACE||has||Sim . MRU and MRU
RACE||has||GRU and LSTM models
RACE||has||no encoder
",
results,"Finally , an ensemble of Sim . MRU models achieve state - of - the - art performance on the RACE dataset , achieving and over all score of 53.3 % . :","ensemble of Sim . MRU models
achieve
state - of - the - art performance
on
RACE dataset
achieving
over all score
of
53.3 %","ensemble of Sim . MRU models||achieve||state - of - the - art performance
state - of - the - art performance||achieving||over all score
over all score||of||53.3 %
state - of - the - art performance||on||RACE dataset
",,,,,,,"RACE||has||ensemble of Sim . MRU models
",
results,are baselines reported by . reports our results on the Narrative QA benchmark .,Narrative QA benchmark,,,,"Results||on||Narrative QA benchmark
",,,,,
results,"First , we observe that 300d MRU can achieve comparable performance with BiDAF .","observe that
300d MRU
can achieve
comparable performance
with
BiDAF","300d MRU||can achieve||comparable performance
comparable performance||with||BiDAF
",,,,,"Narrative QA benchmark||observe that||300d MRU
",,,
results,"When compared with a BiLSTM of equal output dimensions ( 150 d ) , we find that our MRU model performs competitively , with less than 1 % deprovement across all metrics .","compared with
BiLSTM
of
equal output dimensions ( 150 d )
find that
MRU model
performs
competitively
with
less than 1 % deprovement
across
all metrics","BiLSTM||find that||MRU model
MRU model||performs||competitively
competitively||with||less than 1 % deprovement
less than 1 % deprovement||across||all metrics
BiLSTM||of||equal output dimensions ( 150 d )
",,,,,"Narrative QA benchmark||compared with||BiLSTM
",,,
results,The performance of our model is significantly better than 300d LSTM model while also being significantly faster .,"performance of
our model
is
significantly better
than
300d LSTM model
also being
significantly faster","our model||is||significantly better
significantly better||than||300d LSTM model
our model||also being||significantly faster
",,,,,"Narrative QA benchmark||performance of||our model
",,,
results,"Finally , the MRU - LSTM significantly outperforms all models , including BiDAF on this dataset .","MRU - LSTM
significantly outperforms
all models
including
BiDAF","all models||including||BiDAF
","MRU - LSTM||has||significantly outperforms
significantly outperforms||has||all models
",,,,,,"Narrative QA benchmark||has||MRU - LSTM
",
results,"Performance improvement over the vanilla BiLSTM model ranges from 1 % ? 3 % across all metrics , suggesting that MRU encoders are also effective as a complementary neural building block .","Performance improvement
over
vanilla BiLSTM model
ranges from
1 % ? 3 %
across
all metrics","Performance improvement||over||vanilla BiLSTM model
vanilla BiLSTM model||ranges from||1 % ? 3 %
1 % ? 3 %||across||all metrics
",,,,,,,"Narrative QA benchmark||has||Performance improvement
",
research-problem,CODAH : An Adversarially - Authored Question Answering Dataset for Common Sense,Question Answering,,,,,"Contribution||has research problem||Question Answering
",,,,
research-problem,"Commonsense reasoning is a critical AI capability , but it is difficult to construct challenging datasets that test commonsense .",Commonsense reasoning,,,,,"Contribution||has research problem||Commonsense reasoning
",,,,
research-problem,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,commonsense reasoning over text,,,,,"Contribution||has research problem||commonsense reasoning over text
",,,,
research-problem,"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion .","introduce
COmmonsense Dataset Adversarially - authored by Humans ( CODAH )
for
commonsense question answering
in the style of
SWAG multiple choice sentence completion","COmmonsense Dataset Adversarially - authored by Humans ( CODAH )||for||commonsense question answering
commonsense question answering||in the style of||SWAG multiple choice sentence completion
",,"Dataset||introduce||COmmonsense Dataset Adversarially - authored by Humans ( CODAH )
",,"Contribution||has research problem||commonsense question answering
",,,,
model,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .","propose
novel method
for
question generation
in which
human annotators
educated on
workings
of
state - of - the - art question answering model
asked to
submit
questions
adversarially target
weaknesses","novel method||in which||human annotators
human annotators||educated on||workings
workings||of||state - of - the - art question answering model
human annotators||asked to||submit
questions||adversarially target||weaknesses
novel method||for||question generation
","submit||has||questions
","Model||propose||novel method
",,,,,,
model,"Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine - tuning on a sample of the submitted questions , encouraging the creation of questions that are not easily learnable .","Annotators
rewarded for
submissions
in which
model
fails
to identify
correct sentence completion
before and after
fine - tuning
on
sample of the submitted questions","Annotators||rewarded for||submissions
submissions||in which||model
fails||to identify||correct sentence completion
correct sentence completion||before and after||fine - tuning
fine - tuning||on||sample of the submitted questions
","model||has||fails
",,"Model||has||Annotators
",,,,,
code,The full dataset is available at https://github.com/Websail-NU /CODAH .,https://github.com/Websail-NU /CODAH,,,,,"Contribution||Code||https://github.com/Websail-NU /CODAH
",,,,
hyperparameters,"Also , when training the initial SWAG model we use the hyperparameters recommended in the BERT paper , namely a batch size of 16 , learning rate of 2 e - 5 , and 3 epochs .","when training
initial SWAG model
use
hyperparameters
recommended in
BERT paper
namely
batch size
of
16
learning rate
of
2 e - 5
3
epochs","initial SWAG model||use||hyperparameters
hyperparameters||namely||learning rate
learning rate||of||2 e - 5
hyperparameters||namely||batch size
batch size||of||16
hyperparameters||namely||epochs
hyperparameters||recommended in||BERT paper
","epochs||has||3
","Hyperparameters||when training||initial SWAG model
",,,,,,
hyperparameters,"In our initial experiments , we found that a lower learning rate and more training epochs produced higher accuracy on CODAH , so we replaced the 5e - 5 learning rate in the original grid search with 1 e - 5 , and we added a 6 - epoch setting .","replaced
5e - 5 learning rate
in
original grid search
with
1 e - 5
added
6 - epoch setting","5e - 5 learning rate||with||1 e - 5
5e - 5 learning rate||in||original grid search
",,"Hyperparameters||added||6 - epoch setting
Hyperparameters||replaced||5e - 5 learning rate
",,,,,,
hyperparameters,The final hyperparameter grid is as follows :,final hyperparameter grid,,,,"Hyperparameters||has||final hyperparameter grid
",,,,,"final hyperparameter grid||has||Number of epochs
final hyperparameter grid||has||Learning rate
final hyperparameter grid||has||Batch size
"
hyperparameters,"Batch size : 16 , 32 Learning rate : 1 e - 5 , 2 e - 5 , 3 e - 5 Number of epochs : 3 , 4 , 6 In addition , we observed that in rare cases BERT fails to train ; that is , after several training epochs it has accuracy approximately equal to that of random guessing .","Batch size
16 , 32
Learning rate
1 e - 5 , 2 e - 5 , 3 e - 5
Number of epochs
3 , 4 , 6",,"Number of epochs||has||3 , 4 , 6
Learning rate||has||1 e - 5 , 2 e - 5 , 3 e - 5
Batch size||has||16 , 32
",,,,,,,
results,"As a baseline , we evaluate both models on the full SWAG training and validation sets , providing an accuracy of 84.2 % on BERT and 80.2 % on GPT .","As
baseline
evaluate
both models
on
full SWAG training and validation sets
providing
accuracy
of
84.2 %
on
BERT
80.2 %
on
GPT","baseline||evaluate||both models
both models||providing||accuracy
accuracy||of||84.2 %
84.2 %||on||BERT
accuracy||of||80.2 %
80.2 %||on||GPT
both models||on||full SWAG training and validation sets
",,"Results||As||baseline
",,,,,,
results,"To adjust for the difference in size between our dataset and SWAG , we also train the models on a sample of 2,241 SWAG questions ( the size of the training set in each of CODAH 's crossvalidation folds ) and evaluate them on the full SWAG validation set .","train
models
on
sample
of
2,241 SWAG questions
evaluate
on
full SWAG validation set","evaluate||on||full SWAG validation set
models||on||sample
sample||of||2,241 SWAG questions
","models||has||evaluate
","Results||train||models
",,,,,,
results,This produces an accuracy of 75.2 % for BERT ( using the cross-validation grid search ) and 63.6 % for GPT . :,"produces
accuracy
of
75.2 %
for
BERT
63.6 %
for
GPT","accuracy||of||63.6 %
63.6 %||for||GPT
accuracy||of||75.2 %
75.2 %||for||BERT
",,,,,"models||produces||accuracy
",,,
research-problem,Semantic Sentence Matching with Densely - connected Recurrent and Co - attentive Information,Semantic Sentence Matching,,,,,"Contribution||has research problem||Semantic Sentence Matching
",,,,
research-problem,"Sentence matching is widely used in various natural language tasks such as natural language inference , paraphrase identification , and question answering .",Sentence matching,,,,,"Contribution||has research problem||Sentence matching
",,,,
model,"Inspired by Densenet ) , we propose a densely - connected recurrent network where the recurrent hidden features are retained to the uppermost layer .","propose
densely - connected recurrent network
where
recurrent hidden features
retained to
uppermost layer","densely - connected recurrent network||where||recurrent hidden features
recurrent hidden features||retained to||uppermost layer
",,"Model||propose||densely - connected recurrent network
",,,,,,
model,"In addition , instead of the conventional summation operation , the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better .","instead of
conventional summation operation
concatenation operation
used
in combination with
attention mechanism
to preserve
co-attentive information
better","conventional summation operation||used||concatenation operation
concatenation operation||in combination with||attention mechanism
concatenation operation||to preserve||co-attentive information
","co-attentive information||has||better
","Model||instead of||conventional summation operation
",,,,,,
model,The proposed architecture shown in is called DRCN which is an abbreviation for Densely - connected Recurrent and Co -attentive neural Network .,"called
DRCN
abbreviation for
Densely - connected Recurrent and Co -attentive neural Network","DRCN||abbreviation for||Densely - connected Recurrent and Co -attentive neural Network
",,"Model||called||DRCN
",,,,,,
model,The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information .,"proposed
DRCN
utilize
increased representational power
of
deeper recurrent networks
attentive information","DRCN||utilize||increased representational power
increased representational power||of||deeper recurrent networks
increased representational power||of||attentive information
",,"Model||proposed||DRCN
",,,,,,
model,"Furthermore , to alleviate the problem of an ever- increasing feature vector size due to concatenation operations , we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure .","to
adopted
autoencoder
forwarded
fixed length vector
higher layer recurrent module","fixed length vector||to||higher layer recurrent module
",,"Model||adopted||autoencoder
Model||forwarded||fixed length vector
",,,,,,
hyperparameters,"We initialized word embedding with 300d Glo Ve vectors pre-trained from the 840B Common Crawl corpus ( Pennington , Socher , and Manning 2014 ) , while the word embeddings for the out - of - vocabulary words were initialized randomly .","initialized
word embedding
with
300d Glo Ve vectors
word embeddings
for
out - of - vocabulary words
initialized
randomly","word embedding||with||300d Glo Ve vectors
word embeddings||for||out - of - vocabulary words
word embeddings||initialized||randomly
",,"Hyperparameters||initialized||word embedding
","Hyperparameters||has||word embeddings
",,,,,
hyperparameters,We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network .,"randomly initialized
character embedding
with
16d vector
extracted
32d character representation
with
convolutional network","character embedding||with||16d vector
32d character representation||with||convolutional network
",,"Hyperparameters||randomly initialized||character embedding
Hyperparameters||extracted||32d character representation
",,,,,,
hyperparameters,"For the densely - connected recurrent layers , we stacked 5 layers each of which have 100 hidden units .","For
densely - connected recurrent layers
stacked
5 layers
each of which have
100 hidden units","densely - connected recurrent layers||stacked||5 layers
5 layers||each of which have||100 hidden units
",,"Hyperparameters||For||densely - connected recurrent layers
",,,,,,
hyperparameters,We set 1000 hidden units with respect to the fullyconnected layers .,"set
1000 hidden units
with respect to
fullyconnected layers","1000 hidden units||with respect to||fullyconnected layers
",,"Hyperparameters||set||1000 hidden units
",,,,,,
hyperparameters,The dropout was applied after the word and character embedding layers with a keep rate of 0.5 .,"dropout
applied after
word and character embedding layers
with
keep rate
of
0.5","dropout||applied after||word and character embedding layers
word and character embedding layers||with||keep rate
keep rate||of||0.5
",,,"Hyperparameters||has||dropout
",,,,,
hyperparameters,It was also applied before the fully - connected layers with a keep rate of 0.8 .,"applied before
fully - connected layers
with
keep rate
of
0.8","fully - connected layers||with||keep rate
keep rate||of||0.8
",,,,,"dropout||applied before||fully - connected layers
",,,
hyperparameters,"For the bottleneck component , we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2 .","bottleneck component
set
200 hidden units
as
encoded features
of
autoencoder
with
dropout rate
of
0.2","bottleneck component||set||200 hidden units
200 hidden units||with||dropout rate
dropout rate||of||0.2
200 hidden units||as||encoded features
encoded features||of||autoencoder
",,,"Hyperparameters||For||bottleneck component
",,,,,
hyperparameters,"The batch normalization was applied on the fully - connected layers , only for the one - way type datasets .","batch normalization
applied on
fully - connected layers
for
one - way type datasets","batch normalization||applied on||fully - connected layers
batch normalization||for||one - way type datasets
",,,"Hyperparameters||has||batch normalization
",,,,,
hyperparameters,The RMSProp optimizer with an initial learning rate of 0.001 was applied .,"RMSProp optimizer
with
initial learning rate
of
0.001","RMSProp optimizer||with||initial learning rate
initial learning rate||of||0.001
",,,"Hyperparameters||has||RMSProp optimizer
",,,,,
hyperparameters,The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve .,"learning rate
decreased by
factor
of
0.85
when
dev accuracy
does not improve","learning rate||decreased by||factor
factor||of||0.85
factor||when||dev accuracy
","dev accuracy||has||does not improve
",,"Hyperparameters||has||learning rate
",,,,,
hyperparameters,All weights except embedding matrices are constrained by L2 regularization with a regularization constant ? = 10 ?6 .,"weights
except
embedding matrices
constrained by
L2 regularization
with
regularization constant ? = 10 ?6","weights||constrained by||L2 regularization
L2 regularization||with||regularization constant ? = 10 ?6
weights||except||embedding matrices
",,,"Hyperparameters||has||weights
",,,,,
hyperparameters,"The sequence lengths of the sentence are all different for each dataset : 35 for SNLI , 55 for MultiNLI , 25 for Quora question pair and 50 for TrecQA .","sequence lengths
of
sentence
are
all different
for
each dataset
35
for
SNLI
55
for
MultiNLI
25
for
Quora question pair
50
for
TrecQA","sequence lengths||are||all different
all different||for||each dataset
55||for||MultiNLI
35||for||SNLI
25||for||Quora question pair
50||for||TrecQA
sequence lengths||of||sentence
","each dataset||has||55
each dataset||has||35
each dataset||has||25
each dataset||has||50
",,"Hyperparameters||has||sequence lengths
",,,,,
results,The proposed DRCN obtains an accuracy of 88.9 % which is a competitive score although we do not use any external knowledge like ESIM + ELMo and LM - Transformer .,"proposed DRCN
obtains
accuracy
of
88.9 %
is
competitive score","proposed DRCN||obtains||accuracy
accuracy||of||88.9 %
88.9 %||is||competitive score
",,,"Results||has||proposed DRCN
",,,,,
results,"The ensemble model achieves an accuracy of 90.1 % , which sets the new state - of the - art performance .","ensemble model
achieves
accuracy
of
90.1 %
sets
new state - of the - art performance","ensemble model||achieves||accuracy
accuracy||sets||new state - of the - art performance
accuracy||of||90.1 %
",,,"Results||has||ensemble model
",,,,,
results,Our ensemble model with 53 m parameters ( 6.7 m 8 ) outperforms the LM - Transformer whose the number of parameters is 85 m .,"with
53 m parameters ( 6.7 m 8 )
outperforms
LM - Transformer
whose
number of parameters
is
85 m","53 m parameters ( 6.7 m 8 )||outperforms||LM - Transformer
LM - Transformer||whose||number of parameters
number of parameters||is||85 m
",,,,,"ensemble model||with||53 m parameters ( 6.7 m 8 )
",,,
results,"Furthermore , in case of the encoding - based method , we obtain the best performance of 86.5 % without the co-attention and exact match flag .","in case of
encoding - based method
obtain
best performance
of
86.5 %
without
co-attention and exact match flag","encoding - based method||obtain||best performance
best performance||of||86.5 %
best performance||without||co-attention and exact match flag
",,"Results||in case of||encoding - based method
",,,,,,
results,shows the results on MATCHED and MISMATCHED problems of MultiNLI dataset .,"of
MultiNLI dataset",,,"Results||of||MultiNLI dataset
",,,,,,"MultiNLI dataset||has||Our plain DRCN
"
results,Our plain DRCN has a competitive performance without any contextualized knowledge .,"Our plain DRCN
competitive performance
without
contextualized knowledge","competitive performance||without||contextualized knowledge
","Our plain DRCN||has||competitive performance
",,,,,,,
results,"And , by combining DRCN with the ELMo , one of the contextualized embeddings from language models , our model outperforms the LM - Transformer which has 85 m parameters with fewer parameters of 61 m .","by combining
DRCN
with
ELMo
of
our model
outperforms
LM - Transformer
which has
85 m parameters
with
fewer parameters
61 m","DRCN||with||ELMo
our model||with||fewer parameters
fewer parameters||of||61 m
our model||outperforms||LM - Transformer
LM - Transformer||which has||85 m parameters
","DRCN||has||our model
",,,,"MultiNLI dataset||by combining||DRCN
",,,
results,Quora Question,,,,,,,,,,
results,Pair shows our results on the Quora question pair dataset .,"on
Quora question pair dataset",,,"Results||on||Quora question pair dataset
",,,,,,
results,"We obtained accuracies of 90.15 % and 91.30 % in single and ensemble methods , respectively , surpassing the previous state - of - the - art model of DIIN .","obtained
accuracies
of
90.15 % and 91.30 %
in
single and ensemble methods
surpassing
previous state - of - the - art model
of
DIIN","accuracies||of||90.15 % and 91.30 %
90.15 % and 91.30 %||in||single and ensemble methods
90.15 % and 91.30 %||surpassing||previous state - of - the - art model
previous state - of - the - art model||of||DIIN
",,,,,"Quora question pair dataset||obtained||accuracies
",,,
results,TrecQA and SelQA shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question .,TrecQA and SelQA datasets,,,,"Results||on||TrecQA and SelQA datasets
",,,,,"TrecQA and SelQA datasets||has||proposed DRCN
"
results,"However , the proposed DRCN using collective attentions over multiple layers , achieves the new state - of the - art performance , exceeding the current state - of - the - art performance significantly on both datasets .","proposed DRCN
using
collective attentions
over
multiple layers
achieves
new state - of the - art performance
exceeding
current state - of - the - art performance
significantly","proposed DRCN||using||collective attentions
collective attentions||over||multiple layers
proposed DRCN||achieves||new state - of the - art performance
new state - of the - art performance||exceeding||current state - of - the - art performance
","current state - of - the - art performance||has||significantly
",,,,,,,
ablation-analysis,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the performance was rather higher because of the regularization effect .","could see
performance
was
rather higher
because of
regularization effect","performance||because of||regularization effect
performance||was||rather higher
",,"Ablation analysis||could see||performance
",,,,,,
ablation-analysis,The result shows that the dense connections over attentive features are more effective .,"result
shows
dense connections
over
attentive features
are
more effective","result||shows||dense connections
dense connections||over||attentive features
dense connections||are||more effective
",,,"Ablation analysis||has||result
",,,,,
ablation-analysis,"In , we removed dense connections over both co-attentive and recurrent features , and the performance degraded to 88.5 % .","removed
dense connections
over
both co-attentive and recurrent features
performance
degraded
to
88.5 %","dense connections||over||both co-attentive and recurrent features
degraded||to||88.5 %
","dense connections||has||performance
performance||has||degraded
","Ablation analysis||removed||dense connections
",,,,,,
ablation-analysis,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .","demonstrate
dense connection
using
concatenation operation
over
deeper layers
more powerful capability
retaining
collective knowledge
to learn
textual semantics","dense connection||using||concatenation operation
concatenation operation||over||deeper layers
more powerful capability||retaining||collective knowledge
collective knowledge||to learn||textual semantics
","dense connection||has||more powerful capability
","Ablation analysis||demonstrate||dense connection
",,,,,,
ablation-analysis,The result of ( 10 ) shows that the connections among the layers are important to help gradient flow .,"shows
connections
among
layers
important to help
gradient flow","connections||among||layers
connections||important to help||gradient flow
",,"Ablation analysis||shows||connections
",,,,,,
ablation-analysis,"And , the result of ( 11 ) shows that the attentive information functioning as a soft - alignment is significantly effective in semantic sentence matching .","attentive information
functioning as
soft - alignment
is
significantly effective
in
semantic sentence matching","attentive information||functioning as||soft - alignment
attentive information||is||significantly effective
significantly effective||in||semantic sentence matching
",,,"Ablation analysis||shows||attentive information
",,,,,
ablation-analysis,"The models ( 5 - 9 ) which have connections between layers , are more robust to the increased depth of network , however , the performances of ( 10 - 11 ) tend to degrade as layers get deeper .","models
which have
connections
between
layers
are
more robust
to
increased depth of network
performances
tend to
degrade
as
layers
get
deeper","models||which have||connections
connections||are||more robust
more robust||to||increased depth of network
connections||between||layers
performances||tend to||degrade
degrade||as||layers
layers||get||deeper
","models||has||performances
",,"Ablation analysis||has||models
",,,,,
ablation-analysis,"In addition , the models with dense connections rather than residual connections , have higher performance in general .","with
dense connections
rather than
residual connections
have
higher performance","dense connections||have||higher performance
dense connections||rather than||residual connections
",,,,,"models||with||dense connections
",,,
ablation-analysis,"shows that the connection between layers is essential , especially in deep models , endowing more representational power , and the dense connection is more effective than the residual connection .","connection
between
layers
is
essential
endowing
more representational power
dense connection
is
more effective
than
residual connection","connection||endowing||more representational power
connection||is||essential
dense connection||is||more effective
more effective||than||residual connection
connection||between||layers
","connection||has||dense connection
",,"Ablation analysis||shows||connection
",,,,,
research-problem,Natural language sentence matching is a fundamental technology for a variety of tasks .,Natural language sentence matching,,,,,"Contribution||has research problem||Natural language sentence matching
",,,,
model,"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .","BiLSTM layer
utilized to
aggregate
matching results
into
fixed - length matching vector","BiLSTM layer||utilized to||aggregate
matching results||into||fixed - length matching vector
","aggregate||has||matching results
",,"Model||has||BiLSTM layer
",,,,,
model,"Finally , based on the matching vector , a decision is made through a fully connected layer .","based on
matching vector
decision
made through
fully connected layer","decision||made through||fully connected layer
","matching vector||has||decision
","Model||based on||matching vector
",,,,,,
research-problem,Natural language sentence matching ( NLSM ) is the task of comparing two sentences and identifying the relationship between them .,Natural language sentence matching ( NLSM ),,,,,"Contribution||has research problem||Natural language sentence matching ( NLSM )
",,,,
research-problem,"For example , in a paraphrase identification task , NLSM is used to determine whether two sentences are paraphrase or not .",NLSM,,,,,"Contribution||has research problem||NLSM
",,,,
model,"In this paper , to tackle these limitations , we propose a bilateral multi-perspective matching ( BiMPM ) model for NLSM tasks .","propose
bilateral multi-perspective matching ( BiMPM ) model
for
NLSM tasks","bilateral multi-perspective matching ( BiMPM ) model||for||NLSM tasks
",,"Model||propose||bilateral multi-perspective matching ( BiMPM ) model
",,,,,,
model,"Our model essentially belongs to the "" matching aggregation "" framework .","belongs to
"" matching aggregation "" framework",,,"Model||belongs to||"" matching aggregation "" framework
",,,,,,
model,"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .",,,,,,,,,,
model,"Finally , based on the matching vector , a decision is made through a fully connected layer .",,,,,,,,,,
hyperparameters,We initialize word embeddings in the word representation layer with the 300 - dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus .,"initialize
word embeddings
in
word representation layer
with
300 - dimensional GloVe word vectors
pretrained from
840B Common Crawl corpus","word embeddings||with||300 - dimensional GloVe word vectors
300 - dimensional GloVe word vectors||pretrained from||840B Common Crawl corpus
word embeddings||in||word representation layer
",,"Hyperparameters||initialize||word embeddings
",,,,,,
hyperparameters,"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .","For
out - of - vocabulary ( OOV ) words
initialize
word embeddings
randomly","out - of - vocabulary ( OOV ) words||initialize||word embeddings
","word embeddings||has||randomly
","Hyperparameters||For||out - of - vocabulary ( OOV ) words
",,,,,,
hyperparameters,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a LSTM layer .","charactercomposed embeddings
initialize
each character
as
20 - dimensional vector
compose
each word
into
50 dimensional vector
with
LSTM layer","charactercomposed embeddings||compose||each word
each word||into||50 dimensional vector
50 dimensional vector||with||LSTM layer
charactercomposed embeddings||initialize||each character
each character||as||20 - dimensional vector
",,,"Hyperparameters||For||charactercomposed embeddings
",,,,,
hyperparameters,We set the hidden size as 100 for all BiLSTM layers .,"set
hidden size
as
100
for
all BiLSTM layers","hidden size||as||100
hidden size||for||all BiLSTM layers
",,"Hyperparameters||set||hidden size
",,,,,,
hyperparameters,"We apply dropout to every layers in , and set the dropout ratio as 0.1 .","apply
dropout
to
every layers
set
dropout ratio
as
0.1","dropout||set||dropout ratio
dropout ratio||as||0.1
dropout||to||every layers
",,"Hyperparameters||apply||dropout
",,,,,,
hyperparameters,"To train the model , we minimize the cross entropy of the training set , and use the ADAM optimizer [ Kingma and Ba , 2014 ] to update parameters .","minimize
cross entropy
of
training set
use
ADAM optimizer
to update
parameters","cross entropy||of||training set
ADAM optimizer||to update||parameters
",,"Hyperparameters||minimize||cross entropy
Hyperparameters||use||ADAM optimizer
",,,,,,
hyperparameters,We set the learning rate as 0.001 .,"learning rate
as
0.001","learning rate||as||0.001
",,,"Hyperparameters||set||learning rate
",,,,,
hyperparameters,"During training , we do not update the pre-trained word embeddings .","During
training
do not update
pre-trained word embeddings","training||do not update||pre-trained word embeddings
",,"Hyperparameters||During||training
",,,,,,
experiments,"In this Sub-section , we compare our model with state - of - theart models on the paraphrase identification task .",paraphrase identification task,,,,,,,,,"paraphrase identification task||has||Results
"
experiments,"We still experiment on the "" Quora Question Pairs "" dataset , and use the same dataset partition as Sub-section 4.2 .","experiment on
"" Quora Question Pairs "" dataset",,,,,,"Results||experiment on||"" Quora Question Pairs "" dataset
",,,
experiments,"First , under the Siamese framework , we implement two baseline models : "" Siamese - CNN "" and "" Siamese - LSTM "" .","under
Siamese framework
implement
two baseline models
Siamese - CNN
Siamese - LSTM","Siamese framework||implement||two baseline models
","two baseline models||name||Siamese - CNN
two baseline models||name||Siamese - LSTM
",,,,"Baselines||under||Siamese framework
",,,
experiments,"Second , based on the two baseline models , we implement two more baseline models "" Multi - Perspective - CNN "" and "" Multi - Perspective - LSTM "" .","on
implement
two more baseline models
Multi - Perspective - CNN
Multi - Perspective - LSTM",,"two more baseline models||name||Multi - Perspective - CNN
two more baseline models||name||Multi - Perspective - LSTM
",,,,"Baselines||implement||two more baseline models
",,,
experiments,"Third , we re-implement the "" L.D.C. "" model proposed by , which is a model under the "" matchingaggregation "" framework and acquires the state - of - the - art performance on several tasks .","re-implement
"" L.D.C. "" model
is
model
under
"" matchingaggregation "" framework
on",""" L.D.C. "" model||is||model
model||under||"" matchingaggregation "" framework
",,,,,"Baselines||re-implement||"" L.D.C. "" model
",,,
experiments,"We can see that "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) works much better than "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .","see that
"" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" )
works
much better
than
"" Siamese - CNN "" ( or "" Siamese - LSTM "" )",""" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" )||works||much better
much better||than||"" Siamese - CNN "" ( or "" Siamese - LSTM "" )
",,,,,"Results||see that||"" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" )
",,,
experiments,"Our "" BiMPM "" model outperforms the "" L.D.C. "" model by more than two percent .","Our "" BiMPM "" model
outperforms
"" L.D.C. "" model
by
more than two percent","outperforms||by||more than two percent
","Our "" BiMPM "" model||has||outperforms
outperforms||has||"" L.D.C. "" model
",,,,,,"Results||has||Our "" BiMPM "" model
",
experiments,"In this Sub-section , we evaluate our model on the natural language inference task over the SNLI dataset .","natural language inference task
over
SNLI dataset","natural language inference task||over||SNLI dataset
",,,,,,,"Tasks||on||natural language inference task
","SNLI dataset||has||Results
"
experiments,"First , we can see that "" Only P ? Q "" works significantly better than "" Only P ? Q "" , which tells us that , for natural language inference , matching the hypothesis against the premise is more effective than the other way around .","see that
Only P ? Q
works significantly better
than
Only P ? Q
tells
matching the hypothesis
against
premise
is
more effective
than
other way around","works significantly better||tells||matching the hypothesis
matching the hypothesis||against||premise
matching the hypothesis||is||more effective
more effective||than||other way around
works significantly better||than||Only P ? Q
","Only P ? Q||has||works significantly better
",,,,"Results||see that||Only P ? Q
",,,
experiments,"Second , our "" BiMPM "" model works much better than "" Only P ? Q "" , which reveals that matching premise against the hypothesis can also bring some benefits .","our "" BiMPM "" model
works
much better
than
Only P ? Q","our "" BiMPM "" model||works||much better
much better||than||Only P ? Q
",,,,,,,"Results||has||our "" BiMPM "" model
",
experiments,"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the state - of - the - art single models , and our ' BiMPM ( Ensemble ) "" works much better than "" ( Ensemble ) "" .","observe that
our single model "" BiMPM ""
on par with
state - of - the - art single models
our ' BiMPM ( Ensemble )
works
much better
than
Ensemble","our ' BiMPM ( Ensemble )||works||much better
much better||than||Ensemble
our single model "" BiMPM ""||on par with||state - of - the - art single models
",,,,,"Results||observe that||our ' BiMPM ( Ensemble )
Results||observe that||our single model "" BiMPM ""
",,,
experiments,"Therefore , our models achieve the state - of - the - art performance in both single and ensemble scenarios for the natural language inference task .","our models
achieve
state - of - the - art performance
in both
single and ensemble scenarios
for
natural language inference task","our models||achieve||state - of - the - art performance
state - of - the - art performance||in both||single and ensemble scenarios
state - of - the - art performance||for||natural language inference task
",,,,,,,"Results||has||our models
",
experiments,"In this Sub-section , we study the effectiveness of our model for answer sentence selection tasks .","for
answer sentence selection tasks",,,,,,"Tasks||for||answer sentence selection tasks
",,,
experiments,We experiment on two datasets : TREC - QA and WikiQA .,"experiment on
two datasets
TREC - QA
WikiQA",,"two datasets||name||TREC - QA
two datasets||name||WikiQA
",,,,"answer sentence selection tasks||experiment on||two datasets
",,,
experiments,We can see that the performance from our model is on par with the state - of - the - art models .,"see that
performance
from
our model
is
on par
with
state - of - the - art models","performance||from||our model
performance||is||on par
on par||with||state - of - the - art models
",,,,,"answer sentence selection tasks||see that||performance
",,,
research-problem,Reinforced Mnemonic Reader for Machine Reading Comprehension,Machine Reading Comprehension,,,,,"Contribution||has research problem||Machine Reading Comprehension
",,,,
model,"To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them to refine current attentions in a multi-round alignment architecture .","present
reattention mechanism
that
temporally memorizes
past attentions
to refine
current attentions
in
multi-round alignment architecture","reattention mechanism||that||temporally memorizes
past attentions||to refine||current attentions
current attentions||in||multi-round alignment architecture
","temporally memorizes||has||past attentions
","Model||present||reattention mechanism
",,,,,,
model,"The computation is based on the fact that two words should share similar semantics if their attentions about same texts are highly overlapped , and be less similar vice versa .","computation
based on
two words
share
similar semantics
if
attentions
about
same texts
are
highly overlapped
be
less similar","computation||based on||two words
two words||share||similar semantics
two words||if||attentions
attentions||about||same texts
same texts||be||less similar
same texts||are||highly overlapped
",,,"Model||has||computation
",,,,,
model,"Therefore , the reattention can be more concentrated if past attentions focus on same parts of the input , or be relatively more distracted so as to focus on new regions if past attentions are not overlapped at all .","reattention
be
more concentrated
if
past attentions
focus on
same parts
of
input
relatively more distracted
to focus on
new regions
if
past attentions
are
not overlapped at all","reattention||be||more concentrated
more concentrated||if||past attentions
past attentions||focus on||same parts
same parts||of||input
reattention||be||relatively more distracted
relatively more distracted||to focus on||new regions
new regions||if||past attentions
past attentions||are||not overlapped at all
",,,"Model||has||reattention
",,,,,
model,"As for the second problem , we extend the traditional training method with a novel approach called dynamic - critical reinforcement learning .","extend
traditional training method
with
novel approach
called
dynamic - critical reinforcement learning","traditional training method||with||novel approach
novel approach||called||dynamic - critical reinforcement learning
",,"Model||extend||traditional training method
",,,,,,
model,"Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach dynamically decides the reward and the baseline according to two sampling strategies , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .","dynamically decides
reward and the baseline
according to
two sampling strategies","reward and the baseline||according to||two sampling strategies
",,,,,"dynamic - critical reinforcement learning||dynamically decides||reward and the baseline
",,,
hyperparameters,"We use the Adam optimizer [ Kingma and Ba , 2014 ] for both ML and DCRL training .","use
Adam optimizer
for
ML and DCRL training","Adam optimizer||for||ML and DCRL training
",,"Hyperparameters||use||Adam optimizer
",,,,,,
hyperparameters,"The initial learning rates are 0.0008 and 0.0001 respectively , and are halved whenever meeting a bad iteration .",initial learning rates,,,,"Hyperparameters||has||initial learning rates
",,,,,"initial learning rates||are||0.0008 and 0.0001
initial learning rates||are||halved
"
hyperparameters,The batch size is 48 and a dropout rate of 0.3 is used to prevent overfitting .,"batch size
is
48
dropout rate
of
0.3
to prevent
overfitting","batch size||is||48
dropout rate||of||0.3
","overfitting||has||batch size
overfitting||has||dropout rate
","Hyperparameters||to prevent||overfitting
",,,,,,
hyperparameters,Word embeddings remain fixed during training .,"Word embeddings
remain
fixed
during
training","Word embeddings||remain||fixed
fixed||during||training
",,,"Hyperparameters||has||Word embeddings
",,,,,
hyperparameters,"For out of vocabulary words , we set the embeddings from Gaussian distributions and keep them trainable .","For
out of vocabulary words
set
embeddings
from
Gaussian distributions
keep them
trainable","out of vocabulary words||set||embeddings
embeddings||keep them||trainable
embeddings||from||Gaussian distributions
",,"Hyperparameters||For||out of vocabulary words
",,,,,,
hyperparameters,"The size of character embedding and corresponding LSTMs is 50 , the main hidden size is 100 , and the hyperparameter ? is 3 .","size
of
character embedding and corresponding LSTMs
is
50
main hidden size
is
100
hyperparameter
is
3","hyperparameter||is||3
size||of||character embedding and corresponding LSTMs
character embedding and corresponding LSTMs||is||50
main hidden size||is||100
",,,"Hyperparameters||has||hyperparameter
Hyperparameters||has||size
Hyperparameters||has||main hidden size
",,,,,
results,We submitted our model on the hidden test set of SQuAD for evaluation .,"on
hidden test set
of
SQuAD","hidden test set||of||SQuAD
",,"Results||on||hidden test set
",,,,,,"SQuAD||has||R.M - Reader
"
results,"As shown in , R.M - Reader achieves an EM score of 79.5 % and F1 score of 86.6 % .","R.M - Reader
achieves
EM score
of
79.5 %
F1 score
of
86.6 %","R.M - Reader||achieves||EM score
EM score||of||79.5 %
R.M - Reader||achieves||F1 score
F1 score||of||86.6 %
",,,,,,,,
results,"Our ensemble model improves the metrics to 82.3 % and 88.5 % respectively 2 . shows the performance comparison on two adversarial datasets , Add Sent and Add OneSent .","Our ensemble model
improves
metrics
to
82.3 % and 88.5 %","Our ensemble model||improves||metrics
metrics||to||82.3 % and 88.5 %
",,,,,,,"SQuAD||has||Our ensemble model
",
results,"As we can see , R.M - Reader comfortably outperforms all previous models by more than 6 % in both EM and F 1 scores , indicating that our model is more robust against adversarial attacks .","comfortably outperforms
previous models
by
more than 6 %
in
EM and F 1 scores","previous models||by||more than 6 %
more than 6 %||in||EM and F 1 scores
","comfortably outperforms||has||previous models
",,,,,,"R.M - Reader||has||comfortably outperforms
",
ablation-analysis,"We notice that reattention has more influences on EM score while DCRL contributes more to F1 metric , and removing both of them results in huge drops on both metrics .","notice
reattention
more influences
on
EM score
DCRL
contributes more to
F1 metric
removing both
results in
huge drops
on
both metrics","more influences||on||EM score
DCRL||contributes more to||F1 metric
removing both||results in||huge drops
huge drops||on||both metrics
","reattention||has||more influences
","Ablation analysis||notice||reattention
Ablation analysis||notice||DCRL
Ablation analysis||notice||removing both
",,,,,,
ablation-analysis,Replacing DCRL with SCST also causes a marginal decline of performance on both metrics .,"Replacing
DCRL
with
SCST
causes
marginal decline
of
performance
on
both metrics","DCRL||with||SCST
DCRL||causes||marginal decline
marginal decline||of||performance
performance||on||both metrics
",,"Ablation analysis||Replacing||DCRL
",,,,,,
ablation-analysis,"Next , we relace the default attention function with the dot product : f ( u , v ) = u v ( 5 ) , and both metrics suffer from degradations .","relace
default attention function
with
dot product : f ( u , v ) = u v ( 5 )
both metrics
suffer from
degradations","default attention function||with||dot product : f ( u , v ) = u v ( 5 )
both metrics||suffer from||degradations
","default attention function||has||both metrics
","Ablation analysis||relace||default attention function
",,,,,,
ablation-analysis,"Removing any of the two heuristics leads to some performance declines , and heuristic subtraction is more effective than multiplication .","Removing
any of the two heuristics
leads to
some performance declines
heuristic subtraction
is
more effective
than
multiplication","any of the two heuristics||leads to||some performance declines
heuristic subtraction||is||more effective
more effective||than||multiplication
","any of the two heuristics||has||heuristic subtraction
","Ablation analysis||Removing||any of the two heuristics
",,,,,,
ablation-analysis,In both cases the highway - like function has outperformed its simpler variants .,"highway - like function
outperformed
simpler variants",,"highway - like function||has||outperformed
outperformed||has||simpler variants
",,"Ablation analysis||has||highway - like function
",,,,,
ablation-analysis,"We notice that using 2 blocks causes a slight performance drop , while increasing to 4 blocks barely affects the SoTA result .","using
2 blocks
causes
slight performance drop
increasing to
4 blocks
barely affects
SoTA result","2 blocks||causes||slight performance drop
4 blocks||barely affects||SoTA result
",,"Ablation analysis||using||2 blocks
Ablation analysis||increasing to||4 blocks
",,,,,,
ablation-analysis,"Interestingly , a very deep alignment with 5 blocks results in a significant performance decline .","very deep alignment
with
5 blocks
results in
significant performance decline","very deep alignment||with||5 blocks
5 blocks||results in||significant performance decline
",,,"Ablation analysis||has||very deep alignment
",,,,,
research-problem,Neural Variational Inference for Text Processing Phil Blunsom 12,Neural Variational Inference,,,,,"Contribution||has research problem||Neural Variational Inference
",,,,
model,"This paper introduces a neural variational framework for generative models of text , inspired by the variational autoencoder .","introduces
neural variational framework
for
generative models
of
text
inspired by
variational autoencoder","neural variational framework||for||generative models
generative models||of||text
neural variational framework||inspired by||variational autoencoder
",,"Model||introduces||neural variational framework
",,,,,,
model,"The principle idea is to build an inference network , implemented by a deep neural network conditioned on text , to approximate the intractable distributions over the latent variables .","build
inference network
implemented by
deep neural network
conditioned on
text
to approximate
intractable distributions
over
latent variables","inference network||implemented by||deep neural network
deep neural network||conditioned on||text
inference network||to approximate||intractable distributions
intractable distributions||over||latent variables
",,"Model||build||inference network
",,,,,,
model,"Instead of providing an analytic approximation , as in traditional variational Bayes , neural variational inference learns to model the posterior probability , thus endowing the model with strong generalis ation abilities .","neural variational inference
learns to model
posterior probability","neural variational inference||learns to model||posterior probability
",,,"Model||has||neural variational inference
",,,,,
model,"By using the reparameteris ation method , the inference network is trained through back - propagating unbiased and low variance gradients w.r.t. the latent variables .","using
reparameteris ation method
inference network
trained through
back - propagating
unbiased and low variance gradients
w.r.t.
latent variables","inference network||trained through||back - propagating
unbiased and low variance gradients||w.r.t.||latent variables
","reparameteris ation method||has||inference network
back - propagating||has||unbiased and low variance gradients
","Model||using||reparameteris ation method
",,,,,,
model,"Within this framework , we propose a Neural Variational Document Model ( NVDM ) for document modelling and a Neural Answer Selection Model ( NASM ) for question answering , a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences .","propose
Neural Variational Document Model ( NVDM )
for
document modelling
Neural Answer Selection Model ( NASM )
for
question answering","Neural Answer Selection Model ( NASM )||for||question answering
Neural Variational Document Model ( NVDM )||for||document modelling
",,"Model||propose||Neural Answer Selection Model ( NASM )
Model||propose||Neural Variational Document Model ( NVDM )
",,,,,,
model,A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector .,"primary feature
of
NVDM
is
each word
generated directly from
dense continuous document representation
instead of
more common binary semantic vector","primary feature||of||NVDM
NVDM||is||each word
each word||generated directly from||dense continuous document representation
dense continuous document representation||instead of||more common binary semantic vector
",,,"Model||has||primary feature
",,,,,
model,The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,"NASM
is
supervised conditional model
imbues
LSTMs
with
latent stochastic attention mechanism
to model
semantics
of
question - answer pairs","NASM||is||supervised conditional model
supervised conditional model||imbues||LSTMs
LSTMs||with||latent stochastic attention mechanism
LSTMs||to model||semantics
semantics||of||question - answer pairs
",,,"Model||has||NASM
",,,,,
model,The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution .,"attention model
designed to
focus
on
phrases
of
answer
strongly connected to
question semantics
modelled by
latent distribution","attention model||designed to||focus
focus||on||phrases
phrases||of||answer
attention model||modelled by||latent distribution
attention model||strongly connected to||question semantics
",,,,,,,"NASM||has||attention model
",
model,"Bayesian inference provides a natural safeguard against overfitting , especially as the training sets available for this task are small .","Bayesian inference
provides
natural safeguard
against
overfitting","Bayesian inference||provides||natural safeguard
natural safeguard||against||overfitting
",,,,,,,"NASM||has||Bayesian inference
",
experiments,Experiments on Document Modelling,"on
Document Modelling",,,"Experiments||on||Document Modelling
",,,,,,"Document Modelling||has||Results
"
experiments,The experimental results indicate that NVDM achieves the best performance on both datasets .,"indicate
NVDM
achieves
best performance
on
both datasets","NVDM||achieves||best performance
best performance||on||both datasets
",,,,,"Results||indicate||NVDM
",,,
experiments,"For the experiments on RCV1 - v2 dataset , the NVDM with latent variable of 50 dimension performs even better than the fDARN with 200 dimension .","For
experiments
on
RCV1 - v2 dataset
NVDM
with
latent variable
of
50 dimension
performs
even better
than
fDARN
with
200 dimension","experiments||on||RCV1 - v2 dataset
NVDM||with||latent variable
latent variable||performs||even better
even better||than||fDARN
fDARN||with||200 dimension
latent variable||of||50 dimension
","RCV1 - v2 dataset||has||NVDM
",,,,"Results||For||experiments
",,,
experiments,Dataset & Setup for Answer Sentence Selection,"for
Answer Sentence Selection",,,"Experiments||for||Answer Sentence Selection
",,,,,,"Answer Sentence Selection||has||Hyperparameters
"
experiments,The word embeddings ( K = 50 ) are obtained by running the word2vec tool on the English Wikipedia dump and the AQUAINT 5 corpus .,"word embeddings ( K = 50 )
are
obtained
by running
word2vec tool
on
English Wikipedia dump
AQUAINT 5 corpus","word embeddings ( K = 50 )||are||obtained
obtained||by running||word2vec tool
word2vec tool||on||English Wikipedia dump
word2vec tool||on||AQUAINT 5 corpus
",,,,,,,"Hyperparameters||has||word embeddings ( K = 50 )
",
experiments,"We use LSTMs with 3 layers and 50 hidden units , and apply 40 % dropout after the embedding layer .","use
LSTMs
with
3 layers
50 hidden units
apply
40 % dropout
after
embedding layer","LSTMs||with||3 layers
LSTMs||with||50 hidden units
LSTMs||apply||40 % dropout
40 % dropout||after||embedding layer
",,,,,"Hyperparameters||use||LSTMs
",,,
experiments,"For the construction of the inference network , we use an MLP ( Eq. 10 ) with 2 layers and tanh units of 50 dimension , and an MLP ( Eq. 17 ) with 2 layers and tanh units of 150 dimension for modelling the joint representation .","For the construction of
inference network
use
MLP
with
2 layers
tanh units
of
50 dimension
MLP
with
2 layers
tanh units
of
150 dimension
for modelling
joint representation","inference network||use||MLP
MLP||with||2 layers
inference network||use||tanh units
tanh units||of||50 dimension
MLP||with||2 layers
tanh units||of||150 dimension
","joint representation||has||MLP
joint representation||has||tanh units
",,,,"Hyperparameters||For the construction of||inference network
Hyperparameters||for modelling||joint representation
",,,
experiments,"During training we carry out stochastic estimation by taking one sample for computing the gradients , while in prediction we use 20 samples to calculate the expectation of the lower bound .","During
training
carry out
stochastic estimation
by taking
one sample
for computing
gradients
in
prediction
use
20 samples
to calculate
expectation
of
lower bound","prediction||use||20 samples
20 samples||to calculate||expectation
expectation||of||lower bound
training||carry out||stochastic estimation
stochastic estimation||by taking||one sample
one sample||for computing||gradients
",,,,,"Hyperparameters||in||prediction
Hyperparameters||During||training
",,,
experiments,"The LSTM + Att performs slightly better than the vanilla LSTM model , and our NASM improves the results further .","LSTM + Att
performs
slightly better
than
vanilla LSTM model
our NASM
improves
results","LSTM + Att||performs||slightly better
slightly better||than||vanilla LSTM model
our NASM||improves||results
","slightly better||has||our NASM
",,,,,,"Results||has||LSTM + Att
",
experiments,"Since the QASent dataset is biased towards lexical overlapping features , after combining with a co-occurrence word count feature , our best model NASM outperforms all the previous models , including both neural network based models and classifiers with a set of hand - crafted features ( e.g. LCLR ) .","after combining with
co-occurrence word count feature
our best model
outperforms
all the previous models
including
neural network based models
classifiers
with
set
of
hand - crafted features","our best model||after combining with||co-occurrence word count feature
our best model||outperforms||all the previous models
all the previous models||including||neural network based models
all the previous models||including||classifiers
classifiers||with||set
set||of||hand - crafted features
",,,,,,,"Results||has||our best model
",
experiments,"Similarly , on the Wik - iQA dataset , all of our models outperform the previous distributional models by a large margin .","on
Wik - iQA dataset
all of our models
outperform
previous distributional models
by
large margin","all of our models||outperform||previous distributional models
previous distributional models||by||large margin
","Wik - iQA dataset||has||all of our models
",,,,"Results||on||Wik - iQA dataset
",,,
experiments,"By including a word count feature , our models improve further and achieve the state - of - the - art .","including
word count feature
our models
improve further
achieve
state - of - the - art","our models||achieve||state - of - the - art
","word count feature||has||our models
our models||has||improve further
",,,,"Results||including||word count feature
",,,
research-problem,Distance - based Self - Attention Network for Natural Language Inference,Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,"Our model shows good performance with NLI data , and it records the new state - of - the - art result with SNLI data .",NLI,,,,,"Contribution||has research problem||NLI
",,,,
research-problem,"More recently , models incorporating attention mechanisms have shown good performance in machine translation , Natural Language Inference ( NLI ) , and Question Answering ( QA ) etc .",Natural Language Inference ( NLI ),,,,,"Contribution||has research problem||Natural Language Inference ( NLI )
",,,,
model,"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a distance mask which models the relative distance between words .","propose
Distancebased Self - Attention Network
introduces
distance mask
models
relative distance
between
words","Distancebased Self - Attention Network||introduces||distance mask
distance mask||models||relative distance
relative distance||between||words
",,"Model||propose||Distancebased Self - Attention Network
",,,,,,
model,"In conjunction with a directional mask , the distance mask allows us to incorporate complete positional information of words in our model .","In conjunction with
directional mask
distance mask
allows us to incorporate
complete positional information of words
in
our model","distance mask||allows us to incorporate||complete positional information of words
complete positional information of words||in||our model
","directional mask||has||distance mask
","Model||In conjunction with||directional mask
",,,,,,
experimental-setup,We used the Glove 840B 300D 1 ( d e = 300 ) for the pre-trained word embedding without any finetuning .,"used
Glove 840B 300D 1 ( d e = 300 )
for
pre-trained word embedding
without
finetuning","Glove 840B 300D 1 ( d e = 300 )||for||pre-trained word embedding
pre-trained word embedding||without||finetuning
",,"Experimental setup||used||Glove 840B 300D 1 ( d e = 300 )
",,,,,,
experimental-setup,This is to train the more universally usable sentence encoder .,"train
more universally usable sentence encoder",,,,,,"Glove 840B 300D 1 ( d e = 300 )||train||more universally usable sentence encoder
",,,
experimental-setup,"Layer normalization was applied to all linear projections of masked multihead attention , fusion gate , and multi-dimensional attention .","Layer normalization
applied to
all linear projections
of
masked multihead attention
fusion gate
multi-dimensional attention","Layer normalization||applied to||all linear projections
all linear projections||of||masked multihead attention
all linear projections||of||fusion gate
all linear projections||of||multi-dimensional attention
",,,"Experimental setup||has||Layer normalization
",,,,,
experimental-setup,"We applied residual dropout as used in , with dropout to the output of masked multi-head attention and SF +H F +b F of fusion gate .","applied
residual dropout
with
dropout
to
output
of
masked multi-head attention
SF +H F +b F of fusion gate","residual dropout||with||dropout
dropout||to||output
output||of||masked multi-head attention
output||of||SF +H F +b F of fusion gate
",,"Experimental setup||applied||residual dropout
",,,,,,
experimental-setup,"We set h = 5 , ? = 1.5 in the masked multi-head attention , and the dropout probability was set to 0.1 .","set
h = 5 , ? = 1.5
in
masked multi-head attention
dropout probability
set to
0.1","h = 5 , ? = 1.5||in||masked multi-head attention
",,"Experimental setup||set||h = 5 , ? = 1.5
",,,,,,
experimental-setup,"Batch size was 64 , and the model was trained with Adam optimizer , with a learning rate of 0.001 .","Batch size
was
64
model
trained with
Adam optimizer
with
learning rate
of
0.001","Batch size||was||64
model||trained with||Adam optimizer
Adam optimizer||with||learning rate
learning rate||of||0.001
",,,"Experimental setup||has||Batch size
Experimental setup||has||model
",,,,,
experimental-setup,All models were implemented via Tensorflow on single Nvidia Geforce GTX 1080 Ti GPU .,"implemented via
Tensorflow
on
single Nvidia Geforce GTX 1080 Ti GPU","Tensorflow||on||single Nvidia Geforce GTX 1080 Ti GPU
",,"Experimental setup||implemented via||Tensorflow
",,,,,,
results,Experimental results of SNLI data compared with the existing models on the SNLI leader - board 2 are shown in .,"of
SNLI data",,,"Results||of||SNLI data
",,,,,,
results,"Compared with the existing state - of - the - art model , the number of parameters and the training time increased , but our results show the new state - of - theart record .","Compared with
existing state - of - the - art model
our results
show
new state - of - theart record","our results||show||new state - of - theart record
","existing state - of - the - art model||has||our results
",,,,"SNLI data||Compared with||existing state - of - the - art model
",,,
results,Results show that the addition of the distance mask improved the performance without significantly affecting the training time or increasing the number of parameters . 49.4 50.4 + Unigram and bigram features 99.7 78.2 Sentence encoding - based models 100D LSTM encoders 220 k 84.8 77.6 300D LSTM encoders 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3.0 m 86.2 84.6 600D,"show
addition
of
distance mask
improved
performance
without
significantly affecting
training time
increasing
parameters","addition||of||distance mask
distance mask||improved||performance
performance||without||significantly affecting
performance||without||increasing
","significantly affecting||has||training time
increasing||has||parameters
",,,,"SNLI data||show||addition
",,,
results,"2 The improvement of the test accuracy by introducing the distance mask is only by 0.3 % point , potentially because SNLI data mostly consist of short sentences .","improvement
of
test accuracy
by introducing
distance mask
is only by
0.3 % point","improvement||by introducing||distance mask
improvement||of||test accuracy
test accuracy||is only by||0.3 % point
",,,,,,,"SNLI data||has||improvement
",
results,The results of applying SNLI best model to MultiNLI dataset without additional parameter tuning are presented in .,"applying
SNLI best model
to
MultiNLI dataset","SNLI best model||to||MultiNLI dataset
",,"Results||applying||SNLI best model
",,,,,,
results,"Compared with the result of RepEVAL 2017 , we can see that the Distance - based Self - Attention Network performs well .","Compared with
result of RepEVAL 2017
see that
Distance - based Self - Attention Network
performs
well","result of RepEVAL 2017||see that||Distance - based Self - Attention Network
Distance - based Self - Attention Network||performs||well
",,,,,"MultiNLI dataset||Compared with||result of RepEVAL 2017
",,,
results,"When compared with the model of , our model showed similar average test accuracy with much lower number of parameters .","with
our model
showed
similar average test accuracy
much lower number of parameters","our model||showed||similar average test accuracy
similar average test accuracy||with||much lower number of parameters
",,,,,,,"MultiNLI dataset||has||our model
",
research-problem,A Question - Focused Multi- Factor Attention Network for Question Answering,Question Answering,,,,,"Contribution||has research problem||Question Answering
",,,,
research-problem,Neural network models recently proposed for question answering ( QA ) primarily focus on capturing the passagequestion relation .,question answering ( QA ),,,,,"Contribution||has research problem||question answering ( QA )
",,,,
research-problem,They also do not explicitly focus on the question and answer type which often plays a critical role in QA .,QA,,,,,"Contribution||has research problem||QA
",,,,
research-problem,"In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts .",machine comprehension - based ( MC ) question answering ( QA ),,,,,"Contribution||has research problem||machine comprehension - based ( MC ) question answering ( QA )
",,,,
model,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .","propose
end - to - end question - focused multi-factor attention network
for
document - based question answering
AMANDA
learns to
aggregate
evidence
distributed across
multiple sentences
identifies
important question words
to help
extract
answer","end - to - end question - focused multi-factor attention network||learns to||aggregate
evidence||distributed across||multiple sentences
end - to - end question - focused multi-factor attention network||identifies||important question words
important question words||to help||extract
end - to - end question - focused multi-factor attention network||for||document - based question answering
","aggregate||has||evidence
extract||has||answer
end - to - end question - focused multi-factor attention network||name||AMANDA
","Model||propose||end - to - end question - focused multi-factor attention network
",,,,,,
model,"Intuitively , AMANDA extracts the answer not only by synthesizing relevant facts from the passage but also by implicitly determining the suitable answer type during prediction .","AMANDA
extracts
answer
by synthesizing
relevant facts
from
passage
by implicitly determining
suitable answer type
during
prediction","AMANDA||extracts||answer
answer||by implicitly determining||suitable answer type
suitable answer type||during||prediction
answer||by synthesizing||relevant facts
relevant facts||from||passage
",,,"Model||has||AMANDA
",,,,,
experimental-setup,We tokenize the corpora with NLTK 2 .,"tokenize
corpora
with
NLTK","corpora||with||NLTK
",,"Experimental setup||tokenize||corpora
",,,,,,
experimental-setup,"We use the 300 dimension pre-trained word vectors from GloVe ( Pennington , Socher , and Manning 2014 ) and we do not update them during training .","use
300 dimension
pre-trained word vectors
from
GloVe","pre-trained word vectors||from||GloVe
","300 dimension||has||pre-trained word vectors
","Experimental setup||use||300 dimension
",,,,,,
experimental-setup,The out - of - vocabulary words are initialized with zero vectors .,"out - of - vocabulary words
initialized with
zero vectors","out - of - vocabulary words||initialized with||zero vectors
",,,"Experimental setup||has||out - of - vocabulary words
",,,,,
experimental-setup,We use 50 - dimension character - level embedding vectors .,"50 - dimension
character - level embedding vectors",,"50 - dimension||has||character - level embedding vectors
",,"Experimental setup||use||50 - dimension
",,,,,
experimental-setup,The number of hidden units in all the LSTMs is 150 .,"number of hidden units
in
all the LSTMs
is
150","number of hidden units||in||all the LSTMs
number of hidden units||is||150
",,,"Experimental setup||has||number of hidden units
",,,,,
experimental-setup,We use dropout ) with probability 0.3 for every learnable layer .,"dropout
with
probability 0.3
for
every learnable layer","dropout||with||probability 0.3
probability 0.3||for||every learnable layer
",,,"Experimental setup||use||dropout
",,,,,
experimental-setup,"For multi-factor attentive encoding , we choose 4 factors ( m ) based on our experimental findings ( refer to ) .","For
multi-factor attentive encoding
choose
4 factors","multi-factor attentive encoding||choose||4 factors
",,"Experimental setup||For||multi-factor attentive encoding
",,,,,,
experimental-setup,"During training , the minibatch size is fixed at 60 .","During
training
minibatch size
fixed at
60","minibatch size||fixed at||60
","training||has||minibatch size
","Experimental setup||During||training
",,,,,,
experimental-setup,We use the Adam optimizer with learning rate of 0.001 and clipnorm of 5 .,"Adam optimizer
with
learning rate
of
0.001
clipnorm
of
5","Adam optimizer||with||learning rate
learning rate||of||0.001
Adam optimizer||with||clipnorm
clipnorm||of||5
",,,"Experimental setup||use||Adam optimizer
",,,,,
results,shows that AMANDA outperforms all the stateof - the - art models by a significant margin on the New s QA dataset .,"shows that
AMANDA
outperforms
all the stateof - the - art models
by
significant margin
on
New s QA dataset","AMANDA||outperforms||all the stateof - the - art models
all the stateof - the - art models||by||significant margin
all the stateof - the - art models||on||New s QA dataset
",,"Results||shows that||AMANDA
",,,,,,
results,shows the results on the TriviaQA dataset .,"shows
results
on
TriviaQA dataset","results||on||TriviaQA dataset
",,"Results||shows||results
",,,,"TriviaQA dataset||shows||AMANDA
",,
results,shows that AMANDA achieves state - of the - art results in both Wikipedia and Web domain on distantly supervised and verified data .,"shows
AMANDA
achieves
state - of the - art results
in
Wikipedia and Web domain
on
distantly supervised and verified data","AMANDA||achieves||state - of the - art results
state - of the - art results||in||Wikipedia and Web domain
Wikipedia and Web domain||on||distantly supervised and verified data
",,,,,,,,
results,Results on the Search QA dataset are shown in .,"on
Search QA dataset",,,"Results||on||Search QA dataset
",,,,,,"Search QA dataset||has||AMANDA
"
results,"AMANDA outperforms both systems , especially for multi-word - answer questions by a huge margin .","AMANDA
outperforms
both systems
especially for
multi-word - answer questions
by
huge margin","AMANDA||outperforms||both systems
both systems||especially for||multi-word - answer questions
multi-word - answer questions||by||huge margin
",,,,,,,,
results,"shows that AMANDA performs better than any of the ablated models which include the ablation of multifactor attentive encoding , max - attentional question aggregation ( q ma ) , and question type representation ( q f ) .","performs
better
than
any of the ablated models
include
ablation
of
multifactor attentive encoding
max - attentional question aggregation ( q ma )
question type representation ( q f )","better||than||any of the ablated models
any of the ablated models||include||ablation
ablation||of||multifactor attentive encoding
ablation||of||max - attentional question aggregation ( q ma )
ablation||of||question type representation ( q f )
",,,,,"AMANDA||performs||better
",,,
research-problem,Sentence Embeddings in NLI with Iterative Refinement Encoders,Sentence Embeddings,,,,,"Contribution||has research problem||Sentence Embeddings
",,,,
research-problem,Sentence - level representations are necessary for various NLP tasks .,Sentence - level representations,,,,,"Contribution||has research problem||Sentence - level representations
",,,,
model,"With the goal of obtaining general - purpose sentence representations in mind , we opt for the sentence encoding approach .","opt for
sentence encoding approach",,,"Model||opt for||sentence encoding approach
",,,,,,
model,Motivated by the success of the InferSent architecture we extend their architecture with a hierarchylike structure of bidirectional LSTM ( BiLSTM ) layers with max pooling .,"of
InferSent architecture
extend
with
hierarchylike structure
bidirectional LSTM ( BiLSTM ) layers
with
max pooling","InferSent architecture||with||hierarchylike structure
hierarchylike structure||of||bidirectional LSTM ( BiLSTM ) layers
bidirectional LSTM ( BiLSTM ) layers||with||max pooling
",,"Model||extend||InferSent architecture
",,,,,,
experimental-setup,The architecture was implemented using PyTorch .,"architecture
implemented using
PyTorch","architecture||implemented using||PyTorch
",,,"Experimental setup||has||architecture
",,,,,
code,We have published our code in GitHub : https://github.com/Helsinki-NLP/HBMP.,,,,,,,,,,
experimental-setup,"For all of our models we used a gradient descent optimization algorithm based on the Adam update rule , which is pre-implemented in PyTorch .","used
gradient descent optimization algorithm
based on
Adam update rule
pre-implemented in
PyTorch","gradient descent optimization algorithm||based on||Adam update rule
gradient descent optimization algorithm||pre-implemented in||PyTorch
",,"Experimental setup||used||gradient descent optimization algorithm
",,,,,,
experimental-setup,We used a learning rate of 5e - 4 for all our models .,"learning rate
of
5e - 4","learning rate||of||5e - 4
",,,"Experimental setup||used||learning rate
",,,,,
experimental-setup,The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve .,"decreased by
factor
of
0.2
after
each epoch
if
model
not
improve","factor||of||0.2
factor||after||each epoch
each epoch||if||model
model||not||improve
",,,,,"learning rate||decreased by||factor
",,,
experimental-setup,We used a batch size of 64 .,"batch size
of
64","batch size||of||64
",,,"Experimental setup||used||batch size
",,,,,
experimental-setup,"We use pre-trained Glo Ve word embeddings of size 300 dimensions ( Glo Ve 840B 300D ; , which were fine - tuned during training .","use
pre-trained Glo Ve word embeddings
of
size
300 dimensions","pre-trained Glo Ve word embeddings||of||size
","size||has||300 dimensions
","Experimental setup||use||pre-trained Glo Ve word embeddings
",,,,,,
experimental-setup,"The sentence embeddings have hidden size of 600 for both direction ( except for SentEval test , where we test models with 600D and 1200D per direction ) and the 3 - layer multilayer perceptron ( MLP ) have the size of 600 dimensions .","sentence embeddings
have
hidden size
of
600
for
both direction
3 - layer multilayer perceptron ( MLP )
have
size
of
600 dimensions","3 - layer multilayer perceptron ( MLP )||have||size
size||of||600 dimensions
sentence embeddings||have||hidden size
hidden size||of||600
600||for||both direction
",,,"Experimental setup||has||3 - layer multilayer perceptron ( MLP )
Experimental setup||has||sentence embeddings
",,,,,
experimental-setup,We use a dropout of 0.1 between the MLP layers ( except just before the final layer ) .,"dropout
of
0.1
between
MLP layers","dropout||of||0.1
0.1||between||MLP layers
",,,"Experimental setup||use||dropout
",,,,,
experimental-setup,Our models were trained using one NVIDIA Tesla P100 GPU .,"trained using
one NVIDIA Tesla P100 GPU",,,"Experimental setup||trained using||one NVIDIA Tesla P100 GPU
",,,,,,
results,It clearly outperforms the similar but non-hierarchical BiLSTM models reported in the literature and fares well in comparison to other state of the art architectures in the sentence encoding category .,"clearly outperforms
non-hierarchical BiLSTM models
in
fares
well
in comparison to
other state of the art architectures
sentence encoding category","well||in comparison to||other state of the art architectures
other state of the art architectures||in||sentence encoding category
","clearly outperforms||has||non-hierarchical BiLSTM models
","Results||fares||well
","Results||has||clearly outperforms
",,,,,
results,"In particular , our results are close to the current state of the art on SNLI in this category and strong on both , the matched and mismatched test sets of MultiNLI .","close to
current state of the art
on
SNLI
strong on
matched and mismatched test sets
of
MultiNLI","current state of the art||on||SNLI
matched and mismatched test sets||of||MultiNLI
",,"Results||close to||current state of the art
Results||strong on||matched and mismatched test sets
",,,,,,
results,"Finally , on SciTail , we achieve the new state of the art with an accuracy of 86.0 % .","on
SciTail
achieve
new state of the art
with
accuracy
of
86.0 %","SciTail||achieve||new state of the art
new state of the art||with||accuracy
accuracy||of||86.0 %
",,"Results||on||SciTail
",,,,,,
results,"For the SNLI dataset , our model provides the test accuracy of 86.6 % after 4 epochs of training .","For
SNLI dataset
our model
provides
test accuracy
of
86.6 %
after
4 epochs
of
training","our model||provides||test accuracy
test accuracy||of||86.6 %
test accuracy||after||4 epochs
4 epochs||of||training
4 epochs||of||training
","SNLI dataset||has||our model
","Results||For||SNLI dataset
",,,,,,
results,"For the MultiNLI matched test set ( MultiNLI - m ) our model achieves a test accuracy of 73.7 % after 3 epochs of training , which is 0.8 % points lower than the state of the art 74.5 % by .","MultiNLI matched test set ( MultiNLI - m )
our model
achieves
test accuracy
of
73.7 %
after
3 epochs
of
training
which is
0.8 % points lower
than
state of the art 74.5 %","our model||achieves||test accuracy
test accuracy||of||73.7 %
73.7 %||after||3 epochs
3 epochs||of||training
73.7 %||which is||0.8 % points lower
0.8 % points lower||than||state of the art 74.5 %
our model||achieves||test accuracy
3 epochs||of||training
","MultiNLI matched test set ( MultiNLI - m )||has||our model
",,"Results||For||MultiNLI matched test set ( MultiNLI - m )
",,,,,
results,"For the mismatched test set ( MultiNLI - mm ) our model achieves a test accuracy of 73.0 % after 3 epochs of training , which is 0.6 % points lower than the state of the art 73.6 % by Chen , Zhu , Ling , Wei , Jiang , and Inkpen ( 2017 b ) .","mismatched test set ( MultiNLI - mm )
our model
achieves
test accuracy
of
73.0 %
after
3 epochs
of
training
which is
0.6 % points lower
than
state of the art 73.6 %","test accuracy||of||73.0 %
73.0 %||after||3 epochs
73.0 %||which is||0.6 % points lower
0.6 % points lower||than||state of the art 73.6 %
","mismatched test set ( MultiNLI - mm )||has||our model
",,"Results||For||mismatched test set ( MultiNLI - mm )
",,,,,
results,"On the SciTail dataset we compared our model also against non-sentence embedding - based models , as no results have been previously published which are based on independent sentence embeddings .","On
SciTail dataset
compared
our model
against
non-sentence embedding - based models","SciTail dataset||compared||our model
our model||against||non-sentence embedding - based models
",,"Results||On||SciTail dataset
",,,,,,
results,"We obtain a score of 86.0 % after 4 epochs of training , which is + 2.7 % points absolute improvement on the previous published state of the art by .","obtain
score
of
86.0 %
after
4 epochs
of
training
is
2.7 % points absolute improvement
on
previous published state of the art","score||of||86.0 %
86.0 %||is||2.7 % points absolute improvement
2.7 % points absolute improvement||on||previous published state of the art
86.0 %||after||4 epochs
",,,,,"SciTail dataset||obtain||score
",,,
results,Our model also outperforms In - fer Sent which achieves an accuracy of 85.1 % in our experiments .,"Our model
outperforms
In - fer Sent
which
achieves
accuracy
85.1 %","Our model||outperforms||In - fer Sent
In - fer Sent||which||achieves
achieves||accuracy||85.1 %
",,,,,,,"SciTail dataset||has||Our model
",
results,The results achieved by our proposed model are significantly higher than the previously published results .,"results
achieved by
proposed model
are
significantly higher
than
previously published results","results||achieved by||proposed model
proposed model||are||significantly higher
significantly higher||than||previously published results
",,,,,,,"SciTail dataset||has||results
",
research-problem,DiSAN : Directional Self - Attention Network for RNN / CNN - Free Language Understanding,RNN / CNN - Free Language Understanding,,,,,"Contribution||has research problem||RNN / CNN - Free Language Understanding
",,,,
model,"We propose a novel attention mechanism that differs from previous ones in that it is 1 ) multi-dimensional : the attention w.r.t. each pair of elements from the source ( s ) is a vector , where each entry is the attention computed on each feature ; and 2 ) directional : it uses one or multiple positional masks to model the asymmetric attention between two elements .","propose
novel attention mechanism
differs from
previous ones
in
multi-dimensional
directional","novel attention mechanism||differs from||previous ones
previous ones||in||multi-dimensional
previous ones||in||directional
",,"Model||propose||novel attention mechanism
",,,,,,
model,"We compute feature - wise attention since each element in a sequence is usually represented by a vector , e.g. , word / character embedding , and attention on different features can contain different information about dependency , thus to handle the variation of contexts around the same word .","compute
feature - wise attention
since
each element
in
sequence
represented by
vector","feature - wise attention||since||each element
each element||represented by||vector
each element||in||sequence
",,"Model||compute||feature - wise attention
",,,,,,
model,We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing .,"apply
positional masks
to
attention distribution
easily encode
prior structure knowledge
such as
temporal order
dependency parsing","positional masks||easily encode||prior structure knowledge
prior structure knowledge||such as||temporal order
prior structure knowledge||such as||dependency parsing
positional masks||to||attention distribution
",,"Model||apply||positional masks
",,,,,,
model,"We then build a light - weight and RNN / CNN - free neural network , "" Directional Self - Attention Network ( DiSAN ) "" , for sentence encoding .","build
light - weight and RNN / CNN - free neural network
Directional Self - Attention Network ( DiSAN )
for
sentence encoding","light - weight and RNN / CNN - free neural network||for||sentence encoding
","light - weight and RNN / CNN - free neural network||name||Directional Self - Attention Network ( DiSAN )
","Model||build||light - weight and RNN / CNN - free neural network
",,,,,,
model,"In DiSAN , the input sequence is processed by directional ( forward and backward ) self - attentions to model context dependency and produce context - aware representations for all tokens .","In
DiSAN
input sequence
processed by
directional ( forward and backward ) self - attentions
to model
context dependency
produce
context - aware representations
for
all tokens","input sequence||processed by||directional ( forward and backward ) self - attentions
directional ( forward and backward ) self - attentions||produce||context - aware representations
context - aware representations||for||all tokens
directional ( forward and backward ) self - attentions||to model||context dependency
","DiSAN||has||input sequence
","Model||In||DiSAN
",,,,,,
model,"Then , a multi-dimensional attention computes a vector representation of the entire sequence , which can be passed into a classification / regression module to compute the final prediction for a particular task .","multi-dimensional attention
computes
vector representation
of
entire sequence
passed into
classification / regression module
to compute
final prediction for a particular task","multi-dimensional attention||computes||vector representation
vector representation||passed into||classification / regression module
classification / regression module||to compute||final prediction for a particular task
vector representation||of||entire sequence
",,,,,,,"DiSAN||has||multi-dimensional attention
",
experimental-setup,We use cross-entropy loss plus L2 regularization penalty as optimization objective .,"use
cross-entropy loss
plus
L2 regularization penalty
as
optimization objective","optimization objective||use||cross-entropy loss
cross-entropy loss||plus||L2 regularization penalty
",,"Experimental setup||as||optimization objective
",,,,,,"cross-entropy loss||has||minimize
"
experimental-setup,We minimize it by Adadelta ) ( an optimizer of mini - batch SGD ) with batch size of 64 .,"minimize
by
Adadelta
of
with
batch size
64","minimize||with||batch size
batch size||of||64
minimize||by||Adadelta
",,,,,,,,
experimental-setup,Initial learning rate is set to 0.5 .,"Initial learning rate
set to
0.5","Initial learning rate||set to||0.5
",,,"Experimental setup||has||Initial learning rate
",,,,,
experimental-setup,"All weight matrices are initialized by Glorot Initialization , and the biases are initialized with 0 .","weight matrices
initialized by
Glorot Initialization
biases
initialized with
0","biases||initialized with||0
weight matrices||initialized by||Glorot Initialization
",,,"Experimental setup||has||biases
Experimental setup||has||weight matrices
",,,,,
experimental-setup,We initialize the word embedding in x by 300D Glo Ve 6B pre-trained vectors .,"initialize
word embedding
in
x
by
300D Glo Ve 6B pre-trained vectors","word embedding||in||x
word embedding||by||300D Glo Ve 6B pre-trained vectors
",,"Experimental setup||initialize||word embedding
",,,,,,
experimental-setup,"The Out - of - Vocabulary words in training set are randomly initialized by uniform distribution between ( ? 0.05 , 0.05 ) .","Out - of - Vocabulary words
in
training set
randomly initialized
by
uniform distribution
between
( ? 0.05 , 0.05 )","Out - of - Vocabulary words||in||training set
randomly initialized||by||uniform distribution
uniform distribution||between||( ? 0.05 , 0.05 )
","Out - of - Vocabulary words||has||randomly initialized
",,"Experimental setup||has||Out - of - Vocabulary words
",,,,,
experimental-setup,We use Dropout ) with keep probability 0.75 for language inference and 0.8 for sentiment analysis .,"use
Dropout
with
keep probability
0.75
for
language inference
0.8
for
sentiment analysis","Dropout||with||keep probability
0.75||for||language inference
0.8||for||sentiment analysis
","keep probability||has||0.75
keep probability||has||0.8
","Experimental setup||use||Dropout
",,,,,,
experimental-setup,"The L2 regularization decay factors ? are 5 10 ?5 and 10 ? 4 for language inference and sentiment analysis , respectively .","L2 regularization decay factors
are
5 10 ?5 and 10 ? 4
for
language inference and sentiment analysis","L2 regularization decay factors||are||5 10 ?5 and 10 ? 4
5 10 ?5 and 10 ? 4||for||language inference and sentiment analysis
",,,"Experimental setup||has||L2 regularization decay factors
",,,,,
experimental-setup,Hidden units number d h is set to 300 .,"Hidden units number
d h
set to
300","d h||set to||300
","Hidden units number||has||d h
",,"Experimental setup||has||Hidden units number
",,,,,
experimental-setup,"Activation functions ? ( ) are ELU ( exponential linear unit ) ( Clevert , Unterthiner , and Hochreiter 2016 ) if not specified .","Activation functions
are
ELU ( exponential linear unit )","Activation functions||are||ELU ( exponential linear unit )
",,,"Experimental setup||has||Activation functions
",,,,,
experimental-setup,All models are implemented with TensorFlow 2 and run on sin - 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3 gle Nvidia GTX 1080 Ti graphic card .,"implemented with
TensorFlow 2
run on
Nvidia GTX 1080 Ti graphic card",,,"Experimental setup||implemented with||TensorFlow 2
Experimental setup||run on||Nvidia GTX 1080 Ti graphic card
",,,,,,
results,Natural Language Inference,Natural Language Inference,,,,"Results||has||Natural Language Inference
",,,,,
results,"Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .","Compared to
results
from
official leaderboard of SNLI
DiSAN
outperforms
previous works
improves
best latest test accuracy
achieved by
memory - based NSE encoder network
by
remarkable margin
of
1.02 %","results||from||official leaderboard of SNLI
DiSAN||improves||best latest test accuracy
best latest test accuracy||achieved by||memory - based NSE encoder network
best latest test accuracy||by||remarkable margin
remarkable margin||of||1.02 %
DiSAN||outperforms||previous works
","official leaderboard of SNLI||has||DiSAN
",,,,"Natural Language Inference||Compared to||results
",,,
results,"DiSAN surpasses the RNN / CNN based models with more complicated architecture and more parameters by large margins , e.g. , + 2.32 % to Bi - LSTM , + 1.42 % to Bi - LSTM with additive attention .","DiSAN
surpasses
RNN / CNN based models
with
more complicated architecture
more parameters
by
large margins","DiSAN||surpasses||RNN / CNN based models
RNN / CNN based models||with||more complicated architecture
RNN / CNN based models||with||more parameters
RNN / CNN based models||by||large margins
",,,,,,,"Natural Language Inference||has||DiSAN
",
results,"It even outperforms models with the assistance of a semantic parsing tree , e.g. , + 3.52 % to Tree - based CNN , + 2.42 % to SPINN - PI .","outperforms
models
with
assistance
of
semantic parsing tree","models||with||assistance
assistance||of||semantic parsing tree
",,,,,"DiSAN||outperforms||models
",,,
results,"First , a comparison between the first two models shows that changing token - wise attention to multi-dimensional / feature - wise attention leads to 3.31 % improvement on a word embedding based model .","shows
changing
token - wise attention
to
multi-dimensional / feature - wise attention
leads to
3.31 % improvement
on
word embedding based model","token - wise attention||leads to||3.31 % improvement
3.31 % improvement||on||word embedding based model
token - wise attention||to||multi-dimensional / feature - wise attention
","changing||has||token - wise attention
",,,,"Natural Language Inference||shows||changing
",,,
results,"Also , a comparison between the third baseline and DiSAN shows that DiSAN can substantially outperform multi-head attention by 1.45 % .","comparison
between
third baseline and DiSAN
shows
DiSAN
substantially outperform
multi-head attention
by
1.45 %","comparison||between||third baseline and DiSAN
third baseline and DiSAN||shows||DiSAN
multi-head attention||by||1.45 %
","DiSAN||has||substantially outperform
substantially outperform||has||multi-head attention
",,,,,,"Natural Language Inference||has||comparison
",
results,"Moreover , a comparison between the forth baseline and DiSAN shows that the DiSA block can even outperform Bi - LSTM layer in context encoding , improving test accuracy by 0.64 % .","forth baseline and DiSAN
shows
DiSA block
outperform
Bi - LSTM layer
in
context encoding
improving
test accuracy
by
0.64 %","forth baseline and DiSAN||shows||DiSA block
outperform||improving||test accuracy
test accuracy||by||0.64 %
Bi - LSTM layer||in||context encoding
","DiSA block||has||outperform
outperform||has||Bi - LSTM layer
",,,,,,"comparison||between||forth baseline and DiSAN
",
results,A comparison between the fifth baseline and DiSAN shows that directional self - attention with forward and backward masks ( with temporal order encoded ) can bring 0.96 % improvement .,"fifth baseline and DiSAN
shows
directional self - attention
with
forward and backward masks ( with temporal order encoded )
bring
0.96 % improvement","fifth baseline and DiSAN||shows||directional self - attention
directional self - attention||with||forward and backward masks ( with temporal order encoded )
directional self - attention||bring||0.96 % improvement
",,,,,,,"comparison||between||fifth baseline and DiSAN
",
results,Sentiment Analysis,Sentiment Analysis,,,,"Results||has||Sentiment Analysis
",,,,,"Sentiment Analysis||has||DiSAN
"
results,"To the best of our knowledge , DiSAN improves the last best accuracy ( given by CNN - Tensor ) by 0.52 % .","DiSAN
improves
last best accuracy
given by
CNN - Tensor
by
0.52 %","DiSAN||improves||last best accuracy
last best accuracy||by||0.52 %
last best accuracy||given by||CNN - Tensor
",,,,,,,,
results,"Compared to tree - based models with heavy use of the prior structure , e.g. , MV - RNN , RNTN and Tree - LSTM , DiSAN outperforms them by 7.32 % , 6.02 % and 0.72 % , respectively .","Compared to
tree - based models
with
heavy use
of
prior structure
e.g.
MV - RNN
RNTN
Tree - LSTM
DiSAN
outperforms
by
7.32 % , 6.02 % and 0.72 %","tree - based models||with||heavy use
heavy use||of||prior structure
outperforms||by||7.32 % , 6.02 % and 0.72 %
tree - based models||e.g.||MV - RNN
tree - based models||e.g.||RNTN
tree - based models||e.g.||Tree - LSTM
","tree - based models||has||DiSAN
",,,,"Sentiment Analysis||Compared to||tree - based models
",,,
results,"Additionally , DiSAN achieves better performance than CNN - based models .","achieves
better performance
than
CNN - based models","better performance||than||CNN - based models
",,,,,"DiSAN||achieves||better performance
",,,
results,"Nonetheless , DiSAN still outperforms these fancy models , such as NCSL ( + 0.62 % ) and LR- Bi- LSTM ( + 1.12 % ) . :","outperforms
such as
NCSL
+ 0.62 %
LR- Bi- LSTM
+ 1.12 %","outperforms||such as||LR- Bi- LSTM
outperforms||such as||NCSL
","LR- Bi- LSTM||has||+ 1.12 %
NCSL||has||+ 0.62 %
",,,,,,"DiSAN||has||outperforms
DiSAN||has||outperforms
",
research-problem,"To answer the question in machine comprehension ( MC ) task , the models need to establish the interaction between the question and the context .",machine comprehension ( MC ),,,,,"Contribution||has research problem||machine comprehension ( MC )
",,,,
code,1 The latest results are listed at https://rajpurkar.github.io/SQuAD -explorer/,https://rajpurkar.github.io/SQuAD -explorer/,,,,,"Contribution||Code||https://rajpurkar.github.io/SQuAD -explorer/
",,,,
research-problem,Machine comprehension ( MC ) - especially in the form of question answering ( QA ) - is therefore attracting a significant amount of attention from the machine learning community .,question answering ( QA ),,,,,"Contribution||has research problem||question answering ( QA )
",,,,
model,"We propose an extension of BIDAF , called Ruminating Reader , which uses a second pass of reading and reasoning to allow it to learn to avoid mistakes and to ensure that it is able to effectively use the full context when selecting an answer .","propose
extension
of
BIDAF
called
Ruminating Reader
uses
second pass
of
reading and reasoning
to avoid
mistakes
to ensure
effectively use
full context
when selecting
answer","extension||called||Ruminating Reader
extension||of||BIDAF
extension||uses||second pass
second pass||of||reading and reasoning
reading and reasoning||to avoid||mistakes
reading and reasoning||to ensure||effectively use
full context||when selecting||answer
","effectively use||has||full context
","Model||propose||extension
",,,,,,
model,"In addition to adding a second pass , we also introduce two novel layer types , the ruminate layers , which use gating mechanisms to fuse the obtained from the first and second passes .","introduce
two novel layer types
ruminate layers
use
gating mechanisms
to fuse
first and second passes","two novel layer types||use||gating mechanisms
gating mechanisms||to fuse||first and second passes
","two novel layer types||name||ruminate layers
","Model||introduce||two novel layer types
",,,,,,
model,"In addition , we introduce an answer-question similarity loss to penalize overlap between question and predicted answer , a common feature in the errors of our base model .","answer-question similarity loss
to penalize
overlap
between
question and predicted answer","answer-question similarity loss||to penalize||overlap
overlap||between||question and predicted answer
",,,"Model||introduce||answer-question similarity loss
",,,,,
experimental-setup,"In the character encoding layer , we use 100 filters of width 5 .","In
character encoding layer
use
100 filters
of
width 5","character encoding layer||use||100 filters
100 filters||of||width 5
",,"Experimental setup||In||character encoding layer
",,,,,,
experimental-setup,"In the remainder of the model , we set the hidden layer dimension ( d ) to 100 .","set
hidden layer dimension ( d )
to
100","hidden layer dimension ( d )||to||100
",,"Experimental setup||set||hidden layer dimension ( d )
",,,,,,
experimental-setup,We use pretrained 100D Glo Ve vectors ( 6B - token version ) as word embeddings .,"use
pretrained 100D Glo Ve vectors ( 6B - token version )
as
word embeddings","pretrained 100D Glo Ve vectors ( 6B - token version )||as||word embeddings
",,"Experimental setup||use||pretrained 100D Glo Ve vectors ( 6B - token version )
",,,,,,
experimental-setup,"Out - of - vocobulary tokens are represented by an UNK symbol in the word embedding layer , but treated normally by the character embedding layer .","Out - of - vocobulary tokens
represented by
UNK symbol
in
word embedding layer
treated
normally
by
character embedding layer","Out - of - vocobulary tokens||represented by||UNK symbol
UNK symbol||in||word embedding layer
Out - of - vocobulary tokens||treated||normally
normally||by||character embedding layer
",,,"Experimental setup||has||Out - of - vocobulary tokens
",,,,,
experimental-setup,"We use the AdaDelta optimizer ( Zeiler , 2012 ) for optimization .","AdaDelta optimizer
for
optimization","AdaDelta optimizer||for||optimization
",,,"Experimental setup||use||AdaDelta optimizer
",,,,,
experimental-setup,We selected hyperparameter values through random search .,"selected
hyperparameter values
through
random search","hyperparameter values||through||random search
",,"Experimental setup||selected||hyperparameter values
",,,,,,
experimental-setup,Batch size is 30 .,"Batch size
is
30","Batch size||is||30
",,,"Experimental setup||has||Batch size
",,,,,
experimental-setup,"Learning rate starts at 0.5 , and decreases to 0.2 once the model stops improving .","Learning rate
starts at
0.5
decreases to
0.2
model
stops improving","Learning rate||starts at||0.5
Learning rate||decreases to||0.2
","0.2||has||stops improving
stops improving||has||model
",,"Experimental setup||has||Learning rate
",,,,,
experimental-setup,"The L2-regularization weight is 1 e - 4 , AQSL weight is 1 and dropout with a drop rate of 0.2 is A typical model run converges in about 40 k steps .","L2-regularization weight
is
1 e - 4
AQSL weight
is
1
dropout
with
drop rate
of
0.2
typical model run
converges
in about
40 k steps","converges||in about||40 k steps
L2-regularization weight||is||1 e - 4
dropout||with||drop rate
drop rate||of||0.2
AQSL weight||is||1
","typical model run||has||converges
",,"Experimental setup||has||typical model run
Experimental setup||has||L2-regularization weight
Experimental setup||has||dropout
Experimental setup||has||AQSL weight
",,,,,
experimental-setup,This takes two days using Tensorflow and a single NVIDIA K80 GPU . provide an official evaluation script that allows us to measure F 1 score and EM score by comparing the prediction and ground truth answers .,"using
Tensorflow
single NVIDIA K80 GPU",,,"Experimental setup||using||Tensorflow
Experimental setup||using||single NVIDIA K80 GPU
",,,,,,
results,"At the time of submission , our model is tied in accuracy on the hidden test set with the bestperforming published single model .","model
is
tied
in
accuracy
on
hidden test set
with
bestperforming published single model","model||is||tied
tied||with||bestperforming published single model
tied||in||accuracy
tied||on||hidden test set
",,,"Results||has||model
",,,,,
results,We achieve an F 1 score of 79.5 and EM score of 70.6 .,"achieve
F 1 score
of
79.5
EM score
of
70.6","F 1 score||of||79.5
EM score||of||70.6
",,"Results||achieve||F 1 score
Results||achieve||EM score
",,,,,,
ablation-analysis,Experiments 3 and 4 show that the two ruminate layers are both important and helpful in contributing performance .,"show
two ruminate layers
are
important and helpful
in contributing
performance","two ruminate layers||are||important and helpful
important and helpful||in contributing||performance
",,"Ablation analysis||show||two ruminate layers
",,,,,,
ablation-analysis,It is worth noting that the BiLSTM in the context ruminate layer contributes substantially to model performance .,"worth noting
BiLSTM
in
context ruminate layer
contributes
substantially
to
model performance","BiLSTM||in||context ruminate layer
BiLSTM||contributes||substantially
substantially||to||model performance
",,"Ablation analysis||worth noting||BiLSTM
",,,,,,
research-problem,FUSIONNET : FUSING VIA FULLY - AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION,MACHINE COMPREHENSION,,,,,"Contribution||has research problem||MACHINE COMPREHENSION
",,,,
research-problem,"Teaching machines to read , process and comprehend text and then answer questions is one of key problems in artificial intelligence .","Teaching machines to read , process and comprehend text and then answer questions",,,,,"Contribution||has research problem||Teaching machines to read , process and comprehend text and then answer questions
",,,,
research-problem,Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension ( MRC ) task .,machine reading comprehension ( MRC ),,,,,"Contribution||has research problem||machine reading comprehension ( MRC )
",,,,
research-problem,We argue that this hypothesis also holds in language understanding and MRC .,"language understanding
MRC",,,,,"Contribution||has research problem||language understanding
Contribution||has research problem||MRC
",,,,
model,"To alleviate this challenge , we identify an attention scoring function utilizing all layers of representation with less training burden .","identify
attention scoring function
utilizing
all layers
of
representation
with
less training burden","attention scoring function||utilizing||all layers
all layers||with||less training burden
all layers||of||representation
",,"Model||identify||attention scoring function
",,,,,,
model,This leads to an attention that thoroughly captures the complete information between the question and the context .,"leads to
attention
thoroughly captures
complete information
between
question and the context","attention||thoroughly captures||complete information
complete information||between||question and the context
",,"Model||leads to||attention
",,,,,,
model,"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it layer by layer on the context side .","With
fully - aware attention
put forward
multi -level attention mechanism
to understand
information
in
question
exploit it
layer by layer
on
context side","fully - aware attention||put forward||multi -level attention mechanism
multi -level attention mechanism||to understand||information
information||in||question
information||exploit it||layer by layer
layer by layer||on||context side
",,"Model||With||fully - aware attention
",,,,,,
model,"All of these innovations are integrated into a new end - to - end structure called FusionNet in , with details described in Section 3 .","innovations
integrated into
new end - to - end structure
called
FusionNet","innovations||integrated into||new end - to - end structure
new end - to - end structure||called||FusionNet
",,,"Model||has||innovations
",,,,,
results,"From the results , we can see that our models not only perform well on the original SQuAD dataset , but also outperform all previous models by more than 5 % in EM score on the adversarial datasets .","see that
our models
not only perform
well
on
original SQuAD dataset
also
outperform
all previous models
by
more than 5 %
in
EM score
on
adversarial datasets","our models||not only perform||well
well||on||original SQuAD dataset
our models||also||outperform
all previous models||by||more than 5 %
more than 5 %||in||EM score
more than 5 %||on||adversarial datasets
","outperform||has||all previous models
","Results||see that||our models
",,,,,,
results,This shows that FusionNet is better at language understanding of both the context and question .,"shows
FusionNet
is
better
at
language understanding
of both
context and question","FusionNet||is||better
better||at||language understanding
language understanding||of both||context and question
",,"Results||shows||FusionNet
",,,,,,
research-problem,Stochastic Answer Networks for Machine Reading Comprehension,Machine Reading Comprehension,,,,,"Contribution||has research problem||Machine Reading Comprehension
",,,,
research-problem,Machine reading comprehension ( MRC ) is a challenging task : the goal is to have machines read a text passage and then answer any question about the passage .,Machine reading comprehension ( MRC ),,,,,"Contribution||has research problem||Machine reading comprehension ( MRC )
",,,,
research-problem,It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning .,MRC,,,,,"Contribution||has research problem||MRC
",,,,
model,"In this work , we derive an alternative multi-step reasoning neural network for MRC .","derive
alternative multi-step reasoning neural network
for
MRC","alternative multi-step reasoning neural network||for||MRC
",,"Model||derive||alternative multi-step reasoning neural network
",,,,,,
model,"During training , we fix the number of reasoning steps , but perform stochastic dropout on the answer module ( final layer predictions ) .","During
training
fix
number of reasoning steps
perform
stochastic dropout
on
answer module ( final layer predictions )","training||fix||number of reasoning steps
training||perform||stochastic dropout
stochastic dropout||on||answer module ( final layer predictions )
",,"Model||During||training
",,,,,,
model,"During decoding , we generate answers based on the average of predictions in all steps , rather than the final step .","decoding
generate
answers
based on
average
of
predictions
in
all steps
rather than
final step","decoding||generate||answers
answers||based on||average
average||of||predictions
predictions||in||all steps
all steps||rather than||final step
",,,"Model||During||decoding
",,,,,
model,"We call this a stochastic answer network ( SAN ) because the stochastic dropout is applied to the answer module ; albeit simple , this technique significantly improves the robustness and over all accuracy of the model .","call
stochastic answer network ( SAN )",,,"Model||call||stochastic answer network ( SAN )
",,,,,,
experimental-setup,"The spaCy tool 2 is used to tokenize the both passages and questions , and generate lemma , part - of - speech and named entity tags .","spaCy tool
used to
tokenize
both
passages and questions
generate
lemma
part - of - speech
named entity tags","spaCy tool||used to||generate
spaCy tool||used to||tokenize
tokenize||both||passages and questions
","generate||has||lemma
generate||has||part - of - speech
generate||has||named entity tags
",,"Experimental setup||has||spaCy tool
",,,,,
experimental-setup,We use 2 - layer BiLSTM with d = 128 hidden units for both passage and question encoding .,"use
2 - layer BiLSTM
with
d = 128 hidden units
for
both passage and question encoding","2 - layer BiLSTM||with||d = 128 hidden units
d = 128 hidden units||for||both passage and question encoding
",,"Experimental setup||use||2 - layer BiLSTM
",,,,,,
experimental-setup,The mini-batch size is set to 32 and Adamax is used as our optimizer .,"mini-batch size
set to
32
Adamax
used as
our optimizer","mini-batch size||set to||32
Adamax||used as||our optimizer
",,,"Experimental setup||has||mini-batch size
Experimental setup||has||Adamax
",,,,,
experimental-setup,The learning rate is set to 0.002 at first and decreased by half after every 10 epochs .,"learning rate
set to
0.002
at
first
decreased
by
half
after
every 10 epochs","learning rate||set to||0.002
0.002||at||first
decreased||by||half
half||after||every 10 epochs
","0.002||has||decreased
",,"Experimental setup||has||learning rate
",,,,,
experimental-setup,"We set the dropout rate for all the hidden units of LSTM , and the answer module output layer to 0.4 .","set
dropout rate
for
all the hidden units
of
LSTM
answer module output layer
to
0.4","dropout rate||to||0.4
0.4||for||all the hidden units
all the hidden units||of||LSTM
all the hidden units||of||answer module output layer
",,"Experimental setup||set||dropout rate
",,,,,,
experimental-setup,"To prevent degenerate output , we ensure that at least one step in the answer module is active during training .","To prevent
degenerate output
ensure that
at least one step
in
answer module
is
active
during
training","degenerate output||ensure that||at least one step
at least one step||in||answer module
at least one step||is||active
active||during||training
",,"Experimental setup||To prevent||degenerate output
",,,,,,
results,"We observe that SAN achieves 76.235 EM and 84.056 F1 , outperforming all other models .","observe
SAN
achieves
76.235 EM
84.056 F1
outperforming
all other models","SAN||achieves||76.235 EM
SAN||achieves||84.056 F1
","SAN||has||outperforming
outperforming||has||all other models
","Results||observe||SAN
",,,,,,
results,Standard 1 - step model only achieves 75.139 EM and dynamic steps ( via ReasoNet ) achieves only 75.355 EM .,"Standard 1 - step model
achieves
75.139 EM
dynamic steps ( via ReasoNet )
achieves
only 75.355 EM","Standard 1 - step model||achieves||75.139 EM
dynamic steps ( via ReasoNet )||achieves||only 75.355 EM
",,,"Results||has||Standard 1 - step model
Results||has||dynamic steps ( via ReasoNet )
",,,,,
results,"SAN also outperforms a 5 - step memory net with averaging , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .","SAN
outperforms
5 - step memory net
with
averaging","5 - step memory net||with||averaging
","SAN||has||outperforms
outperforms||has||5 - step memory net
",,"Results||has||SAN
",,,,,
results,SAN also outperforms the other models in terms of K- best oracle scores .,"other
models
in terms of
K- best oracle scores","models||in terms of||K- best oracle scores
",,,,,"outperforms||other||models
",,,
results,We see that SAN is very competitive in both single and ensemble settings ( ranked in second ) despite its simplicity .,"see that
SAN
is
very competitive
in
both single and ensemble settings
ranked
second","SAN||is||very competitive
very competitive||in||both single and ensemble settings
very competitive||ranked||second
",,"Results||see that||SAN
",,,,,,
research-problem,Contextualized Word Representations for Reading Comprehension,Reading Comprehension,,,,,"Contribution||has research problem||Reading Comprehension
",,,,
research-problem,Reading comprehension ( RC ) is a high - level task in natural language understanding that requires reading a document and answering questions about its content .,Reading comprehension ( RC ),,,,,"Contribution||has research problem||Reading comprehension ( RC )
",,,,
research-problem,"RC has attracted substantial attention over the last few years with the advent of large annotated datasets , computing resources , and neural network models and optimization procedures .",RC,,,,,"Contribution||has research problem||RC
",,,,
model,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .","take
model
carries out
only basic question - document interaction
prepend to
module
that produces
token embeddings
by
explicitly gating
between
contextual and non-contextual representations","model||carries out||only basic question - document interaction
only basic question - document interaction||prepend to||module
module||that produces||token embeddings
token embeddings||by||explicitly gating
explicitly gating||between||contextual and non-contextual representations
",,"Model||take||model
",,,,,,
model,"Motivated by these findings , we turn to a semisupervised setting in which we leverage a language model , pre-trained on large amounts of data , as a sequence encoder which forcibly facilitates context utilization .","turn to
semisupervised setting
leverage
language model
pre-trained on
large amounts of data
as
sequence encoder
forcibly facilitates
context utilization","semisupervised setting||leverage||language model
language model||pre-trained on||large amounts of data
large amounts of data||as||sequence encoder
sequence encoder||forcibly facilitates||context utilization
",,"Model||turn to||semisupervised setting
",,,,,,
results,"In we compare these two variants over the development set and observe superior performance by the contextual one , illustrating the benefit of contextualization and specifically per-sequence contextualization which is done separately for the question and for the passage .","observe
superior performance
by
contextual one
illustrating
benefit
of
contextualization","superior performance||illustrating||benefit
benefit||of||contextualization
superior performance||by||contextual one
",,"Results||observe||superior performance
",,,,,,
results,"On average , the less frequent a word - type is , the smaller are its gate activations , i.e. , the reembedded representation of a rare word places less weight on its fixed word - embedding and more on its contextual representation , compared to a common word .","less frequent
word - type
is
smaller
are
gate activations","less frequent||is||word - type
gate activations||are||smaller
","word - type||has||gate activations
",,"Results||has||less frequent
",,,,,
results,Supplementing the calculation of token reembeddings with the hidden states of a strong language model proves to be highly effective .,"Supplementing
calculation
of
token reembeddings
with
hidden states
of
strong language model
proves to be
highly effective","calculation||of||token reembeddings
token reembeddings||with||hidden states
hidden states||of||strong language model
token reembeddings||proves to be||highly effective
",,"Results||Supplementing||calculation
",,,,,,
results,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a QA model in a semisupervised fashion with a large language model .","observe
significant improvement
with
showing
benefit
of
training
QA model
in
semisupervised fashion
large language model","benefit||of||training
training||in||semisupervised fashion
semisupervised fashion||with||large language model
training||observe||significant improvement
","training||has||QA model
","Results||showing||benefit
",,,,,,
results,"Besides a crosscutting boost in results , we note that the performance due to utilizing the LM hidden states of the first LSTM layer significantly surpasses the other two variants .","performance
due to
utilizing
LM hidden states
of
first LSTM layer
significantly surpasses
other two variants","performance||due to||utilizing
LM hidden states||of||first LSTM layer
","utilizing||has||LM hidden states
LM hidden states||has||significantly surpasses
significantly surpasses||has||other two variants
",,"Results||has||performance
",,,,,
hyperparameters,We use pre-trained GloVe embeddings of dimension d w = 300 and produce character - based word representations via dc = 100 convolutional filters over character embeddings as in .,"use
pre-trained GloVe embeddings
of dimension d
w = 300
produce
character - based word representations
via
dc = 100
convolutional filters
over
character embeddings","pre-trained GloVe embeddings||of dimension d||w = 300
character - based word representations||via||dc = 100
convolutional filters||over||character embeddings
","dc = 100||has||convolutional filters
","Hyperparameters||use||pre-trained GloVe embeddings
Hyperparameters||produce||character - based word representations
",,,,,,
research-problem,"The latest work on language representations carefully integrates contextualized features into language model training , which enables a series of success especially in various machine reading comprehension and natural language inference tasks .",language representations,,,,,"Contribution||has research problem||language representations
",,,,
research-problem,"Recently , deep contextual language model ( LM ) has been shown effective for learning universal language representations , achieving state - of - the - art results in a series of flagship natural language understanding ( NLU ) tasks .",learning universal language representations,,,,,"Contribution||has research problem||learning universal language representations
",,,,
model,"Thus we are motivated to enrich the sentence contextual semantics in multiple predicate - specific argument sequences by presenting SemBERT : Semantics - aware BERT , which is a fine - tuned BERT with explicit contextual semantic clues .","enrich
sentence contextual semantics
in
multiple predicate - specific argument sequences
by presenting
SemBERT : Semantics - aware BERT
is
fine - tuned BERT
with
explicit contextual semantic clues","sentence contextual semantics||in||multiple predicate - specific argument sequences
multiple predicate - specific argument sequences||by presenting||SemBERT : Semantics - aware BERT
SemBERT : Semantics - aware BERT||is||fine - tuned BERT
fine - tuned BERT||with||explicit contextual semantic clues
",,"Model||enrich||sentence contextual semantics
",,,,,,
model,The proposed SemBERT learns the representation in a fine - grained manner and takes both strengths of BERT on plain context representation and explicit semantics for deeper meaning representation .,"proposed SemBERT
learns
representation
in
fine - grained manner
takes
strengths
of
BERT
on
plain context representation
explicit semantics
for
deeper meaning representation","proposed SemBERT||learns||representation
representation||in||fine - grained manner
proposed SemBERT||takes||strengths
strengths||of||BERT
BERT||on||plain context representation
proposed SemBERT||takes||explicit semantics
explicit semantics||for||deeper meaning representation
",,,"Model||has||proposed SemBERT
",,,,,
model,Our model consists of three components :,"consists of
three components",,,"Model||consists of||three components
",,,,,,"three components||has||sequence encoder
three components||has||an out - ofshelf semantic role labeler
three components||has||semantic integration component
"
model,1 ) an out - ofshelf semantic role labeler to annotate the input sentences with a variety of semantic role labels ; 2 ) an sequence encoder where a pre-trained language model is used to build representation for input raw texts and the semantic role labels are mapped to embedding in parallel ; 3 ) a semantic integration component to integrate the text representation with the contextual explicit semantic embedding to obtain the joint representation for downstream tasks .,"an out - ofshelf semantic role labeler
to annotate
input sentences
with
variety of semantic role labels
sequence encoder
where
pre-trained language model
used to build
representation
for input
raw texts
semantic role labels
mapped to
embedding
in
parallel
semantic integration component
to integrate
text representation
with
contextual explicit semantic embedding
to obtain
joint representation
for
downstream tasks","sequence encoder||where||pre-trained language model
pre-trained language model||used to build||representation
representation||for input||raw texts
sequence encoder||where||semantic role labels
semantic role labels||mapped to||embedding
embedding||in||parallel
an out - ofshelf semantic role labeler||to annotate||input sentences
input sentences||with||variety of semantic role labels
semantic integration component||to integrate||text representation
text representation||with||contextual explicit semantic embedding
contextual explicit semantic embedding||to obtain||joint representation
joint representation||for||downstream tasks
",,,,,,,,
experimental-setup,Our implementation is based on the PyTorch implementation of BERT 6 .,"Our implementation
based on
PyTorch implementation
of
BERT","Our implementation||based on||PyTorch implementation
PyTorch implementation||of||BERT
",,,"Experimental setup||has||Our implementation
",,,,,
experimental-setup,"We use the pre-trained weights of BERT and follow the same fine - tuning procedure as BERT without any modification , and all the layers are tuned with moderate model size increasing , as the extra SRL embedding volume is less than 15 % of the original encoder size .","use
pre-trained weights
of
BERT
follow
same fine - tuning procedure
as
BERT
all the layers
tuned with
moderate model size
increasing
as
extra SRL embedding volume
is
less than 15 %
of
original encoder size","pre-trained weights||of||BERT
same fine - tuning procedure||as||BERT
all the layers||tuned with||moderate model size
moderate model size||as||extra SRL embedding volume
extra SRL embedding volume||is||less than 15 %
less than 15 %||of||original encoder size
","BERT||has||all the layers
moderate model size||has||increasing
","Experimental setup||use||pre-trained weights
Experimental setup||follow||same fine - tuning procedure
",,,,,,
experimental-setup,"We set the initial learning rate in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .","set
initial learning rate
in
{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }
with
warm - up rate
of
0.1
L2 weight decay
of
0.01","initial learning rate||with||warm - up rate
warm - up rate||of||0.1
initial learning rate||with||L2 weight decay
L2 weight decay||of||0.01
initial learning rate||in||{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }
",,"Experimental setup||set||initial learning rate
",,,,,,
experimental-setup,"The batch size is selected in { 16 , 24 , 32 } .","batch size
selected in
{ 16 , 24 , 32 }","batch size||selected in||{ 16 , 24 , 32 }
",,,"Experimental setup||has||batch size
",,,,,
experimental-setup,"The maximum number of epochs is set in [ 2 , 5 ] depending on tasks .","maximum number of epochs
set in
[ 2 , 5 ]
depending on
tasks","maximum number of epochs||set in||[ 2 , 5 ]
maximum number of epochs||depending on||tasks
",,,"Experimental setup||has||maximum number of epochs
",,,,,
experimental-setup,"Texts are tokenized using wordpieces , with maximum length of 384 for SQuAD and 128 or 200 for other tasks .","Texts
tokenized using
wordpieces
with
maximum length
of
384
for
SQuAD
128 or 200
for
other tasks","Texts||with||maximum length
maximum length||of||128 or 200
128 or 200||for||other tasks
maximum length||of||384
384||for||SQuAD
Texts||tokenized using||wordpieces
",,,"Experimental setup||has||Texts
",,,,,
experimental-setup,The dimension of SRL embedding is set to 10 .,"dimension
of
SRL embedding
set to
10","dimension||of||SRL embedding
SRL embedding||set to||10
",,,"Experimental setup||has||dimension
",,,,,
experimental-setup,The default maximum number of predicateargument structures m is set to 3 .,"default maximum number
of
predicateargument structures m
set to
3","default maximum number||of||predicateargument structures m
predicateargument structures m||set to||3
",,,"Experimental setup||has||default maximum number
",,,,,
ablation-analysis,"From the results , we observe that the concatenation would yield an improvement , verifying that integrating contextual semantics would be quite useful for language understanding .","observe that
concatenation
yield
improvement
verifying that
integrating
contextual semantics
be
quite useful
for
language understanding","concatenation||yield||improvement
improvement||verifying that||integrating
integrating||be||quite useful
quite useful||for||language understanding
","integrating||has||contextual semantics
","Ablation analysis||observe that||concatenation
",,,,,,
ablation-analysis,"However , SemBERT still outperforms the simple BERT + SRL model just like the latter outperforms the original BERT by a large performance margin , which shows that SemBERT works more effectively for integrating both plain contextual representation and contextual semantics at the same time .","SemBERT
outperforms
simple BERT + SRL model
shows that
SemBERT
works
more effectively
for
integrating
both
plain contextual representation
contextual semantics","SemBERT||shows that||SemBERT
SemBERT||works||more effectively
more effectively||for||integrating
integrating||both||plain contextual representation
integrating||both||contextual semantics
","SemBERT||has||outperforms
outperforms||has||simple BERT + SRL model
",,"Ablation analysis||has||SemBERT
",,,,,
research-problem,COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF - ATTENTION FOR READING COMPRE - HENSION,READING COMPRE - HENSION,,,,,"Contribution||has research problem||READING COMPRE - HENSION
",,,,
research-problem,Current end - to - end machine reading and question answering ( Q&A ) models are primarily based on recurrent neural networks ( RNNs ) with attention .,machine reading and question answering ( Q&A ),,,,,"Contribution||has research problem||machine reading and question answering ( Q&A )
",,,,
research-problem,There is growing interest in the tasks of machine reading comprehension and automated question answering .,"machine reading comprehension
automated question answering",,,,,"Contribution||has research problem||machine reading comprehension
Contribution||has research problem||automated question answering
",,,,
research-problem,"In this paper , aiming to make the machine comprehension fast , we propose to remove the recurrent nature of these models .","to make
machine comprehension
fast
remove
recurrent nature of these models","fast||remove||recurrent nature of these models
","machine comprehension||has||fast
","Model||to make||machine comprehension
",,"Contribution||has research problem||machine comprehension
",,,,
model,We instead exclusively use convolutions and self - attentions as the building blocks of encoders that separately encodes the query and context .,"use
convolutions and self - attentions
as
building blocks
of
encoders
separately encodes
query and context","convolutions and self - attentions||as||building blocks
building blocks||separately encodes||query and context
building blocks||of||encoders
",,"Model||use||convolutions and self - attentions
",,,,,,
model,Then we learn the interactions between context and question by standard attentions .,"learn
interactions
between
context and question
by
standard attentions","interactions||by||standard attentions
interactions||between||context and question
",,"Model||learn||interactions
",,,,,,
model,The resulting representation is encoded again with our recurrency - free encoder before finally decoding to the probability of each position being the start or end of the answer span .,"resulting representation
is
encoded again
with
our recurrency - free encoder
before
decoding
to
probability
of
each position
being
start or end
of
answer span","resulting representation||is||encoded again
encoded again||with||our recurrency - free encoder
encoded again||before||decoding
decoding||to||probability
probability||of||each position
each position||being||start or end
start or end||of||answer span
",,,"Model||has||resulting representation
",,,,,
model,"We call this architecture QANet , which is shown in .","call
architecture QANet",,,"Model||call||architecture QANet
",,,,,,
experiments,EXPERIMENTS ON SQUAD,"ON
SQUAD",,,"Experiments||ON||SQUAD
",,,,,,"SQUAD||has||Experimental setup
"
experiments,We employ two types of standard regularizations .,"employ
two types
of
standard regularizations","two types||of||standard regularizations
",,,,,"Experimental setup||employ||two types
",,,
experiments,"First , we use L2 weight decay on all the trainable variables , with parameter ? = 3 10 ?7 .","use
L2 weight decay
on
all the trainable variables
with parameter
? = 3 10 ?7","L2 weight decay||with parameter||? = 3 10 ?7
L2 weight decay||on||all the trainable variables
",,,,,"Experimental setup||use||L2 weight decay
",,,
experiments,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .","dropout
on
word
character embeddings
between layers
where
word and character dropout rates
are
0.1 and 0.05
dropout rate
between
every two layers
is
0.1","dropout||where||dropout rate
dropout rate||between||every two layers
every two layers||is||0.1
dropout||where||word and character dropout rates
word and character dropout rates||are||0.1 and 0.05
dropout||on||word
dropout||on||character embeddings
dropout||on||between layers
",,,,,,,"Experimental setup||use||dropout
",
experiments,"We also adopt the stochastic depth method ( layer dropout ) within each embedding or model encoder layer , where sublayer l has survival probability pl = 1 ? l L ( 1 ? p L ) where L is the last layer and p L = 0.9 .","adopt
stochastic depth method ( layer dropout )
within
each embedding or model encoder layer
where
sublayer l
survival probability pl = 1 ? l L ( 1 ? p L )
where
L
is
last layer
p L
=
0.9","stochastic depth method ( layer dropout )||within||each embedding or model encoder layer
each embedding or model encoder layer||where||sublayer l
survival probability pl = 1 ? l L ( 1 ? p L )||where||L
L||is||last layer
survival probability pl = 1 ? l L ( 1 ? p L )||where||p L
p L||=||0.9
","sublayer l||has||survival probability pl = 1 ? l L ( 1 ? p L )
",,,,"Experimental setup||adopt||stochastic depth method ( layer dropout )
",,,
experiments,"The hidden size and the convolution filter number are all 128 , the batch size is 32 , training steps are 150 K for original data , 250 K for "" data augmentation 2 "" , and 340 K for "" data augmentation 3 "" .","hidden size and the convolution filter number
are
128
batch size
is
32
training steps
are
150 K
for
original data
250 K
for
data augmentation 2
340 K
for
data augmentation 3","batch size||is||32
hidden size and the convolution filter number||are||128
training steps||are||250 K
250 K||for||data augmentation 2
training steps||are||150 K
150 K||for||original data
training steps||are||340 K
340 K||for||data augmentation 3
",,,,,,,"Experimental setup||has||batch size
Experimental setup||has||hidden size and the convolution filter number
Experimental setup||has||training steps
",
experiments,"The numbers of convolution layers in the embedding and modeling encoder are 4 and 2 , kernel sizes are 7 and 5 , and the block numbers for the encoders are 1 and 7 , respectively .","numbers of convolution layers
in
embedding and modeling encoder
are
4 and 2
kernel sizes
are
7 and 5
block numbers
for
encoders
are
1 and 7","kernel sizes||are||7 and 5
numbers of convolution layers||in||embedding and modeling encoder
embedding and modeling encoder||are||4 and 2
block numbers||for||encoders
encoders||are||1 and 7
",,,,,,,"Experimental setup||has||kernel sizes
Experimental setup||has||numbers of convolution layers
Experimental setup||has||block numbers
",
experiments,"We use the ADAM optimizer ( Kingma & Ba , 2014 ) with ? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7 .","ADAM optimizer
with
? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7","ADAM optimizer||with||? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7
",,,,,,,"Experimental setup||use||ADAM optimizer
",
experiments,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .","learning rate warm - up scheme
with
inverse exponential
increase
from
0.0
to
0.001
in
first 1000 steps
then
maintain
constant learning rate
for
remainder of training","learning rate warm - up scheme||with||inverse exponential
increase||in||first 1000 steps
increase||from||0.0
0.0||to||0.001
increase||then||maintain
constant learning rate||for||remainder of training
","inverse exponential||has||increase
maintain||has||constant learning rate
",,,,,,"Experimental setup||use||learning rate warm - up scheme
",
experiments,Exponential moving average is applied on all trainable variables with a decay rate 0.9999 .,"Exponential moving average
applied on
all trainable variables
with
decay rate 0.9999","Exponential moving average||applied on||all trainable variables
all trainable variables||with||decay rate 0.9999
",,,,,,,"Experimental setup||has||Exponential moving average
",
experiments,"Finally , we implement our model in Python using Tensorflow and carry out our experiments on an NVIDIA p 100 GPU .","implement
our model
in
Python
using
Tensorflow
carry out
our experiments
on
NVIDIA p 100 GPU","our experiments||on||NVIDIA p 100 GPU
our model||using||Tensorflow
our model||in||Python
",,,,,"Experimental setup||carry out||our experiments
Experimental setup||implement||our model
",,,
experiments,"As can be seen from the table , the accuracy ( EM / F1 ) performance of our model is on par with the state - of - the - art models .","accuracy ( EM / F1 ) performance
of
our model
is
on par
with
state - of - the - art models","accuracy ( EM / F1 ) performance||of||our model
accuracy ( EM / F1 ) performance||is||on par
on par||with||state - of - the - art models
",,,,,,,"Results||has||accuracy ( EM / F1 ) performance
",
experiments,"In particular , our model trained on the original dataset outperforms all the documented results in the literature , in terms of both EM and F1 scores ( see second column of ) .","our model
trained on
original dataset
outperforms
all the documented results in the literature
in terms of
EM and F1 scores","our model||trained on||original dataset
outperforms||in terms of||EM and F1 scores
","our model||has||outperforms
outperforms||has||all the documented results in the literature
",,,,,,"Results||has||our model
",
experiments,"When trained with the augmented data with proper sampling scheme , our model can get significant gain 1.5/1.1 on EM / F1 .","trained with
augmented data
with
proper sampling scheme
our model
can get
significant gain 1.5/1.1
on
EM / F1","augmented data||with||proper sampling scheme
our model||can get||significant gain 1.5/1.1
significant gain 1.5/1.1||on||EM / F1
","augmented data||has||our model
",,,,"Results||trained with||augmented data
",,,
experiments,"Finally , our result on the official test set is 76.2/84.6 , which significantly outperforms the best documented result 73.2/81.8 .","on
official test set
is
76.2/84.6
which
significantly outperforms
best documented result 73.2/81.8","official test set||is||76.2/84.6
76.2/84.6||which||significantly outperforms
","significantly outperforms||has||best documented result 73.2/81.8
",,,,"Results||on||official test set
",,,
ablation-analysis,"As can be seen from the table , the use of convolutions in the encoders is crucial : both F1 and EM drop drastically by almost 3 percent if it is removed .","use of
convolutions
in
encoders
is
crucial
both F1 and EM
drop drastically
by
almost 3 percent
if
removed","convolutions||in||encoders
convolutions||is||crucial
drop drastically||by||almost 3 percent
almost 3 percent||if||removed
","crucial||has||both F1 and EM
both F1 and EM||has||drop drastically
","Ablation analysis||use of||convolutions
",,,,,,
ablation-analysis,Self- attention in the encoders is also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,"Self- attention
in
encoders
is
necessary component
that contributes
1.4/1.3 gain of EM / F1
to
ultimate performance","Self- attention||in||encoders
encoders||is||necessary component
necessary component||that contributes||1.4/1.3 gain of EM / F1
1.4/1.3 gain of EM / F1||to||ultimate performance
",,,"Ablation analysis||has||Self- attention
",,,,,
ablation-analysis,"As the last block of rows in the table shows , data augmentation proves to be helpful in further boosting performance .","in
data augmentation
proves to be
helpful
further boosting performance","data augmentation||proves to be||helpful
helpful||in||further boosting performance
",,,"Ablation analysis||has||data augmentation
",,,,,
ablation-analysis,"Making the training data twice as large by adding the En - Fr - En data only ( ratio 1:1 between original training data and augmented data , as indicated by row "" data augmentation 2 ( 1:1:0 ) "" ) yields an increase in the F1 by 0.5 percent .","Making
training data
twice as large
by adding
En - Fr - En data only
by
yields
increase
in
F1
0.5 percent","twice as large||by adding||En - Fr - En data only
twice as large||yields||increase
increase||in||F1
F1||by||0.5 percent
","training data||has||twice as large
","Ablation analysis||Making||training data
",,,,,,
ablation-analysis,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data during training can further boost the model performance .","observe that
good sampling ratio
between
original and augmented data
during
training
can further
boost
model performance","good sampling ratio||between||original and augmented data
original and augmented data||can further||boost
original and augmented data||during||training
","boost||has||model performance
","Ablation analysis||observe that||good sampling ratio
",,,,,,
ablation-analysis,"In particular , when we increase the sampling weight of augmented data from ( 1:1:1 ) to ( 1:2:1 ) , the EM / F1 performance drops by 0.5/0.3 .","increase
sampling weight
of
augmented data
from
( 1:1:1 )
to
( 1:2:1 )
EM / F1 performance
drops
by
0.5/0.3","sampling weight||of||augmented data
sampling weight||from||( 1:1:1 )
( 1:1:1 )||to||( 1:2:1 )
drops||by||0.5/0.3
","sampling weight||has||EM / F1 performance
EM / F1 performance||has||drops
","Ablation analysis||increase||sampling weight
",,,,,,
ablation-analysis,"Empirically , the ratio ( 3:1:1 ) yields the best performance , with 1.5/1.1 gain over the base model on EM / F1 .","ratio ( 3:1:1 )
yields
best performance
with
1.5/1.1 gain
over
base model
on
EM / F1","ratio ( 3:1:1 )||yields||best performance
best performance||with||1.5/1.1 gain
1.5/1.1 gain||over||base model
base model||on||EM / F1
",,,"Ablation analysis||has||ratio ( 3:1:1 )
",,,,,
research-problem,The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks ( RNN ) to state - of - the - art performance in a variety of sequential tasks .,Recurrent Neural Networks ( RNN ),,,,,"Contribution||has research problem||Recurrent Neural Networks ( RNN )
",,,,
research-problem,"However , RNN still have a limited capacity to manipulate long - term memory .",RNN,,,,,"Contribution||has research problem||RNN
",,,,
model,"Here , we propose a novel RNN cell that resolves simultaneously those weaknesses of basic RNN .","propose
novel RNN cell
resolves simultaneously
weaknesses
of
basic RNN","weaknesses||of||basic RNN
","novel RNN cell||has||resolves simultaneously
resolves simultaneously||has||weaknesses
","Model||propose||novel RNN cell
",,,,,,
model,The Rotational Unit of Memory is a modified gated model whose rotational operation acts as associative memory and is strictly an orthogonal matrix .,"Rotational Unit of Memory
is
modified gated model
whose
rotational operation
acts as
associative memory
orthogonal matrix","Rotational Unit of Memory||is||modified gated model
modified gated model||whose||rotational operation
rotational operation||acts as||associative memory
Rotational Unit of Memory||is||orthogonal matrix
",,,"Model||has||Rotational Unit of Memory
",,,,,
experiments,COPYING MEMORY TASK,COPYING MEMORY TASK,,,,,,,,"Tasks||has||COPYING MEMORY TASK
","COPYING MEMORY TASK||has||RUM
"
experiments,"1 . RUM utilizes a different representation of memory that outperforms those of LSTM and GRU ; 2 . RUM solves the task completely , despite its update gate , which does not allow all of the information encoded in the hidden stay to pass through .","RUM
utilizes
different representation of memory
that
outperforms
those of
LSTM and GRU
solves
task
completely","RUM||solves||task
RUM||utilizes||different representation of memory
different representation of memory||that||outperforms
outperforms||those of||LSTM and GRU
","task||has||completely
",,,,,,,
experiments,ASSOCIATIVE RECALL TASK,ASSOCIATIVE RECALL TASK,,,,,,,,"Tasks||has||ASSOCIATIVE RECALL TASK
","ASSOCIATIVE RECALL TASK||has||Hyperparameters
"
experiments,All the models have the same hidden state N h = 50 for different lengths T .,"have
same hidden state N h = 50
for
different lengths T","same hidden state N h = 50||for||different lengths T
",,,,,"Hyperparameters||have||same hidden state N h = 50
",,,
experiments,We use a batch size 128 .,"use
batch size
128",,"batch size||has||128
",,,,"Hyperparameters||use||batch size
",,,
experiments,The optimizer is RMSProp with a learning rate 0.001 .,"optimizer
is
RMSProp
with
learning rate
0.001","optimizer||is||RMSProp
RMSProp||with||learning rate
","learning rate||of||0.001
",,,,,,"Hyperparameters||has||optimizer
",
experiments,"We find that LSTM fails to learn the task , because of its lack of sufficient memory capacity .","find that
LSTM
fails
to learn
task
of","fails||to learn||task
","LSTM||has||fails
",,,,"Results||find that||LSTM
",,,
experiments,"NTM and Fast - weight RNN fail longer tasks , which means they can not learn to manipulate their memory efficiently .","NTM and Fast - weight RNN
fail
longer tasks",,"NTM and Fast - weight RNN||has||fail
fail||has||longer tasks
",,,,,,"Results||has||NTM and Fast - weight RNN
",
experiments,QUESTION ANSWERING,QUESTION ANSWERING,,,,,,,,"Tasks||has||QUESTION ANSWERING
","QUESTION ANSWERING||has||Baselines
"
experiments,"We compare our model with several baselines : a simple LSTM , an End - to - end Memory Network ) and a GORU .","simple LSTM
End - to - end Memory Network
GORU",,,,,,,,"Baselines||name||simple LSTM
Baselines||name||End - to - end Memory Network
Baselines||name||GORU
",
experiments,"We find that RUM outperforms significantly LSTM and GORU and achieves competitive result with those of MemN2N , which has an attention mechanism .","find that
RUM
outperforms significantly
LSTM and GORU
achieves
competitive result
with
those of MemN2N
attention mechanism","RUM||achieves||competitive result
competitive result||with||those of MemN2N
","those of MemN2N||has||attention mechanism
RUM||has||outperforms significantly
outperforms significantly||has||LSTM and GORU
",,,,"Results||find that||RUM
",,,
experiments,CHARACTER LEVEL LANGUAGE MODELING,CHARACTER LEVEL LANGUAGE MODELING,,,,,,,,"Tasks||has||CHARACTER LEVEL LANGUAGE MODELING
","CHARACTER LEVEL LANGUAGE MODELING||has||PENN TREEBANK CORPUS DATA SET
"
experiments,PENN TREEBANK CORPUS DATA SET,PENN TREEBANK CORPUS DATA SET,,,,,,,,,"PENN TREEBANK CORPUS DATA SET||has||Results
"
experiments,"FS - RUM - 2 generalizes better than other gated models , such as GRU and LSTM , because it learns efficient patterns for activation in its kernels .","FS - RUM - 2
generalizes better
than
other gated models
such as
GRU and LSTM","generalizes better||than||other gated models
other gated models||such as||GRU and LSTM
","FS - RUM - 2||has||generalizes better
",,,,,,"Results||has||FS - RUM - 2
",
research-problem,Product - Aware Answer Generation in E - Commerce Question - Answering,Product - Aware Answer Generation,,,,,"Contribution||has research problem||Product - Aware Answer Generation
",,,,
research-problem,"In recent years , the explosive popularity of question - answering ( QA ) is revitalizing the task of reading comprehension with promising results .","question - answering ( QA )
reading comprehension",,,,,"Contribution||has research problem||question - answering ( QA )
Contribution||has research problem||reading comprehension
",,,,
model,"In this paper , we propose the product - aware answer generator ( PAAG ) , a product related question answering model which incorporates customer reviews with product attributes .","propose
product - aware answer generator ( PAAG )
product related question answering model
incorporates
customer reviews
with
product attributes","product related question answering model||incorporates||customer reviews
customer reviews||with||product attributes
","product - aware answer generator ( PAAG )||has||product related question answering model
","Model||propose||product - aware answer generator ( PAAG )
",,,,,,
model,"Specifically , at the beginning we employ an attention mechanism to model interactions between a question and reviews .","employ
attention mechanism
to model
interactions
between
question and reviews","attention mechanism||to model||interactions
interactions||between||question and reviews
",,"Model||employ||attention mechanism
",,,,,,
model,"Simultaneously , we employ a key - value memory network to store the product attributes and extract the relevance values according to the question .","key - value memory network
to store
product attributes
extract
relevance values
according to
question","key - value memory network||extract||relevance values
relevance values||according to||question
key - value memory network||to store||product attributes
",,,"Model||employ||key - value memory network
",,,,,
model,"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which combines product - aware review representation and attributes to generate the answer .","recurrent neural network ( RNN ) based decoder
combines
product - aware review representation and attributes
to generate
answer","recurrent neural network ( RNN ) based decoder||combines||product - aware review representation and attributes
product - aware review representation and attributes||to generate||answer
",,,"Model||propose||recurrent neural network ( RNN ) based decoder
",,,,,
model,"More importantly , to tackle the problem of meaningless answers , we propose an adversarial learning mechanism in the loss calculation for optimizing parameters .","to tackle
meaningless answers
propose
adversarial learning mechanism
in
loss calculation
for optimizing
parameters","meaningless answers||propose||adversarial learning mechanism
adversarial learning mechanism||for optimizing||parameters
adversarial learning mechanism||in||loss calculation
",,"Model||to tackle||meaningless answers
",,,,,,
baselines,( 1 ) S2SA : Sequence - to - sequence framework has been proposed for language generation task .,"S2SA
Sequence - to - sequence framework
proposed for
language generation task","S2SA||proposed for||language generation task
","S2SA||name||Sequence - to - sequence framework
",,"Baselines||has||S2SA
",,,,,
baselines,( 2 ) S2SAR : We implement a simple method which can incorporate the review information when generating the answer .,"S2SAR
implement
simple method
incorporate
review information
when generating
answer","S2SAR||implement||simple method
simple method||incorporate||review information
review information||when generating||answer
",,,"Baselines||has||S2SAR
",,,,,
baselines,( 3 ) SNet : S- Net is a two - stage state - of - the - art model which extracts some text spans from multiple documents context and synthesis the answer from those spans .,"SNet
is
two - stage state - of - the - art model
which extracts
some text spans
from
multiple documents context
synthesis
answer
from
spans","SNet||is||two - stage state - of - the - art model
two - stage state - of - the - art model||synthesis||answer
answer||from||spans
two - stage state - of - the - art model||which extracts||some text spans
some text spans||from||multiple documents context
",,,"Baselines||has||SNet
",,,,,
baselines,( 4 ) QS : We implement the query - based summarization model proposed by Hasselqvist et al ..,"QS
implement
query - based summarization model","QS||implement||query - based summarization model
",,,"Baselines||has||QS
",,,,,
baselines,( 5 ) BM25 : BM25 is a bag - of - words retrieval function that ranks a set of reviews based on the question terms appearing in each review .,"BM25
is
bag - of - words retrieval function
ranks
set of reviews
based on
question terms
appearing in
each review","BM25||is||bag - of - words retrieval function
bag - of - words retrieval function||ranks||set of reviews
set of reviews||based on||question terms
question terms||appearing in||each review
",,,"Baselines||has||BM25
",,,,,
baselines,( 6 ) TF - IDF : Term Frequency - Inverse Document Frequency is a numerical statistic that is intended to reflect how important a question word is to a review .,"TF - IDF
Term Frequency - Inverse Document Frequency
is
numerical statistic
to reflect
how important
question word
to
review","TF - IDF||is||numerical statistic
numerical statistic||to reflect||how important
question word||to||review
","TF - IDF||name||Term Frequency - Inverse Document Frequency
how important||has||question word
",,"Baselines||has||TF - IDF
",,,,,
experimental-setup,"Without using pre-trained embeddings , we randomly initialize the network parameters at the beginning of our experiments .","Without using
pre-trained embeddings
randomly initialize
network parameters
at
beginning
of
experiments","network parameters||at||beginning
beginning||of||experiments
network parameters||Without using||pre-trained embeddings
","randomly initialize||has||network parameters
",,"Experimental setup||has||randomly initialize
",,,,,
experimental-setup,All the RNN networks have 512 hidden units and the dimension of word embedding is 256 .,"All the RNN networks
have
512 hidden units
dimension
of
word embedding
is
256","All the RNN networks||have||512 hidden units
dimension||of||word embedding
word embedding||is||256
",,,"Experimental setup||has||All the RNN networks
Experimental setup||has||dimension
",,,,,
experimental-setup,"To produce better answers , we use beam search with beam size","To produce
better answers
use
beam search
with
beam size","better answers||use||beam search
beam search||with||beam size
",,"Experimental setup||To produce||better answers
",,,,,,"beam size||has||4
"
experimental-setup,4 .,4,,,,,,,,,
experimental-setup,Adagrad with learning rate 0.1 is used to optimize the parameters and batch size is 64 .,"Adagrad
with
learning rate
0.1
is
to optimize
parameters
batch size
64","Adagrad||with||learning rate
0.1||to optimize||parameters
Adagrad||with||batch size
batch size||is||64
","learning rate||has||0.1
",,"Experimental setup||has||Adagrad
",,,,,
experimental-setup,We implement our model using TensorFlow framework and train our model and all baseline models on NVIDIA Tesla P40 GPU .,"implement
our model
using
TensorFlow framework
train
our model and all baseline models
on
NVIDIA Tesla P40 GPU","our model||using||TensorFlow framework
our model and all baseline models||on||NVIDIA Tesla P40 GPU
",,"Experimental setup||implement||our model
Experimental setup||train||our model and all baseline models
",,,,,,
results,"In these experimental results , we see that PAAG achieves a 111 % , 8 % and 62.73 % increment over the stateof - the - art baseline SNet in terms of BLEU , embedding greedy and consistency score , respectively .","see that
PAAG
achieves
111 %
8 %
62.73 % increment
over
stateof - the - art baseline SNet
in terms of
BLEU
embedding greedy
consistency score","PAAG||over||stateof - the - art baseline SNet
stateof - the - art baseline SNet||achieves||111 %
stateof - the - art baseline SNet||achieves||8 %
stateof - the - art baseline SNet||achieves||62.73 % increment
stateof - the - art baseline SNet||in terms of||BLEU
stateof - the - art baseline SNet||in terms of||embedding greedy
stateof - the - art baseline SNet||in terms of||consistency score
",,"Results||see that||PAAG
",,,,,,
results,"In , we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth .","outperforms
all the baseline
significantly
in
semantic distance
with respect to
ground truth","all the baseline||in||semantic distance
semantic distance||with respect to||ground truth
","outperforms||has||all the baseline
all the baseline||has||significantly
",,,,,,"PAAG||has||outperforms
",
results,"In , we can see that PAAG outperforms other baseline models in both sentence fluency and consistency with the facts .","other baseline models
in
both sentence fluency and consistency
with
facts","other baseline models||in||both sentence fluency and consistency
both sentence fluency and consistency||with||facts
",,,,,,,"outperforms||has||other baseline models
",
results,"Although there is a small increment of S2 SAR with respect to S2SA in all metrics , we still find a noticeable gap between S2SAR and PAAG .","small increment
of
S2 SAR
with respect to
S2SA
in
all metrics
find
noticeable gap
between
S2SAR and PAAG","noticeable gap||between||S2SAR and PAAG
small increment||of||S2 SAR
S2 SAR||with respect to||S2SA
S2SA||in||all metrics
",,"Results||find||noticeable gap
","Results||has||small increment
",,,,,
ablation-analysis,"There is a slight increment from RAGF to RAGFD , which demonstrates the effectiveness of discriminator .","slight increment
from
RAGF
to
RAGFD
demonstrates
effectiveness
of
discriminator","slight increment||demonstrates||effectiveness
effectiveness||of||discriminator
slight increment||from||RAGF
RAGF||to||RAGFD
",,,"Ablation analysis||has||slight increment
",,,,,
ablation-analysis,"From , we find that RAGFWD achieves a 4.3 % improvement over RAGFD in terms of BLEU , and PAAG outperforms RAGFWD 4.1 % in terms of BLEU .","find that
RAGFWD
achieves
4.3 % improvement
over
RAGFD
in terms of
BLEU
PAAG
outperforms
RAGFWD
4.1 %
in terms of
BLEU","4.1 %||in terms of||BLEU
RAGFWD||achieves||4.3 % improvement
4.3 % improvement||over||RAGFD
4.3 % improvement||in terms of||BLEU
","PAAG||has||outperforms
outperforms||has||RAGFWD
RAGFWD||has||4.1 %
","Ablation analysis||find that||PAAG
Ablation analysis||find that||RAGFWD
",,,,,,
ablation-analysis,"Accordingly , we conclude that the performance of PAAG benefits from using Wasserstein distance based adversarial learning with gradient penalty .","conclude that
performance
of
PAAG
benefits from
using
Wasserstein distance based adversarial learning
with
gradient penalty","performance||of||PAAG
PAAG||benefits from||Wasserstein distance based adversarial learning
Wasserstein distance based adversarial learning||with||gradient penalty
",,"Ablation analysis||conclude that||performance
",,,,,,
ablation-analysis,This approach can help our model to achieve a better performance than the model using the vanilla GAN architecture .,"can help
our model
to achieve
better performance
than
model
vanilla GAN architecture","our model||to achieve||better performance
better performance||than||model
","model||using||vanilla GAN architecture
",,,,"Wasserstein distance based adversarial learning||can help||our model
",,,
research-problem,Modelling Interaction of Sentence Pair with Coupled- LSTMs,Modelling Interaction of Sentence Pair,,,,,"Contribution||has research problem||Modelling Interaction of Sentence Pair
",,,,
research-problem,"Recently , there is rising interest in modelling the interactions of two sentences with deep neural networks .",modelling the interactions of two sentences,,,,,"Contribution||has research problem||modelling the interactions of two sentences
",,,,
research-problem,"Among these tasks , a common problem is modelling the relevance / similarity of the sentence pair , which is also called text semantic matching .","modelling the relevance / similarity of the sentence pair
text semantic matching",,,,,"Contribution||has research problem||modelling the relevance / similarity of the sentence pair
Contribution||has research problem||text semantic matching
",,,,
model,"In this paper , we propose a new deep neural network architecture to model the strong interactions of two sentences .","propose
new deep neural network architecture
to model
strong interactions
of
two sentences","new deep neural network architecture||to model||strong interactions
strong interactions||of||two sentences
",,"Model||propose||new deep neural network architecture
",,,,,,
model,"Different with modelling two sentences with separated LSTMs , we utilize two interdependent LSTMs , called coupled - LSTMs , to fully affect each other at different time steps .","utilize
two interdependent LSTMs
called
coupled - LSTMs
to fully affect
each other
at
different time steps","two interdependent LSTMs||called||coupled - LSTMs
two interdependent LSTMs||to fully affect||each other
each other||at||different time steps
",,"Model||utilize||two interdependent LSTMs
",,,,,,
model,The output of coupled - LSTMs at each step depends on both sentences .,"output
of
coupled - LSTMs
at
each step
depends on
both sentences","output||of||coupled - LSTMs
coupled - LSTMs||at||each step
coupled - LSTMs||depends on||both sentences
",,,"Model||has||output
",,,,,
model,"Specifically , we propose two interdependent ways for the coupled - LSTMs : loosely coupled model ( LC - LSTMs ) and tightly coupled model ( TC - LSTMs ) .","two interdependent ways
for
coupled - LSTMs
loosely coupled model ( LC - LSTMs )
tightly coupled model ( TC - LSTMs )","two interdependent ways||for||coupled - LSTMs
","coupled - LSTMs||name||loosely coupled model ( LC - LSTMs )
coupled - LSTMs||name||tightly coupled model ( TC - LSTMs )
",,"Model||propose||two interdependent ways
",,,,,
model,"To utilize all the information of four directions of coupled - LSTMs , we aggregate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals .","all the information
of
four directions
of
coupled - LSTMs
aggregate
adopt
dynamic pooling strategy
to automatically select
most informative interaction signals","all the information||adopt||dynamic pooling strategy
dynamic pooling strategy||to automatically select||most informative interaction signals
all the information||of||four directions
four directions||of||coupled - LSTMs
","all the information||has||aggregate
",,"Model||utilize||all the information
",,,,,
model,"Finally , we feed them into a fully connected layer , followed by an output layer to compute the matching score .","feed them into
fully connected layer
followed by
output layer
to compute
matching score","fully connected layer||followed by||output layer
output layer||to compute||matching score
",,,,,"all the information||feed them into||fully connected layer
",,,
hyperparameters,"The word embeddings for all of the models are initialized with the 100d GloVe vectors ( 840B token version , ) and fine - tuned during training to improve the performance .","word embeddings
for
all of the models
initialized with
100d GloVe vectors
fine - tuned during
training
to improve
performance","word embeddings||for||all of the models
word embeddings||fine - tuned during||training
training||to improve||performance
word embeddings||initialized with||100d GloVe vectors
",,,"Hyperparameters||has||word embeddings
",,,,,
hyperparameters,"The other parameters are initialized by randomly sampling from uniform distribution in [ ? 0.1 , 0.1 ] .","other parameters
initialized by
randomly sampling
from
uniform distribution
in
[ ? 0.1 , 0.1 ]","other parameters||initialized by||randomly sampling
randomly sampling||from||uniform distribution
uniform distribution||in||[ ? 0.1 , 0.1 ]
",,,"Hyperparameters||has||other parameters
",,,,,
hyperparameters,"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value","For
each task
take
hyperparameters
achieve
best performance
on
development set
via
small grid search
over
combinations
of
initial learning rate [ 0.05 , 0.0005 , 0.0001 ]
l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]
threshold value","each task||take||hyperparameters
hyperparameters||achieve||best performance
best performance||on||development set
best performance||via||small grid search
small grid search||over||combinations
combinations||of||initial learning rate [ 0.05 , 0.0005 , 0.0001 ]
combinations||of||l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]
combinations||of||threshold value
",,"Hyperparameters||For||each task
",,,,,,
baselines,Neural bag - of - words ( NBOW ) :,Neural bag - of - words ( NBOW ),,,,"Baselines||has||Neural bag - of - words ( NBOW )
",,,,,"Neural bag - of - words ( NBOW )||has||Each sequence
"
baselines,"Each sequence as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .","Each sequence
sum of
embeddings
of
words it contains
concatenated and fed
to
MLP","concatenated and fed||to||MLP
concatenated and fed||to||MLP
Each sequence||sum of||embeddings
embeddings||of||words it contains
","Each sequence||has||concatenated and fed
",,,,,,,
baselines,"Single LSTM : A single LSTM to encode the two sequences , which is used in .","Single LSTM
encode
two sequences","Single LSTM||encode||two sequences
",,,"Baselines||has||Single LSTM
",,,,,
baselines,"Parallel LSTMs : Two sequences are encoded by two LSTMs separately , then they are concatenated and fed to a MLP .","Parallel LSTMs
Two sequences
are
encoded by
two LSTMs separately
concatenated and fed
to
MLP","Two sequences||encoded by||two LSTMs separately
Two sequences||are||concatenated and fed
","Parallel LSTMs||has||Two sequences
",,"Baselines||has||Parallel LSTMs
",,,,,
baselines,"Attention LSTMs : An attentive LSTM to encode two sentences into a semantic space , which used in .","Attention LSTMs
attentive LSTM
to encode
two sentences
into
semantic space","attentive LSTM||to encode||two sentences
two sentences||into||semantic space
","Attention LSTMs||has||attentive LSTM
",,"Baselines||has||Attention LSTMs
",,,,,
results,Experiment - I : Recognizing Textual Entailment,Experiment - I : Recognizing Textual Entailment,,,,"Results||has||Experiment - I : Recognizing Textual Entailment
",,,,,"Experiment - I : Recognizing Textual Entailment||has||proposed two C - LSTMs models with four stacked blocks
"
results,"Our proposed two C - LSTMs models with four stacked blocks outperform all the competitor models , which indicates that our thinner and deeper network does work effectively .","proposed two C - LSTMs models with four stacked blocks
outperform
all the competitor models
indicates that
our thinner and deeper network
does work effectively","proposed two C - LSTMs models with four stacked blocks||outperform||all the competitor models
proposed two C - LSTMs models with four stacked blocks||indicates that||our thinner and deeper network
","our thinner and deeper network||has||does work effectively
",,,,,,,
results,"Compared with attention LSTMs , our two models achieve comparable results to them using much fewer parameters ( nearly 1 / 5 ) .","Compared with
attention LSTMs
our two models
achieve
comparable results
using
much fewer parameters ( nearly 1 / 5 )","our two models||using||much fewer parameters ( nearly 1 / 5 )
our two models||achieve||comparable results
","attention LSTMs||has||our two models
",,,,"Experiment - I : Recognizing Textual Entailment||Compared with||attention LSTMs
",,,
results,"By stacking C - LSTMs , the performance of them are improved significantly , and the four stacked TC - LSTMs achieve 85.1 % accuracy on this dataset .","By stacking
C - LSTMs
performance
improved
significantly","performance||improved||significantly
","C - LSTMs||has||performance
",,,,"Experiment - I : Recognizing Textual Entailment||By stacking||C - LSTMs
",,,
research-problem,Open Question Answering with Weakly Supervised Embedding Models,Open Question Answering,,,,,"Contribution||has research problem||Open Question Answering
",,,,
research-problem,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",open - domain question answering,,,,,"Contribution||has research problem||open - domain question answering
",,,,
research-problem,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,Question answering,,,,,"Contribution||has research problem||Question answering
",,,,
approach,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .","take
approach
of
converting questions
to
( uninterpretable ) vectorial representations
require no
pre-defined grammars or lexicons
can
query
any KB
independent
of
schema","approach||of||converting questions
converting questions||to||( uninterpretable ) vectorial representations
( uninterpretable ) vectorial representations||can||query
query||any KB||independent
independent||of||schema
( uninterpretable ) vectorial representations||require no||pre-defined grammars or lexicons
",,"Approach||take||approach
",,,,,,
approach,"Following , we focus on answering simple factual questions on a broad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .","focus on
answering
simple factual questions
on
broad range of topics","simple factual questions||on||broad range of topics
","answering||has||simple factual questions
","Approach||focus on||answering
",,,,,,
approach,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,"based on
learning
low - dimensional vector embeddings
of
words
of
KB triples
so that
representations
questions and corresponding answers
end up
similar
in
embedding space","low - dimensional vector embeddings||so that||representations
representations||end up||similar
similar||in||embedding space
representations||of||questions and corresponding answers
low - dimensional vector embeddings||of||words
low - dimensional vector embeddings||of||KB triples
","learning||has||low - dimensional vector embeddings
","Approach||based on||learning
",,,,,,
approach,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .","make use of
weak supervision",,,"Approach||make use of||weak supervision
",,,,,,
approach,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,"model
able to
take advantage
of
noisy and indirect supervision
by
automatically generating
questions
from
KB triples
treating this as
training data
supplementing
with
data set of questions
collaboratively marked as
paraphrases
with no
associated answers","model||able to||take advantage
take advantage||of||noisy and indirect supervision
noisy and indirect supervision||by||automatically generating
questions||treating this as||training data
questions||from||KB triples
noisy and indirect supervision||by||supplementing
supplementing||with||data set of questions
data set of questions||collaboratively marked as||paraphrases
data set of questions||with no||associated answers
","automatically generating||has||questions
",,"Approach||has||model
",,,,,
approach,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,"end up learning
meaningful vectorial representations
for
questions
involving up to
800 k words
triples
of
mostly automatically created KB
with
2.4 M entities
600 k relationships","meaningful vectorial representations||for||triples
triples||of||mostly automatically created KB
mostly automatically created KB||with||2.4 M entities
mostly automatically created KB||with||600 k relationships
meaningful vectorial representations||for||questions
questions||involving up to||800 k words
",,"Approach||end up learning||meaningful vectorial representations
",,,,,,
results,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .","see that
multitasking
with
paraphrase data
is
essential
improves
F1
from
0.60
to
0.68","multitasking||with||paraphrase data
paraphrase data||improves||F1
F1||from||0.60
0.60||to||0.68
paraphrase data||is||essential
",,"Results||see that||multitasking
",,,,,,
results,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,"Fine - tuning
embedding model
is
very beneficial
to optimize
top of the list
grants
bump
of
5 points
of
F1","embedding model||grants||bump
bump||of||5 points
5 points||of||F1
embedding model||is||very beneficial
very beneficial||to optimize||top of the list
","Fine - tuning||has||embedding model
",,"Results||has||Fine - tuning
",,,,,
results,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .","All versions of our system
greatly outperform
paralex
fine - tuned model
improves
F1 - score
by
almost 20 points","fine - tuned model||improves||F1 - score
F1 - score||by||almost 20 points
","All versions of our system||has||greatly outperform
greatly outperform||has||paralex
All versions of our system||has||fine - tuned model
",,"Results||has||All versions of our system
",,,,,
results,"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .","string matching
greatly improves
results
both
precision and recall
significantly reduces
evaluation time","greatly improves||both||precision and recall
","string matching||has||significantly reduces
significantly reduces||has||evaluation time
string matching||has||greatly improves
greatly improves||has||results
",,"Results||has||string matching
",,,,,
results,"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .","final F1
obtained by
our fine - tuned model
is
even better
then
result
of
paralex
in
reranking","final F1||obtained by||our fine - tuned model
our fine - tuned model||is||even better
even better||in||reranking
even better||then||result
result||of||paralex
",,,"Results||has||final F1
",,,,,
research-problem,Simple and Effective Text Matching with Richer Alignment Features,Text Matching,,,,,"Contribution||has research problem||Text Matching
",,,,
model,"This paper presents RE2 , a fast and strong neural architecture with multiple alignment processes for general purpose text matching .","presents
RE2
fast and strong neural architecture
with
multiple alignment processes
for
general purpose text matching","fast and strong neural architecture||with||multiple alignment processes
fast and strong neural architecture||for||general purpose text matching
","RE2||has||fast and strong neural architecture
","Model||presents||RE2
",,,,,,
model,"These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .","components
previous aligned features
Residual vectors
original point - wise features
Embedding vectors
contextual features
Encoded vectors",,"components||has||contextual features
contextual features||name||Encoded vectors
components||has||previous aligned features
previous aligned features||name||Residual vectors
components||has||original point - wise features
original point - wise features||name||Embedding vectors
",,"Model||has||components
",,,,,
model,An embedding layer first embeds discrete tokens .,"embedding layer
embeds
discrete tokens","embedding layer||embeds||discrete tokens
",,,"Model||has||embedding layer
",,,,,
model,"Several same - structured blocks consisting of encoding , alignment and fusion layers then process the sequences consecutively .","same - structured blocks
consisting of
encoding
alignment
fusion layers
process
sequences
consecutively","same - structured blocks||process||sequences
same - structured blocks||consisting of||encoding
same - structured blocks||consisting of||alignment
same - structured blocks||consisting of||fusion layers
","sequences||has||consecutively
",,"Model||has||same - structured blocks
",,,,,
model,These blocks are connected by an augmented version of residual connections ( see section 2.1 ) .,"connected by
augmented version of residual connections",,,,,,"same - structured blocks||connected by||augmented version of residual connections
",,,
model,A pooling layer aggregates sequential representations into vectors which are finally processed by a prediction layer to give the final prediction .,"pooling layer
aggregates
sequential representations
into
vectors
processed by
prediction layer
to give
final prediction","pooling layer||aggregates||sequential representations
sequential representations||into||vectors
vectors||processed by||prediction layer
prediction layer||to give||final prediction
",,,"Model||has||pooling layer
",,,,,
model,"The implementation of each layer is kept as simple as possible , and the whole model , as a well - organized combination , is quite powerful and lightweight at the same time .","implementation
of
each layer
kept
as simple as possible","implementation||of||each layer
each layer||kept||as simple as possible
",,,"Model||has||implementation
",,,,,
experimental-setup,We implement our model with TensorFlow and train on Nvidia P100 GPUs .,"implement
model
with
TensorFlow
train on
Nvidia P100 GPUs","model||with||TensorFlow
",,,,,"Experimental Setup||train on||Nvidia P100 GPUs
Experimental Setup||implement||model
",,,
experimental-setup,"We tokenize sentences with the NLTK toolkit , convert them to lower cases and remove all punctuations .","tokenize
sentences
with
NLTK toolkit
convert
lower cases
remove
all punctuations","sentences||with||NLTK toolkit
sentences||convert||lower cases
sentences||remove||all punctuations
",,,,,"Experimental Setup||tokenize||sentences
",,,
experimental-setup,Word embeddings are initialized with 840B - 300d,"Word embeddings
initialized with
840B - 300d","Word embeddings||initialized with||840B - 300d
",,,,,,,"Experimental Setup||has||Word embeddings
",
experimental-setup,Glo Ve word vectors and fixed during training .,"Glo Ve word vectors
fixed during
training","Glo Ve word vectors||fixed during||training
",,,,,,,"Experimental Setup||has||Glo Ve word vectors
",
experimental-setup,Embeddings of out - ofvocabulary words are initialized to zeros and fixed as well .,"Embeddings
of
out - ofvocabulary words
initialized to
zeros","Embeddings||of||out - ofvocabulary words
Embeddings||initialized to||zeros
",,,,,,,"Experimental Setup||has||Embeddings
",
experimental-setup,All other parameters are initialized with He initialization and normalized by weight normalization .,"All other parameters
are
initialized
with
He initialization
normalized
by
weight normalization","All other parameters||are||normalized
normalized||by||weight normalization
All other parameters||are||initialized
initialized||with||He initialization
",,,,,,,"Experimental Setup||has||All other parameters
",
experimental-setup,Dropout with a keep probability of 0.8 is applied before every fully - connected or convolutional layer .,"Dropout
with
keep probability
of
0.8
applied before
fully - connected
convolutional layer","Dropout||with||keep probability
keep probability||applied before||fully - connected
keep probability||applied before||convolutional layer
keep probability||of||0.8
",,,,,,,"Experimental Setup||has||Dropout
",
experimental-setup,The kernel size of the convolutional encoder is set to 3 .,"kernel size
of
convolutional encoder
set to
3","kernel size||of||convolutional encoder
kernel size||set to||3
",,,,,,,"Experimental Setup||has||kernel size
",
experimental-setup,The prediction layer is a two - layer feed - forward network .,"prediction layer
is
two - layer feed - forward network","prediction layer||is||two - layer feed - forward network
",,,,,,,"Experimental Setup||has||prediction layer
",
experimental-setup,The hidden size is set to 150 in all experiments .,"hidden size
set to
150","hidden size||set to||150
",,,,,,,"Experimental Setup||has||hidden size
",
experimental-setup,"Activations in all feed - forward networks are GeLU activations , and we use ?","Activations
in
all feed - forward networks
are
GeLU activations","Activations||in||all feed - forward networks
Activations||are||GeLU activations
",,,,,,,"Experimental Setup||has||Activations
",
experimental-setup,We scale the summation in augmented residual connections by 1 / ? 2 when n ? 3 to preserve the variance under the assumption that the two addends have the same variance .,"scale
summation
in
augmented residual connections
by
1 / ? 2
when
n ? 3
to preserve
variance
under
assumption
that
two addends
have
same variance","summation||in||augmented residual connections
summation||by||1 / ? 2
1 / ? 2||when||n ? 3
1 / ? 2||to preserve||variance
variance||under||assumption
assumption||that||two addends
two addends||have||same variance
",,,,,"Experimental Setup||scale||summation
",,,
experimental-setup,The number of blocks is tuned in a range from 1 to 3 .,"number of blocks
tuned in
range
from
1 to 3","number of blocks||tuned in||range
range||from||1 to 3
",,,,,,,"Experimental Setup||has||number of blocks
",
experimental-setup,The number of layers of the convolutional encoder is tuned from 1 to 3 .,"number of layers
of
convolutional encoder
tuned from
1
to
3","number of layers||of||convolutional encoder
convolutional encoder||tuned from||1
1||to||3
",,,,,,,"Experimental Setup||has||number of layers
",
experimental-setup,"We use the Adam optimizer ( Kingma and Ba , 2015 ) and an exponentially decaying learning rate with a linear warmup .",,,,,,,,,,
experimental-setup,The initial learning rate is tuned from 0.0001 to 0.003 .,"initial learning rate
tuned from
0.0001 to 0.003","initial learning rate||tuned from||0.0001 to 0.003
",,,,,,,"Experimental Setup||has||initial learning rate
",
experimental-setup,The batch size is tuned from 64 to 512 .,"batch size
tuned from
64 to 512","batch size||tuned from||64 to 512
",,,,,,,"Experimental Setup||has||batch size
",
experimental-setup,The threshold for gradient clipping is set to 5 .,"threshold
for
gradient clipping
set to
5","threshold||for||gradient clipping
gradient clipping||set to||5
",,,,,,,"Experimental Setup||has||threshold
",
results,Results on WikiQA dataset are listed in .,"on
WikiQA dataset",,,"Results||on||WikiQA dataset
",,,,,,
results,We obtain a result on par with the state - of - the - art reported on this dataset .,"obtain
result
on par with
state - of - the - art","result||on par with||state - of - the - art
",,,,,"WikiQA dataset||obtain||result
",,,
results,Our method can perform well in the answer selection task without any taskspecific modifications .,"method
perform
well
in
answer selection task
without
any taskspecific modifications","method||perform||well
well||in||answer selection task
well||without||any taskspecific modifications
",,,"Results||has||method
",,,,,
ablation-analysis,"The first ablation baseline shows that without richer features as the alignment input , the performance on all datasets degrades significantly .","first ablation baseline
shows
without richer features
as
alignment input
performance
on
all datasets
degrades significantly","first ablation baseline||shows||without richer features
without richer features||as||alignment input
performance||on||all datasets
","without richer features||has||performance
performance||has||degrades significantly
",,"Ablation analysis||has||first ablation baseline
",,,,,
ablation-analysis,The results of the second baseline show that vanilla residual connections without direct access to the original pointwise features are not enough to model the relations in many text matching tasks .,"second baseline
show
vanilla residual connections
without direct access to
original pointwise features
are
not enough
to model
relations
in
many text matching tasks","second baseline||show||vanilla residual connections
vanilla residual connections||without direct access to||original pointwise features
vanilla residual connections||are||not enough
not enough||to model||relations
relations||in||many text matching tasks
",,,"Ablation analysis||has||second baseline
",,,,,
ablation-analysis,"The simpler implementation of the fusion layer leads to evidently worse performance , indicating that the fu- sion layer can not be further simplified .","simpler implementation
of
fusion layer
leads to
evidently worse performance","simpler implementation||leads to||evidently worse performance
simpler implementation||of||fusion layer
",,,"Ablation analysis||has||simpler implementation
",,,,,
ablation-analysis,"In the last ablation study , we can see that parallel blocks perform worse than stacked blocks , which supports the preference for deeper models over wider ones .","see that
parallel blocks
perform
worse
than
stacked blocks
supports
preference
for
deeper models
over
wider ones","parallel blocks||perform||worse
worse||than||stacked blocks
worse||supports||preference
preference||for||deeper models
deeper models||over||wider ones
",,"Ablation analysis||see that||parallel blocks
",,,,,,
research-problem,FLOWQA : GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION,CONVERSATIONAL MACHINE COMPREHENSION,,,,,"Contribution||has research problem||CONVERSATIONAL MACHINE COMPREHENSION
",,,,
model,"We present FLOWQA , a model designed for conversational machine comprehension .","present
FLOWQA
designed for
conversational machine comprehension","FLOWQA||designed for||conversational machine comprehension
",,"Model||present||FLOWQA
",,,,,,
model,FLOWQA consists of two main components : a base neural model for single - turn MC and a FLOW mechanism that encodes the conversation history .,"FLOWQA
consists of
two main components
base neural model
for
single - turn MC
FLOW mechanism
encodes
conversation history","FLOWQA||consists of||two main components
base neural model||for||single - turn MC
FLOW mechanism||encodes||conversation history
","two main components||has||base neural model
two main components||has||FLOW mechanism
",,"Model||has||FLOWQA
",,,,,
model,"Instead of using the shallow history , i.e. , previous questions and answers , we feed the model with the entire hidden representations generated during the process of answering previous questions .","of
feed
entire hidden representations
generated during
process
answering previous questions","entire hidden representations||generated during||process
process||of||answering previous questions
",,"Model||feed||entire hidden representations
",,,,,,
model,"This FLOW mechanism is also remarkably effective at tracking the world states for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .","FLOW mechanism
is
remarkably effective
at tracking
world states
for
sequential instruction understanding","FLOW mechanism||is||remarkably effective
remarkably effective||at tracking||world states
world states||for||sequential instruction understanding
",,,"Model||has||FLOW mechanism
",,,,,
model,"The FLOW mechanism can be viewed as stacking single - turn QA models along the dialog progression ( i.e. , the question turns ) and building information flow along the dialog .","can be
viewed
as
stacking single - turn QA models
along
dialog progression
building
information flow
along
dialog","viewed||as||stacking single - turn QA models
stacking single - turn QA models||along||dialog progression
stacking single - turn QA models||building||information flow
information flow||along||dialog
",,,,,"FLOW mechanism||can be||viewed
",,,
model,"This information transfer happens for each context word , allowing rich information in the reasoning process to flow .","information transfer
happens for
each context word
allowing
rich information
in
reasoning process
to
flow","information transfer||happens for||each context word
information transfer||allowing||rich information
rich information||in||reasoning process
rich information||to||flow
",,,"Model||has||information transfer
",,,,,
model,"To handle this issue , we propose an alternating parallel processing structure , which alternates between sequentially processing one dimension in parallel of the other dimension , and thus speeds up training significantly .","propose
alternating parallel processing structure
which
alternates
between
sequentially processing
one dimension
in parallel of
other dimension
speeds up
training
significantly","alternating parallel processing structure||which||alternates
alternates||between||sequentially processing
one dimension||in parallel of||other dimension
","sequentially processing||has||one dimension
sequentially processing||has||speeds up
speeds up||has||training
training||has||significantly
","Model||propose||alternating parallel processing structure
",,,,,,
research-problem,Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,conversational machine comprehension ( MC ),,,,,"Contribution||has research problem||conversational machine comprehension ( MC )
",,,,
code,Our code can be found in https://github.com/momohuang/FlowQA.,,,,,,,,,,
baselines,"applied BiDAF ++ , a strong extractive QA model to QuAC dataset .","applied
BiDAF ++
strong extractive QA model
to
QuAC dataset","strong extractive QA model||to||QuAC dataset
","BiDAF ++||has||strong extractive QA model
","Baselines||applied||BiDAF ++
",,,,,,
baselines,"Here we briefly describe the ablated systems : "" - FLOW "" removes the flow component from IF layer ( Eq. 2 in Section 3.2 ) , "" - QHIER - RNN "" removes the hierarchical LSTM layers on final question vectors ( Eq. 7 in Section 3.3 ) .","- FLOW
removes
flow component
from
IF layer
- QHIER - RNN
removes
hierarchical LSTM layers
on
final question vectors","- QHIER - RNN||removes||hierarchical LSTM layers
hierarchical LSTM layers||on||final question vectors
- FLOW||removes||flow component
flow component||from||IF layer
",,,"Baselines||has||- QHIER - RNN
Baselines||has||- FLOW
",,,,,
results,"FLOWQA yields substantial improvement over existing models on both datasets ( + 7.2 % F 1 on CoQA , + 4.0 % F 1 on QuAC ) .","FLOWQA
yields
substantial improvement
over
existing models
on
both datasets
+ 7.2 % F 1
on
CoQA
+ 4.0 % F 1
on
QuAC","FLOWQA||yields||substantial improvement
substantial improvement||over||existing models
+ 4.0 % F 1||on||QuAC
+ 7.2 % F 1||on||CoQA
substantial improvement||on||both datasets
","substantial improvement||has||+ 4.0 % F 1
substantial improvement||has||+ 7.2 % F 1
",,"Results||has||FLOWQA
",,,,,
results,We find that FLOW is a critical component .,"find that
FLOW
is
critical component","FLOW||is||critical component
",,"Results||find that||FLOW
",,,,,,
results,"Removing QHier - RNN has a minor impact ( 0.1 % on both datasets ) , while removing FLOW results in a substantial performance drop , with or without using QHierRNN ( 2 - 3 % on QuAC , 4.1 % on CoQA ) .","Removing
QHier - RNN
minor impact
0.1 %
on
both datasets
removing
FLOW
results in
substantial performance drop
with or without using
QHierRNN
2 - 3 %
on
QuAC
4.1 %
on
CoQA","minor impact||on||both datasets
FLOW||results in||substantial performance drop
substantial performance drop||with or without using||QHierRNN
2 - 3 %||on||QuAC
4.1 %||on||CoQA
","QHier - RNN||has||minor impact
minor impact||has||0.1 %
substantial performance drop||has||2 - 3 %
substantial performance drop||has||4.1 %
","Results||Removing||QHier - RNN
Results||removing||FLOW
",,,,,,
results,"By comparing 0 - Ans and 1 - Ans on two datasets , we can see that providing gold answers is more crucial for QuAC .","comparing
0 - Ans and 1 - Ans
on
two datasets
providing
gold answers
is
more crucial
for
QuAC","0 - Ans and 1 - Ans||providing||gold answers
gold answers||is||more crucial
more crucial||for||QuAC
0 - Ans and 1 - Ans||on||two datasets
",,"Results||comparing||0 - Ans and 1 - Ans
",,,,,,
results,"Based on the training time each epoch takes ( i.e. , time needed for passing through the data once ) , the speedup is 8.1x on CoQA and 4.2 x on QuAC .","Based on
training time
each epoch
takes
speedup
is
8.1x
on
CoQA
4.2 x
on
QuAC","speedup||is||4.2 x
4.2 x||on||QuAC
speedup||is||8.1x
8.1x||on||CoQA
training time||takes||each epoch
","training time||has||speedup
","Results||Based on||training time
",,,,,,
research-problem,Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS VIA LARGE SCALE MULTI - TASK LEARNING,LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS,,,,,"Contribution||has research problem||LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS
",,,,
research-problem,A lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,distributed vector representations of words,,,,,"Contribution||has research problem||distributed vector representations of words
",,,,
research-problem,"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem .",learning representations of sequences of words,,,,,"Contribution||has research problem||learning representations of sequences of words
",,,,
research-problem,Some recent work has addressed this by learning general - purpose sentence representations .,learning general - purpose sentence representations,,,,,"Contribution||has research problem||learning general - purpose sentence representations
",,,,
approach,"Our work exploits this in the context of a simple one - to - many multi -task learning ( MTL ) framework , wherein a single recurrent sentence encoder is shared across multiple tasks .","in
context
of
simple one - to - many multi -task learning ( MTL ) framework
wherein
single recurrent sentence encoder
shared across
multiple tasks","context||of||simple one - to - many multi -task learning ( MTL ) framework
simple one - to - many multi -task learning ( MTL ) framework||wherein||single recurrent sentence encoder
single recurrent sentence encoder||shared across||multiple tasks
",,"Approach||in||context
",,,,,,
approach,"While our work aims at learning fixed - length distributed sentence representations , it is not always practical to assume that the entire "" meaning "" of a sentence can be encoded into a fixed - length vector .","aims at
learning
fixed - length distributed sentence representations",,"learning||has||fixed - length distributed sentence representations
","Approach||aims at||learning
",,,,,,
approach,The primary contribution of our work is to combine the benefits of diverse sentence - representation learning objectives into a single multi-task framework .,"of
combine
benefits
diverse sentence - representation learning objectives
into
single multi-task framework","benefits||of||diverse sentence - representation learning objectives
diverse sentence - representation learning objectives||into||single multi-task framework
",,"Approach||combine||benefits
",,,,,,
results,It is evident from that adding more tasks improves the transfer performance of our model .,"adding
more tasks
improves
transfer performance
of
our model","more tasks||improves||transfer performance
transfer performance||of||our model
",,"Results||adding||more tasks
",,,,,,
results,Increasing the capacity our sentence encoder with more hidden units ( + L ) as well as an additional layer ( + 2L ) also lead to improved transfer performance .,"Increasing
capacity
our sentence encoder
with
more hidden units ( + L )
as well as
additional layer ( + 2L )
lead to
improved transfer performance","our sentence encoder||lead to||improved transfer performance
our sentence encoder||with||more hidden units ( + L )
our sentence encoder||as well as||additional layer ( + 2L )
","capacity||has||our sentence encoder
","Results||Increasing||capacity
",,,,,,
results,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .","observe
gains
of
1.1 - 2.0 %
on
sentiment classification tasks
MR
CR
SUBJ
MPQA
over
Infersent","gains||of||1.1 - 2.0 %
1.1 - 2.0 %||over||Infersent
1.1 - 2.0 %||on||sentiment classification tasks
","sentiment classification tasks||name||MR
sentiment classification tasks||name||CR
sentiment classification tasks||name||SUBJ
sentiment classification tasks||name||MPQA
","Results||observe||gains
",,,,,,
results,"We demonstrate substantial gains on TREC ( 6 % over Infersent and roughly 2 % over the CNN - LSTM ) , outperforming even a competitive supervised baseline .","demonstrate
substantial gains
on
TREC
6 %
over
Infersent
roughly 2 %
over
CNN - LSTM
outperforming
competitive supervised baseline","roughly 2 %||over||CNN - LSTM
6 %||over||Infersent
substantial gains||on||TREC
","substantial gains||has||roughly 2 %
substantial gains||has||outperforming
outperforming||has||competitive supervised baseline
substantial gains||has||6 %
","Results||demonstrate||substantial gains
",,,,,,
results,"We see similar gains ( 2.3 % ) on paraphrase identification ( MPRC ) , closing the gap on supervised approaches trained from scratch .","see
similar gains
2.3 %
on
paraphrase identification ( MPRC )
closing
gap
on
supervised approaches
trained from
scratch","similar gains||closing||gap
gap||on||supervised approaches
supervised approaches||trained from||scratch
similar gains||on||paraphrase identification ( MPRC )
","similar gains||has||2.3 %
","Results||see||similar gains
",,,,,,
results,The addition of constituency parsing improves performance on sentence relatedness ( SICK - R ) and entailment ( SICK - E ) consistent with observations made by .,"addition of
constituency parsing
improves
performance
on
sentence relatedness ( SICK - R )
entailment ( SICK - E )","performance||on||sentence relatedness ( SICK - R )
performance||on||entailment ( SICK - E )
","constituency parsing||has||improves
improves||has||performance
","Results||addition of||constituency parsing
",,,,,,
results,"In , we show that simply training an MLP on top of our fixed sentence representations outperforms several strong & complex supervised approaches that use attention mechanisms , even on this fairly large dataset .","show that
training
MLP
on top of
our fixed sentence representations
outperforms
several strong & complex supervised approaches
that use
attention mechanisms","MLP||on top of||our fixed sentence representations
several strong & complex supervised approaches||that use||attention mechanisms
","training||has||MLP
our fixed sentence representations||has||outperforms
outperforms||has||several strong & complex supervised approaches
","Results||show that||training
",,,,,,
results,"For example , we observe a 0.2-0.5 % improvement over the decomposable attention model .","0.2-0.5 % improvement
over
decomposable attention model","0.2-0.5 % improvement||over||decomposable attention model
",,,"Results||observe||0.2-0.5 % improvement
",,,,,
results,"When using only a small fraction of the training data , indicated by the columns 1 k - 25 k , we are able to outperform the Siamese and Multi - Perspective CNN using roughly 6 % of the available training set .","When using
small fraction
of
training data
able to
outperform
Siamese and Multi - Perspective CNN
using
roughly 6 %
of
available training set","small fraction||able to||outperform
Siamese and Multi - Perspective CNN||using||roughly 6 %
roughly 6 %||of||available training set
small fraction||of||training data
","outperform||has||Siamese and Multi - Perspective CNN
","Results||When using||small fraction
",,,,,,
results,We also outperform the Deconv LVM model proposed by in this low - resource setting .,"outperform
Deconv LVM model",,"outperform||has||Deconv LVM model
",,"Results||has||outperform
",,,,,
results,"Somewhat surprisingly , in we observe that the learned word embeddings are competitive with popular methods such as GloVe , word2vec , and fasttext on the benchmarks presented by and .","observe that
learned word embeddings
are
competitive
with
popular methods
such as
GloVe
word2vec
fasttext","learned word embeddings||are||competitive
competitive||with||popular methods
popular methods||such as||GloVe
popular methods||such as||word2vec
popular methods||such as||fasttext
",,"Results||observe that||learned word embeddings
",,,,,,
results,Representations learned solely from NLI do appear to encode syntax but incorporation into our multi-task framework does not amplify this signal .,"Representations
learned solely from
NLI
appear to encode
syntax
incorporation into
our multi-task framework
does not
amplify
signal","Representations||learned solely from||NLI
NLI||appear to encode||syntax
Representations||incorporation into||our multi-task framework
our multi-task framework||does not||amplify
","amplify||has||signal
",,"Results||has||Representations
",,,,,
results,"Similarly , we observe that sentence characteristics such as length and word order are better encoded with the addition of parsing .","sentence characteristics
such as
length and word order
are
better encoded
addition of
parsing","sentence characteristics||such as||length and word order
length and word order||are||better encoded
better encoded||addition of||parsing
",,,"Results||observe that||sentence characteristics
",,,,,
research-problem,Dynamically Fused Graph Network for Multi-hop Reasoning,Multi-hop Reasoning,,,,,"Contribution||has research problem||Multi-hop Reasoning
",,,,
research-problem,Text - based question answering ( TBQA ) has been studied extensively in recent years .,Text - based question answering ( TBQA ),,,,,"Contribution||has research problem||Text - based question answering ( TBQA )
",,,,
research-problem,Question answering ( QA ) has been a popular topic in natural language processing .,Question answering ( QA ),,,,,"Contribution||has research problem||Question answering ( QA )
",,,,
research-problem,QA provides a quantifiable way to evaluate an NLP system 's capability on language understanding and reasoning .,QA,,,,,"Contribution||has research problem||QA
",,,,
model,"In this paper , we propose Dynamically Fused Graph Network ( DFGN ) , a novel method to address the aforementioned concerns for multi-hop text - based QA .","propose
Dynamically Fused Graph Network ( DFGN )
novel method",,"Dynamically Fused Graph Network ( DFGN )||has||novel method
","Model||propose||Dynamically Fused Graph Network ( DFGN )
",,,,,,
model,"For the first challenge , DFGN constructs a dynamic entity graph based on entity mentions in the query and documents .","DFGN
constructs
dynamic entity graph
based on
entity mentions
in
query and documents","DFGN||constructs||dynamic entity graph
dynamic entity graph||based on||entity mentions
entity mentions||in||query and documents
",,,"Model||has||DFGN
",,,,,"dynamic entity graph||has||process
"
model,This process iterates in multiple rounds to achieve multihop reasoning .,"process
iterates in
multiple rounds
to achieve
multihop reasoning","process||iterates in||multiple rounds
multiple rounds||to achieve||multihop reasoning
",,,,,,,,
model,"In each round , DFGN generates and reasons on a dynamic graph , where irrelevant entities are masked out while only reasoning sources are preserved , via a mask prediction module .","In
each round
DFGN
generates and reasons
on
dynamic graph
irrelevant entities
are
masked out
while
reasoning sources
are
preserved
via
mask prediction module","generates and reasons||on||dynamic graph
irrelevant entities||are||masked out
masked out||while||reasoning sources
reasoning sources||are||preserved
preserved||via||mask prediction module
","each round||has||DFGN
DFGN||has||generates and reasons
dynamic graph||has||irrelevant entities
","Model||In||each round
",,,,,,
model,"To solve the second challenge , we propose a fusion process in DFGN to solve the unrestricted QA challenge .","fusion process
in
DFGN
to solve
unrestricted QA challenge","fusion process||in||DFGN
DFGN||to solve||unrestricted QA challenge
",,,"Model||propose||fusion process
",,,,,
model,"We not only aggregate information from documents to the entity graph ( doc2 graph ) , but also propagate the information of the entity graph back to document representations ( graph2doc ) .","aggregate
information
from
documents
to
entity graph
doc2 graph
propagate
information
of
entity graph
back to
document representations","entity graph||back to||document representations
information||from||documents
documents||to||entity graph
","documents||name||doc2 graph
","Model||propagate||information
Model||aggregate||information
",,,,,,
model,"The fusion process is iteratively performed at each hop through the document tokens and entities , and the final resulting answer is then obtained from document tokens .","fusion process
is
iteratively performed
at
each hop
through
document tokens and entities
final resulting answer
obtained from
document tokens","fusion process||is||iteratively performed
iteratively performed||through||document tokens and entities
iteratively performed||at||each hop
final resulting answer||obtained from||document tokens
","iteratively performed||has||final resulting answer
",,"Model||has||fusion process
",,,,,
model,"The fusion process of doc2 graph and graph2doc along with the dynamic entity graph jointly improve the interaction between the information of documents and the entity graph , leading to a less noisy entity graph and thus more accurate answers .","of
doc2 graph and graph2doc
along with
dynamic entity graph
jointly improve
interaction
between
information
of
documents
entity graph
leading to
less noisy entity graph
more accurate answers","information||of||entity graph
doc2 graph and graph2doc||along with||dynamic entity graph
interaction||leading to||less noisy entity graph
interaction||between||information
information||of||documents
information||of||entity graph
","doc2 graph and graph2doc||has||jointly improve
jointly improve||has||interaction
less noisy entity graph||has||more accurate answers
",,,,"fusion process||of||doc2 graph and graph2doc
",,,
hyperparameters,"In paragraph selection stage , we use the uncased version of BERT Tokenizer to tokenize all passages and questions .","In
paragraph selection stage
use
uncased version of BERT Tokenizer
to tokenize
all passages and questions","paragraph selection stage||use||uncased version of BERT Tokenizer
uncased version of BERT Tokenizer||to tokenize||all passages and questions
",,"Hyperparameters||In||paragraph selection stage
",,,,,,
hyperparameters,The encoding vectors of sentence pairs are generated from a pre-trained BERT model .,"encoding vectors
of
sentence pairs
generated from
pre-trained BERT model","encoding vectors||of||sentence pairs
encoding vectors||generated from||pre-trained BERT model
",,,"Hyperparameters||has||encoding vectors
",,,,,
hyperparameters,We set a relatively low threshold during selection to keep a high recall ( 97 % ) and a reasonable precision ( 69 % ) on supporting facts .,"set
relatively low threshold
during
selection
to keep
high recall ( 97 % )
reasonable precision ( 69 % )
on
supporting facts","relatively low threshold||during||selection
selection||on||supporting facts
supporting facts||to keep||high recall ( 97 % )
supporting facts||to keep||reasonable precision ( 69 % )
",,"Hyperparameters||set||relatively low threshold
",,,,,,
hyperparameters,"In graph construction stage , we use a pretrained NER model from Stanford CoreNLP Toolkits 1 to extract named entities .","graph construction stage
use
pretrained NER model
from
Stanford CoreNLP Toolkits
to extract
named entities","graph construction stage||use||pretrained NER model
pretrained NER model||to extract||named entities
pretrained NER model||from||Stanford CoreNLP Toolkits
",,,"Hyperparameters||In||graph construction stage
",,,,,
hyperparameters,The maximum number of entities in a graph is set to be 40 .,"maximum number of entities
in
graph
set to
40","maximum number of entities||in||graph
maximum number of entities||set to||40
",,,"Hyperparameters||has||maximum number of entities
",,,,,
hyperparameters,Each entity node in the entity graphs has an average degree of 3.52 .,"Each entity node
in
entity graphs
average degree
of
3.52","Each entity node||in||entity graphs
average degree||of||3.52
","Each entity node||has||average degree
",,"Hyperparameters||has||Each entity node
",,,,,
hyperparameters,"In the encoding stage , we also use a pre-trained BERT model as the encoder , thus d 1 is 768 .","encoding stage
use
pre-trained BERT model
as
encoder
d 1
is
768","encoding stage||use||pre-trained BERT model
pre-trained BERT model||as||encoder
d 1||is||768
","pre-trained BERT model||has||d 1
",,"Hyperparameters||In||encoding stage
",,,,,
hyperparameters,All the hidden state dimensions d 2 are set to 300 .,"hidden state dimensions d 2
set to
300","hidden state dimensions d 2||set to||300
",,,"Hyperparameters||has||hidden state dimensions d 2
",,,,,
hyperparameters,We set the dropout rate for all hidden units of LSTM and dynamic graph attention to 0.3 and 0.5 respectively .,"dropout rate
for
all hidden units
of
LSTM and dynamic graph attention
to
0.3 and 0.5","dropout rate||for||all hidden units
all hidden units||of||LSTM and dynamic graph attention
LSTM and dynamic graph attention||to||0.3 and 0.5
",,,"Hyperparameters||set||dropout rate
",,,,,
hyperparameters,"For optimization , we use Adam Optimizer with an initial learning rate of 1 e ?4 .","For
optimization
use
Adam Optimizer
with
initial learning rate
of
1 e ?4","optimization||use||Adam Optimizer
Adam Optimizer||with||initial learning rate
initial learning rate||of||1 e ?4
",,"Hyperparameters||For||optimization
",,,,,,
results,We first present a comparison between baseline models and our DFGN 2 . shows the performance of different models in the private test set of Hotpot QA .,"of
in
private test set
Hotpot QA","private test set||of||Hotpot QA
",,"Results||in||private test set
",,,,,,
results,From the table we can see that our model achieves the second best result on the leaderboard now 3 ( on March 1st ) .,"see that
our model
achieves
second best result
on
leaderboard
March 1st","our model||achieves||second best result
second best result||on||leaderboard
second best result||on||March 1st
",,,,,"Hotpot QA||see that||our model
",,,
results,"Besides , the answer performance and the joint performance of our model are competitive against state - of - the - art unpublished models .","joint performance
of
our model
are
competitive
against
state - of - the - art unpublished models","joint performance||of||our model
our model||are||competitive
competitive||against||state - of - the - art unpublished models
",,,"Results||has||joint performance
",,,,,
results,The results show that our model achieves a 1.5 % gain in the joint F1 - score with the entity graph built from a better entity recognizer .,"show
our model
achieves
1.5 % gain
in
joint F1 - score
with
entity graph
built from
better entity recognizer","our model||achieves||1.5 % gain
1.5 % gain||with||entity graph
entity graph||built from||better entity recognizer
1.5 % gain||in||joint F1 - score
",,"Results||show||our model
",,,,,,
ablation-analysis,The ablation results of QA performances in the development set of Hotpot QA are shown in .,"of
QA performances
in
development set
of
Hotpot QA","QA performances||in||development set
development set||of||Hotpot QA
",,"Ablation analysis||of||QA performances
",,,,,,
ablation-analysis,From the table we can see that each of our model components can provide from 1 % to 2 % relative gain over the QA performance .,"see that
each of our model components
provide
from 1 % to 2 %
relative gain
over
QA performance","each of our model components||provide||from 1 % to 2 %
relative gain||over||QA performance
","from 1 % to 2 %||has||relative gain
",,,,"development set||see that||each of our model components
",,,
ablation-analysis,"Particularly , using a 1 - layer fusion block leads to an obvious performance loss , which implies the significance of performing multi-hop reasoning in Hotpot QA .","using
1 - layer fusion block
leads to
obvious performance loss
implies
significance
of
performing
multi-hop reasoning
in
Hotpot QA","1 - layer fusion block||leads to||obvious performance loss
obvious performance loss||implies||significance
significance||of||performing
multi-hop reasoning||in||Hotpot QA
","performing||has||multi-hop reasoning
","Ablation analysis||using||1 - layer fusion block
",,,,,,
ablation-analysis,"Besides , the dataset abla-tion results show that our model is not very sensitive to the noisy paragraphs comparing with the baseline model which can achieve a more than 5 % performance gain in the "" gold paragraphs only "" and "" supporting facts only "" settings .","dataset abla-tion results
show
our model
not very sensitive
to
noisy paragraphs
comparing with
baseline model
can achieve
more than 5 % performance gain
in
gold paragraphs only
supporting facts only","dataset abla-tion results||show||our model
not very sensitive||can achieve||more than 5 % performance gain
more than 5 % performance gain||in||gold paragraphs only
more than 5 % performance gain||in||supporting facts only
not very sensitive||to||noisy paragraphs
not very sensitive||comparing with||baseline model
","our model||has||not very sensitive
",,"Ablation analysis||has||dataset abla-tion results
",,,,,
research-problem,Multi - Style Generative Reading Comprehension,Generative Reading Comprehension,,,,,"Contribution||has research problem||Generative Reading Comprehension
",,,,
research-problem,"This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) .",generative reading comprehension ( RC ),,,,,"Contribution||has research problem||generative reading comprehension ( RC )
",,,,
research-problem,"Recently , reading comprehension ( RC ) , a challenge to answer a question given textual evidence provided in a document set , has received much attention .",reading comprehension ( RC ),,,,,"Contribution||has research problem||reading comprehension ( RC )
",,,,
research-problem,"Current mainstream studies have treated RC as a process of extracting an answer span from one passage or multiple passages , which is usually done by predicting the start and end positions of the answer .",RC,,,,,"Contribution||has research problem||RC
",,,,
research-problem,"In this study , we propose Masque , a generative model for multi-passage RC .","propose
Masque
generative model
for
multi-passage RC","generative model||for||multi-passage RC
","Masque||has||generative model
","Model||propose||Masque
",,"Contribution||has research problem||multi-passage RC
",,,,
model,"We introduce the pointer - generator mechanism for generating an abstractive answer from the question and multiple passages , which covers various answer styles .","introduce
pointer - generator mechanism
for generating
abstractive answer
from
question
multiple passages
covers
various answer styles","pointer - generator mechanism||for generating||abstractive answer
abstractive answer||from||question
abstractive answer||from||multiple passages
multiple passages||covers||various answer styles
",,"Model||introduce||pointer - generator mechanism
",,,,,,
model,We extend the mechanism to a Transformer based one that allows words to be generated from a vocabulary and to be copied from the question and passages .,"extend
mechanism
to
Transformer based one
that allows
words
to be generated from
vocabulary
to be copied from
question
passages","mechanism||to||Transformer based one
Transformer based one||that allows||words
words||to be generated from||vocabulary
words||to be copied from||question
words||to be copied from||passages
",,"Model||extend||mechanism
",,,,,,
model,We introduce multi-style learning that enables our model to control answer styles and improves RC for all styles involved .,"multi-style learning
that enables
our model
to control
answer styles
improves
RC
for
all styles involved","multi-style learning||that enables||our model
our model||to control||answer styles
RC||for||all styles involved
","our model||has||improves
improves||has||RC
",,"Model||introduce||multi-style learning
",,,,,
model,"We also extend the pointer - generator to a conditional decoder by introducing an artificial token corresponding to each style , as in .","pointer - generator
to
conditional decoder
introducing
artificial token
corresponding to
each style","pointer - generator||introducing||artificial token
artificial token||corresponding to||each style
pointer - generator||to||conditional decoder
",,,"Model||extend||pointer - generator
",,,,,
model,"For each decoding step , it controls the mixture weights over three distributions with the given style ( ) .","For
each decoding step
controls
mixture weights
over
three distributions","each decoding step||controls||mixture weights
mixture weights||over||three distributions
",,"Model||For||each decoding step
",,,,,,
,Does our model achieve state - of - the - art performance ?,,,,,,,,,,
results,"shows that our single model , trained with two styles and controlled with the NQA style , pushed forward the state - of - the - art by a significant margin .","shows
our single model
trained with
two styles
controlled with
NQA style
pushed forward
state - of - the - art
by
significant margin","our single model||pushed forward||state - of - the - art
state - of - the - art||by||significant margin
our single model||controlled with||NQA style
our single model||trained with||two styles
",,,"Results||has||our single model
",,,,,
results,The evaluation scores of the model controlled with the NLG style were low because the two styles are different .,"evaluation scores
of
model
controlled with
NLG style
were
low","evaluation scores||of||model
model||were||low
model||controlled with||NLG style
",,,"Results||has||evaluation scores
",,,,,
results,"Also , our model without multi-style learning ( trained with only the NQA style ) outperformed the baselines in terms of ROUGE - L .","our model
without
multi-style learning
outperformed
baselines
in terms of
ROUGE - L","outperformed||in terms of||ROUGE - L
our model||without||multi-style learning
","our model||has||outperformed
outperformed||has||baselines
",,"Results||has||our model
",,,,,
,Experiments on NarrativeQA,,,,,,,,,,
research-problem,Explicit Contextual Semantics for Text Comprehension,Text Comprehension,,,,,"Contribution||has research problem||Text Comprehension
",,,,
research-problem,"This paper focuses on two core text comprehension ( TC ) tasks , machine reading comprehension ( MRC ) and textual entailment ( TE ) .","text comprehension ( TC )
machine reading comprehension ( MRC )
textual entailment ( TE )",,,,,"Contribution||has research problem||text comprehension ( TC )
Contribution||has research problem||machine reading comprehension ( MRC )
Contribution||has research problem||textual entailment ( TE )
",,,,
model,"In this work , to alleviate such an obvious shortcoming about semantics , we make attempt to explore integrative models for finer - grained text comprehension and inference .","explore
integrative models
for
finer - grained text comprehension and inference","integrative models||for||finer - grained text comprehension and inference
",,"Model||explore||integrative models
",,,,,,
model,"In this work , we propose a semantics enhancement framework for TC tasks , which boosts the strong baselines effectively .","propose
semantics enhancement framework
for
TC tasks","semantics enhancement framework||for||TC tasks
",,"Model||propose||semantics enhancement framework
",,,,,,
model,We implement an easy and feasible scheme to integrate semantic signals in downstream neural models in end - to - end manner to boost strong baselines effectively .,"implement
easy and feasible scheme
to integrate
semantic signals
in
downstream neural models
in
end - to - end manner
to boost
strong baselines
effectively","easy and feasible scheme||to integrate||semantic signals
semantic signals||in||downstream neural models
downstream neural models||in||end - to - end manner
downstream neural models||to boost||effectively
","effectively||has||strong baselines
","Model||implement||easy and feasible scheme
",,,,,,
experiments,Textual Entailment,Textual Entailment,,,,,,,,"Tasks||has||Textual Entailment
","Textual Entailment||has||Results
"
experiments,Machine Reading Comprehension,Machine Reading Comprehension,,,,,,,,"Tasks||has||Machine Reading Comprehension
","Machine Reading Comprehension||has||Baselines
"
experiments,Textual Entailment,,,,,,,,,,
experiments,Results in show that SRL embedding can boost the ESIM + ELMo model by + 0.7 % improvement .,"show
SRL embedding
boost
ESIM + ELMo model
by
+ 0.7 % improvement","ESIM + ELMo model||by||+ 0.7 % improvement
","SRL embedding||has||boost
boost||has||ESIM + ELMo model
",,,,"Results||show||SRL embedding
",,,
experiments,"With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .","With
semantic cues
simple sequential encoding model
yields
substantial gains
our single BERT LARGE model
achieves
new stateof - the - art
outperforms
all the ensemble models
in
leaderboard","simple sequential encoding model||yields||substantial gains
our single BERT LARGE model||achieves||new stateof - the - art
all the ensemble models||in||leaderboard
","semantic cues||has||simple sequential encoding model
semantic cues||has||our single BERT LARGE model
our single BERT LARGE model||has||outperforms
outperforms||has||all the ensemble models
",,,,"Results||With||semantic cues
",,,
experiments,Machine Reading Comprehension,,,,,,,,,,
experiments,"Our baseline includes MQAN for single task and multi-task with SRL , BiDAF + ELMo , R.M. Reader and BERT .","includes
MQAN
for
single task and multi-task
with
SRL
BiDAF + ELMo
R.M. Reader
BERT","MQAN||for||single task and multi-task
single task and multi-task||with||SRL
single task and multi-task||with||BiDAF + ELMo
single task and multi-task||with||R.M. Reader
single task and multi-task||with||BERT
",,,,,"Baselines||includes||MQAN
",,,
experiments,"9 . The SRL embeddings give substantial performance gains over all the strong baselines , showing it is also quite effective for more complex document and question encoding .","SRL embeddings
give
substantial performance gains
over
all the strong baselines
showing
quite effective
for
more complex document and question encoding","SRL embeddings||give||substantial performance gains
substantial performance gains||over||all the strong baselines
SRL embeddings||showing||quite effective
quite effective||for||more complex document and question encoding
",,,,,,,"Results||has||SRL embeddings
",
research-problem,Simple and Effective Multi - Paragraph Reading Comprehension,Multi - Paragraph Reading Comprehension,,,,,"Contribution||has research problem||Multi - Paragraph Reading Comprehension
",,,,
research-problem,We consider the problem of adapting neural paragraph - level question answering models to the case where entire documents are given as input .,neural paragraph - level question answering,,,,,"Contribution||has research problem||neural paragraph - level question answering
",,,,
research-problem,The recent success of neural models at answering questions given a related paragraph suggests neural models have the potential to be a key part of a solution to this problem .,answering questions given a related paragraph,,,,,"Contribution||has research problem||answering questions given a related paragraph
",,,,
model,In this paper we start by proposing an improved pipelined method which achieves state - of - the - art results .,"proposing
improved pipelined method",,,"Model||proposing||improved pipelined method
",,,,,,
model,"Then we introduce a method for training models to produce accurate per-paragraph confidence scores , and we show how combining this method with multiple paragraph selection further increases performance .","introduce
method
for training
models
to produce
accurate per-paragraph confidence scores","method||for training||models
models||to produce||accurate per-paragraph confidence scores
",,"Model||introduce||method
",,,,,,
model,We propose a TF - IDF heuristic to select which paragraphs to train and test on .,"propose
TF - IDF heuristic
to select
paragraphs
to
train and test on","TF - IDF heuristic||to select||paragraphs
paragraphs||to||train and test on
",,"Model||propose||TF - IDF heuristic
",,,,,,
model,"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output over all locations the answer text occurs .","use
summed objective function
that marginalizes
model 's output
over
all locations
answer text
occurs","summed objective function||that marginalizes||model 's output
model 's output||over||all locations
all locations||occurs||answer text
",,"Model||use||summed objective function
",,,,,,
model,"We resolve these problems by sampling paragraphs from the context documents , including paragraphs that do not contain an answer , to train on .","sampling
paragraphs
from
context documents
including
paragraphs
do not contain
answer
to
train on","paragraphs||including||paragraphs
paragraphs||do not contain||answer
answer||to||train on
paragraphs||from||context documents
",,"Model||sampling||paragraphs
",,,,,,
model,"We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .","shared - normalization objective
where
paragraphs
processed
independently
probability
of
answer candidate
is
marginalized
over
all paragraphs
sampled from
same document","shared - normalization objective||where||paragraphs
paragraphs||processed||independently
probability||of||answer candidate
probability||is||marginalized
marginalized||over||all paragraphs
all paragraphs||sampled from||same document
","paragraphs||has||probability
",,"Model||use||shared - normalization objective
",,,,,
hyperparameters,"We train the model with the Adadelta optimizer ( Zeiler , 2012 ) with a batch size 60 for Triv - ia QA and 45 for SQuAD .","train
model
with
Adadelta optimizer ( Zeiler , 2012 )
with
batch size
60
for
Triv - ia QA
45
for
SQuAD","model||with||Adadelta optimizer ( Zeiler , 2012 )
Adadelta optimizer ( Zeiler , 2012 )||with||batch size
45||for||SQuAD
60||for||Triv - ia QA
","batch size||has||45
batch size||has||60
","Hyperparameters||train||model
",,,,,,
hyperparameters,The Glo Ve 300 dimensional word vectors released by are used for word embeddings .,"Glo Ve 300 dimensional word vectors
used for
word embeddings","Glo Ve 300 dimensional word vectors||used for||word embeddings
",,,"Hyperparameters||has||Glo Ve 300 dimensional word vectors
",,,,,
hyperparameters,"On SQuAD , we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism .","On
SQuAD
use
dimensionality
of size
100
for
GRUs
200
for
linear layers
employed after
each attention mechanism","SQuAD||use||dimensionality
dimensionality||of size||100
100||for||GRUs
dimensionality||of size||200
200||for||linear layers
linear layers||employed after||each attention mechanism
",,"Hyperparameters||On||SQuAD
",,,,,,
hyperparameters,"We find for TriviaQA , likely because there is more data , using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial .","for
TriviaQA
is
using
larger dimensionality
of
140
for
each GRU
280
for
linear layers
beneficial","TriviaQA||using||larger dimensionality
larger dimensionality||of||280
280||for||linear layers
linear layers||is||beneficial
larger dimensionality||of||140
140||for||each GRU
",,"Hyperparameters||for||TriviaQA
",,,,,,
hyperparameters,"During training , we maintain an exponential moving average of the weights with a decay rate of 0.999 .","During
training
maintain
exponential moving average
of
weights
with
decay rate
of
0.999","training||maintain||exponential moving average
exponential moving average||with||decay rate
decay rate||of||0.999
exponential moving average||of||weights
",,"Hyperparameters||During||training
",,,,,,
results,Trivia QA Web,Trivia QA Web,,,,"Results||has||Trivia QA Web
",,,,,
results,We find both TF - IDF ranking and the sum objective to be effective ; even without changing the model we achieve state - of - the - art results .,"find
TF - IDF ranking
sum objective
to be
effective","effective||to be||TF - IDF ranking
effective||to be||sum objective
",,,,,"Trivia QA Web||find||effective
",,,
results,Using our refined model increases the gain by another 4 points .,"Using
refined model
increases
gain
by
another 4 points","gain||by||another 4 points
","refined model||has||increases
increases||has||gain
",,,,"Trivia QA Web||Using||refined model
",,,
results,"The shared - norm , merge , and no-answer training methods improve the model 's ability to utilize more text , with the shared - norm method being significantly ahead of the others on the verified set and tied with the merge approach on the general set .","shared - norm , merge , and no-answer training methods
improve
model 's ability
to utilize
more text
with
shared - norm method
being
significantly ahead
of
others
on
verified set
tied
with
merge approach
on
general set","improve||with||shared - norm method
shared - norm method||being||tied
tied||with||merge approach
merge approach||on||general set
shared - norm method||being||significantly ahead
significantly ahead||of||others
others||on||verified set
model 's ability||to utilize||more text
","shared - norm , merge , and no-answer training methods||has||improve
improve||has||model 's ability
",,,,,,"Trivia QA Web||has||shared - norm , merge , and no-answer training methods
",
results,Trivia QA Unfiltered,Trivia QA Unfiltered,,,,"Results||has||Trivia QA Unfiltered
",,,,,"Trivia QA Unfiltered||has||base model
"
results,"Note the base model starts to lose performance as more paragraphs are used , showing that errors are being caused by the model being overly confident in incorrect extractions . :","base model
starts to
lose
performance
as
more paragraphs
are
used","base model||starts to||lose
lose||as||more paragraphs
more paragraphs||are||used
","lose||has||performance
",,,,,,,
results,SQuAD,SQuAD,,,,"Results||has||SQuAD
",,,,,"SQuAD||has||shared - norm model
SQuAD||has||all our approaches
"
results,"While all our approaches had some benefit , the shared - norm model is the strongest , and is the only one to not lose performance as large numbers of paragraphs are used .","all our approaches
some benefit
shared - norm model
is
strongest
not lose
performance
as
large numbers of paragraphs
are
used","shared - norm model||is||strongest
performance||as||large numbers of paragraphs
large numbers of paragraphs||are||used
","shared - norm model||has||not lose
not lose||has||performance
all our approaches||has||some benefit
",,,,,,,
results,"Our paragraph - level model is competitive on this task , and our variations to handle the multi-paragraph setting only cause a minor loss of performance .","Our paragraph - level model
is
competitive
our variations
to handle
multi-paragraph setting
cause
minor loss
of
performance","our variations||to handle||multi-paragraph setting
multi-paragraph setting||cause||minor loss
minor loss||of||performance
Our paragraph - level model||is||competitive
",,,,,,,"SQuAD||has||our variations
SQuAD||has||Our paragraph - level model
",
results,The base model starts to drop in performance once more than two paragraphs are used .,"base model
starts to
drop
in
performance
once
more than two paragraphs
are
used","base model||starts to||drop
drop||in||performance
drop||once||more than two paragraphs
more than two paragraphs||are||used
",,,,,,,"SQuAD||has||base model
",
results,"However , the shared - norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs .","shared - norm approach
able to
reach
peak performance
of
72.37 F1
64.08 EM
given
15 paragraphs","shared - norm approach||able to||reach
peak performance||given||15 paragraphs
peak performance||of||72.37 F1
peak performance||of||64.08 EM
","reach||has||peak performance
",,,,,,"SQuAD||has||shared - norm approach
",
research-problem,MEMEN : Multi-layer Embedding with Memory Networks for Machine Comprehension,Machine Comprehension,,,,,"Contribution||has research problem||Machine Comprehension
",,,,
research-problem,Machine comprehension ( MC ) style question answering is a representative problem in natural language processing .,Machine comprehension ( MC ) style question answering,,,,,"Contribution||has research problem||Machine comprehension ( MC ) style question answering
",,,,
research-problem,Machine comprehension ( MC ) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language processing and artificial intelligence .,Machine comprehension ( MC ),,,,,"Contribution||has research problem||Machine comprehension ( MC )
",,,,
model,"In this paper , we introduce the Multi - layer Embedding with Memory Networks ( MEMEN ) , an end - to - end neural network for machine comprehension task .","introduce
Multi - layer Embedding with Memory Networks ( MEMEN )
end - to - end neural network
for
machine comprehension task","end - to - end neural network||for||machine comprehension task
","Multi - layer Embedding with Memory Networks ( MEMEN )||has||end - to - end neural network
","Model||introduce||Multi - layer Embedding with Memory Networks ( MEMEN )
",,,,,,
model,Our model consists of three parts :,"consists of
three parts",,,"Model||consists of||three parts
",,,,,,"three parts||has||high - efficiency multilayer memory network
three parts||has||pointer - network based answer boundary prediction layer
three parts||has||encoding
"
model,"1 ) the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .","encoding
of
context and query
in which
add
useful syntactic and semantic information
in
embedding
of
every word
high - efficiency multilayer memory network
of
full - orientation matching
to match
question and context
pointer - network based answer boundary prediction layer
to get
location
of
answer
in
passage","high - efficiency multilayer memory network||of||full - orientation matching
full - orientation matching||to match||question and context
pointer - network based answer boundary prediction layer||to get||location
location||of||answer
answer||in||passage
encoding||in which||add
useful syntactic and semantic information||in||embedding
embedding||of||every word
encoding||of||context and query
","add||has||useful syntactic and semantic information
",,,,,,,
experimental-setup,The tokenizers we use in the step of preprocessing data are from Stanford CoreNLP .,"tokenizers
use in
step
of
preprocessing
data
from
Stanford CoreNLP","tokenizers||use in||step
step||of||preprocessing
tokenizers||from||Stanford CoreNLP
","preprocessing||has||data
",,"Experimental setup||has||tokenizers
",,,,,
experimental-setup,We also use part - of - speech tagger and named - entity recognition tagger in Stanford CoreNLP utilities to transform the passage and question .,"use
part - of - speech tagger
named - entity recognition tagger
in
Stanford CoreNLP utilities
to transform
passage and question","passage and question||in||Stanford CoreNLP utilities
Stanford CoreNLP utilities||use||part - of - speech tagger
Stanford CoreNLP utilities||use||named - entity recognition tagger
",,"Experimental setup||to transform||passage and question
",,,,,,
experimental-setup,"For the skip - gram model , our model refers to the word2 vec module in open source software library , Tensorflow , the skip window is set as 2 .","For
skip - gram model
our model
refers to
word2 vec module
in
open source software library
Tensorflow
skip window
set as
2","our model||refers to||word2 vec module
word2 vec module||in||open source software library
skip window||set as||2
","skip - gram model||has||our model
open source software library||name||Tensorflow
word2 vec module||has||skip window
","Experimental setup||For||skip - gram model
",,,,,,
experimental-setup,"To improve the reliability and stabllity , we screen out the sentences whose length are shorter than 9 .","To improve
reliability and stabllity
screen out
sentences
whose
length
are
shorter than 9","reliability and stabllity||screen out||sentences
sentences||whose||length
length||are||shorter than 9
",,"Experimental setup||To improve||reliability and stabllity
",,,,,,
experimental-setup,"We use 100 one dimensional filters for CNN in the character level embedding , with width of 5 for each one .","use
100 one dimensional
filters
for
CNN
in
character level embedding
with
width
of
5
for
each one","filters||with||width
width||of||5
5||for||each one
filters||for||CNN
CNN||in||character level embedding
","100 one dimensional||has||filters
","Experimental setup||use||100 one dimensional
",,,,,,
experimental-setup,We set the hidden size as 100 for all the LSTM and GRU layers and apply dropout between layers with a dropout ratio as 0.2 .,"set
hidden size
as
100
for
all the LSTM and GRU layers
apply
dropout
between
layers
with
dropout ratio
as
0.2","hidden size||as||100
100||for||all the LSTM and GRU layers
dropout||between||layers
layers||with||dropout ratio
dropout ratio||as||0.2
",,"Experimental setup||set||hidden size
Experimental setup||apply||dropout
",,,,,,
experimental-setup,"We use the AdaDelta ( Zeiler , 2012 ) optimizer with a initial learning rate as 0.001 .","AdaDelta ( Zeiler , 2012 ) optimizer
with
initial learning rate
as
0.001","AdaDelta ( Zeiler , 2012 ) optimizer||with||initial learning rate
initial learning rate||as||0.001
",,,"Experimental setup||use||AdaDelta ( Zeiler , 2012 ) optimizer
",,,,,
experimental-setup,"For the memory networks , we set the number of layer as 3 .","memory networks
set
number of layer
as
3","memory networks||set||number of layer
number of layer||as||3
",,,"Experimental setup||For||memory networks
",,,,,
results,"As we can see in , our model outperforms all other baselines and achieves the state - of - the - art result on all subsets on TriviaQA .","see
our model
outperforms
all other baselines
achieves
state - of - the - art result
on
all subsets on TriviaQA","our model||achieves||state - of - the - art result
state - of - the - art result||on||all subsets on TriviaQA
","our model||has||outperforms
outperforms||has||all other baselines
","Results||see||our model
",,,,,,
results,We also use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .,"use
Stanford Question Answering Dataset ( SQuAD ) v 1.1
to",,,"Results||use||Stanford Question Answering Dataset ( SQuAD ) v 1.1
",,,,,,"Stanford Question Answering Dataset ( SQuAD ) v 1.1||has||our model
"
results,"The results of this dataset are all exhibited on a leaderboard , and top methods are almost all ensemble models , our model achieves an exact match score of 75.37 % and an F1 score of 82 . 66 % , which is competitive to state - of - the - art method .","of
our model
achieves
exact match score
of
75.37 %
F1 score
82 . 66 %
is
competitive
state - of - the - art method","our model||is||competitive
state - of - the - art method||achieves||exact match score
exact match score||of||75.37 %
state - of - the - art method||achieves||F1 score
F1 score||of||82 . 66 %
","competitive||to||state - of - the - art method
",,,,,,,
ablation-analysis,We also run the ablations of our single model on SQ u AD dev set to evaluate the individual contribution .,"of
single model
on
SQ u AD dev set","single model||on||SQ u AD dev set
",,"Ablation analysis||of||single model
",,,,,,"single model||has||contribute
"
ablation-analysis,"As shows , both syntactic embeddings and semantic embeddings contribute towards the model 's performance and the POS tags seem to be more important .","both
syntactic embeddings
semantic embeddings
contribute
towards
model 's performance
POS tags
to be
more important","contribute||towards||model 's performance
POS tags||to be||more important
model 's performance||both||syntactic embeddings
model 's performance||both||semantic embeddings
","model 's performance||has||POS tags
",,,,,,,
ablation-analysis,"For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of query for each word in passage is crucial .","For ablating
integral query matching
result
drops
about
2 %
shows that
integral information
of
query
for
each word
in
passage
is
crucial","integral query matching||shows that||integral information
integral information||of||query
query||for||each word
each word||in||passage
each word||is||crucial
drops||about||2 %
","integral query matching||has||result
result||has||drops
","Ablation analysis||For ablating||integral query matching
",,,,,,
ablation-analysis,"The query - based similarity matching accounts for about 10 % performance degradation , which proves the effectiveness of alignment context words against query .","query - based similarity matching
accounts for
about 10 % performance degradation
proves
effectiveness
of
alignment context words
against
query","query - based similarity matching||accounts for||about 10 % performance degradation
about 10 % performance degradation||proves||effectiveness
effectiveness||of||alignment context words
alignment context words||against||query
",,,"Ablation analysis||has||query - based similarity matching
",,,,,
ablation-analysis,"For context - based similarity matching , we simply took out the M 3 from the linear function and it is proved to be contributory to the performance of full - orientation matching .","For
context - based similarity matching
took out
M 3
from
linear function
proved to be
contributory
to
performance
of
full - orientation matching","context - based similarity matching||took out||M 3
M 3||proved to be||contributory
contributory||to||performance
performance||of||full - orientation matching
M 3||from||linear function
",,"Ablation analysis||For||context - based similarity matching
",,,,,,
research-problem,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,Semantic Similarity Measurement,,,,,"Contribution||has research problem||Semantic Similarity Measurement
",,,,
research-problem,"Textual similarity measurement is a challenging problem , as it requires understanding the semantics of input sentences .",Textual similarity measurement,,,,,"Contribution||has research problem||Textual similarity measurement
",,,,
research-problem,"Given two pieces of text , measuring their semantic textual similarity ( STS ) remains a fundamental problem in language research and lies at the core of many language processing tasks , including question answering , query ranking , and paraphrase generation .",semantic textual similarity ( STS ),,,,,"Contribution||has research problem||semantic textual similarity ( STS )
",,,,
approach,"In contrast , we focus on capturing fine - grained word - level information directly .","focus on
capturing
fine - grained word - level information",,"capturing||has||fine - grained word - level information
","Approach||focus on||capturing
",,,,,,
approach,"First , instead of using sentence modeling , we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences .","instead of using
sentence modeling
propose
pairwise word interaction modeling
that encourages
explicit word context interactions
across
sentences","sentence modeling||propose||pairwise word interaction modeling
pairwise word interaction modeling||that encourages||explicit word context interactions
explicit word context interactions||across||sentences
",,"Approach||instead of using||sentence modeling
",,,,,,
approach,"Second , based on the pairwise word interactions , we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement .","based on
pairwise word interactions
describe
novel similarity focus layer
which
helps
model
selectively identify
important word interactions
for
similarity measurement","pairwise word interactions||describe||novel similarity focus layer
novel similarity focus layer||which||helps
model||selectively identify||important word interactions
important word interactions||for||similarity measurement
","helps||has||model
","Approach||based on||pairwise word interactions
",,,,,,
experimental-setup,"For the SICK and MSRVID experiments , we used 300 - dimension Glo Ve word embeddings .","For
SICK and MSRVID experiments
used
300 - dimension
Glo Ve word embeddings","SICK and MSRVID experiments||used||300 - dimension
","300 - dimension||has||Glo Ve word embeddings
","Experimental setup||For||SICK and MSRVID experiments
",,,,,,
experimental-setup,"For the STS2014 , WikiQA , and TrecQA experiments , we used 300 dimension PARAGRAM - SL999 embeddings from and the PARAGRAM - PHRASE embeddings from , trained on word pairs from the Paraphrase Database ( PPDB ) .","STS2014 , WikiQA , and TrecQA experiments
used
300 dimension
PARAGRAM - SL999 embeddings
from
PARAGRAM - PHRASE embeddings
trained on
word pairs
Paraphrase Database ( PPDB )","STS2014 , WikiQA , and TrecQA experiments||trained on||word pairs
word pairs||from||Paraphrase Database ( PPDB )
Paraphrase Database ( PPDB )||used||300 dimension
","300 dimension||has||PARAGRAM - SL999 embeddings
300 dimension||has||PARAGRAM - PHRASE embeddings
",,"Experimental setup||For||STS2014 , WikiQA , and TrecQA experiments
",,,,,
experimental-setup,Our timing experiments were conducted on an Intel Xeon E5 - 2680 CPU .,"Our timing experiments
conducted on
Intel Xeon E5 - 2680 CPU","Our timing experiments||conducted on||Intel Xeon E5 - 2680 CPU
",,,"Experimental setup||has||Our timing experiments
",,,,,
experimental-setup,"Due to sentence length variations , for the SICK and MSRVID data we padded the sentences to 32 words ; for the STS2014 , WikiQA , and TrecQA data , we padded the sentences to 48 words ..","Due to
sentence length variations
for
SICK and MSRVID data
padded
sentences
to
32 words
STS2014 , WikiQA , and TrecQA data
padded
sentences
to
48 words","sentence length variations||for||SICK and MSRVID data
SICK and MSRVID data||padded||sentences
sentences||to||32 words
sentence length variations||for||STS2014 , WikiQA , and TrecQA data
STS2014 , WikiQA , and TrecQA data||padded||sentences
sentences||to||48 words
",,"Experimental setup||Due to||sentence length variations
",,,,,,
results,Wiki QA Results .,Wiki QA,,,,"Results||has||Wiki QA
",,,,,"Wiki QA||has||paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features
"
results,"The neural network models in the table , paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features , are mostly based on sentence modeling .","paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features",,,,,,,,,"paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features||has||Our model
"
results,Our model outperforms them all .,"Our model
outperforms",,"Our model||has||outperforms
",,,,,,,
ablation-analysis,"We found large drops when removing the context modeling component , indicating that the context information provided by the Bi - LSTMs is crucial for the following components ( e.g. , interaction modeling ) .","found
large drops
removing
context modeling component","large drops||removing||context modeling component
",,"Ablation analysis||found||large drops
",,,,,,
ablation-analysis,"The use of our similarity focus layer is also beneficial , especially on the WikiQA data .","use
similarity focus layer
is
beneficial
on
WikiQA data","similarity focus layer||is||beneficial
beneficial||on||WikiQA data
",,"Ablation analysis||use||similarity focus layer
",,,,,,
ablation-analysis,"When we replaced the entire similarity focus layer with a random dropout layer ( p = 0.3 ) , the dropout layer hurts accuracy ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .","replaced
entire similarity focus layer
with
random dropout layer ( p = 0.3 )
hurts
accuracy","entire similarity focus layer||with||random dropout layer ( p = 0.3 )
","random dropout layer ( p = 0.3 )||has||hurts
hurts||has||accuracy
","Ablation analysis||replaced||entire similarity focus layer
",,,,,,
research-problem,DCN + : MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING,QUESTION ANSWERING,,,,,"Contribution||has research problem||QUESTION ANSWERING
",,,,
model,"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning .","propose
mixed objective
combines
traditional cross entropy loss
over
positions
with
measure of word overlap
trained with
reinforcement learning","mixed objective||combines||traditional cross entropy loss
traditional cross entropy loss||over||positions
traditional cross entropy loss||with||measure of word overlap
measure of word overlap||trained with||reinforcement learning
",,"Model||propose||mixed objective
",,,,,,
model,We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,"obtain
latter objective
using
self - critical policy learning
in which
reward
based on
word overlap
between
proposed answer
ground truth answer","latter objective||using||self - critical policy learning
self - critical policy learning||in which||reward
reward||based on||word overlap
word overlap||between||proposed answer
word overlap||between||ground truth answer
",,"Model||obtain||latter objective
",,,,,,
model,"In addition to our mixed training objective , we extend the Dynamic Coattention Network ( DCN ) by with a deep residual coattention encoder .","extend
Dynamic Coattention Network ( DCN )
with
deep residual coattention encoder","Dynamic Coattention Network ( DCN )||with||deep residual coattention encoder
",,"Model||extend||Dynamic Coattention Network ( DCN )
",,,,,,
experimental-setup,"To preprocess the corpus , we use the reversible tokenizer from Stanford CoreNLP .","preprocess
corpus
use
reversible tokenizer
from
Stanford CoreNLP","corpus||use||reversible tokenizer
reversible tokenizer||from||Stanford CoreNLP
",,"Experimental setup||preprocess||corpus
",,,,,,
experimental-setup,"For word embeddings , we use GloVe embeddings pretrained on the 840B Common Crawl corpus as well as character ngram embeddings by .","For
word embeddings
use
GloVe embeddings
pretrained on
840B Common Crawl corpus","word embeddings||use||GloVe embeddings
GloVe embeddings||pretrained on||840B Common Crawl corpus
",,"Experimental setup||For||word embeddings
",,,,,,
experimental-setup,"In addition , we concatenate these embeddings with context vectors ( CoVe ) trained on .","concatenate
these embeddings
with
context vectors ( CoVe )","these embeddings||with||context vectors ( CoVe )
",,,,,"word embeddings||concatenate||these embeddings
",,,
experimental-setup,"For out of vocabulary words , we set the embeddings and context vectors to zero .","out of vocabulary words
set
embeddings and context vectors
to
zero","out of vocabulary words||set||embeddings and context vectors
embeddings and context vectors||to||zero
",,,"Experimental setup||For||out of vocabulary words
",,,,,
experimental-setup,We perform word dropout on the document which zeros a word embedding with probability 0.075 .,"perform
word dropout
on
document
zeros
word embedding
with
probability 0.075","word dropout||on||document
document||zeros||word embedding
word embedding||with||probability 0.075
",,"Experimental setup||perform||word dropout
",,,,,,
experimental-setup,"In addition , we swap the first maxout layer of the highway maxout network in the DCN decoder with a sparse mixture of experts layer .","swap
first maxout layer
of
highway maxout network
in
DCN decoder
with
sparse mixture
of
experts layer","first maxout layer||with||sparse mixture
sparse mixture||of||experts layer
first maxout layer||of||highway maxout network
highway maxout network||in||DCN decoder
",,"Experimental setup||swap||first maxout layer
",,,,,,
results,Comparison to baseline DCN with CoVe. DCN + outperforms the baseline by 3.2 % exact match accuracy and 3.2 % F1 on the SQuAD development set .,"Comparison to
DCN +
outperforms
baseline
by
3.2 % exact match accuracy
3.2 % F1
on
SQuAD development set","DCN +||outperforms||baseline
baseline||on||SQuAD development set
SQuAD development set||by||3.2 % exact match accuracy
SQuAD development set||by||3.2 % F1
",,,,,,,"baseline DCN with CoVe||has||DCN +
","Results||Comparison to||baseline DCN with CoVe
"
results,"shows the consistent performance gain of DCN + over the baseline across question types , question lengths , and answer lengths .","shows
consistent performance gain
of
DCN +
over
baseline
across
question types
question lengths
answer lengths","consistent performance gain||over||baseline
consistent performance gain||across||question types
consistent performance gain||across||question lengths
consistent performance gain||across||answer lengths
consistent performance gain||of||DCN +
",,"Results||shows||consistent performance gain
",,,,,,
results,"In particular , DCN + provides a significant advantage for long questions .","DCN +
provides
significant advantage
for
long questions","DCN +||provides||significant advantage
significant advantage||for||long questions
",,,"Results||has||DCN +
",,,,,
ablation-analysis,"We note that the deep residual coattention yielded the highest contribution to model performance , followed by the mixed objective .","note
deep residual coattention
yielded
highest contribution
to
model performance
followed by
mixed objective","deep residual coattention||yielded||highest contribution
highest contribution||followed by||mixed objective
highest contribution||to||model performance
",,"Ablation analysis||note||deep residual coattention
",,,,,,
ablation-analysis,The sparse mixture of experts layer in the decoder added minor improvements to the model performance . :,"sparse mixture
of
experts layer
in
decoder
added
minor improvements
to
model performance","sparse mixture||of||experts layer
experts layer||in||decoder
experts layer||added||minor improvements
minor improvements||to||model performance
",,,"Ablation analysis||has||sparse mixture
",,,,,
research-problem,Linguistic Knowledge as Memory for Recurrent Neural Networks,Recurrent Neural Networks,,,,,"Contribution||has research problem||Recurrent Neural Networks
",,,,
research-problem,Training recurrent neural networks to model long term dependencies is difficult .,Training recurrent neural networks to model long term dependencies,,,,,"Contribution||has research problem||Training recurrent neural networks to model long term dependencies
",,,,
approach,"Instead , we utilize the order inherent in the the unaugmented sequence to decompose the graph into two Directed Acyclic Graphs ( DAGs ) with a topological ordering .","utilize
order inherent
in
unaugmented sequence
to decompose
graph
into
two Directed Acyclic Graphs ( DAGs )
with
topological ordering","order inherent||in||unaugmented sequence
order inherent||to decompose||graph
graph||into||two Directed Acyclic Graphs ( DAGs )
two Directed Acyclic Graphs ( DAGs )||with||topological ordering
",,"Approach||utilize||order inherent
",,,,,,
approach,"We introduce the Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework to compute the representation of such graphs while touching every node only once , and implement a GRU version of it called MAGE - GRU .","introduce
Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework
to compute
representation
of
graphs
while
touching
every
node
only once
implement
GRU version
called
MAGE - GRU","Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework||to compute||representation
representation||of||graphs
representation||while||touching
touching||every||node
Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework||implement||GRU version
GRU version||called||MAGE - GRU
","touching||has||only once
","Approach||introduce||Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework
",,,,,,
approach,"MAGE - RNN learns separate representations for propagation along each edge type , which leads to superior performance empirically .","MAGE - RNN
learns
separate representations
for
propagation
along
each edge type
leads to
superior performance empirically","MAGE - RNN||learns||separate representations
separate representations||leads to||superior performance empirically
separate representations||for||propagation
propagation||along||each edge type
",,,"Approach||has||MAGE - RNN
",,,,,
approach,"We use MAGE - RNN to model coreference relations for text comprehension tasks , where answers to a query have to be extracted from a context document .","use
MAGE - RNN
to model
coreference relations
for
text comprehension tasks
where
answers to a query
have to be extracted from
context document","MAGE - RNN||to model||coreference relations
coreference relations||for||text comprehension tasks
text comprehension tasks||where||answers to a query
answers to a query||have to be extracted from||context document
",,"Approach||use||MAGE - RNN
",,,,,,"coreference relations||has||Tokens
"
approach,Tokens in a document are connected by a coreference relation if they refer to the same underlying entity .,"Tokens
in
document
connected by
coreference relation
if
refer
to
same underlying entity","Tokens||in||document
Tokens||connected by||coreference relation
coreference relation||if||refer
refer||to||same underlying entity
",,,,,,,,
results,Story Based,Story Based,,,,"Results||has||Story Based
",,,,,"Story Based||has||Our model
"
results,"Our model achieves new state - of - the - art results , outperforming strong baselines such as QRNs .","Our model
achieves
new state - of - the - art results
outperforming
strong baselines
such as
QRNs","Our model||achieves||new state - of - the - art results
strong baselines||such as||QRNs
","Our model||has||outperforming
outperforming||has||strong baselines
",,,,,,,
results,"Moreover , we observe that the proposed MAGE architecture can substantially improve the performance for both bi - GRUs and GAs .","observe
proposed MAGE architecture
substantially improve
performance
for
both bi - GRUs and GAs","performance||for||both bi - GRUs and GAs
","proposed MAGE architecture||has||substantially improve
substantially improve||has||performance
",,,,"Story Based||observe||proposed MAGE architecture
",,,
results,"Adding the same information as one - hot features fails to improve the performance , which indicates that the inductive bias we employ on MAGE is useful .","Adding
same information
as
one - hot features
fails to
improve
performance","same information||as||one - hot features
one - hot features||fails to||improve
","improve||has||performance
",,,,"Story Based||Adding||same information
",,,
results,"The DAG - RNN baseline from and the shared version of MAGE ( where edge representations are tied ) also perform worse , showing that our proposed architecture is superior .","DAG - RNN baseline
shared version of MAGE
perform
worse
showing
our proposed architecture
is
superior","our proposed architecture||perform||worse
our proposed architecture||is||superior
","worse||has||DAG - RNN baseline
worse||has||shared version of MAGE
",,,,"Story Based||showing||our proposed architecture
",,,
results,"Both variants of MAGE substantially outperform QRNs , which are the current state - of - the - art models on the bAbi dataset .","Both variants of MAGE
substantially outperform
QRNs
are
current state - of - the - art models
on
bAbi dataset","QRNs||are||current state - of - the - art models
current state - of - the - art models||on||bAbi dataset
","Both variants of MAGE||has||substantially outperform
substantially outperform||has||QRNs
",,,,,,"Story Based||has||Both variants of MAGE
",
results,Broad Context Language Modeling :,Broad Context Language Modeling,,,,"Results||has||Broad Context Language Modeling
",,,,,
results,"For our second benchmark we pick the LAMBADA dataset from , where the task is to predict the last word in a given passage .","pick
LAMBADA dataset",,,,,,"Broad Context Language Modeling||pick||LAMBADA dataset
",,,"LAMBADA dataset||has||Our implementation of GA
"
results,"Our implementation of GA gave higher performance than that reported by , without the use of linguistic features .","Our implementation of GA
gave
higher performance","Our implementation of GA||gave||higher performance
",,,,,,,,
results,"On the simple bi - GRU architecture we see an improvement of 1.7 % by incorporating coreference edges in the graph , whereas the one - hot baseline does not lead to any improvement .","On
simple bi - GRU architecture
see
improvement
of
1.7 %
by incorporating
coreference edges
in
graph","simple bi - GRU architecture||see||improvement
improvement||of||1.7 %
1.7 %||by incorporating||coreference edges
coreference edges||in||graph
",,,,,"LAMBADA dataset||On||simple bi - GRU architecture
",,,
results,"On the multi - layer GA architecture , the coreference edges again lead to an improvement of 2 % , setting a new state - of - theart on this dataset .","multi - layer GA architecture
coreference edges
lead to
improvement
of
2 %
setting
new state - of - theart","coreference edges||lead to||improvement
improvement||of||2 %
2 %||setting||new state - of - theart
","multi - layer GA architecture||has||coreference edges
",,,,,,"LAMBADA dataset||On||multi - layer GA architecture
",
results,"Cloze - style QA : Lastly , we test our models on the CNN dataset from , which consists of pairs of news articles and a cloze - style question over the contents .","Cloze - style QA
on
CNN dataset
of","Cloze - style QA||on||CNN dataset
",,,"Results||has||Cloze - style QA
",,,,,
results,Augmenting the bi - GRU model with MAGE leads to an improvement of 2.5 % on the test set .,"Augmenting
bi - GRU model
with
MAGE
leads to
improvement
2.5 %
on
test set","bi - GRU model||with||MAGE
MAGE||leads to||improvement
improvement||on||test set
","improvement||of||2.5 %
",,,,"CNN dataset||Augmenting||bi - GRU model
",,,
results,"The previous best results for this dataset were achieved by the GA Reader , and we see that adding MAGE to it leads to a further improvement of 0.7 % , setting a new state of the art .","previous best results
achieved by
GA Reader
adding
MAGE
leads to
further improvement
of
0.7 %
setting
new state of the art","previous best results||achieved by||GA Reader
GA Reader||adding||MAGE
MAGE||leads to||further improvement
further improvement||of||0.7 %
further improvement||setting||new state of the art
",,,,,,,"CNN dataset||has||previous best results
",
research-problem,"BART : Denoising Sequence - to - Sequence Pre-training for Natural Language Generation , Translation , and Comprehension",Sequence - to - Sequence Pre-training,,,,,"Contribution||has research problem||Sequence - to - Sequence Pre-training
",,,,
research-problem,"We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .",pretraining sequence - to - sequence models,,,,,"Contribution||has research problem||pretraining sequence - to - sequence models
",,,,
model,"In this paper , we present BART , which pre-trains a model combining Bidirectional and Auto - Regressive Transformers .","present
BART
pre-trains
model
combining
Bidirectional and Auto - Regressive Transformers","BART||pre-trains||model
model||combining||Bidirectional and Auto - Regressive Transformers
",,"Model||present||BART
",,,,,,
model,BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .,"BART
is
denoising autoencoder
built with
sequence - to - sequence model
applicable to
very wide range of end tasks","BART||is||denoising autoencoder
denoising autoencoder||built with||sequence - to - sequence model
denoising autoencoder||applicable to||very wide range of end tasks
",,,"Model||has||BART
",,,,,
model,"Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .","Pretraining
two stages
text
is
corrupted with
arbitrary noising function
sequence - to - sequence model
learned
to reconstruct
original text","sequence - to - sequence model||is||learned
learned||to reconstruct||original text
text||corrupted with||arbitrary noising function
","Pretraining||has||two stages
two stages||has||sequence - to - sequence model
two stages||has||text
",,"Model||has||Pretraining
",,,,,
model,"BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see .","uses
standard Tranformer - based neural machine translation architecture
seen as
generalizing
BERT
due to
bidirectional encoder
GPT
with
left - to - right decoder","standard Tranformer - based neural machine translation architecture||seen as||generalizing
BERT||due to||bidirectional encoder
GPT||with||left - to - right decoder
","generalizing||has||BERT
generalizing||has||GPT
",,,,"BART||uses||standard Tranformer - based neural machine translation architecture
",,,
experiments,"We pre-train a large model with 12 layers in each of the encoder and decoder , and a hidden size of 1024 .","pre-train
large model
with
12 layers
in
of
encoder and decoder
hidden size
1024","large model||with||12 layers
12 layers||in||encoder and decoder
large model||with||hidden size
hidden size||of||1024
",,,,,"Experimental setup||pre-train||large model
",,,
experiments,"Following RoBERTa , we use a batch size of 8000 , and train the model for 500000 steps .","Following
RoBERTa
use
batch size
of
8000
train
model
for
500000 steps","RoBERTa||use||batch size
batch size||of||8000
RoBERTa||train||model
model||for||500000 steps
",,,,,"Experimental setup||Following||RoBERTa
",,,
experiments,Documents are tokenized with the same byte - pair encoding as GPT - 2 .,"Documents
tokenized with
same byte - pair encoding
as
GPT - 2","Documents||tokenized with||same byte - pair encoding
same byte - pair encoding||as||GPT - 2
",,,,,,,"Experimental setup||has||Documents
",
experiments,"Based on the results in Section 4 , we use a combination of text infilling and sentence permutation .","use
combination
of
text infilling
sentence permutation","combination||of||text infilling
combination||of||sentence permutation
",,,,,"Experimental setup||use||combination
",,,
experiments,"We mask 30 % of tokens in each document , and permute all sentences .","mask
30 %
of
tokens
in
each document
permute
all sentences","30 %||of||tokens
tokens||in||each document
",,,,,"Experimental setup||permute||all sentences
Experimental setup||mask||30 %
",,,
experiments,"To help the model better fit the data , we dis abled dropout for the final 10 % of training steps .","To help
model
better fit
data
dis abled
dropout
for
final 10 %
of
training steps","dropout||for||final 10 %
final 10 %||of||training steps
final 10 %||To help||model
model||better fit||data
",,,,,"Experimental setup||dis abled||dropout
",,,
experiments,"The most directly comparable baseline is RoBERTa , which was pre-trained with the same resources , but a different objective .","RoBERTa
pre-trained with
same resources
different objective","RoBERTa||pre-trained with||same resources
RoBERTa||pre-trained with||different objective
",,,,,,,"Baselines||has||RoBERTa
",
experiments,We also experiment with several text generation tasks .,"experiment with
several text generation tasks",,,,,,"Tasks||experiment with||several text generation tasks
",,,
experiments,"To provide a comparison with the state - of - the - art in summarization , we present results on two summarization datasets , CNN / DailyMail and XSum , which have distinct properties .","present
results
on
two summarization datasets
CNN / DailyMail
XSum","results||on||two summarization datasets
","two summarization datasets||name||CNN / DailyMail
two summarization datasets||name||XSum
",,,,"several text generation tasks||present||results
",,,"two summarization datasets||has||BART
"
experiments,"Nevertheless , BART outperforms all existing work .","BART
outperforms",,"BART||has||outperforms
BART||has||outperforms
BART||has||outperforms
",,,,,,,"outperforms||has||all existing work
"
experiments,"BART outperforms the best previous work , which leverages BERT , by roughly 6.0 points on all ROUGE metrics - representing a significant advance in performance on this problem .","best previous work
leverages
BERT
by
roughly 6.0 points
on
all ROUGE metrics
representing
significant advance
in
performance","best previous work||by||roughly 6.0 points
roughly 6.0 points||representing||significant advance
significant advance||in||performance
roughly 6.0 points||on||all ROUGE metrics
best previous work||leverages||BERT
",,,,,,,"outperforms||has||best previous work
outperforms||has||best previous work
",
experiments,"We evaluate dialogue response generation on CONVAI2 , in which agents must generate responses conditioned on both the previous context and a textually - specified persona .","evaluate
dialogue response generation
on
CONVAI2","dialogue response generation||on||CONVAI2
",,,,,"several text generation tasks||evaluate||dialogue response generation
",,,"dialogue response generation||has||BART
"
experiments,BART outperforms previous work on two automated metrics .,"BART
outperforms
previous work
on
two automated metrics","outperforms||on||two automated metrics
","outperforms||has||previous work
",,,,,,,
experiments,We use the recently proposed ELI5 dataset to test the model 's ability to generate long freeform answers .,"use
recently proposed ELI5 dataset",,,,,,"several text generation tasks||use||recently proposed ELI5 dataset
",,,
experiments,"We find BART outperforms the best previous work by 1.2 ROUGE - L , but the dataset remains a challenging , because answers are only weakly specified by the question .","find
BART
outperforms
best previous work
by
1.2 ROUGE - L","outperforms||by||1.2 ROUGE - L
",,,,,"recently proposed ELI5 dataset||find||BART
",,,
experiments,For each row we experiment on the original WMT16 Romanian - English augmented with back - translation data .,"experiment on
original WMT16 Romanian - English
augmented with
back - translation data","original WMT16 Romanian - English||augmented with||back - translation data
",,,,,"several text generation tasks||experiment on||original WMT16 Romanian - English
",,,"original WMT16 Romanian - English||has||Preliminary results
"
experiments,"1 . Preliminary results suggested that our approach was less effective without back - translation data , and prone to overfitting - future work should explore additional regularization techniques .","Preliminary results
suggested
our approach
was
less effective
without
back - translation data
prone to
overfitting","Preliminary results||suggested||our approach
our approach||prone to||overfitting
our approach||was||less effective
less effective||without||back - translation data
",,,,,,,,
research-problem,A Fully Attention - Based Information Retriever,Information Retriever,,,,,"Contribution||has research problem||Information Retriever
",,,,
research-problem,Question - answering ( QA ) systems that can answer queries expressed in natural language have been a perennial goal of the artificial intelligence community .,Question - answering ( QA ),,,,,"Contribution||has research problem||Question - answering ( QA )
",,,,
research-problem,"That is , in fact , the proposed focus of recent open - domain QA datasets , such as SQuAD .",open - domain QA,,,,,"Contribution||has research problem||open - domain QA
",,,,
research-problem,"In SQuAD , each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P .",QA,,,,,"Contribution||has research problem||QA
",,,,
model,"Inspired by the positive results of Vaswani et al. in machine translation , we have applied a similar architecture to the domain of question - answering , a model that we have named Fully Attention - Based Information Retriever ( FABIR ) .","named
Fully Attention - Based Information Retriever ( FABIR )",,,"Model||named||Fully Attention - Based Information Retriever ( FABIR )
",,,,,,
model,"Our goal then was to verify how much performance we can get exclusively from the attention mechanism , without combining it with several other techniques .","to verify
how much performance
get exclusively from
attention mechanism
without combining it with
several other techniques","how much performance||get exclusively from||attention mechanism
attention mechanism||without combining it with||several other techniques
",,"Model||to verify||how much performance
",,,,,,
model,"Convolutional attention : a novel attention mechanism that encodes many - to - many relationships between words , enabling richer contextual representations .","Convolutional attention
novel attention mechanism
encodes
many - to - many relationships
between
words
enabling
richer contextual representations","novel attention mechanism||encodes||many - to - many relationships
many - to - many relationships||enabling||richer contextual representations
many - to - many relationships||between||words
","Convolutional attention||has||novel attention mechanism
",,"Model||has||Convolutional attention
",,,,,
model,Reduction layer : a new layer design that fits the pipeline proposed by Vaswani et al .,"Reduction layer
new layer design
fits
pipeline
proposed by
Vaswani et al .","new layer design||fits||pipeline
pipeline||proposed by||Vaswani et al .
","Reduction layer||has||new layer design
",,"Model||has||Reduction layer
",,,,,
model,and compresses the input embedding size for subsequent layers ( this is especially beneficial when employing pre-trained embeddings ) .,,,,,,,,,,
model,Column - wise cross - attention : we modify the crossattention operation by and propose a new technique that is better suited to question - answering .,"Column - wise cross - attention
modify
crossattention operation
propose
new technique
better suited to
question - answering","Column - wise cross - attention||modify||crossattention operation
Column - wise cross - attention||propose||new technique
new technique||better suited to||question - answering
",,,"Model||has||Column - wise cross - attention
",,,,,
experimental-setup,We have trained our FABIR model during 54 epochs with a batch size of 75 in a GPU NVidia Titan X with 12 GB of RAM .,"trained
FABIR model
during
54 epochs
with
batch size
of
75
in
GPU NVidia Titan X
with
12 GB
of
RAM","FABIR model||with||batch size
batch size||of||75
FABIR model||in||GPU NVidia Titan X
GPU NVidia Titan X||with||12 GB
12 GB||of||RAM
FABIR model||during||54 epochs
",,"Experimental setup||trained||FABIR model
",,,,,,
experimental-setup,We developed our model in Tensorflow and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297 / for replicability .,"developed
our model
in
Tensorflow","our model||in||Tensorflow
",,"Experimental setup||developed||our model
",,,,,,
experimental-setup,We pre-processed the texts with the NLTK Tokenizer .,"pre-processed
texts
with
NLTK Tokenizer","texts||with||NLTK Tokenizer
",,"Experimental setup||pre-processed||texts
",,,,,,
experimental-setup,"For regularization , we applied residual and attention dropout of 0.9 in processing layers and of 0.8 in the reduction layer .","For
regularization
applied
residual and attention dropout
of
0.9
in
processing layers
0.8
in
reduction layer","regularization||applied||residual and attention dropout
residual and attention dropout||of||0.8
0.8||in||reduction layer
residual and attention dropout||of||0.9
0.9||in||processing layers
",,"Experimental setup||For||regularization
",,,,,,
experimental-setup,"In the character - level embedding process , a dropout of 0.75 was added before the convolution .","In
character - level embedding process
dropout
of
0.75
added before
convolution","dropout||of||0.75
0.75||added before||convolution
","character - level embedding process||has||dropout
","Experimental setup||In||character - level embedding process
",,,,,,
experimental-setup,"Additionally , a dropout of 0.8 was added before each convolutional layer in the answer selector .","dropout
of
0.8
added before
each convolutional layer
in
answer selector","dropout||of||0.8
0.8||added before||each convolutional layer
each convolutional layer||in||answer selector
",,,"Experimental setup||has||dropout
",,,,,
experimental-setup,"We set processing layers dimension d model to 100 , the number of heads n heads in each attention sublayer to 4 , the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer .","set
processing layers dimension
d model
to
100
number of heads
n heads
in
each attention sublayer
to
4
feed - forward hidden size
to
200
in
processing layers
400
in
reduction layer","feed - forward hidden size||to||200
200||in||processing layers
feed - forward hidden size||to||400
400||in||reduction layer
d model||to||100
n heads||in||each attention sublayer
each attention sublayer||to||4
","processing layers dimension||has||d model
number of heads||has||n heads
","Experimental setup||set||feed - forward hidden size
Experimental setup||set||processing layers dimension
Experimental setup||set||number of heads
",,,,,,
ablation-analysis,"This analysis confirms the effectiveness of char- embeddings , as its addition increased the F1 and EM scores , by 2.7 % and 3.1 % , respectively .","confirms
effectiveness
of
char- embeddings
addition
increased
F1 and EM scores
by
2.7 % and 3.1 %","effectiveness||of||char- embeddings
F1 and EM scores||by||2.7 % and 3.1 %
",,"Ablation analysis||confirms||effectiveness
",,,"has||increased||F1 and EM scores
","char- embeddings||addition||has
",,
ablation-analysis,"Most importantly , when the convolutional attention was replaced by the standard attention mechanism proposed in , the performance dropped by 2.4 % in F1 and 2.5 % in EM .","when
convolutional attention
replaced by
standard attention mechanism
in
performance
dropped
by
2.4 %
in
F1
2.5 %
EM","convolutional attention||replaced by||standard attention mechanism
dropped||by||2.5 %
2.5 %||in||EM
dropped||by||2.4 %
2.4 %||in||F1
","standard attention mechanism||has||performance
performance||has||dropped
","Ablation analysis||when||convolutional attention
",,,,,,
ablation-analysis,"Moreover , the tests also indicate that the reduction layer is capable of producing useful word representations when compressing the embeddings .","indicate
reduction layer
capable of producing
useful word representations
when
compressing
embeddings","reduction layer||capable of producing||useful word representations
useful word representations||when||compressing
","compressing||has||embeddings
","Ablation analysis||indicate||reduction layer
",,,,,,
ablation-analysis,"Indeed , when we replaced that layer by a standard feedforward layer with the same reduction ratio , there was a drop of 2.1 % and 2.5 % in the F1 and EM scores , respectively .","replaced
standard feedforward layer
with
same reduction ratio
drop
of
2.1 % and 2.5 %
in
F1 and EM scores","standard feedforward layer||with||same reduction ratio
drop||of||2.1 % and 2.5 %
2.1 % and 2.5 %||in||F1 and EM scores
","standard feedforward layer||has||drop
",,,,"reduction layer||replaced||standard feedforward layer
",,,
results,"Regarding EM and F 1 scores , FABIR and BiDAF showed similar performances .","Regarding
EM and F 1 scores
FABIR and BiDAF
showed
similar performances","FABIR and BiDAF||showed||similar performances
","EM and F 1 scores||has||FABIR and BiDAF
","Results||Regarding||EM and F 1 scores
",,,,,,
results,In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD .,"analyze
performance
of
FABIR and BiDAF","performance||of||FABIR and BiDAF
",,"Results||analyze||performance
",,,,,,
results,"shows that shorter answers are easier for both models : while they reach more than 75 % F1 for answers that are shorter than four words , for answers longer than ten words these scores drop to 60.4 % and 67.3 % for FABIR and BiDAF , respectively .","shows
shorter answers
are
easier
for
both models","shorter answers||are||easier
easier||for||both models
",,,,,"FABIR and BiDAF||shows||shorter answers
",,,
results,"shows that both models had their best performance with "" when "" questions .","best performance
with
"" when "" questions","best performance||with||"" when "" questions
",,,,,,,"FABIR and BiDAF||shows||best performance
",
results,"Together with "" when "" questions , "" how long "" and "" how many "" also proved easier to respond , as they possess the same property of having a smaller universe of possible answers .",""" how long "" and "" how many ""
proved
easier
to
respond
possess
same property
of having
smaller universe
of
possible answers",""" how long "" and "" how many ""||proved||easier
easier||to||respond
"" how long "" and "" how many ""||possess||same property
same property||of having||smaller universe
smaller universe||of||possible answers
",,,,,,,"FABIR and BiDAF||has||"" how long "" and "" how many ""
",
results,"In contrast to these , "" how "" and "" why "" questions resulted in considerably lower F1 and EM scores , as they can be answered by any sentence , and hence require a deeper understanding of the text .",""" how "" and "" why "" questions
resulted in
considerably lower F1 and EM scores",""" how "" and "" why "" questions||resulted in||considerably lower F1 and EM scores
",,,,,,,"FABIR and BiDAF||has||"" how "" and "" why "" questions
",
results,"Questions which expect a "" yes "" or a "" no "" as an answer are also difficult because it is not always possible to find those words in a snippet from the passage .","Questions
expect
"" yes "" or a "" no ""
as
answer
are
also difficult","Questions||expect||"" yes "" or a "" no ""
"" yes "" or a "" no ""||as||answer
"" yes "" or a "" no ""||are||also difficult
",,,,,,,"FABIR and BiDAF||has||Questions
",
results,It is curious that shorter passages showed the worst performance for both models .,"shorter passages
showed
worst performance
for
both models","shorter passages||showed||worst performance
worst performance||for||both models
",,,,,,,"FABIR and BiDAF||has||shorter passages
",
research-problem,Evaluating Semantic Parsing against a Simple Web - based Question Answering Model,Semantic Parsing,,,,,"Contribution||has research problem||Semantic Parsing
",,,,
model,"We develop a simple log - linear model , in the spirit of traditional web - based QA systems , that answers questions by querying the web and extracting the answer from returned web snippets .","develop
simple log - linear model
in the spirit of
traditional web - based QA systems
that
answers questions
by
querying
web
extracting
answer
from
returned web snippets","simple log - linear model||in the spirit of||traditional web - based QA systems
simple log - linear model||that||answers questions
answers questions||by||querying
simple log - linear model||extracting||answer
answer||from||returned web snippets
","querying||has||web
","Model||develop||simple log - linear model
",,,,,,
model,"Thus , our evaluation scheme is suitable for semantic parsing benchmarks in which the knowledge required for answering questions is covered by the web ( in contrast with virtual assitants for which the knowledge is specific to an application ) .","our evaluation scheme
suitable for
semantic parsing benchmarks
in which
knowledge
required for
answering questions
covered by
web","our evaluation scheme||suitable for||semantic parsing benchmarks
semantic parsing benchmarks||in which||knowledge
knowledge||required for||answering questions
answering questions||covered by||web
",,,"Model||has||our evaluation scheme
",,,,,
experiments,"We compare our model , WEBQA , to STAGG and COMPQ , which are to the best of our knowledge the highest performing semantic parsing models on both COMPLEXQUESTIONS and WEBQUES - TIONS .","STAGG and COMPQ
which are
highest performing semantic parsing models
on
COMPLEXQUESTIONS and WEBQUES - TIONS","STAGG and COMPQ||which are||highest performing semantic parsing models
highest performing semantic parsing models||on||COMPLEXQUESTIONS and WEBQUES - TIONS
",,,,,,,"Baselines||has||STAGG and COMPQ
",
experiments,"WEBQA obtained 32.6 F 1 ( 33.5 p@1 , 42.4 MRR ) compared to 40.9 F 1 of COMPQ .","WEBQA
obtained
32.6 F 1
compared to
40.9 F 1
of
COMPQ","WEBQA||obtained||32.6 F 1
32.6 F 1||compared to||40.9 F 1
40.9 F 1||of||COMPQ
",,,,,,,"Results||has||WEBQA
",
experiments,Our candidate extraction step finds the correct answer in the top - K candidates in 65.9 % of development examples and 62.7 % of test examples .,"candidate extraction step
finds
correct answer
in
top - K candidates
in
65.9 %
of
development examples
62.7 %
of
test examples","candidate extraction step||finds||correct answer
correct answer||in||top - K candidates
top - K candidates||in||62.7 %
62.7 %||of||test examples
top - K candidates||in||65.9 %
65.9 %||of||development examples
",,,,,,,"Results||has||candidate extraction step
",
experiments,"Thus , our test F 1 on examples for which candidate extraction succeeded ( WEBQA - SUBSET ) is 51.9 ( 53.4 p@1 , 67.5 MRR ) .","test F 1
on
examples
for which
candidate extraction succeeded ( WEBQA - SUBSET )
is
51.9 ( 53.4 p@1 , 67.5 MRR )","test F 1||on||examples
examples||for which||candidate extraction succeeded ( WEBQA - SUBSET )
candidate extraction succeeded ( WEBQA - SUBSET )||is||51.9 ( 53.4 p@1 , 67.5 MRR )
",,,,,,,"Results||has||test F 1
",
experiments,"In this setup , COMPQ obtained 42.2 F 1 on the test set ( compared to 40.9 F 1 , when training on COM - PLEXQUESTIONS only , as we do ) .","COMPQ
obtained
42.2 F 1
on
test set
compared to
40.9 F 1
when training on
COM - PLEXQUESTIONS only","COMPQ||obtained||42.2 F 1
42.2 F 1||compared to||40.9 F 1
40.9 F 1||when training on||COM - PLEXQUESTIONS only
42.2 F 1||on||test set
",,,,,,,"Results||has||COMPQ
",
experiments,"Restricting the predictions to the subset for which candidate extraction succeeded , the F 1 of COMPQ - SUBSET is 48.5 , which is 3.4 F 1 points lower than WEBQA - SUBSET , which was trained on less data .","Restricting
predictions
to
subset
for which
candidate extraction
succeeded
F 1
of
COMPQ - SUBSET
is
48.5
is
3.4 F 1 points lower
than
WEBQA - SUBSET","predictions||to||subset
subset||for which||candidate extraction
F 1||of||COMPQ - SUBSET
COMPQ - SUBSET||is||48.5
48.5||is||3.4 F 1 points lower
3.4 F 1 points lower||than||WEBQA - SUBSET
","candidate extraction||has||succeeded
predictions||has||F 1
",,,,"Results||Restricting||predictions
",,,
ablation-analysis,"Note that TF - IDF is by far the most impactful feature , leading to a large drop of 12 points in performance .","TF - IDF
is
most impactful feature
leading to
large drop
of
12 points","TF - IDF||leading to||large drop
large drop||of||12 points
TF - IDF||is||most impactful feature
",,,"Ablation analysis||has||TF - IDF
",,,,,
code,"Code , data , annotations , and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0x91d77db37e0a4bbbaeb37b8972f4784f/.",,,,,,,,,,
research-problem,Efficient and Robust Question Answering from Minimal Context over Documents,Question Answering,,,,,"Contribution||has research problem||Question Answering
",,,,
research-problem,Neural models for question answering ( QA ) over documents have achieved significant performance improvements .,question answering ( QA ),,,,,"Contribution||has research problem||question answering ( QA )
",,,,
research-problem,"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .",QA,,,,,"Contribution||has research problem||QA
",,,,
model,"In this paper , we aim to develop a QA system that is scalable to large documents as well as robust to adversarial inputs .","develop
QA system
scalable to
large documents
robust to
adversarial inputs","QA system||scalable to||large documents
QA system||robust to||adversarial inputs
",,"Model||develop||QA system
",,,,,,
model,"First , we study the context required to answer the question by sampling examples in the dataset and carefully analyzing them .","study
context
required to
answer the question
by sampling
examples
in
dataset","context||by sampling||examples
examples||in||dataset
context||required to||answer the question
",,"Model||study||context
",,,,,,
model,"Second , inspired by this observation , we propose a sentence selector to select the minimal set of sentences to give to the QA model in order to answer the question .","propose
sentence selector
to select
minimal set of sentences
to give
QA model
to answer
question","sentence selector||to select||minimal set of sentences
minimal set of sentences||to give||QA model
QA model||to answer||question
",,"Model||propose||sentence selector
",,,,,,
model,"Since the minimum number of sentences depends on the question , our sentence selector chooses a different number of sentences for each question , in contrast with previous models that select a fixed number of sentences .","chooses
different number of sentences
for
each question","different number of sentences||for||each question
",,,,,"sentence selector||chooses||different number of sentences
",,,
model,"Our sentence selector leverages three simple techniques - weight transfer , data modification and score normalization , which we show to be highly effective on the task of sentence selection .","leverages
three simple techniques
weight transfer
data modification
score normalization",,"three simple techniques||has||weight transfer
three simple techniques||has||data modification
three simple techniques||has||score normalization
",,,,"sentence selector||leverages||three simple techniques
",,,
experiments,"Results shows results in the task of sentence selection on SQuAD and New s QA . First , our selector outperforms TF - IDF method and the previous state - of - the - art by large margin ( up to 2.9 % MAP ) .","SQuAD and New s QA
selector
outperforms
TF - IDF method and the previous state - of - the - art
by
large margin
up to
2.9 % MAP","TF - IDF method and the previous state - of - the - art||by||large margin
large margin||up to||2.9 % MAP
","selector||has||outperforms
outperforms||has||TF - IDF method and the previous state - of - the - art
",,,,,,"Tasks||has||SQuAD and New s QA
Results||has||selector
","SQuAD and New s QA||has||Results
"
experiments,"Second , our three training techniques - weight transfer , data modification and score normalization - improve performance by up to 5.6 % MAP .","three training techniques
weight transfer
data modification
score normalization
improve
performance
by
up to 5.6 % MAP","three training techniques||improve||performance
performance||by||up to 5.6 % MAP
","three training techniques||has||weight transfer
three training techniques||has||data modification
three training techniques||has||score normalization
",,,,,,"Results||has||three training techniques
",
experiments,"Finally , our Dyn method achieves higher accuracy with less sentences than the Top k method .","Dyn method
achieves
higher accuracy
with
less sentences
than
Top k method","Dyn method||achieves||higher accuracy
higher accuracy||with||less sentences
less sentences||than||Top k method
",,,,,,,"Results||has||Dyn method
",
experiments,"On News QA , Top 4 achieves 92.5 accuracy , whereas Dyn achieves 94.6 accuracy with 3.9 sentences per example .","On
News QA
Top 4
achieves
92.5 accuracy
Dyn
achieves
94.6 accuracy
with
3.9 sentences per example","Dyn||achieves||94.6 accuracy
94.6 accuracy||with||3.9 sentences per example
Top 4||achieves||92.5 accuracy
","News QA||has||Dyn
News QA||has||Top 4
",,,,"Results||On||News QA
",,,
experiments,"On SQuAD , S - Reader achieves 6.7 training and 3.6 inference speedup on SQuAD , and 15.0 training and 6.9 inference speedup on News QA .","SQuAD
S - Reader
achieves
6.7 training
3.6 inference
speedup
on
SQuAD
15.0 training
6.9 inference
speedup
News QA","S - Reader||on||SQuAD
SQuAD||achieves||speedup
S - Reader||on||News QA
","SQuAD||has||S - Reader
speedup||has||6.7 training
speedup||has||3.6 inference
News QA||has||speedup
speedup||has||15.0 training
speedup||has||6.9 inference
",,,,,,"Results||On||SQuAD
",
experiments,Trivia QA and SQuAD - Open,Trivia QA and SQuAD - Open,,,,,,,,"Tasks||has||Trivia QA and SQuAD - Open
","Trivia QA and SQuAD - Open||has||Baselines
"
experiments,We compare with the results from the sentences selected by TF - IDF method and our selector ( Dyn ) .,"compare with
TF - IDF method
our selector ( Dyn )",,,,,,"Baselines||compare with||TF - IDF method
Baselines||compare with||our selector ( Dyn )
",,,"Baselines||compare with||published Rank1 - 3 models
"
experiments,We also compare with published Rank1 - 3 models .,published Rank1 - 3 models,,,,,,,,,
experiments,"Results shows results on Trivia QA ( Wikipedia ) and SQuAD - Open. First , MINI - MAL obtains higher F1 and EM over FULL , with the inference speedup of up to 13.8 .","MINI - MAL
obtains
higher F1 and EM
over
FULL","MINI - MAL||obtains||higher F1 and EM
higher F1 and EM||over||FULL
",,,,,,,"Results||has||MINI - MAL
",
experiments,"Second , the model with our sentence selector with Dyn achieves higher F1 and EM over the model with TF - IDF selector .","model
with
our sentence selector
with
Dyn
achieves
higher F1 and EM
over
model
with
TF - IDF selector","model||with||our sentence selector
our sentence selector||with||Dyn
our sentence selector||achieves||higher F1 and EM
higher F1 and EM||over||model
model||with||TF - IDF selector
",,,,,,,"Results||has||model
",
experiments,"Third , we outperforms the published state - of - the - art on both dataset .","outperforms
state - of - the - art
on
both dataset","state - of - the - art||on||both dataset
","outperforms||has||state - of - the - art
",,,,,,"Results||has||outperforms
",
experiments,SQuAD - Adversarial,SQuAD - Adversarial,,,,,,,,"Tasks||has||SQuAD - Adversarial
","SQuAD - Adversarial||has||Results
"
experiments,"Results shows that MINIMAL outperforms FULL , achieving the new state - of - the - art by large margin ( + 11.1 and + 11.5 F1 on AddSent and Add OneSent , respectively ) .","shows
MINIMAL
outperforms
FULL
achieving
new state - of - the - art
by
large margin
+ 11.1 and + 11.5 F1
on
AddSent and Add OneSent","outperforms||achieving||new state - of - the - art
new state - of - the - art||by||large margin
+ 11.1 and + 11.5 F1||on||AddSent and Add OneSent
","MINIMAL||has||outperforms
large margin||has||+ 11.1 and + 11.5 F1
outperforms||has||FULL
",,,,"Results||shows||MINIMAL
",,,
research-problem,Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS ? FINE - GRAINED GATING FOR READING COMPREHENSION,READING COMPREHENSION,,,,,"Contribution||has research problem||READING COMPREHENSION
",,,,
model,"In this work , we present a fine - grained gating mechanism to combine the word - level and characterlevel representations .","present
fine - grained gating mechanism
to combine
word - level and characterlevel representations","fine - grained gating mechanism||to combine||word - level and characterlevel representations
",,"Model||present||fine - grained gating mechanism
",,,,,,
model,We compute a vector gate as a linear projection of the token features followed 1 Code is available at https://github.com/kimiyoung/fg-gating 1 ar Xiv: 1611.01724v2 [ cs.CL ] 11 Sep 2017,"compute
vector gate
as
linear projection
of
token features
https://github.com/kimiyoung/fg-gating","vector gate||as||linear projection
linear projection||of||token features
",,"Model||compute||vector gate
",,"Contribution||Code||https://github.com/kimiyoung/fg-gating
",,,,
model,We then multiplicatively apply the gate to the character - level and wordlevel representations .,"multiplicatively
apply
gate
to
character - level and wordlevel representations","multiplicatively||apply||gate
gate||to||character - level and wordlevel representations
",,,"Model||has||multiplicatively
",,,,,
model,Each dimension of the gate controls how much information is flowed from the word - level and character - level representations respectively .,"Each dimension
of
gate
controls
how much information
flowed from
word - level and character - level representations","Each dimension||controls||how much information
how much information||flowed from||word - level and character - level representations
Each dimension||of||gate
",,,"Model||has||Each dimension
",,,,,
model,"We use named entity tags , part - ofspeech tags , document frequencies , and word - level representations as the features for token properties which determine the gate .","use
named entity tags
part - ofspeech tags
document frequencies
word - level representations
for
token properties
determine
gate","token properties||determine||gate
token properties||use||named entity tags
token properties||use||part - ofspeech tags
token properties||use||document frequencies
token properties||use||word - level representations
",,"Model||for||token properties
",,,,,,
model,"More generally , our fine - grained gating mechanism can be used to model multiple levels of structure in language , including words , characters , phrases , sentences and paragraphs .","our fine - grained gating mechanism
used to
model
multiple levels
structure
in
language
including
words
characters
phrases
sentences
paragraphs","our fine - grained gating mechanism||used to||model
model||multiple levels||structure
structure||including||words
structure||including||characters
structure||including||phrases
structure||including||sentences
structure||including||paragraphs
structure||in||language
",,,"Model||has||our fine - grained gating mechanism
",,,,,
research-problem,The NarrativeQA Reading Comprehension Challenge,Reading Comprehension,,,,,"Contribution||has research problem||Reading Comprehension
",,,,
research-problem,"Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document .",Reading comprehension ( RC ),,,,,"Contribution||has research problem||Reading comprehension ( RC )
",,,,
research-problem,"Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read .",RC,,,,,"Contribution||has research problem||RC
",,,,
dataset,"We present a new task and dataset , which we call NarrativeQA , which will test and reward artificial agents approaching this level of competence ( Section 3 ) .","call
NarrativeQA",,,"Dataset||call||NarrativeQA
",,,,,,
dataset,"The dataset consists of stories , which are books and movie scripts , with human written questions and answers based solely on human - generated abstractive summaries .","consists of
stories
which are
books and movie scripts
with
human written questions and answers
based solely on
human - generated abstractive summaries","stories||which are||books and movie scripts
books and movie scripts||with||human written questions and answers
human written questions and answers||based solely on||human - generated abstractive summaries
",,"Dataset||consists of||stories
",,,,,,
dataset,"For the RC tasks , questions maybe answered using just the summaries or the full story text .","questions
maybe
answered
using
summaries
full story text","questions||maybe||answered
answered||using||summaries
answered||using||full story text
",,,"Dataset||has||questions
",,,,,
experiments,Reading Summaries Only,Reading Summaries Only,,,,,,,,"Tasks||has||Reading Summaries Only
","Reading Summaries Only||has||Results
"
experiments,"This is indeed the case , with the neural span prediction model significantly outperforming all other proposed methods .","neural span prediction model
significantly outperforming
all other proposed methods",,"neural span prediction model||has||significantly outperforming
significantly outperforming||has||all other proposed methods
",,,,,,"Results||has||neural span prediction model
",
experiments,"Both the plain sequence to sequence model and the AS Reader , successfully applied to the CNN / DailyMail reading comprehension task , also perform well on this task .","Both the plain sequence to sequence model and the AS Reader
perform
well","Both the plain sequence to sequence model and the AS Reader||perform||well
",,,,,,,"Results||has||Both the plain sequence to sequence model and the AS Reader
",
experiments,An additional inductive bias results in higher performance for the span prediction model .,"additional inductive bias
results in
higher performance
for
span prediction model","additional inductive bias||results in||higher performance
higher performance||for||span prediction model
",,,,,,,"Results||has||additional inductive bias
",
experiments,"summarizes the results on the full Narra - tive QA task , where the context documents are full stories .","on
full Narra - tive QA task
where
context documents
are
full stories","full Narra - tive QA task||where||context documents
context documents||are||full stories
",,,,,"Results||on||full Narra - tive QA task
",,,
experiments,"As expected ( and desired ) , we observe a decline in performance of the span- selection oracle IR model , compared with the results on summaries .","observe
decline
in
performance
of
span- selection oracle IR model","decline||in||performance
performance||of||span- selection oracle IR model
",,,,,"full Narra - tive QA task||observe||decline
",,,
experiments,Reading Full Stories Only,Reading Full Stories Only,,,,,,,,"Tasks||has||Reading Full Stories Only
","Reading Full Stories Only||has||Results
"
experiments,"The AS Reader , which was the better - performing model on the summaries task , underperforms the simple no -context Seq2Seq baseline ( shown in ) in terms of MRR .","AS Reader
underperforms
simple no -context Seq2Seq baseline
in terms of
MRR","underperforms||in terms of||MRR
","AS Reader||has||underperforms
underperforms||has||simple no -context Seq2Seq baseline
",,,,,,"Results||has||AS Reader
",
experiments,"As with the AS Reader , we observed no significant differences for varying number of chunks .","observed
no significant differences
for
varying
number of chunks","no significant differences||for||varying
","varying||has||number of chunks
",,,,"Results||observed||no significant differences
",,,
research-problem,A large annotated corpus for learning natural language inference,natural language inference,,,,,"Contribution||has research problem||natural language inference
",,,,
research-problem,"Thus , natural language inference ( NLI ) - characterizing and using these relations in computational systems ) - is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning .",natural language inference ( NLI ),,,,,"Contribution||has research problem||natural language inference ( NLI )
",,,,
research-problem,"NLI has been addressed using a variety of techniques , including those based on symbolic logic , knowledge bases , and neural networks .",NLI,,,,,"Contribution||has research problem||NLI
",,,,
dataset,"To address this , this paper introduces the Stanford Natural Language Inference ( SNLI ) corpus , a collection of sentence pairs labeled for entailment , contradiction , and semantic independence .","introduces
Stanford Natural Language Inference ( SNLI ) corpus
collection
of
sentence pairs
labeled for
entailment
contradiction
semantic independence","collection||of||sentence pairs
sentence pairs||labeled for||entailment
sentence pairs||labeled for||contradiction
sentence pairs||labeled for||semantic independence
","Stanford Natural Language Inference ( SNLI ) corpus||has||collection
","Dataset||introduces||Stanford Natural Language Inference ( SNLI ) corpus
",,,,,,
dataset,"At 570,152 sentence pairs , SNLI is two orders of magnitude larger than all other resources of its type .","At
570,152 sentence pairs
SNLI
is
two orders of magnitude larger
than
other resources
of
its type","SNLI||is||two orders of magnitude larger
two orders of magnitude larger||than||other resources
other resources||of||its type
","570,152 sentence pairs||has||SNLI
","Dataset||At||570,152 sentence pairs
",,,,,,
dataset,"And , in contrast to many such resources , all of its sentences and labels were written by humans in a grounded , naturalistic context .","in
sentences and labels
written by
humans
grounded , naturalistic context","sentences and labels||written by||humans
humans||in||grounded , naturalistic context
",,,"Dataset||has||sentences and labels
",,,,,
dataset,"In a separate validation phase , we collected four additional judgments for each label for 56,941 of the examples .","In
separate validation phase
collected
four additional judgments
for
each label
for
56,941
of
examples","separate validation phase||collected||four additional judgments
four additional judgments||for||each label
each label||for||56,941
56,941||of||examples
",,"Dataset||In||separate validation phase
",,,,,,
dataset,"Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .","Of these
98 %
of
cases
emerge with
threeannotator consensus
58 %
see
unanimous consensus
from
all five annotators","58 %||see||unanimous consensus
unanimous consensus||from||all five annotators
98 %||of||cases
cases||emerge with||threeannotator consensus
",,,,,"four additional judgments||Of these||58 %
four additional judgments||Of these||98 %
",,,
results,"The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier while the sum of words model can use pretrained word embeddings to better handle rare words , it lacks even the rudimentary sensitivity to word order that the lexicalized model 's bigram features provide .","sum of words model
performed
slightly worse
than
fundamentally similar lexicalized classifier","sum of words model||performed||slightly worse
slightly worse||than||fundamentally similar lexicalized classifier
",,,"Results||has||sum of words model
",,,,,
results,"Of the two RNN models , the LSTM 's more robust ability to learn long - term dependencies serves it well , giving it a substantial advantage over the plain RNN , and resulting in performance that is essentially equivalent to the lexicalized classifier on the test set ( LSTM performance near the stopping iteration varies by up to 0.5 % between evaluation steps ) .","Of
two RNN models
LSTM 's
more robust ability
to learn
long - term dependencies
serves
well
resulting in
performance
that is
essentially equivalent
to
lexicalized classifier
on
test set","more robust ability||serves||well
more robust ability||resulting in||performance
performance||that is||essentially equivalent
essentially equivalent||to||lexicalized classifier
lexicalized classifier||on||test set
more robust ability||to learn||long - term dependencies
","two RNN models||has||LSTM 's
LSTM 's||has||more robust ability
","Results||Of||two RNN models
",,,,,,
results,"While the lexicalized model fits the training set almost perfectly , the gap between train and test set accuracy is relatively small for all three neural network models , suggesting that research into significantly higher capacity versions of these models would be productive .","gap
between
train and test set accuracy
is
relatively small
for
all three neural network models","gap||between||train and test set accuracy
train and test set accuracy||is||relatively small
relatively small||for||all three neural network models
",,,"Results||has||gap
",,,,,
results,"In addition , though the LSTM and the lexicalized model show similar performance when trained on the current full corpus , the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets .","LSTM
on
somewhat steeper slope
for
hints that
ability to learn
arbitrarily structured representations
of
sentence meaning
give it
advantage
over
more constrained lexicalized model
larger datasets","somewhat steeper slope||for||LSTM
somewhat steeper slope||hints that||ability to learn
arbitrarily structured representations||give it||advantage
advantage||over||more constrained lexicalized model
more constrained lexicalized model||on||larger datasets
arbitrarily structured representations||of||sentence meaning
","ability to learn||has||arbitrarily structured representations
",,"Results||has||somewhat steeper slope
",,,,,
research-problem,Iterative Alternating Neural Attention for Machine Reading,Machine Reading,,,,,"Contribution||has research problem||Machine Reading
",,,,
research-problem,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",machine comprehension,,,,,"Contribution||has research problem||machine comprehension
",,,,
model,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .","propose
novel neural attention - based inference model
to perform
machine reading comprehension tasks","novel neural attention - based inference model||to perform||machine reading comprehension tasks
",,"Model||propose||novel neural attention - based inference model
",,,,,,
model,The model first reads the document and the query using a recurrent neural network .,"first reads
document and the query
using
recurrent neural network","document and the query||using||recurrent neural network
",,"Model||first reads||document and the query
",,,,,,
model,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .","deploys
iterative inference process
to uncover
inferential links
between
missing query word
query
document","iterative inference process||to uncover||inferential links
inferential links||between||missing query word
inferential links||between||query
inferential links||between||document
",,"Model||deploys||iterative inference process
",,,,,,
model,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .","involves
novel alternating attention mechanism
first attends
some parts
of
query
then finds
corresponding matches
by attending
document","novel alternating attention mechanism||then finds||corresponding matches
corresponding matches||by attending||document
novel alternating attention mechanism||first attends||some parts
some parts||of||query
",,,,,"iterative inference process||involves||novel alternating attention mechanism
",,,"novel alternating attention mechanism||has||result
"
model,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,"result
fed back into
iterative inference process
to seed
next search step","result||fed back into||iterative inference process
iterative inference process||to seed||next search step
",,,,,,,,
model,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .","After
fixed number of iterations
model
uses
summary
of
inference process
to predict
answer","model||uses||summary
summary||of||inference process
summary||to predict||answer
","fixed number of iterations||has||model
",,,,"iterative inference process||After||fixed number of iterations
",,,
experimental-setup,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","To train
our model
used
stochastic gradient descent
with
ADAM optimizer
initial learning rate
of
0.001","our model||used||stochastic gradient descent
stochastic gradient descent||with||ADAM optimizer
stochastic gradient descent||with||initial learning rate
initial learning rate||of||0.001
",,"Experimental setup||To train||our model
",,,,,,
experimental-setup,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .","set
batch size
to
32
decay
learning rate
by
0.8
if
accuracy
on
validation set
not increase
after
half - epoch
i.e.
2000 batches
for
CBT
5000 batches
for
CNN","batch size||to||32
learning rate||by||0.8
learning rate||if||accuracy
accuracy||on||validation set
not increase||after||half - epoch
half - epoch||i.e.||2000 batches
2000 batches||for||CBT
half - epoch||i.e.||5000 batches
5000 batches||for||CNN
","validation set||has||not increase
","Experimental setup||set||batch size
Experimental setup||decay||learning rate
",,,,,,
experimental-setup,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .","initialize
all weights
of
our model
by
sampling
from
normal distribution N ( 0 , 0.05 )","all weights||of||our model
all weights||by||sampling
sampling||from||normal distribution N ( 0 , 0.05 )
",,"Experimental setup||initialize||all weights
",,,,,,
experimental-setup,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .","GRU recurrent weights
are
initialized
to be
orthogonal
biases
are
initialized
to
zero","biases||are||initialized
initialized||to||zero
GRU recurrent weights||are||initialized
initialized||to be||orthogonal
",,,"Experimental setup||has||biases
Experimental setup||has||GRU recurrent weights
",,,,,
experimental-setup,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .","to stabilize
learning
clip
gradients
if
norm
greater than
5","learning||clip||gradients
gradients||if||norm
norm||greater than||5
",,"Experimental setup||to stabilize||learning
",,,,,,
experimental-setup,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .","setting
embedding regularization
to
0.0001
worked
robustly
across
datasets","embedding regularization||worked||robustly
robustly||across||datasets
embedding regularization||to||0.0001
",,"Experimental setup||setting||embedding regularization
",,,,,,
experimental-setup,"Our model is implemented in Theano , using the Keras library .","Our model
implemented in
Theano
using
Keras library","Our model||implemented in||Theano
Theano||using||Keras library
",,,"Experimental setup||has||Our model
",,,,,
research-problem,Published as a conference paper at ICLR 2018 NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE,NATURAL LANGUAGE INFERENCE,,,,,"Contribution||has research problem||NATURAL LANGUAGE INFERENCE
",,,,
research-problem,"Natural Language Inference ( NLI also known as recognizing textual entiailment , or RTE ) task requires one to determine whether the logical relationship between two sentences is among entailment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither entailment nor contradiction ) .","NLI
recognizing textual entiailment
RTE",,,,,"Contribution||has research problem||NLI
Contribution||has research problem||recognizing textual entiailment
Contribution||has research problem||RTE
",,,,
model,"In this work , we push the multi-head attention to a extreme by building a word - by - word dimension - wise alignment tensor which we call interaction tensor .","push
multi-head attention
to
extreme
by building
word - by - word dimension - wise alignment tensor
call
interaction tensor","multi-head attention||by building||word - by - word dimension - wise alignment tensor
word - by - word dimension - wise alignment tensor||call||interaction tensor
multi-head attention||to||extreme
",,"Model||push||multi-head attention
",,,,,,
model,The interaction tensor encodes the high - order alignment relationship between sentences pair .,"interaction tensor
encodes
high - order alignment relationship
between
sentences pair","interaction tensor||encodes||high - order alignment relationship
high - order alignment relationship||between||sentences pair
",,,"Model||has||interaction tensor
",,,,,
model,We dub the general framework as Interactive Inference Network ( IIN ) .,"dub
general framework
as
Interactive Inference Network ( IIN )","general framework||as||Interactive Inference Network ( IIN )
",,"Model||dub||general framework
",,,,,,
,We implement our algorithm with Tensorflow framework .,"implement
our algorithm
with
Tensorflow framework",,,,,,,,,
,"An Adadelta optimizer ( Zeiler , 2012 ) with ? as 0.95 and as 1e ? 8 is used to optimize all the trainable weights .","Adadelta optimizer
with
? as 0.95 and as 1e ? 8
used to
optimize
all the trainable weights",,,,,,,,,
,The initial learning rate is set to 0.5 and batch size to 70 .,"initial learning rate
set to
0.5
batch size
to
70",,,,,,,,,
,"When the model does not improve best in domain performance for 30,000 steps , an SGD optimizer with learning rate of 3e ? 4 is used to help model to find a better local optimum .","model
not improve
best in domain performance
for
30,000 steps
SGD optimizer
with
learning rate
of
3e ? 4
to help
find
better local optimum",,,,,,,,,
,Dropout layers are applied before all linear layers and after word - embedding layer .,"Dropout layers
applied before
all linear layers
after
word - embedding layer",,,,,,,,,
,"We use an exponential decayed keep rate during training , where the initial keep rate is 1.0 and the decay rate is 0.977 for every 10,000 step .","use
exponential decayed keep rate
during
training
where
initial keep rate
is
1.0
decay rate
is
0.977
for
every 10,000 step",,,,,,,,,
,We initialize our word embeddings with pre-trained 300D Glo Ve 840B vectors while the out - of - vocabulary word are randomly initialized with uniform distribution .,"initialize
word embeddings
with
pre-trained 300D Glo Ve 840B vectors
out - of - vocabulary word
are
randomly initialized
with
uniform distribution",,,,,,,,,
,The character embeddings are randomly initialized with 100D .,"character embeddings
are
randomly initialized
with
100D",,,,,,,,,
,We crop or pad each token to have 16 characters .,"crop or pad
each
token
to have
16 characters",,,,,,,,,
,The 1D convolution kernel size for character embedding is 5 .,"1D convolution kernel size
for
character embedding
is
5",,,,,,,,,
,"All weights are constraint by L2 regularization , and the L2 regularization at step t is calculated as follows :","All weights
constraint by
L2 regularization",,,,,,,,,
,The first scale down ratio ?,first scale down ratio,,,,,,,,,
,in feature extraction layer is set to 0.3 and transitional scale down ratio ?,"in
feature extraction layer
set to
0.3
transitional scale down ratio",,,,,,,,,
,is set to 0.5 .,"set to
0.5",,,,,,,,,
,"The sequence length is set as a hard cutoff on all experiments : 48 for MultiNLI , 32 for SNLI and 24 for Quora Question Pair Dataset .","sequence length
set as
hard cutoff
on
all experiments
48
for
MultiNLI
32
for
SNLI
24
for
Quora Question Pair Dataset",,,,,,,,,
results,EXPERIMENT ON MULTINLI,MULTINLI,,,,"Results||has||MULTINLI
",,,,,"MULTINLI||has||Our approach
"
results,"Our approach , without using any recurrent structure , achieves the new state - of - the - art performance of 80.0 % , exceeding current state - of - the - art performance by more than 5 % .","Our approach
without using
recurrent structure
achieves
new state - of - the - art performance
of
80.0 %
exceeding
current state - of - the - art performance
by
more than 5 %","Our approach||achieves||new state - of - the - art performance
new state - of - the - art performance||of||80.0 %
80.0 %||exceeding||current state - of - the - art performance
current state - of - the - art performance||by||more than 5 %
Our approach||without using||recurrent structure
",,,,,,,,
results,"Unlike the observation from , we find the out - of - domain test performance is consistently lower than in - domain test performance .","find
out - of - domain test performance
is
consistently lower
than
in - domain test performance","out - of - domain test performance||is||consistently lower
consistently lower||than||in - domain test performance
",,,,,"MULTINLI||find||out - of - domain test performance
",,,
results,EXPERIMENT ON SNLI,SNLI,,,,"Results||has||SNLI
",,,,,
results,"We show our model , DIIN , achieves state - of - the - art performance on the competitive leaderboard .","show
our model , DIIN
achieves
state - of - the - art performance
on
competitive leaderboard","our model , DIIN||achieves||state - of - the - art performance
state - of - the - art performance||on||competitive leaderboard
",,,,,"SNLI||show||our model , DIIN
",,,
results,EXPERIMENT ON QUORA QUESTION PAIR DATASET,QUORA QUESTION PAIR DATASET,,,,"Results||has||QUORA QUESTION PAIR DATASET
",,,,,"QUORA QUESTION PAIR DATASET||has||BIMPM
"
results,"BIMPM models different perspective of matching between sentence pair on both direction , then aggregates matching vector with LSTM .","BIMPM
models
different perspective of matching
between
sentence pair
on
both direction
aggregates
matching vector
with
LSTM","BIMPM||models||different perspective of matching
different perspective of matching||between||sentence pair
sentence pair||on||both direction
BIMPM||aggregates||matching vector
matching vector||with||LSTM
",,,,,,,,
results,DECATT word and DECATT char uses automatically collected in - domain paraphrase data to noisy pretrain n-gram word embedding and ngram subword embedding correspondingly on decomposable attention model proposed by .,"DECATT word and DECATT char
uses
automatically collected in - domain paraphrase data
to noisy pretrain
n-gram word embedding and ngram subword embedding
on
decomposable attention model","DECATT word and DECATT char||uses||automatically collected in - domain paraphrase data
automatically collected in - domain paraphrase data||to noisy pretrain||n-gram word embedding and ngram subword embedding
n-gram word embedding and ngram subword embedding||on||decomposable attention model
",,,,,,,"QUORA QUESTION PAIR DATASET||has||DECATT word and DECATT char
",
ablation-analysis,"After removing the exact match binary feature , we find the performance degrade to 78.2 on matched score on development set and 78.0 on mismatched score .","After removing
exact match binary feature
find
performance
degrade
to
78.2
on
matched score
on
development set
78.0
on
mismatched score","exact match binary feature||find||performance
degrade||to||78.0
78.0||on||mismatched score
degrade||to||78.2
78.2||on||matched score
matched score||on||development set
","performance||has||degrade
performance||has||degrade
","Ablation analysis||After removing||exact match binary feature
",,,,,,
ablation-analysis,We obtain 73.2 for matched score and 73.6 on mismatched data .,"obtain
73.2
for
matched score
73.6
on
mismatched data","73.2||for||matched score
73.6||on||mismatched data
",,"Ablation analysis||obtain||73.2
Ablation analysis||obtain||73.6
",,,,,,
ablation-analysis,"If we remove encoding layer completely , then we 'll obtain a 73.5 for matched score and 73.2 for mismatched score .","remove
encoding layer completely
obtain
73.5
for
matched score
73.2
for
mismatched score","encoding layer completely||obtain||73.2
73.2||for||mismatched score
encoding layer completely||obtain||73.5
73.5||for||matched score
73.5||for||matched score
",,"Ablation analysis||remove||encoding layer completely
",,,,,,
ablation-analysis,The result demonstrate the feature extraction layer have powerful capability to capture the semantic feature .,"demonstrate
feature extraction layer
have
powerful capability
to capture
semantic feature","feature extraction layer||have||powerful capability
powerful capability||to capture||semantic feature
",,,,,"encoding layer completely||demonstrate||feature extraction layer
",,,
ablation-analysis,"In experiment 5 , we remove both self - attention and fuse gate , thus retaining only highway network .","both self - attention and fuse gate
retaining only
highway network","both self - attention and fuse gate||retaining only||highway network
",,,"Ablation analysis||remove||both self - attention and fuse gate
",,,,,"both self - attention and fuse gate||has||result
"
ablation-analysis,The result improves to 77.7 and 77.3 respectively on matched and mismatched development set .,"result
improves
to
77.7 and 77.3
on
matched and mismatched development set","improves||to||77.7 and 77.3
77.7 and 77.3||on||matched and mismatched development set
","result||has||improves
",,,,,,,
ablation-analysis,"However , in experiment 6 , when we only remove fuse gate , to our surprise , the performance degrade to 73.5 for matched score and 73.8 for mismatched .","fuse gate
to
performance
degrade
73.5
for
matched score
73.8
for
mismatched","degrade||to||73.5
degrade||to||73.8
73.8||for||mismatched
","fuse gate||has||performance
",,"Ablation analysis||remove||fuse gate
",,,,,
ablation-analysis,"On the other hand , if we use the addition of the representation after highway network and the representation after self - attention as skip connection as in experiment 7 , the performance increase to 77.3 and 76.3 .","use
addition
of
representation
after
highway network
and
representation
after
self - attention
as
skip connection
performance increase
to
77.3 and 76.3","addition||as||skip connection
performance increase||to||77.3 and 76.3
addition||of||representation
representation||after||highway network
highway network||and||representation
representation||after||self - attention
","skip connection||has||performance increase
","Ablation analysis||use||addition
",,,,,,
research-problem,A Compare - Aggregate Model with Latent Clustering for Answer Selection,Answer Selection,,,,,"Contribution||has research problem||Answer Selection
",,,,
research-problem,"In this paper , we propose a novel method for a sentence - level answer- selection task that is a fundamental problem in natural language processing .",sentence - level answer- selection,,,,,"Contribution||has research problem||sentence - level answer- selection
",,,,
research-problem,Automatic question answering ( QA ) is a primary objective of artificial intelligence .,Automatic question answering ( QA ),,,,,"Contribution||has research problem||Automatic question answering ( QA )
",,,,
approach,"First , we explore the effect of additional information by adopting a pretrained language model ( LM ) to compute the vector representation of the input text .","explore
effect
of
additional information
by adopting
pretrained language model ( LM )
to compute
vector representation
of
input text","effect||of||additional information
additional information||by adopting||pretrained language model ( LM )
pretrained language model ( LM )||to compute||vector representation
vector representation||of||input text
",,"Approach||explore||effect
",,,,,,
approach,"Following this study , we select an ELMo language model for this study .","select
ELMo language model",,,"Approach||select||ELMo language model
",,,,,,
approach,"We investigate the applicability of transfer learning ( TL ) using a large - scale corpus that is created for a relevant - sentence - selection task ( i.e. , question - answering NLI ( QNLI ) dataset ) .","investigate
applicability
of
transfer learning ( TL )
using
large - scale corpus
created for
relevant - sentence - selection task
i.e.
question - answering NLI ( QNLI ) dataset","applicability||of||transfer learning ( TL )
transfer learning ( TL )||using||large - scale corpus
large - scale corpus||created for||relevant - sentence - selection task
relevant - sentence - selection task||i.e.||question - answering NLI ( QNLI ) dataset
",,"Approach||investigate||applicability
",,,,,,
approach,"Second , we further enhance one of the baseline models , Comp - Clip ( refer to the discussion in 3.1 ) , for the target QA task by proposing a novel latent clustering ( LC ) method .","enhance
one of the baseline models
Comp - Clip
for
target QA task
by proposing
novel latent clustering ( LC ) method","one of the baseline models||for||target QA task
target QA task||by proposing||novel latent clustering ( LC ) method
","one of the baseline models||name||Comp - Clip
","Approach||enhance||one of the baseline models
",,,,,,
approach,The LC method computes latent cluster information for target samples by creating a latent memory space and calculating the similarity between the sample and the memory .,"LC method
computes
latent cluster information
for
target samples
by creating
latent memory space
calculating
similarity
between
sample
memory","LC method||computes||latent cluster information
latent cluster information||by creating||latent memory space
latent cluster information||for||target samples
latent cluster information||calculating||similarity
similarity||between||sample
similarity||between||memory
",,,"Approach||has||LC method
",,,,,
approach,"By an endto - end learning process with the answer-selection task , the LC method assigns true - label question - answer pairs to similar clusters .","By
endto - end learning process
assigns
true - label question - answer pairs
to
similar clusters","true - label question - answer pairs||to||similar clusters
similar clusters||By||endto - end learning process
",,,,,"LC method||assigns||true - label question - answer pairs
",,,
approach,"Last , we explore the effect of different objective functions ( listwise and pointwise learning ) .","different objective functions
listwise and pointwise learning",,"different objective functions||has||listwise and pointwise learning
",,,,,,"effect||of||different objective functions
",
hyperparameters,"To implement the Comp - Clip model , we apply a context projection weight matrix with 100 dimensions that are shared between the question part and the answer part ( eq. 1 ) .","implement
Comp - Clip model
apply
context projection weight matrix
with
100 dimensions
shared between
question part
answer part","Comp - Clip model||apply||context projection weight matrix
context projection weight matrix||with||100 dimensions
context projection weight matrix||shared between||question part
context projection weight matrix||shared between||answer part
",,"Hyperparameters||implement||Comp - Clip model
",,,,,,
hyperparameters,"In the aggregation part , we use 1 - D CNN with a total of 500 filters , which involves five types of filters K ? R {1,2,3,4,5}100 , 100 per type .","In
aggregation part
use
1 - D CNN
with
total of 500 filters","aggregation part||use||1 - D CNN
1 - D CNN||with||total of 500 filters
",,"Hyperparameters||In||aggregation part
",,,,,,
hyperparameters,"We select k ( for the kmax - pool in equation 5 ) as 6 and 4 for the WikiQA and TREC - QA case , respectively .","select
k
for
as
6 and 4
WikiQA and TREC - QA case","k||as||6 and 4
6 and 4||for||WikiQA and TREC - QA case
",,"Hyperparameters||select||k
",,,,,,
hyperparameters,"In both datasets , we apply 8 latent clusters .","apply
8 latent clusters",,,,,,"WikiQA and TREC - QA case||apply||8 latent clusters
",,,
hyperparameters,"The vocabulary size in the WiKiQA , TREC - QA and QNLI dataset are 30,104 , 56,908 and 154,442 , respectively .","vocabulary size
in
WiKiQA , TREC - QA and QNLI dataset
are
30,104 , 56,908 and 154,442","vocabulary size||in||WiKiQA , TREC - QA and QNLI dataset
WiKiQA , TREC - QA and QNLI dataset||are||30,104 , 56,908 and 154,442
",,,"Hyperparameters||has||vocabulary size
",,,,,
hyperparameters,"When applying the TL , the vocabulary size is set to 154,442 , and the dimension of the context projection weight matrix is set to 300 .","applying
TL
vocabulary size
set to
154,442
dimension
of
context projection weight matrix
set to
300","vocabulary size||set to||154,442
dimension||of||context projection weight matrix
context projection weight matrix||set to||300
","TL||has||vocabulary size
TL||has||dimension
","Hyperparameters||applying||TL
",,,,,,
hyperparameters,"We use the Adam optimizer , including gradient clipping , by the norm at a threshold of 5 .","use
Adam optimizer
including
gradient clipping
by
norm
at
threshold
of
5","Adam optimizer||including||gradient clipping
gradient clipping||by||norm
norm||at||threshold
threshold||of||5
",,"Hyperparameters||use||Adam optimizer
",,,,,,
hyperparameters,"For the purpose of regularization , we applied a dropout with a ratio of 0.5 ..","For
of
regularization
applied
dropout
with
ratio
0.5","regularization||applied||dropout
dropout||with||ratio
ratio||of||0.5
",,"Hyperparameters||For||regularization
",,,,,,
results,"Wiki QA : For the WikiQA dataset , the pointwise learning approach shows a better performance than the listwise learning approach .","For
WikiQA dataset
pointwise learning approach
shows
better performance
than
listwise learning approach","pointwise learning approach||shows||better performance
better performance||than||listwise learning approach
","WikiQA dataset||has||pointwise learning approach
","Results||For||WikiQA dataset
",,,,,,
results,We combine LM with the base model ( Comp - Clip + LM ) and observe a significant improvement in performance in terms of MAP ( 0.714 to 0.746 absolute ) .,"combine
LM
with
base model ( Comp - Clip + LM )
observe
significant improvement
in
performance
in terms of
MAP ( 0.714 to 0.746 absolute )","LM||with||base model ( Comp - Clip + LM )
LM||observe||significant improvement
significant improvement||in||performance
performance||in terms of||MAP ( 0.714 to 0.746 absolute )
",,,,,"WikiQA dataset||combine||LM
",,,
results,"When we add the LC method ( Comp - Clip + LM + LC ) , the best previous results are surpassed in terms of MAP ( 0.718 to 0.764 absolute ) .","add
LC method ( Comp - Clip + LM + LC )
best previous results
are
surpassed
in terms of
MAP ( 0.718 to 0.764 absolute )","best previous results||are||surpassed
surpassed||in terms of||MAP ( 0.718 to 0.764 absolute )
","LC method ( Comp - Clip + LM + LC )||has||best previous results
",,,,"WikiQA dataset||add||LC method ( Comp - Clip + LM + LC )
",,,
results,The pointwise learning approach also shows excellent performance with the TREC - QA dataset .,"pointwise learning approach
shows
excellent performance
with
TREC - QA dataset","pointwise learning approach||shows||excellent performance
","TREC - QA dataset||has||pointwise learning approach
","Results||with||TREC - QA dataset
",,,,,,
results,"As in the WikiQA case , we achieve additional performance gains in terms of the MAP as we apply LM , LC , and TL ( 0.850 , 0.868 and 0.875 , respectively ) .","achieve
additional performance gains
in terms of
MAP
apply
LM , LC , and TL
0.850 , 0.868 and 0.875","additional performance gains||in terms of||MAP
MAP||apply||LM , LC , and TL
","LM , LC , and TL||has||0.850 , 0.868 and 0.875
",,,,"TREC - QA dataset||achieve||additional performance gains
",,,
results,"In particular , our model outperforms the best previous result when we add LC method , ( Comp - Clip + LM + LC ) in terms of MAP ( 0.865 to 0.868 ) .","our model
outperforms
best previous result
add
LC method , ( Comp - Clip + LM + LC )
in terms of
MAP
0.865
to
0.868","our model||outperforms||best previous result
best previous result||add||LC method , ( Comp - Clip + LM + LC )
best previous result||in terms of||MAP
0.865||to||0.868
","MAP||has||0.865
",,,,,,"TREC - QA dataset||has||our model
",
research-problem,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,Neural Question Answering,,,,,"Contribution||has research problem||Neural Question Answering
",,,,
model,"In this paper , we propose an extremely simple neural ranking model for question answering that achieves highly competitive results on several benchmarks with only a fraction of the runtime and only 40K - 90 K parameters ( as opposed to millions ) .","propose
extremely simple neural ranking model
for
question answering","extremely simple neural ranking model||for||question answering
",,"Model||propose||extremely simple neural ranking model
",,,,,,
model,Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space .,"neural ranking
models
relationships
between
QA pairs
in
Hyperbolic space
instead of
Euclidean space","neural ranking||models||relationships
relationships||between||QA pairs
QA pairs||in||Hyperbolic space
Hyperbolic space||instead of||Euclidean space
",,,"Model||has||neural ranking
",,,,,
model,Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially .,"Hyperbolic space
is
embedding space
with
constant negative curvature
in which
distance
towards
border
is
increasing exponentially","Hyperbolic space||is||embedding space
embedding space||with||constant negative curvature
constant negative curvature||in which||distance
distance||is||increasing exponentially
distance||towards||border
",,,"Model||has||Hyperbolic space
",,,,,
baselines,YahooCQA,YahooCQA,,,,"Baselines||has||YahooCQA
",,,,,"YahooCQA||has||key competitors
"
baselines,YahooCQA,,,,,,,,,,
baselines,- The key competitors of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and HD - LSTM from Tay et al.,"key competitors
are
Neural Tensor LSTM ( NTN - LSTM )
HD - LSTM
from
Tay et al.","key competitors||are||Neural Tensor LSTM ( NTN - LSTM )
key competitors||are||HD - LSTM
HD - LSTM||from||Tay et al.
",,,,,,,,
baselines,"along with their implementation of the Convolutional Neural Tensor Network , vanilla CNN model , and the Okapi BM - 25 benchmark .","implementation
of
Convolutional Neural Tensor Network
vanilla CNN model
Okapi BM - 25 benchmark","implementation||of||Convolutional Neural Tensor Network
implementation||of||vanilla CNN model
implementation||of||Okapi BM - 25 benchmark
",,,,,,,"YahooCQA||has||implementation
",
baselines,"Additionally , we also report our own implementations of QA - BiLSTM , QA - CNN , AP - BiLSTM and AP - CNN on this dataset based on our experimental setup . WikiQA","of
WikiQA",,,,"Baselines||has||WikiQA
",,,,,"WikiQA||has||key competitors
"
baselines,"- The key competitors of this dataset are the Paragraph Vector ( PV ) and PV + Cnt models of Le and Mikolv , CNN + Cnt model from Yu et al. and LCLR ( Yih et al . ) .","key competitors
are
Paragraph Vector ( PV )
PV + Cnt models
Le and Mikolv
CNN + Cnt model
from
Yu et al.
LCLR ( Yih et al . )","key competitors||are||Paragraph Vector ( PV )
key competitors||are||PV + Cnt models
key competitors||are||CNN + Cnt model
CNN + Cnt model||from||Yu et al.
key competitors||are||LCLR ( Yih et al . )
","PV + Cnt models||of||Le and Mikolv
",,,,,,,
baselines,"For the clean version of this dataset , we also compare with AP - CNN and QA - BiLSTM / CNN .","compare with
AP - CNN
QA - BiLSTM / CNN",,,,,,"WikiQA||compare with||AP - CNN
WikiQA||compare with||QA - BiLSTM / CNN
",,,
hyperparameters,Hyper QA is implemented in Tensor - Flow .,"Hyper QA
implemented in
Tensor - Flow","Hyper QA||implemented in||Tensor - Flow
",,,"Hyperparameters||has||Hyper QA
",,,,,
hyperparameters,"We adopt the AdaGrad optimizer with initial learning rate tuned amongst { 0.2 , 0.1 , 0.05 , 0.01 } .","adopt
AdaGrad optimizer
with
initial learning rate
tuned amongst
{ 0.2 , 0.1 , 0.05 , 0.01 }","AdaGrad optimizer||with||initial learning rate
initial learning rate||tuned amongst||{ 0.2 , 0.1 , 0.05 , 0.01 }
",,"Hyperparameters||adopt||AdaGrad optimizer
",,,,,,
hyperparameters,"The batch size is tuned amongst { 50 , 100 , 200 } .","batch size
is
tuned
amongst
{ 50 , 100 , 200 }","batch size||is||tuned
tuned||amongst||{ 50 , 100 , 200 }
",,,"Hyperparameters||has||batch size
",,,,,
hyperparameters,Models are trained for 25 epochs and the model parameters are saved each time the performance on the validation set is topped .,"Models
are
trained for
25 epochs
model parameters
saved
each time
performance
on
validation set
is
topped","Models||trained for||25 epochs
model parameters||are||saved
saved||each time||performance
performance||is||topped
performance||on||validation set
","Models||has||model parameters
",,"Hyperparameters||has||Models
",,,,,
hyperparameters,"The dimension of the projection layer is tuned amongst { 100 , 200 , 300 , 400 } .","dimension
of
projection layer
is
tuned
amongst
{ 100 , 200 , 300 , 400 }","dimension||of||projection layer
projection layer||is||tuned
tuned||amongst||{ 100 , 200 , 300 , 400 }
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,"L2 regularization is tuned amongst { 0.001 , 0.0001 , 0.00001 }.","L2 regularization
is
tuned
amongst","L2 regularization||is||tuned
",,,"Hyperparameters||has||L2 regularization
",,,"tuned||amongst||{ 0.001 , 0.0001 , 0.00001 }
",,
hyperparameters,The negative sampling rate is tuned from 2 to 8 .,"negative sampling rate
is
tuned
from
2 to 8","negative sampling rate||is||tuned
tuned||from||2 to 8
",,,"Hyperparameters||has||negative sampling rate
",,,,,
results,reports the experimental results on SemEvalCQA .,"on
SemEvalCQA",,,"Results||on||SemEvalCQA
",,,,,,"SemEvalCQA||has||proposed approach
"
results,Our proposed approach achieves highly competitive performance on this dataset .,"proposed approach
achieves
highly competitive performance","proposed approach||achieves||highly competitive performance
",,,,,,,,
results,"Specifically , we have obtained the best P@1 performance over all , outperforming the state - of - the - art AI - CNN model by 3 % in terms of P@1 .","obtained
best P@1 performance over all
outperforming
state - of - the - art AI - CNN model
by
3 %
in terms of
P@1","state - of - the - art AI - CNN model||by||3 %
3 %||in terms of||P@1
","best P@1 performance over all||has||outperforming
outperforming||has||state - of - the - art AI - CNN model
",,,,"SemEvalCQA||obtained||best P@1 performance over all
",,,
results,The performance of our model on MAP is marginally short from the best performing model .,"performance
of
our model
on
MAP
is
marginally short
from
best performing model","performance||of||our model
our model||is||marginally short
marginally short||from||best performing model
our model||on||MAP
",,,,,,,"SemEvalCQA||has||performance
",
results,reports the results on TrecQA ( raw ) .,"on
TrecQA ( raw )",,,"Results||on||TrecQA ( raw )
",,,,,,"TrecQA ( raw )||has||Hyper QA
"
results,Hyper QA achieves very competitive performance on both MAP and MRR metrics .,"Hyper QA
achieves
competitive performance
MAP and MRR metrics","Hyper QA||achieves||competitive performance
","competitive performance||on||MAP and MRR metrics
",,,,,,,
results,"Specifically , Hyper QA outperforms the basic CNN model of ( S&M ) by 2 % ? 3 % in terms of MAP / MRR .","outperforms
basic CNN model of ( S&M )
by
2 % ? 3 %
in terms of
MAP / MRR","basic CNN model of ( S&M )||by||2 % ? 3 %
basic CNN model of ( S&M )||in terms of||MAP / MRR
","outperforms||has||basic CNN model of ( S&M )
",,,,,,"Hyper QA||has||outperforms
Hyper QA||has||outperforms
",
results,"Similarly , reports the results on TrecQA ( clean ) .",TrecQA ( clean ),,,,"Results||on||TrecQA ( clean )
",,,,,"TrecQA ( clean )||has||Hyper QA
"
results,"Similarly , Hyper QA also outperforms MP - CNN , AP - CNN and QA - CNN .","Hyper QA
outperforms
MP - CNN
AP - CNN
QA - CNN",,"outperforms||has||MP - CNN
outperforms||has||AP - CNN
outperforms||has||QA - CNN
",,,,,,,
research-problem,Neural Stored - program Memory,Neural Stored - program Memory,,,,,"Contribution||has research problem||Neural Stored - program Memory
",,,,
research-problem,"In this paper , we introduce a new memory to store weights for the controller , analogous to the stored - program memory in modern computer architectures .",stored - program memory,,,,,"Contribution||has research problem||stored - program memory
",,,,
model,Our goal is to advance a step further towards UTM by coupling a MANN with an external program memory .,"step further towards
UTM
by coupling
MANN
with
external program memory","UTM||by coupling||MANN
MANN||with||external program memory
",,"Model||step further towards||UTM
",,,,,,
model,"The program memory co-exists with the data memory in the MANN , providing more flexibility , reuseability and modularity in learning complicated tasks .","program memory
co-exists with
data memory
in
MANN
providing
learning
complicated tasks","program memory||co-exists with||data memory
data memory||in||MANN
data memory||in||learning
","learning||has||complicated tasks
",,"Model||has||program memory
",,,"learning||providing||flexibility
learning||providing||reuseability
learning||providing||modularity
",,
model,"The program memory stores the weights of the MANN 's controller network , which are retrieved quickly via a key - value attention mechanism across timesteps yet updated slowly via backpropagation .","stores
weights
of
MANN 's controller network
retrieved
quickly
via
key - value attention mechanism
across
timesteps
updated
slowly
via
backpropagation","weights||of||MANN 's controller network
weights||retrieved||quickly
quickly||via||key - value attention mechanism
key - value attention mechanism||across||timesteps
weights||updated||slowly
slowly||via||backpropagation
",,,,,"program memory||stores||weights
",,,
model,"By introducing a meta network to moderate the operations of the program memory , our model , henceforth referred to as Neural Stored - program Memory ( NSM ) , can learn to switch the programs / weights in the controller network appropriately , adapting to different functionalities aligning with different parts of a sequential task , or different tasks in continual and few - shot learning .","of
referred to
Neural Stored - program Memory ( NSM )
learn to
switch
programs / weights
in
controller network
appropriately
adapting to
different functionalities
aligning with
different parts
sequential task
different tasks
in
continual and few - shot learning","Neural Stored - program Memory ( NSM )||learn to||switch
programs / weights||in||controller network
programs / weights||adapting to||different functionalities
different functionalities||aligning with||different parts
different parts||of||sequential task
different functionalities||aligning with||different tasks
different tasks||in||continual and few - shot learning
","switch||has||programs / weights
controller network||has||appropriately
","Model||referred to||Neural Stored - program Memory ( NSM )
",,,,,,
results,"Except for the Copy task , which is too simple , other tasks observe convergence speed improvement of NUTM over that of NTM , thereby validating the benefit of using two programs across timesteps even for the single task setting .","other tasks
observe
convergence speed improvement
of
NUTM
over
NTM
validating
benefit
of using
two programs
across
timesteps
even for
single task setting","other tasks||observe||convergence speed improvement
convergence speed improvement||validating||benefit
benefit||of using||two programs
two programs||across||timesteps
two programs||even for||single task setting
convergence speed improvement||of||NUTM
NUTM||over||NTM
",,,"Results||has||other tasks
",,,,,
results,NUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences .,"NUTM
requires
fewer training samples
to
converge
generalizes
better
to
unseen sequences
that are
longer
than
training sequences","NUTM||generalizes||better
better||to||unseen sequences
unseen sequences||that are||longer
longer||than||training sequences
NUTM||requires||fewer training samples
fewer training samples||to||converge
",,,"Results||has||NUTM
",,,,,
ablation-analysis,"We run the task with three additional baselines : NUTM using direct attention ( DA ) , NUTM using key - value without regularization ( KV ) , NUTM using fixed , uniform program distribution ( UP ) and a vanilla NTM with 2 memory heads ( h = 2 ) .","with
three additional baselines
NUTM
using
direct attention ( DA )
NUTM
using
key - value without regularization ( KV )
NUTM
using
fixed , uniform program distribution ( UP )
vanilla NTM
with
2 memory heads ( h = 2 )","NUTM||using||direct attention ( DA )
NUTM||using||key - value without regularization ( KV )
NUTM||using||fixed , uniform program distribution ( UP )
vanilla NTM||with||2 memory heads ( h = 2 )
","three additional baselines||name||NUTM
three additional baselines||name||NUTM
three additional baselines||name||NUTM
three additional baselines||name||vanilla NTM
","Ablation analysis||with||three additional baselines
",,,,,,
ablation-analysis,The results demonstrate that DA exhibits fast yet shallow convergence .,"demonstrate
DA
exhibits
fast yet shallow convergence","DA||exhibits||fast yet shallow convergence
",,"Ablation analysis||demonstrate||DA
",,,,,,"DA||has||fails
"
ablation-analysis,"It tends to fall into local minima , which finally fails to reach zero loss .","fall into
local minima
fails
to reach
zero loss","fails||to reach||zero loss
",,,,,"DA||fall into||local minima
",,,
ablation-analysis,Key- value attention helps NUTM converge completely with fewer iterations .,"Key- value attention
helps
NUTM converge
with
fewer iterations","Key- value attention||helps||NUTM converge
NUTM converge||with||fewer iterations
",,,"Ablation analysis||has||Key- value attention
",,,,,"Key- value attention||has||performance
"
ablation-analysis,The performance is further improved with the proposed regularization loss .,"performance
is
further improved
with
proposed regularization loss","performance||is||further improved
further improved||with||proposed regularization loss
",,,,,,,,
ablation-analysis,UP underperforms NUTM as it lacks dynamic programs .,"UP
underperforms
NUTM
as
lacks
dynamic programs","underperforms||as||lacks
","UP||has||underperforms
lacks||has||dynamic programs
underperforms||has||NUTM
",,"Ablation analysis||has||UP
",,,,,
ablation-analysis,"The NTM with 2 heads shows slightly better convergence compared to the NTM , yet obviously underperforms NUTM ( p = 2 ) with 1 head and fewer parameters .","NTM
with
2 heads
shows
slightly better convergence
compared to
NTM
underperforms
NUTM ( p = 2 )
with
1 head and fewer parameters","NTM||with||2 heads
NTM||shows||underperforms
NUTM ( p = 2 )||with||1 head and fewer parameters
NTM||shows||slightly better convergence
slightly better convergence||compared to||NTM
","underperforms||has||NUTM ( p = 2 )
",,"Ablation analysis||has||NTM
",,,,,
research-problem,Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification,Answer Justification,,,,,"Contribution||has research problem||Answer Justification
",,,,
research-problem,"Developing interpretable machine learning ( ML ) models , that is , models where a human user can understand what the model is learning , is considered by many to be crucial for ensuring usability and accelerating progress .",Developing interpretable machine learning ( ML ) models,,,,,"Contribution||has research problem||Developing interpretable machine learning ( ML ) models
",,,,
approach,"Within this domain , we propose an approach that learns to both select and explain answers , when the only supervision available is for which answer is correct ( but not how to explain it ) .","that
learns
to
select and explain answers
when
only supervision available
is
which answer is correct
not
how to explain it","learns||to||select and explain answers
learns||when||only supervision available
only supervision available||is||which answer is correct
which answer is correct||not||how to explain it
",,"Approach||that||learns
",,,,,,
approach,"Intuitively , our approach chooses the justifications that provide the most help towards ranking the correct answers higher than incorrect ones .","chooses
justifications
that provide
most help
towards
ranking
correct answers
higher
than
incorrect ones","justifications||that provide||most help
most help||towards||ranking
higher||than||incorrect ones
","ranking||has||correct answers
correct answers||has||higher
","Approach||chooses||justifications
",,,,,,
approach,"More formally , our neural network approach alternates between using the current model with max - pooling to choose the highest scoring justifications for correct answers , and optimizing the answer ranking model given these justifications .","formally
our neural network approach
alternates
using
current model
with
max - pooling
to choose
highest scoring justifications
for
correct answers
optimizing
answer ranking model
given
justifications","alternates||using||current model
current model||with||max - pooling
current model||to choose||highest scoring justifications
highest scoring justifications||for||correct answers
alternates||optimizing||answer ranking model
answer ranking model||given||justifications
","our neural network approach||has||alternates
","Approach||formally||our neural network approach
",,,,,,
baselines,IR Baseline :,IR Baseline,,,,"Baselines||has||IR Baseline
",,,,,
baselines,"For this baseline , we rank answer candidates by the maximum tf .idf document retrieval score using an unboosted query of question and answer terms ( see Section 4.1 for retrieval details ) .","rank
answer candidates
by
maximum tf .idf document retrieval score
using
unboosted query
of
question and answer terms","answer candidates||using||unboosted query
unboosted query||of||question and answer terms
answer candidates||by||maximum tf .idf document retrieval score
",,,,,"IR Baseline||rank||answer candidates
",,,
baselines,IR ++ :,IR ++,,,,"Baselines||has||IR ++
",,,,,
baselines,"This baseline uses the same architecture as the full model , as described in Section 4.3 , but with only the IR ++ feature group .","uses
same architecture
as
full model
with
only the IR ++ feature group","same architecture||as||full model
",,,,,"IR ++||with||only the IR ++ feature group
IR ++||uses||same architecture
",,,
results,QA Performance,QA Performance,,,,"Results||has||QA Performance
",,,,,"QA Performance||has||Our full model
"
results,"Our full model that combines IR ++ , lexical overlap , discourse , and embeddings - based features , has a P@1 of 53.3 % ( line 7 ) , an absolute gain of 6.3 % over the strong IR baseline despite using the same background knowledge .","Our full model
combines
IR ++ , lexical overlap , discourse , and embeddings - based features
P@1
of
53.3 %
absolute gain
of
6.3 %
over
strong IR baseline","Our full model||combines||IR ++ , lexical overlap , discourse , and embeddings - based features
P@1||of||53.3 %
absolute gain||over||strong IR baseline
absolute gain||of||6.3 %
","IR ++ , lexical overlap , discourse , and embeddings - based features||has||P@1
53.3 %||has||absolute gain
",,,,,,,
results,also tackle the AI2 Kaggle question set with an approach that learns alignments between questions and structured and semistructured KB data .,"tackle
AI2 Kaggle question set",,,,,,"QA Performance||tackle||AI2 Kaggle question set
",,,
results,"By way of a loose comparison ( since we are evaluating on different data partitions ) , our model has approximately 5 % higher performance despite our simpler set of features and unstructured KB .","By way of
loose comparison
our model
approximately 5 % higher performance",,"loose comparison||has||our model
our model||has||approximately 5 % higher performance
",,,,"AI2 Kaggle question set||By way of||loose comparison
",,,
results,"In comparison to other systems that competed in the Kaggle challenge , our system comes in in 7th place out of 170 competitors ( top 4 % ) .","In comparison to
other systems
that
competed
in
Kaggle challenge
our system
comes in
7th place
out of
170 competitors","other systems||that||competed
competed||in||Kaggle challenge
our system||comes in||7th place
7th place||out of||170 competitors
","Kaggle challenge||has||our system
",,,,"QA Performance||In comparison to||other systems
",,,
results,Justification Performance,Justification Performance,,,,"Results||has||Justification Performance
",,,,,"Justification Performance||has||61 %
"
results,"Note that 61 % of the top - ranked justifications from our system were rated as Good as compared to 52 % from the IR baseline ( a gain of 9 % ) , despite the systems using identical corpora .","61 %
of
top - ranked justifications
from
our system
rated as
Good
compared to
52 %
from
IR baseline","61 %||of||top - ranked justifications
top - ranked justifications||from||our system
top - ranked justifications||rated as||Good
Good||compared to||52 %
52 %||from||IR baseline
",,,,,,,,
research-problem,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,Supervised Learning of Universal Sentence Representations,,,,,"Contribution||has research problem||Supervised Learning of Universal Sentence Representations
",,,,
approach,"In this paper , we study the task of learning universal representations of sentences , i.e. , a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks .","study
task
of
learning
universal representations of sentences
sentence encoder model
trained on
large corpus
subsequently transferred to
other tasks","task||of||learning
sentence encoder model||subsequently transferred to||other tasks
sentence encoder model||trained on||large corpus
","learning||has||universal representations of sentences
","Approach||study||task
","Approach||has||sentence encoder model
",,,,,
approach,"Here , we investigate whether supervised learning can be leveraged instead , taking inspiration from previous results in computer vision , where many models are pretrained on the ImageNet ) before being transferred .","investigate
supervised learning
can be
leveraged","supervised learning||can be||leveraged
",,"Approach||investigate||supervised learning
",,,,,,
approach,"Hence , we investigate the impact of the sentence encoding architecture on representational transferability , and compare convolutional , recurrent and even simpler word composition schemes .","impact
of
sentence encoding architecture
on
representational transferability
compare
convolutional , recurrent and even simpler word composition schemes","impact||of||sentence encoding architecture
sentence encoding architecture||compare||convolutional , recurrent and even simpler word composition schemes
sentence encoding architecture||on||representational transferability
",,,"Approach||investigate||impact
",,,,,
hyperparameters,"For all our models trained on SNLI , we use SGD with a learning rate of 0.1 and a weight decay of 0.99 .","For
models
trained on
SNLI
use
SGD
with
learning rate
of
0.1
weight decay
of
0.99","models||use||SGD
SGD||with||learning rate
learning rate||of||0.1
SGD||with||weight decay
weight decay||of||0.99
models||trained on||SNLI
",,"Hyperparameters||For||models
",,,,,,
hyperparameters,"At each epoch , we divide the learning rate by 5 if the dev accuracy decreases .","At
each epoch
divide
learning rate
by
5
if
dev accuracy
decreases","each epoch||divide||learning rate
learning rate||by||5
learning rate||if||dev accuracy
","dev accuracy||has||decreases
","Hyperparameters||At||each epoch
",,,,,,
hyperparameters,We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10 ?5 .,"use
minibatches
of size
64
training
is
stopped
when
learning rate
goes under
threshold of 10 ?5","minibatches||of size||64
training||is||stopped
stopped||when||learning rate
learning rate||goes under||threshold of 10 ?5
","minibatches||has||training
","Hyperparameters||use||minibatches
",,,,,,
hyperparameters,"For the classifier , we use a multi - layer perceptron with 1 hidden - layer of 512 hidden units .","classifier
use
multi - layer perceptron
with
1 hidden - layer
of
512 hidden units","classifier||use||multi - layer perceptron
multi - layer perceptron||with||1 hidden - layer
1 hidden - layer||of||512 hidden units
",,,"Hyperparameters||For||classifier
",,,,,
hyperparameters,We use opensource GloVe vectors trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .,"opensource GloVe vectors
trained on
Common Crawl 840B
with
300 dimensions
as
fixed word embeddings","opensource GloVe vectors||trained on||Common Crawl 840B
Common Crawl 840B||with||300 dimensions
Common Crawl 840B||as||fixed word embeddings
",,,"Hyperparameters||use||opensource GloVe vectors
",,,,,
results,Architecture impact,Architecture impact,,,,"Results||has||Architecture impact
",,,,,"Architecture impact||has||BiLSTM - 4096
"
results,The BiLSTM - 4096 with the max - pooling operation performs best on both SNLI and transfer tasks .,"BiLSTM - 4096
with
max - pooling operation
performs
best
on
SNLI and transfer tasks","BiLSTM - 4096||with||max - pooling operation
BiLSTM - 4096||performs||best
best||on||SNLI and transfer tasks
",,,,,,,,
results,"Looking at the micro and macro averages , we see that it performs significantly better than the other models LSTM , GRU , BiGRU - last , BiLSTM - Mean , inner-attention and the hierarchical - ConvNet. also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM - Mean for instance .","Looking at
micro and macro averages
performs
significantly better
than
other models
LSTM
GRU
BiGRU - last
BiLSTM - Mean
inner-attention","micro and macro averages||performs||significantly better
significantly better||than||other models
","other models||name||LSTM
other models||name||GRU
other models||name||BiGRU - last
other models||name||BiLSTM - Mean
other models||name||inner-attention
",,,,"BiLSTM - 4096||Looking at||micro and macro averages
",,,"other models||name||hierarchical - ConvNet
"
results,"For a given model , the transfer quality is also sensitive to the optimization algorithm : when training with Adam instead of SGD , we observed that the BiLSTM - max converged faster on SNLI ( 5 epochs instead of 10 ) , but obtained worse results on the transfer tasks , most likely because of the model and classifier 's increased capability to over-specialize on the training task .","transfer quality
sensitive to
optimization algorithm
when training with
Adam
instead of
SGD
observed
BiLSTM - max
converged
faster
on
SNLI
obtained
worse results
on
transfer tasks","transfer quality||sensitive to||optimization algorithm
transfer quality||when training with||Adam
Adam||instead of||SGD
Adam||observed||BiLSTM - max
BiLSTM - max||converged||faster
faster||on||SNLI
BiLSTM - max||obtained||worse results
worse results||on||transfer tasks
",,,,,,,"Architecture impact||has||transfer quality
",
results,Embedding size,Embedding size,,,,"Results||has||Embedding size
",,,,,"Embedding size||has||increased embedding sizes
"
results,"Since it is easier to linearly separate in high dimension , especially with logistic regression , it is not surprising that increased embedding sizes lead to increased performance for almost all models .","increased embedding sizes
lead to
increased performance
for
almost all models","increased embedding sizes||lead to||increased performance
increased performance||for||almost all models
",,,,,,,,
results,Comparison with SkipThought,Comparison with SkipThought,,,,"Results||has||Comparison with SkipThought
",,,,,
results,"With much less data ( 570 k compared to 64M sentences ) but with high - quality supervision from the SNLI dataset , we are able to consistently outperform the results obtained by SkipThought vectors .","With
much less data ( 570 k compared to 64M sentences )
with
high - quality supervision
from
SNLI dataset
able to
consistently outperform
results
obtained by
SkipThought vectors","much less data ( 570 k compared to 64M sentences )||with||high - quality supervision
high - quality supervision||from||SNLI dataset
much less data ( 570 k compared to 64M sentences )||able to||consistently outperform
results||obtained by||SkipThought vectors
","consistently outperform||has||results
",,,,"Comparison with SkipThought||With||much less data ( 570 k compared to 64M sentences )
",,,
results,"Our BiLSTM - max trained on SNLI performs much better than released SkipThought vectors on MR , CR , MPQA , SST , MRPC - accuracy , SICK - R , SICK - E and STS14 ( see ) .","Our BiLSTM - max
trained on
SNLI
performs
much better
than
released SkipThought vectors
on
MR
CR
MPQA
SST
MRPC - accuracy
SICK - R
SICK - E","Our BiLSTM - max||performs||much better
much better||than||released SkipThought vectors
released SkipThought vectors||on||MR
released SkipThought vectors||on||CR
released SkipThought vectors||on||MPQA
released SkipThought vectors||on||SST
released SkipThought vectors||on||MRPC - accuracy
released SkipThought vectors||on||SICK - R
released SkipThought vectors||on||SICK - E
Our BiLSTM - max||trained on||SNLI
",,,,,,"released SkipThought vectors||on||STS
","Comparison with SkipThought||has||Our BiLSTM - max
",
results,"Except for the SUBJ dataset , it also performs better than SkipThought - LN on MR , CR and MPQA .","Except for
SUBJ dataset
performs
better
than
SkipThought
on
MR
CR
MPQA","better||than||SkipThought
SkipThought||Except for||SUBJ dataset
SkipThought||on||MR
SkipThought||on||CR
SkipThought||on||MPQA
",,,,,"Comparison with SkipThought||performs||better
",,,
results,We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space ( pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST - LN ) .,"observe
looking at
STS14 results
cosine metrics
in
our embedding space
is
more semantically informative
than
SkipThought embedding space","STS14 results||observe||cosine metrics
cosine metrics||in||our embedding space
cosine metrics||is||more semantically informative
more semantically informative||than||SkipThought embedding space
",,,,,"Comparison with SkipThought||looking at||STS14 results
",,,
results,NLI as a supervised training set,NLI as a supervised training set,,,,"Results||has||NLI as a supervised training set
",,,,,
results,"Our findings indicate that our model trained on SNLI obtains much better over all results than models trained on other supervised tasks such as COCO , dictionary definitions , NMT , PPDB and SST .","indicate
our model
trained on
SNLI
obtains
much better over all results
than
models
trained on
other supervised tasks
such as
COCO
dictionary definitions
NMT
PPDB
SST","our model||obtains||much better over all results
much better over all results||than||models
models||trained on||other supervised tasks
other supervised tasks||such as||COCO
other supervised tasks||such as||dictionary definitions
other supervised tasks||such as||NMT
other supervised tasks||such as||PPDB
other supervised tasks||such as||SST
our model||trained on||SNLI
",,,,,"NLI as a supervised training set||indicate||our model
",,,
results,Domain adaptation on SICK tasks,Domain adaptation on SICK tasks,,,,"Results||has||Domain adaptation on SICK tasks
",,,,,"Domain adaptation on SICK tasks||has||Our transfer learning approach
"
results,Our transfer learning approach obtains better results than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,"Our transfer learning approach
obtains
better results
than
previous state - of - the - art","Our transfer learning approach||obtains||better results
better results||than||previous state - of - the - art
",,,,,,,,
results,"We obtain a pearson score of 0.885 on SICK - R while obtained 0.868 , and we obtain 86.3 % test accuracy on SICK - E while previous best handengineered models obtained 84.5 % .","obtain
pearson score
of
0.885
on
SICK - R
while
obtained
86.3 % test accuracy
on
SICK - E
previous best handengineered models
84.5 %","86.3 % test accuracy||while||previous best handengineered models
previous best handengineered models||obtained||84.5 %
86.3 % test accuracy||on||SICK - E
pearson score||of||0.885
0.885||on||SICK - R
",,,,,"Domain adaptation on SICK tasks||obtain||86.3 % test accuracy
Domain adaptation on SICK tasks||obtain||pearson score
",,,
results,"We also significantly outperformed previous transfer learning approaches on SICK - E ( Bowman et al. , 2015 ) that used the parameters of an LSTM model trained on SNLI to fine - tune on SICK ( 80.8 % accuracy ) .","significantly outperformed
previous transfer learning approaches
on
SICK - E
that used
parameters
of
LSTM model
trained on
SNLI
to fine - tune on
SICK","previous transfer learning approaches||that used||parameters
parameters||to fine - tune on||SICK
parameters||of||LSTM model
parameters||trained on||SNLI
previous transfer learning approaches||on||SICK - E
","significantly outperformed||has||previous transfer learning approaches
",,,,,,"Domain adaptation on SICK tasks||has||significantly outperformed
",
results,Image - caption retrieval results,Image - caption retrieval results,,,,"Results||has||Image - caption retrieval results
",,,,,
results,"When trained with ResNet features and 30 k more training data , the SkipThought vectors perform significantly better than the original setting , going from 33.8 to 37.9 for caption retrieval R@1 , and from 25.9 to 30.6 on image retrieval R@1 .","trained with
ResNet features and 30 k more training data
SkipThought vectors
perform
significantly better
than
original setting
going from
33.8 to 37.9
for
caption retrieval R@1
from
25.9 to 30.6
on
image retrieval R@1","SkipThought vectors||going from||33.8 to 37.9
33.8 to 37.9||for||caption retrieval R@1
SkipThought vectors||perform||significantly better
significantly better||than||original setting
SkipThought vectors||from||25.9 to 30.6
25.9 to 30.6||on||image retrieval R@1
","ResNet features and 30 k more training data||has||SkipThought vectors
",,,,"Image - caption retrieval results||trained with||ResNet features and 30 k more training data
",,,
results,"Our approach pushes the results even further , from 37.9 to 42.4 on cap-tion retrieval , and 30.6 to 33.2 on image retrieval .","Our approach
pushes
results
even further
from
37.9
to
42.4
on
cap-tion retrieval
30.6
to
33.2
on
image retrieval","Our approach||pushes||results
results||from||30.6
30.6||to||33.2
33.2||on||image retrieval
results||from||37.9
37.9||to||42.4
42.4||on||cap-tion retrieval
","results||has||even further
",,,,,,"Image - caption retrieval results||has||Our approach
",
results,MultiGenre NLI,MultiGenre NLI,,,,"Results||has||MultiGenre NLI
",,,,,
results,We observe a significant boost in performance over all compared to the model trained only on SLNI .,"observe
significant boost
in
performance over all
compared to
model
trained only on
SLNI","significant boost||in||performance over all
significant boost||compared to||model
model||trained only on||SLNI
",,,,,"MultiGenre NLI||observe||significant boost
",,,
results,"Our model even reaches AdaSent performance on CR , suggesting that having a larger coverage for the training task helps learn even better general representations .","Our model
reaches
AdaSent performance
on
CR","Our model||reaches||AdaSent performance
AdaSent performance||on||CR
",,,,,,,"MultiGenre NLI||has||Our model
",
research-problem,Structural Embedding of Syntactic Trees for Machine Comprehension,Machine Comprehension,,,,,"Contribution||has research problem||Machine Comprehension
",,,,
research-problem,"Reading comprehension such as SQuAD or News QA requires identifying a span from a given context , which is an extension to the traditional question answering task , aiming at responding questions posed by human with natural language .",Reading comprehension,,,,,"Contribution||has research problem||Reading comprehension
",,,,
model,"In this paper , we propose Structural Embedding of Syntactic Trees ( SEST ) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task .","propose
Structural Embedding of Syntactic Trees ( SEST )
encode
syntactic information
structured by
constituency tree and dependency tree
into
neural attention models
for
question answering task","Structural Embedding of Syntactic Trees ( SEST )||encode||syntactic information
syntactic information||into||neural attention models
neural attention models||for||question answering task
syntactic information||structured by||constituency tree and dependency tree
",,"Model||propose||Structural Embedding of Syntactic Trees ( SEST )
",,,,,,
experimental-setup,We run our experiments on a machine that contains a single GTX 1080 GPU with 8 GB VRAM .,"run
our experiments
on
machine
contains
single GTX 1080 GPU
with
8 GB VRAM","our experiments||on||machine
machine||contains||single GTX 1080 GPU
single GTX 1080 GPU||with||8 GB VRAM
",,"Experimental setup||run||our experiments
",,,,,,
experimental-setup,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as part of the input into the model .","use
variable character embedding
with
fixed pre-trained word embedding
to serve
part
of
input
into
model","variable character embedding||with||fixed pre-trained word embedding
variable character embedding||to serve||part
part||of||input
input||into||model
",,"Experimental setup||use||variable character embedding
",,,,,,
experimental-setup,The character embedding is implemented using CNN with a one -dimensional layer consists of 100 units with a channel size of 5 .,"character embedding
implemented using
CNN
with
one -dimensional layer
consists of
100 units
with
channel size
of
5","character embedding||with||one -dimensional layer
one -dimensional layer||consists of||100 units
100 units||with||channel size
channel size||of||5
character embedding||implemented using||CNN
",,,"Experimental setup||has||character embedding
",,,,,
experimental-setup,It has an input depth of 8 .,"input depth
of
8","input depth||of||8
",,,"Experimental setup||has||input depth
",,,,,
experimental-setup,The max length of SQuAD is 16 which means there are a maximum 16 words in a sentence .,"max length
of
SQuAD
is
16","max length||of||SQuAD
SQuAD||is||16
",,,"Experimental setup||has||max length
",,,,,
experimental-setup,"The fixed word embedding has a dimension of 100 , which is provided by the GloVe data set .","fixed word embedding
dimension
of
100
provided by
GloVe data set","dimension||of||100
fixed word embedding||provided by||GloVe data set
","fixed word embedding||has||dimension
",,"Experimental setup||has||fixed word embedding
",,,,,
experimental-setup,The POS model contains syntactic information with 39 different POS tags that serve as both input and output .,"POS model
contains
syntactic information
with
39 different POS tags
serve
both input and output","POS model||contains||syntactic information
syntactic information||with||39 different POS tags
syntactic information||serve||both input and output
",,,"Experimental setup||has||POS model
",,,,,
experimental-setup,For SECT and SEDT the input of the model has a size of 8 with 30 units to be output .,"For
SECT and SEDT
input
of
model
size
of
8
with
30 units
to be
output","input||with||30 units
30 units||to be||output
input||of||model
size||of||8
","SECT and SEDT||has||input
input||has||size
","Experimental setup||For||SECT and SEDT
",,,,,,
experimental-setup,"Both of them has a maximum length size that is set to be 10 and 20 respectively , which values will be further discussed in Section 4.5 .","maximum length size
set to be
10 and 20","maximum length size||set to be||10 and 20
",,,,,,,"SECT and SEDT||has||maximum length size
",
experiments,Predictive Performance,Predictive Performance,,,,,,,,"Tasks||has||Predictive Performance
","Predictive Performance||has||Baselines
"
experiments,"We first compared the performance of single models between the baseline approach BiDAF and the proposed SEST approaches , including SE - POS , SECT - LSTM , SECT - CNN , SEDT - LSTM , and SEDT - CNN , on the development dataset of SQuAD .","compared
performance
of
baseline approach BiDAF
proposed SEST approaches
including
SE - POS
SECT - LSTM
SECT - CNN
SEDT - LSTM
SEDT - CNN
on
development dataset
SQuAD","proposed SEST approaches||including||SE - POS
proposed SEST approaches||including||SECT - LSTM
proposed SEST approaches||including||SECT - CNN
proposed SEST approaches||including||SEDT - LSTM
proposed SEST approaches||including||SEDT - CNN
performance||on||development dataset
development dataset||of||SQuAD
","performance||has||baseline approach BiDAF
performance||has||proposed SEST approaches
",,,,"Baselines||compared||performance
",,,
experiments,"Another observation is that our propose models achieve higher relative improvements in EM scores than F 1 scores over the baseline methods , providing the evidence that syntactic information can accurately locate the boundaries of the answer .","our propose models
achieve
higher relative improvements
in
EM scores
than
F 1 scores
over
baseline methods","our propose models||achieve||higher relative improvements
higher relative improvements||in||EM scores
EM scores||over||baseline methods
EM scores||than||F 1 scores
",,,,,,,"Results||has||our propose models
",
experiments,"Moreover , we found that both SECT - LSTM and SEDT - LSTM have better performance than their CNN counterparts , which suggests that LSTM can more effectively preserve the syntactic information .","found that
SECT - LSTM and SEDT - LSTM
have
better performance
than
CNN counterparts","SECT - LSTM and SEDT - LSTM||have||better performance
better performance||than||CNN counterparts
",,,,,"Results||found that||SECT - LSTM and SEDT - LSTM
",,,
experiments,Contribution of Syntactic Sequence,Contribution of Syntactic Sequence,,,,,,,,"Tasks||has||Contribution of Syntactic Sequence
","Contribution of Syntactic Sequence||has||Results
"
experiments,From the table we see that both the ordering and the contents of the syntactic tree are important for the models to work properly : constituency and dependency trees achieved over 20 % boost on performance compared to the randomly generated ones and our proposed ordering also out - performed the random ordering .,"both
ordering
contents
of
syntactic tree
are
important
for
models
to work
properly
constituency and dependency trees
achieved
over 20 % boost
on
performance
compared to
randomly generated ones
our proposed ordering
out - performed
random ordering","important||for||models
models||to work||properly
important||both||ordering
important||both||contents
contents||of||syntactic tree
constituency and dependency trees||achieved||over 20 % boost
our proposed ordering||out - performed||random ordering
over 20 % boost||compared to||randomly generated ones
over 20 % boost||on||performance
","over 20 % boost||has||our proposed ordering
",,,,"Results||are||important
",,"Results||has||constituency and dependency trees
",
experiments,It also worth mentioning that the ordering of dependency trees seems to have less impact on the performance compared to that of the constituency trees .,"ordering
of
dependency trees
seems to have
less impact
on
performance
compared to
constituency trees","ordering||of||dependency trees
dependency trees||seems to have||less impact
less impact||compared to||constituency trees
less impact||on||performance
",,,,,,,"Results||has||ordering
",
experiments,Window Size Analysis,Window Size Analysis,,,,,,,,"Tasks||has||Window Size Analysis
","Window Size Analysis||has||Results
"
experiments,"In practice , we found that limiting the window size also benefits the performance of our models .","limiting
window size
benefits
performance
of
our models","performance||of||our models
","window size||has||benefits
benefits||has||performance
",,,,"Results||limiting||window size
",,,
experiments,In general the results illustrate that performances of the models increase with the length of the window .,"illustrate
performances
of
models
increase
with
length
of
window","performances||of||models
increase||with||length
length||of||window
","performances||has||increase
",,,,"Results||illustrate||performances
",,,
experiments,We also observed that larger window size does not generate predictive results that is as good as the one with window size set to 10 .,"observed
larger window size
does not generate
predictive results
as good as
one with window size
set to
10","larger window size||does not generate||predictive results
predictive results||as good as||one with window size
one with window size||set to||10
",,,,,"Results||observed||larger window size
",,,
research-problem,Question Answering with Subgraph Embeddings,Question Answering,,,,,"Contribution||has research problem||Question Answering
",,,,
research-problem,This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features .,answer questions on a broad range of topics,,,,,"Contribution||has research problem||answer questions on a broad range of topics
",,,,
research-problem,Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been along standing goal in Artificial Intelligence .,automatically answer questions asked in natural language on any topic or in any domain,,,,,"Contribution||has research problem||automatically answer questions asked in natural language on any topic or in any domain
",,,,
research-problem,"With the rise of large scale structured knowledge bases ( KBs ) , this problem , known as open - domain question answering ( or open QA ) , boils down to being able to query efficiently such databases with natural language .",open - domain question answering ( or open QA ),,,,,"Contribution||has research problem||open - domain question answering ( or open QA )
",,,,
research-problem,"These KBs , such as Freebase encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format .",open QA,,,,,"Contribution||has research problem||open QA
",,,,
model,"In this paper , we improve the model of by providing the ability to answer more complicated questions .","providing
ability
to answer
more complicated questions","ability||to answer||more complicated questions
",,"Model||providing||ability
",,,,,,
model,s The main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which encodes the question - answer path and surrounding subgraph of the KB .,"of
more sophisticated inference procedure
is
efficient
consider
longer paths
answers
richer representation
of
encodes
question - answer path
surrounding subgraph
KB","richer representation||of||answers
richer representation||encodes||question - answer path
richer representation||encodes||surrounding subgraph
surrounding subgraph||of||KB
more sophisticated inference procedure||is||efficient
more sophisticated inference procedure||consider||longer paths
",,,"Model||has||richer representation
Model||has||more sophisticated inference procedure
",,,,,
results,Replacing C 2 by C 1 induces a large drop in performance because many questions do not have answers that are directly connected to their inluded entity ( not in C 1 ) .,"Replacing
C 2
by
C 1
induces
large drop
in
performance","C 2||induces||large drop
large drop||in||performance
C 2||by||C 1
",,"Results||Replacing||C 2
",,,,,,
results,"However , using all 2 - hops connections as a candidate set is also detrimental , because the larger number of candidates confuses ( and slows a lot ) our ranking based inference .","using
all 2 - hops connections
as
candidate set
is
detrimental","all 2 - hops connections||as||candidate set
all 2 - hops connections||is||detrimental
",,"Results||using||all 2 - hops connections
",,,,,,
results,"Our results also verify our hypothesis of Section 3.1 , that a richer representation for answers ( using the local subgraph ) can store more pertinent information .","verify
hypothesis
that
richer representation
for
answers
store
more pertinent information","hypothesis||that||richer representation
richer representation||for||answers
richer representation||store||more pertinent information
",,"Results||verify||hypothesis
",,,,,,
results,"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the Path representation and C 1 as candidate set .","demonstrate
greatly improve
upon
model
corresponds to
setting
with
Path representation
C 1
as
candidate set","greatly improve||corresponds to||setting
setting||with||Path representation
setting||with||C 1
C 1||as||candidate set
greatly improve||upon||model
",,"Results||demonstrate||greatly improve
",,,,,,
results,"The ensemble improves the state - of - the - art , and indicates that our models are significantly different in their design .","ensemble
improves
state - of - the - art",,"ensemble||has||improves
improves||has||state - of - the - art
",,"Results||has||ensemble
",,,,,
research-problem,Recurrent Relational Networks,Recurrent Relational Networks,,,,,"Contribution||has research problem||Recurrent Relational Networks
",,,,
research-problem,"We introduce the recurrent relational network , a general purpose module that operates on a graph representation of objects .",recurrent relational network,,,,,"Contribution||has research problem||recurrent relational network
",,,,
model,"Toward generally realizing the ability to methodically reason about objects and their interactions over many steps , this paper introduces a composite function , the recurrent relational network .","Toward generally realizing
ability
to methodically reason about
objects
interactions
over
many steps
introduces
composite function
recurrent relational network","composite function||Toward generally realizing||ability
ability||to methodically reason about||objects
ability||to methodically reason about||interactions
interactions||over||many steps
","composite function||name||recurrent relational network
","Model||introduces||composite function
",,,,,,
model,It serves as a modular component for many - step relational reasoning in end - to - end differentiable learning systems .,"serves
modular component
for
many - step relational reasoning
in
end - to - end differentiable learning systems","modular component||for||many - step relational reasoning
many - step relational reasoning||in||end - to - end differentiable learning systems
",,,,,"composite function||serves||modular component
",,,
model,"It encodes the inductive biases that 1 ) objects exists in the world 2 ) they can be sufficiently described by properties 3 ) properties can changeover time 4 ) objects can affect each other and 5 ) given the properties , the effects object have on each other is invariant to time .","encodes
inductive biases
that
objects
exists in
world
described by
properties
changeover
time
affect
each other
invariant to
time","inductive biases||that||objects
objects||exists in||world
objects||affect||each other
each other||invariant to||time
objects||described by||properties
properties||changeover||time
",,,,,"composite function||encodes||inductive biases
",,,
model,"An important insight from the work of is to decompose a function for relational reasoning into two components or "" modules "" :","decompose
function
for
relational reasoning
into
two components","function||into||two components
function||for||relational reasoning
",,"Model||decompose||function
",,,,,,"two components||name||perceptual front - end
two components||name||relational reasoning module
"
model,"a perceptual front - end , which is tasked to recognize objects in the raw input and represent them as vectors , and a relational reasoning module , which uses the representation to reason about the objects and their interactions .","perceptual front - end
to recognize
objects
in
raw input
represent them as
vectors
relational reasoning module
uses
representation","perceptual front - end||to recognize||objects
objects||in||raw input
perceptual front - end||represent them as||vectors
relational reasoning module||uses||representation
",,,,,,,,"representation||to reason about||objects and their interactions
"
model,Both modules are trained jointly end - to - end .,"trained
jointly end - to - end",,,,,,"two components||trained||jointly end - to - end
",,,
model,"In computer science parlance , the relational reasoning module implements an interface : it operates on a graph of nodes and directed edges , where the nodes are represented by real valued vectors , and is differentiable .","relational reasoning module
implements
interface
operates on
graph
of
nodes and directed edges
where
nodes
represented by
real valued vectors
is
differentiable","relational reasoning module||implements||interface
interface||operates on||graph
graph||of||nodes and directed edges
graph||where||nodes
nodes||represented by||real valued vectors
nodes||is||differentiable
",,,"Model||has||relational reasoning module
",,,,,
research-problem,Recurrent Relational Networks,,,,,,,,,,
code,Code to reproduce all experiments can be found at github.com/rasmusbergpalm/recurrent-relationalnetworks. designed as a set of prerequisite tasks for reasoning .,github.com/rasmusbergpalm/recurrent-relationalnetworks.,,,,,"Contribution||Code||github.com/rasmusbergpalm/recurrent-relationalnetworks.
",,,,
experiments,bAbI question - answering tasks,bAbI question - answering tasks,,,,,,,,"Tasks||has||bAbI question - answering tasks
","bAbI question - answering tasks||has||Results
"
experiments,"Surprisingly , we find that we only need a single step of relational reasoning to solve all the bAbI tasks .","need
single step of relational reasoning
to solve
all the bAbI tasks","single step of relational reasoning||to solve||all the bAbI tasks
",,,,,"Results||need||single step of relational reasoning
",,,
experiments,"Regardless , it appears multiple steps of relational reasoning are not important for the bAbI dataset .","appears
multiple steps of relational reasoning
not
important
for
bAbI dataset","multiple steps of relational reasoning||not||important
important||for||bAbI dataset
",,,,,"Results||appears||multiple steps of relational reasoning
",,,
experiments,Pretty - CLEVR,Pretty - CLEVR,,,,,,,,"Tasks||has||Pretty - CLEVR
","Pretty - CLEVR||has||Results
"
experiments,"Mirroring the results from the "" Sort - of - CLEVR "" dataset the MLP perfectly solves the non-relational questions , but struggle with even single jump questions and seem to lower bound the performance of the relational networks .","MLP
perfectly solves
non-relational questions
struggle
with
single jump questions","struggle||with||single jump questions
","MLP||has||perfectly solves
perfectly solves||has||non-relational questions
MLP||has||struggle
",,,,,,"Results||has||MLP
",
experiments,"The relational network solves the non-relational questions as well as the ones requiring a single jump , but the accuracy sharply drops off with more jumps .","relational network
solves
non-relational questions
as well as
ones
requiring
single jump
accuracy
sharply drops off
with
more jumps","relational network||solves||non-relational questions
sharply drops off||with||more jumps
relational network||as well as||ones
ones||requiring||single jump
","relational network||has||accuracy
accuracy||has||sharply drops off
",,,,,,"Results||has||relational network
",
experiments,Sudoku,Sudoku,,,,,,,,"Tasks||has||Sudoku
","Sudoku||has||Results
"
experiments,Our network learns to solve 94.1 % of even the hardest 17 - givens Sudokus after 32 steps .,"Our network
learns to
solve
94.1 %
of
even the hardest 17 - givens Sudokus
after
32 steps","Our network||learns to||solve
94.1 %||of||even the hardest 17 - givens Sudokus
94.1 %||after||32 steps
","solve||has||94.1 %
",,,,,,"Results||has||Our network
",
experiments,"See figure 4 . We can see that even simple Sudokus with 33 givens require upwards of 10 steps of relational reasoning , whereas the harder 17 givens continue to improve even after 32 steps .","see that
even simple Sudokus
with
33 givens
require
upwards of 10 steps
of
relational reasoning
harder
17 givens
continue to
improve
after
32 steps","harder||continue to||improve
improve||after||32 steps
even simple Sudokus||with||33 givens
even simple Sudokus||require||upwards of 10 steps
upwards of 10 steps||of||relational reasoning
","harder||has||17 givens
",,,,"Results||see that||harder
Results||see that||even simple Sudokus
",,,
experiments,At 64 steps the accuracy for the 17 givens puzzles increases to 96.6 % .,"At
64 steps
accuracy
for
17 givens puzzles
increases
to
96.6 %","accuracy||for||17 givens puzzles
increases||to||96.6 %
","64 steps||has||accuracy
accuracy||has||increases
",,,,"Results||At||64 steps
",,,
experiments,"Our network outperforms loopy belief propagation , with parallel and random messages passing updates .","outperforms
loopy belief propagation
with
parallel and random messages
passing
updates","loopy belief propagation||with||parallel and random messages
parallel and random messages||passing||updates
","outperforms||has||loopy belief propagation
",,,,,,"Our network||has||outperforms
",
experiments,"It also outperforms a version of loopy belief propagation modified specifically for solving Sudokus that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .","version
of
loopy belief propagation
modified specifically
for solving
Sudokus
that uses
250 steps","version||of||loopy belief propagation
modified specifically||for solving||Sudokus
Sudokus||that uses||250 steps
","loopy belief propagation||has||modified specifically
",,,,,,"outperforms||has||version
",
experiments,"Finally we outperform Park which treats the Sudoku as a 9x9 image , uses 10 convolutional layers , iteratively picks the most probable digit , and evaluate on easier Sudokus with 24 - 36 givens .","outperform
Park
treats
Sudoku
as
9x9 image
10 convolutional layers","Sudoku||as||9x9 image
outperform||Park||10 convolutional layers
",,,,,"has||treats||Sudoku
","outperform||Park||has
","Results||has||outperform
",
research-problem,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,,,,,"Contribution||has research problem||ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE
",,,,
research-problem,"Hypernymy , textual entailment , and image captioning can be seen as special cases of a single visual - semantic hierarchy over words , sentences , and images .","single visual - semantic hierarchy over words , sentences , and images",,,,,"Contribution||has research problem||single visual - semantic hierarchy over words , sentences , and images
",,,,
research-problem,"In fact , all three relations can be seen as special cases of a partial order over images and language , illustrated in , which we refer to as the visualsemantic hierarchy .",visualsemantic hierarchy,,,,,"Contribution||has research problem||visualsemantic hierarchy
",,,,
model,"In contrast , we propose to exploit the partial order structure of the visual - semantic hierarchy by learning a mapping which is not distance - preserving but order - preserving between the visualsemantic hierarchy and a partial order over the embedding space .","exploit
partial order structure
of
visual - semantic hierarchy
by learning
mapping
not
distance - preserving
but
order - preserving
between
visualsemantic hierarchy
partial order
over
embedding space",,,,,,,,,
model,We call embeddings learned in this way order- embeddings .,"call
embeddings
learned
order- embeddings","embeddings||learned||order- embeddings
",,,,,"mapping||call||embeddings
",,,
experiments,HYPERNYM PREDICTION,HYPERNYM PREDICTION,,,,,,,,"Tasks||has||HYPERNYM PREDICTION
","HYPERNYM PREDICTION||has||Hyperparameters
"
experiments,"To train the model , we use the standard pairwise ranking objective from Eq. ( 5 ) .","use
standard pairwise ranking objective",,,,,,"Hyperparameters||use||standard pairwise ranking objective
",,,
experiments,"We sample minibatches of 128 random image - caption pairs , and draw all contrastive terms from the minibatch , giving us 127 contrastive images for each caption and captions for each image .","sample
minibatches
of
128 random image - caption pairs
draw
contrastive terms
from
minibatch
giving
127 contrastive images
for
each caption
captions
for
each image","contrastive terms||from||minibatch
contrastive terms||giving||127 contrastive images
127 contrastive images||for||each caption
contrastive terms||giving||captions
captions||for||each image
minibatches||of||128 random image - caption pairs
",,,,,"Hyperparameters||draw||contrastive terms
Hyperparameters||sample||minibatches
",,,
experiments,"We train for 15 - 30 epochs using the Adam optimizer with learning rate 0.001 , and early stopping on the validation set .","train for
15 - 30 epochs
using
Adam optimizer
with
learning rate 0.001
early stopping
on
validation set","15 - 30 epochs||with||learning rate 0.001
15 - 30 epochs||with||early stopping
early stopping||on||validation set
15 - 30 epochs||using||Adam optimizer
early stopping||on||validation set
",,,,,"Hyperparameters||train for||15 - 30 epochs
",,,
experiments,"We set the dimension of the embedding space and the GRU hidden state N to 1024 , the dimension of the learned word embeddings to 300 , and the margin ? to 0.05 .","set
dimension
of
embedding space and the GRU hidden state N
to
1024
learned word embeddings
to
300
margin
to
0.05","dimension||of||margin
margin||to||0.05
dimension||of||embedding space and the GRU hidden state N
embedding space and the GRU hidden state N||to||1024
dimension||of||learned word embeddings
learned word embeddings||to||300
",,,,,"Hyperparameters||set||dimension
Hyperparameters||set||dimension
",,,
experiments,"For consistency with and to mitigate overfitting , we constrain the caption and image embeddings to have unit L2 norm .","constrain
caption and image embeddings
to have
unit L2 norm","caption and image embeddings||to have||unit L2 norm
",,,,,"Hyperparameters||constrain||caption and image embeddings
",,,
experiments,We see that order- embeddings outperform the skipthought baseline despite not using external text corpora .,"see that
order- embeddings
outperform
skipthought baseline
not using
external text corpora","outperform||not using||external text corpora
","order- embeddings||has||outperform
outperform||has||skipthought baseline
",,,,"Results||see that||order- embeddings
",,,
experiments,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,,,,,,,,"Tasks||has||TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE
","TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE||has||Hyperparameters
"
experiments,"Just as for caption - image ranking , we set the dimensions of the embedding space and GRU hidden state to be 1024 , the dimension of the word embeddings to be 300 , and constrain the embeddings to have unit L2 norm .","set
dimensions
of
embedding space and GRU hidden state
to be
1024
dimension
of
word embeddings
to be
300
constrain
embeddings
to have
unit L2 norm","dimension||of||word embeddings
word embeddings||to be||300
dimensions||of||embedding space and GRU hidden state
embedding space and GRU hidden state||to be||1024
embeddings||to have||unit L2 norm
",,,,,"Hyperparameters||set||dimensions
Hyperparameters||constrain||embeddings
",,,
experiments,We train for 10 epochs with batches of 128 sentence pairs .,"train for
10 epochs
with
batches
of
128 sentence pairs","10 epochs||with||batches
batches||of||128 sentence pairs
",,,,,"Hyperparameters||train for||10 epochs
",,,
experiments,We use the Adam optimizer with learning rate 0.001 and early stopping on the validation set .,"use
Adam optimizer
with
learning rate 0.001
early stopping
on
validation set","Adam optimizer||with||learning rate 0.001
Adam optimizer||with||early stopping
",,,,,"Hyperparameters||use||Adam optimizer
",,,
research-problem,ReasoNet : Learning to Stop Reading in Machine Comprehension,Machine Comprehension,,,,,"Contribution||has research problem||Machine Comprehension
",,,,
model,"With this motivation , we propose a novel neural network architecture called Reasoning Network ( ReasoNet ) .","propose
novel neural network architecture
called
Reasoning Network ( ReasoNet )","novel neural network architecture||called||Reasoning Network ( ReasoNet )
",,"Model||propose||novel neural network architecture
",,,,,,
model,"With a question in mind , ReasoNets read a document repeatedly , each time focusing on di erent parts of the document until a satisfying answer is found or formed .","With
question
in
mind
ReasoNets
read
document
repeatedly
focusing on
di erent parts
of
document
until
satisfying answer
is
found or formed","question||in||mind
ReasoNets||read||document
document||focusing on||di erent parts
di erent parts||of||document
di erent parts||until||satisfying answer
satisfying answer||is||found or formed
","mind||has||ReasoNets
document||has||repeatedly
","Model||With||question
",,,,,,
model,"Moreover , unlike previous approaches using xed number of hops or iterations , ReasoNets introduce a termination state in the inference .","introduce
termination state
in
inference","termination state||in||inference
",,,,,"ReasoNets||introduce||termination state
",,,
model,"Motivated by , we tackle this challenge by proposing a reinforcement learning approach , which utilizes an instance - dependent reward baseline , to successfully train ReasoNets .","proposing
reinforcement learning approach
utilizes
instance - dependent reward baseline
to successfully train
ReasoNets","ReasoNets||proposing||reinforcement learning approach
reinforcement learning approach||utilizes||instance - dependent reward baseline
instance - dependent reward baseline||to successfully train||ReasoNets
",,,,,,,,
experiments,CNN and Daily Mail Datasets,CNN and Daily Mail Datasets,,,,,,,,"Tasks||has||CNN and Daily Mail Datasets
","CNN and Daily Mail Datasets||has||Experimental setup
"
experiments,"Vocab Size : For training our ReasoNet , we keep the most frequent | V | = 101 k words ( not including 584 entities and 1 placeholder marker ) in the CNN dataset , and | V | = 151 k words ( not including 530 entities and 1 placeholder marker ) in the Daily Mail dataset .","Vocab Size
keep
most frequent | V | = 101 k words
in
CNN dataset
| V | = 151 k words
in
Daily Mail dataset","Vocab Size||keep||most frequent | V | = 101 k words
most frequent | V | = 101 k words||in||CNN dataset
Vocab Size||keep||| V | = 151 k words
| V | = 151 k words||in||Daily Mail dataset
",,,,,,,"Experimental setup||has||Vocab Size
Experimental setup||has||Vocab Size
",
experiments,Embedding Layer :,Embedding Layer,,,,,,,,"Experimental setup||has||Embedding Layer
Experimental setup||has||Embedding Layer
Experimental setup||has||Embedding Layer
",
experiments,"We choose 300 - dimensional word embeddings , and use the 300 - dimensional pretrained Glove word embeddings for initialization .","choose
300 - dimensional word embeddings
use
300 - dimensional pretrained Glove word embeddings
for
initialization","300 - dimensional pretrained Glove word embeddings||for||initialization
",,,,,"Embedding Layer||use||300 - dimensional pretrained Glove word embeddings
Embedding Layer||choose||300 - dimensional word embeddings
",,,
experiments,We also apply dropout with probability 0.2 to the embedding layer .,"apply
dropout
with
probability 0.2
to
embedding layer","dropout||with||probability 0.2
probability 0.2||to||embedding layer
",,,,,"Embedding Layer||apply||dropout
",,,
experiments,"We use ADAM optimizer for parameter optimization with an initial learning rate of 0.0005 , ? 1 = 0.9 and ? 2 = 0.999 ;","use
ADAM optimizer
for
parameter optimization
with
initial learning rate
of
0.0005 , ? 1 = 0.9 and ? 2 = 0.999","ADAM optimizer||with||initial learning rate
initial learning rate||of||0.0005 , ? 1 = 0.9 and ? 2 = 0.999
ADAM optimizer||for||parameter optimization
",,,,,"Experimental setup||use||ADAM optimizer
",,,
experiments,The absolute value of gradient on each parameter is clipped within 0.001 .,"absolute value
of
gradient
on
each parameter
is
clipped
within
0.001","absolute value||of||gradient
gradient||is||clipped
clipped||within||0.001
gradient||on||each parameter
",,,,,,,"Experimental setup||has||absolute value
",
experiments,The batch size is 64 for both CNN and Daily Mail datasets .,"batch size
is
64
for
both CNN and Daily Mail datasets","batch size||is||64
64||for||both CNN and Daily Mail datasets
",,,,,,,"Experimental setup||has||batch size
",
experiments,Models are trained on GTX TitanX 12 GB .,"trained on
GTX TitanX 12 GB",,,,,,"Experimental setup||trained on||GTX TitanX 12 GB
",,,
experiments,"Comparing with the AS Reader , ReasoNet shows the signi cant improvement by capturing multi-turn reasoning in the paragraph .","Comparing with
AS Reader
ReasoNet
shows
signi cant improvement
by capturing
reasoning
in
paragraph","ReasoNet||shows||signi cant improvement
signi cant improvement||by capturing||reasoning
reasoning||in||paragraph
","AS Reader||has||ReasoNet
",,,,"Results||Comparing with||AS Reader
",,,
experiments,"Iterative Attention Reader , EpiReader and GA Reader are the three multi-turn reasoning models with xed reasoning steps .","Iterative Attention Reader , EpiReader and GA Reader
are
three multi-turn reasoning models
with
xed reasoning steps","Iterative Attention Reader , EpiReader and GA Reader||are||three multi-turn reasoning models
three multi-turn reasoning models||with||xed reasoning steps
",,,,,,,"Results||has||Iterative Attention Reader , EpiReader and GA Reader
","Iterative Attention Reader , EpiReader and GA Reader||has||outperforms
"
experiments,ReasoNet also outperforms all of them by integrating termination gate in the model which allows di erent reasoning steps for di erent test cases .,"ReasoNet
outperforms
by integrating
termination gate
in
model
allows
di erent reasoning steps
for
di erent test cases","outperforms||by integrating||termination gate
termination gate||allows||di erent reasoning steps
di erent reasoning steps||for||di erent test cases
termination gate||in||model
","outperforms||has||ReasoNet
ReasoNet||has||outperforms
",,,,,,,
experiments,ReasoNet obtains comparable results with AoA Reader on CNN test set .,"ReasoNet
obtains
comparable results
with
AoA Reader
on
CNN test set","ReasoNet||obtains||comparable results
comparable results||with||AoA Reader
comparable results||on||CNN test set
",,,,,,,"Results||has||ReasoNet
Results||has||ReasoNet
",
experiments,SQuAD Dataset,SQuAD Dataset,,,,,,,,"Tasks||has||SQuAD Dataset
","SQuAD Dataset||has||Experimental setup
"
experiments,Vocab Size :,Vocab Size,,,,,,,,,
experiments,"We use the python NLTK tokenizer 6 to preprocess passages and questions , and obtain about 100K words in the vocabulary .","use
python NLTK tokenizer
to preprocess
passages and questions
obtain about
100K words
in
vocabulary","python NLTK tokenizer||to preprocess||passages and questions
passages and questions||obtain about||100K words
100K words||in||vocabulary
",,,,,"Vocab Size||use||python NLTK tokenizer
",,,
experiments,Embedding Layer : We use the 100 - dimensional pretrained Glove vectors as word embeddings .,"Embedding Layer
use
100 - dimensional pretrained Glove vectors
as
word embeddings","Embedding Layer||use||100 - dimensional pretrained Glove vectors
100 - dimensional pretrained Glove vectors||as||word embeddings
",,,,,,,,
experiments,The maximum reasoning step T max is set to 10 in SQuAD experiments .,"maximum reasoning step T max
set to
10","maximum reasoning step T max||set to||10
",,,,,,,"Experimental setup||has||maximum reasoning step T max
Experimental setup||has||maximum reasoning step T max
",
experiments,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0.5 and a batch size Results :,"use
AdaDelta optimizer
for
parameter optimization
with
initial learning rate
of
0.5","initial learning rate||of||0.5
AdaDelta optimizer||for||parameter optimization
AdaDelta optimizer||for||parameter optimization
parameter optimization||with||initial learning rate
initial learning rate||of||0.5
",,,,,"Experimental setup||use||AdaDelta optimizer
Experimental setup||use||AdaDelta optimizer
",,,
experiments,"In , we demonstrate that ReasoNet outperforms all existing published approaches .","demonstrate
ReasoNet
outperforms
all existing published approaches",,"outperforms||has||all existing published approaches
",,,,"Results||demonstrate||ReasoNet
",,,
experiments,"While we compare ReasoNet with BiDAF , ReasoNet exceeds BiDAF both in single model and ensemble model cases .","compare
ReasoNet
BiDAF
exceeds
both in
single model and ensemble model cases","ReasoNet||exceeds||BiDAF
ReasoNet||both in||single model and ensemble model cases
",,,,,"Results||compare||ReasoNet
",,,
experiments,"In the bottom part of , we compare ReasoNet with all unpublished methods at the time of this submission , ReasoNet holds the second position in all the competing approaches in the SQuAD leaderboard .","ReasoNet
holds
second position
in
all the competing approaches
in
SQuAD leaderboard","ReasoNet||holds||second position
second position||in||all the competing approaches
all the competing approaches||in||SQuAD leaderboard
",,,,,,,,
experiments,Graph Reachability,Graph Reachability,,,,,,,,"Tasks||has||Graph Reachability
","Graph Reachability||has||Experimental setup
"
experiments,Embedding Layer,Embedding Layer,,,,,,,,,
experiments,We use a 100 - dimensional embedding vector for each symbol in the query and graph description .,"use
100 - dimensional embedding vector
for
each symbol
in
query and graph description","100 - dimensional embedding vector||for||each symbol
each symbol||in||query and graph description
",,,,,"Embedding Layer||use||100 - dimensional embedding vector
",,,
experiments,"The maximum reasoning step T max is set to 15 and 25 for the small graph and large graph dataset , respectively .","maximum reasoning step T max
set to
15 and 25
for
small graph and large graph dataset","maximum reasoning step T max||set to||15 and 25
15 and 25||for||small graph and large graph dataset
",,,,,,,,
experiments,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0.5 and a batch size of 32 .,"use
AdaDelta optimizer
for
parameter optimization
with
initial learning rate
of
0.5
batch size
of
32","AdaDelta optimizer||with||initial learning rate
AdaDelta optimizer||with||batch size
batch size||of||32
",,,,,,,,
experiments,"Deep LSTM Reader achieves 90.92 % and 71.55 % accuracy in the small and large graph dataset , respectively , which indicates the graph reachibility task is not trivial .","Deep LSTM Reader
achieves
90.92 % and 71.55 % accuracy
in
small and large graph dataset","Deep LSTM Reader||achieves||90.92 % and 71.55 % accuracy
90.92 % and 71.55 % accuracy||in||small and large graph dataset
",,,,,,,"Results||has||Deep LSTM Reader
",
research-problem,Neural Paraphrase Identification of Questions with Noisy Pretraining,Neural Paraphrase Identification of Questions,,,,,"Contribution||has research problem||Neural Paraphrase Identification of Questions
",,,,
research-problem,We present a solution to the problem of paraphrase identification of questions .,paraphrase identification of questions,,,,,"Contribution||has research problem||paraphrase identification of questions
",,,,
research-problem,Question paraphrase identification is a widely useful NLP application .,Question paraphrase identification,,,,,"Contribution||has research problem||Question paraphrase identification
",,,,
approach,"We examine a simple model family , the decomposable attention model of , that has shown promise in modeling natural language inference and has inspired recent work on similar tasks .","examine
simple model family
decomposable attention model
shown promise in modeling
natural language inference","decomposable attention model||shown promise in modeling||natural language inference
","simple model family||has||decomposable attention model
","Approach||examine||simple model family
",,,,,,
approach,"First , to mitigate data sparsity , we modify the input representation of the decomposable attention model to use sums of character n-gram embeddings instead of word embeddings .","to mitigate
data sparsity
modify
input representation
of
decomposable attention model
to use
sums
of
character n-gram embeddings
instead of
word embeddings","data sparsity||modify||input representation
input representation||of||decomposable attention model
input representation||to use||sums
sums||of||character n-gram embeddings
character n-gram embeddings||instead of||word embeddings
",,"Approach||to mitigate||data sparsity
",,,,,,
approach,"Second , to significantly improve our model performance , we pretrain all our model parameters on the noisy , automatically collected question - paraphrase corpus Paralex , followed by fine - tuning the parameters on the Quora dataset .","pretrain
all our model parameters
on
noisy , automatically collected question - paraphrase corpus
Paralex
followed by
fine - tuning
parameters
on
Quora dataset","all our model parameters||on||noisy , automatically collected question - paraphrase corpus
noisy , automatically collected question - paraphrase corpus||followed by||fine - tuning
parameters||on||Quora dataset
","fine - tuning||has||parameters
noisy , automatically collected question - paraphrase corpus||name||Paralex
","Approach||pretrain||all our model parameters
",,,,,,
hyperparameters,"We tuned the following hyperparameters by grid search on the development set ( settings for our best model are in parenthesis ) : embedding dimension ( 300 ) , shape of all feedforward networks ( two layers with 400 and 200 width ) , character n -gram sizes ( 5 ) , context size ( 1 ) , learning rate ( 0.1 for both pretraining and tuning ) , batch size ( 256 for pretraining and 64 for tuning ) , dropout ratio ( 0.1 for tuning ) and prediction threshold ( positive paraphrase for a score ? 0.3 ) .","by
grid search
on
development set
for
embedding dimension
300
shape of all feedforward networks
two layers
with
400 and 200 width
character n -gram sizes
5
context size
1
learning rate
0.1
for
pretraining and tuning
batch size
256
for
pretraining
64
for
tuning
dropout ratio
0.1
for
tuning
prediction threshold
positive paraphrase
score ? 0.3","two layers||with||400 and 200 width
positive paraphrase||for||score ? 0.3
0.1||for||pretraining and tuning
0.1||for||tuning
256||for||pretraining
64||for||tuning
grid search||on||development set
","grid search||has||shape of all feedforward networks
shape of all feedforward networks||has||two layers
grid search||has||prediction threshold
prediction threshold||has||positive paraphrase
grid search||has||character n -gram sizes
character n -gram sizes||has||5
grid search||has||learning rate
learning rate||has||0.1
grid search||has||dropout ratio
dropout ratio||has||0.1
grid search||has||context size
context size||has||1
grid search||has||batch size
batch size||has||256
batch size||has||64
grid search||has||embedding dimension
embedding dimension||has||300
","Hyperparameters||by||grid search
",,,,,,
results,"We observe that the simple FFNN baselines work better than more complex Siamese and Multi - Perspective CNN or LSTM models , more so if character n-gram based embeddings are used .","observe
simple FFNN baselines
work better
than
more complex Siamese and Multi - Perspective CNN or LSTM models","work better||than||more complex Siamese and Multi - Perspective CNN or LSTM models
","simple FFNN baselines||has||work better
","Results||observe||simple FFNN baselines
",,,,,,
results,"Our basic decomposable attention model DECATT word without pre-trained embeddings is better than most of the models , all of which used GloVe embeddings .","Our basic decomposable attention model DECATT word
without
pre-trained embeddings
is
better
than
most of the models
used
GloVe embeddings","Our basic decomposable attention model DECATT word||is||better
better||than||most of the models
most of the models||used||GloVe embeddings
Our basic decomposable attention model DECATT word||without||pre-trained embeddings
",,,"Results||has||Our basic decomposable attention model DECATT word
",,,,,
results,An interesting observation is that DECATT char model without any pretrained embeddings outperforms DE - CATT glove that uses task - agnostic GloVe embeddings .,"DECATT char model
without
any pretrained embeddings
outperforms
DE - CATT glove
that uses
task - agnostic GloVe embeddings","DECATT char model||without||any pretrained embeddings
DECATT char model||outperforms||DE - CATT glove
DE - CATT glove||that uses||task - agnostic GloVe embeddings
",,,"Results||has||DECATT char model
",,,,,
results,"Furthermore , when character n-gram embeddings are pre-trained in a task - specific manner in DECATT paralex ? char model , we observe a significant boost in performance .","when
character n-gram embeddings
are
pre-trained
in
task - specific manner
in
DECATT paralex ? char model
observe
significant boost
in
performance","character n-gram embeddings||are||pre-trained
pre-trained||in||task - specific manner
task - specific manner||in||DECATT paralex ? char model
task - specific manner||observe||significant boost
significant boost||in||performance
",,"Results||when||character n-gram embeddings
",,,,,,
results,"Finally , we note that our best performing model is pt - DECATT char , which leverages the full power of character embeddings and pretraining the model on Paralex .","note
our best performing model
is
pt - DECATT char
leverages
full power
of
character embeddings","our best performing model||is||pt - DECATT char
pt - DECATT char||leverages||full power
full power||of||character embeddings
",,"Results||note||our best performing model
",,,,,,
research-problem,Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond,Massively Multilingual Sentence Embeddings,,,,,"Contribution||has research problem||Massively Multilingual Sentence Embeddings
",,,,
research-problem,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different families and written in 28 different scripts .",joint multilingual sentence representations,,,,,"Contribution||has research problem||joint multilingual sentence representations
",,,,
code,"Our implementation , the pretrained encoder and the multilingual test set are available at https://github.com / facebookresearch/LASER . . 2018 .",https://github.com / facebookresearch/LASER,,,,,"Contribution||Code||https://github.com / facebookresearch/LASER
",,,,
model,"In this work , we are interested in universal language agnostic sentence embeddings , that is , vector representations of sentences that are general with respect to two dimensions : the input language and the NLP task .","interested in
universal language agnostic sentence embeddings
that is
vector representations
of
sentences
that are
general
with respect to
two dimensions
input language
NLP task","universal language agnostic sentence embeddings||that is||vector representations
vector representations||of||sentences
sentences||that are||general
general||with respect to||two dimensions
","two dimensions||name||input language
two dimensions||name||NLP task
","Model||interested in||universal language agnostic sentence embeddings
",,,,,,
model,"To that end , we train a single encoder to handle multiple languages , so that semantically similar sentences in different languages are close in the embedding space .","train
single encoder
to handle
multiple languages","single encoder||to handle||multiple languages
",,"Model||train||single encoder
",,,,,,
experiments,XNLI : cross - lingual NLI,XNLI : cross - lingual NLI,,,,"Experiments||has||XNLI : cross - lingual NLI
",,,,,"XNLI : cross - lingual NLI||has||Our proposed method
"
experiments,9 Our proposed method obtains the best results in zero - shot cross - lingual transfer for all languages but Spanish .,"Our proposed method
obtains
best results
in
zero - shot cross - lingual transfer
for
all languages but Spanish","Our proposed method||obtains||best results
best results||in||zero - shot cross - lingual transfer
zero - shot cross - lingual transfer||for||all languages but Spanish
",,,,,,,,
experiments,"Moreover , our transfer results are strong and homogeneous across all languages :","transfer results
are
strong and homogeneous
across
all languages","transfer results||are||strong and homogeneous
strong and homogeneous||across||all languages
",,,,,,,"XNLI : cross - lingual NLI||has||transfer results
",
experiments,"for 11 of them , the zero - short performance is ( at most ) 5 % lower than the one on English , including distant languages like Arabic , Chinese and Vietnamese , and we also achieve remarkable good results on low - resource languages like Swahili .","zero - short performance
is
( at most ) 5 % lower
than
on
English
including
distant languages
like
Arabic , Chinese and Vietnamese
achieve
remarkable good results
low - resource languages
like
Swahili","remarkable good results||on||low - resource languages
low - resource languages||like||Swahili
zero - short performance||is||( at most ) 5 % lower
( at most ) 5 % lower||than||English
English||including||distant languages
distant languages||like||Arabic , Chinese and Vietnamese
",,,,,"XNLI : cross - lingual NLI||achieve||remarkable good results
",,"XNLI : cross - lingual NLI||has||zero - short performance
",
experiments,"10 Finally , we also outperform all baselines of Conneau et al. by a substantial margin , with the additional advantage that we use a single pre-trained encoder , whereas X - BiLSTM learns a separate encoder for each language .","outperform
all baselines
by
substantial margin","all baselines||by||substantial margin
",,,,,"XNLI : cross - lingual NLI||outperform||all baselines
",,,
experiments,MLDoc : cross - lingual classification,MLDoc : cross - lingual classification,,,,"Experiments||has||MLDoc : cross - lingual classification
",,,,,"MLDoc : cross - lingual classification||has||our system
"
experiments,"As shown in , our system obtains the best published results for 5 of the 7 transfer languages .","our system
obtains
best published results
for
5 of the 7 transfer languages","our system||obtains||best published results
best published results||for||5 of the 7 transfer languages
",,,,,,,,
experiments,BUCC : bitext mining,BUCC : bitext mining,,,,"Experiments||has||BUCC : bitext mining
",,,,,"BUCC : bitext mining||has||our system
"
experiments,"As shown in , our system establishes a new state - of - the - art for all language pairs with the exception of English - Chinese test .","our system
establishes
new state - of - the - art
for
all language pairs
with the exception of
English - Chinese test","our system||establishes||new state - of - the - art
new state - of - the - art||for||all language pairs
new state - of - the - art||with the exception of||English - Chinese test
",,,,,,,,
experiments,"We also outperform Artetxe and Schwenk ( 2018 ) themselves , who use two separate models covering 4 languages each .","outperform
Artetxe and Schwenk ( 2018 )",,,,,,"BUCC : bitext mining||outperform||Artetxe and Schwenk ( 2018 )
",,,
experiments,Tatoeba : similarity search,Tatoeba : similarity search,,,,"Experiments||has||Tatoeba : similarity search
",,,,,"Tatoeba : similarity search||are||55
Tatoeba : similarity search||are||48 languages
Tatoeba : similarity search||are||15 languages
Tatoeba : similarity search||has||similarity error rates
"
experiments,"Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .","with
similarity error rates
below 5 %
are",,"similarity error rates||has||below 5 %
",,,,,,,
experiments,"11 This is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % .","for
37 languages
48 languages
with
error rate
below 10 %
55
less than 20 %","55||with||less than 20 %
48 languages||with||error rate
","error rate||has||below 10 %
",,,,"below 5 %||for||37 languages
",,,
experiments,There are only 15 languages with error rates above 50 % .,"15 languages
with error rates
above 50 %","15 languages||with error rates||above 50 %
",,,,,,,,
ablation-analysis,We were notable to achieve good convergence with deeper models .,"achieve
good convergence
with
deeper models","good convergence||with||deeper models
",,"Ablation analysis||achieve||good convergence
",,,,,,
ablation-analysis,"It can be seen that all tasks benefit from deeper models , in particular XNLI and Tatoeba , suggesting that a single layer BiLSTM has not enough capacity to encode so many languages .","seen that
all tasks
benefit from
deeper models","all tasks||benefit from||deeper models
",,"Ablation analysis||seen that||all tasks
",,,,,,
ablation-analysis,Multitask learning has been shown to be helpful to learn English sentence embeddings .,Multitask learning,,,,"Ablation analysis||has||Multitask learning
",,,,,"Multitask learning||has||NLI objective
"
ablation-analysis,"As shown in , the NLI objective leads to a better performance on the English NLI test set , but this comes at the cost of a worse cross - lingual transfer performance in XNLI and Tatoeba .","in
NLI objective
leads to
better performance
on
English NLI test set
comes at
cost
of
worse cross - lingual transfer performance
XNLI and Tatoeba","NLI objective||leads to||better performance
better performance||comes at||cost
cost||of||worse cross - lingual transfer performance
worse cross - lingual transfer performance||in||XNLI and Tatoeba
better performance||on||English NLI test set
",,,,,,,,
research-problem,Dynamic Meta - Embeddings for Improved Sentence Representations,Improved Sentence Representations,,,,,"Contribution||has research problem||Improved Sentence Representations
",,,,
research-problem,"In this work , we explore the supervised learning of task - specific , dynamic meta-embeddings , and apply the technique to sentence representations .","explore
supervised learning
of
task - specific , dynamic meta-embeddings
apply
sentence representations","supervised learning||of||task - specific , dynamic meta-embeddings
task - specific , dynamic meta-embeddings||apply||sentence representations
",,"Approach||explore||supervised learning
",,"Contribution||has research problem||sentence representations
",,,,
approach,"First , it is embedding - agnostic , meaning that one of the main ( and perhaps most important ) hyperparameters in NLP pipelines is made obsolete .","is
embedding - agnostic",,,"Approach||is||embedding - agnostic
",,,,,,
research-problem,"is paper proposes an a ention boosted natural language inference model named a ESIM by adding word a ention and adaptive direction - oriented a ention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM . is makes the inference model a ESIM has the ability toe ectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .",natural language inference,,,,,"Contribution||has research problem||natural language inference
",,,,
research-problem,Natural language inference ( NLI ) is an important and signicant task in natural language processing ( NLP ) .,Natural language inference ( NLI ),,,,,"Contribution||has research problem||Natural language inference ( NLI )
",,,,
research-problem,"In the literature , the task of NLI is usually viewed as a relation classi cation .",NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,"erefore , in this study , using ESIM model as the baseline , we add an a ention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .","using
ESIM model
as
baseline
add
a ention layer
behind
each Bi - LSTM layer
use
adaptive orientation embedding layer
to jointly represent
forward and backward vectors","ESIM model||add||a ention layer
a ention layer||behind||each Bi - LSTM layer
ESIM model||as||baseline
ESIM model||use||adaptive orientation embedding layer
adaptive orientation embedding layer||to jointly represent||forward and backward vectors
",,"Model||using||ESIM model
",,,,,,
model,"We name this a ention boosted Bi - LSTM as Bi - a LSTM , and denote the modi ed ESIM as aESIM .","a ention boosted Bi - LSTM
as
Bi - a LSTM
denote
modi ed ESIM
as
aESIM","modi ed ESIM||as||aESIM
a ention boosted Bi - LSTM||as||Bi - a LSTM
",,"Model||denote||modi ed ESIM
","Model||name||a ention boosted Bi - LSTM
",,,,,
hyperparameters,We use the Adam method for optimization .,"use
Adam method
for
optimization","Adam method||for||optimization
",,"Hyperparameters||use||Adam method
",,,,,,
hyperparameters,"e initial learning rate is set to 0.0005 , and the batch size is 128 .","initial learning rate
is
set to
0.0005
batch size
128","initial learning rate||set to||0.0005
batch size||is||128
",,,"Hyperparameters||has||initial learning rate
Hyperparameters||has||batch size
",,,,,
hyperparameters,e dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .,"dimensions
of
hidden states
of
Bi - aLSTM and word embedding
are
300","dimensions||of||hidden states
hidden states||of||Bi - aLSTM and word embedding
Bi - aLSTM and word embedding||are||300
",,,"Hyperparameters||has||dimensions
",,,,,
hyperparameters,We employ non-linearity function f = selu replacing recti ed linear unit ReLU on account of its faster convergence rate .,"employ
non-linearity function f = selu
replacing
linear unit ReLU
on account of
faster convergence rate","non-linearity function f = selu||on account of||faster convergence rate
non-linearity function f = selu||replacing||linear unit ReLU
",,"Hyperparameters||employ||non-linearity function f = selu
",,,,,,
hyperparameters,Dropout rate is set to 0.2 during training .,"Dropout rate
set to
0.2
during
training","Dropout rate||set to||0.2
0.2||during||training
",,,"Hyperparameters||has||Dropout rate
",,,,,
hyperparameters,We use pre-trained 300 - D Glove 840B vectors to initialize word embeddings .,"pre-trained 300 - D Glove 840B vectors
to initialize
word embeddings","pre-trained 300 - D Glove 840B vectors||to initialize||word embeddings
",,,"Hyperparameters||use||pre-trained 300 - D Glove 840B vectors
",,,,,
hyperparameters,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"Out - of - vocabulary ( OOV ) words
are
initialized randomly
with
Gaussian samples","Out - of - vocabulary ( OOV ) words||are||initialized randomly
initialized randomly||with||Gaussian samples
",,,"Hyperparameters||has||Out - of - vocabulary ( OOV ) words
",,,,,
results,"According to the results in , a ESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .","ESIM model
achieved
88.1 %
on
SNLI corpus
elevating 0.8 percent
higher than
ESIM model","ESIM model||achieved||88.1 %
elevating 0.8 percent||higher than||ESIM model
88.1 %||on||SNLI corpus
","88.1 %||has||elevating 0.8 percent
",,"Results||has||ESIM model
",,,,,
results,It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .,"promoted
almost 0.5 percent accuracy
outperformed
baselines
on
MultiNLI","baselines||on||MultiNLI
","outperformed||has||baselines
",,,,"ESIM model||promoted||almost 0.5 percent accuracy
",,"ESIM model||has||outperformed
",
research-problem,Explicit Utilization of General Knowledge in Machine Reading Comprehension,Machine Reading Comprehension,,,,,"Contribution||has research problem||Machine Reading Comprehension
",,,,
research-problem,"To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings .","Machine Reading Comprehension ( MRC )
MRC",,,,,"Contribution||has research problem||Machine Reading Comprehension ( MRC )
Contribution||has research problem||MRC
",,,,
approach,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .","propose
data enrichment method
which uses
WordNet
to extract
inter-word semantic connections
as
general knowledge
from
each given passage - question pair","data enrichment method||to extract||inter-word semantic connections
inter-word semantic connections||as||general knowledge
general knowledge||from||each given passage - question pair
data enrichment method||which uses||WordNet
",,"Approach||propose||data enrichment method
",,,,,,
approach,"On the other hand , we propose an end - to - end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .","end - to - end MRC model
named
Knowledge Aided Reader ( KAR )
explicitly uses
extracted general knowledge
to assist
attention mechanisms","end - to - end MRC model||named||Knowledge Aided Reader ( KAR )
Knowledge Aided Reader ( KAR )||explicitly uses||extracted general knowledge
extracted general knowledge||to assist||attention mechanisms
",,,"Approach||propose||end - to - end MRC model
",,,,,
approach,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .",,,,,,,,,,
approach,"On the other hand , we propose an end - to - end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .",,,,,,,,,,
experimental-setup,"We tokenize the MRC dataset with spa Cy 2.0.13 , manipulate WordNet 3.0 with NLTK 3.3 , and implement KAR with TensorFlow 1.11.0 .","tokenize
MRC dataset
with
spa Cy 2.0.13
manipulate
WordNet 3.0
with
NLTK 3.3
implement
KAR
with
TensorFlow 1.11.0","KAR||with||TensorFlow 1.11.0
MRC dataset||with||spa Cy 2.0.13
WordNet 3.0||with||NLTK 3.3
",,"Experimental setup||implement||KAR
Experimental setup||tokenize||MRC dataset
Experimental setup||manipulate||WordNet 3.0
",,,,,,
experimental-setup,"For the dense layers and the BiLSTMs , we set the dimensionality unit d to 600 .","For
dense layers and the BiLSTMs
set
dimensionality unit d
to
600","dense layers and the BiLSTMs||set||dimensionality unit d
dimensionality unit d||to||600
",,"Experimental setup||For||dense layers and the BiLSTMs
",,,,,,
experimental-setup,"For model optimization , we apply the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.0005 and a minibatch size of 32 .","model optimization
apply
Adam ( Kingma and Ba , 2014 ) optimizer
with
learning rate
of
0.0005
minibatch size
of
32","model optimization||apply||Adam ( Kingma and Ba , 2014 ) optimizer
Adam ( Kingma and Ba , 2014 ) optimizer||with||minibatch size
minibatch size||of||32
Adam ( Kingma and Ba , 2014 ) optimizer||with||learning rate
learning rate||of||0.0005
",,,"Experimental setup||For||model optimization
",,,,,
experimental-setup,"To avoid overfitting , we apply dropout to the dense layers and the BiLSTMs with a dropout rate of 0.3 .","To avoid
overfitting
apply
dropout
to
dense layers
BiLSTMs
with
dropout rate
of
0.3","overfitting||apply||dropout
dropout||with||dropout rate
dropout rate||of||0.3
dropout||to||dense layers
dropout||to||BiLSTMs
",,"Experimental setup||To avoid||overfitting
",,,,,,
experimental-setup,"To boost the performance , we apply exponential moving average with a decay rate of 0.999 .","To boost
performance
apply
exponential moving average
with
decay rate
of
0.999","performance||apply||exponential moving average
exponential moving average||with||decay rate
decay rate||of||0.999
",,"Experimental setup||To boost||performance
",,,,,,
ablation-analysis,"Then we conduct an ablation study by replacing the knowledge aided attention mechanisms with the mutual attention proposed by and the self attention proposed by separately , and find that the F 1 score of KAR drops by 4.2 on the development set , 7.8 on AddSent , and 9.1 on AddOneSent .","replacing
knowledge aided attention mechanisms
with
mutual attention
self attention
find that
F 1 score
of
KAR
drops by
4.2
on
development set
7.8
on
AddSent
9.1
on
AddOneSent","knowledge aided attention mechanisms||with||mutual attention
knowledge aided attention mechanisms||with||self attention
knowledge aided attention mechanisms||find that||F 1 score
F 1 score||of||KAR
F 1 score||drops by||7.8
7.8||on||AddSent
F 1 score||drops by||4.2
4.2||on||development set
F 1 score||drops by||9.1
9.1||on||AddOneSent
",,"Ablation analysis||replacing||knowledge aided attention mechanisms
",,,,,,
results,"Finally we find that after only one epoch of training , KAR already achieves an EM of 71.9 and an F 1 score of 80.8 on the development set , which is even better than the final performance of several strong baselines , such as DCN ( EM / F1 : 65.4 / 75.6 ) and BiDAF ( EM / F1 : 67.7 / 77.3 ) .","of
KAR
achieves
EM
of
71.9
F 1 score
of
80.8
on
development set
even better
than
final performance
several strong baselines
such as
DCN ( EM / F1 : 65.4 / 75.6 )
BiDAF ( EM / F1 : 67.7 / 77.3 )","KAR||on||development set
development set||achieves||F 1 score
F 1 score||of||80.8
development set||achieves||EM
EM||of||71.9
development set||achieves||even better
even better||than||final performance
final performance||of||several strong baselines
several strong baselines||such as||DCN ( EM / F1 : 65.4 / 75.6 )
several strong baselines||such as||BiDAF ( EM / F1 : 67.7 / 77.3 )
",,,"Results||has||KAR
",,,,,
research-problem,A Multi - Stage Memory Augmented Neural Network for Machine Reading Comprehension,Machine Reading Comprehension,,,,,"Contribution||has research problem||Machine Reading Comprehension
",,,,
research-problem,Reading Comprehension ( RC ) of text is one of the fundamental tasks in natural language processing .,Reading Comprehension ( RC ),,,,,"Contribution||has research problem||Reading Comprehension ( RC )
",,,,
research-problem,"In recent years , several end - to - end neural network models have been proposed to solve RC tasks .",RC,,,,,"Contribution||has research problem||RC
",,,,
research-problem,"One possible way of measuring RC is by formulating it as answer span prediction style Question Answering ( QA ) task , which is finding an answer to the question based on the given document ( s ) .",answer span prediction style Question Answering ( QA ),,,,,"Contribution||has research problem||answer span prediction style Question Answering ( QA )
",,,,
research-problem,"Recently , influential deep learning approaches have been proposed to solve this QA task . ; propose the attention mechanism between question and context for question - aware contextual representation .",QA,,,,,"Contribution||has research problem||QA
",,,,
model,"In this work , we build a QA model that can understand long documents by utilizing Memory Augmented Neural Networks ( MANNs ) .","build
QA model
understand
long documents
by utilizing
Memory Augmented Neural Networks ( MANNs )","QA model||understand||long documents
long documents||by utilizing||Memory Augmented Neural Networks ( MANNs )
",,"Model||build||QA model
",,,,,,
model,This type of neural networks decouples the memory capacity from the number of model parameters .,"decouples
memory capacity
from
number of model parameters","memory capacity||from||number of model parameters
",,"Model||decouples||memory capacity
",,,,,,
experimental-setup,We develop MAMCN using Tensorflow 1 deep learning framework and Sonnet 2 library .,"develop
MAMCN
using
Tensorflow 1 deep learning framework
Sonnet 2 library","MAMCN||using||Tensorflow 1 deep learning framework
MAMCN||using||Sonnet 2 library
",,"Experimental setup||develop||MAMCN
",,,,,,
experimental-setup,"For the word - level embedding , we tokenize the documents using NLTK toolkit and substitute words with GloVe 6B 43.16 46.90 49.28 55.83 BiDAF 40.32 45.91 44.86 50.71 hidden size is set to 200 for QUASAR - T and Triv - iaQA , and 100 for SQuAD .","For
word - level embedding
tokenize
documents
using
NLTK toolkit
substitute
words
with
GloVe 6B","word - level embedding||using||NLTK toolkit
word - level embedding||tokenize||documents
word - level embedding||substitute||words
words||with||GloVe 6B
",,"Experimental setup||For||word - level embedding
",,,,,,
experimental-setup,"In the memory controller , we use 100 x 36 size memory initialized with zeros , 4 read heads and 1 write head .","In
memory controller
use
100 x 36 size memory
initialized with
zeros
4
read heads
1
write head","memory controller||use||1
memory controller||use||4
memory controller||use||100 x 36 size memory
100 x 36 size memory||initialized with||zeros
","1||has||write head
4||has||read heads
","Experimental setup||In||memory controller
",,,,,,
experimental-setup,"The optimizer is AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 0.5 .","optimizer
is
AdaDelta ( Zeiler , 2012 )
with
initial learning rate
of
0.5","optimizer||is||AdaDelta ( Zeiler , 2012 )
AdaDelta ( Zeiler , 2012 )||with||initial learning rate
initial learning rate||of||0.5
",,,"Experimental setup||has||optimizer
",,,,,
experimental-setup,"We train our model for 12 epochs , and batch size is set to 30 .","train
model
for
12 epochs
batch size
set to
30","batch size||set to||30
model||for||12 epochs
",,"Experimental setup||train||model
","Experimental setup||has||batch size
",,,,,
experimental-setup,"During the training , we keep the exponential moving average of weights with 0.001 decay and use these averages at test time .","During
training
keep
exponential moving average
of
weights
with
0.001 decay
use
averages
at
test time","training||keep||exponential moving average
exponential moving average||with||0.001 decay
exponential moving average||use||averages
averages||at||test time
exponential moving average||of||weights
",,"Experimental setup||During||training
",,,,,,
results,QUASAR - T:,,,,,,,,,,
results,"As described in , the baseline ( BiDAF + DNC ) results in a reasonable gain , however , our proposed memory controller gives more performance improvement .","our proposed memory controller
gives
more performance improvement","our proposed memory controller||gives||more performance improvement
",,,,,,,"QUASAR - T||has||our proposed memory controller
",
results,We achieve 68.13 EM and 70.32 F1 for short documents and 63.44 and 65.19 for long documents which are the current best results .,"achieve
68.13 EM and 70.32 F1
for
short documents
63.44 and 65.19
for
long documents","68.13 EM and 70.32 F1||for||short documents
63.44 and 65.19||for||long documents
",,,,,"more performance improvement||achieve||68.13 EM and 70.32 F1
more performance improvement||achieve||63.44 and 65.19
",,,
results,TriviaQA : We compare proposed model with all the previously suggested approaches as shown in .,TriviaQA,,,,"Results||has||TriviaQA
",,,,,"TriviaQA||has||Our model
"
results,Our model achieves the state of the art performance over the existing approaches as shown in 77.58 84.16 O - QANet 76.24 84.60 O O SAN 76.83 84.40 O O Fusion Net 75.97 83.90 O O RaSoR + TR 75.79 83.26 O - Conducter- net 74.41 82.74 O O Reinforced Mnemonic Reader 73.20 81.80 O O BiDAF + Self-attention 72.14 81.05 - O MEMEN 70.98 80.36 O - MAMCN 70.99 79.94 -r- net 71.30 79.70 - O Document Reader 70.73 79.35 O - FastQAExt 70 .,"Our model
achieves
state of the art performance","Our model||achieves||state of the art performance
",,,,,,,,
ablation-analysis,"First , we add ELMo which is the weighted sum of hidden layers of language model with regularization as an additional feature to our word embeddings .","add
ELMo
is
to",,,"Ablation analysis||add||ELMo
",,,,,,
ablation-analysis,This helped our model ( MAMCN + ELMo ) to improve F1 to 85.13 and EM to 77.44 and is the best among the models only with the additional feature augmentation .,"helped
our model ( MAMCN + ELMo )
to improve
F1
to
85.13
EM
77.44
best
among
models
only with
additional feature augmentation","our model ( MAMCN + ELMo )||to improve||EM
EM||to||77.44
our model ( MAMCN + ELMo )||to improve||F1
F1||to||85.13
best||among||models
models||only with||additional feature augmentation
","our model ( MAMCN + ELMo )||is||best
",,,,"ELMo||helped||our model ( MAMCN + ELMo )
",,,
ablation-analysis,We replace all the BiGRU units with this embedding block except the controller layer in our model ( MAMCN + ELMo + DC ) .,"replace
BiGRU units
with
embedding block
except
controller layer
in
our model ( MAMCN + ELMo + DC )","BiGRU units||with||embedding block
embedding block||in||our model ( MAMCN + ELMo + DC )
embedding block||except||controller layer
",,"Ablation analysis||replace||BiGRU units
",,,,,,
ablation-analysis,"We achieve the state of the art performance , 86.73 F1 and 79.69 EM , with the help of this em-bedding block .","achieve
state of the art performance
86.73 F1 and 79.69 EM",,"state of the art performance||has||86.73 F1 and 79.69 EM
",,,,"our model ( MAMCN + ELMo + DC )||achieve||state of the art performance
",,,
research-problem,TANDA : Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection,Answer Sentence Selection,,,,,"Contribution||has research problem||Answer Sentence Selection
",,,,
research-problem,"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering .",Question Answering,,,,,"Contribution||has research problem||Question Answering
",,,,
research-problem,"This has renewed the research interest in Question Answering ( QA ) and , in particular , in two main tasks :",Question Answering ( QA ),,,,,"Contribution||has research problem||Question Answering ( QA )
",,,,
research-problem,"( i ) answer sentence selection ( AS2 ) , which , given a question and a set of answer sentence candidates , consists in selecting sentences ( e.g. , retrieved by a search engine ) correctly answering the question ; and ( ii ) machine reading ( MR ) or reading comprehension , which , given a question and a reference text , consists in finding a text span answering it .",answer sentence selection ( AS2 ),,,,,"Contribution||has research problem||answer sentence selection ( AS2 )
",,,,
research-problem,"Even though the latter is gaining more and more popularity , AS2 is more relevant to a production scenario since , a combination of a search engine and an AS2 model already implements an initial QA system .","AS2
QA",,,,,"Contribution||has research problem||AS2
Contribution||has research problem||QA
",,,,
model,"In this paper , we study the use of Transformer - based models for AS2 and provide effective solutions to tackle the data scarceness problem for AS2 and the instability of the finetuning step .","study
of
Transformer - based models
for
AS2
provide
effective solutions
to tackle
data scarceness problem
for
AS2
instability
finetuning step","Transformer - based models||provide||effective solutions
effective solutions||to tackle||instability
instability||of||finetuning step
effective solutions||to tackle||data scarceness problem
data scarceness problem||for||AS2
Transformer - based models||for||AS2
",,"Model||study||Transformer - based models
",,,,,,
model,"We improve stability of Transformer models by adding an intermediate fine - tuning step , which aims at specializing them to the target task ( AS2 ) , i.e. , this step transfers a pretrained language model to a model for the target task .","improve
stability
of
Transformer models
by adding
intermediate fine - tuning step
aims at
specializing
to
target task ( AS2 )","stability||by adding||intermediate fine - tuning step
intermediate fine - tuning step||aims at||specializing
specializing||to||target task ( AS2 )
stability||of||Transformer models
","improve||has||stability
",,"Model||has||improve
",,,,,
model,"We show that the transferred model can be effectively adapted to the target domain with a subsequent finetuning step , even when using target data of small size .","show
transferred model
can be
effectively adapted
to
target domain
with
subsequent finetuning step
when using
target data
of
small size","transferred model||can be||effectively adapted
effectively adapted||with||subsequent finetuning step
subsequent finetuning step||when using||target data
target data||of||small size
effectively adapted||to||target domain
",,"Model||show||transferred model
",,,,,,
dataset,"We built ASNQ , a dataset for AS2 , by transforming the recently released Natural Questions ( NQ ) corpus ) from MR to AS2 task .","built
ASNQ
for
AS2
by transforming
recently released Natural Questions ( NQ ) corpus
from
MR
to
AS2 task","ASNQ||by transforming||recently released Natural Questions ( NQ ) corpus
recently released Natural Questions ( NQ ) corpus||from||MR
MR||to||AS2 task
ASNQ||for||AS2
",,"Dataset||built||ASNQ
",,,,,,
hyperparameters,We adopt Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 2e - 5 for the transfer step on the ASNQ dataset and a learning rate of 1e - 6 for the adapt step on the target dataset .,"adopt
Adam optimizer
with
learning rate
of
2e - 5
for
transfer step
on
ASNQ dataset
1e - 6
for
adapt step
on
target dataset","Adam optimizer||with||learning rate
learning rate||of||2e - 5
2e - 5||for||transfer step
transfer step||on||ASNQ dataset
learning rate||of||1e - 6
1e - 6||for||adapt step
adapt step||on||target dataset
",,"Hyperparameters||adopt||Adam optimizer
",,,,,,
hyperparameters,We apply early stopping on the dev. set of the target corpus for both steps based on the highest MAP score .,"apply
early stopping
on
dev. set
of
target corpus","early stopping||on||dev. set
dev. set||of||target corpus
",,"Hyperparameters||apply||early stopping
",,,,,,
hyperparameters,"We set the max number of epochs equal to 3 and 9 for adapt and transfer steps , respectively .","set
max number of epochs
equal to
3 and 9
for
adapt and transfer steps","max number of epochs||equal to||3 and 9
3 and 9||for||adapt and transfer steps
",,"Hyperparameters||set||max number of epochs
",,,,,,
hyperparameters,We set the maximum sequence length for BERT / RoBERTa to 128 tokens .,"maximum sequence length
for
BERT / RoBERTa
to
128 tokens","maximum sequence length||for||BERT / RoBERTa
BERT / RoBERTa||to||128 tokens
",,,"Hyperparameters||set||maximum sequence length
",,,,,
results,"TANDA provides a large improvement over the state of the art , which has been regularly contributed to by hundreds of researchers .","TANDA
provides
large improvement
over
state of the art","TANDA||provides||large improvement
large improvement||over||state of the art
",,,"Results||has||TANDA
",,,,,
results,RoBERTa- Large TANDA using ASNQ ?,"RoBERTa- Large TANDA
using
ASNQ","RoBERTa- Large TANDA||using||ASNQ
",,,"Results||has||RoBERTa- Large TANDA
",,,,,
results,"Wiki QA establish an impressive new state of the art for AS2 on WikiQA of 0.920 and 0.933 in MAP and MRR , respectively .","establish
impressive new state of the art
for
AS2
on
WikiQA
of
0.920 and 0.933
in
MAP and MRR","impressive new state of the art||of||0.920 and 0.933
0.920 and 0.933||in||MAP and MRR
impressive new state of the art||for||AS2
AS2||on||WikiQA
",,,,,"RoBERTa- Large TANDA||establish||impressive new state of the art
",,,
results,RoBERTa - Large TANDA with ASNQ ?,"RoBERTa - Large TANDA
with
ASNQ","RoBERTa - Large TANDA||with||ASNQ
",,,"Results||has||RoBERTa - Large TANDA
",,,,,
results,"TREC - QA again establishes an impressive performance of 0.943 in MAP and 0.974 in MRR , outperforming the previous state of the art by .","establishes
impressive performance
of
0.943
in
MAP
0.974
in
MRR
outperforming
previous state of the art","impressive performance||of||0.943
0.943||in||MAP
impressive performance||of||0.974
0.974||in||MRR
","impressive performance||has||outperforming
",,,,"RoBERTa - Large TANDA||establishes||impressive performance
",,,
results,"TANDA improves all the models : BERT - Base , RoBERTa- Base , BERT - Large and RoBERTa - Large , outperforming the previous state of the art with all of them .","improves
all the models
BERT - Base
RoBERTa- Base
BERT - Large
RoBERTa - Large
outperforming
previous state of the art",,"all the models||name||BERT - Base
all the models||name||RoBERTa- Base
all the models||name||BERT - Large
all the models||name||RoBERTa - Large
all the models||has||outperforming
outperforming||has||previous state of the art
outperforming||has||previous state of the art
",,,,"TANDA||improves||all the models
",,,
research-problem,"Neural networks ( NN ) with attention mechanisms have recently proven to be successful at different computer vision ( CV ) and natural language processing ( NLP ) tasks such as image captioning , machine translation and factoid question answering .",Neural networks ( NN ) with attention mechanisms,,,,,"Contribution||has research problem||Neural networks ( NN ) with attention mechanisms
",,,,
research-problem,"However , most recent work on neural attention models have focused on one - way attention mechanisms based on recurrent neural networks designed . for generation tasks .",attention mechanisms,,,,,"Contribution||has research problem||attention mechanisms
",,,,
model,"The key contribution of this work is that we propose Attentive Pooling ( AP ) , a two - way attention mechanism , that significantly improves such discriminative models ' performance on pair - wise ranking or classification , by enabling a joint learning of the representations of both inputs as well as their similarity measurement .","of
that
propose
Attentive Pooling ( AP )
two - way attention mechanism
significantly improves
discriminative models ' performance
on
pair - wise ranking or classification
by enabling
joint learning
of
representations
inputs
as well as
their similarity measurement","two - way attention mechanism||that||significantly improves
discriminative models ' performance||by enabling||joint learning
joint learning||of||representations
representations||of||inputs
inputs||as well as||their similarity measurement
discriminative models ' performance||on||pair - wise ranking or classification
","Attentive Pooling ( AP )||has||two - way attention mechanism
significantly improves||has||discriminative models ' performance
","Model||propose||Attentive Pooling ( AP )
",,,,,,
model,"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .","AP
enables
pooling layer
to be aware of
current input pair
in a way that
information
from
two input items
directly influence
computation
of
each other 's representations","AP||enables||pooling layer
pooling layer||to be aware of||current input pair
current input pair||in a way that||information
information||from||two input items
information||directly influence||computation
computation||of||each other 's representations
",,,"Model||has||AP
",,,,,
model,"The main idea in AP consists of learning a similarity measure over projected segments ( e.g. trigrams ) of the two items in the input pair , and using the similarity scores between the segments to compute attention vectors in both directions .","in
consists of
learning
similarity measure
over
projected segments ( e.g. trigrams )
of
two items
in
input pair
using
similarity scores
between
segments
to compute
attention vectors
both directions","similarity measure||over||projected segments ( e.g. trigrams )
projected segments ( e.g. trigrams )||using||similarity scores
similarity scores||to compute||attention vectors
attention vectors||in||both directions
similarity scores||between||segments
projected segments ( e.g. trigrams )||of||two items
two items||in||input pair
","learning||has||similarity measure
",,,,"AP||consists of||learning
",,,
model,"Next , the attention vectors are used to perform pooling .","attention vectors
to perform
pooling","attention vectors||to perform||pooling
",,,"Model||has||attention vectors
",,,,,
experimental-setup,"We use a context window of size 3 for Insurance QA , while we set this parameter to 4 for TREC - QA and Wiki QA .","use
context window
of
size
3
for
Insurance QA
4
for
TREC - QA
Wiki QA","context window||of||size
3||for||Insurance QA
4||for||TREC - QA
4||for||Wiki QA
","size||has||3
size||has||4
","Experimental setup||use||context window
",,,,,,
experimental-setup,"Using the selected hyperparameters , the best results are normally achieved using between 15 and 25 training epochs .","best results
achieved using
between 15 and 25 training epochs","best results||achieved using||between 15 and 25 training epochs
",,,"Experimental setup||has||best results
",,,,,
experimental-setup,"For AP - CNN , AP - biLSTM and QA - LSTM , we also use a learning rate schedule that decreases the learning rate ?","For
AP - CNN
AP - biLSTM
QA - LSTM
learning rate schedule
that
decreases
learning rate","learning rate schedule||that||decreases
learning rate||For||AP - CNN
learning rate||For||AP - biLSTM
learning rate||For||QA - LSTM
","decreases||has||learning rate
",,"Experimental setup||use||learning rate schedule
",,,,,
experimental-setup,"In our experiments , the four NN architectures QA - CNN , AP - CNN , QA - biLSTM and AP - biLSTM are implemented using Theano .","four NN architectures
QA - CNN
AP - CNN
QA - biLSTM
AP - biLSTM
implemented using
Theano","four NN architectures||implemented using||Theano
","four NN architectures||name||QA - CNN
four NN architectures||name||AP - CNN
four NN architectures||name||QA - biLSTM
four NN architectures||name||AP - biLSTM
",,"Experimental setup||has||four NN architectures
",,,,,
results,"In , we present the experimental results of the four NNs for the Insurance QA dataset .","for
Insurance QA dataset",,,"Results||for||Insurance QA dataset
",,,,,,
results,"On the bottom part of this table , we can see that AP - CNN outperforms QA - CNN by a large margin in both test sets , as well as in the dev set .","see that
AP - CNN
outperforms
QA - CNN
by
large margin
in
both test sets
dev set","outperforms||by||large margin
large margin||in||both test sets
large margin||in||dev set
","AP - CNN||has||outperforms
outperforms||has||QA - CNN
AP - CNN||has||outperforms
outperforms||has||QA - CNN
AP - CNN||has||outperforms
outperforms||has||QA - CNN
",,,,"Insurance QA dataset||see that||AP - CNN
",,,
results,AP - CNN and AP - biLSTM have similar performance .,"AP - CNN and AP - biLSTM
have
similar performance","AP - CNN and AP - biLSTM||have||similar performance
",,,,,,,"Insurance QA dataset||has||AP - CNN and AP - biLSTM
",
results,Both AP - CNN and AP - biLSTM outperform the state - of - the - art systems .,"outperform
state - of - the - art systems",,"outperform||has||state - of - the - art systems
",,,,,,"AP - CNN and AP - biLSTM||has||outperform
",
results,"In , we present the experimental results of the four NNs for the TREC - QA dataset .",TREC - QA dataset,,,,"Results||for||TREC - QA dataset
",,,,,"TREC - QA dataset||has||AP - CNN
"
results,We use the official trec eval that AP - CNN outperforms QA - CNN by a large margin in both metrics .,"AP - CNN
outperforms
QA - CNN
by
large margin
in
both metrics","QA - CNN||by||large margin
large margin||in||both metrics
",,,,,,,,
results,"AP - biLSTM outperforms the QA - biLSTM , but its performance is not as good as the of AP - CNN .","AP - biLSTM
outperforms
QA - biLSTM
performance
not as good
as
AP - CNN","not as good||as||AP - CNN
","AP - biLSTM||has||outperforms
outperforms||has||QA - biLSTM
QA - biLSTM||has||performance
performance||has||not as good
AP - biLSTM||has||outperforms
outperforms||has||QA - biLSTM
",,,,,,"TREC - QA dataset||has||AP - biLSTM
",
results,"AP - CNN outperforms the state - of - the - art systems in both metrics , MAP and MRR .","state - of - the - art systems
in
both metrics
MAP and MRR","state - of - the - art systems||in||both metrics
","both metrics||name||MAP and MRR
",,,,,,"outperforms||has||state - of - the - art systems
",
results,shows the experimental results of the four NNs for the WikiQA dataset .,WikiQA dataset,,,,"Results||for||WikiQA dataset
",,,,,"WikiQA dataset||has||AP - CNN
WikiQA dataset||has||AP - biLSTM
"
results,"Like in the other two datasets , AP - CNN outperforms QA - CNN , and AP - biLSTM outperforms the QA - biLSTM .","AP - CNN
outperforms
QA - CNN
AP - biLSTM
outperforms
QA - biLSTM",,,,,,,,,
results,The difference of performance between AP - CNN and QA - CNN is smaller than the one for the Insurance QA dataset .,"difference of performance
between
AP - CNN and QA - CNN
is
smaller
than
Insurance QA dataset","difference of performance||between||AP - CNN and QA - CNN
AP - CNN and QA - CNN||is||smaller
smaller||than||Insurance QA dataset
",,,,,,,"WikiQA dataset||has||difference of performance
",
research-problem,Learning Natural Language Inference with LSTM,Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,Natural language inference ( NLI ) is a fundamentally important task in natural language processing that has many applications .,Natural language inference ( NLI ),,,,,"Contribution||has research problem||Natural language inference ( NLI )
",,,,
research-problem,"In this paper , we propose a special long short - term memory ( LSTM ) architecture for NLI .",NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,"In this paper , we propose a new LSTM - based architecture for learning natural language inference .","propose
new LSTM - based architecture
for learning
natural language inference","new LSTM - based architecture||for learning||natural language inference
",,"Model||propose||new LSTM - based architecture
",,,,,,
model,"Instead , we use an LSTM to perform word - by - word matching of the hypothesis with the premise .","use
LSTM
to perform
word - by - word matching
of
hypothesis
with
premise","LSTM||to perform||word - by - word matching
word - by - word matching||of||hypothesis
hypothesis||with||premise
",,"Model||use||LSTM
",,,,,,
model,"Our LSTM sequentially processes the hypothesis , and at each position , it tries to match the current word in the hypothesis with an attention - weighted representation of the premise .","Our LSTM
sequentially processes
hypothesis
at each position
tries to match
current word
in
hypothesis
with
attention - weighted representation
of
premise","Our LSTM||at each position||tries to match
current word||with||attention - weighted representation
attention - weighted representation||of||premise
current word||in||hypothesis
Our LSTM||sequentially processes||hypothesis
","tries to match||has||current word
",,"Model||has||Our LSTM
",,,,,
model,"We refer to this architecture a match - LSTM , or m LSTM for short .",,,,,,,,,,
code,1 https://github.com/shuohangwang/,https://github.com/shuohangwang/,,,,,"Contribution||Code||https://github.com/shuohangwang/
",,,,
hyperparameters,"We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ?","use
Adam method
with
hyperparameters",,,"Hyperparameters||use||Adam method
",,,"optimization||with||hyperparameters
",,,"hyperparameters||set to||0.999
"
hyperparameters,1 set to 0.9 and ?,"set to
0.9",,,,,,"hyperparameters||set to||0.9
",,,
hyperparameters,2 set to 0.999 for optimization .,"0.999
for
optimization",,,,,,"Adam method||for||optimization
",,,
hyperparameters,The initial learning rate is set to be 0.001 with a decay ratio of 0.95 for each iteration .,"initial learning rate
set to
0.001
decay ratio
of
0.95
for
each iteration","initial learning rate||set to||0.001
decay ratio||of||0.95
0.95||for||each iteration
",,,"Hyperparameters||has||initial learning rate
Hyperparameters||has||decay ratio
",,,,,
hyperparameters,The batch size is set to be 30 .,"batch size
set to
30","batch size||set to||30
",,,"Hyperparameters||has||batch size
",,,,,
hyperparameters,We experiment with d = 150 and d = 300 where d is the dimension of all the hidden states .,"experiment with
d = 150 and d = 300",,,"Hyperparameters||experiment with||d = 150 and d = 300
",,,,,,
results,"We have the following observations : ( 1 ) First of all , we can see that when we set d to 300 , our model achieves an accuracy of 86.1 % on the test data , which to the best of our knowledge is the highest on and |?| M is the number of parameters excluding the word embeddings .","of
see that
set d
to
300
our model
achieves
accuracy
86.1 %
on
test data","set d||to||300
our model||achieves||accuracy
accuracy||of||86.1 %
86.1 %||on||test data
","300||has||our model
","Results||see that||set d
",,,,,,
results,"( 2 ) If we compare our m LSTM model with our implementation of the word - by - word attention model by under the same setting with d = 150 , we can see that our performance on the test data ( 85.7 % ) is higher than that of their model ( 82.6 % ) .","compare
our m LSTM model
with
word - by - word attention model
under
same setting
with
d
=
150
see that
our performance
on
test data ( 85.7 % )
is
higher
than
their model ( 82.6 % )","our m LSTM model||with||word - by - word attention model
our m LSTM model||see that||our performance
our performance||on||test data ( 85.7 % )
test data ( 85.7 % )||is||higher
higher||than||their model ( 82.6 % )
our m LSTM model||under||same setting
same setting||with||d
d||=||150
",,"Results||compare||our m LSTM model
",,,,,,
results,"( 3 ) The performance of mLSTM with bi - LSTM sentence modeling compared with the model with standard LSTM sentence modeling when d is set to 150 shows that using bi - LSTM to process the original sentences helps ( 86.0 % vs. 85.7 % on the test data ) , but the difference is small and the complexity of bi - LSTM is much higher than LSTM .","performance
of
mLSTM
with
bi - LSTM sentence modeling
compared with
model
with
standard LSTM sentence modeling
when
d
set to
150
shows that
bi - LSTM
to process
original sentences
helps
86.0 % vs. 85.7 %
on
test data","performance||of||mLSTM
mLSTM||with||bi - LSTM sentence modeling
mLSTM||compared with||model
model||with||standard LSTM sentence modeling
model||shows that||bi - LSTM
86.0 % vs. 85.7 %||on||test data
bi - LSTM||to process||original sentences
model||when||d
d||set to||150
","bi - LSTM||has||helps
helps||has||86.0 % vs. 85.7 %
",,"Results||has||performance
",,,,,
results,"( 4 ) Interestingly , when we experimented with the m LSTM model using the pre-trained word embeddings instead of LSTMgenerated hidden states as initial representations of the premise and the hypothesis , we were able to achieve an accuracy of 85.3 % on the test data , which is still better than previously reported state of the art .","experimented with
m LSTM model
using
pre-trained word embeddings
instead of
LSTMgenerated hidden states
as
initial representations
of
premise and the hypothesis
able to achieve
accuracy
of
85.3 %
on
test data
is
better
than
previously reported state of the art","m LSTM model||using||pre-trained word embeddings
pre-trained word embeddings||as||initial representations
initial representations||of||premise and the hypothesis
initial representations||able to achieve||accuracy
accuracy||of||85.3 %
85.3 %||is||better
better||than||previously reported state of the art
85.3 %||on||test data
pre-trained word embeddings||instead of||LSTMgenerated hidden states
",,"Results||experimented with||m LSTM model
",,,,,,
research-problem,End - to - End Answer Chunk Extraction and Ranking for Reading Comprehension,Reading Comprehension,,,,,"Contribution||has research problem||Reading Comprehension
",,,,
research-problem,"This paper proposes dynamic chunk reader ( DCR ) , an end - toend neural reading comprehension ( RC ) model that is able to extract and rank a set of answer candidates from a given document to answer questions .",neural reading comprehension ( RC ),,,,,"Contribution||has research problem||neural reading comprehension ( RC )
",,,,
research-problem,Reading comprehension - based question answering ( RCQA ) is the task of answering a question with a chunk of text taken from related document ( s ) .,Reading comprehension - based question answering ( RCQA ),,,,,"Contribution||has research problem||Reading comprehension - based question answering ( RCQA )
",,,,
research-problem,"Different from the above two assumptions for RCQA , in the real - world QA scenario , people may ask questions about both entities ( factoid ) and non-entities such as explanations and reasons ( non -factoid ) ( see for examples ) .",RCQA,,,,,"Contribution||has research problem||RCQA
",,,,
model,"Our proposed model , called dynamic chunk reader ( DCR ) , not only significantly differs from both the above systems in the way that answer candidates are generated and ranked , but also shares merits with both works .","called
dynamic chunk reader ( DCR )",,,"Model||called||dynamic chunk reader ( DCR )
",,,,,,
model,"First , our model uses deep networks to learn better representations for candidate answer chunks , instead of using fixed feature representations as in .","uses
deep networks
to learn
better representations
for
candidate answer chunks
instead of
fixed feature representations","deep networks||to learn||better representations
better representations||for||candidate answer chunks
better representations||instead of||fixed feature representations
",,"Model||uses||deep networks
",,,,,,
model,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .","represents
answer candidates
as
chunks
instead of
word - level representations
to make
model
aware of
subtle differences
among
candidates","answer candidates||as||chunks
chunks||to make||model
model||aware of||subtle differences
subtle differences||among||candidates
chunks||instead of||word - level representations
",,"Model||represents||answer candidates
",,,,,,
experimental-setup,We pre-processed the SQuAD dataset using Stanford CoreNLP tool 5 with its default setting to tokenize the text and obtain the POS and NE annotations .,"pre-processed
SQuAD dataset
using
Stanford CoreNLP tool
with
default setting
to tokenize
text
obtain
POS and NE annotations","SQuAD dataset||using||Stanford CoreNLP tool
Stanford CoreNLP tool||with||default setting
Stanford CoreNLP tool||to tokenize||text
text||obtain||POS and NE annotations
",,"Experimental setup||pre-processed||SQuAD dataset
",,,,,,
experimental-setup,"To train our model , we used stochastic gradient descent with the ADAM optimizer , with an initial learning rate of 0.001 .","used
stochastic gradient descent
with
ADAM optimizer
initial learning rate
of
0.001","stochastic gradient descent||with||ADAM optimizer
stochastic gradient descent||with||initial learning rate
initial learning rate||of||0.001
",,"Experimental setup||used||stochastic gradient descent
",,,,,,
experimental-setup,"All GRU weights were initialized from a uniform distribution between ( - 0.01 , 0.01 ) .","All GRU weights
initialized from
uniform distribution
between
( - 0.01 , 0.01 )","All GRU weights||initialized from||uniform distribution
uniform distribution||between||( - 0.01 , 0.01 )
",,,"Experimental setup||has||All GRU weights
",,,,,
experimental-setup,"The hidden state size , d , was set to 300 for all GRUs .","hidden state size
d
set to
300
for
all GRUs","d||set to||300
300||for||all GRUs
","hidden state size||has||d
",,"Experimental setup||has||hidden state size
",,,,,
experimental-setup,"We also applied dropout of rate 0.2 to the embedding layer of input bi - GRU encoder , and gradient clipping when the norm of gradients exceeded 10 .","dropout
of rate
0.2
to
embedding layer
of
input bi - GRU encoder
gradient clipping
when
norm
of
gradients
exceeded
10","gradient clipping||when||norm
norm||exceeded||10
norm||of||gradients
dropout||of rate||0.2
0.2||to||embedding layer
embedding layer||of||input bi - GRU encoder
",,,"Experimental setup||applied||gradient clipping
Experimental setup||applied||dropout
",,,,,
experimental-setup,We trained in mini-batch style ( mini - batch size is 180 ) and applied zero - padding to the passage and question inputs in each batch .,"trained in
mini-batch style
mini - batch size
is
180
applied
zero - padding
to
passage and question inputs
in
each batch","zero - padding||to||passage and question inputs
passage and question inputs||in||each batch
mini - batch size||is||180
","mini-batch style||has||mini - batch size
","Experimental setup||applied||zero - padding
Experimental setup||trained in||mini-batch style
",,,,,,
experimental-setup,"We also set the maximum passage length to be 300 tokens , and pruned all the tokens after the 300 - th token in the training set to save memory and speedup the training process .","set
maximum passage length
to be
300 tokens
pruned
all the tokens
after
300 - th token
in
training set","all the tokens||after||300 - th token
300 - th token||in||training set
maximum passage length||to be||300 tokens
","maximum passage length||has||pruned
pruned||has||all the tokens
","Experimental setup||set||maximum passage length
",,,,,,
experimental-setup,"We trained the model for at most 30 epochs , and in case the accuracy did not improve for 10 epochs , we stopped training .","trained
model
for
at most 30 epochs","model||for||at most 30 epochs
",,"Experimental setup||trained||model
",,,,,,
experimental-setup,"For the feature ranking - based system , we used jforest ranker ( Ganjis affar , Caruana , and Lopes 2011 ) with Lambda MART - Regression Tree algorithm and the ranking metric was NDCG @ 10 .","For
feature ranking - based system
used
jforest ranker
with
Lambda MART - Regression Tree algorithm","feature ranking - based system||used||jforest ranker
jforest ranker||with||Lambda MART - Regression Tree algorithm
",,"Experimental setup||For||feature ranking - based system
",,,,,,
results,Results shows our main results on the SQuAD dataset .,"on
SQuAD dataset",,,"Results||on||SQuAD dataset
",,,,,,
results,"Compared to the scores reported in , our exact match ( EM ) and F1 on the development set and EM score on the test set are better , and F1 on the test set is comparable .","our exact match ( EM ) and F1
on
development set
on
test set
are
better
F1
comparable","our exact match ( EM ) and F1||on||development set
F1||on||test set
","better||has||our exact match ( EM ) and F1
comparable||has||F1
",,,,"SQuAD dataset||are||better
SQuAD dataset||are||comparable
",,,
results,"As the first row of shows , our baseline system improves 10 % ( EM ) over , row 1 ) , the feature - based ranking system .","our baseline system
improves
10 % ( EM )
over
feature - based ranking system","10 % ( EM )||over||feature - based ranking system
","our baseline system||has||improves
improves||has||10 % ( EM )
",,"Results||has||our baseline system
",,,,,
results,"However when compared to our DCR model , row 2 ) , the baseline ( row 1 ) is more than 12 % ( EM ) behind even though it is based on the state - of - the - art model for cloze - style RC tasks .","compared to
our DCR model
baseline
is
more than 12 % ( EM )
behind","baseline||is||more than 12 % ( EM )
","our DCR model||has||baseline
more than 12 % ( EM )||has||behind
","Results||compared to||our DCR model
",,,,,,
ablation-analysis,"First , replacing the word - by - word attention with Attentive Reader style attention decreases the EM score by about 4.5 % , showing the strength of our proposed attention mechanism .","replacing
word - by - word attention
with
Attentive Reader
style attention
decreases
EM score
by about
4.5 %","word - by - word attention||with||Attentive Reader
decreases||by about||4.5 %
","Attentive Reader||has||style attention
style attention||has||decreases
decreases||has||EM score
","Ablation analysis||replacing||word - by - word attention
",,,,,,
ablation-analysis,The result shows that POS feature ( 1 ) and question - word feature ( 3 ) are the two most important features .,"shows
POS feature ( 1 ) and question - word feature ( 3 )
are
two most important features","POS feature ( 1 ) and question - word feature ( 3 )||are||two most important features
",,"Ablation analysis||shows||POS feature ( 1 ) and question - word feature ( 3 )
",,,,,,
ablation-analysis,"Finally , combining the DCR model with the proposed POS - trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks .","combining
DCR model
with
proposed POS - trie constraints
yields
score
similar to
DCR model
with
all possible n-gram chunks","DCR model||with||proposed POS - trie constraints
DCR model||yields||score
score||similar to||DCR model
DCR model||with||all possible n-gram chunks
",,"Ablation analysis||combining||DCR model
",,,,,,
research-problem,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,MACHINE COMPREHENSION,,,,,"Contribution||has research problem||MACHINE COMPREHENSION
",,,,
research-problem,Machine comprehension of text is an important problem in natural language processing .,Machine comprehension of text,,,,,"Contribution||has research problem||Machine comprehension of text
",,,,
model,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .",,,,,,,,,,
model,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .",,,,,,,,,,
model,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,"propose
two ways
to apply
Ptr - Net model
for
our task
sequence model
boundary model","two ways||to apply||Ptr - Net model
Ptr - Net model||for||our task
","two ways||name||sequence model
two ways||name||boundary model
","Model||propose||two ways
",,,,,,
model,We also further extend the boundary model with a search mechanism .,"extend
boundary model
with
search mechanism","boundary model||with||search mechanism
",,"Model||extend||boundary model
",,,,,,
experimental-setup,"We first tokenize all the passages , questions and answers .","tokenize
all the passages , questions and answers",,,"Experimental setup||tokenize||all the passages , questions and answers
",,,,,,
experimental-setup,We use word embeddings from GloVe to initialize the model .,"use
word embeddings
from
GloVe
to initialize
model","word embeddings||to initialize||model
word embeddings||from||GloVe
",,"Experimental setup||use||word embeddings
",,,,,,
experimental-setup,Words not found in Glo Ve are initialized as zero vectors .,"Words
not found in
Glo Ve
initialized as
zero vectors","Words||not found in||Glo Ve
Words||initialized as||zero vectors
",,,"Experimental setup||has||Words
",,,,,
experimental-setup,The dimensionality l of the hidden layers is set to be 150 or 300 .,"dimensionality l
of
hidden layers
set to be
150 or 300","dimensionality l||set to be||150 or 300
dimensionality l||of||hidden layers
",,,"Experimental setup||has||dimensionality l
",,,,,
experimental-setup,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,"ADAMAX
with
coefficients
? 1 = 0.9 and ? 2 = 0.999
to optimize
model","ADAMAX||with||coefficients
coefficients||to optimize||model
","coefficients||has||? 1 = 0.9 and ? 2 = 0.999
",,"Experimental setup||use||ADAMAX
",,,,,
results,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .","our boundary model
outperformed
sequence model
achieving
exact match score
of
61.1 %
F1 score
of
71.2 %","our boundary model||achieving||exact match score
exact match score||of||61.1 %
our boundary model||achieving||F1 score
F1 score||of||71.2 %
","our boundary model||has||outperformed
outperformed||has||sequence model
",,"Results||has||our boundary model
",,,,,
results,"In particular , in terms of the exact match score , the boundary model has a clear advantage over the sequence model .","in terms of
exact match score
boundary model
clear advantage
over
sequence model","clear advantage||over||sequence model
","exact match score||has||boundary model
boundary model||has||clear advantage
","Results||in terms of||exact match score
",,,,,,
results,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .","adding
Bi - Ans - Ptr
with
bi-directional pre-processing LSTM
get
1.2 % improvement
in
F1","Bi - Ans - Ptr||with||bi-directional pre-processing LSTM
Bi - Ans - Ptr||get||1.2 % improvement
1.2 % improvement||in||F1
",,"Results||adding||Bi - Ans - Ptr
",,,,,,
research-problem,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,Multi-hop Reading Comprehension,,,,,"Contribution||has research problem||Multi-hop Reading Comprehension
",,,,
research-problem,"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document .",Reading Comprehension,,,,,"Contribution||has research problem||Reading Comprehension
",,,,
research-problem,Contemporary end - to - end Reading Comprehension ( RC ) methods can learn to extract the correct answer span within a given text and approach human - level performance .,Reading Comprehension ( RC ),,,,,"Contribution||has research problem||Reading Comprehension ( RC )
",,,,
dataset,"The first , WIKIHOP , uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity can not be located in the entity 's article .","WIKIHOP
uses
sets
of
WIKIPEDIA articles
where
answers
to
queries
about
specific properties
of
entity
can not be located in
entity 's article","WIKIHOP||uses||sets
sets||of||WIKIPEDIA articles
WIKIPEDIA articles||where||answers
answers||to||queries
queries||about||specific properties
specific properties||of||entity
specific properties||can not be located in||entity 's article
",,,"Dataset||has||WIKIHOP
",,,,,
dataset,"In the second dataset , MEDHOP , the goal is to establish drug - drug interactions based on scientific findings about drugs and proteins and their interactions , found across multiple MEDLINE abstracts .","MEDHOP
goal
to establish
drug - drug interactions
based on
scientific findings
about
drugs and proteins and their interactions
found across
multiple MEDLINE abstracts","goal||to establish||drug - drug interactions
drug - drug interactions||based on||scientific findings
scientific findings||about||drugs and proteins and their interactions
drugs and proteins and their interactions||found across||multiple MEDLINE abstracts
","MEDHOP||has||goal
",,"Dataset||has||MEDHOP
",,,,,
dataset,"For both datasets we draw upon existing Knowledge Bases ( KBs ) , WIKIDATA and DRUG - BANK , as ground truth , utilizing distant supervision ) to induce the data - similar to and .","draw upon
existing Knowledge Bases ( KBs )
WIKIDATA and DRUG - BANK",,"existing Knowledge Bases ( KBs )||name||WIKIDATA and DRUG - BANK
","Dataset||draw upon||existing Knowledge Bases ( KBs )
",,,,,,
baselines,Random Selects a random candidate ; note that the number of candidates differs between samples .,"Random
Selects
random candidate","Random||Selects||random candidate
",,,"Baselines||has||Random
",,,,,
baselines,Max- mention,Max- mention,,,,"Baselines||has||Max- mention
",,,,,
baselines,Predicts the most frequently mentioned candidate in the support documents,"Predicts
most frequently mentioned candidate
in
support documents","most frequently mentioned candidate||in||support documents
",,,,,"Max- mention||Predicts||most frequently mentioned candidate
",,,
baselines,Majority - candidate - per-query - type,Majority - candidate - per-query - type,,,,"Baselines||has||Majority - candidate - per-query - type
",,,,,
baselines,Predicts the candidate c ?,"Predicts
candidate",,,,,,"Majority - candidate - per-query - type||Predicts||candidate
",,,
baselines,"C q that was most frequently observed as the true answer in the training set , given the query type of q .","that was
most frequently observed
as
true answer","most frequently observed||as||true answer
",,,,,"candidate||that was||most frequently observed
",,,
baselines,TF - IDF,TF - IDF,,,,"Baselines||has||TF - IDF
",,,,,"TF - IDF||has||Retrieval - based models
"
baselines,Retrieval - based models are known to be strong QA baselines if candidate answers are provided .,"Retrieval - based models
are
known to be
strong QA baselines
if
candidate answers
provided","Retrieval - based models||known to be||strong QA baselines
strong QA baselines||if||candidate answers
candidate answers||are||provided
",,,,,,,,
baselines,"( 1 ) Document - cue During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the effect that the correct candidate is often indicated solely by the presence of certain documents in Sq .","Document - cue
that
effect
correct candidate
indicated solely by
presence of certain documents","effect||that||correct candidate
correct candidate||indicated solely by||presence of certain documents
","Document - cue||has||effect
",,"Baselines||has||Document - cue
",,,,,
results,"The Document - cue baseline can predict more than a third of the samples correctly , for both datasets , even after sub - sampling frequent document - answer pairs for WIKIHOP .","Document - cue baseline
predict
more than a third of the samples
correctly
for
after sub - sampling
frequent document - answer pairs
WIKIHOP","Document - cue baseline||predict||more than a third of the samples
more than a third of the samples||after sub - sampling||frequent document - answer pairs
frequent document - answer pairs||for||WIKIHOP
","more than a third of the samples||has||correctly
",,"Results||has||Document - cue baseline
",,,,,
results,"In the masked setup all baseline models reliant on lexical cues fail in the face of the randomized answer expressions , since the same answer option has different placeholders in different samples .","In
masked setup
all baseline models
reliant on
lexical cues
fail
in the face of the randomized answer expressions","all baseline models||fail||in the face of the randomized answer expressions
all baseline models||reliant on||lexical cues
","masked setup||has||all baseline models
","Results||In||masked setup
",,,,,,
results,Both neural RC models are able to largely retain or even improve their strong performance when answers are masked : they are able to leverage the textual context of the candidate expressions .,"Both neural RC models
are
able to
largely retain or even improve
strong performance
when
answers
masked","Both neural RC models||able to||largely retain or even improve
strong performance||when||answers
answers||are||masked
","largely retain or even improve||has||strong performance
",,"Results||has||Both neural RC models
",,,,,
results,"In contrast , for the open - domain setting of WIKIHOP , a reduction of the answer vocabulary to 100 random single - token mask expressions clearly helps the model in selecting a candidate span , compared to the multi-token candidate expressions in the unmasked setting .","for
open - domain setting
of
WIKIHOP
reduction
of
answer vocabulary
to
100 random single - token mask expressions
helps
model
in selecting
candidate span
compared to
multi-token candidate expressions
in
unmasked setting","open - domain setting||of||WIKIHOP
reduction||of||answer vocabulary
answer vocabulary||to||100 random single - token mask expressions
100 random single - token mask expressions||helps||model
model||in selecting||candidate span
model||compared to||multi-token candidate expressions
multi-token candidate expressions||in||unmasked setting
","open - domain setting||has||reduction
","Results||for||open - domain setting
",,,,,,
research-problem,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,EXTRACTIVE QUESTION ANSWERING,,,,,"Contribution||has research problem||EXTRACTIVE QUESTION ANSWERING
",,,,
research-problem,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",answer extraction,,,,,"Contribution||has research problem||answer extraction
",,,,
model,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .","present
novel neural architecture
called
RASOR
builds
fixed - length span representations
reusing
recurrent computations
for
shared substructures","novel neural architecture||called||RASOR
novel neural architecture||builds||fixed - length span representations
fixed - length span representations||reusing||recurrent computations
recurrent computations||for||shared substructures
",,"Model||present||novel neural architecture
",,,,,,
model,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .","demonstrate
directly classifying
each of
competing spans
training
with
global normalization
over
all possible spans
leads to
significant increase
in
performance","training||with||global normalization
global normalization||over||all possible spans
global normalization||leads to||significant increase
significant increase||in||performance
directly classifying||each of||competing spans
",,"Model||demonstrate||training
Model||demonstrate||directly classifying
",,,,,,
experimental-setup,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,"represent
of
words
in
question and document
using
300 dimensional GloVe embeddings
trained on
corpus
of
840 bn words","words||using||300 dimensional GloVe embeddings
300 dimensional GloVe embeddings||trained on||corpus
corpus||of||840 bn words
words||in||question and document
",,"Experimental setup||represent||words
",,,,,,"300 dimensional GloVe embeddings||has||all out of vocabulary ( OOV ) words
"
experimental-setup,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,"cover
200 k words
all out of vocabulary ( OOV ) words
projected onto
one
1 m randomly initialized 300d embeddings","all out of vocabulary ( OOV ) words||projected onto||one
","one||of||1 m randomly initialized 300d embeddings
",,,,"300 dimensional GloVe embeddings||cover||200 k words
",,,
experimental-setup,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .","couple
input and forget gates
in
our LSTMs
use
single dropout mask
to apply
dropout
across
all LSTM time - steps","input and forget gates||in||our LSTMs
single dropout mask||to apply||dropout
dropout||across||all LSTM time - steps
",,"Experimental setup||couple||input and forget gates
Experimental setup||use||single dropout mask
",,,,,,
experimental-setup,Hidden layers in the feed forward neural networks use rectified linear units .,"Hidden layers
in
feed forward neural networks
use
rectified linear units","Hidden layers||in||feed forward neural networks
Hidden layers||use||rectified linear units
",,,"Experimental setup||has||Hidden layers
",,,,,
experimental-setup,Answer candidates are limited to spans with at most 30 words .,"Answer candidates
limited to
spans
with
at most 30 words","Answer candidates||limited to||spans
spans||with||at most 30 words
",,,"Experimental setup||has||Answer candidates
",,,,,
experimental-setup,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .","To choose
final model configuration
ran
grid searches
over
dimensionality
of
LSTM hidden states
width and depth
of
feed forward neural networks
dropout
for
LSTMs
number
of
stacked LSTM layers
decay multiplier [ 0.9 , 0.95 , 1.0 ]
multiply
learning rate every 10 k steps","final model configuration||ran||grid searches
grid searches||over||dimensionality
dimensionality||of||LSTM hidden states
grid searches||over||width and depth
width and depth||of||feed forward neural networks
grid searches||over||dropout
dropout||for||LSTMs
grid searches||over||number
number||of||stacked LSTM layers
grid searches||over||decay multiplier [ 0.9 , 0.95 , 1.0 ]
decay multiplier [ 0.9 , 0.95 , 1.0 ]||multiply||learning rate every 10 k steps
",,"Experimental setup||To choose||final model configuration
",,,,,,
experimental-setup,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,"best model
uses
50d LSTM states
two - layer BiLSTMs
for
span encoder
passage - independent question representation
dropout
of
0.1
learning rate decay
of
5 % every 10 k steps","two - layer BiLSTMs||for||span encoder
two - layer BiLSTMs||for||passage - independent question representation
learning rate decay||of||5 % every 10 k steps
dropout||of||0.1
best model||uses||50d LSTM states
",,,"Experimental setup||has||two - layer BiLSTMs
Experimental setup||has||learning rate decay
Experimental setup||has||dropout
Experimental setup||has||best model
",,,,,
experimental-setup,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,"models
implemented using
TensorFlow
trained on
SQUAD training set
using
ADAM optimizer
with
mini-batch size
of
4
trained using
10 asynchronous training threads
on
single machine","models||implemented using||TensorFlow
models||trained on||SQUAD training set
SQUAD training set||using||ADAM optimizer
ADAM optimizer||with||mini-batch size
mini-batch size||of||4
models||trained using||10 asynchronous training threads
10 asynchronous training threads||on||single machine
",,,"Experimental setup||has||models
",,,,,
results,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .","of
RASOR
achieves
error reduction
more than 50 %
in terms of
exact match and F1
relative to
human performance upper bound","RASOR||achieves||error reduction
error reduction||of||more than 50 %
more than 50 %||relative to||human performance upper bound
more than 50 %||in terms of||exact match and F1
",,,"Results||has||RASOR
",,,,,
results,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .","efficiently and explicitly
model
quadratic number of possible answers
leads to
14 % error reduction
over
best performing Match - LSTM model","quadratic number of possible answers||leads to||14 % error reduction
14 % error reduction||over||best performing Match - LSTM model
","efficiently and explicitly||has||quadratic number of possible answers
",,,,"RASOR||model||efficiently and explicitly
",,,
ablation-analysis,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .","passage - aligned question representation
is
crucial","passage - aligned question representation||is||crucial
",,,"Ablation analysis||has||passage - aligned question representation
",,,,,
ablation-analysis,"First , we observe general improvements when using labels that closely align with the task .","observe
general improvements
when using
labels
that
closely align
with
task","general improvements||when using||labels
labels||that||closely align
closely align||with||task
",,"Ablation analysis||observe||general improvements
",,,,,,
ablation-analysis,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .","interactions between the endpoints
using
spanlevel FFNN","interactions between the endpoints||using||spanlevel FFNN
",,,"Ablation analysis||observe||interactions between the endpoints
",,,,,"interactions between the endpoints||has||RASOR
"
ablation-analysis,"RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .","RASOR
outperforms
endpoint prediction model
by
1.1
in
exact match","RASOR||outperforms||endpoint prediction model
endpoint prediction model||by||1.1
1.1||in||exact match
",,,,,,,,
research-problem,This paper describes the KeLP system participating in the SemEval - 2016 Community Question Answering ( c QA ) task .,Community Question Answering ( c QA ),,,,,"Contribution||has research problem||Community Question Answering ( c QA )
",,,,
research-problem,"In this task , participants are asked to automatically provide good answers in a c QA setting .",c QA,,,,,"Contribution||has research problem||c QA
",,,,
model,All the above subtasks have been modeled as binary classification problems : kernel - based classifiers are trained and the classification score is used to sort the instances and produce the final ranking .,"modeled as
binary classification problems
kernel - based classifiers
are
trained
classification score
to sort
instances
produce
final ranking","kernel - based classifiers||are||trained
classification score||to sort||instances
classification score||produce||final ranking
","binary classification problems||has||kernel - based classifiers
kernel - based classifiers||has||classification score
","Model||modeled as||binary classification problems
",,,,,,
model,"All classifiers and kernels have been implemented within the Kernel - based Learning Platform 2 ( KeLP ) , thus determining the team 's name .","classifiers and kernels
implemented within
Kernel - based Learning Platform 2 ( KeLP )","classifiers and kernels||implemented within||Kernel - based Learning Platform 2 ( KeLP )
",,,"Model||has||classifiers and kernels
",,,,,
model,"The proposed solution provides three main contributions : ( i ) we employ the approach proposed in , which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees .","applies
tree kernels
directly to
question and answer texts
modeled as
pairs
of
linked syntactic trees","tree kernels||directly to||question and answer texts
question and answer texts||modeled as||pairs
pairs||of||linked syntactic trees
",,"Model||applies||tree kernels
",,,,,,
model,( iii ) we propose a stacking schema so that classifiers for Subtask B and C exploit the inferences obtained in the previous subtasks .,"propose
stacking schema
classifiers
for
Subtask B and C
exploit
inferences
obtained in
previous subtasks","classifiers||for||Subtask B and C
classifiers||exploit||inferences
inferences||obtained in||previous subtasks
","stacking schema||has||classifiers
","Model||propose||stacking schema
",,,,,,
experiments,Subtask A,Subtask A,,,,,,,,"Tasks||has||Subtask A
","Subtask A||has||Results
"
experiments,Results : reports the outcome on Subtask A .,,,,,,,,,,
experiments,"The good results on the 10 fold cross validations are confirmed on the official test set : the model is very accurate and achieved the first position among 12 systems , with the best MAP .","achieved
first position
among
12 systems
with
best MAP","first position||with||best MAP
first position||among||12 systems
",,,,,"Results||achieved||first position
",,,
experiments,Subtask B,Subtask B,,,,,,,,"Tasks||has||Subtask B
","Subtask B||has||Results
"
experiments,"On the official test set , our primary submission achieved the third position w.r.t. MAP among 11 systems .","our primary submission
achieved
third position
w.r.t.
MAP
among
11 systems","our primary submission||achieved||third position
third position||w.r.t.||MAP
third position||among||11 systems
",,,,,,,"Results||has||our primary submission
",
experiments,The primary system achieves the highest F 1 and accuracy on both tuning and test stages .,"primary system
achieves
highest F 1 and accuracy
on
both tuning and test stages","primary system||achieves||highest F 1 and accuracy
highest F 1 and accuracy||on||both tuning and test stages
",,,,,,,"Results||has||primary system
",
experiments,Subtask C Model :,Subtask C,,,,,,,,"Tasks||has||Subtask C
","Subtask C||has||Results
"
experiments,"Our primary submission achieved the second highest MAP , while our Contrastive 2 is the best result .","primary submission
achieved
second highest MAP","primary submission||achieved||second highest MAP
",,,,,,,"Results||has||primary submission
",
experiments,It should be also noted that the F 1 our system is the best among 10 primary submissions .,"noted that
F 1 our system
is
best
among
10 primary submissions","F 1 our system||is||best
best||among||10 primary submissions
",,,,,"Results||noted that||F 1 our system
",,,
research-problem,We present a novel recurrent neural network ( RNN ) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant / irrelevant information in its memory .,recurrent neural network ( RNN ),,,,,"Contribution||has research problem||recurrent neural network ( RNN )
",,,,
research-problem,Recurrent Neural Networks with gating units - such as Long Short Term Memory ( LSTMs ) and Gated Recurrent Units ( GRUs ) ),Recurrent Neural Networks with gating units,,,,,"Contribution||has research problem||Recurrent Neural Networks with gating units
",,,,
research-problem,These works have proven the importance of gating units for Recurrent Neural Networks .,gating units for Recurrent Neural Networks,,,,,"Contribution||has research problem||gating units for Recurrent Neural Networks
",,,,
research-problem,The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs .,RNNs,,,,,"Contribution||has research problem||RNNs
",,,,
model,"We propose a new architecture , the Gated Orthogonal Recurrent Unit ( GORU ) , which combines the advantages of the above two frameworks , namely ( i ) the ability to capture long term dependencies by using orthogonal matrices and ( ii ) the ability to "" forget "" by using a GRU structure .","propose
new architecture
Gated Orthogonal Recurrent Unit ( GORU )
combines
ability
to capture
long term dependencies
by using
orthogonal matrices
to
forget
by using
GRU structure","new architecture||combines||ability
ability||to capture||long term dependencies
long term dependencies||by using||orthogonal matrices
ability||to||forget
forget||by using||GRU structure
","new architecture||name||Gated Orthogonal Recurrent Unit ( GORU )
","Model||propose||new architecture
",,,,,,
model,"We demonstrate that GORU is able to learn long term dependencies effectively , even in complicated datasets which require a forgetting ability .","demonstrate
GORU
able to learn
long term dependencies
effectively","GORU||able to learn||long term dependencies
","long term dependencies||has||effectively
","Model||demonstrate||GORU
",,,,,,
model,"In this work , we focus on implementation of orthogonal transition matrices which is just a subset of the unitary matrices .","focus on
implementation
of
orthogonal transition matrices
which is
subset
of
unitary matrices","implementation||of||orthogonal transition matrices
orthogonal transition matrices||which is||subset
subset||of||unitary matrices
",,"Model||focus on||implementation
",,,,,,
code,"GORU is implemented in Tensorflow , available from https://github.com/jingli9111/GORU-tensorflow",https://github.com/jingli9111/GORU-tensorflow,,,,,"Contribution||Code||https://github.com/jingli9111/GORU-tensorflow
",,,,
,The first task we consider is the well known Copying Memory Task .,Copying Memory Task,,,,,,,,,
,"In this experiment , we use RMSProp optimization with a learning rate of 0.001 and a decay rate of 0.9 for all models .","use
RMSProp optimization
with
learning rate
of
0.001
decay rate
of
0.9",,,,,,,,,
,The batch size is set to 128 .,"batch size
set to
128",,,,,,,,,
,"Hidden state sizes are set to 128 , 100 , 90 , 512 , respectively to match total number of hidden to hidden parameters .","Hidden state sizes
set to
128 , 100 , 90 , 512
to match
total number
of
hidden to hidden parameters",,,,,,,,,
,The GORU is the only gated - system to successfully solve this task while the GRU and LSTM get stuck at the baseline as shown in .,"GORU
is
only gated - system
to
successfully solve
task",,,,,,,,,
,Denoise Task,Denoise Task,,,,,,,,,
,"Just as in the previous experiment , we use RM - SProp optimization algorithm with a learning rate of 0.01 and a decay rate of 0.9 for all models .","use
RM - SProp optimization algorithm
with
learning rate
of
0.01
decay rate
of
0.9",,,,,,,,,
,The batch size is set to 128 .,"batch size
set to
128",,,,,,,,,
,"Hidden state sizes are set to 128 , 100 , 90 , 512 , respectively to match total number of hidden to hidden parameters .","Hidden state sizes
set to
128 , 100 , 90 , 512
to match
total number of hidden to hidden parameters",,,,,,,,,
,"EURNN get stuck at the baseline because of lacking forgetting mechanism , while GORU and GRU successfully solve the task .","GORU and GRU
successfully solve
task",,,,,,,,,
,Parenthesis Task,Parenthesis Task,,,,,,,,,
,"In our experiment , the total input length is set to 200 .","total input length
set to
200",,,,,,,,,
,"We used batch size 128 and RMSProp Optimizer with a learning rate 0.001 , decay rate 0.9 on all models .","used
batch size
128
RMSProp Optimizer
with
learning rate
0.001
decay rate
0.9",,,,,,,,,
,"The GORU is able to successfully outperform GRU , LSTM and EURNN in terms of both learning speed and final performances as shown in .","GORU
able to
successfully outperform
GRU
LSTM
EURNN
in terms of
learning speed
final performances",,,,,,,,,
,We also analyzed the activations of the update gates for GORU and GRU .,"analyzed
update gates
for
GORU and GRU",,,,,,,,,
,Algorithmic Task,Algorithmic Task,,,,,,,,,
,We used batch size 50 and hidden size 128 for all models .,"used
batch size
50
hidden size
128",,,,,,,,,
,The RNNs are trained with RMSProp optimizer with a learning rate of 0.001 and decay rate of 0.9 .,"RNNs
trained with
RMSProp optimizer
with
learning rate
of
0.001
decay rate
of
0.9",,,,,,,,,
,We found that the GORU performs averagely better than GRU / LSTM and EURNN .,"found that
GORU
performs
averagely better
than
GRU / LSTM and EURNN",,,,,,,,,
,We found that the GORU performs averagely better than GRU / LSTM and EURNN .,,,,,,,,,,
research-problem,CliCR : A Dataset of Clinical Case Reports for Machine Reading Comprehension *,Machine Reading Comprehension,,,,,"Contribution||has research problem||Machine Reading Comprehension
",,,,
research-problem,We present a new dataset for machine comprehension in the medical domain .,machine comprehension,,,,,"Contribution||has research problem||machine comprehension
",,,,
dataset,"For our dataset , we construct queries , answers and supporting passages from BMJ Case Reports , the largest online repository of such documents .","construct
queries , answers and supporting passages
from
BMJ Case Reports","queries , answers and supporting passages||from||BMJ Case Reports
",,"Dataset||construct||queries , answers and supporting passages
",,,,,,
dataset,"A case report is a detailed description of a clinical case that focuses on rare diseases , unusual presentation of common conditions and novel treatment methods .","case report
is
detailed description
of
clinical case
focuses on
rare diseases
unusual presentation of common conditions
novel treatment methods","case report||is||detailed description
detailed description||of||clinical case
clinical case||focuses on||rare diseases
clinical case||focuses on||unusual presentation of common conditions
clinical case||focuses on||novel treatment methods
",,,"Dataset||has||case report
",,,,,
dataset,"Each report contains a Learning points section , summarizing the key pieces of information from that report .","contains
Learning points section
summarizing
key pieces
of
information","Learning points section||summarizing||key pieces
key pieces||of||information
",,,,,"case report||contains||Learning points section
",,,
dataset,We use these learning points to create queries by blanking out a medical entity .,"use
learning points
to create
queries
by blanking out
medical entity","learning points||to create||queries
queries||by blanking out||medical entity
",,"Dataset||use||learning points
",,,,,,
dataset,"Our dataset contains around 100,000 queries on 12,000 case reports , has long support passages ( around 1,500 tokens on average ) and includes answers which are single - or multiword medical entities .","contains
around 100,000 queries
on
12,000 case reports
long support passages
includes
answers
which are
single - or multiword medical entities","long support passages||includes||answers
answers||which are||single - or multiword medical entities
around 100,000 queries||on||12,000 case reports
",,"Dataset||contains||long support passages
Dataset||contains||around 100,000 queries
",,,,,,
baselines,We also include a distance - based method that uses word embeddings ( sim-entity ) .,"include
distance - based method
that uses
word embeddings","distance - based method||that uses||word embeddings
",,"Baselines||include||distance - based method
",,,,,,
baselines,We trained a 4 - gram Kneser - Ney model on CliCR training data ( with multi-word entities represented as a single token ) using SRILM .,"trained
4 - gram Kneser - Ney model
on
CliCR training data
with
multi-word entities
represented as
single token
using
SRILM","4 - gram Kneser - Ney model||on||CliCR training data
CliCR training data||with||multi-word entities
multi-word entities||represented as||single token
CliCR training data||using||SRILM
",,"Baselines||trained||4 - gram Kneser - Ney model
",,,,,,
results,"We see that answer prediction based on contextual representation of queries and passages ( sim -entity ) achieves a strong base performance that is only outperformed by GA 7 In precision , the number of correct words is divided by the number of all predicted words .","see that
answer prediction
based on
contextual representation
of
queries and passages ( sim -entity )
achieves
strong base performance
that is
outperformed
by
GA","answer prediction||based on||contextual representation
contextual representation||of||queries and passages ( sim -entity )
answer prediction||achieves||strong base performance
strong base performance||that is||outperformed
outperformed||by||GA
",,"Results||see that||answer prediction
",,,,,,
results,"The language model performs poorly on EM and F1 , but the embedding - metric score is higher , likely reflecting the fact that the predicted answers - though mostly incorrect - are related to the ground - truth answers .","language model
performs
poorly
on
EM and F1
embedding - metric score
is
higher","embedding - metric score||is||higher
language model||performs||poorly
poorly||on||EM and F1
",,,"Results||has||embedding - metric score
Results||has||language model
",,,,,
results,"The GA reader performs well across all entity set - ups , even when the entities are not marked in the passage .","GA reader
performs
well
across
all entity set - ups","GA reader||performs||well
well||across||all entity set - ups
",,,"Results||has||GA reader
",,,,,
results,"Upon inspecting the predicted answers more closely , we have observed that GA - NoEnt tends to predict longer answers than GA - Ent / Anonym .","observed that
GA - NoEnt
tends to predict
longer answers
than
GA - Ent / Anonym","GA - NoEnt||tends to predict||longer answers
longer answers||than||GA - Ent / Anonym
",,"Results||observed that||GA - NoEnt
",,,,,,
results,The results for SA reader are far below the per-formance of GA reader .,"SA reader
are
far below
per-formance
of
GA reader","SA reader||are||far below
per-formance||of||GA reader
","far below||has||per-formance
",,"Results||has||SA reader
",,,,,
results,We also see that it performs much better on anonymized entities than on non-anonymized ones .,"performs
much better
on
anonymized entities
than on
non-anonymized ones","much better||than on||non-anonymized ones
much better||on||anonymized entities
",,,,,"GA reader||performs||much better
",,,
research-problem,Equipping deep neural networks ( DNN ) with attention mechanisms provides an effective and parallelizable approach for context fusion and sequence compression .,attention mechanisms,,,,,"Contribution||has research problem||attention mechanisms
",,,,
model,"In this paper , we first propose a novel hard attention mechanism called "" reinforced sequence sampling ( RSS ) "" , which selects tokens from an input sequence in parallel , and differs from existing ones in that it is highly parallelizable without any recurrent structure .","propose
novel hard attention mechanism
called
reinforced sequence sampling ( RSS )
selects
tokens
from
input sequence
in
parallel
is
highly parallelizable
without
any recurrent structure","novel hard attention mechanism||selects||tokens
tokens||from||input sequence
input sequence||in||parallel
novel hard attention mechanism||called||reinforced sequence sampling ( RSS )
novel hard attention mechanism||is||highly parallelizable
highly parallelizable||without||any recurrent structure
",,"Model||propose||novel hard attention mechanism
",,,,,,
model,"We then develop a model , "" reinforced self - attention ( ReSA ) "" , which naturally combines the RSS with a soft self - attention .","develop
reinforced self - attention ( ReSA )
combines
RSS
with
soft self - attention","reinforced self - attention ( ReSA )||combines||RSS
RSS||with||soft self - attention
",,"Model||develop||reinforced self - attention ( ReSA )
",,,,,,
model,"In ReSA , two parameter - untied RSS are respectively applied to two copies of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .","In
ReSA
two parameter - untied RSS
applied to
two copies
of
input sequence","two parameter - untied RSS||applied to||two copies
two copies||of||input sequence
","ReSA||has||two parameter - untied RSS
","Model||In||ReSA
",,,,,,
model,Re SA only models the sparse dependencies between the head and dependent tokens selected by the two RSS modules .,"Re SA
models
sparse dependencies
between
head and dependent tokens
selected by
two RSS modules","Re SA||models||sparse dependencies
sparse dependencies||between||head and dependent tokens
head and dependent tokens||selected by||two RSS modules
",,,"Model||has||Re SA
",,,,,
model,"Finally , we build an sentence - encoding model , "" reinforced self - attention network ( ReSAN ) "" , based on ReSA without any CNN / RNN structure .","build
sentence - encoding model
reinforced self - attention network ( ReSAN )
based on
ReSA
without
any CNN / RNN structure","sentence - encoding model||based on||ReSA
ReSA||without||any CNN / RNN structure
","sentence - encoding model||name||reinforced self - attention network ( ReSAN )
","Model||build||sentence - encoding model
",,,,,,
code,All the experiments codes are released at https://github.com/ taoshen58/DiSAN /tree/master/ReSAN .,https://github.com/ taoshen58/DiSAN /tree/master/ReSAN,,,,,"Contribution||Code||https://github.com/ taoshen58/DiSAN /tree/master/ReSAN
",,,,
experimental-setup,All experiments are conducted in Python with Tensorflow and run on a Nvidia GTX 1080 Ti .,"experiments
conducted in
Python
with
Tensorflow
run on
Nvidia GTX 1080 Ti","experiments||conducted in||Python
Python||with||Tensorflow
experiments||run on||Nvidia GTX 1080 Ti
",,,"Experimental setup||has||experiments
",,,,,
experimental-setup,"We use Adadelta as optimizer , which performs more stable than Adam on ReSAN .","use
Adadelta
as
optimizer
performs
more stable
than
Adam
on
ReSAN","Adadelta||performs||more stable
more stable||than||Adam
more stable||on||ReSAN
Adadelta||as||optimizer
",,"Experimental setup||use||Adadelta
",,,,,,
experimental-setup,We use 300D Glo Ve 6B pre-trained vectors,300D Glo Ve 6B pre-trained vectors,,,,"Experimental setup||use||300D Glo Ve 6B pre-trained vectors
",,,,,
,Natural Language Inference,Natural Language Inference,,,,,,,,,
,"Compared to the methods from official leaderboard , ReSAN outperforms all the sentence encoding based methods and achieves the best test accuracy .","Compared to
methods from official leaderboard
ReSAN
outperforms
all the sentence encoding based methods
achieves
best test accuracy",,,,,,,,,
experiments,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses far fewer parameters with better performance .","compared to
last best models
i.e.
600D Gumbel TreeLSTM encoders
600D Residual stacked encoders
ReSAN
uses
far fewer parameters
with
better performance","ReSAN||with||better performance
ReSAN||uses||far fewer parameters
last best models||i.e.||600D Gumbel TreeLSTM encoders
last best models||i.e.||600D Residual stacked encoders
","last best models||has||ReSAN
",,,,"Results||compared to||last best models
",,,
experiments,"Furthermore , ReSAN even outperforms the 300D SPINN - PI encoders by 3.1 %. , which is a recursive model and uses the result of an external semantic parsing tree as an extra input .","ReSAN
outperforms
300D SPINN - PI encoders
by",,"ReSAN||has||outperforms
outperforms||has||300D SPINN - PI encoders
",,,,,"300D SPINN - PI encoders||by||3.1 %
","Results||has||ReSAN
",
experiments,"Compared to the recurrent models ( e.g. , Bi - LSTM and Bi - GRU ) , ReSAN shows better prediction quality and more compelling efficiency due to parallelizable computations .","Compared to
recurrent models
e.g.
Bi - LSTM
Bi - GRU
ReSAN
shows
better prediction quality
more compelling efficiency
due to
parallelizable computations","recurrent models||due to||parallelizable computations
ReSAN||shows||better prediction quality
ReSAN||shows||more compelling efficiency
recurrent models||e.g.||Bi - LSTM
recurrent models||e.g.||Bi - GRU
","parallelizable computations||has||ReSAN
",,,,"Results||Compared to||recurrent models
",,,
experiments,"Compared to the convolutional models ( i.e. , Multiwindow CNN and Hierarchical CNN ) , ReSAN significantly outperforms them by 3.1 % and 2.4 % respectively due to the weakness of CNNs in modeling long - range dependencies .","convolutional models
i.e.
Multiwindow CNN
Hierarchical CNN
ReSAN
significantly outperforms
by
3.1 % and 2.4 %","significantly outperforms||by||3.1 % and 2.4 %
convolutional models||i.e.||Multiwindow CNN
convolutional models||i.e.||Hierarchical CNN
","convolutional models||has||ReSAN
ReSAN||has||significantly outperforms
",,,,,,"Results||Compared to||convolutional models
",
experiments,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN uses a similar number of parameters with better test performance and less time cost .","attention - based models
multi-head attention
DiSAN
ReSAN
uses
similar number of parameters
with
better test performance
less time cost","ReSAN||with||better test performance
ReSAN||with||less time cost
ReSAN||uses||similar number of parameters
","attention - based models||name||multi-head attention
attention - based models||name||DiSAN
attention - based models||has||ReSAN
",,,,,,"Results||Compared to||attention - based models
",
ablation-analysis,"In terms of prediction quality , the results show that 1 ) the unselected head tokens do contribute to the prediction , bringing 0.2 % improvement ; 2 ) using separate RSS modules to select the head and dependent tokens improves accuracy by 0.5 % ; and 3 ) hard attention and soft self - attention modules improve the accuracy by 0.3 % and 2.9 % respectively .","In terms of
prediction quality
unselected head tokens
contribute to
prediction
bringing
0.2 % improvement
using
separate RSS modules
to select
head and dependent tokens
improves
accuracy
by
0.5 %
hard attention and soft self - attention modules
improve
accuracy
by
0.3 % and 2.9 %","prediction quality||using||hard attention and soft self - attention modules
hard attention and soft self - attention modules||improve||accuracy
accuracy||by||0.3 % and 2.9 %
prediction quality||using||separate RSS modules
separate RSS modules||improves||accuracy
accuracy||by||0.5 %
separate RSS modules||to select||head and dependent tokens
unselected head tokens||contribute to||prediction
unselected head tokens||bringing||0.2 % improvement
","prediction quality||has||unselected head tokens
","Ablation analysis||In terms of||prediction quality
",,,,,,
research-problem,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,"Natural Language Inference ( NLI ) , also known as Recognizing Textual Entailment ( RTE ) , is one of the most important problems in natural language processing .","Natural Language Inference ( NLI )
Recognizing Textual Entailment ( RTE )",,,,,"Contribution||has research problem||Natural Language Inference ( NLI )
Contribution||has research problem||Recognizing Textual Entailment ( RTE )
",,,,
research-problem,"While current approaches mostly focus on the interaction architectures of the sentences , in this paper , we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model .",NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .","propose
Discourse Marker Augmented Network
for
natural language inference
where
transfer
knowledge
from
existing supervised task
Discourse Marker Prediction ( DMP )
to
integrated NLI model","Discourse Marker Augmented Network||for||natural language inference
Discourse Marker Augmented Network||where||transfer
knowledge||from||existing supervised task
existing supervised task||to||integrated NLI model
","transfer||has||knowledge
existing supervised task||name||Discourse Marker Prediction ( DMP )
","Model||propose||Discourse Marker Augmented Network
",,,,,,
model,We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network .,"sentence encoder model
that learns
representations
of
sentences
from
DMP task
inject
encoder
to
NLI network","sentence encoder model||that learns||representations
representations||of||sentences
sentences||from||DMP task
representations||inject||encoder
encoder||to||NLI network
",,,"Model||propose||sentence encoder model
",,,,,
model,"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the uniformity extent of the original labels to train the model .","of
employ
reinforcement learning
with
reward
defined by
uniformity extent
original labels
to train
model","reinforcement learning||with||reward
reward||defined by||uniformity extent
uniformity extent||of||original labels
uniformity extent||to train||model
",,"Model||employ||reinforcement learning
",,,,,,
experimental-setup,We use the Stanford CoreNLP toolkit to tokenize the words and generate POS and NER tags .,"use
Stanford CoreNLP toolkit
to tokenize
words
generate
POS and NER tags","Stanford CoreNLP toolkit||to tokenize||words
Stanford CoreNLP toolkit||generate||POS and NER tags
",,"Experimental setup||use||Stanford CoreNLP toolkit
",,,,,,
experimental-setup,"The word embeddings are initialized by 300d Glove , the dimensions of POS and NER embeddings are 30 and 10 .","word embeddings
are
initialized by
300d Glove
dimensions
of
POS and NER embeddings
30 and 10","word embeddings||initialized by||300d Glove
dimensions||of||POS and NER embeddings
POS and NER embeddings||are||30 and 10
",,,"Experimental setup||has||word embeddings
Experimental setup||has||dimensions
",,,,,
experimental-setup,We apply Tensorflow r 1.3 as our neural network framework .,"Tensorflow r 1.3
as
our neural network framework","Tensorflow r 1.3||as||our neural network framework
",,,"Experimental setup||apply||Tensorflow r 1.3
",,,,,
experimental-setup,"We set the hidden size as 300 for all the LSTM layers and apply dropout between layers with an initial ratio of 0.9 , the decay rate as 0.97 for every 5000 step .","hidden size
as
300
for
all the LSTM layers
apply
dropout
between
layers
with
initial ratio
of
0.9
decay rate
as
0.97
for
every 5000 step","hidden size||as||300
300||for||all the LSTM layers
decay rate||as||0.97
0.97||for||every 5000 step
dropout||with||initial ratio
initial ratio||of||0.9
dropout||between||layers
",,"Experimental setup||apply||decay rate
Experimental setup||apply||dropout
","Experimental setup||set||hidden size
",,,,,
experimental-setup,We use the AdaDelta for optimization as described in with ? as 0.95 and as 1 e - 8 .,"AdaDelta
for
optimization
with
? as 0.95 and as 1 e - 8","AdaDelta||with||? as 0.95 and as 1 e - 8
AdaDelta||for||optimization
",,,"Experimental setup||use||AdaDelta
",,,,,
experimental-setup,We set our batch size as 36 and the initial learning rate as 0.6 .,"set
our batch size
as
36
initial learning rate
as
0.6","our batch size||as||36
initial learning rate||as||0.6
",,"Experimental setup||set||our batch size
Experimental setup||set||initial learning rate
",,,,,,
experimental-setup,"For DMP task , we use stochastic gradient descent with initial learning rate as 0.1 , and we anneal by half each time the validation accuracy is lower than the previous epoch .","For
DMP task
use
stochastic gradient descent
with
initial learning rate
as
0.1
anneal
by
half
each time
validation accuracy
is
lower
than
previous epoch","DMP task||use||stochastic gradient descent
stochastic gradient descent||with||initial learning rate
initial learning rate||as||0.1
anneal||by||half
validation accuracy||is||lower
lower||than||previous epoch
","DMP task||has||anneal
anneal||has||each time
each time||has||validation accuracy
",,,,,,,
experimental-setup,"The number of epochs is set to be 10 , and the feedforward dropout rate is 0.2 .","number of epochs
is
set to
10
feedforward dropout rate
0.2","number of epochs||set to||10
feedforward dropout rate||is||0.2
",,,,,,,"DMP task||has||number of epochs
DMP task||has||feedforward dropout rate
",
results,"Obviously , the performance of most of the integrated methods are better than the sentence encoding based models above .","performance
of
most of the integrated methods
are
better
than
sentence encoding based models","performance||of||most of the integrated methods
most of the integrated methods||are||better
better||than||sentence encoding based models
",,,"Results||has||performance
",,,,,
results,"The performance of our model achieves 89.6 % on SNLI , 80.3 % on matched MultiNLI and 79.4 % on mismatched MultiNLI , which are all state - of - the - art results .","our model
achieves
89.6 %
on
SNLI
80.3 %
on
matched MultiNLI
79.4 %
on
mismatched MultiNLI
all state - of - the - art results","our model||achieves||89.6 %
89.6 %||on||SNLI
our model||achieves||80.3 %
80.3 %||on||matched MultiNLI
our model||achieves||79.4 %
79.4 %||on||mismatched MultiNLI
our model||achieves||all state - of - the - art results
",,,,,,,"performance||of||our model
",
ablation-analysis,"As shown in , we conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model .","in
conduct
an ablation experiment
on
SNLI development dataset","an ablation experiment||on||SNLI development dataset
",,"Ablation analysis||conduct||an ablation experiment
",,,,,,"SNLI development dataset||has||result
"
ablation-analysis,"The result is obviously not satisfactory , which indicates that only using sentence embedding from discourse markers to predict the answer is not ideal in large - scale datasets .","result
is
not satisfactory
only using
sentence embedding
from
discourse markers
to predict
answer
is
not ideal
large - scale datasets","result||only using||sentence embedding
sentence embedding||from||discourse markers
sentence embedding||is||not ideal
sentence embedding||to predict||answer
result||is||not satisfactory
","not ideal||in||large - scale datasets
",,,,,,,
ablation-analysis,"We then remove the sentence encoder model , which means we do n't use the knowledge transferred from the DMP task and thus the representations r p and r hare set to be zero vectors in the equation ( 6 ) and the equation .","remove
sentence encoder model
to",,,,,,"SNLI development dataset||remove||sentence encoder model
",,,
ablation-analysis,"We observe that the performance drops significantly to 87 . 24 % , which is nearly 1.5 % to our DMAN model , which indicates that the discourse markers have deep connections with the logical relations between two sentences they links .","observe that
performance
drops significantly
87 . 24 %",,"performance||has||drops significantly
drops significantly||to||87 . 24 %
",,,,"sentence encoder model||observe that||performance
",,,
ablation-analysis,"we remove the character - level embedding and the POS and NER features , the performance drops a lot .","remove
character - level embedding
POS and NER features
performance
drops a lot","drops a lot||remove||character - level embedding
drops a lot||remove||POS and NER features
","performance||has||drops a lot
",,,,,,"SNLI development dataset||has||performance
",
ablation-analysis,The exact match feature also demonstrates its effectiveness in the ablation result .,"exact match feature
demonstrates
effectiveness
in
ablation result","exact match feature||demonstrates||effectiveness
effectiveness||in||ablation result
",,,,,,,"SNLI development dataset||has||exact match feature
",
ablation-analysis,"Finally , we ablate the reinforcement learning part , in other words , we only use the original loss function to optimize the model ( set ? = 1 ) .","ablate
reinforcement learning part",,,,,,"SNLI development dataset||ablate||reinforcement learning part
",,,"reinforcement learning part||has||result
"
ablation-analysis,"The result drops about 0.5 % , which proves that it is helpful to utilize all the information from the annotators .","result
drops about 0.5 %",,"result||has||drops about 0.5 %
",,,,,,,
research-problem,Directly reading documents and being able to answer questions from them is an unsolved challenge .,Directly reading documents,,,,,"Contribution||has research problem||Directly reading documents
",,,,
research-problem,"To avoid its inherent difficulty , question answering ( QA ) has been directed towards using Knowledge Bases ( KBs ) instead , which has proven effective .",question answering ( QA ),,,,,"Contribution||has research problem||question answering ( QA )
",,,,
research-problem,"To compare using KBs , information extraction or Wikipedia documents directly in a single framework we construct an analysis tool , WIKIMOVIES , a QA dataset that contains raw text alongside a preprocessed KB , in the domain of movies .",QA,,,,,"Contribution||has research problem||QA
",,,,
model,"In this work we propose the Key - Value Memory Network ( KV - MemNN ) , a new neural network architecture that generalizes the original Memory Network and can work with either knowledge source .","propose
Key - Value Memory Network ( KV - MemNN )
new neural network architecture
generalizes
original Memory Network","new neural network architecture||generalizes||original Memory Network
","Key - Value Memory Network ( KV - MemNN )||has||new neural network architecture
","Model||propose||Key - Value Memory Network ( KV - MemNN )
",,,,,,
model,The KV - MemNN performs QA by first storing facts in a key - value structured memory before reasoning on them in order to predict an answer .,"KV - MemNN
performs
QA
storing
facts
in
value structured memory","KV - MemNN||performs||QA
QA||storing||facts
facts||in||value structured memory
",,,"Model||has||KV - MemNN
",,,,,
model,"The memory is designed so that the model learns to use keys to address relevant memories with respect to the question , whose corresponding values are subsequently returned .","memory
is
designed
to use
keys
to address
relevant memories
with respect to
question","memory||is||designed
designed||to use||keys
keys||to address||relevant memories
relevant memories||with respect to||question
",,,"Model||has||memory
",,,,,
model,"This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values , while still being trained using standard backpropagation via stochastic gradient descent .","encode
prior knowledge
to leverage
complex transforms
between
keys and values","complex transforms||between||keys and values
",,"Model||encode||prior knowledge
Model||to leverage||complex transforms
",,,,,,
results,WikiMovies,WikiMovies,,,,"Results||has||WikiMovies
",,,,,"WikiMovies||has||Key - Value Memory Networks
"
results,"However , Key - Value Memory Networks outperform all other methods on all three data source types .","Key - Value Memory Networks
outperform
all other methods",,"Key - Value Memory Networks||has||outperform
outperform||has||all other methods
Key - Value Memory Networks||has||outperform
",,,,,,,
results,"Reading from Wikipedia documents directly ( Doc ) outperforms an IE - based KB ( IE ) , which is an encouraging result towards automated machine reading though a gap to a humanannotated KB still remains ( 93.9 vs. 76.2 ) .","Reading
from
Wikipedia documents
outperforms
IE - based KB ( IE )","Reading||from||Wikipedia documents
","Wikipedia documents||has||outperforms
outperforms||has||IE - based KB ( IE )
",,,,,,"WikiMovies||has||Reading
",
results,WikiQA,WikiQA,,,,"Results||has||WikiQA
",,,,,"WikiQA||has||Key - Value Memory Networks
"
results,"Key - Value Memory Networks outperform a large set of other methods , although the results of the L.D.C. method of are very similar .","Key - Value Memory Networks
outperform
large set of other methods",,"outperform||has||large set of other methods
",,,,,,,
research-problem,"Recognizing textual entailment ( RTE ) is the task of determining whether two natural language sentences are ( i ) contradicting each other , ( ii ) not related , or whether ( iii ) the first sentence ( called premise ) entails the second sentence ( called hypothesis ) .",Recognizing textual entailment ( RTE ),,,,,"Contribution||has research problem||Recognizing textual entailment ( RTE )
",,,,
research-problem,"This task is important since many natural language processing ( NLP ) problems , such as information extraction , relation extraction , text summarization or machine translation , rely on it explicitly or implicitly and could benefit from more accurate RTE systems .",RTE,,,,,"Contribution||has research problem||RTE
",,,,
model,"In contrast , we are proposing an attentive neural network that is capable of reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned on the premise .","proposing
attentive neural network
capable of
reasoning
over
entailments
of
pairs
of
words and phrases
by processing
hypothesis
conditioned on
premise","attentive neural network||capable of||reasoning
reasoning||over||entailments
entailments||of||pairs
pairs||of||words and phrases
entailments||by processing||hypothesis
hypothesis||conditioned on||premise
reasoning||over||entailments
entailments||of||pairs
pairs||of||words and phrases
",,"Model||proposing||attentive neural network
",,,,,,
model,"Our contributions are threefold : ( i ) We present a neural model based on LSTMs that reads two sentences in one go to determine entailment , as opposed to mapping each sentence independently into a semantic space ( 2.2 ) , ( ii ) We extend this model with a neural word - by - word attention mechanism to encourage reasoning over entailments of pairs of words and phrases ( 2.4 ) , and ( iii ) We provide a detailed qualitative analysis of neural attention for RTE ( 4.1 ) .","present
neural model
based on
LSTMs
reads
two sentences
in
one go
extend
neural word - by - word attention mechanism
to encourage
reasoning
over
entailments
of
pairs
of
words and phrases","neural word - by - word attention mechanism||to encourage||reasoning
neural model||based on||LSTMs
neural model||reads||two sentences
two sentences||in||one go
",,"Model||extend||neural word - by - word attention mechanism
Model||present||neural model
",,,,,,
results,We found that processing the hypothesis conditioned on the premise instead of encoding each sentence independently gives an improvement of 3.3 percentage points in accuracy over Bowman et al. 's LSTM .,"processing
hypothesis
conditioned on
premise
of
gives
improvement
3.3 percentage points
in
accuracy
over
Bowman et al. 's LSTM","hypothesis||conditioned on||premise
hypothesis||gives||improvement
improvement||of||3.3 percentage points
3.3 percentage points||over||Bowman et al. 's LSTM
3.3 percentage points||in||accuracy
",,"Results||processing||hypothesis
",,,,,,
results,Our LSTM outperforms a simple lexicalized classifier by 2.7 percentage points .,"Our LSTM
outperforms
simple lexicalized classifier
by
2.7 percentage points","outperforms||by||2.7 percentage points
","Our LSTM||has||outperforms
outperforms||has||simple lexicalized classifier
",,"Results||has||Our LSTM
",,,,,
results,"By incorporating an attention mechanism we found a 0.9 percentage point improvement over a single LSTM with a hidden size of 159 , and a 1.4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding ( one for the premise and one for the hypothesis conditioned on the representation of the premise ) .","incorporating
attention mechanism
found
0.9 percentage point
improvement
over
single LSTM
1.4 percentage point
increase
over
benchmark model
uses
two LSTMs
for
conditional encoding","attention mechanism||found||0.9 percentage point
improvement||over||single LSTM
attention mechanism||found||1.4 percentage point
increase||over||benchmark model
benchmark model||uses||two LSTMs
two LSTMs||for||conditional encoding
","0.9 percentage point||has||improvement
1.4 percentage point||has||increase
","Results||incorporating||attention mechanism
",,,,,,
results,Enabling the model to attend over output vectors of the premise for every word in the hypothesis yields another 1.2 percentage point improvement compared to attending based only on the last output vector of the premise .,"Enabling
model
attend
output vectors
of
premise
for
every word
in
hypothesis
yields
another 1.2 percentage point
improvement
compared to
attending
based only on
last output vector
of
premise","model||attend||output vectors
output vectors||of||premise
output vectors||yields||another 1.2 percentage point
improvement||compared to||attending
attending||based only on||last output vector
last output vector||of||premise
output vectors||for||every word
every word||in||hypothesis
","another 1.2 percentage point||has||improvement
","Results||Enabling||model
",,,,,,
results,Allowing the model to also attend over the hypothesis based on the premise does not seem to improve performance for RTE .,"Allowing
model
attend
hypothesis
based on
premise
not
improve
performance
for
RTE","model||attend||hypothesis
hypothesis||not||improve
performance||for||RTE
hypothesis||based on||premise
","improve||has||performance
","Results||Allowing||model
",,,,,,
research-problem,Making Neural QA as Simple as Possible but not Simpler,Neural QA,,,,,"Contribution||has research problem||Neural QA
",,,,
research-problem,Recent development of large - scale question answering ( QA ) datasets triggered a substantial amount of research into end - toend neural architectures for QA .,"question answering ( QA )
QA",,,,,"Contribution||has research problem||question answering ( QA )
Contribution||has research problem||QA
",,,,
research-problem,Question answering is an important end - user task at the intersection of natural language processing ( NLP ) and information retrieval ( IR ) .,Question answering,,,,,"Contribution||has research problem||Question answering
",,,,
model,"In particular , we develop a simple neural , bag - of - words ( BoW ) - and a recurrent neural network ( RNN ) baseline , namely FastQA .","develop
neural , bag - of - words ( BoW )
recurrent neural network ( RNN )
namely
FastQA","FastQA||develop||neural , bag - of - words ( BoW )
FastQA||develop||recurrent neural network ( RNN )
",,"Model||namely||FastQA
",,,,,,
model,"Crucially , both models do not make use of a complex interaction layer but model interaction between question and context only through computable features on the word level .","interaction
model
between
question and context
through
computable features
on
word level","interaction||between||question and context
question and context||through||computable features
computable features||on||word level
",,"Model||model||interaction
",,,,,,
experimental-setup,FastQA,FastQA,,,,"Experimental setup||has||FastQA
",,,,,
experimental-setup,BoW Model,BoW Model,,,,"Experimental setup||has||BoW Model
",,,,,
experimental-setup,The BoW model is trained on spans up to length 10 to keep the computation tractable .,"trained on
spans
up to length
10","spans||up to length||10
",,,,,"BoW Model||trained on||spans
",,,
experimental-setup,As pre-processing steps we lowercase all inputs and tokenize it using spacy 4 .,"As
pre-processing steps
lowercase
all inputs
tokenize
using
spacy","tokenize||using||spacy
","pre-processing steps||has||lowercase
lowercase||has||all inputs
pre-processing steps||has||tokenize
",,,,"BoW Model||As||pre-processing steps
",,,
experimental-setup,The binary word in question feature is computed on lemmas provided by spacy and restricted to alphanumeric words that are not stopwords .,"binary word
in
question feature
computed on
lemmas
provided by
spacy
restricted to
alphanumeric words
are not
stopwords","binary word||in||question feature
binary word||computed on||lemmas
lemmas||provided by||spacy
lemmas||restricted to||alphanumeric words
alphanumeric words||are not||stopwords
binary word||in||question feature
",,,,,,,"BoW Model||has||binary word
",
experimental-setup,"Throughout all experiments we use a hidden dimensionality of n = 150 , dropout at the input embeddings with the same mask for all words and a rate of 0.2 and 300 - dimensional fixed word - embeddings from Glove .","use
hidden dimensionality
of
n
=
150
dropout
at
input embeddings
with
same mask
for
all words
rate
of
0.2
300 - dimensional
fixed word - embeddings
from
Glove","hidden dimensionality||of||fixed word - embeddings
300 - dimensional||from||Glove
hidden dimensionality||of||dropout
dropout||with||same mask
same mask||for||all words
dropout||at||input embeddings
rate||of||0.2
hidden dimensionality||of||n
n||=||150
hidden dimensionality||of||n
same mask||for||all words
","fixed word - embeddings||has||300 - dimensional
dropout||has||rate
",,,,"BoW Model||use||hidden dimensionality
",,,
experimental-setup,We employed ADAM for optimization with an initial learning - rate of 10 ?3 which was halved whenever the F 1 measure on the development set dropped between epochs .,"employed
ADAM
for
optimization
with
initial learning - rate
of
10 ?3","ADAM||with||initial learning - rate
initial learning - rate||of||10 ?3
ADAM||for||optimization
ADAM||with||initial learning - rate
initial learning - rate||of||10 ?3
ADAM||for||optimization
",,,,,"BoW Model||employed||ADAM
",,,
experimental-setup,We used mini-batches of size 32 .,"used
mini-batches
of
size
32","mini-batches||of||size
","size||has||32
",,,,"BoW Model||used||mini-batches
",,,
experimental-setup,FastQA,,,,,,,,,,
experimental-setup,We tokenize the input on whitespaces ( exclusive ) and non-alphanumeric characters ( inclusive ) .,"tokenize
input
on
whitespaces
non-alphanumeric characters","input||on||whitespaces
input||on||non-alphanumeric characters
",,,,,"FastQA||tokenize||input
",,,
experimental-setup,The binary word in question feature is computed on the words as they appear in context .,"binary word
in
question feature
computed on
words
appear in
context","binary word||computed on||words
words||appear in||context
",,,,,,,"FastQA||has||binary word
",
experimental-setup,"Throughout all experiments we use a hidden dimensionality of n = 300 , variational dropout at the input embeddings with the same mask for all words and a rate of 0.5 and 300 dimensional fixed word - embeddings from Glove .","use
hidden dimensionality
of
n
=
300
variational dropout
at
input embeddings
with
same mask
for
all words
rate
of
0.5
300 dimensional
fixed word - embeddings
from
Glove","n||=||300
variational dropout||with||same mask
variational dropout||at||input embeddings
rate||of||0.5
300 dimensional||from||Glove
","variational dropout||has||rate
variational dropout||has||fixed word - embeddings
fixed word - embeddings||has||300 dimensional
",,,,"FastQA||use||hidden dimensionality
FastQA||use||variational dropout
",,,
experimental-setup,We employed ADAM for optimization with an initial learning - rate of 10 ?3 which was halved whenever the F 1 measure on the development set dropped between checkpoints .,"employed
ADAM
for
optimization
with
initial learning - rate
of
10 ?3",,,,,,"FastQA||employed||ADAM
",,,
results,Our neural BoW baseline achieves good results on both datasets ( Tables 3 and 1 ) 5 .,"neural BoW baseline
achieves
good results","neural BoW baseline||achieves||good results
",,,"Results||has||neural BoW baseline
",,,,,
results,"For instance , it outperforms a feature rich logistic - regression baseline on the SQuAD development set and nearly reaches the BiLSTM baseline system ( i.e. , FastQA without character embeddings and features ) .","outperforms
feature rich logistic - regression baseline
on
SQuAD development set
nearly reaches
BiLSTM baseline system","feature rich logistic - regression baseline||on||SQuAD development set
","nearly reaches||has||BiLSTM baseline system
outperforms||has||feature rich logistic - regression baseline
",,"Results||has||nearly reaches
Results||has||outperforms
",,,,,
results,It is very competitive to previously established stateof - the - art results on the two datasets and even improves those for News QA .,"very competitive
to
previously established stateof - the - art results
improves
for
News QA","improves||for||News QA
very competitive||to||previously established stateof - the - art results
",,,"Results||has||improves
Results||has||very competitive
",,,,,
research-problem,"Compare , Compress and Propagate : Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,This paper presents a new deep learning architecture for Natural Language Inference ( NLI ) .,Natural Language Inference ( NLI ),,,,,"Contribution||has research problem||Natural Language Inference ( NLI )
",,,,
research-problem,"More concretely , given a premise and hypothesis , NLI aims to detect whether the latter entails or contradicts the former .",NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,This paper presents a new neural model for NLI .,,,,,,,,,,
model,There are several new novel components in our work .,,,,,,,,,,
model,"Firstly , we propose a compare , compress and propagate ( Com Prop ) architecture where compressed alignment features are propagated to upper layers ( such as a RNN - based encoder ) for enhancing representation learning .","propose
compare , compress and propagate ( Com Prop ) architecture
compressed alignment features
propagated to
upper layers
such as
RNN - based encoder
for enhancing
representation learning","compressed alignment features||for enhancing||representation learning
compressed alignment features||propagated to||upper layers
upper layers||such as||RNN - based encoder
","compare , compress and propagate ( Com Prop ) architecture||has||compressed alignment features
","Model||propose||compare , compress and propagate ( Com Prop ) architecture
",,,,,,
model,"Secondly , in order to achieve an efficient propagation of alignment features , we propose alignment factorization layers to reduce each alignment vector to a single scalar valued feature .","to
achieve
efficient propagation
of
alignment features
propose
alignment factorization layers
to reduce
each alignment vector
single scalar valued feature","efficient propagation||of||alignment features
efficient propagation||propose||alignment factorization layers
alignment factorization layers||to reduce||each alignment vector
each alignment vector||to||single scalar valued feature
",,"Model||achieve||efficient propagation
",,,,,,
model,"Each scalar valued feature is used to augment the base word representation , allowing the subsequent RNN encoder layers to benefit from not only global but also cross sentence information .","scalar valued feature
used to
augment
base word representation","scalar valued feature||used to||augment
","augment||has||base word representation
",,"Model||has||scalar valued feature
",,,,,
experimental-setup,We implement our model in TensorFlow and train them on Nvidia P100 GPUs .,"implement
our model
in
TensorFlow
train them on
Nvidia P100 GPUs","our model||in||TensorFlow
",,"Experimental setup||train them on||Nvidia P100 GPUs
Experimental setup||implement||our model
",,,,,,
experimental-setup,"We use the Adam optimizer ( Kingma and Ba , 2014 ) with an initial learning rate of 0.0003 .","use
Adam optimizer
with
initial learning rate
of
0.0003","Adam optimizer||with||initial learning rate
initial learning rate||of||0.0003
",,"Experimental setup||use||Adam optimizer
",,,,,,
experimental-setup,L2 regularization is set to 10 ?6 .,"L2 regularization
set to
10 ?6","L2 regularization||set to||10 ?6
",,,"Experimental setup||has||L2 regularization
",,,,,
experimental-setup,"Dropout with a keep probability of 0.8 is applied after each fullyconnected , recurrent or highway layer .","Dropout
with
keep probability
of
0.8
applied after
each fullyconnected , recurrent or highway layer","Dropout||with||keep probability
keep probability||of||0.8
keep probability||applied after||each fullyconnected , recurrent or highway layer
",,,"Experimental setup||has||Dropout
",,,,,
experimental-setup,"The batch size is tuned amongst { 128 , 256 , 512 } .","batch size
tuned amongst
{ 128 , 256 , 512 }","batch size||tuned amongst||{ 128 , 256 , 512 }
",,,"Experimental setup||has||batch size
",,,,,
experimental-setup,"The number of latent factors k for the factorization layer is tuned amongst { 5 , 10 , 50 , 100 , 150 } .","number of latent factors k
for
factorization layer
tuned amongst
{ 5 , 10 , 50 , 100 , 150 }","number of latent factors k||for||factorization layer
factorization layer||tuned amongst||{ 5 , 10 , 50 , 100 , 150 }
",,,"Experimental setup||has||number of latent factors k
",,,,,
experimental-setup,The size of the hidden layers of the highway network layers are set to 300 .,"size
of
hidden layers
of
highway network layers
set to
300","size||of||hidden layers
hidden layers||of||highway network layers
highway network layers||set to||300
",,,"Experimental setup||has||size
",,,,,
experimental-setup,All parameters are initialized with xavier initialization .,"parameters
initialized with
xavier initialization","parameters||initialized with||xavier initialization
",,,"Experimental setup||has||parameters
",,,,,
experimental-setup,Word embeddings are preloaded with 300d Glo Ve embeddings and fixed during training .,"Word embeddings
preloaded with
300d Glo Ve embeddings
fixed during
training","Word embeddings||fixed during||training
Word embeddings||preloaded with||300d Glo Ve embeddings
",,,"Experimental setup||has||Word embeddings
",,,,,
experimental-setup,Sequence lengths are padded to batch - wise maximum .,"Sequence lengths
padded to
batch - wise maximum","Sequence lengths||padded to||batch - wise maximum
",,,"Experimental setup||has||Sequence lengths
",,,,,
experimental-setup,The batch order is ( randomly ) sorted within buckets following .,"batch order
( randomly ) sorted within
buckets","batch order||( randomly ) sorted within||buckets
",,,"Experimental setup||has||batch order
",,,,,
results,Table 1 reports our results on the SNLI benchmark .,"on
SNLI benchmark",,,"Results||on||SNLI benchmark
",,,,,,
results,"On the cross sentence ( single model setting ) , the performance of our proposed CAFE model is extremely competitive .","On
cross sentence ( single model setting )
performance
of
proposed CAFE model
is
extremely competitive","performance||of||proposed CAFE model
performance||is||extremely competitive
","cross sentence ( single model setting )||has||performance
",,,,"SNLI benchmark||On||cross sentence ( single model setting )
",,,
results,CAFE obtains,"CAFE
obtains",,,,,,,"CAFE||obtains||88.5 % accuracy
","SNLI benchmark||has||CAFE
",
results,"88.5 % accuracy on the SNLI test set , an extremely competitive score on the extremely popular benchmark .","88.5 % accuracy
on
SNLI test set
extremely competitive score","88.5 % accuracy||on||SNLI test set
","88.5 % accuracy||has||extremely competitive score
",,,,,,,
results,"For example , CAFE also achieves 88.3 % and 88.1 % test accuracy with only 3.5 M and 1.5 M parameters","achieves
88.3 % and 88.1 % test accuracy
with
only 3.5 M and 1.5 M parameters","88.3 % and 88.1 % test accuracy||with||only 3.5 M and 1.5 M parameters
",,,,,"CAFE||achieves||88.3 % and 88.1 % test accuracy
",,,
results,"Due to resource constraints , we did not train CAFE + ELMo ensembles but a single run ( and single model ) of CAFE + ELMo already achieves 89.0 score on SNLI .","CAFE + ELMo
achieves
89.0 score
on
SNLI","CAFE + ELMo||achieves||89.0 score
89.0 score||on||SNLI
",,,,,,,"SNLI benchmark||has||CAFE + ELMo
",
results,This outperforms the state - of - theart ESIM and DIIN models with only a fraction of the parameter cost .,"outperforms
state - of - theart ESIM and DIIN models
with
fraction
of
parameter cost","state - of - theart ESIM and DIIN models||with||fraction
fraction||of||parameter cost
","outperforms||has||state - of - theart ESIM and DIIN models
",,,,,,"CAFE + ELMo||has||outperforms
",
results,"Moreover , our lightweight adaptation achieves 87.7 % with only 750K parameters , which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model ( 86.8 % ) .","lightweight adaptation
achieves
87.7 %","lightweight adaptation||achieves||87.7 %
",,,,,,,"CAFE + ELMo||has||lightweight adaptation
",
results,"Finally , an ensemble of 5 CAFE models achieves 89.3 % test accuracy , the best test scores on the SNLI benchmark to date 3 .","ensemble
of
5 CAFE models
achieves
89.3 % test accuracy
best test scores","ensemble||of||5 CAFE models
5 CAFE models||achieves||89.3 % test accuracy
","89.3 % test accuracy||has||best test scores
",,,,,,"SNLI benchmark||has||ensemble
",
results,"On MultiNLI , CAFE significantly outperforms ESIM , a strong state - of - the - art model on both settings .","MultiNLI
CAFE
significantly outperforms
ESIM",,"MultiNLI||has||CAFE
CAFE||has||significantly outperforms
significantly outperforms||has||ESIM
",,"Results||on||MultiNLI
",,,,,
results,We also outperform the ESIM + Read model .,"outperform
ESIM + Read model",,"outperform||has||ESIM + Read model
",,,,,,"CAFE||has||outperform
",
results,An ensemble of CAFE models achieve competitive re-sult on the MultiNLI dataset .,"ensemble of CAFE models
achieve
competitive re-sult","ensemble of CAFE models||achieve||competitive re-sult
",,,,,,,"MultiNLI||has||ensemble of CAFE models
",
results,"On SciTail , our proposed CAFE model achieves state - of - the - art performance .","On
SciTail
proposed CAFE model
achieves
state - of - the - art performance","proposed CAFE model||achieves||state - of - the - art performance
","SciTail||has||proposed CAFE model
","Results||On||SciTail
",,,,,,
results,The performance gain over strong baselines such as DecompAtt and ESIM are ?,"performance gain
over
strong baselines
such as
DecompAtt
ESIM
are","performance gain||over||strong baselines
strong baselines||such as||DecompAtt
strong baselines||such as||ESIM
",,,,,,"performance gain||are||10 % ? 13 %
","SciTail||has||performance gain
",
results,10 % ? 13 % in terms of accuracy .,"10 % ? 13 %
in terms of
accuracy","10 % ? 13 %||in terms of||accuracy
",,,,,,,,
results,"CAFE also outperforms DGEM , which uses a graph - based attention for improved performance , by a significant margin of 5 % .","CAFE
outperforms
DGEM
by
significant margin
of
5 %","outperforms||by||significant margin
significant margin||of||5 %
","CAFE||has||outperforms
outperforms||has||DGEM
",,,,,,"SciTail||has||CAFE
",
ablation-analysis,The 1 - layer linear setting performs the best and is therefore reported in .,"1 - layer linear setting
performs
best","1 - layer linear setting||performs||best
",,,"Ablation analysis||has||1 - layer linear setting
",,,,,
ablation-analysis,Using ReLU seems to be worse than nonlinear FC layers .,"Using
ReLU
seems to be
worse
than
nonlinear FC layers","ReLU||seems to be||worse
worse||than||nonlinear FC layers
",,"Ablation analysis||Using||ReLU
",,,,,,
ablation-analysis,"In , we explore the utility of using character and syntactic embeddings , which we found to have helped CAFE marginally .","explore
utility
of using
character and syntactic embeddings
found to
helped
CAFE","utility||of using||character and syntactic embeddings
character and syntactic embeddings||found to||helped
","helped||has||CAFE
","Ablation analysis||explore||utility
",,,,,,
ablation-analysis,"In ( 4 ) , we remove the inter-attention alignment features , which naturally impact the model performance significantly .","remove
inter-attention alignment features
naturally impact
model performance
significantly",,"inter-attention alignment features||has||naturally impact
naturally impact||has||model performance
model performance||has||significantly
","Ablation analysis||remove||inter-attention alignment features
",,,,,,
ablation-analysis,We observe that both highway layers have marginally helped the over all performance .,"observe
both highway layers
have
marginally helped
over all performance","both highway layers||have||marginally helped
","marginally helped||has||over all performance
","Ablation analysis||observe||both highway layers
",,,,,,
ablation-analysis,We observe that the Sub and Concat compositions were more important than the Mul composition .,"Sub and Concat compositions
were
more important
than
Mul composition","Sub and Concat compositions||were||more important
more important||than||Mul composition
",,,"Ablation analysis||observe||Sub and Concat compositions
",,,,,
ablation-analysis,"Finally , in ( 10 ) , we replace the LSTM encoder with a BiLSTM , observing that adding bi-directionality did not improve performance for our model .","replace
LSTM encoder
with
BiLSTM
observing
adding bi-directionality
did not improve
performance
for
our model","LSTM encoder||with||BiLSTM
LSTM encoder||observing||adding bi-directionality
performance||for||our model
","adding bi-directionality||has||did not improve
did not improve||has||performance
","Ablation analysis||replace||LSTM encoder
",,,,,,
research-problem,Distributed Representations of Sentences and Documents,Distributed Representations of Sentences and Documents,,,,,"Contribution||has research problem||Distributed Representations of Sentences and Documents
",,,,
model,"In this paper , we propose Paragraph Vector , an unsupervised framework that learns continuous distributed vector representations for pieces of texts .","propose
Paragraph Vector
unsupervised framework
that learns
continuous distributed vector representations
for
pieces of texts","unsupervised framework||that learns||continuous distributed vector representations
continuous distributed vector representations||for||pieces of texts
","Paragraph Vector||has||unsupervised framework
","Model||propose||Paragraph Vector
",,,,,,
model,"The name Paragraph Vector is to emphasize the fact that the method can be applied to variable - length pieces of texts , anything from a phrase or sentence to a large document .","Paragraph Vector
can be applied to
variable - length pieces of texts","Paragraph Vector||can be applied to||variable - length pieces of texts
",,,"Model||name||Paragraph Vector
",,,,,
model,"In our model , the vector representation is trained to be useful for predicting words in a paragraph .","vector representation
trained to be
useful
for predicting
words
in
paragraph","vector representation||trained to be||useful
useful||for predicting||words
words||in||paragraph
",,,"Model||has||vector representation
",,,,,
model,"More precisely , we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context .","concatenate
paragraph vector
with
several word vectors
from
paragraph
predict
following word
in
given context","paragraph vector||with||several word vectors
several word vectors||from||paragraph
paragraph vector||predict||following word
following word||in||given context
",,"Model||concatenate||paragraph vector
",,,,,,
model,Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation .,"Both word vectors and paragraph vectors
trained by
stochastic gradient descent
backpropagation","Both word vectors and paragraph vectors||trained by||stochastic gradient descent
Both word vectors and paragraph vectors||trained by||backpropagation
",,,"Model||has||Both word vectors and paragraph vectors
",,,,,
model,"While paragraph vectors are unique among paragraphs , the word vectors are shared .","paragraph vectors
are
unique
among
paragraphs
word vectors
are
shared","paragraph vectors||are||unique
unique||among||paragraphs
word vectors||are||shared
",,,"Model||has||paragraph vectors
Model||has||word vectors
",,,,,
model,"At prediction time , the paragraph vectors are inferred by fixing the word vectors and training the new paragraph vector until convergence .","At
prediction time
paragraph vectors
inferred by
fixing
word vectors
training
new paragraph vector
until
convergence","prediction time||training||new paragraph vector
new paragraph vector||until||convergence
paragraph vectors||inferred by||fixing
","prediction time||has||paragraph vectors
fixing||has||word vectors
","Model||At||prediction time
",,,,,,
experimental-setup,"We learn the word vectors and paragraph vectors using 75,000 training documents ( 25,000 labeled and 50,000 unlabeled instances ) .","learn
word vectors and paragraph vectors
using
75,000 training documents
25,000 labeled
50,000 unlabeled instances","word vectors and paragraph vectors||using||75,000 training documents
","75,000 training documents||has||25,000 labeled
75,000 training documents||has||50,000 unlabeled instances
","Experimental setup||learn||word vectors and paragraph vectors
",,,,,,
experimental-setup,"The paragraph vectors for the 25,000 labeled instances are then fed through a neural network with one hidden layer with 50 units and a logistic classifier to learn to predict the sentiment .","paragraph vectors
for
25,000 labeled instances
fed through
neural network
with
one hidden layer
with
50 units
logistic classifier
to predict
sentiment","paragraph vectors||fed through||neural network
neural network||with||one hidden layer
one hidden layer||with||50 units
paragraph vectors||fed through||logistic classifier
logistic classifier||to predict||sentiment
paragraph vectors||for||25,000 labeled instances
",,,"Experimental setup||has||paragraph vectors
",,,,,
experimental-setup,"In particular , we cross validate the window size , and the optimal window size is 10 words .","optimal window size
is
10 words","optimal window size||is||10 words
",,,"Experimental setup||has||optimal window size
",,,,,
experimental-setup,"The vector presented to the classifier is a concatenation of two vectors , one from PV - DBOW and one from PV - DM .","vector
presented to
classifier
concatenation of
PV - DBOW
PV - DM","vector||presented to||classifier
classifier||concatenation of||PV - DBOW
classifier||concatenation of||PV - DM
",,,"Experimental setup||has||vector
",,,,,"PV - DBOW||has||learned vector representations
PV - DM||has||learned vector representations
"
experimental-setup,"In PV - DBOW , the learned vector representations have 400 dimensions .","learned vector representations
have
400 dimensions","learned vector representations||have||400 dimensions
learned vector representations||have||400 dimensions
",,,,,,,,
experimental-setup,"In PV - DM , the learned vector representations have 400 dimensions for both words and documents .","learned vector representations
have
400 dimensions
for
words and documents","400 dimensions||for||words and documents
",,,,,,,,
experimental-setup,"To predict the 10 - th word , we concatenate the paragraph vectors and word vectors .","To predict
10 - th word
concatenate
paragraph vectors
word vectors","10 - th word||concatenate||paragraph vectors
10 - th word||concatenate||word vectors
",,"Experimental setup||To predict||10 - th word
",,,,,,
experimental-setup,"Special characters such as , .!?","Special characters
such as
, .!?","Special characters||such as||, .!?
",,,"Experimental setup||has||Special characters
",,,,,
experimental-setup,are treated as a normal word .,"treated as
normal word",,,,,,", .!?||treated as||normal word
",,,
results,"As can be seen from the for long documents , bag - of - words models perform quite well and it is difficult to improve upon them using word vectors .","for
long documents
bag - of - words models
perform
quite well
difficult to improve
using
word vectors","bag - of - words models||perform||quite well
difficult to improve||using||word vectors
","long documents||has||bag - of - words models
bag - of - words models||has||difficult to improve
","Results||for||long documents
",,,,,,
results,The combination of two models yields an improvement approximately 1.5 % in terms of error rates .,"combination of two models
yields
improvement
approximately 1.5 %
in terms of
error rates","combination of two models||yields||improvement
approximately 1.5 %||in terms of||error rates
","improvement||has||approximately 1.5 %
",,"Results||has||combination of two models
",,,,,
results,The method described in this paper is the only approach that goes significantly beyond the barrier of 10 % error rate .,"method described
is
only approach
goes
significantly beyond the barrier
of
10 % error rate","method described||is||only approach
only approach||goes||significantly beyond the barrier
significantly beyond the barrier||of||10 % error rate
",,,"Results||has||method described
",,,,,
results,It achieves 7.42 % which is another 1.3 % absolute improvement ( or 15 % relative improvement ) over the best previous result of ..,"achieves
7.42 %
which is
another 1.3 % absolute improvement
15 % relative improvement
over
best previous result","7.42 %||over||best previous result
7.42 %||which is||another 1.3 % absolute improvement
7.42 %||which is||15 % relative improvement
",,,,,"method described||achieves||7.42 %
",,,
research-problem,Deep Learning for Answer Sentence Selection,Answer Sentence Selection,,,,,"Contribution||has research problem||Answer Sentence Selection
",,,,
model,"In this paper , we show that a neural network - based sentence model can be applied to the task of answer sentence selection .","show
neural network - based sentence model
can be applied to
task of answer sentence selection","neural network - based sentence model||can be applied to||task of answer sentence selection
",,"Model||show||neural network - based sentence model
",,,,,,
model,"We construct two distributional sentence models ; first a bag - of - words model , and second , a bigram model based on a convolutional neural network .","construct
two distributional sentence models
first
bag - of - words model
second
bigram model
based on
convolutional neural network","two distributional sentence models||first||bag - of - words model
two distributional sentence models||second||bigram model
bigram model||based on||convolutional neural network
",,"Model||construct||two distributional sentence models
",,,,,,
model,"Assuming a set of pre-trained semantic word embeddings , we train a supervised model to learn a semantic matching between question and answer pairs .","Assuming a set of
pre-trained semantic word embeddings
train
supervised model
to learn
semantic matching
between
question and answer pairs","supervised model||to learn||semantic matching
semantic matching||Assuming a set of||pre-trained semantic word embeddings
semantic matching||between||question and answer pairs
",,"Model||train||supervised model
",,,,,,
model,"We also present an enhanced version of this model , which combines the signal of the distributed matching algorithm with two simple word matching features .","present
enhanced version
of
combines
signal
distributed matching algorithm
with
two simple word matching features","enhanced version||combines||signal
signal||of||distributed matching algorithm
distributed matching algorithm||with||two simple word matching features
",,"Model||present||enhanced version
",,,,,,
experimental-setup,We used word embeddings ( d = 50 ) that were computed using Collobert and Weston 's neural language model and provided by Turian et al ..,"used
word embeddings ( d = 50 )
computed using
Collobert and Weston 's neural language model","word embeddings ( d = 50 )||computed using||Collobert and Weston 's neural language model
",,"Experimental setup||used||word embeddings ( d = 50 )
",,,,,,
experimental-setup,"The other model weights were randomly intitialised using a Gaussian distribution ( = 0 , ? = 0.01 ) .","other model weights
randomly intitialised
using
Gaussian distribution ( = 0 , ? = 0.01 )","randomly intitialised||using||Gaussian distribution ( = 0 , ? = 0.01 )
","other model weights||has||randomly intitialised
",,"Experimental setup||has||other model weights
",,,,,
experimental-setup,All hyperparameters were optimised via grid search on the MAP score on the development data .,"All hyperparameters
optimised via
grid search
on
MAP score on the development data","All hyperparameters||optimised via||grid search
grid search||on||MAP score on the development data
",,,"Experimental setup||has||All hyperparameters
",,,,,
experimental-setup,We use the AdaGrad algorithm for training .,"use
AdaGrad algorithm
for
training","AdaGrad algorithm||for||training
",,"Experimental setup||use||AdaGrad algorithm
",,,,,,
experimental-setup,"L - BFGS was used to train the logistic regression classifier , with L2 regulariser of 0.01 .","L - BFGS
to train
logistic regression classifier
with
L2 regulariser
of
0.01","L - BFGS||to train||logistic regression classifier
logistic regression classifier||with||L2 regulariser
L2 regulariser||of||0.01
",,,"Experimental setup||has||L - BFGS
",,,,,
results,"As can be seen , the bigram model performs better than the unigram model and the addition of the IDF - weighted word count features significantly improve performance for both models by 10 % - 15 % .","bigram model
performs
better
than
unigram model
addition of
IDF - weighted word count features
significantly improve
performance
for
both models
by
10 % - 15 %","bigram model||performs||better
better||than||unigram model
bigram model||addition of||IDF - weighted word count features
significantly improve||by||10 % - 15 %
performance||for||both models
","IDF - weighted word count features||has||significantly improve
significantly improve||has||performance
",,"Results||has||bigram model
",,,,,
results,"As can be seen in , our best models ( bigram + count ) outperform all baselines and prior work on MAP and are very close to the best model proposed by Yih et al. on MRR .","best models ( bigram + count )
outperform
all baselines","best models ( bigram + count )||outperform||all baselines
",,,"Results||has||best models ( bigram + count )
",,,,,
research-problem,DR- BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference,Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,We present a novel deep learning architecture to address the natural language inference ( NLI ) task .,natural language inference ( NLI ),,,,,"Contribution||has research problem||natural language inference ( NLI )
",,,,
research-problem,"The goal of NLI is to identify the logical relationship ( entailment , neutral , or contradiction ) between a premise and a corresponding hypothesis .",NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,We propose a dependent reading bidirectional LSTM ( DR - BiLSTM ) model to address these limitations .,"propose
dependent reading bidirectional LSTM ( DR - BiLSTM ) model",,,"Model||propose||dependent reading bidirectional LSTM ( DR - BiLSTM ) model
",,,,,,
model,"Given a premise u and a hypothesis v , our model first encodes them considering dependency on each other .","premise u and a hypothesis v
first encodes
considering
dependency
on
each other","premise u and a hypothesis v||considering||dependency
dependency||on||each other
",,"Model||first encodes||premise u and a hypothesis v
",,,,,,
model,"Next , the model employs a soft attention mechanism to extract relevant information from these encodings .","employs
soft attention mechanism
to extract
relevant information
from
these encodings","soft attention mechanism||to extract||relevant information
relevant information||from||these encodings
",,"Model||employs||soft attention mechanism
",,,,,,
model,"The augmented sentence representations are then passed to the inference stage , which uses a similar dependent reading strategy in both directions , i.e. u ? v and v ? u .","augmented sentence representations
passed to
inference stage
uses
similar dependent reading strategy
in
both directions","augmented sentence representations||passed to||inference stage
inference stage||uses||similar dependent reading strategy
similar dependent reading strategy||in||both directions
",,,"Model||has||augmented sentence representations
",,,,,
model,"Finally , a decision is made through a multi - layer perceptron ( MLP ) based on the aggregated information .","decision
made through
multi - layer perceptron ( MLP )
based on
aggregated information","decision||made through||multi - layer perceptron ( MLP )
multi - layer perceptron ( MLP )||based on||aggregated information
",,,"Model||has||decision
",,,,,
hyperparameters,We use pre-trained 300 - D Glove 840B vectors to initialize our word embedding vectors .,"use
pre-trained 300 - D Glove 840B vectors
to initialize
our word embedding vectors","pre-trained 300 - D Glove 840B vectors||to initialize||our word embedding vectors
",,"Hyperparameters||use||pre-trained 300 - D Glove 840B vectors
",,,,,,
hyperparameters,All hidden states of BiLSTMs during input encoding and inference have 450 dimensions ( r = 300 and d = 450 ) .,"All hidden states
of
BiLSTMs
during
input encoding and inference
have
450 dimensions","All hidden states||of||BiLSTMs
BiLSTMs||during||input encoding and inference
input encoding and inference||have||450 dimensions
",,,"Hyperparameters||has||All hidden states
",,,,,
hyperparameters,"The weights are learned by minimizing the log - loss on the training data via the Adam optimizer ( Kingma and Ba , 2014 ) .","weights
learned by
minimizing
log - loss
on
training data
via
Adam optimizer","weights||learned by||minimizing
log - loss||on||training data
minimizing||via||Adam optimizer
","minimizing||has||log - loss
",,"Hyperparameters||has||weights
",,,,,
hyperparameters,The initial learning rate is 0.0004 .,"initial learning rate
is
0.0004","initial learning rate||is||0.0004
",,,"Hyperparameters||has||initial learning rate
",,,,,
hyperparameters,"To avoid overfitting , we use dropout with the rate of 0.4 for regularization , which is applied to all feedforward connections .","To avoid
overfitting
use
dropout
with
rate
of
0.4
for
regularization
applied to
all feedforward connections","overfitting||use||dropout
dropout||with||rate
rate||of||0.4
dropout||for||regularization
dropout||applied to||all feedforward connections
",,"Hyperparameters||To avoid||overfitting
",,,,,,
hyperparameters,"During training , the word embeddings are updated to learn effective representations for the NLI task .","During
training
word embeddings
updated to learn
effective representations
for
NLI task","word embeddings||updated to learn||effective representations
effective representations||for||NLI task
","training||has||word embeddings
","Hyperparameters||During||training
",,,,,,
hyperparameters,We use a fairly small batch size of 32 to provide more exploration power to the model .,"fairly small batch size
of
32","fairly small batch size||of||32
",,,"Hyperparameters||use||fairly small batch size
",,,,,
results,DR - BiLSTM ( Single ) achieves 88.5 % accuracy on the test set which is noticeably the best reported result among the existing single models for this task .,"DR - BiLSTM ( Single )
achieves
88.5 % accuracy
on
test set
is
best reported result
among
existing single models","DR - BiLSTM ( Single )||achieves||88.5 % accuracy
88.5 % accuracy||is||best reported result
best reported result||among||existing single models
88.5 % accuracy||on||test set
",,,"Results||has||DR - BiLSTM ( Single )
",,,,,
results,"DR - BiLSTM ( Ensemble ) achieves the accuracy of 89.3 % , the best result observed on SNLI , while DR - BiLSTM ( Single ) obtains the accuracy of 88.5 % , which considerably outperforms the previous non-ensemble models .","DR - BiLSTM ( Ensemble )
achieves
accuracy
of
89.3 %
best result
observed on
SNLI
obtains
accuracy
of
88.5 %
considerably outperforms
previous non-ensemble models","accuracy||of||88.5 %
DR - BiLSTM ( Ensemble )||achieves||accuracy
accuracy||of||89.3 %
best result||observed on||SNLI
","88.5 %||has||considerably outperforms
considerably outperforms||has||previous non-ensemble models
89.3 %||has||best result
",,"Results||has||DR - BiLSTM ( Ensemble )
",,"DR - BiLSTM ( Single )||obtains||accuracy
",,,
results,"Also , utilizing a trivial preprocessing step yields to further improvements of 0.4 % and 0.3 % for single and ensemble DR - BiLSTM models respectively .","utilizing
trivial preprocessing step
yields to
further improvements
of
0.4 % and 0.3 %
for
single and ensemble DR - BiLSTM models","trivial preprocessing step||yields to||further improvements
further improvements||for||single and ensemble DR - BiLSTM models
",,"Results||utilizing||trivial preprocessing step
",,,,,,
results,Our ensemble model considerably outperforms the current state - of - the - art by obtaining 89.3 % accuracy .,"ensemble model
considerably outperforms
current state - of - the - art
by obtaining
89.3 % accuracy","current state - of - the - art||by obtaining||89.3 % accuracy
","ensemble model||has||considerably outperforms
considerably outperforms||has||current state - of - the - art
",,"Results||has||ensemble model
",,,,,
results,We can see that our preprocessing mechanism leads to further improvements of 0.4 % and 0.3 % on the SNLI test set for our single and ensemble models respectively .,"preprocessing mechanism
leads to
further improvements
of
0.4 % and 0.3 %
on
SNLI test set
for
our single and ensemble models","further improvements||of||0.4 % and 0.3 %
preprocessing mechanism||leads to||further improvements
further improvements||of||0.4 % and 0.3 %
0.4 % and 0.3 %||for||our single and ensemble models
0.4 % and 0.3 %||on||SNLI test set
",,,"Results||has||preprocessing mechanism
",,,,,
results,"In fact , our single model ( "" DR - BiLSTM ( Single ) + Process "" ) obtains the state - of - the - art performance over both reported single and ensemble models by performing a simple preprocessing step .","our single model
DR - BiLSTM ( Single ) + Process
obtains
state - of - the - art performance
over
reported single and ensemble models
by performing
simple preprocessing step","our single model||obtains||state - of - the - art performance
state - of - the - art performance||over||reported single and ensemble models
reported single and ensemble models||by performing||simple preprocessing step
","our single model||name||DR - BiLSTM ( Single ) + Process
",,"Results||has||our single model
",,,,,
results,"Furthermore , "" DR - BiLSTM ( Ensem . ) + Process "" outperforms the existing state - of - the - art remarkably ( 0.7 % improvement ) .","DR - BiLSTM ( Ensem . ) + Process
outperforms
existing state - of - the - art
remarkably
0.7 % improvement",,"DR - BiLSTM ( Ensem . ) + Process||has||outperforms
outperforms||has||existing state - of - the - art
existing state - of - the - art||has||remarkably
existing state - of - the - art||has||0.7 % improvement
",,"Results||has||DR - BiLSTM ( Ensem . ) + Process
",,,,,
ablation-analysis,We can see that all modifications lead to a new model and their differences are statistically significant with a p-value of < 0.001 over Chi square test .,"see that
all modifications
lead to
new model","all modifications||lead to||new model
",,"Ablation analysis||see that||all modifications
",,,,,,
ablation-analysis,"Among all components , three of them have noticeable influences : max pooling , difference in the attention stage , and dependent reading .","Among
all components
three of them
have
noticeable influences
max pooling
difference in the attention stage
dependent reading","three of them||have||noticeable influences
","all components||has||three of them
three of them||has||max pooling
three of them||has||difference in the attention stage
three of them||has||dependent reading
","Ablation analysis||Among||all components
",,,,,,
ablation-analysis,"They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement , specifically in the encoding stage .","illustrate
importance
of
our proposed dependent reading strategy
leads to
significant improvement
in
encoding stage","importance||of||our proposed dependent reading strategy
our proposed dependent reading strategy||leads to||significant improvement
significant improvement||in||encoding stage
",,"Ablation analysis||illustrate||importance
",,,,,,
ablation-analysis,demonstrates that we achieve the best performance with 450 - dimensional BiLSTMs .,"demonstrates
best performance
with
450 - dimensional BiLSTMs","best performance||with||450 - dimensional BiLSTMs
",,"Ablation analysis||demonstrates||best performance
",,,,,,
research-problem,COARSE - GRAIN FINE - GRAIN COATTENTION NET - WORK FOR MULTI - EVIDENCE QUESTION ANSWERING,MULTI - EVIDENCE QUESTION ANSWERING,,,,,"Contribution||has research problem||MULTI - EVIDENCE QUESTION ANSWERING
",,,,
research-problem,"End - to - end neural models have made significant progress in question answering , however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document .",question answering,,,,,"Contribution||has research problem||question answering
",,,,
research-problem,A requirement of scalable and practical question answering ( QA ) systems is the ability to reason over multiple documents and combine their information to answer questions .,question answering ( QA ),,,,,"Contribution||has research problem||question answering ( QA )
",,,,
research-problem,"Although existing datasets enabled the development of effective end - to - end neural question answering systems , they tend to focus on reasoning over localized sections of a single document .",neural question answering,,,,,"Contribution||has research problem||neural question answering
",,,,
model,"Our multi-evidence QA model , the Coarse - grain Fine - grain Coattention Network ( CFC ) , selects among a set of candidate answers given a set of support documents and a query .","Our multi-evidence QA model
Coarse - grain Fine - grain Coattention Network ( CFC )
selects among
set
of
candidate answers
given
set
of
support documents
query","Our multi-evidence QA model||selects among||set
set||given||set
set||of||support documents
set||of||query
set||of||candidate answers
","Our multi-evidence QA model||name||Coarse - grain Fine - grain Coattention Network ( CFC )
",,"Model||has||Our multi-evidence QA model
",,,,,
model,The CFC is inspired by coarse - grain reasoning and fine - grain reasoning .,"CFC
inspired by
coarse - grain reasoning
fine - grain reasoning","CFC||inspired by||coarse - grain reasoning
CFC||inspired by||fine - grain reasoning
",,,"Model||has||CFC
",,,,,
model,"In coarse - grain reasoning , the model builds a coarse summary of support documents conditioned on the query without knowing what candidates are available , then scores each candidate .","In
coarse - grain reasoning
model
builds
coarse summary
of
support documents
conditioned on
query","model||builds||coarse summary
coarse summary||of||support documents
coarse summary||conditioned on||query
","coarse - grain reasoning||has||model
","Model||In||coarse - grain reasoning
",,,,,,
model,"In fine - grain reasoning , the model matches specific finegrain contexts in which the candidate is mentioned with the query in order to gauge the relevance of the candidate .","fine - grain reasoning
model
matches
specific finegrain contexts
candidate
to gauge
relevance
of","model||matches||specific finegrain contexts
specific finegrain contexts||to gauge||relevance
relevance||of||candidate
","fine - grain reasoning||has||model
",,"Model||In||fine - grain reasoning
",,,,,
model,Each module employs a novel hierarchical attention - a hierarchy of coattention and self - attention - to combine information from the support documents conditioned on the query and candidates .,"Each module
employs
novel hierarchical attention
hierarchy
of
coattention
self - attention
to combine
information
from
support documents
conditioned on
query
candidates","Each module||employs||novel hierarchical attention
hierarchy||to combine||information
information||conditioned on||query
information||conditioned on||candidates
information||from||support documents
hierarchy||of||coattention
hierarchy||of||self - attention
","novel hierarchical attention||has||hierarchy
",,"Model||has||Each module
",,,,,
experiments,MULTI - EVIDENCE QUESTION ANSWERING ON WIKIHOP,"MULTI - EVIDENCE QUESTION ANSWERING
ON
WIKIHOP","MULTI - EVIDENCE QUESTION ANSWERING||ON||WIKIHOP
",,,,,,,"Tasks||has||MULTI - EVIDENCE QUESTION ANSWERING
","MULTI - EVIDENCE QUESTION ANSWERING||has||Hyperparameters
"
experiments,We tokenize the data using Stanford CoreNLP .,"tokenize
using
Stanford CoreNLP","tokenize||using||Stanford CoreNLP
",,,,,,,"Hyperparameters||has||tokenize
",
experiments,We use fixed Glo Ve embeddings as well as character ngram embeddings .,"use
fixed Glo Ve embeddings
as well as
character ngram embeddings","fixed Glo Ve embeddings||as well as||character ngram embeddings
",,,,,"Hyperparameters||use||fixed Glo Ve embeddings
",,,
experiments,We split symbolic query relations into words .,"split
symbolic query relations
into
words","symbolic query relations||into||words
",,,,,"Hyperparameters||split||symbolic query relations
",,,
experiments,All models are trained using ADAM .,"trained using
ADAM",,,,,,"Hyperparameters||trained using||ADAM
",,,
experiments,The CFC achieves state - of - the - art results on both the masked and unmasked versions of WikiHop .,"CFC
achieves
state - of - the - art results
on
masked and unmasked versions
of
WikiHop","CFC||achieves||state - of - the - art results
state - of - the - art results||on||masked and unmasked versions
masked and unmasked versions||of||WikiHop
",,,,,,,"Results||has||CFC
",
experiments,"In particular , on the blind , held - out WikiHop test set , the CFC achieves a new best accuracy of 70.6 % .","on
blind , held - out WikiHop test set
CFC
achieves
new best accuracy
of
70.6 %","CFC||achieves||new best accuracy
new best accuracy||of||70.6 %
","blind , held - out WikiHop test set||has||CFC
",,,,"Results||on||blind , held - out WikiHop test set
",,,
experiments,RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,"RERANKING EXTRACTIVE QUESTION ANSWERING
ON
TRIVIAQA","RERANKING EXTRACTIVE QUESTION ANSWERING||ON||TRIVIAQA
",,,,,,,"Tasks||has||RERANKING EXTRACTIVE QUESTION ANSWERING
",
experiments,Our experimental results in show that reranking using the CFC provides consistent performance gains over only using the span extraction question answering model .,"Our experimental results
show
reranking
using
CFC
provides
consistent performance gains
over only using
span extraction question answering model","Our experimental results||show||reranking
reranking||using||CFC
reranking||provides||consistent performance gains
consistent performance gains||over only using||span extraction question answering model
reranking||using||CFC
reranking||using||CFC
",,,,,,,"RERANKING EXTRACTIVE QUESTION ANSWERING||has||Our experimental results
",
experiments,"In particular , reranking using the CFC improves performance regardless of whether the candidate answer set obtained from the span extraction model contains correct answers .","reranking
using
CFC
improves
performance",,"reranking||has||improves
improves||has||performance
",,,,,,"RERANKING EXTRACTIVE QUESTION ANSWERING||has||reranking
",
experiments,"On the whole Trivia QA dev set , reranking using the CFC results in again of 3.1 % EM and 3.0 % F1 , which suggests that the CFC can be used to further refine the outputs produced by span extraction question answering models .","On
whole Trivia QA dev set
reranking
using
CFC
results in
again
of
3.1 % EM and 3.0 % F1","reranking||results in||again
again||of||3.1 % EM and 3.0 % F1
","whole Trivia QA dev set||has||reranking
",,,,"RERANKING EXTRACTIVE QUESTION ANSWERING||On||whole Trivia QA dev set
",,,
ablation-analysis,Both the coarse - grain module and the fine - grain module significantly contribute to model performance .,"Both the coarse - grain module and the fine - grain module
contribute to
model performance","Both the coarse - grain module and the fine - grain module||contribute to||model performance
",,,"Ablation analysis||has||Both the coarse - grain module and the fine - grain module
",,,,,
ablation-analysis,Replacing selfattention layers with mean - pooling and the bidirectional GRUs with unidirectional GRUs result in less performance degradation .,"Replacing
selfattention layers
with
mean - pooling
bidirectional GRUs
with
unidirectional GRUs
result in
less performance degradation","less performance degradation||Replacing||bidirectional GRUs
bidirectional GRUs||with||unidirectional GRUs
less performance degradation||Replacing||selfattention layers
selfattention layers||with||mean - pooling
",,"Ablation analysis||result in||less performance degradation
",,,,,,
ablation-analysis,"Replacing the encoder with a projection over word embeddings result in significant performance drop , which suggests that contextual encodings that capture positional information is crucial to this task .","Replacing
encoder
with
projection
over
word embeddings
result in
significant performance drop","encoder||with||projection
projection||over||word embeddings
projection||result in||significant performance drop
",,"Ablation analysis||Replacing||encoder
",,,,,,
ablation-analysis,The fine - grain - only model under-performs the coarse - grain - only model consistently across almost all length measures .,"fine - grain - only model
under-performs
coarse - grain - only model
consistently
across
almost all length measures","consistently||across||almost all length measures
","fine - grain - only model||has||under-performs
under-performs||has||coarse - grain - only model
under-performs||has||consistently
",,"Ablation analysis||has||fine - grain - only model
",,,,,
ablation-analysis,"However , the fine - grain - only model matches or outperforms the coarse - grain - only model on examples with a large number of support documents or with long support documents .","matches or outperforms
coarse - grain - only model
on
examples
with
large number
of
support documents
long support documents","coarse - grain - only model||on||examples
examples||with||large number
large number||of||support documents
examples||with||long support documents
","matches or outperforms||has||coarse - grain - only model
",,,,,,"fine - grain - only model||has||matches or outperforms
",
research-problem,Dynamic Entity Representation with Max - pooling Improves Machine Reading,Machine Reading,,,,,"Contribution||has research problem||Machine Reading
",,,,
code,Our code for the model is available at https://github.com/soskek/der-network,https://github.com/soskek/der-network,,,,,"Contribution||Code||https://github.com/soskek/der-network
",,,,
model,"We , however , take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity , by gathering and accumulating information on that entity as it reads a document ( Section 2 ) .","implement
reader
dynamically builds
meaning representations
for
each entity
by gathering and accumulating
information
on
entity
as it reads
document","reader||dynamically builds||meaning representations
meaning representations||for||each entity
meaning representations||by gathering and accumulating||information
information||as it reads||document
information||on||entity
",,"Model||implement||reader
",,,,,,
hyperparameters,"For preprocessing , we segment sentences at punctuation marks "" . "" , "" ! "" , and "" ? "" .","For
preprocessing
segment
sentences
punctuation marks
.
!
?","preprocessing||segment||sentences
preprocessing||segment||punctuation marks
","punctuation marks||has||.
punctuation marks||has||!
punctuation marks||has||?
","Hyperparameters||For||preprocessing
",,,,,,
hyperparameters,"We train our model 8 with hyper - parameters lightly tuned on the validation set 9 , and we conduct ablation test on several techniques that improve our basic model .","train
model
with
hyper - parameters
lightly tuned
on
validation set","model||with||hyper - parameters
lightly tuned||on||validation set
","hyper - parameters||has||lightly tuned
","Hyperparameters||train||model
",,,,,,
results,"As shown in , Max - pooling described in Section 2.2 drastically improves performance , showing the effect of accumulating information on entities .","Max - pooling
drastically improves
performance",,"Max - pooling||has||drastically improves
drastically improves||has||performance
",,"Results||has||Max - pooling
",,,,,
results,"Further , we note that initializing our model with pre-trained word vectors 10 is helpful , though world knowledge of entities has been prevented by the anonymization process .","note that
initializing
our model
with
pre-trained word vectors
is
helpful","our model||with||pre-trained word vectors
pre-trained word vectors||is||helpful
","initializing||has||our model
","Results||note that||initializing
",,,,,,
results,"Finally , we note that our model , full DER Network , shows the best results compared to several previous reader models , endorsing our approach as promising .","our model
full DER Network
shows
best results
compared to
several previous reader models","our model||shows||best results
best results||compared to||several previous reader models
","our model||name||full DER Network
",,"Results||note that||our model
",,,,,
results,"The 99 % confidence intervals of the results of full DER Network and the one initialized by word2vec on the test set were [ 0.700 , 0.740 ] and [ 0.708 , 0.749 ] , respectively ( measured by bootstrap tests ) .","99 % confidence intervals
of
results
of
full DER Network and the one initialized by word2vec
on
test set
were
[ 0.700 , 0.740 ]
[ 0.708 , 0.749 ]","99 % confidence intervals||of||results
results||of||full DER Network and the one initialized by word2vec
full DER Network and the one initialized by word2vec||were||[ 0.700 , 0.740 ]
full DER Network and the one initialized by word2vec||were||[ 0.708 , 0.749 ]
full DER Network and the one initialized by word2vec||on||test set
",,,"Results||has||99 % confidence intervals
",,,,,
research-problem,Story Comprehension for Predicting What Happens Next,Story Comprehension,,,,,"Contribution||has research problem||Story Comprehension
",,,,
research-problem,"Automatic story comprehension is a fundamental challenge in Natural Language Understanding , and can enable computers to learn about social norms , human behavior and commonsense .",Automatic story comprehension,,,,,"Contribution||has research problem||Automatic story comprehension
",,,,
research-problem,"For these reasons , automatically understanding stories is an interesting but challenging task for Computational Linguists .",automatically understanding stories,,,,,"Contribution||has research problem||automatically understanding stories
",,,,
research-problem,"Recently , introduced the story - cloze task for testing this ability , albeit without the aspect of language generation .",story - cloze,,,,,"Contribution||has research problem||story - cloze
",,,,
model,"In this paper we explore three semantic aspects of story understanding : ( i ) the sequence of events described in the story , ( ii ) the evolution of sentiment and emotional trajectories , and ( iii ) topical consistency .","explore
three semantic aspects
of
story understanding
sequence of events
described in
story
evolution
of
sentiment and emotional trajectories
topical consistency","three semantic aspects||of||story understanding
sequence of events||described in||story
evolution||of||sentiment and emotional trajectories
","three semantic aspects||has||sequence of events
three semantic aspects||has||evolution
three semantic aspects||has||topical consistency
","Model||explore||three semantic aspects
",,,,,,
model,"The first aspect is motivated from approaches in semantic script induction , and evaluates if events described in an ending - alternative are likely to occur within the sequence of events described in the preceding context .","first aspect
motivated from
semantic script induction
evaluates if
events
described in
ending - alternative
likely to
occur
within
sequence of events
described in
preceding context","first aspect||motivated from||semantic script induction
first aspect||evaluates if||events
events||likely to||occur
occur||within||sequence of events
sequence of events||described in||preceding context
events||described in||ending - alternative
",,,"Model||has||first aspect
",,,,,
model,Our model captures this by evaluating if the sentiment described in an ending option makes sense considering the context of the story .,"evaluating if
sentiment
described in
ending option
makes sense
considering
context
of
story","makes sense||considering||context
context||of||story
sentiment||described in||ending option
","sentiment||has||makes sense
","Model||evaluating if||sentiment
",,,,,,
model,Our model accounts for that by analyzing if the topic of an ending option is consistent with the preceding context .,"analyzing if
topic
of
ending option
consistent with
preceding context","topic||of||ending option
topic||consistent with||preceding context
",,"Model||analyzing if||topic
",,,,,,
model,We present a log - linear model that is used to weigh the various aspects of the story using a hidden variable .,"present
log - linear model
used to
weigh
various aspects
of
story
using
hidden variable","log - linear model||used to||weigh
weigh||using||hidden variable
various aspects||of||story
","weigh||has||various aspects
","Model||present||log - linear model
",,,,,,
baselines,DSSM : It trains two deep neural networks to project the context and the ending - options into the same vector space .,"DSSM
trains
two deep neural networks
to project
context and the ending - options
into
same vector space","DSSM||trains||two deep neural networks
two deep neural networks||to project||context and the ending - options
context and the ending - options||into||same vector space
",,,"Baselines||has||DSSM
",,,,,
baselines,Msap :,Msap,,,,"Baselines||has||Msap
",,,,,
baselines,It trains a logistic regression based on stylistic and languagemodel based features .,"trains
logistic regression
based on
stylistic and languagemodel based features","logistic regression||based on||stylistic and languagemodel based features
",,,,,"Msap||trains||logistic regression
",,,
baselines,LR : Our next baseline is a simple logistic regression model which is agnostic to the fact that there are multiple types of aspects .,"LR
is
simple logistic regression model
agnostic to
multiple types of aspects","LR||is||simple logistic regression model
simple logistic regression model||agnostic to||multiple types of aspects
",,,"Baselines||has||LR
",,,,,
baselines,Majority Vote :,Majority Vote,,,,"Baselines||has||Majority Vote
",,,,,"Majority Vote||has||ensemble method
"
baselines,"This ensemble method uses the features extracted for each of the K = 3 aspects , to train K separate logistic regression models .","ensemble method
uses
features
extracted for
K = 3 aspects
to train
K separate logistic regression models","ensemble method||uses||features
features||extracted for||K = 3 aspects
features||to train||K separate logistic regression models
",,,,,,,,
baselines,Soft Voting :,Soft Voting,,,,"Baselines||has||Soft Voting
",,,,,
baselines,This baseline also learns K different aspect - specific classifiers .,"learns
K different aspect - specific classifiers",,,,,,"Soft Voting||learns||K different aspect - specific classifiers
",,,
baselines,Aspect - aware Ensemble :,Aspect - aware Ensemble,,,,"Baselines||has||Aspect - aware Ensemble
",,,,,
baselines,"Like the voting methods , this baseline also trains K different aspectspecific classifiers .","trains
K different aspectspecific classifiers",,,,,,"Aspect - aware Ensemble||trains||K different aspectspecific classifiers
",,,
ablation-analysis,shows the performance of a logistic regression model trained using all the features ( All ) and then using individual feature - groups .,"shows
performance
of
logistic regression model
trained
using
all the features ( All )
individual feature - groups","performance||of||logistic regression model
trained||using||all the features ( All )
trained||using||individual feature - groups
","logistic regression model||has||trained
","Ablation analysis||shows||performance
",,,,,,
ablation-analysis,"We can see that the features extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those characterizing Sentiment - trajectory .","see that
features
extracted from
aspect
analyzing
event - sequence
have
strongest predictive power
followed by
characterizing
Sentiment - trajectory","features||extracted from||aspect
aspect||analyzing||event - sequence
event - sequence||have||strongest predictive power
strongest predictive power||followed by||characterizing
","characterizing||has||Sentiment - trajectory
","Ablation analysis||see that||features
",,,,,,
ablation-analysis,The features measuring top - ical consistency result in lowest accuracy but they still perform better than random on the task .,"features
measuring
top - ical consistency
result in
lowest accuracy
perform better
than
random","features||measuring||top - ical consistency
top - ical consistency||result in||lowest accuracy
perform better||than||random
","top - ical consistency||has||perform better
",,"Ablation analysis||has||features
",,,,,
research-problem,LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY,COMPUTE WORD EMBEDDINGS ON THE FLY,,,,,"Contribution||has research problem||COMPUTE WORD EMBEDDINGS ON THE FLY
",,,,
research-problem,"Learning representations for rare words is a well - known challenge of natural language understanding , since the standard end - to - end supervised learning methods require many occurrences of each word to generalize well .",Learning representations for rare words,,,,,"Contribution||has research problem||Learning representations for rare words
",,,,
model,"In this paper we propose a new method for computing embeddings "" on the fly "" , which jointly addresses the large vocabulary problem and the paucity of data for learning representations in the long tail of the Zipfian distribution .","propose
new method
for computing
embeddings "" on the fly ""
jointly addresses
large vocabulary problem
paucity of data
for learning
representations
in
long tail of the Zipfian distribution","new method||for computing||embeddings "" on the fly ""
embeddings "" on the fly ""||jointly addresses||large vocabulary problem
embeddings "" on the fly ""||jointly addresses||paucity of data
paucity of data||for learning||representations
representations||in||long tail of the Zipfian distribution
",,"Model||propose||new method
",,,,,,
model,"This method , which we illustrate in , can be summarized as follows : instead of directly learning separate representations for all words in a potentially unbounded vocabulary , we train a network to predict the representations of words based on auxiliary data .","of
representations
words
train
network
to predict
based on
auxiliary data","network||to predict||representations
representations||based on||auxiliary data
representations||of||words
",,"Model||train||network
",,,,,,
model,Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation .,"Several sources of auxiliary data
used
simultaneously
as
input
to
neural network
compute
combined representation","Several sources of auxiliary data||as||input
input||to||neural network
neural network||compute||combined representation
Several sources of auxiliary data||used||simultaneously
",,,"Model||has||Several sources of auxiliary data
",,,,,
model,"These representations can then be used for out - of - vocabulary words , or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an external data source .","used for
out - of - vocabulary words
combined with
withinvocabulary word embeddings
directly trained on
task of interest
pretrained from
external data source","withinvocabulary word embeddings||directly trained on||task of interest
withinvocabulary word embeddings||pretrained from||external data source
",,,,,"combined representation||used for||out - of - vocabulary words
combined representation||combined with||withinvocabulary word embeddings
",,,
model,"Importantly , the auxiliary data encoders are trained jointly with the objective , ensuring the preservation of semantic alignment with representations of within - vocabulary words .","auxiliary data encoders
trained jointly with
objective
ensuring
preservation
of
semantic alignment
with
representations
of
within - vocabulary words","auxiliary data encoders||ensuring||preservation
preservation||of||semantic alignment
semantic alignment||with||representations
representations||of||within - vocabulary words
auxiliary data encoders||trained jointly with||objective
",,,"Model||has||auxiliary data encoders
",,,,,
experiments,QUESTION ANSWERING,QUESTION ANSWERING,,,,,,,,"Tasks||has||QUESTION ANSWERING
","QUESTION ANSWERING||has||Results
"
experiments,Looking at the results one can see that adding any external information results in a significant improvement over the baseline model ( B ) ( 3.7 - 10.5 points ) .,"adding
any external information
results in
significant improvement
over
baseline model
3.7 - 10.5 points","any external information||results in||significant improvement
significant improvement||over||baseline model
","significant improvement||has||3.7 - 10.5 points
",,,,"Results||adding||any external information
",,,
experiments,"When the dictionary alone is used , mean pooling ( D3 ) performs similarly to LSTM ( D4 ) .","dictionary alone
mean pooling
performs
similarly
to
LSTM","mean pooling||performs||similarly
similarly||to||LSTM
","dictionary alone||has||mean pooling
",,,,,,"Results||has||dictionary alone
",
experiments,"We found that adding the spelling ( S ) helps more than adding a dictionary ( D ) ( 3 points difference ) , possibly due to relatively lower coverage of our dictionary .","adding
spelling
helps more
than
dictionary
3 points difference","helps more||than||adding
","spelling||has||helps more
adding||has||dictionary
helps more||has||3 points difference
",,,,"Results||adding||spelling
",,,
experiments,"However , the model that uses both ( SD ) has a 1.1 point advantage over the model that uses just the spelling ( S ) , demonstrating that combining several forms of auxiliary data allows the model to exploit the complementary information they provide .","model
uses
SD
1.1 point advantage
over
uses
just the spelling","1.1 point advantage||over||model
model||uses||just the spelling
","SD||has||1.1 point advantage
",,,,"Results||uses||SD
",,,
experiments,"The model with GLoVe embeddings ( G ) is still ahead with a 1.1 point margin , but the gap has been shrunk .","model with GLoVe embeddings ( G )
is
still ahead
with
1.1 point margin","model with GLoVe embeddings ( G )||is||still ahead
still ahead||with||1.1 point margin
",,,,,,,"Results||has||model with GLoVe embeddings ( G )
",
experiments,ENTAILMENT PREDICTION,ENTAILMENT PREDICTION,,,,,,,,"Tasks||has||ENTAILMENT PREDICTION
","ENTAILMENT PREDICTION||has||Results
"
experiments,"Compared to the SQuAD results , an important difference is that spelling was not as useful on SNLI and MultiNLI .","spelling
was
not as useful
on
SNLI and MultiNLI","spelling||was||not as useful
not as useful||on||SNLI and MultiNLI
",,,,,,,"Results||has||spelling
",
experiments,"We also note that we tried using fixed random embeddings for OOV words as proposed by , and that this method did not bring a significant advantage over the baseline .","using
fixed random embeddings
for
OOV words
did not bring a significant advantage
over
baseline","fixed random embeddings||for||OOV words
did not bring a significant advantage||over||baseline
","fixed random embeddings||has||did not bring a significant advantage
",,,,"Results||using||fixed random embeddings
",,,
experiments,"shows that , as expected , dictionary - enabled models significantly outperform baseline models for sentences containing rare words .","dictionary - enabled models
significantly outperform
baseline models
for
sentences
containing
rare words","significantly outperform||for||sentences
sentences||containing||rare words
","dictionary - enabled models||has||significantly outperform
significantly outperform||has||baseline models
",,,,,,"Results||has||dictionary - enabled models
",
experiments,LANGUAGE MODELLING,LANGUAGE MODELLING,,,,,,,,"Tasks||has||LANGUAGE MODELLING
","LANGUAGE MODELLING||has||Results
"
experiments,"Similarly to our other experiments , using external information to compute embeddings of unknown words helps in all cases .","using
external information
to compute
embeddings
of
unknown words
helps
in
all cases","external information||to compute||embeddings
embeddings||of||unknown words
helps||in||all cases
","external information||has||helps
",,,,"Results||using||external information
",,,
experiments,"We note that lemma + lowercase performs worse than any model with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .","note that
lemma + lowercase
performs
worse
than
any model
with
dictionary","lemma + lowercase||performs||worse
worse||than||any model
any model||with||dictionary
",,,,,"Results||note that||lemma + lowercase
",,,
experiments,Adding spelling consistently helps more than adding dictionary definitions .,"Adding
spelling
consistently helps more
than
adding
dictionary definitions","consistently helps more||than||adding
","spelling||has||consistently helps more
adding||has||dictionary definitions
",,,,"Results||Adding||spelling
",,,
experiments,"Using both dictionary and spelling is consistently slightly better than using just spelling , and the improvement is more pronounced in the restricted setting .","Using
dictionary and spelling
consistently slightly better
than
just spelling","consistently slightly better||than||just spelling
","dictionary and spelling||has||consistently slightly better
",,,,"Results||Using||dictionary and spelling
",,,
experiments,Using Glo Ve embeddings results in the best perplexity .,"Glo Ve embeddings
results in
best perplexity","Glo Ve embeddings||results in||best perplexity
",,,,,,,"Results||Using||Glo Ve embeddings
",
research-problem,Enhanced LSTM for Natural Language Inference,Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,Reasoning and inference are central to human and artificial intelligence .,Reasoning and inference,,,,,"Contribution||has research problem||Reasoning and inference
",,,,
research-problem,"Specifically , natural language inference ( NLI ) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p , as depicted in the following example from MacCartney ( 2009 ) , where the hypothesis is regarded to be entailed from the premise .",natural language inference ( NLI ),,,,,"Contribution||has research problem||natural language inference ( NLI )
",,,,
model,"While some previous top - performing models use rather complicated network architectures to achieve the state - of - the - art results , we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results , suggesting that the potentials of such sequential inference approaches have not been fully exploited yet .","enhancing
sequential inference models
based on
chain models","sequential inference models||based on||chain models
",,"Model||enhancing||sequential inference models
",,,,,,
research-problem,Exploring syntax for NLI is very attractive to us .,NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,"We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework , we achieve additional improvement , increasing the performance to a new state of the art with an 88.6 % accuracy .","explicitly encoding
parsing information
with
recursive networks
in
local inference modeling
inference composition","parsing information||with||recursive networks
parsing information||in||local inference modeling
parsing information||in||inference composition
",,"Model||explicitly encoding||parsing information
",,,,,,
hyperparameters,"We use the Adam method ( Kingma and Ba , 2014 ) for optimization .","use
Adam method
for
optimization","Adam method||for||optimization
",,"Hyperparameters||use||Adam method
",,,,,,
hyperparameters,The first momentum is set to be 0.9 and the second 0.999 .,"first momentum
set to be
0.9
second
0.999","first momentum||set to be||0.9
","second||has||0.999
",,"Hyperparameters||has||first momentum
Hyperparameters||has||second
",,,,,
hyperparameters,The initial learning rate is 0.0004 and the batch size is 32 .,"initial learning rate
is
0.0004
batch size
is
32","initial learning rate||is||0.0004
batch size||is||32
",,,"Hyperparameters||has||initial learning rate
Hyperparameters||has||batch size
",,,,,
hyperparameters,"All hidden states of LSTMs , tree - LSTMs , and word embeddings have 300 dimensions .","hidden states
of
LSTMs
tree - LSTMs
word embeddings
have
300 dimensions","hidden states||of||LSTMs
hidden states||of||tree - LSTMs
hidden states||of||word embeddings
hidden states||have||300 dimensions
",,,"Hyperparameters||has||hidden states
",,,,,
hyperparameters,"We use dropout with a rate of 0.5 , which is applied to all feedforward connections .","dropout
with
rate
of
0.5
applied to
all feedforward connections","dropout||with||rate
rate||of||0.5
dropout||applied to||all feedforward connections
",,,"Hyperparameters||use||dropout
",,,,,
hyperparameters,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,"pre-trained 300 - D Glove 840B vectors
to initialize
our word embeddings","pre-trained 300 - D Glove 840B vectors||to initialize||our word embeddings
",,,"Hyperparameters||use||pre-trained 300 - D Glove 840B vectors
",,,,,
hyperparameters,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"Out - of - vocabulary ( OOV ) words
initialized
randomly
with
Gaussian samples","Out - of - vocabulary ( OOV ) words||initialized||randomly
randomly||with||Gaussian samples
",,,"Hyperparameters||has||Out - of - vocabulary ( OOV ) words
",,,,,
results,"Our final model achieves the accuracy of 88.6 % , the best result observed on SNLI , while our enhanced sequential encoding model attains an accuracy of 88.0 % , which also outperform the previous models .","Our final model
achieves
accuracy
of
88.6 %
best result
observed on
SNLI
our enhanced sequential encoding model
attains
accuracy
of
88.0 %
outperform
previous models","accuracy||of||88.6 %
accuracy||of||88.0 %
Our final model||achieves||accuracy
accuracy||of||88.6 %
best result||observed on||SNLI
our enhanced sequential encoding model||attains||accuracy
accuracy||of||88.0 %
","88.6 %||has||best result
88.0 %||has||outperform
outperform||has||previous models
",,"Results||has||Our final model
Results||has||our enhanced sequential encoding model
",,,,,
results,"In general , adding intra-sentence attention yields further improvement , which is not very surprising as it could help align the relevant text spans between premise and hypothesis .","adding
intra-sentence attention
yields
further improvement","intra-sentence attention||yields||further improvement
",,"Results||adding||intra-sentence attention
",,,,,,
results,"The table shows that our ESIM model achieves an accuracy of 88.0 % , which has already outperformed all the previous models , including those using much more complicated network architectures .","our ESIM model
achieves
accuracy
of
88.0 %
outperformed
previous models
including
more complicated network architectures","our ESIM model||achieves||accuracy
previous models||including||more complicated network architectures
","our ESIM model||has||outperformed
outperformed||has||previous models
",,"Results||has||our ESIM model
",,,,,
results,"We ensemble our ESIM model with syntactic tree - LSTMs based on syntactic parse trees and achieve significant improvement over our best sequential encoding model ESIM , attaining an accuracy of 88.6 % .","ensemble
our ESIM model
with
syntactic tree - LSTMs
based on
syntactic parse trees
achieve
significant improvement
over
our best sequential encoding model ESIM
attaining
accuracy
of
88.6 %","our ESIM model||with||syntactic tree - LSTMs
syntactic tree - LSTMs||based on||syntactic parse trees
our ESIM model||achieve||significant improvement
significant improvement||over||our best sequential encoding model ESIM
significant improvement||attaining||accuracy
",,"Results||ensemble||our ESIM model
",,,,,,
ablation-analysis,Each tree node is implemented with a tree - LSTM block same as in model .,"Each tree node
implemented with
tree - LSTM block","Each tree node||implemented with||tree - LSTM block
",,,"Ablation analysis||has||Each tree node
",,,,,
ablation-analysis,"shows that with this replacement , the performance drops to 88.2 % .","performance
drops
to
88.2 %","drops||to||88.2 %
",,,,,"tree - LSTM block||performance||drops
",,,
ablation-analysis,"If we remove the pooling layer in inference composition and replace it with summation as in , the accuracy drops to 87.1 % .","remove
pooling layer
in
inference composition
replace it with
summation
accuracy
drops
to
87.1 %","pooling layer||in||inference composition
pooling layer||replace it with||summation
drops||to||87.1 %
","accuracy||has||drops
summation||has||accuracy
accuracy||has||drops
accuracy||has||drops
","Ablation analysis||remove||pooling layer
",,,,,,
ablation-analysis,"If we remove the difference and elementwise product from the local inference enhancement layer , the accuracy drops to 87.0 % .","difference and elementwise product
from
local inference enhancement layer
accuracy
drops
to
87.0 %","difference and elementwise product||from||local inference enhancement layer
drops||to||87.0 %
","local inference enhancement layer||has||accuracy
",,"Ablation analysis||remove||difference and elementwise product
",,,,,
ablation-analysis,"To provide some detailed comparison with , replacing bidirectional LSTMs in inference composition and also input encoding with feedforward neural network reduces the accuracy to 87.3 % and 86.3 % respectively .","with
replacing
bidirectional LSTMs
in
inference composition
input encoding
feedforward neural network
reduces
accuracy
to
87.3 % and 86.3 %","bidirectional LSTMs||in||inference composition
accuracy||to||87.3 % and 86.3 %
input encoding||with||feedforward neural network
","reduces||has||accuracy
","Ablation analysis||replacing||bidirectional LSTMs
Ablation analysis||replacing||reduces
Ablation analysis||replacing||input encoding
",,,,,,
ablation-analysis,"If we remove the premise - based attention from ESIM ( model 23 ) , the accuracy drops to 87.2 % on the test set .","premise - based attention
from
ESIM
accuracy
drops
to
87.2 %
on
test set","premise - based attention||from||ESIM
drops||to||87.2 %
drops||on||test set
","premise - based attention||has||accuracy
",,"Ablation analysis||remove||premise - based attention
",,,,,
ablation-analysis,"Removing the hypothesis - based attention ( model 24 ) decrease the accuracy to 86.5 % , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .","Removing
hypothesis - based attention ( model 24 )
decrease
accuracy
to
86.5 %","accuracy||to||86.5 %
","hypothesis - based attention ( model 24 )||has||decrease
decrease||has||accuracy
","Ablation analysis||Removing||hypothesis - based attention ( model 24 )
",,,,,,
research-problem,Multi - Perspective Context Matching for Machine Comprehension,Machine Comprehension,,,,,"Contribution||has research problem||Machine Comprehension
",,,,
research-problem,"Previous machine comprehension ( MC ) datasets are either too small to train endto - end deep learning models , or not difficult enough to evaluate the ability of current MC techniques .","machine comprehension ( MC )
MC",,,,,"Contribution||has research problem||machine comprehension ( MC )
Contribution||has research problem||MC
",,,,
model,"In this work , we focus on the SQuAD dataset and propose an end - to - end deep neural network model for machine comprehension .","propose
end - to - end deep neural network model
for
machine comprehension","end - to - end deep neural network model||for||machine comprehension
",,"Model||propose||end - to - end deep neural network model
",,,,,,
model,"Based on this assumption , we design a Multi - Perspective Context Matching ( MPCM ) model to identify the answer span by matching the context of each point in the passage with the question from multiple perspectives .","design
Multi - Perspective Context Matching ( MPCM ) model
to identify
answer span
by matching
context
of
each point
in
passage
with
question
from
multiple perspectives","Multi - Perspective Context Matching ( MPCM ) model||to identify||answer span
answer span||by matching||context
context||with||question
question||from||multiple perspectives
context||of||each point
each point||in||passage
",,"Model||design||Multi - Perspective Context Matching ( MPCM ) model
",,,,,,
model,"Instead of enumerating all the possible spans explicitly and ranking them , our model identifies the answer span by predicting the beginning and ending points individually with globally normalized probability distributions across the whole passage .","identifies
answer span
by predicting
beginning and ending points
individually
with
globally normalized probability distributions
across
whole passage","answer span||by predicting||beginning and ending points
beginning and ending points||with||globally normalized probability distributions
globally normalized probability distributions||across||whole passage
","beginning and ending points||has||individually
","Model||identifies||answer span
",,,,,,
experimental-setup,We process the corpus with the tokenizer from Stanford CorNLP .,"process
corpus
with
tokenizer
from
Stanford CorNLP","corpus||with||tokenizer
tokenizer||from||Stanford CorNLP
",,"Experimental setup||process||corpus
",,,,,,
experimental-setup,"To initialize the word embeddings in the word representation layer , we use the 300 - dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus .","initialize
word embeddings
in
word representation layer
use
300 - dimensional GloVe word vectors
pre-trained from
840B Common Crawl corpus","word embeddings||in||word representation layer
word embeddings||use||300 - dimensional GloVe word vectors
300 - dimensional GloVe word vectors||pre-trained from||840B Common Crawl corpus
",,"Experimental setup||initialize||word embeddings
",,,,,,
experimental-setup,"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .","For
out - of - vocabulary ( OOV ) words
initialize
word embeddings
randomly","out - of - vocabulary ( OOV ) words||initialize||word embeddings
","word embeddings||has||randomly
","Experimental setup||For||out - of - vocabulary ( OOV ) words
",,,,,,
experimental-setup,"We set the hidden size as 100 for all the LSTM layers , and set the number of perspectives l of our multiperspective matching function ( Equation ( 5 ) ) as 50 .","set
hidden size
as
100
for
all the LSTM layers
number of perspectives l
of
our multiperspective matching function
as
50","number of perspectives l||as||50
number of perspectives l||of||our multiperspective matching function
hidden size||as||100
hidden size||for||all the LSTM layers
",,"Experimental setup||set||number of perspectives l
Experimental setup||set||hidden size
",,,,,,
experimental-setup,"We apply dropout to every layers in , and set the dropout ratio as 0.2 .","apply
dropout
to
every layers
dropout ratio
as
0.2","dropout ratio||as||0.2
dropout||to||every layers
",,"Experimental setup||apply||dropout
","Experimental setup||set||dropout ratio
",,,,,
experimental-setup,"To train the model , we minimize the cross entropy of the be - ginning and end points , and use the ADAM optimizer to update parameters .","minimize
cross entropy
of
be - ginning and end points
use
ADAM optimizer
to update
parameters","cross entropy||of||be - ginning and end points
ADAM optimizer||to update||parameters
",,"Experimental setup||minimize||cross entropy
Experimental setup||use||ADAM optimizer
",,,,,,
experimental-setup,We set the learning rate as 0.0001 .,"learning rate
as
0.0001","learning rate||as||0.0001
",,,"Experimental setup||set||learning rate
",,,,,
ablation-analysis,We can see that removing any components from the MPCM model decreases the performance significantly .,"removing
any components
from
MPCM model
decreases
performance
significantly","any components||from||MPCM model
","any components||has||decreases
decreases||has||performance
performance||has||significantly
","Ablation analysis||removing||any components
",,,,,,
ablation-analysis,"Among all the layers , the Aggregation Layer is the most crucial layer .","Among
all the layers
Aggregation Layer
is
most crucial layer","Aggregation Layer||is||most crucial layer
","all the layers||has||Aggregation Layer
","Ablation analysis||Among||all the layers
",,,,,,
ablation-analysis,"Among all the matching strategies , Maxpooling - Matching has the biggest effect .","all the matching strategies
Maxpooling - Matching
biggest effect",,"all the matching strategies||has||Maxpooling - Matching
Maxpooling - Matching||has||biggest effect
",,"Ablation analysis||Among||all the matching strategies
",,,,,
research-problem,SG - Net : Syntax - Guided Machine Reading Comprehension,Machine Reading Comprehension,,,,,"Contribution||has research problem||Machine Reading Comprehension
",,,,
research-problem,"Understanding the meaning of a sentence is a prerequisite to solve many natural language understanding ( NLU ) problems , such as machine reading comprehension ( MRC ) based question answering .",machine reading comprehension ( MRC ),,,,,"Contribution||has research problem||machine reading comprehension ( MRC )
",,,,
research-problem,We observe that the accuracy of MRC models decreases when answering long questions ( shown in Section 5.1 ) .,MRC,,,,,"Contribution||has research problem||MRC
",,,,
model,"In this paper , we extend the self - attention mechanism with syntax - guided constraint , to capture syntax related parts with each concerned word .","extend
self - attention mechanism
with
syntax - guided constraint
to capture
syntax related parts
with
each concerned word","self - attention mechanism||with||syntax - guided constraint
self - attention mechanism||to capture||syntax related parts
syntax related parts||with||each concerned word
",,"Model||extend||self - attention mechanism
",,,,,,
model,"Specifically , we adopt pre-trained dependency syntactic parse tree structure to produce the related nodes for each word in a sentence , namely syntactic dependency of interest ( SDOI ) , by regarding each word as a child node and the SDOI consists all its ancestor nodes and itself in the dependency parsing tree .","adopt
pre-trained dependency syntactic parse tree structure
to produce
related nodes
for
each word
in
sentence
namely
syntactic dependency of interest ( SDOI )
regarding
each word
as
child node","pre-trained dependency syntactic parse tree structure||namely||syntactic dependency of interest ( SDOI )
pre-trained dependency syntactic parse tree structure||to produce||related nodes
related nodes||for||each word
each word||in||sentence
each word||regarding||each word
each word||as||child node
",,"Model||adopt||pre-trained dependency syntactic parse tree structure
",,,,,,
model,"To effectively accommodate such SDOI information , we propose a novel syntax - guided network ( SG - Net ) , which fuses the original SAN and SDOI - SAN , to provide more linguistically inspired representation for challenging reading comprehension tasks 1 .","accommodate
SDOI information
propose
novel syntax - guided network ( SG - Net )
fuses
original SAN and SDOI - SAN
to provide
more linguistically inspired representation
for
reading comprehension tasks","SDOI information||propose||novel syntax - guided network ( SG - Net )
novel syntax - guided network ( SG - Net )||to provide||more linguistically inspired representation
more linguistically inspired representation||for||reading comprehension tasks
novel syntax - guided network ( SG - Net )||fuses||original SAN and SDOI - SAN
",,"Model||accommodate||SDOI information
",,,,,,
hyperparameters,We adopt the Whole Word Masking BERT as the baseline 6 .,"adopt
Whole Word Masking BERT",,,"Hyperparameters||adopt||Whole Word Masking BERT
",,,,,,
hyperparameters,"The initial learning rate is set in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .","initial learning rate
set in
{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }
with
warm - up rate
of
0.1
L2 weight decay
of
0.01","initial learning rate||with||warm - up rate
warm - up rate||of||0.1
initial learning rate||with||L2 weight decay
L2 weight decay||of||0.01
initial learning rate||set in||{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }
",,,"Hyperparameters||has||initial learning rate
",,,,,
hyperparameters,"The batch size is selected in { 16 , 20 , 32 } .","batch size
selected in
{ 16 , 20 , 32 }","batch size||selected in||{ 16 , 20 , 32 }
",,,"Hyperparameters||has||batch size
",,,,,
hyperparameters,The maximum number of epochs is set to 3 or 10 depending on tasks .,"maximum number of epochs
set to
3 or 10","maximum number of epochs||set to||3 or 10
",,,"Hyperparameters||has||maximum number of epochs
",,,,,
hyperparameters,The weight ?,weight,,,,"Hyperparameters||has||weight
",,,,,
hyperparameters,in the dual context aggregation is 0.5 .,"in
dual context aggregation
is
0.5",,,,,,"weight||in||dual context aggregation
weight||is||0.5
",,,
hyperparameters,"All the texts are tokenized using wordpieces , and the maximum input length is set to 384 for both of SQuAD and RACE .","texts
are
tokenized
using
wordpieces
maximum input length
set to
384
for
SQuAD and RACE","maximum input length||set to||384
384||for||SQuAD and RACE
texts||are||tokenized
tokenized||using||wordpieces
",,,"Hyperparameters||has||maximum input length
Hyperparameters||has||texts
",,,,,
results,It also outperforms all the published works and achieves the 2nd place on the leaderboard when submitting SG - NET .,"outperforms
all the published works
achieves
2nd place
on
leaderboard
when submitting
SG - NET","outperforms||achieves||2nd place
2nd place||when submitting||SG - NET
2nd place||on||leaderboard
","outperforms||has||all the published works
",,"Results||has||outperforms
",,,,,
results,"We also find that adding an extra answer verifier module could yield better result , which is pre-trained only to determine whether question is answerable or not with the same training data as SG - Net .","adding
extra answer verifier module
yield
better result","extra answer verifier module||yield||better result
",,"Results||adding||extra answer verifier module
",,,,,,
research-problem,Long Short - Term Memory - Networks for Machine Reading,Machine Reading,,,,,"Contribution||has research problem||Machine Reading
",,,,
model,The idea is to use multiple memory slots outside the recurrence to piece - wise store representations of the input ; read and write operations for each slot can be modeled as an attention mechanism with a recurrent controller .,"to
use
multiple memory slots
outside
recurrence
piece - wise store representations
of
input
read and write operations
for
each slot
modeled as
attention mechanism
with
recurrent controller","multiple memory slots||outside||recurrence
recurrence||to||piece - wise store representations
piece - wise store representations||of||input
read and write operations||for||each slot
each slot||modeled as||attention mechanism
attention mechanism||with||recurrent controller
",,"Model||use||multiple memory slots
Model||use||read and write operations
",,,,,,
model,We also leverage memory and attention to empower a recurrent network with stronger memorization capability and more importantly the ability to discover relations among tokens .,"leverage
memory and attention
to empower
recurrent network
with
stronger memorization capability
ability
to discover
relations
among
tokens","memory and attention||to empower||recurrent network
recurrent network||with||stronger memorization capability
recurrent network||with||ability
ability||to discover||relations
relations||among||tokens
",,"Model||leverage||memory and attention
",,,,,,
model,This is realized by inserting a memory network module in the update of a recurrent network together with attention for memory addressing .,"realized by
inserting
memory network module
in
update
of
recurrent network
together with
attention
for
memory addressing","memory network module||together with||attention
attention||for||memory addressing
memory network module||in||update
update||of||recurrent network
","inserting||has||memory network module
","Model||realized by||inserting
",,,,,,
model,"The resulting model , which we term Long Short - Term Memory - Network ( LSTMN ) , is a reading simulator that can be used for sequence processing tasks .","term
Long Short - Term Memory - Network ( LSTMN )
is
reading simulator
used for
sequence processing tasks","Long Short - Term Memory - Network ( LSTMN )||is||reading simulator
reading simulator||used for||sequence processing tasks
",,"Model||term||Long Short - Term Memory - Network ( LSTMN )
",,,,,,
model,The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed .,"processes
text
incrementally
while
learning
past tokens
in the memory
to what extent
relate to
current token
being
processed","text||while||learning
learning||past tokens||in the memory
learning||past tokens||to what extent
to what extent||relate to||current token
current token||being||processed
","text||has||incrementally
","Model||processes||text
",,,,,,
model,"As a result , the model induces undirected relations among tokens as an intermediate step of learning representations .","induces
undirected relations
among
tokens
as an
intermediate step
of learning
representations","undirected relations||as an||intermediate step
intermediate step||of learning||representations
undirected relations||among||tokens
",,"Model||induces||undirected relations
",,,,,,
code,Our code is available at https://github.com/cheng6076/,https://github.com/cheng6076/,,,,,"Contribution||Code||https://github.com/cheng6076/
",,,,
experiments,Language Modeling,Language Modeling,,,,,,,,"Tasks||has||Language Modeling
","Language Modeling||has||Hyperparameters
"
experiments,"We used stochastic gradient descent for optimization with an initial learning rate of 0.65 , which decays by a factor of 0.85 per epoch if no significant improvement has been observed on the validation set .","used
stochastic gradient descent
for
optimization
with
initial learning rate
of
0.65
decays
by
factor
of
0.85
per
epoch","stochastic gradient descent||with||initial learning rate
initial learning rate||of||0.65
stochastic gradient descent||for||optimization
decays||by||factor
factor||of||0.85
factor||per||epoch
","stochastic gradient descent||has||decays
",,,,"Hyperparameters||used||stochastic gradient descent
",,,
experiments,We renormalize the gradient if its norm is greater than 5 .,"renormalize
gradient
if
norm
greater than
5","gradient||if||norm
norm||greater than||5
","renormalize||has||gradient
",,,,,,"Hyperparameters||has||renormalize
",
experiments,The mini - batch size was set to 40 .,"mini - batch size
set to
40","mini - batch size||set to||40
",,,,,,,"Hyperparameters||has||mini - batch size
",
experiments,The dimensions of the word embeddings were set to 150 for all models .,"dimensions
of
word embeddings
set to
150
for
all models","dimensions||of||word embeddings
dimensions||set to||150
150||for||all models
",,,,,,,"Hyperparameters||has||dimensions
",
experiments,The first one is a Kneser - Ney 5 - gram language model ( KN5 ) which generally serves as a non-neural baseline for the language modeling task .,"Kneser - Ney 5 - gram language model ( KN5 )
serves as
non-neural baseline
for
language modeling task","Kneser - Ney 5 - gram language model ( KN5 )||serves as||non-neural baseline
non-neural baseline||for||language modeling task
",,,,,,,"Baselines||has||Kneser - Ney 5 - gram language model ( KN5 )
",
experiments,The gated - feedback LSTM has feedback gates connecting the hidden states across multiple time steps as an adaptive control of the information flow .,"gated - feedback LSTM
feedback gates
connecting
hidden states
across
multiple time steps
as
adaptive control
of
information flow","feedback gates||connecting||hidden states
hidden states||across||multiple time steps
hidden states||as||adaptive control
adaptive control||of||information flow
","gated - feedback LSTM||has||feedback gates
",,,,,,"Baselines||has||gated - feedback LSTM
",
experiments,The depth - gated LSTM uses a depth gate to connect memory cells of vertically adjacent layers .,"depth - gated LSTM
uses
depth gate
to connect
memory cells
of
vertically adjacent layers","depth - gated LSTM||uses||depth gate
depth gate||to connect||memory cells
memory cells||of||vertically adjacent layers
",,,,,,,"Baselines||has||depth - gated LSTM
",
experiments,"Amongst all deep architectures , the three - layer LSTMN also performs best .","Amongst
all deep architectures
three - layer LSTMN
performs
best","three - layer LSTMN||performs||best
","all deep architectures||has||three - layer LSTMN
",,,,"Results||Amongst||all deep architectures
",,,
experiments,Sentiment Analysis,Sentiment Analysis,,,,,,,,"Tasks||has||Sentiment Analysis
","Sentiment Analysis||has||Baselines
"
experiments,We used pretrained 300 - D Glove 840B vectors to initialize the word embeddings .,"used
pretrained 300 - D Glove 840B vectors
to initialize
word embeddings","pretrained 300 - D Glove 840B vectors||to initialize||word embeddings
",,,,,"Hyperparameters||used||pretrained 300 - D Glove 840B vectors
",,,
experiments,"The gradient for words with Glove embeddings , was scaled by 0.35 in the first epoch after which all word embeddings were updated normally .","gradient
for
words
with
Glove embeddings
scaled by
0.35
in
first epoch
after
all word embeddings
updated
normally","first epoch||after||all word embeddings
all word embeddings||updated||normally
gradient||for||words
words||with||Glove embeddings
gradient||scaled by||0.35
0.35||in||first epoch
first epoch||after||all word embeddings
all word embeddings||updated||normally
",,,,,,,"Hyperparameters||has||gradient
",
experiments,"We used Adam ( Kingma and Ba , 2015 ) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively .","Adam ( Kingma and Ba , 2015 )
for
optimization
with
two momentum parameters
set to
0.9 and 0.999","two momentum parameters||set to||0.9 and 0.999
Adam ( Kingma and Ba , 2015 )||for||optimization
Adam ( Kingma and Ba , 2015 )||for||optimization
optimization||with||two momentum parameters
two momentum parameters||set to||0.9 and 0.999
",,,,,,,"Hyperparameters||used||Adam ( Kingma and Ba , 2015 )
",
experiments,The initial learning rate was set to 2E - 3 .,"initial learning rate
set to
2E - 3","initial learning rate||set to||2E - 3
",,,,,,,"Hyperparameters||has||initial learning rate
",
experiments,The regularization constant was 1E - 4 and the mini-batch size was 5 .,"regularization constant
was
1E - 4
mini-batch size
was
5","regularization constant||was||1E - 4
mini-batch size||was||5
",,,,,,,"Hyperparameters||has||regularization constant
Hyperparameters||has||mini-batch size
",
experiments,A dropout rate of 0.5 was applied to the neural network classifier .,"dropout rate
of
0.5
applied to
neural network classifier","dropout rate||of||0.5
dropout rate||applied to||neural network classifier
",,,,,,,"Hyperparameters||has||dropout rate
",
experiments,"Most of these models ( including ours ) are LSTM variants ( third block in , recursive neural networks ( first block ) , or convolutional neural networks ( CNNs ; second block ) .","are
LSTM variants",,,,,,"Baselines||are||LSTM variants
",,,
experiments,"For comparison , we also report the performance of the paragraph vector model ( PV ; ; see , second block ) which neither operates on trees nor sequences but learns distributed document representations parameterized directly .","report
performance
of
paragraph vector model","performance||of||paragraph vector model
",,,,,"Baselines||report||performance
",,,
experiments,The results in show that both 1 - and 2 - layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art .,"show that
both 1 - and 2 - layer LSTMNs
outperform
LSTM baselines",,"both 1 - and 2 - layer LSTMNs||has||outperform
outperform||has||LSTM baselines
",,,,"Results||show that||both 1 - and 2 - layer LSTMNs
",,,
experiments,On the fine - grained and binary classification tasks our 2 - layer LSTMN performs close to the best system T -. shows examples of intra-attention for sentiment words .,"On
fine - grained and binary classification tasks
our 2 - layer LSTMN
performs
close
to
best system","our 2 - layer LSTMN||performs||close
close||to||best system
","fine - grained and binary classification tasks||has||our 2 - layer LSTMN
",,,,"Results||On||fine - grained and binary classification tasks
",,,
experiments,Natural Language Inference,Natural Language Inference,,,,,,,,"Tasks||has||Natural Language Inference
","Natural Language Inference||has||Experimental setup
"
experiments,We used pre-trained 300 - D Glove 840B vectors to initialize the word embeddings .,"used
pre-trained 300 - D Glove 840B vectors
to initialize
word embeddings","pre-trained 300 - D Glove 840B vectors||to initialize||word embeddings
",,,,,"Experimental setup||used||pre-trained 300 - D Glove 840B vectors
",,,
experiments,"Out - of - vocabulary ( OOV ) words were initialized randomly with Gaussian samples ( = 0 , ?= 1 ) .","Out - of - vocabulary ( OOV ) words
initialized
randomly
with
Gaussian samples","Out - of - vocabulary ( OOV ) words||initialized||randomly
randomly||with||Gaussian samples
",,,,,,,"Experimental setup||used||Out - of - vocabulary ( OOV ) words
",
experiments,"We only updated OOV vectors in the first epoch , after which all word embeddings were updated normally .","updated
OOV vectors
in
first epoch
after
all word embeddings
updated
normally","OOV vectors||in||first epoch
",,,,,"Experimental setup||updated||OOV vectors
",,,
experiments,"The dropout rate was selected from [ 0.1 , 0.2 , 0.3 , 0.4 ] .","dropout rate
selected from
[ 0.1 , 0.2 , 0.3 , 0.4 ]","dropout rate||selected from||[ 0.1 , 0.2 , 0.3 , 0.4 ]
",,,,,,,"Experimental setup||has||dropout rate
",
experiments,"We used Adam ( Kingma and Ba , 2015 ) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively , and the initial learning rate set to 1E - 3 .","Adam ( Kingma and Ba , 2015 )
for
optimization
with
two momentum parameters
set to
0.9 and 0.999
initial learning rate
set to
1E - 3","Adam ( Kingma and Ba , 2015 )||with||initial learning rate
initial learning rate||set to||1E - 3
Adam ( Kingma and Ba , 2015 )||with||two momentum parameters
",,,,,,,"Experimental setup||used||Adam ( Kingma and Ba , 2015 )
",
experiments,The mini- batch size was set to 16 or 32 .,"mini- batch size
set to
16 or 32","mini- batch size||set to||16 or 32
",,,,,,,"Experimental setup||used||mini- batch size
",
experiments,"Specifically , these include a model which encodes the premise and hypothesis independently with two LSTMs , a shared LSTM ( Rocktschel et al. , 2016 ) , a word - by - word attention model , and a matching LSTM ( m LSTM ; ) .","shared LSTM ( Rocktschel et al. , 2016 )
word - by - word attention model
matching LSTM ( m LSTM ; )",,,,,,,,"Baselines||has||shared LSTM ( Rocktschel et al. , 2016 )
Baselines||has||word - by - word attention model
Baselines||has||matching LSTM ( m LSTM ; )
",
experiments,We also compared our models with a bag - of - words baseline which averages the pre-trained embeddings for the words in each sentence and concatenates them to create features for a logistic regression classifier ( first block in ) .,"compared
bag - of - words baseline",,,,,,"Baselines||compared||bag - of - words baseline
",,,
experiments,LSTMNs achieve better performance compared Models,"LSTMNs
achieve
better performance","LSTMNs||achieve||better performance
",,,,,,,"Results||has||LSTMNs
",
experiments,"We also observe that fusion is generally beneficial , and that deep fusion slightly improves over shallow fusion .","observe
fusion
is
generally beneficial
deep fusion
slightly improves
over
shallow fusion","fusion||is||generally beneficial
slightly improves||over||shallow fusion
","deep fusion||has||slightly improves
",,,,"Results||observe||fusion
Results||observe||deep fusion
",,,
experiments,"With standard training , our deep fusion yields the state - of - the - art performance in this task .","With
standard training
deep fusion
yields
state - of - the - art performance","deep fusion||yields||state - of - the - art performance
","standard training||has||deep fusion
",,,,"Results||With||standard training
",,,
research-problem,Read + Verify : Machine Reading Comprehension with Unanswerable Questions,Machine Reading Comprehension,,,,,"Contribution||has research problem||Machine Reading Comprehension
",,,,
model,"To address the above issue , we propose a read - then - verify system that aims to be robust to unanswerable questions in this paper .","propose
read - then - verify system
aims to be
robust
to
unanswerable questions","read - then - verify system||aims to be||robust
robust||to||unanswerable questions
",,"Model||propose||read - then - verify system
",,,,,,
model,"As shown in , our system consists of two components : ( 1 ) a no-answer reader for extracting candidate answers and detecting unanswerable questions , and ( 2 ) an answer verifier for deciding whether or not the extracted candidate is legitimate .","our system
consists of
two components
no-answer reader
for extracting
candidate answers
detecting
unanswerable questions
answer verifier
for deciding
extracted candidate
is
legitimate","our system||consists of||two components
no-answer reader||for extracting||candidate answers
no-answer reader||detecting||unanswerable questions
answer verifier||for deciding||extracted candidate
extracted candidate||is||legitimate
","two components||has||no-answer reader
two components||has||answer verifier
",,"Model||has||our system
",,,,,
model,"First , we augment existing readers with two auxiliary losses , to better handle answer extraction and no - answer detection respectively .","augment
existing readers
with
two auxiliary losses","existing readers||with||two auxiliary losses
",,"Model||augment||existing readers
",,,,,,
model,We solve this problem by introducing an independent span loss that aims to concentrate on the answer extraction task regardless of the answerability of the question .,"introducing
independent span loss
aims to
concentrate
on
answer extraction task","independent span loss||aims to||concentrate
concentrate||on||answer extraction task
",,"Model||introducing||independent span loss
",,,,,,
model,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .","with
leverage
multi-head pointer network
to generate
two pairs of span scores
where
one pair
is
normalized
no -answer score
other
used for
auxiliary loss","multi-head pointer network||to generate||two pairs of span scores
two pairs of span scores||where||other
other||used for||auxiliary loss
two pairs of span scores||where||one pair
one pair||is||normalized
normalized||with||no -answer score
",,"Model||leverage||multi-head pointer network
",,,,,,
model,"Besides , we present another independent noanswer loss to further alleviate the confliction , by focusing on the no-answer detection task without considering the shared normalization of answer extraction .","present
another independent noanswer loss
to further alleviate
confliction
focusing on
no-answer detection task
without
shared normalization
of
answer extraction","another independent noanswer loss||to further alleviate||confliction
another independent noanswer loss||focusing on||no-answer detection task
no-answer detection task||without||shared normalization
shared normalization||of||answer extraction
",,"Model||present||another independent noanswer loss
",,,,,,
model,"Second , in addition to the standard reading phase , we introduce an additional answer verifying phase , which aims at finding local entailment that supports the answer by comparing the answer sentence with the question .","introduce
additional answer verifying phase
aims at
finding
local entailment
supports
answer","additional answer verifying phase||aims at||finding
local entailment||supports||answer
","finding||has||local entailment
","Model||introduce||additional answer verifying phase
",,,,,,
model,"Inspired by recent advances in natural language inference ( NLI ) , we investigate three different architectures for the answer verifying task .","investigate
three different architectures
for
answer verifying task","three different architectures||for||answer verifying task
",,"Model||investigate||three different architectures
",,,,,,
model,"The first one is a sequential model that takes two sentences as along sequence , while the second one attempts to capture interactions between two sentences .","first one
is
sequential model
takes
two sentences
as
along sequence
second one
capture
interactions
between
two sentences","first one||is||sequential model
sequential model||takes||two sentences
two sentences||as||along sequence
second one||capture||interactions
interactions||between||two sentences
",,,,,,,"three different architectures||has||first one
three different architectures||has||second one
",
model,The last one is a hybrid model that combines the above two models to test if the performance can be further improved .,"last one
is
hybrid model
that combines
above two models","last one||is||hybrid model
hybrid model||that combines||above two models
",,,,,,,"three different architectures||has||last one
",
experimental-setup,"We run a grid search on ? and ? among [ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ] .","run
grid search
among
[ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ]","grid search||among||[ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ]
",,"Experimental setup||run||grid search
",,,,,,
experimental-setup,"As for answer verifiers , we use the original configuration from for Model - I. For Model - II , the Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 0.0008 is used , the hidden size is set as 300 , and a dropout ) of 0.3 is applied for preventing overfitting .","For
Model - II
Adam optimizer ( Kingma and Ba 2014 )
with
learning rate
of
0.0008
hidden size
set as
300
dropout
of
0.3
applied for
preventing overfitting","Adam optimizer ( Kingma and Ba 2014 )||with||learning rate
learning rate||of||0.0008
dropout||of||0.3
0.3||applied for||preventing overfitting
hidden size||set as||300
","Model - II||has||Adam optimizer ( Kingma and Ba 2014 )
Model - II||has||dropout
Model - II||has||hidden size
","Experimental setup||For||Model - II
",,,,,,
experimental-setup,"The batch size is 48 for the reader , 64 for Model - II , and 32 for Model - I as well as Model - III .","batch size
is
48
for
reader
64
for
Model - II
32
for
Model - I
Model - III","batch size||is||48
48||for||reader
batch size||is||64
64||for||Model - II
batch size||is||32
32||for||Model - I
32||for||Model - III
",,,"Experimental setup||has||batch size
",,,,,
experimental-setup,"We use the Glo Ve 100D embeddings for the reader , and 300D embeddings for Model - II and Model - III .","use
Glo Ve 100D embeddings
for
reader
300D embeddings
for
Model - II
Model - III","300D embeddings||for||Model - II
300D embeddings||for||Model - III
Glo Ve 100D embeddings||for||reader
",,"Experimental setup||use||300D embeddings
Experimental setup||use||Glo Ve 100D embeddings
",,,,,,
experimental-setup,"We utilize the nltk tokenizer 3 to preprocess passages and questions , as well as split sentences .","utilize
nltk tokenizer
to preprocess
passages and questions
split
sentences","nltk tokenizer||split||sentences
nltk tokenizer||to preprocess||passages and questions
",,"Experimental setup||utilize||nltk tokenizer
",,,,,,
results,"As we can see , our system obtains state - of the - art results by achieving an EM score of 71.7 and a F 1 score of 74.2 on the test set .","our system
obtains
state - of the - art results
by achieving
EM score
of
71.7
F 1 score
of
74.2
on
test set","our system||obtains||state - of the - art results
state - of the - art results||on||test set
test set||by achieving||F 1 score
F 1 score||of||74.2
test set||by achieving||EM score
EM score||of||71.7
",,,"Results||has||our system
",,,,,
results,Notice that SLQA + has reached a comparable result compared to our approach .,"Notice that
SLQA +
reached
comparable result
compared to
our approach","SLQA +||reached||comparable result
comparable result||compared to||our approach
",,"Results||Notice that||SLQA +
",,,,,,
ablation-analysis,"Removing the independent span loss ( indep - I ) results in a performance drop for all answerable questions ( HasAns ) , indicating that this loss helps the model in better identifying the answer boundary .","Removing
independent span loss ( indep - I )
results in
performance drop
for
all answerable questions ( HasAns )","independent span loss ( indep - I )||results in||performance drop
performance drop||for||all answerable questions ( HasAns )
",,"Ablation analysis||Removing||independent span loss ( indep - I )
",,,,,,
ablation-analysis,"Ablating independent no - answer loss ( indep - II ) , on the other hand , causes little influence on HasAns , but leads to a severe decline on no - answer accuracy ( NoAns ACC ) .","Ablating
independent no - answer loss ( indep - II )
on
causes
little influence
on
HasAns
leads to
severe decline
no - answer accuracy ( NoAns ACC )","independent no - answer loss ( indep - II )||leads to||severe decline
severe decline||on||no - answer accuracy ( NoAns ACC )
independent no - answer loss ( indep - II )||causes||little influence
little influence||on||HasAns
",,"Ablation analysis||Ablating||independent no - answer loss ( indep - II )
",,,,,,
ablation-analysis,"Finally , deleting both of two losses causes a degradation of more than 1.5 points on the over all performance in terms of F1 , with or without ELMo embeddings .","deleting
both of two losses
causes
degradation
of
more than 1.5 points
on
over all performance
in terms of
F1
with or without
ELMo embeddings","both of two losses||causes||degradation
degradation||of||more than 1.5 points
more than 1.5 points||on||over all performance
over all performance||with or without||ELMo embeddings
over all performance||in terms of||F1
",,"Ablation analysis||deleting||both of two losses
",,,,,,
ablation-analysis,"Adding ELMo embeddings , however , does not boost the performance .","Adding
ELMo embeddings
does not
boost
performance","ELMo embeddings||does not||boost
","boost||has||performance
","Ablation analysis||Adding||ELMo embeddings
",,,,,,
ablation-analysis,We find that the improvement on noanswer accuracy is significant .,"find that
improvement
on
noanswer accuracy
is
significant","improvement||is||significant
improvement||on||noanswer accuracy
",,"Ablation analysis||find that||improvement
",,,,,,
ablation-analysis,We observe that RMR + ELMo + Verifier achieves the best precision when the recall is less than 80 .,"observe
RMR + ELMo + Verifier
achieves
best precision
when
recall
is
less than 80","RMR + ELMo + Verifier||achieves||best precision
best precision||when||recall
recall||is||less than 80
",,"Ablation analysis||observe||RMR + ELMo + Verifier
",,,,,,
ablation-analysis,"Ablating two auxiliary losses , however , leads to an over all degradation on the curve , but it still outperforms the baseline by a large margin .","two auxiliary losses
leads to
over all degradation
on
curve
outperforms
baseline
by
large margin","two auxiliary losses||leads to||over all degradation
over all degradation||on||curve
two auxiliary losses||leads to||outperforms
outperforms||by||large margin
","outperforms||has||baseline
",,"Ablation analysis||Ablating||two auxiliary losses
",,,,,
research-problem,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,QUESTION ANSWERING,,,,,"Contribution||has research problem||QUESTION ANSWERING
",,,,
research-problem,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",question answering when reasoning over multiple facts,,,,,"Contribution||has research problem||question answering when reasoning over multiple facts
",,,,
model,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .","Query - Reduction Network 1 ( QRN )
is
single recurrent unit
that addresses
long - term dependency problem
of
most RNN - based models
by simplifying
recurrent update
taking
advantage
of
RNN 's capability
to model
sequential data","Query - Reduction Network 1 ( QRN )||is||single recurrent unit
single recurrent unit||that addresses||long - term dependency problem
long - term dependency problem||by simplifying||recurrent update
long - term dependency problem||of||most RNN - based models
long - term dependency problem||taking||advantage
advantage||of||RNN 's capability
RNN 's capability||to model||sequential data
",,,"Model||has||Query - Reduction Network 1 ( QRN )
",,,,,
model,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .","considers
context sentences
as
sequence
of
state - changing triggers
transforms
original query
to
more informed query
observes
through time","context sentences||as||sequence
sequence||of||state - changing triggers
state - changing triggers||observes||through time
through time||transforms||original query
original query||to||more informed query
",,,,,"Query - Reduction Network 1 ( QRN )||considers||context sentences
",,,
model,"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .","better encodes
locality information
query updates
performed
locally","query updates||performed||locally
","locality information||has||query updates
",,,,"Query - Reduction Network 1 ( QRN )||better encodes||locality information
",,,
hyperparameters,We withhold 10 % of the training for development .,"withhold
10 %
of
training
for
development","10 %||of||training
10 %||for||development
",,"Hyperparameters||withhold||10 %
",,,,,,
hyperparameters,We use the hidden state size of 50 by deafult .,"use
hidden state size
of
50
by
deafult","hidden state size||of||50
50||by||deafult
",,"Hyperparameters||use||hidden state size
",,,,,,
hyperparameters,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .","Batch sizes
of
32
for
bAbI story - based QA 1k
bAb I dialog
DSTC2 dialog
128
for
bAbI QA 10 k","Batch sizes||of||128
128||for||bAbI QA 10 k
Batch sizes||of||32
32||for||bAbI story - based QA 1k
32||for||bAb I dialog
32||for||DSTC2 dialog
",,,"Hyperparameters||has||Batch sizes
",,,,,
hyperparameters,The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / ? d.,"weights
in
input and output modules
initialized with
zero mean","weights||in||input and output modules
input and output modules||initialized with||zero mean
",,,"Hyperparameters||has||weights
",,,"input and output modules||initialized with||standard deviation of 1 / ? d
",,
hyperparameters,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,"Forget bias
of
2.5
used for
update gates","Forget bias||of||2.5
2.5||used for||update gates
",,,"Hyperparameters||has||Forget bias
",,,,,
hyperparameters,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,"L2 weight decay
of
0.001 ( 0.0005 for QA 10 k )
used for
all weights","L2 weight decay||of||0.001 ( 0.0005 for QA 10 k )
0.001 ( 0.0005 for QA 10 k )||used for||all weights
",,,"Hyperparameters||has||L2 weight decay
",,,,,
hyperparameters,The loss function is the cross entropy between v and the one - hot vector of the true answer .,"loss function
is
cross entropy
between
v and the one - hot vector
of
true answer","loss function||is||cross entropy
cross entropy||between||v and the one - hot vector
v and the one - hot vector||of||true answer
",,,"Hyperparameters||has||loss function
",,,,,
hyperparameters,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .","loss
is
minimized by
stochastic gradient descent
for
maximally 500 epochs
training
early stopped
if
loss
on
development data
not decrease
for
50 epochs","loss||minimized by||stochastic gradient descent
stochastic gradient descent||for||maximally 500 epochs
training||is||early stopped
early stopped||if||loss
not decrease||for||50 epochs
loss||on||development data
","loss||has||not decrease
",,"Hyperparameters||has||loss
Hyperparameters||has||training
",,,,,
hyperparameters,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,"learning rate
controlled by
AdaGrad
with
initial learning rate
of
0.5 ( 0.1 for QA 10 k )","learning rate||controlled by||AdaGrad
AdaGrad||with||initial learning rate
initial learning rate||of||0.5 ( 0.1 for QA 10 k )
",,,"Hyperparameters||has||learning rate
",,,,,
hyperparameters,"Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .","repeat
each training procedure
10 times ( 50 times for 10 k )
with
new random initialization
of
weights","10 times ( 50 times for 10 k )||with||new random initialization
new random initialization||of||weights
","each training procedure||has||10 times ( 50 times for 10 k )
","Hyperparameters||repeat||each training procedure
",,,,,,
baselines,"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .","include
LSTM
End - to - end Memory Networks ( N2N )
Dynamic Memory Networks ( DMN + )
Gated End - to - end Memory Networks ( GMe m N2N )
Differentiable Neural Computer ( DNC )",,,"Baselines||include||LSTM
Baselines||include||End - to - end Memory Networks ( N2N )
Baselines||include||Dynamic Memory Networks ( DMN + )
Baselines||include||Gated End - to - end Memory Networks ( GMe m N2N )
Baselines||include||Differentiable Neural Computer ( DNC )
",,,,,,
results,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .","In
1 k data
QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )
outperforms
all other models
by
large margin ( 2.8 + % )","QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )||outperforms||all other models
all other models||by||large margin ( 2.8 + % )
","1 k data||has||QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )
","Results||In||1 k data
",,,,,,
results,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .","10 k dataset
average accuracy
of
QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model
outperforms
all previous models
by
large margin ( 2.5 + % )
achieving
nearly perfect score
of
99.7 %","average accuracy||of||QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model
QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model||outperforms||all previous models
all previous models||achieving||nearly perfect score
nearly perfect score||of||99.7 %
all previous models||by||large margin ( 2.5 + % )
","10 k dataset||has||average accuracy
",,"Results||In||10 k dataset
",,,,,
results,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,"QRN
outperforms
previous work
by
large margin ( 2.0 + % )","QRN||outperforms||previous work
previous work||by||large margin ( 2.0 + % )
",,,"Results||has||QRN
",,,,,
ablation-analysis,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .","When
number of layers
is
only one
model
lacks
reasoning capability","number of layers||is||only one
model||lacks||reasoning capability
","number of layers||has||model
","Ablation analysis||When||number of layers
",,,,,,
ablation-analysis,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .","In the case of
1 k dataset
when
too many layers
correctly training
model
becomes
increasingly difficult","1 k dataset||when||too many layers
too many layers||correctly training||model
too many layers||becomes||increasingly difficult
",,"Ablation analysis||In the case of||1 k dataset
",,,,,,
ablation-analysis,"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .","10 k dataset
many layers ( 6 ) and hidden dimensions ( 200 )
helps
reasoning","many layers ( 6 ) and hidden dimensions ( 200 )||helps||reasoning
","10 k dataset||has||many layers ( 6 ) and hidden dimensions ( 200 )
",,"Ablation analysis||In the case of||10 k dataset
",,,,,
ablation-analysis,( b ) Adding the reset gate helps .,"Adding
reset gate
helps",,"reset gate||has||helps
","Ablation analysis||Adding||reset gate
",,,,,,
ablation-analysis,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .","Including
vector gates
hurts
in
1 k datasets","hurts||in||1 k datasets
","vector gates||has||hurts
","Ablation analysis||Including||vector gates
",,,,,,
ablation-analysis,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .","vector gates
in
bAbI story - based QA 10 k dataset
sometimes
help","vector gates||in||bAbI story - based QA 10 k dataset
bAbI story - based QA 10 k dataset||sometimes||help
",,,"Ablation analysis||has||vector gates
",,,,,
ablation-analysis,"( d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .","Increasing
dimension
of
hidden state
to
100
in
dialog 's Task 6 ( DSTC2 )
helps","dimension||of||hidden state
hidden state||in||dialog 's Task 6 ( DSTC2 )
hidden state||to||100
","dialog 's Task 6 ( DSTC2 )||has||helps
","Ablation analysis||Increasing||dimension
",,,,,,
ablation-analysis,It can be hypothesized that a larger hidden state is required for real data . Parallelization .,"hypothesized that
larger hidden state
required for
real data","larger hidden state||required for||real data
",,"Ablation analysis||hypothesized that||larger hidden state
",,,,,,
research-problem,A Decomposable Attention Model for Natural Language Inference,Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,Natural language inference ( NLI ) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis .,Natural language inference ( NLI ),,,,,"Contribution||has research problem||Natural language inference ( NLI )
",,,,
research-problem,NLI is a central problem in language understanding ) and recently the large SNLI corpus of 570K sentence pairs was created for this task .,NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,"In contrast to existing approaches , our approach only relies on alignment and is fully computationally decomposable with respect to the input text .","relies on
alignment
fully computationally decomposable
with respect to
input text","fully computationally decomposable||with respect to||input text
",,"Model||relies on||alignment
","Model||has||fully computationally decomposable
",,,,,
model,"Given two sentences , where each word is repre-sented by an embedding vector , we first create a soft alignment matrix using neural attention .","Given
two sentences
each word
repre-sented by
embedding vector
create
soft alignment matrix
using
neural attention","two sentences||create||soft alignment matrix
soft alignment matrix||using||neural attention
each word||repre-sented by||embedding vector
","two sentences||has||each word
",,,,,,,
model,We then use the ( soft ) alignment to decompose the task into subproblems that are solved separately .,"use
( soft ) alignment
to decompose
task
into
subproblems","( soft ) alignment||to decompose||task
task||into||subproblems
",,,,,"two sentences||use||( soft ) alignment
",,,
model,"Finally , the results of these subproblems are merged to produce the final classification .","results
of
subproblems
merged
to produce
final classification","results||of||subproblems
merged||to produce||final classification
","results||has||merged
",,,,,,"two sentences||has||results
",
model,"In addition , we optionally apply intra-sentence attention to endow the model with a richer encoding of substructures prior to the alignment step .","apply
intra-sentence attention
to endow
model
with
richer encoding
of
substructures
prior to
alignment step","intra-sentence attention||to endow||model
model||with||richer encoding
richer encoding||prior to||alignment step
richer encoding||of||substructures
",,"Model||apply||intra-sentence attention
",,,,,,
experimental-setup,The method was implemented in TensorFlow .,"implemented in
TensorFlow",,,"Experimental setup||implemented in||TensorFlow
",,,,,,
experimental-setup,We use 300 dimensional GloVe embeddings to represent words .,"use
300 dimensional GloVe embeddings
to represent
words","300 dimensional GloVe embeddings||to represent||words
",,"Experimental setup||use||300 dimensional GloVe embeddings
",,,,,,
experimental-setup,"Each embedding vector was normalized to have 2 norm of 1 and projected down to 200 dimensions , a number determined via hyperparameter tuning .","Each embedding vector
normalized
to have
2 norm of 1
projected down
to
200 dimensions","projected down||to||200 dimensions
normalized||to have||2 norm of 1
","Each embedding vector||has||projected down
Each embedding vector||has||normalized
",,"Experimental setup||has||Each embedding vector
",,,,,
experimental-setup,Out - of - vocabulary ( OOV ) words are hashed to one of 100 random embeddings each initialized to mean 0 and standard deviation 1 .,"Out - of - vocabulary ( OOV ) words
hashed to
one of 100 random embeddings
initialized to
mean 0
standard deviation 1","Out - of - vocabulary ( OOV ) words||hashed to||one of 100 random embeddings
one of 100 random embeddings||initialized to||mean 0
one of 100 random embeddings||initialized to||standard deviation 1
",,,"Experimental setup||has||Out - of - vocabulary ( OOV ) words
",,,,,
experimental-setup,All other parameter weights ( hidden layers etc. ) were initialized from random Gaussians with mean 0 and standard deviation 0.01 .,"other parameter weights ( hidden layers etc. )
initialized from
random Gaussians
with
mean 0
standard deviation 0.01","other parameter weights ( hidden layers etc. )||initialized from||random Gaussians
random Gaussians||with||mean 0
random Gaussians||with||standard deviation 0.01
",,,"Experimental setup||has||other parameter weights ( hidden layers etc. )
",,,,,
experimental-setup,"Each hyperparameter setting was run on a single machine with 10 asynchronous gradient - update threads , using Adagrad for optimization with the default initial accumulator value of 0.1 .","Each hyperparameter setting
run on
single machine
with
10 asynchronous gradient - update threads
using
Adagrad
for
optimization
with
default initial accumulator value
of
0.1","Each hyperparameter setting||using||Adagrad
Adagrad||with||default initial accumulator value
default initial accumulator value||of||0.1
Adagrad||for||optimization
Each hyperparameter setting||run on||single machine
single machine||with||10 asynchronous gradient - update threads
",,,"Experimental setup||has||Each hyperparameter setting
",,,,,
experimental-setup,"Dropout regularization was used for all ReLU layers , but not for the final linear layer .","Dropout regularization
used for
all ReLU layers
not for
final linear layer","Dropout regularization||used for||all ReLU layers
all ReLU layers||not for||final linear layer
",,,"Experimental setup||has||Dropout regularization
",,,,,
experimental-setup,"We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( 0.05 - vanilla , 0.025 - intra-attention ) .","dropout ratio
0.2
learning rate
0.05
vanilla
0.025
intra-attention",,"dropout ratio||has||0.2
learning rate||has||intra-attention
intra-attention||has||0.025
learning rate||has||vanilla
vanilla||has||0.05
",,"Experimental setup||has||dropout ratio
Experimental setup||has||learning rate
",,,,,
results,Our vanilla approach achieves state - of - theart results with almost an order of magnitude fewer parameters than the LSTMN of .,"Our vanilla approach
achieves
state - of - theart results
with
almost an order of magnitude fewer parameters
than
LSTMN","Our vanilla approach||achieves||state - of - theart results
state - of - theart results||with||almost an order of magnitude fewer parameters
almost an order of magnitude fewer parameters||than||LSTMN
",,,"Results||has||Our vanilla approach
",,,,,
results,Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art .,"Adding
intra-sentence attention
gives
considerable improvement
of
0.5 percentage points
over
existing state of the art","intra-sentence attention||gives||considerable improvement
considerable improvement||of||0.5 percentage points
0.5 percentage points||over||existing state of the art
",,"Results||Adding||intra-sentence attention
",,,,,,
research-problem,A Fast Unified Model for Parsing and Sentence Understanding,"Parsing
Sentence Understanding",,,,,"Contribution||has research problem||Parsing
Contribution||has research problem||Sentence Understanding
",,,,
model,"This paper introduces a new model to address both these issues : the Stack - augmented Parser - Interpreter Neural Network , or SPINN , shown in .","introduces
Stack - augmented Parser - Interpreter Neural Network
SPINN",,"Stack - augmented Parser - Interpreter Neural Network||name||SPINN
","Model||introduces||Stack - augmented Parser - Interpreter Neural Network
",,,,,,
model,"SPINN executes the computations of a tree - structured model in a linearized sequence , and can incorporate a neural network parser that produces the required parse structure on the fly .","SPINN
executes
computations
of
tree - structured model
in
linearized sequence
incorporate
neural network parser
that produces
required parse structure","SPINN||executes||computations
computations||in||linearized sequence
computations||of||tree - structured model
SPINN||incorporate||neural network parser
neural network parser||that produces||required parse structure
",,,"Model||has||SPINN
",,,,,
model,This design improves upon the TreeRNN architecture in three ways :,"improves upon
TreeRNN architecture",,,"Model||improves upon||TreeRNN architecture
",,,,,,
model,"At test time , it can simultaneously parse and interpret unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .","At
test time
can simultaneously
parse
interpret
unparsed sentences
removing
dependence
on
external parser","test time||can simultaneously||parse
test time||can simultaneously||interpret
test time||removing||dependence
dependence||on||external parser
","interpret||has||unparsed sentences
",,,,"TreeRNN architecture||At||test time
",,,
model,"Secondly , it supports batched computation for both parsed and unparsed sentences , yielding dramatic speedups over standard TreeRNNs .","supports
batched computation
for
parsed and unparsed sentences
yielding
dramatic speedups
over
standard TreeRNNs","batched computation||yielding||dramatic speedups
dramatic speedups||over||standard TreeRNNs
batched computation||for||parsed and unparsed sentences
",,,,,"TreeRNN architecture||supports||batched computation
",,,
model,"Finally , it supports a novel tree - sequence hybrid architecture for handling local linear context in sentence interpretation .","novel tree - sequence hybrid architecture
for handling
local linear context
in
sentence interpretation","novel tree - sequence hybrid architecture||for handling||local linear context
local linear context||in||sentence interpretation
",,,,,,,"TreeRNN architecture||supports||novel tree - sequence hybrid architecture
",
experimental-setup,Our optimized C ++/ CUDA models and the Theano source code for the full SPINN are available at https://github.com / stanfordnlp/spinn. 30 tokens or fewer .,"Our optimized C ++/ CUDA models
Theano source code
full SPINN
https://github.com / stanfordnlp/spinn.",,"full SPINN||has||Our optimized C ++/ CUDA models
full SPINN||has||Theano source code
",,"Experimental setup||has||full SPINN
","Contribution||Code||https://github.com / stanfordnlp/spinn.
",,,,
experimental-setup,We fix the model dimension D and the word embedding dimension at 300 .,"fix
model dimension D
word embedding dimension
at
300","fix||at||300
","300||has||model dimension D
300||has||word embedding dimension
",,"Experimental setup||has||fix
",,,,,
experimental-setup,We run the CPU performance test on a 2.20 GHz 16 core Intel Xeon E5-2660 processor with hyperthreading enabled .,"CPU performance test
on
2.20 GHz 16 core Intel Xeon E5-2660 processor
with
hyperthreading","CPU performance test||on||2.20 GHz 16 core Intel Xeon E5-2660 processor
2.20 GHz 16 core Intel Xeon E5-2660 processor||with||hyperthreading
",,,"Experimental setup||has||CPU performance test
",,,,,
experimental-setup,We test our thin - stack implementation and the RNN model on an NVIDIA Titan X GPU .,"test
thin - stack implementation
RNN model
on
NVIDIA Titan X GPU","NVIDIA Titan X GPU||test||thin - stack implementation
NVIDIA Titan X GPU||test||RNN model
",,"Experimental setup||on||NVIDIA Titan X GPU
",,,,,,
results,"We find that the bare SPINN - PI - NT model performs little better than the RNN baseline , but that SPINN - PI with the added tracking LSTM performs well .","find that
bare SPINN - PI - NT model
performs
little better
than
RNN baseline
SPINN - PI
with
added tracking LSTM
performs
well","SPINN - PI||with||added tracking LSTM
SPINN - PI||performs||well
bare SPINN - PI - NT model||performs||little better
little better||than||RNN baseline
",,"Results||find that||SPINN - PI
Results||find that||bare SPINN - PI - NT model
",,,,,,
results,"The full SPINN model with its relatively weak internal parser performs slightly less well , but nonetheless robustly exceeds the performance of the RNN baseline .","full SPINN model
with
relatively weak internal parser
performs
slightly less well
robustly exceeds
performance
of
RNN baseline","full SPINN model||with||relatively weak internal parser
full SPINN model||performs||slightly less well
performance||of||RNN baseline
","full SPINN model||has||robustly exceeds
robustly exceeds||has||performance
",,"Results||has||full SPINN model
",,,,,
results,Both SPINN - PI and the full SPINN significantly outperform all previous sentence - encoding models .,"SPINN - PI and the full SPINN
significantly outperform
previous sentence - encoding models",,"SPINN - PI and the full SPINN||has||significantly outperform
significantly outperform||has||previous sentence - encoding models
",,"Results||has||SPINN - PI and the full SPINN
",,,,,
results,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition for local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .","outperform
tree - based CNN
uses
tree - structured composition
for
local feature extraction
simpler pooling techniques
to build
sentence features","tree - based CNN||uses||tree - structured composition
tree - structured composition||for||local feature extraction
tree - based CNN||uses||simpler pooling techniques
simpler pooling techniques||to build||sentence features
","outperform||has||tree - based CNN
",,,,,,"SPINN - PI and the full SPINN||has||outperform
",
results,"Our results show that a model that uses tree - structured composition fully ( SPINN ) outper - forms one which uses it only partially ( tree - based CNN ) , which in turn outperforms one which does not use it at all ( RNN ) .","show that
model
that uses
tree - structured composition fully ( SPINN )
outper - forms
one
which
only partially ( tree - based CNN )
outperforms
not use it at all ( RNN )","model||that uses||tree - structured composition fully ( SPINN )
one||which||not use it at all ( RNN )
","tree - structured composition fully ( SPINN )||has||outper - forms
outper - forms||has||only partially ( tree - based CNN )
only partially ( tree - based CNN )||has||outperforms
outperforms||has||one
","Results||show that||model
",,,,,,
results,"The full SPINN performed moderately well at reproducing the Stanford Parser 's parses of the SNLI data at a transition - by - transition level , with 92.4 % accuracy at test time .","full SPINN
performed
moderately well
at reproducing
Stanford Parser 's parses
of
SNLI data
at
transition - by - transition level
with
92.4 % accuracy
at
test time","full SPINN||performed||moderately well
moderately well||at reproducing||Stanford Parser 's parses
Stanford Parser 's parses||at||transition - by - transition level
Stanford Parser 's parses||of||SNLI data
moderately well||with||92.4 % accuracy
92.4 % accuracy||at||test time
",,,"Results||has||full SPINN
",,,,,
research-problem,A Discrete Hard EM Approach for Weakly Supervised Question Answering,Weakly Supervised Question Answering,,,,,"Contribution||has research problem||Weakly Supervised Question Answering
",,,,
research-problem,Many question answering ( QA ) tasks only provide weak supervision for how the answer should be computed .,question answering ( QA ),,,,,"Contribution||has research problem||question answering ( QA )
",,,,
research-problem,"Despite its simplicity , we show that this approach significantly outperforms previous methods on six QA tasks , including absolute gains of 2 - 10 % , and achieves the stateof - the - art on five of them .",QA,,,,,"Contribution||has research problem||QA
",,,,
model,"In this paper , we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent - variable learning problems .","formulate
wide range of weakly supervised QA tasks
as
discrete latent - variable learning problems","wide range of weakly supervised QA tasks||as||discrete latent - variable learning problems
",,"Model||formulate||wide range of weakly supervised QA tasks
",,,,,,
,"We demonstrate that for many recently introduced tasks , which we group into three categories as given in , it is relatively easy to precompute a discrete , task - specific set of possible solutions that contains the correct solution along with a modest number of spurious options .","demonstrate
for
many recently introduced tasks
group into
three categories
relatively easy
to precompute
discrete , task - specific set of possible solutions
that contains
correct solution
along with
modest number of spurious options",,,,,,,,,
model,"The learning challenge is then to determine which solution in the set is the correct one , while estimating a complete QA model .","learning challenge
is
to determine
which solution
in
set
correct one
while estimating
complete QA model","learning challenge||to determine||which solution
which solution||in||set
which solution||is||correct one
correct one||while estimating||complete QA model
",,,"Model||has||learning challenge
",,,,,
model,"We model the set of possible solutions as a discrete latent variable , and develop a learning strategy that uses hard - EM - style parameter updates .","model
set of possible solutions
as
discrete latent variable
develop
learning strategy
uses
hard - EM - style parameter updates","set of possible solutions||as||discrete latent variable
learning strategy||uses||hard - EM - style parameter updates
",,"Model||model||set of possible solutions
Model||develop||learning strategy
",,,,,,
model,"This algorithm repeatedly ( i ) predicts the most likely solution according to the current model from the precomputed set , and ( ii ) updates the model parameters to further encourage its own prediction .","predicts
most likely solution
according to
current model
from
precomputed set
updates
model parameters
to further encourage
own prediction","model parameters||to further encourage||own prediction
most likely solution||according to||current model
current model||from||precomputed set
",,"Model||updates||model parameters
Model||predicts||most likely solution
",,,,,,
model,"Intuitively , these hard updates more strongly enforce our prior beliefs that there is a single correct solution .","hard updates
strongly enforce
prior beliefs
there is
single correct solution","prior beliefs||there is||single correct solution
","hard updates||has||strongly enforce
strongly enforce||has||prior beliefs
",,"Model||has||hard updates
",,,,,
experiments,Multi-mention Reading Comprehension,Multi-mention Reading Comprehension,,,,,,,,"Tasks||has||Multi-mention Reading Comprehension
","Multi-mention Reading Comprehension||has||Experimental setup
"
experiments,We use uncased version of BERT base .,"use
uncased version
of
BERT base","uncased version||of||BERT base
",,,,,"Experimental setup||use||uncased version
",,,
experiments,We use batch size of 20 for two reading comprehension tasks and 192 for two open - domain QA tasks .,"batch size
of
20
for
two reading comprehension tasks
192
for
two open - domain QA tasks","batch size||of||192
192||for||two open - domain QA tasks
batch size||of||20
20||for||two reading comprehension tasks
",,,,,,,"Experimental setup||use||batch size
",
experiments,"For opendomain QA tasks , we retrieve 50 Wikipedia articles through TF - IDF ( Chen et al. , 2017 ) and further run to retrieve 20 ( for train ) or 80 ( for development and test ) paragraphs .","For
opendomain QA tasks
retrieve
50 Wikipedia articles
through
TF - IDF","opendomain QA tasks||retrieve||50 Wikipedia articles
50 Wikipedia articles||through||TF - IDF
",,,,,"Experimental setup||For||opendomain QA tasks
",,,
experiments,"We try 10 , 20 , 40 and 80 paragraphs on the development set to choose the number of paragraphs to use on the test set .","try
10 , 20 , 40 and 80 paragraphs
on
development set
to choose
number of paragraphs
to use on
test set","10 , 20 , 40 and 80 paragraphs||to choose||number of paragraphs
number of paragraphs||to use on||test set
10 , 20 , 40 and 80 paragraphs||on||development set
",,,,,"Experimental setup||try||10 , 20 , 40 and 80 paragraphs
",,,
experiments,"To avoid local optima , we perform annealing : at training step t , the model optimizes on MML objective with a probability of min ( t / ? , 1 ) and otherwise use our objective , where ?","To avoid
local optima
perform
annealing","local optima||perform||annealing
",,,,,"Experimental setup||To avoid||local optima
",,,
experiments,"6 First of all , we observe that First - Only is a strong baseline across all the datasets .","observe
First - Only
is
strong baseline","First - Only||is||strong baseline
",,,,,"Results||observe||First - Only
",,,
experiments,"Second , while MML achieves comparable result to the First - Only baseline , our learning method outperforms others by 2 + F1 / ROUGE - L / EM consistently on all datasets .","MML
achieves
comparable result
to
First - Only baseline
our learning method
outperforms
by
2 + F1 / ROUGE - L / EM
on
all datasets","MML||achieves||comparable result
comparable result||to||First - Only baseline
outperforms||by||2 + F1 / ROUGE - L / EM
outperforms||on||all datasets
","our learning method||has||outperforms
",,,,,,"Results||has||MML
Results||has||our learning method
",
experiments,"Lastly , our method achieves the new state - of the - art on NARRATIVEQA , TRIVIAQA - OPEN and NATURALQUESTIONS - OPEN , and is comparable to the state - of - the - art on TRIVIAQA , despite our aggressive truncation of documents .","our method
achieves
new state - of the - art
on
NARRATIVEQA
TRIVIAQA - OPEN
NATURALQUESTIONS - OPEN
comparable
to
state - of - the - art
on
TRIVIAQA","our method||achieves||new state - of the - art
new state - of the - art||on||NARRATIVEQA
new state - of the - art||on||TRIVIAQA - OPEN
new state - of the - art||on||NATURALQUESTIONS - OPEN
our method||achieves||comparable
comparable||to||state - of - the - art
comparable||on||TRIVIAQA
",,,,,,,"Results||has||our method
",
experiments,is a hyperparameter .,,,,,,,,,,
research-problem,Gated Self - Matching Networks for Reading Comprehension and Question Answering,Reading Comprehension and Question Answering,,,,,"Contribution||has research problem||Reading Comprehension and Question Answering
",,,,
research-problem,"In this paper , we present the gated selfmatching networks for reading comprehension style question answering , which aims to answer questions from a given passage .",reading comprehension style question answering,,,,,"Contribution||has research problem||reading comprehension style question answering
",,,,
model,"Inspired by , we introduce a gated self - matching network , illustrated in , an end - to - end neural network model for reading comprehension and question answering .","introduce
gated self - matching network
end - to - end neural network model
for
reading comprehension and question answering","end - to - end neural network model||for||reading comprehension and question answering
","gated self - matching network||has||end - to - end neural network model
","Model||introduce||gated self - matching network
",,,,,,
model,Our model consists of four parts :,"consists of
four parts",,,"Model||consists of||four parts
",,,,,,"four parts||has||recurrent network encoder
four parts||has||gated matching layer
four parts||has||self - matching layer
four parts||has||pointernetwork based answer boundary prediction layer
"
model,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .","recurrent network encoder
to build
representation
for
questions and passages
gated matching layer
to match
question and passage
self - matching layer
to aggregate
information
from
whole passage
pointernetwork based answer boundary prediction layer","recurrent network encoder||to build||representation
representation||for||questions and passages
gated matching layer||to match||question and passage
self - matching layer||to aggregate||information
information||from||whole passage
",,,,,,,,
experimental-setup,We use the tokenizer from Stanford CoreNLP to preprocess each passage and question .,"use
tokenizer
from
Stanford CoreNLP
to preprocess
each passage and question","tokenizer||from||Stanford CoreNLP
tokenizer||to preprocess||each passage and question
",,"Experimental setup||use||tokenizer
",,,,,,
experimental-setup,The Gated Recurrent Unit variant of LSTM is used throughout our model .,"Gated Recurrent Unit variant
of
LSTM
used throughout
our model","Gated Recurrent Unit variant||of||LSTM
Gated Recurrent Unit variant||used throughout||our model
",,,"Experimental setup||has||Gated Recurrent Unit variant
",,,,,
experimental-setup,"For word embedding , we use pretrained case - sensitive GloVe embeddings 2 ( Pennington et al. , 2014 ) for both questions and passages , and it is fixed during training ; We use zero vectors to represent all out - of - vocab words .","For
word embedding
use
pretrained case - sensitive GloVe embeddings
for
questions and passages
fixed
during
training
zero vectors
to represent
all out - of - vocab words","word embedding||use||zero vectors
zero vectors||to represent||all out - of - vocab words
word embedding||use||pretrained case - sensitive GloVe embeddings
pretrained case - sensitive GloVe embeddings||for||questions and passages
fixed||during||training
","pretrained case - sensitive GloVe embeddings||has||fixed
","Experimental setup||For||word embedding
",,,,,,
experimental-setup,"We utilize 1 layer of bi-directional GRU to compute character - level embeddings and 3 layers of bi-directional GRU to encode questions and passages , the gated attention - based recurrent network for question and passage matching is also encoded bidirectionally in our experiment .","utilize
1 layer of bi-directional GRU
to compute
character - level embeddings
3 layers of bi-directional GRU
to encode
questions and passages
gated attention - based recurrent network
for
question and passage matching
encoded
bidirectionally","3 layers of bi-directional GRU||to encode||questions and passages
gated attention - based recurrent network||for||question and passage matching
gated attention - based recurrent network||encoded||bidirectionally
1 layer of bi-directional GRU||to compute||character - level embeddings
",,"Experimental setup||utilize||3 layers of bi-directional GRU
Experimental setup||utilize||gated attention - based recurrent network
Experimental setup||utilize||1 layer of bi-directional GRU
",,,,,,
experimental-setup,The hidden vector length is set to 75 for all layers .,"hidden vector length
set to
75
for
all layers","hidden vector length||set to||75
75||for||all layers
",,,"Experimental setup||use||hidden vector length
",,,,,
experimental-setup,The hidden size used to compute attention scores is also 75 .,"hidden size
to compute
attention scores
is
75","hidden size||to compute||attention scores
attention scores||is||75
",,,"Experimental setup||use||hidden size
",,,,,
experimental-setup,We also apply dropout between layers with a dropout rate of 0.2 .,"apply
dropout
between
layers
with
dropout rate
of
0.2","dropout||with||dropout rate
dropout rate||of||0.2
dropout||between||layers
",,"Experimental setup||apply||dropout
",,,,,,
experimental-setup,"The model is optimized with AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 1 .","model
optimized with
AdaDelta ( Zeiler , 2012 )
with
initial learning rate
of
1","model||optimized with||AdaDelta ( Zeiler , 2012 )
AdaDelta ( Zeiler , 2012 )||with||initial learning rate
initial learning rate||of||1
",,,"Experimental setup||has||model
",,,,,
experimental-setup,The ? and used in AdaDelta are 0.95 and 1e ? 6 respectively .,"used in
AdaDelta
are
0.95 and 1e ? 6","AdaDelta||are||0.95 and 1e ? 6
",,"Experimental setup||used in||AdaDelta
",,,,,,
ablation-analysis,attention - based recurrent network ( GARNN ) and self - matching attention mechanism positively contribute to the final results of gated self - matching networks .,"attention - based recurrent network ( GARNN )
self - matching attention mechanism
positively contribute
to
final results
of
gated self - matching networks","positively contribute||to||final results
final results||of||gated self - matching networks
","positively contribute||has||attention - based recurrent network ( GARNN )
positively contribute||has||self - matching attention mechanism
",,"Ablation analysis||has||positively contribute
",,,,,
ablation-analysis,"Removing self - matching results in 3.5 point EM drop , which reveals that information in the passage plays an important role .","Removing
self - matching
results in
3.5 point EM drop","self - matching||results in||3.5 point EM drop
",,"Ablation analysis||Removing||self - matching
",,,,,,
ablation-analysis,Characterlevel embeddings contribute towards the model 's performance since it can better handle out - ofvocab or rare words .,"Characterlevel embeddings
contribute towards
model 's performance
can
better handle
out - ofvocab or rare words","Characterlevel embeddings||can||better handle
Characterlevel embeddings||contribute towards||model 's performance
","better handle||has||out - ofvocab or rare words
",,"Ablation analysis||has||Characterlevel embeddings
",,,,,
ablation-analysis,Character - level embeddings are not utilized .,"Character - level embeddings
not
utilized","Character - level embeddings||not||utilized
",,,"Ablation analysis||has||Character - level embeddings
",,,,,
ablation-analysis,"As shown in , the gate introduced in question and passage matching layer is helpful for both GRU and LSTM on the SQuAD dataset .","gate
introduced in
question and passage matching layer
helpful
for
GRU and LSTM","gate||introduced in||question and passage matching layer
helpful||for||GRU and LSTM
","question and passage matching layer||has||helpful
",,"Ablation analysis||has||gate
",,,,,
research-problem,Commonsense for Generative Multi - Hop Question Answering Tasks,Generative Multi - Hop Question Answering,,,,,"Contribution||has research problem||Generative Multi - Hop Question Answering
",,,,
research-problem,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .",Reading comprehension QA,,,,,"Contribution||has research problem||Reading comprehension QA
",,,,
research-problem,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .",,,,,,,,,,
research-problem,"In this paper , we explore the task of machine reading comprehension ( MRC ) based QA .",machine reading comprehension ( MRC ) based QA,,,,,"Contribution||has research problem||machine reading comprehension ( MRC ) based QA
",,,,
code,"We publicly release all our code , models , and data at :",,,,,,,,,,
code,https://github.com/yicheng-w/CommonSenseMultiHopQA task tests a model 's natural language understanding capabilities by asking it to answer a question based on a passage of relevant content .,https://github.com/yicheng-w/CommonSenseMultiHopQA,,,,,"Contribution||Code||https://github.com/yicheng-w/CommonSenseMultiHopQA
",,,,
research-problem,"Much progress has been made in reasoning - based MRC - QA on the bAbI dataset , which contains questions that require the combination of multiple disjoint pieces of evidence in the context .",reasoning - based MRC - QA,,,,,"Contribution||has research problem||reasoning - based MRC - QA
",,,,
model,"In this paper , we first propose the Multi - Hop Pointer - Generator Model ( MHPGM ) , a strong baseline model that uses multiple hops of bidirectional attention , self - attention , and a pointer - generator decoder to effectively read and reason within along passage and synthesize a coherent response .","first propose
Multi - Hop Pointer - Generator Model ( MHPGM )
strong baseline model
uses
multiple hops
of
bidirectional attention
self - attention
pointer - generator decoder
to effectively
read and reason
within
along passage
synthesize
coherent response","strong baseline model||uses||multiple hops
multiple hops||of||bidirectional attention
multiple hops||of||self - attention
multiple hops||of||pointer - generator decoder
multiple hops||to effectively||read and reason
read and reason||within||along passage
multiple hops||to effectively||synthesize
","Multi - Hop Pointer - Generator Model ( MHPGM )||has||strong baseline model
synthesize||has||coherent response
","Model||first propose||Multi - Hop Pointer - Generator Model ( MHPGM )
",,,,,,
model,"Next , to address the issue that understanding human - generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense ( background ) knowledge , we present an algorithm for selecting useful , grounded multi-hop relational knowledge paths from ConceptNet ) via a pointwise mutual information ( PMI ) and term - frequency - based scoring function .","present
algorithm
for selecting
useful , grounded multi-hop relational knowledge paths
from
ConceptNet
via
pointwise mutual information ( PMI )
term - frequency - based scoring function","algorithm||for selecting||useful , grounded multi-hop relational knowledge paths
useful , grounded multi-hop relational knowledge paths||from||ConceptNet
useful , grounded multi-hop relational knowledge paths||via||pointwise mutual information ( PMI )
useful , grounded multi-hop relational knowledge paths||via||term - frequency - based scoring function
",,"Model||present||algorithm
",,,,,,
model,"We then present a novel method of inserting these selected commonsense paths between the hops of document - context reasoning within our model , via the Necessary and Optional Information Cell ( NOIC ) , which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference .","novel method
of
inserting
selected commonsense paths
between
hops
of
document - context reasoning
within
our model
via
Necessary and Optional Information Cell ( NOIC )
employs
selectivelygated attention mechanism
that utilizes
commonsense information
to effectively fill in
gaps of inference","novel method||of||inserting
selected commonsense paths||between||hops
hops||of||document - context reasoning
document - context reasoning||within||our model
selected commonsense paths||via||Necessary and Optional Information Cell ( NOIC )
Necessary and Optional Information Cell ( NOIC )||employs||selectivelygated attention mechanism
selectivelygated attention mechanism||that utilizes||commonsense information
commonsense information||to effectively fill in||gaps of inference
","inserting||has||selected commonsense paths
",,"Model||present||novel method
",,,,,
results,"We see empirically that our model outperforms all generative models on NarrativeQA , and is competitive with the top span prediction models .","see empirically that
our model
outperforms
all generative models
on
NarrativeQA
competitive
with
top span prediction models","competitive||with||top span prediction models
all generative models||on||NarrativeQA
","our model||has||competitive
our model||has||outperforms
outperforms||has||all generative models
","Results||see empirically that||our model
",,,,,,
results,"Furthermore , with the NOIC commonsense integration , we were able to further improve performance ( p < 0.001 on all metrics 5 ) , establishing a new state - of - the - art for the task .","with
NOIC commonsense integration
able to
further improve
performance
establishing
new state - of - the - art
for
task","NOIC commonsense integration||able to||further improve
performance||establishing||new state - of - the - art
new state - of - the - art||for||task
","further improve||has||performance
","Results||with||NOIC commonsense integration
",,,,,,
results,"We also see that our model performs reasonably well on WikiHop , and further achieves promising initial improvements via the addition of commonsense , hinting at the generalizability of our approaches .","see that
our model
performs
reasonably well
on
WikiHop
achieves
promising initial improvements
via
addition
of
commonsense
hinting at
generalizability
of
our approaches","our model||performs||reasonably well
reasonably well||on||WikiHop
our model||achieves||promising initial improvements
promising initial improvements||via||addition
addition||of||commonsense
commonsense||hinting at||generalizability
generalizability||of||our approaches
",,"Results||see that||our model
",,,,,,
results,"We speculate that the improvement is smaller on Wikihop because only approximately 11 % of WikiHop data points require commonsense and because WikiHop data requires more fact - based commonsense ( e.g. , from Freebase ) as opposed to semantics - based commonsense ( e.g. , from Con-ceptNet ( Speer and Havasi , 2012 ) ) .","speculate
improvement
smaller
on
Wikihop","smaller||on||Wikihop
","improvement||has||smaller
","Results||speculate||improvement
",,,,,,
ablation-analysis,Experiment 1 and 5 are our models presented in were also important for the model 's performance and that self - attention is able to contribute significantly to performance on top of other components of the model .,"performance
self - attention
able to contribute
significantly
to
on top of
other components of the model","self - attention||able to contribute||significantly
significantly||to||performance
performance||on top of||other components of the model
",,,"Ablation analysis||has||self - attention
",,,,,
ablation-analysis,"Finally , we see that effectively introducing external knowledge via our commonsense selection algorithm and NOIC can improve performance even further on top of our strong baseline .","see that
effectively introducing
external knowledge
via
our commonsense selection algorithm
NOIC
improve
performance
on top of
our strong baseline","performance||on top of||our strong baseline
performance||via||our commonsense selection algorithm
performance||via||NOIC
","effectively introducing||has||external knowledge
external knowledge||has||improve
improve||has||performance
","Ablation analysis||see that||effectively introducing
",,,,,,
ablation-analysis,"The results of these are shown in , where we see that neither NumberBatch nor random - relationships nor single - hop common - sense offer statistically significant improvements 7 , whereas our commonsense selection and incorporation mechanism improves performance significantly across all metrics .","our commonsense selection and incorporation mechanism
improves
performance significantly
across
all metrics","performance significantly||across||all metrics
","our commonsense selection and incorporation mechanism||has||improves
improves||has||performance significantly
",,"Ablation analysis||see that||our commonsense selection and incorporation mechanism
",,,,,
research-problem,Multi - Passage Machine Reading Comprehension with Cross - Passage Answer Verification,Multi - Passage Machine Reading Comprehension,,,,,"Contribution||has research problem||Multi - Passage Machine Reading Comprehension
",,,,
research-problem,Machine reading comprehension ( MRC ) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine .,Machine reading comprehension ( MRC ),,,,,"Contribution||has research problem||Machine reading comprehension ( MRC )
",,,,
research-problem,"Compared with MRC on a single passage , multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages .",MRC,,,,,"Contribution||has research problem||MRC
",,,,
model,"The over all framework of our model is demonstrated in , which consists of three modules .","of
consists of
three modules",,,"Model||consists of||three modules
",,,,,,
model,"First , we follow the boundary - based MRC models to find an answer candidate for each passage by identifying the start and end position of the answer ( .","follow
boundary - based MRC models
to find
answer candidate
for
each passage
by identifying
start and end position
answer","boundary - based MRC models||to find||answer candidate
answer candidate||by identifying||start and end position
answer candidate||for||each passage
","start and end position||of||answer
",,,,"three modules||follow||boundary - based MRC models
",,,
model,"Second , we model the meanings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second perspective .","model
meanings
of
answer candidates
extracted from
those passages
use
content scores
to measure
quality
of
candidates","content scores||to measure||quality
quality||of||candidates
meanings||of||answer candidates
answer candidates||extracted from||those passages
",,,,,"three modules||use||content scores
three modules||model||meanings
",,,
model,"Third , we conduct the answer verification by enabling each answer candidate to attend to the other candidates based on their representations .","conduct
answer verification
by enabling
each answer candidate
to attend
other candidates
based on
their representations","answer verification||by enabling||each answer candidate
each answer candidate||to attend||other candidates
other candidates||based on||their representations
",,,,,"three modules||conduct||answer verification
",,,
model,"Therefore , the final answer is determined by three factors : the boundary , the content and the crosspassage answer verification .","final answer
determined by
three factors
boundary
content
crosspassage answer verification","final answer||determined by||three factors
","three factors||has||boundary
three factors||has||content
three factors||has||crosspassage answer verification
",,"Model||has||final answer
",,,,,
model,"The three steps are modeled using different modules , which can be jointly trained in our end - to - end framework .","modeled using
different modules
can be
jointly trained
in
our end - to - end framework","different modules||can be||jointly trained
jointly trained||in||our end - to - end framework
",,,,,"three factors||modeled using||different modules
",,,
experimental-setup,"For MS - MARCO , we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP and we choose the span that achieves the highest ROUGE - L score with the reference answers as the gold span for training .","For
MS - MARCO
preprocess
corpus
with
reversible tokenizer
from
Stanford CoreNLP","MS - MARCO||preprocess||corpus
corpus||with||reversible tokenizer
reversible tokenizer||from||Stanford CoreNLP
",,"Experimental setup||For||MS - MARCO
",,,,,,
experimental-setup,We employ the 300 - D pre-trained Glove embeddings and keep it fixed during training .,"employ
300 - D pre-trained Glove embeddings
fixed
during
training","fixed||during||training
","300 - D pre-trained Glove embeddings||has||fixed
",,,,"MS - MARCO||employ||300 - D pre-trained Glove embeddings
",,,
experimental-setup,The character embeddings are randomly initialized with its dimension as 30 .,"character embeddings
are
randomly initialized
with
dimension
as
30","character embeddings||with||dimension
dimension||as||30
character embeddings||are||randomly initialized
",,,,,,,"MS - MARCO||has||character embeddings
",
results,Results on DuReader,"on
DuReader",,,"Results||on||DuReader
",,,,,,
results,We can see that this paragraph ranking can boost the BiDAF baseline significantly .,"see that
paragraph ranking
boost
BiDAF baseline
significantly","paragraph ranking||boost||BiDAF baseline
paragraph ranking||boost||significantly
",,,,,"DuReader||see that||paragraph ranking
",,,
results,"Finally , we implement our system based on this new strategy , and our system ( single model ) achieves further improvement by a large margin .","our system ( single model )
achieves
further improvement
by
large margin","our system ( single model )||achieves||further improvement
further improvement||by||large margin
",,,,,,,"DuReader||has||our system ( single model )
",
ablation-analysis,"From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .","see that
answer verification
makes
great contribution
to
over all improvement
confirms
our hypothesis
that
cross - passage answer verification
useful
for
multi-passage MRC","answer verification||makes||great contribution
great contribution||confirms||our hypothesis
our hypothesis||that||cross - passage answer verification
useful||for||multi-passage MRC
great contribution||to||over all improvement
","cross - passage answer verification||has||useful
","Ablation analysis||see that||answer verification
",,,,,,
ablation-analysis,"For the ablation of the content model , we analyze that it will not only affect the content score itself , but also violate the verification model since the content probabilities are necessary for the answer representation , which will be further analyzed in Section 4.3 .","For
ablation
of
content model
analyze
not only affect
content score itself
violate
verification model","ablation||of||content model
analyze||violate||verification model
analyze||not only affect||content score itself
","content model||has||analyze
","Ablation analysis||For||ablation
",,,,,,
ablation-analysis,"Another discovery is that jointly training the three models can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .","jointly training
three models
can
provide
great benefits
shows that
three tasks
are
closely related
boost
each other
with
shared representations
at
bottom layers","three models||provide||great benefits
great benefits||shows that||three tasks
three tasks||can||boost
each other||with||shared representations
shared representations||at||bottom layers
three tasks||are||closely related
","boost||has||each other
","Ablation analysis||jointly training||three models
",,,,,,
ablation-analysis,"At last , comparing our method with the baseline , we achieve an improvement of nearly 3 points without the yes / no classification .","achieve
improvement
of
nearly 3 points
without
yes / no classification","improvement||of||nearly 3 points
nearly 3 points||without||yes / no classification
",,"Ablation analysis||achieve||improvement
",,,,,,
research-problem,Swag : A Large - Scale Adversarial Dataset for Grounded Commonsense Inference,Grounded Commonsense Inference,,,,,"Contribution||has research problem||Grounded Commonsense Inference
",,,,
dataset,We use this method to construct Swag : an adversarial dataset with 113 k multiple - choice questions .,"construct
Swag
adversarial dataset
with
113 k multiple - choice questions","adversarial dataset||with||113 k multiple - choice questions
","Swag||has||adversarial dataset
","Dataset||construct||Swag
",,,,,,
dataset,"We start with pairs of temporally adjacent video captions , each with a context and a follow - up event that we know is physically possible .","start with
pairs
of
temporally adjacent video captions
each with
context
follow - up event
know
physically possible","pairs||of||temporally adjacent video captions
temporally adjacent video captions||each with||context
temporally adjacent video captions||each with||follow - up event
follow - up event||know||physically possible
",,"Dataset||start with||pairs
",,,,,,
dataset,We then use a state - of - theart language model fine - tuned on this data to massively oversample a diverse set of possible negative sentence endings ( or counterfactuals ) .,"use
state - of - theart language model
fine - tuned on
data
to massively oversample
diverse set
of
possible negative sentence endings ( or counterfactuals )","state - of - theart language model||to massively oversample||diverse set
diverse set||of||possible negative sentence endings ( or counterfactuals )
state - of - theart language model||fine - tuned on||data
",,"Dataset||use||state - of - theart language model
",,,,,,
dataset,"Next , we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones .","filter
aggressively and adversarially
using
committee of trained models
to obtain
population
of
de-biased endings
with
similar stylistic features
to
real ones","aggressively and adversarially||using||committee of trained models
committee of trained models||to obtain||population
population||of||de-biased endings
de-biased endings||with||similar stylistic features
similar stylistic features||to||real ones
",,,,,"possible negative sentence endings ( or counterfactuals )||filter||aggressively and adversarially
",,,
dataset,"Finally , these filtered counterfactuals are validated by crowd workers to further ensure data quality .","filtered counterfactuals
validated by
crowd workers
to further ensure
data quality","filtered counterfactuals||validated by||crowd workers
crowd workers||to further ensure||data quality
",,,"Dataset||has||filtered counterfactuals
",,,,,
results,"The best model that only uses the ending is the LSTM sequence model with ELMo embeddings , which obtains 43.6 % .","best model
only uses
ending
is
LSTM sequence model
with
ELMo embeddings
obtains
43.6 %","best model||only uses||ending
ending||is||LSTM sequence model
LSTM sequence model||obtains||43.6 %
LSTM sequence model||with||ELMo embeddings
",,,"Results||has||best model
",,,,,"LSTM sequence model||has||greatly improves
"
results,"This model , as with most models studied , greatly improves with more context : by 3.1 % when given the initial noun phrase , and by an ad-ditional 4 % when also given the first sentence .","with
greatly improves
more context
by
3.1 %
given
initial noun phrase
ad-ditional 4 %
given
first sentence","greatly improves||with||more context
greatly improves||by||ad-ditional 4 %
ad-ditional 4 %||given||first sentence
greatly improves||by||3.1 %
3.1 %||given||initial noun phrase
",,,,,,,,
results,Further improvement is gained from models that compute pairwise representations of the inputs .,"Further improvement
gained from
models
that compute
pairwise representations
of
inputs","Further improvement||gained from||models
models||that compute||pairwise representations
pairwise representations||of||inputs
",,,"Results||has||Further improvement
",,,,,
results,"While the simplest such model , Dual - BoW , obtains only 35.1 % accuracy , combining In - fer Sent sentence representations gives 40.5 % accuracy ( InferSent - Bilinear ) .","simplest such model
Dual - BoW
obtains
only 35.1 % accuracy
combining
In - fer Sent sentence representations
gives
40.5 % accuracy
InferSent - Bilinear","simplest such model||obtains||only 35.1 % accuracy
simplest such model||combining||In - fer Sent sentence representations
In - fer Sent sentence representations||gives||40.5 % accuracy
","simplest such model||name||Dual - BoW
In - fer Sent sentence representations||name||InferSent - Bilinear
",,"Results||has||simplest such model
",,,,,
results,"The best results come from pairwise NLI models : when fully trained on Swag , ESIM + ELMo obtains 59.2 % accuracy .","best results
come from
pairwise NLI models
fully trained on
Swag , ESIM + ELMo
obtains
59.2 % accuracy","best results||come from||pairwise NLI models
pairwise NLI models||fully trained on||Swag , ESIM + ELMo
Swag , ESIM + ELMo||obtains||59.2 % accuracy
",,,"Results||has||best results
",,,,,
research-problem,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,Machine Comprehension,,,,,"Contribution||has research problem||Machine Comprehension
",,,,
research-problem,Understanding unstructured text is a major goal within natural language processing .,Understanding unstructured text,,,,,"Contribution||has research problem||Understanding unstructured text
",,,,
research-problem,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,Machine comprehension ( MC ),,,,,"Contribution||has research problem||Machine comprehension ( MC )
",,,,
model,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,"present
parallel - hierarchical approach
to
machine comprehension
designed to
work well
in
data - limited setting","parallel - hierarchical approach||designed to||work well
work well||in||data - limited setting
parallel - hierarchical approach||to||machine comprehension
",,"Model||present||parallel - hierarchical approach
",,,,,,
model,Our model learns to comprehend at a high level even when data is sparse .,"learns to
comprehend
at
high level
even when
data
is
sparse","comprehend||at||high level
comprehend||even when||data
data||is||sparse
",,"Model||learns to||comprehend
",,,,,,
model,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,"to
compares
question and answer candidates
text
using
several distinct perspectives","question and answer candidates||using||several distinct perspectives
question and answer candidates||to||text
",,"Model||compares||question and answer candidates
",,,,,,
model,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .","semantic perspective
compares
hypothesis
to
sentences
in
text
viewed as
single , self - contained thoughts
represented using
sum and transformation
of
word embedding vectors","semantic perspective||compares||hypothesis
hypothesis||to||sentences
sentences||in||text
sentences||viewed as||single , self - contained thoughts
semantic perspective||represented using||sum and transformation
sum and transformation||of||word embedding vectors
",,,"Model||has||semantic perspective
",,,,,
model,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .","word - by - word perspective
focuses on
similarity matches
between
individual words
from
hypothesis and text
at
various scales","word - by - word perspective||focuses on||similarity matches
similarity matches||at||various scales
similarity matches||between||individual words
individual words||from||hypothesis and text
",,,"Model||has||word - by - word perspective
",,,,,
model,"As in the semantic perspective , we consider matches over complete sentences .","consider
matches
over
complete sentences","matches||over||complete sentences
",,,,,"word - by - word perspective||consider||matches
",,,
model,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .","use
sliding window
acting on
subsentential scale","sliding window||acting on||subsentential scale
",,,,,"word - by - word perspective||use||sliding window
",,,
experimental-setup,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .","For
word vectors
use
Google 's publicly available embeddings
trained with
word2vec
on
100 - billion - word News corpus","word vectors||use||Google 's publicly available embeddings
Google 's publicly available embeddings||trained with||word2vec
word2vec||on||100 - billion - word News corpus
",,"Experimental setup||For||word vectors
",,,,,,
experimental-setup,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .","kept
fixed
throughout
training","fixed||throughout||training
",,,,,"word vectors||kept||fixed
",,,
experimental-setup,The vectors are 300 - dimensional ( d = 300 ) .,"are
300 - dimensional",,,,,,"word vectors||are||300 - dimensional
",,,
experimental-setup,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .","used
0.5
as
dropout probability","0.5||as||dropout probability
",,"Experimental setup||used||0.5
",,,,,,
experimental-setup,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .","Dropout
occurs after
all neural - network transformations
allowed to
change
with
training","Dropout||occurs after||all neural - network transformations
all neural - network transformations||allowed to||change
change||with||training
",,,"Experimental setup||has||Dropout
",,,,,
experimental-setup,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .","Adam optimizer
with
standard settings
learning rate
of
0.003","learning rate||of||0.003
Adam optimizer||with||standard settings
",,,"Experimental setup||used||learning rate
Experimental setup||used||Adam optimizer
",,,,,
results,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ?","On
MCTest - 500
Parallel Hierarchical model
significantly outperforms
these methods
on
single questions ( > 2 % )
slightly outperforms
latter two
on
multi questions ( ? 0.3 % )","latter two||on||multi questions ( ? 0.3 % )
these methods||on||single questions ( > 2 % )
","MCTest - 500||has||Parallel Hierarchical model
Parallel Hierarchical model||has||slightly outperforms
slightly outperforms||has||latter two
Parallel Hierarchical model||has||significantly outperforms
significantly outperforms||has||these methods
","Results||On||MCTest - 500
",,,,,,
results,1 % ) .,,,,,,,,,,
results,The method of achieves the best over all result on MCTest - 160 .,"method
achieves
best over all result
on
MCTest - 160","method||achieves||best over all result
best over all result||on||MCTest - 160
",,,"Results||has||method
",,,,,
results,Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,"our model
outperforming
alternatives
by
large margin
across
board ( > 15 % )","outperforming||by||large margin
large margin||across||board ( > 15 % )
","our model||has||outperforming
outperforming||has||alternatives
",,"Results||has||our model
",,,,,
ablation-analysis,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .","n-gram functionality
is
important
contributing
almost 5 % accuracy improvement","n-gram functionality||contributing||almost 5 % accuracy improvement
n-gram functionality||is||important
",,,"Ablation analysis||has||n-gram functionality
",,,,,
ablation-analysis,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .","top N function
contributes
very little
to
over all performance","top N function||contributes||very little
very little||to||over all performance
",,,"Ablation analysis||has||top N function
",,,,,
ablation-analysis,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .","Ablating
sentential component
made
most significant difference
reducing
performance
by
more than 5 %","sentential component||made||most significant difference
sentential component||reducing||performance
performance||by||more than 5 %
",,"Ablation analysis||Ablating||sentential component
",,,,,,
ablation-analysis,Simple word - by - word matching is obviously useful on MCTest .,"Simple word - by - word matching
useful
on
MCTest","useful||on||MCTest
","Simple word - by - word matching||has||useful
",,"Ablation analysis||has||Simple word - by - word matching
",,,,,
ablation-analysis,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .","sequential sliding window
makes
3 % contribution","sequential sliding window||makes||3 % contribution
",,,"Ablation analysis||has||sequential sliding window
",,,,,
ablation-analysis,"On the other hand , the dependency - based sliding window makes only a minor contribution .","dependency - based sliding window
makes
minor contribution","dependency - based sliding window||makes||minor contribution
",,,"Ablation analysis||has||dependency - based sliding window
",,,,,
ablation-analysis,"Finally , the exogenous word weights make a significant contribution of almost 5 % .","exogenous word weights
make
significant contribution
of
almost 5 %","exogenous word weights||make||significant contribution
significant contribution||of||almost 5 %
",,,"Ablation analysis||has||exogenous word weights
",,,,,
research-problem,Recurrent Neural Network - Based Sentence Encoder with Gated Attention for Natural Language Inference,Natural Language Inference,,,,,"Contribution||has research problem||Natural Language Inference
",,,,
research-problem,"Task aims to evaluate language understanding models for sentence representation with natural language inference ( NLI ) tasks , where a sentence is represented as a fixedlength vector .",natural language inference ( NLI ),,,,,"Contribution||has research problem||natural language inference ( NLI )
",,,,
research-problem,"Specifically , NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p.",NLI,,,,,"Contribution||has research problem||NLI
",,,,
model,"We present here the proposed natural language inference networks which are composed of the following major components : word embedding , sequence encoder , composition layer , and the toplayer classifier .","proposed
natural language inference networks
composed of
word embedding
sequence encoder
composition layer
toplayer classifier","natural language inference networks||composed of||word embedding
natural language inference networks||composed of||sequence encoder
natural language inference networks||composed of||composition layer
natural language inference networks||composed of||toplayer classifier
",,"Model||proposed||natural language inference networks
",,,,,,
model,Word Embedding,Word Embedding,,,,"Model||has||Word Embedding
",,,,,
model,We concatenate embeddings learned at two different levels to represent each word in the sentence : the character composition and holistic word - level embedding .,"concatenate
embeddings
learned at
two different levels
to represent
each word
in
sentence
character composition
holistic word - level embedding","embeddings||learned at||two different levels
embeddings||to represent||each word
each word||in||sentence
","two different levels||name||character composition
two different levels||name||holistic word - level embedding
",,,,"Word Embedding||concatenate||embeddings
",,,
model,Sequence Encoder,Sequence Encoder,,,,"Model||has||Sequence Encoder
",,,,,"Sequence Encoder||has||sentence pairs
"
model,"To represent words and their context in a premise and hypothesis , sentence pairs are fed into sentence encoders to obtain hidden vectors ( h p and h h ) .","sentence pairs
fed into
sentence encoders
to obtain
hidden vectors ( h p and h h )","sentence pairs||fed into||sentence encoders
sentence encoders||to obtain||hidden vectors ( h p and h h )
",,,,,,,,
model,We use stacked bidirectional LSTMs ( BiL - STM ) as the encoders .,"use
stacked bidirectional LSTMs ( BiL - STM )
as
encoders","stacked bidirectional LSTMs ( BiL - STM )||as||encoders
",,,,,"Sequence Encoder||use||stacked bidirectional LSTMs ( BiL - STM )
",,,
model,Composition Layer,Composition Layer,,,,"Model||has||Composition Layer
",,,,,
model,"To transform sentences into fixed - length vector representations and reason using those representations , we need to compose the hidden vectors obtained by the sequence encoder layer ( h p and h h ) .","To transform
sentences
into
fixed - length vector representations
compose
hidden vectors
obtained by
sequence encoder layer ( h p and h h )","hidden vectors||obtained by||sequence encoder layer ( h p and h h )
sequence encoder layer ( h p and h h )||To transform||sentences
sentences||into||fixed - length vector representations
",,,,,"Composition Layer||compose||hidden vectors
",,,
model,We propose intra-sentence gated - attention to obtain a fixed - length vector .,"propose
intra-sentence gated - attention
to obtain
fixed - length vector","intra-sentence gated - attention||to obtain||fixed - length vector
",,,,,"Composition Layer||propose||intra-sentence gated - attention
",,,
model,Top - layer Classifier,Top - layer Classifier,,,,"Model||has||Top - layer Classifier
",,,,,"Top - layer Classifier||has||Our inference model
"
model,Our inference model feeds the resulting vectors obtained above to the final classifier to determine the over all inference relationship .,"Our inference model
feeds
resulting vectors
to
final classifier
to determine
over all inference relationship","Our inference model||feeds||resulting vectors
resulting vectors||to determine||over all inference relationship
resulting vectors||to||final classifier
",,,,,,,,
code,"To help replicate our results , we publish our code at https : //github.com/lukecq1231/enc_nli",https : //github.com/lukecq1231/enc_nli,,,,,"Contribution||Code||https : //github.com/lukecq1231/enc_nli
",,,,
experimental-setup,"We use the Adam ( Kingma and Ba , 2014 ) for optimization .","use
Adam ( Kingma and Ba , 2014 )
for
optimization","Adam ( Kingma and Ba , 2014 )||for||optimization
",,"Experimental setup||use||Adam ( Kingma and Ba , 2014 )
",,,,,,
experimental-setup,"Stacked BiLSTM has 3 layers , and all hidden states of BiLSTMs and MLP have 600 dimensions .","Stacked BiLSTM
3 layers
all hidden states
of
BiLSTMs and MLP
have
600 dimensions","all hidden states||of||BiLSTMs and MLP
BiLSTMs and MLP||have||600 dimensions
","Stacked BiLSTM||has||3 layers
",,"Experimental setup||has||Stacked BiLSTM
Experimental setup||has||all hidden states
",,,,,
experimental-setup,"The character embedding has 15 dimensions , and CNN filters length is [ 1 , 3 , 5 ] , each of those is 100 dimensions .","character embedding
15 dimensions
CNN filters length
is
[ 1 , 3 , 5 ]
100 dimensions","CNN filters length||is||[ 1 , 3 , 5 ]
","100 dimensions||has||CNN filters length
100 dimensions||has||character embedding
character embedding||has||15 dimensions
",,"Experimental setup||has||100 dimensions
",,,,,
experimental-setup,We use pretrained GloVe - 840B - 300D vectors as our word - level embeddings and fix these embeddings during the training process .,"pretrained GloVe - 840B - 300D vectors
as
our word - level embeddings
fix
during
training process","pretrained GloVe - 840B - 300D vectors||as||our word - level embeddings
fix||during||training process
","pretrained GloVe - 840B - 300D vectors||has||fix
",,"Experimental setup||use||pretrained GloVe - 840B - 300D vectors
",,,,,
experimental-setup,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"Out - of - vocabulary ( OOV ) words
initialized randomly
with
Gaussian samples","initialized randomly||with||Gaussian samples
","Out - of - vocabulary ( OOV ) words||has||initialized randomly
",,"Experimental setup||has||Out - of - vocabulary ( OOV ) words
",,,,,
results,"In addition , we also use our implementation of ESIM , which achieves an accuracy of 76.8 % in the in - domain test set , and 75.8 % in the cross - domain test set , which presents the state - of - the - art results .","our implementation of ESIM
achieves
accuracy
of
76.8 %
in
in - domain test set
75.8 %
in
cross - domain test set
presents
state - of - the - art results","our implementation of ESIM||achieves||accuracy
accuracy||of||75.8 %
75.8 %||in||cross - domain test set
accuracy||of||76.8 %
76.8 %||in||in - domain test set
our implementation of ESIM||presents||state - of - the - art results
",,,"Results||has||our implementation of ESIM
",,,,,
results,"After removing the cross - sentence attention and adding our gated - attention model , we achieve accuracies of 73.5 % and 73.6 % , which ranks first in the cross - domain test set and ranks second in the in - domain test set among the single models .","After removing
cross - sentence attention
adding
our gated - attention model
achieve
accuracies
of
73.5 % and 73.6 %
ranks
first
in
cross - domain test set
second
in
in - domain test set
among
single models","accuracies||of||73.5 % and 73.6 %
73.5 % and 73.6 %||among||single models
single models||ranks||first
first||in||cross - domain test set
single models||ranks||second
second||in||in - domain test set
73.5 % and 73.6 %||adding||our gated - attention model
73.5 % and 73.6 %||After removing||cross - sentence attention
",,"Results||achieve||accuracies
",,,,,,
results,"When ensembling our models , we obtain accuracies 74.9 % and 74.9 % , which ranks first in both test sets .","When
ensembling
our models
obtain
accuracies
74.9 % and 74.9 %
ranks
first in
both test sets","ensembling||our models||obtain
ranks||first in||both test sets
",,"Results||When||ensembling
",,,"has||74.9 % and 74.9 %||ranks
","obtain||accuracies||has
",,
ablation-analysis,"If we remove the gated - attention , the accuracies drop to 72.8 % and 73.6 % .","remove
gated - attention
accuracies
drop
to
72.8 % and 73.6 %","drop||to||72.8 % and 73.6 %
","accuracies||has||drop
gated - attention||has||accuracies
accuracies||has||drop
accuracies||has||drop
","Ablation analysis||remove||gated - attention
",,,,,,
ablation-analysis,"If we remove charactercomposition vector , the accuracies drop to 72.9 % and 73.5 % .","charactercomposition vector
accuracies
drop
to
72.9 % and 73.5 %","drop||to||72.9 % and 73.5 %
","charactercomposition vector||has||accuracies
",,"Ablation analysis||remove||charactercomposition vector
",,,,,
ablation-analysis,"If we remove word - level embedding , the accuracies drop to 65.6 % and 66.0 % .","word - level embedding
accuracies
drop
to
65.6 % and 66.0 %","drop||to||65.6 % and 66.0 %
","word - level embedding||has||accuracies
",,"Ablation analysis||remove||word - level embedding
",,,,,
research-problem,"Machine Comprehension ( MC ) is a challenging task in Natural Language Processing field , which aims to guide the machine to comprehend a passage and answer the given question .",Machine Comprehension ( MC ),,,,,"Contribution||has research problem||Machine Comprehension ( MC )
",,,,
research-problem,"Many existing approaches on MC task are suffering the inefficiency in some bottlenecks , such as insufficient lexical understanding , complex question - passage interaction , incorrect answer extraction and soon .",MC,,,,,"Contribution||has research problem||MC
",,,,
research-problem,Recently machine comprehension task accumulates much concern among NLP researchers .,machine comprehension,,,,,"Contribution||has research problem||machine comprehension
",,,,
model,"In this paper , we propose the novel framework named Smarnet with the hope that it can become as smart as humans .","propose
novel framework
named
Smarnet","novel framework||named||Smarnet
",,"Model||propose||novel framework
",,,,,,
model,"Specifically , we first introduce the Smarnet framework that exploits fine - grained word understanding with various attribution discriminations , like humans recite words with corresponding properties .","introduce
Smarnet framework
exploits
fine - grained word understanding
with
various attribution discriminations","Smarnet framework||exploits||fine - grained word understanding
fine - grained word understanding||with||various attribution discriminations
",,"Model||introduce||Smarnet framework
",,,,,,
model,We then develop the interactive attention with memory network to mimic human reading procedure .,"develop
interactive attention
with
memory network
to mimic
human reading procedure","interactive attention||with||memory network
interactive attention||to mimic||human reading procedure
",,"Model||develop||interactive attention
",,,,,,
model,We also add a checking layer on the answer refining in order to ensure the accuracy .,"add
checking layer
on
answer refining
to ensure
accuracy","checking layer||to ensure||accuracy
checking layer||on||answer refining
",,"Model||add||checking layer
",,,,,,
experimental-setup,"We preprocess each passage and question using the library of nltk and exploit the popular pretrained word embedding GloVe with 100 - dimensional vectors ( Pennington , Socher , and Manning 2014 ) for both questions and passages .","preprocess
each passage and question
using
library of nltk
exploit
popular pretrained word embedding GloVe
with
100 - dimensional vectors
for
questions and passages","each passage and question||using||library of nltk
popular pretrained word embedding GloVe||with||100 - dimensional vectors
popular pretrained word embedding GloVe||for||questions and passages
",,"Experimental setup||preprocess||each passage and question
Experimental setup||exploit||popular pretrained word embedding GloVe
",,,,,,
experimental-setup,The size of char - level embedding is also set as 100 - dimensional and is obtained by CNN filters under the instruction of ( Kim 2014 ) .,"size
of
char - level embedding
set as
100 - dimensional
obtained by
CNN filters","size||set as||100 - dimensional
size||of||char - level embedding
char - level embedding||obtained by||CNN filters
",,,"Experimental setup||has||size
",,,,,
experimental-setup,We adopt the AdaDelta ( Zeiler 2012 ) optimizer for training with an initial learning rate of 0.0005 .,"adopt
AdaDelta ( Zeiler 2012 ) optimizer
for
training
with
initial learning rate
of
0.0005","AdaDelta ( Zeiler 2012 ) optimizer||with||initial learning rate
initial learning rate||of||0.0005
AdaDelta ( Zeiler 2012 ) optimizer||for||training
",,"Experimental setup||adopt||AdaDelta ( Zeiler 2012 ) optimizer
",,,,,,
experimental-setup,The batch size is set to be 48 for both the SQuAD and TriviaQA datasets .,"batch size
set to be
48
for
SQuAD and TriviaQA datasets","batch size||set to be||48
48||for||SQuAD and TriviaQA datasets
",,,"Experimental setup||has||batch size
",,,,,
experimental-setup,We also apply dropout ( Srivastava et al. 2014 ) between layers with a dropout rate of 0.2 .,"apply
dropout ( Srivastava et al. 2014 )
between
layers
with
dropout rate
of
0.2","dropout ( Srivastava et al. 2014 )||with||dropout rate
dropout rate||of||0.2
dropout ( Srivastava et al. 2014 )||between||layers
",,"Experimental setup||apply||dropout ( Srivastava et al. 2014 )
",,,,,,
experimental-setup,"For the multi-hop reasoning , we set the number of hops as 2 which is imitating human reading procedure on skimming and scanning .","For
multi-hop reasoning
set
number of hops
as
2
imitating
human reading procedure
on
skimming and scanning","multi-hop reasoning||set||number of hops
number of hops||as||2
imitating||on||skimming and scanning
","2||has||imitating
imitating||has||human reading procedure
","Experimental setup||For||multi-hop reasoning
",,,,,,
experimental-setup,"During training , we set the moving averages of all weights as the exponential decay rate of 0.999 .","During
training
set
moving averages
of
all weights
as
exponential decay rate
of
0.999","training||set||moving averages
moving averages||as||exponential decay rate
exponential decay rate||of||0.999
moving averages||of||all weights
",,"Experimental setup||During||training
",,,,,,
results,"From the tables 1 and 2 we can see our single model achieves an EM score of 71.415 % and a F1 score of 80.160 % and the ensemble model improves to EM 75.989 % and F1 83. 475 % , which are both only after the r-net method at the time of submission .","can see
our single model
achieves
EM score
of
71.415 %
F1 score
of
80.160 %
ensemble model
improves
to
EM
75.989 %
F1
83. 475 %","improves||to||EM
improves||to||F1
our single model||achieves||EM score
EM score||of||71.415 %
our single model||achieves||F1 score
F1 score||of||80.160 %
","ensemble model||has||improves
EM||has||75.989 %
F1||has||83. 475 %
","Results||can see||ensemble model
Results||can see||our single model
",,,,,,
results,We also compare our models on the recently proposed dataset Trivia QA. shows the performance comparison on the test set of Trivia QA .,"on
test set
of
Trivia QA","test set||of||Trivia QA
",,"Results||on||test set
",,,,,,
results,We can see our Smarnet model outperforms the other baselines on both wikipedia domain and web domain .,"can see
our Smarnet model
outperforms
other baselines
on
wikipedia domain
web domain","other baselines||on||wikipedia domain
other baselines||on||web domain
","our Smarnet model||has||outperforms
outperforms||has||other baselines
",,,,"Trivia QA||can see||our Smarnet model
",,,
ablation-analysis,"We see the full features integration obtain the best performance , which demonstrates the necessity of combining all the features into consideration .","see
full features integration
obtain
best performance","full features integration||obtain||best performance
",,"Ablation analysis||see||full features integration
",,,,,,
ablation-analysis,"Among all the feature ablations , the Part - Of - Speech , Exact Match , Qtype features drop much more than the other features , which shows the importance of these three features .","Among
all the feature ablations
Part - Of - Speech
Exact Match
Qtype features
drop
much more than
other features","drop||much more than||other features
","all the feature ablations||has||drop
drop||has||Part - Of - Speech
drop||has||Exact Match
drop||has||Qtype features
","Ablation analysis||Among||all the feature ablations
",,,,,,
ablation-analysis,"As for the final ablation of POS and NER , we can see the performance decays over 3 % point , which clearly proves the usefulness of the comprehensive lexical information .","final ablation
of
POS and NER
can see
performance decays
over
3 % point","final ablation||of||POS and NER
POS and NER||can see||performance decays
performance decays||over||3 % point
",,,"Ablation analysis||has||final ablation
",,,,,
ablation-analysis,"We first replace our input gate mechanism into simplified feature concatenation strategy , the performance drops nearly 2.3 % on the EM score , which proves the effectiveness of our proposed dynamic input gating mechanism .","replace
our input gate mechanism
into
simplified feature concatenation strategy
performance
drops
nearly 2.3 %
on
EM score","our input gate mechanism||into||simplified feature concatenation strategy
nearly 2.3 %||on||EM score
","our input gate mechanism||has||performance
performance||has||drops
drops||has||nearly 2.3 %
","Ablation analysis||replace||our input gate mechanism
",,,,,,
ablation-analysis,The result proves that our modification of employing question influence on the passage encoding can boost the result up to 1.3 % on the EM score .,"result
employing
question influence
on
passage encoding
boost
up to 1.3 %
on
EM score","question influence||on||passage encoding
up to 1.3 %||on||EM score
","passage encoding||has||boost
boost||has||result
boost||has||up to 1.3 %
","Ablation analysis||employing||question influence
",,,,,,
research-problem,NegBERT : A Transfer Learning Approach for Negation Detection and Scope Resolution,Negation Detection and Scope Resolution,,,,,"Contribution||has research problem||Negation Detection and Scope Resolution
",,,,
approach,"Motivated by the success of transfer learning , we apply BERT to negation detection and scope resolution .","apply
BERT
to
negation detection and scope resolution","BERT||to||negation detection and scope resolution
",,"Approach||apply||BERT
",,,,,,
approach,"We explore the set of design choices involved , and experiment on all 3 public datasets available : the BioScope Corpus ( Abstracts and Full Papers ) , the Sherlock Dataset and the SFU Review Corpus .","explore
design choices
experiment on
3 public datasets available
BioScope Corpus ( Abstracts and Full Papers )
Sherlock Dataset
SFU Review Corpus",,"3 public datasets available||name||BioScope Corpus ( Abstracts and Full Papers )
3 public datasets available||name||Sherlock Dataset
3 public datasets available||name||SFU Review Corpus
","Approach||explore||design choices
Approach||experiment on||3 public datasets available
",,,,,,
,"We train NegBERT on one dataset and report the scores on testing all datasets , thus showing the generalizability of NegBERT .",,,,,,,,,,
experimental-setup,We use Google 's BERT as the base model to generate contextual embeddings for the sentence .,"use
Google 's BERT
as
base model
to generate
contextual embeddings
for
sentence","Google 's BERT||as||base model
base model||to generate||contextual embeddings
contextual embeddings||for||sentence
",,"Experimental setup||use||Google 's BERT
",,,,,,
experimental-setup,The input to the BERT model is a sequence of tokenized and encoded tokens of a sentence .,"input to
BERT model
is
sequence of tokenized and encoded tokens
of
sentence","BERT model||is||sequence of tokenized and encoded tokens
sequence of tokenized and encoded tokens||of||sentence
",,"Experimental setup||input to||BERT model
",,,,,,
experimental-setup,"We then use a vector of dimension R H x N_C to compute scores per token , for the classification task at hand .","vector
of
dimension R H x N_C
to compute
scores per token","vector||of||dimension R H x N_C
dimension R H x N_C||to compute||scores per token
",,,"Experimental setup||use||vector
",,,,,
experimental-setup,"BERT outputs a vector of size R H per token of the input , which we feed to a common classification layer of dimen-sion R Hx5 for cue detection and R Hx2 for scope resolution .","BERT
outputs
vector
of
size R H per token of the input
feed to
common classification layer
of
dimen-sion R Hx5
for
cue detection
R Hx2
for
scope resolution","BERT||outputs||vector
vector||of||size R H per token of the input
vector||feed to||common classification layer
common classification layer||of||dimen-sion R Hx5
dimen-sion R Hx5||for||cue detection
common classification layer||of||R Hx2
R Hx2||for||scope resolution
",,,"Experimental setup||has||BERT
",,,,,
experimental-setup,"We use early stopping on dev data for 6 epochs as tolerance and F 1 score as the early stopping metric , use the Adam optimizer with an initial learning rate of 3 e - 5 , and the Categorical Cross Entropy Loss with class weights as described above to avoid training on the padded label outputs .","early stopping
on
dev data
for
6 epochs
as
tolerance
F 1 score
as
early stopping metric
Adam optimizer
with
initial learning rate
of
3 e - 5
Categorical Cross Entropy Loss
to avoid
training
on
padded label outputs","F 1 score||as||early stopping metric
early stopping||on||dev data
dev data||for||6 epochs
6 epochs||as||tolerance
Adam optimizer||with||initial learning rate
initial learning rate||of||3 e - 5
Categorical Cross Entropy Loss||to avoid||training
training||on||padded label outputs
",,,"Experimental setup||use||F 1 score
Experimental setup||use||early stopping
Experimental setup||use||Adam optimizer
Experimental setup||use||Categorical Cross Entropy Loss
",,,,,
experimental-setup,"We perform cue detection and scope resolution for all 3 datasets , and train on 1 and test on all datasets .","perform
cue detection and scope resolution
for
all 3 datasets
train on
1
test on
all datasets","cue detection and scope resolution||for||all 3 datasets
cue detection and scope resolution||train on||1
cue detection and scope resolution||test on||all datasets
",,"Experimental setup||perform||cue detection and scope resolution
",,,,,,
experimental-setup,"For all other corpuses , we use a default 70 - 15 - 15 split for the train - dev - test data .","default 70 - 15 - 15 split
for
train - dev - test data","default 70 - 15 - 15 split||for||train - dev - test data
",,,"Experimental setup||use||default 70 - 15 - 15 split
",,,,,
experimental-setup,"We trained the models on free GPUs available via Google Colaboratory , the training scripts are publicly available .","trained
models
on
free GPUs
available via
Google Colaboratory","models||on||free GPUs
free GPUs||available via||Google Colaboratory
",,"Experimental setup||trained||models
",,,,,,
results,"For cue detection , on the Sherlock dataset test data , we see that we outperform the best system [ FBK Chowdhury ] by 0.6 F1 measure .","For
cue detection
on
Sherlock dataset test data
see that
outperform
best system
by
0.6 F1 measure","cue detection||on||Sherlock dataset test data
Sherlock dataset test data||see that||outperform
outperform||by||0.6 F1 measure
","outperform||has||best system
","Results||For||cue detection
",,,,,,
results,"On the BioScope Abstracts , we perform reasonably well .","On
BioScope Abstracts
perform
reasonably well","BioScope Abstracts||perform||reasonably well
",,,,,"cue detection||On||BioScope Abstracts
",,,
results,"On the BioScope Full papers , we are able to achieve 90.43 F1 when training on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .","BioScope Full papers
achieve
90.43 F1
when
training
on
same data","BioScope Full papers||achieve||90.43 F1
90.43 F1||when||training
training||on||same data
",,,,,,,"cue detection||On||BioScope Full papers
",
results,"On the SFU Review Corpus , we achieve an F1 of 87.08 .","SFU Review Corpus
achieve
F1
of
87.08","SFU Review Corpus||achieve||F1
F1||of||87.08
",,,,,,,"cue detection||On||SFU Review Corpus
",
results,For scope resolution :,scope resolution,,,,"Results||For||scope resolution
",,,,,
results,"On the Sherlock dataset , we achieve an F1 of 92.36 , outperforming the previous State of the Art by a significant margin ( almost 3.0 F1 ) .","On
Sherlock dataset
achieve
F1
of
92.36
outperforming
previous State of the Art
by
significant margin ( almost 3.0 F1 )","Sherlock dataset||achieve||F1
F1||of||92.36
92.36||outperforming||previous State of the Art
previous State of the Art||by||significant margin ( almost 3.0 F1 )
",,,,,"scope resolution||On||Sherlock dataset
",,,
results,"On the BioScope Abstracts , we achieve an F1 of 95.68 , outperforming the best architecture by 3.57 F1 .","BioScope Abstracts
achieve
F1
of
95.68
outperforming
best architecture
by
3.57 F1","BioScope Abstracts||achieve||F1
F1||of||95.68
95.68||outperforming||best architecture
best architecture||by||3.57 F1
",,,,,,,"scope resolution||On||BioScope Abstracts
",
results,"On the Bioscope Full Papers , we outperform the best architecture by 2.64 F1 when training on the same dataset","Bioscope Full Papers
outperform
best architecture
by
2.64 F1
training on
same dataset","Bioscope Full Papers||outperform||best architecture
best architecture||training on||same dataset
best architecture||by||2.64 F1
",,,,,,,"scope resolution||On||Bioscope Full Papers
",
results,"On the SFU Review Corpus , we outperform the best system to date by 1.02 F1 .","SFU Review Corpus
outperform
best system
by
1.02 F1","SFU Review Corpus||outperform||best system
best system||by||1.02 F1
",,,,,,,"scope resolution||On||SFU Review Corpus
",
results,"For negation cue detection , we observe a significant gap between our model , NegBERT , and the current state - of the - art systems , while we outperform the baseline systems .","negation cue detection
observe
significant gap
between
our model , NegBERT , and the current state - of the - art systems
outperform
baseline systems","negation cue detection||outperform||baseline systems
negation cue detection||observe||significant gap
significant gap||between||our model , NegBERT , and the current state - of the - art systems
",,,"Results||For||negation cue detection
",,,,,
results,"When we trained on BioScope Abstracts and tested on the BioScope Full Papers , we surprisingly observed a stateof - the - art result of 91.24 ( a gain of 3.89 F1 points over training on BioScope Full Papers ) , which is far beyond the achievable results on training and evaluating on the Bio-Medical sub corpora .","trained on
BioScope Abstracts
tested on
BioScope Full Papers
observed
stateof - the - art result
of
91.24","BioScope Abstracts||tested on||BioScope Full Papers
BioScope Full Papers||observed||stateof - the - art result
stateof - the - art result||of||91.24
",,,,,"negation cue detection||trained on||BioScope Abstracts
",,,
research-problem,Learning Semantic Sentence Embeddings using Pair- wise Discriminator,Learning Semantic Sentence Embeddings,,,,,"Contribution||has research problem||Learning Semantic Sentence Embeddings
",,,,
research-problem,"In this paper , we propose a method for obtaining sentence - level embeddings .",obtaining sentence - level embeddings,,,,,"Contribution||has research problem||obtaining sentence - level embeddings
",,,,
model,Our model consists of a sequential encoder - decoder that is further trained using a pairwise discriminator .,"consists of
sequential encoder - decoder
is
further trained
using
pairwise discriminator","sequential encoder - decoder||is||further trained
further trained||using||pairwise discriminator
",,"Model||consists of||sequential encoder - decoder
",,,,,,
model,The encoder - decoder architecture has been widely used for machine translation and machine comprehension tasks .,"encoder - decoder architecture
widely used for
machine translation
machine comprehension","encoder - decoder architecture||widely used for||machine translation
encoder - decoder architecture||widely used for||machine comprehension
",,,"Model||has||encoder - decoder architecture
",,,,,
model,"In general , the model ensures a ' local ' loss that is incurred for each recurrent unit cell .","ensures
' local ' loss
incurred for
each recurrent unit cell","' local ' loss||incurred for||each recurrent unit cell
",,"Model||ensures||' local ' loss
",,,,,,
model,"To ensure that the whole sentence is correctly encoded , we make further use of a pair - wise discriminator that encodes the whole sentence and obtains an embedding for it .","To ensure
whole sentence
is
correctly encoded
make further use of
pair - wise discriminator
that encodes
whole sentence
obtains
embedding","whole sentence||is||correctly encoded
correctly encoded||make further use of||pair - wise discriminator
pair - wise discriminator||that encodes||whole sentence
pair - wise discriminator||obtains||embedding
",,"Model||To ensure||whole sentence
",,,,,,
model,We further ensure that this is close to the desired ground - truth embeddings while being far from other ( sentences in the corpus ) embeddings .,"further ensure that
close
to
desired ground - truth embeddings
far
from
other ( sentences in the corpus ) embeddings","far||from||other ( sentences in the corpus ) embeddings
close||to||desired ground - truth embeddings
",,,,,"correctly encoded||further ensure that||far
correctly encoded||further ensure that||close
",,,
model,This model thus provides a ' global ' loss that ensures the sentence embedding as a whole is close to other semantically related sentence embeddings .,"provides
' global ' loss
ensures
sentence embedding
close to
other semantically related sentence embeddings","' global ' loss||ensures||sentence embedding
sentence embedding||close to||other semantically related sentence embeddings
",,"Model||provides||' global ' loss
",,,,,,
baselines,We start with baseline model which we take as a simple encoder and decoder network with only the local loss ( ED - Local ) .,"start with
baseline model
take as
simple encoder and decoder network
with
only the local loss ( ED - Local )","baseline model||take as||simple encoder and decoder network
simple encoder and decoder network||with||only the local loss ( ED - Local )
",,"Baselines||start with||baseline model
",,,,,,
baselines,Further we have experimented with encoder - decoder and a discriminator network with only global loss ( EDD - Global ) to distinguish the ground truth paraphrase with the predicted one .,"experimented with
encoder - decoder and a discriminator network
with
only global loss ( EDD - Global )
to distinguish
ground truth paraphrase
with
predicted one","encoder - decoder and a discriminator network||with||only global loss ( EDD - Global )
only global loss ( EDD - Global )||to distinguish||ground truth paraphrase
ground truth paraphrase||with||predicted one
",,"Baselines||experimented with||encoder - decoder and a discriminator network
",,,,,,
baselines,Another variation of our model is used both the global and local loss ( EDD - LG ) .,"variation of our model
used
both the global and local loss ( EDD - LG )","variation of our model||used||both the global and local loss ( EDD - LG )
",,,"Baselines||has||variation of our model
",,,,,
baselines,"Finally , we make the discriminator share weights with the encoder and train this network with both the losses ( EDD - LG ( shared ) ) .","make
discriminator
share
weights
with
encoder
train
network
with
both the losses ( EDD - LG ( shared ) )","discriminator||share||weights
weights||with||encoder
discriminator||train||network
network||with||both the losses ( EDD - LG ( shared ) )
",,"Baselines||make||discriminator
",,,,,,
ablation-analysis,"Among the ablations , the proposed EDD - LG ( shared ) method works way better than the other variants in terms of BLEU and METEOR metrics by achieving an improvement of 8 % and 6 % in the scores respectively over the baseline method for 50 K dataset and an improvement of 10 % and 7 % in the scores respectively for 100 K dataset .","proposed EDD - LG ( shared ) method
works
way better
than
other variants
in terms of
BLEU and METEOR metrics
achieving
improvement
of
8 % and 6 %
in
scores
for
50 K dataset
10 % and 7 %
in
scores
for
100 K dataset","proposed EDD - LG ( shared ) method||works||way better
way better||achieving||improvement
improvement||of||10 % and 7 %
10 % and 7 %||in||scores
10 % and 7 %||for||100 K dataset
improvement||of||8 % and 6 %
8 % and 6 %||in||scores
8 % and 6 %||for||50 K dataset
way better||than||other variants
way better||in terms of||BLEU and METEOR metrics
",,,"Ablation analysis||has||proposed EDD - LG ( shared ) method
",,,,,
research-problem,A Deep Generative Framework for Paraphrase Generation,Paraphrase Generation,,,,,"Contribution||has research problem||Paraphrase Generation
",,,,
research-problem,"In this paper , we address the problem of generating paraphrases automatically .",generating paraphrases automatically,,,,,"Contribution||has research problem||generating paraphrases automatically
",,,,
model,"In this paper , we present a deep generative framework for automatically generating paraphrases , given a sentence .","present
deep generative framework
for automatically generating
paraphrases
given
sentence","deep generative framework||for automatically generating||paraphrases
paraphrases||given||sentence
",,"Model||present||deep generative framework
",,,,,,
model,"Our framework combines the power of sequenceto - sequence models , specifically the long short - term memory ( LSTM ) , and deep generative models , specifically the variational autoencoder ( VAE ) , to develop a novel , end - to - end deep learning architecture for the task of paraphrase generation .","framework
combines
power
of
sequenceto - sequence models
specifically
long short - term memory ( LSTM )
deep generative models
specifically
variational autoencoder ( VAE )
to develop
novel , end - to - end deep learning architecture
for
task
of
paraphrase generation","framework||combines||power
power||of||sequenceto - sequence models
sequenceto - sequence models||specifically||long short - term memory ( LSTM )
power||of||deep generative models
deep generative models||specifically||variational autoencoder ( VAE )
power||to develop||novel , end - to - end deep learning architecture
novel , end - to - end deep learning architecture||for||task
task||of||paraphrase generation
",,,"Model||has||framework
",,,,,
model,"To address this limitation , we present a mechanism to condition our VAE model on the original sentence for which we wish to generate the paraphrases .","mechanism
to condition
our VAE model
on
original sentence
to generate
paraphrases","mechanism||to condition||our VAE model
our VAE model||to generate||paraphrases
our VAE model||on||original sentence
",,,"Model||present||mechanism
",,,,,
model,"Unlike these methods where number of classes are finite , and do not require any intermediate representation , our method conditions both the sides ( i.e. encoder and decoder ) of VAE on the intermediate representation of the input question obtained through LSTM .","of
intermediate representation
our method
conditions
both the sides
i.e.
encoder and decoder
of
VAE
on
input question
obtained through
LSTM","our method||conditions||both the sides
both the sides||of||VAE
VAE||on||intermediate representation
intermediate representation||of||input question
input question||obtained through||LSTM
both the sides||i.e.||encoder and decoder
",,,"Model||has||our method
",,,,,
model,"In contrast , our deep generative model enjoys a simple , modular architecture , and can generate not just a single but multiple , semantically sensible , paraphrases for any given sentence .","deep generative model
enjoys
simple , modular architecture
can generate
single
multiple , semantically sensible , paraphrases","deep generative model||enjoys||simple , modular architecture
deep generative model||can generate||single
deep generative model||can generate||multiple , semantically sensible , paraphrases
",,,"Model||has||deep generative model
",,,,,
model,"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a latent space .","proposed method
where
all variations
of
relatively better quality
are
top beam - search result
generated based on
different z
sampled from
latent space","proposed method||where||all variations
all variations||of||relatively better quality
relatively better quality||generated based on||different z
different z||sampled from||latent space
relatively better quality||are||top beam - search result
",,,"Model||has||proposed method
",,,,,
baselines,Residual LSTM is also the current state - of - the - art on the MSCOCO dataset .,"Residual LSTM
is
current state - of - the - art
on
MSCOCO dataset","Residual LSTM||is||current state - of - the - art
current state - of - the - art||on||MSCOCO dataset
",,,"Baselines||has||Residual LSTM
",,,,,
baselines,"For the Quora dataset , there were no known baseline results , so we compare our model with ( 1 ) standard VAE model i.e. , the unsupervised version , and ( 2 ) a "" supervised "" variant VAE - S of the unsupervised model .","compare
standard VAE model
i.e.
unsupervised version
"" supervised "" variant VAE - S
of
unsupervised model","standard VAE model||i.e.||unsupervised version
"" supervised "" variant VAE - S||of||unsupervised model
",,"Baselines||compare||standard VAE model
Baselines||compare||"" supervised "" variant VAE - S
",,,,,,
hyperparameters,"The dimension of the embedding vector is set to 300 , the dimension of both encoder and decoder is 600 , and the latent space dimension is 1100 .","dimension
of
embedding vector
is
set to
300
both encoder and decoder
is
600
latent space dimension
1100","dimension||of||embedding vector
embedding vector||set to||300
dimension||of||both encoder and decoder
both encoder and decoder||is||600
latent space dimension||is||1100
",,,"Hyperparameters||has||dimension
Hyperparameters||has||latent space dimension
",,,,,
hyperparameters,The number of layers in the encoder is 1 and in decoder,"number of layers
in
encoder
is
1
decoder","number of layers||in||decoder
number of layers||in||encoder
encoder||is||1
",,,"Hyperparameters||has||number of layers
",,,,,"decoder||has||2
"
hyperparameters,2 . Models are trained with stochastic gradient descent with learning rate fixed at a value of 5 10 ? 5 with dropout rate of 30 % .,"2
trained with
stochastic gradient descent
with
learning rate
fixed at
value of 5 10 ? 5
dropout rate
of
30 %","stochastic gradient descent||with||learning rate
learning rate||fixed at||value of 5 10 ? 5
stochastic gradient descent||with||dropout rate
dropout rate||of||30 %
",,"Hyperparameters||trained with||stochastic gradient descent
",,,,,,
hyperparameters,Batch size is kept at 32 .,"Batch size
kept at
32","Batch size||kept at||32
",,,"Hyperparameters||has||Batch size
",,,,,
hyperparameters,"Models are trained for a predefined number of iterations , rather than a fixed number of epochs .","trained for
predefined number of iterations
rather than
fixed number of epochs","predefined number of iterations||rather than||fixed number of epochs
",,"Hyperparameters||trained for||predefined number of iterations
",,,,,,
hyperparameters,Number of units in LSTM are set to be the maximum length of the sequence in the training data .,"Number of units
in
LSTM
set to be
maximum length
of
sequence
in
training data","Number of units||in||LSTM
LSTM||set to be||maximum length
maximum length||of||sequence
sequence||in||training data
",,,"Hyperparameters||has||Number of units
",,,,,
results,"Furthermore , the paraphrases generated by our system are well - formed , semantically sensible , and grammatically correct for the most part .","paraphrases
generated by
our system
are
well - formed
semantically sensible
grammatically correct","paraphrases||are||well - formed
paraphrases||are||semantically sensible
paraphrases||are||grammatically correct
paraphrases||generated by||our system
",,,"Results||has||paraphrases
",,,,,
results,"Those numbers are reported in the Measure column with row best - BLEU / best - METEOR . , we report the results for MSCOCO dataset .","for
MSCOCO dataset",,,"Results||for||MSCOCO dataset
",,,,,,
results,"As we can see , we have a significant improvement w.r.t. the baselines .","have
significant improvement
w.r.t.
baselines","significant improvement||w.r.t.||baselines
",,,,,"MSCOCO dataset||have||significant improvement
",,,
results,"Both variations of our supervised model i.e. , VAE - SVG and VAE - SVG - eq perform better than the state - of - the - art with VAE - SVG performing slightly better than VAE - SVG - eq .","Both variations
of
supervised model
i.e.
VAE - SVG
VAE - SVG - eq
perform
better
than
state - of - the - art
with
VAE - SVG
performing
slightly better
than
VAE - SVG - eq","Both variations||of||supervised model
supervised model||perform||better
better||with||VAE - SVG
VAE - SVG||performing||slightly better
slightly better||than||VAE - SVG - eq
better||than||state - of - the - art
supervised model||i.e.||VAE - SVG
supervised model||i.e.||VAE - SVG - eq
",,,,,,,"MSCOCO dataset||has||Both variations
",
results,"When comparing our results with the state - of - the - art baseline , the average metric of the VAE - SVG model is able to give a 10 % absolute point performance improvement for the TER metric , a significant number with respect to the difference between the best and second best baseline which only stands at 2 % absolute point .","of
average metric
VAE - SVG model
able to give
10 % absolute point performance improvement
for
TER metric","average metric||of||VAE - SVG model
VAE - SVG model||able to give||10 % absolute point performance improvement
10 % absolute point performance improvement||for||TER metric
",,,"Results||has||average metric
",,,,,
results,"For the BLEU and METEOR , our best results are 4.7 % and 4 % absolute point improvement over the state - of - the - art .","For
BLEU and METEOR
best results
are
4.7 % and 4 % absolute point improvement
over
state - of - the - art","best results||are||4.7 % and 4 % absolute point improvement
4.7 % and 4 % absolute point improvement||over||state - of - the - art
","BLEU and METEOR||has||best results
",,,,"MSCOCO dataset||For||BLEU and METEOR
",,,
results,"In , we report results for the Quora dataset .",Quora dataset,,,,"Results||for||Quora dataset
",,,,,"Quora dataset||has||both variations
"
results,"As we can see , both variations of our model perform significantly better than unsupervised VAE and VAE - S , which is not surprising .","both variations
of
model
perform
significantly better
than
unsupervised VAE
VAE - S","both variations||of||model
both variations||perform||significantly better
significantly better||than||unsupervised VAE
significantly better||than||VAE - S
",,,,,,,,
results,"We also report the results on different training sizes , and as expected , as we increase the training data size , results improve .","results
increase
training data size
improve",,"training data size||has||results
results||has||improve
",,,,"Quora dataset||increase||training data size
",,,
results,"Comparing the results across different variants of supervised model , VAE - SVG - eq performs the best .","Comparing
results
across
different variants
of
supervised model
VAE - SVG - eq
performs
best","results||across||different variants
different variants||of||supervised model
VAE - SVG - eq||performs||best
","different variants||has||VAE - SVG - eq
",,,,"Quora dataset||Comparing||results
",,,
results,"We also experimented with generating paraphrases through beam - search , and , unlike MSCOCO , it turns out that beam search improves the results significantly .","experimented with
generating paraphrases
through
beam - search
turns out that
beam search
improves
results
significantly","generating paraphrases||through||beam - search
generating paraphrases||turns out that||beam search
beam search||improves||results
","results||has||significantly
",,,,"Quora dataset||experimented with||generating paraphrases
",,,
results,"When comparing the best variant of our model with unsupervised model ( VAE ) , we are able to get more than 27 % absolute point ( more than 3 times ) boost in BLEU score , and more than 19 % absolute point ( more than 2 times ) boost in METEOR ; and when comparing with VAE - S , we are able to get a boost of almost 19 % absolute points in BLEU ( 2 times ) and more than 10 % absolute points in METEOR ( 1.5 times ) .","comparing
best variant of our model
with
unsupervised model ( VAE )
able to get
more than 27 % absolute point ( more than 3 times ) boost
in
BLEU score
more than 19 % absolute point ( more than 2 times ) boost
in
METEOR
VAE - S
able to get
boost of almost 19 % absolute points
BLEU ( 2 times )
more than 10 % absolute points
in
METEOR ( 1.5 times )","best variant of our model||with||unsupervised model ( VAE )
best variant of our model||able to get||more than 19 % absolute point ( more than 2 times ) boost
more than 19 % absolute point ( more than 2 times ) boost||in||METEOR
best variant of our model||able to get||more than 27 % absolute point ( more than 3 times ) boost
more than 27 % absolute point ( more than 3 times ) boost||in||BLEU score
VAE - S||able to get||more than 10 % absolute points
more than 10 % absolute points||in||METEOR ( 1.5 times )
VAE - S||able to get||boost of almost 19 % absolute points
boost of almost 19 % absolute points||in||BLEU ( 2 times )
",,,,,"Quora dataset||comparing||best variant of our model
Quora dataset||comparing||VAE - S
",,,
research-problem,Robust Multilingual Part - of - Speech Tagging via Adversarial Training,Part - of - Speech Tagging,,,,,"Contribution||has research problem||Part - of - Speech Tagging
",,,,
research-problem,"In this paper , we propose and analyze a neural POS tagging model that exploits AT .",neural POS tagging,,,,,"Contribution||has research problem||neural POS tagging
",,,,
model,"In this paper , spotlighting a well - studied core problem of NLP , we propose and carefully analyze a neural part - of - speech ( POS ) tagging model that exploits adversarial training .","propose and carefully analyze
neural part - of - speech ( POS ) tagging model
that exploits
adversarial training","neural part - of - speech ( POS ) tagging model||that exploits||adversarial training
",,"Model||propose and carefully analyze||neural part - of - speech ( POS ) tagging model
",,,,,,
model,"With a BiLSTM - CRF model as our baseline POS tagger , we apply adversarial training by considering perturbations to input word / character embeddings .","With
BiLSTM - CRF model
as
baseline POS tagger
apply
adversarial training
considering
perturbations
to
input word / character embeddings","BiLSTM - CRF model||as||baseline POS tagger
BiLSTM - CRF model||apply||adversarial training
adversarial training||considering||perturbations
perturbations||to||input word / character embeddings
",,"Model||With||BiLSTM - CRF model
",,,,,,
hyperparameters,"We train the model parameters and word / character embeddings by the mini-batch stochastic gradient descent ( SGD ) with batch size 10 , momentum 0.9 , initial learning rate 0.01 and decay rate 0.05 .","train
model parameters and word / character embeddings
by
mini-batch stochastic gradient descent ( SGD )
with
batch size
10
momentum
0.9
initial learning rate
0.01
decay rate
0.05","model parameters and word / character embeddings||by||mini-batch stochastic gradient descent ( SGD )
mini-batch stochastic gradient descent ( SGD )||with||initial learning rate
mini-batch stochastic gradient descent ( SGD )||with||decay rate
mini-batch stochastic gradient descent ( SGD )||with||batch size
mini-batch stochastic gradient descent ( SGD )||with||momentum
","initial learning rate||has||0.01
decay rate||has||0.05
batch size||has||10
momentum||has||0.9
","Hyperparameters||train||model parameters and word / character embeddings
",,,,,,
hyperparameters,We also use a gradient clipping of 5.0 .,"use
gradient clipping
of
5.0","gradient clipping||of||5.0
",,"Hyperparameters||use||gradient clipping
",,,,,,
hyperparameters,The models are trained with early stopping ) based on the development performance .,"trained with
early stopping
based on
development performance","early stopping||based on||development performance
",,"Hyperparameters||trained with||early stopping
",,,,,,
results,PTB - WSJ dataset .,PTB - WSJ dataset,,,,"Results||has||PTB - WSJ dataset
",,,,,"PTB - WSJ dataset||has||baseline ( BiLSTM - CRF ) model
"
results,"As expected , our baseline ( BiLSTM - CRF ) model ( accuracy 97.54 % ) performs on par with other state - of - the - art systems .","baseline ( BiLSTM - CRF ) model
accuracy
97.54 %
performs
on par
with
other state - of - the - art systems","baseline ( BiLSTM - CRF ) model||performs||on par
on par||with||other state - of - the - art systems
","baseline ( BiLSTM - CRF ) model||has||accuracy
accuracy||has||97.54 %
",,,,,,,
results,"Built upon this baseline , our adversarial training ( AT ) model reaches accuracy 97.58 % thanks to its regularization power , outperforming recent POS taggers except .","Built upon
adversarial training ( AT ) model
reaches
accuracy
97.58 %
outperforming
recent POS taggers","adversarial training ( AT ) model||reaches||accuracy
adversarial training ( AT ) model||outperforming||recent POS taggers
","accuracy||has||97.58 %
",,,,"baseline ( BiLSTM - CRF ) model||Built upon||adversarial training ( AT ) model
",,,
research-problem,Learning Better Internal Structure of Words for Sequence Labeling,Sequence Labeling,,,,,"Contribution||has research problem||Sequence Labeling
",,,,
model,"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural network for learning the internal structure of words by composing their characters .","propose
IntNet
funnel - shaped wide convolutional neural network
for learning
internal structure of words
by composing
characters","funnel - shaped wide convolutional neural network||for learning||internal structure of words
internal structure of words||by composing||characters
","IntNet||has||funnel - shaped wide convolutional neural network
","Model||propose||IntNet
",,,,,,
model,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no down - sampling for learning character - to - word representations from limited supervised training corpora .","funnel - shaped Int - Net
explores
deeper and wider architecture
with no
down - sampling
for learning
character - to - word representations
from
limited supervised training corpora","funnel - shaped Int - Net||explores||deeper and wider architecture
deeper and wider architecture||with no||down - sampling
down - sampling||for learning||character - to - word representations
character - to - word representations||from||limited supervised training corpora
",,,"Model||has||funnel - shaped Int - Net
",,,,,
model,"Lastly , we combine our IntNet model with LSTM - CRF , which captures both word shape and context information , and jointly decode tags for sequence labeling .","combine
IntNet model
with
LSTM - CRF
captures
word shape
context information
jointly decode
tags
for
sequence labeling","IntNet model||with||LSTM - CRF
LSTM - CRF||jointly decode||tags
tags||for||sequence labeling
LSTM - CRF||captures||word shape
LSTM - CRF||captures||context information
",,"Model||combine||IntNet model
",,,,,,
hyperparameters,The size of the dimensions of character embeddings is 32 which are randomly initialized using a uniform distribution .,"size
of
dimensions
of
character embeddings
is
32
are
randomly initialized
using
uniform distribution","size||of||dimensions
dimensions||of||character embeddings
character embeddings||are||randomly initialized
randomly initialized||using||uniform distribution
character embeddings||is||32
",,,"Hyperparameters||has||size
",,,,,
hyperparameters,We adopt the same initialization method for randomly initialized word embeddings that are updated during training .,"adopt
same initialization method
for
randomly initialized word embeddings
updated during
training","same initialization method||for||randomly initialized word embeddings
randomly initialized word embeddings||updated during||training
",,"Hyperparameters||adopt||same initialization method
",,,,,,
hyperparameters,"For IntNet , the filter size of the initial convolution is 32 and that of other convolutions is 16 .","For
IntNet
filter size
of
initial convolution
is
32
other convolutions
is
16","filter size||of||initial convolution
initial convolution||is||32
filter size||of||other convolutions
other convolutions||is||16
","IntNet||has||filter size
","Hyperparameters||For||IntNet
",,,,,,
hyperparameters,"The number of convolutional layers are 5 and 9 for IntNet - 5 and IntNet - 9 , respectively , and we have adopted the same weight initialization as that of ResNet .","number of convolutional layers
are
5 and 9
for
IntNet - 5 and IntNet - 9
adopted
same weight initialization
as
ResNet","number of convolutional layers||are||5 and 9
5 and 9||for||IntNet - 5 and IntNet - 9
same weight initialization||as||ResNet
",,"Hyperparameters||adopted||same weight initialization
","Hyperparameters||has||number of convolutional layers
",,,,,
hyperparameters,"We use pre-trained word embeddings for initialization , GloVe 100 - dimension word embeddings for English , and fastText 300 dimension word embeddings for Spanish , Dutch , and German .","use
pre-trained word embeddings
for
initialization
GloVe 100 - dimension word embeddings
for
English
fastText 300 dimension word embeddings
for
Spanish , Dutch , and German","fastText 300 dimension word embeddings||for||Spanish , Dutch , and German
GloVe 100 - dimension word embeddings||for||English
pre-trained word embeddings||for||initialization
",,"Hyperparameters||use||fastText 300 dimension word embeddings
Hyperparameters||use||GloVe 100 - dimension word embeddings
Hyperparameters||use||pre-trained word embeddings
",,,,,,
hyperparameters,The state size of the bi-directional LSTMs is set to 256 .,"state size
of
bi-directional LSTMs
set to
256","state size||of||bi-directional LSTMs
bi-directional LSTMs||set to||256
",,,"Hyperparameters||has||state size
",,,,,
hyperparameters,We adopt standard BIOES tagging scheme for NER and Chunking .,"standard BIOES tagging scheme
for
NER and Chunking","standard BIOES tagging scheme||for||NER and Chunking
",,,"Hyperparameters||adopt||standard BIOES tagging scheme
",,,,,
hyperparameters,We employ mini-batch stochastic gradient descent with momentum .,"employ
mini-batch stochastic gradient descent
with
momentum","mini-batch stochastic gradient descent||with||momentum
",,"Hyperparameters||employ||mini-batch stochastic gradient descent
",,,,,,
hyperparameters,"0.05 is the decay ratio , the value of gradient clipping is 5 .","0.05
is
decay ratio
gradient clipping
is
5","gradient clipping||is||5
decay ratio||is||0.05
",,,"Hyperparameters||has||gradient clipping
Hyperparameters||has||decay ratio
",,,,,
hyperparameters,"Dropout is applied on the input of IntNet , LSTMs , and CRF , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .","Dropout
applied on
input
of
IntNet , LSTMs , and CRF
ratio
0.5","Dropout||applied on||input
input||of||IntNet , LSTMs , and CRF
Dropout||ratio||0.5
",,,"Hyperparameters||has||Dropout
",,,,,
baselines,"Firstly , we use LSTM - CRF with randomly initialized word embeddings as our initial baseline .","use
LSTM - CRF
with
randomly initialized word embeddings","LSTM - CRF||with||randomly initialized word embeddings
",,"Baselines||use||LSTM - CRF
",,,,,,
baselines,"We adopt two state - of - the - art methods in sequence labeling , denoted as char - LSTM and char - CNN .","adopt
two state - of - the - art methods
in
sequence labeling
denoted as
char - LSTM
char - CNN","two state - of - the - art methods||in||sequence labeling
two state - of - the - art methods||denoted as||char - LSTM
two state - of - the - art methods||denoted as||char - CNN
",,"Baselines||adopt||two state - of - the - art methods
",,,,,,
baselines,"We add more layers to the char - CNN model and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for 5 and 9 convolutional layers .","add
more layers
to
char - CNN model
refer to
char - CNN - 5 and char - CNN - 9
for
5 and 9 convolutional layers","more layers||to||char - CNN model
char - CNN model||refer to||char - CNN - 5 and char - CNN - 9
char - CNN - 5 and char - CNN - 9||for||5 and 9 convolutional layers
",,"Baselines||add||more layers
",,,,,,
baselines,"Furthermore , we add residual connections to the char - CNN - 9 and refer it as char - ResNet .","residual connections
to
char - CNN - 9
refer it as
char - ResNet","residual connections||to||char - CNN - 9
char - CNN - 9||refer it as||char - ResNet
",,,"Baselines||add||residual connections
",,,,,
baselines,"Also , we apply 3 dense blocks based on char - ResNet which we refer to as char - DenseNet , to compare the difference between residual connection and dense connection .","apply
3 dense blocks
based on
char - ResNet
refer to as
char - DenseNet","3 dense blocks||based on||char - ResNet
char - ResNet||refer to as||char - DenseNet
",,"Baselines||apply||3 dense blocks
",,,,,,
results,5 Results and Analysis 5.1 Character - to - word Models presents the performance of different character - to - word models on six benchmark datasets .,Character - to - word Models,,,,"Results||has||Character - to - word Models
",,,,,"Character - to - word Models||has||F1 score
"
results,"The result shows that for most of the datasets , the F1 score does not improve much when we directly add more layers .","F1 score
does not
improve much
when
directly add
more layers","F1 score||does not||improve much
improve much||when||directly add
","directly add||has||more layers
",,,,,,,
results,We also observe some accuracy drop when we continuously increase the depth .,"observe
some accuracy drop
when
continuously increase
depth","some accuracy drop||when||continuously increase
","continuously increase||has||depth
",,,,"Character - to - word Models||observe||some accuracy drop
",,,
results,"Furthermore , we add residual connections to char - CNN - 9 as char - ResNet - 9 , which confirms that residual connections can help train deep layers .","add
residual connections
to
char - CNN - 9
as
char - ResNet - 9
confirms that
residual connections
help train
deep layers","residual connections||to||char - CNN - 9
char - CNN - 9||as||char - ResNet - 9
char - CNN - 9||confirms that||residual connections
residual connections||help train||deep layers
",,,,,"Character - to - word Models||add||residual connections
",,,
results,"We further improve char - ResNet - 9 by changing residual connections into dense connection blocks as char - DenseNet - 9 , which shows that the dense connections are better than residual connections for learning word shape information .","improve
char - ResNet - 9
by changing
residual connections
into
dense connection blocks
as
char - DenseNet - 9
shows that
dense connections
are
better
than
residual connections
for learning
word shape information","char - ResNet - 9||by changing||residual connections
residual connections||into||dense connection blocks
dense connection blocks||as||char - DenseNet - 9
dense connection blocks||shows that||dense connections
dense connections||are||better
better||than||residual connections
residual connections||for learning||word shape information
",,,,,"Character - to - word Models||improve||char - ResNet - 9
",,,
results,"Our proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9 generally improves the results across all datasets .","proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9
improves
results
across
all datasets","proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9||improves||results
results||across||all datasets
",,,,,,,"Character - to - word Models||has||proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9
",
results,"Our IntNet significantly outperforms other character embedding models , for example , the improvement is more than 2 % in terms of F 1 score for German and Dutch .","IntNet
significantly outperforms
other character embedding models","IntNet||significantly outperforms||other character embedding models
",,,,,,,"Character - to - word Models||has||IntNet
",
results,"Also , we observe that char - IntNet - 5 is more effective for learning character - to - word representations than char - IntNet - 9 in most of the cases .","char - IntNet - 5
is
more effective
for learning
character - to - word representations
than
char - IntNet - 9
in
most of the cases","char - IntNet - 5||is||more effective
more effective||for learning||character - to - word representations
character - to - word representations||in||most of the cases
character - to - word representations||than||char - IntNet - 9
",,,,,,,"Character - to - word Models||observe||char - IntNet - 5
",
results,Table 2 presents our proposed model in comparison with state - of - the - art results .,"in comparison with
state - of - the - art results",,,"Results||in comparison with||state - of - the - art results
",,,,,,
results,These experiments show that our char - IntNet generally improves results across different models and datasets .,"show that
our char - IntNet
improves
results
across
different models and datasets","our char - IntNet||improves||results
results||across||different models and datasets
",,,,,"state - of - the - art results||show that||our char - IntNet
",,,
results,"The improvement is more pronounced for non-English datasets , for example , IntNet improves the F - 1 score over the stateof - the - art results by more than 2 % for Dutch and Spanish .","for
IntNet
improves
F - 1 score
over
stateof - the - art results
by more than
2 %
Dutch and Spanish","IntNet||improves||F - 1 score
F - 1 score||over||stateof - the - art results
F - 1 score||by more than||2 %
2 %||for||Dutch and Spanish
",,,"Results||has||IntNet
",,,,,
results,"It also shows that the results of LSTM - CRF are significantly improved after adding character - to - word models , which confirms that word shape information is very important for sequence labeling .","shows that
results
of
LSTM - CRF
are
significantly improved
after adding
character - to - word models","results||of||LSTM - CRF
LSTM - CRF||are||significantly improved
significantly improved||after adding||character - to - word models
",,"Results||shows that||results
",,,,,,
research-problem,TRANSFER LEARNING FOR SEQUENCE TAGGING WITH HIERARCHICAL RECURRENT NETWORKS,SEQUENCE TAGGING,,,,,"Contribution||has research problem||SEQUENCE TAGGING
",,,,
code,1 Code is available at https://github.com/kimiyoung/transfer 1 ar Xiv:1703.06345v1 [ cs.CL ],https://github.com/kimiyoung/transfer,,,,,"Contribution||Code||https://github.com/kimiyoung/transfer
",,,,
approach,"We present a transfer learning approach based on a deep hierarchical recurrent neural network , which shares the hidden feature repre-sentation and part of the model parameters between the source task and the target task .","present
transfer learning approach
based on
deep hierarchical recurrent neural network
shares
hidden feature repre-sentation
part of the model parameters
between
source task and the target task","transfer learning approach||based on||deep hierarchical recurrent neural network
transfer learning approach||between||source task and the target task
source task and the target task||shares||hidden feature repre-sentation
source task and the target task||shares||part of the model parameters
",,"Approach||present||transfer learning approach
",,,,,,
approach,Our approach combines the objectives of the two tasks and uses gradient - based methods for efficient training .,"uses
gradient - based methods
for
efficient training","gradient - based methods||for||efficient training
",,"Approach||uses||gradient - based methods
",,,,,,
experiments,TRANSFER LEARNING PERFORMANCE,TRANSFER LEARNING PERFORMANCE,,,,"Experiments||has||TRANSFER LEARNING PERFORMANCE
",,,,,"TRANSFER LEARNING PERFORMANCE||has||Hyperparameters
"
experiments,"We fix the hyperparameters for all the results reported in this section : we set the character embedding dimension at 25 , the word embedding dimension at 50 for English and 64 for Spanish , the dimension of hidden states of the character - level GRUs at 80 , the dimension of hidden states of the word - level GRUs at 300 , and the initial learning rate at 0.01 .","for
set
character embedding dimension
at
25
word embedding dimension
at
50
for
English
64
Spanish
dimension
of
hidden states
of
character - level GRUs
at
80
word - level GRUs
at
300
initial learning rate
at
0.01","word embedding dimension||at||50
50||for||English
word embedding dimension||at||64
64||for||Spanish
initial learning rate||at||0.01
character embedding dimension||at||25
dimension||of||hidden states
hidden states||of||character - level GRUs
character - level GRUs||at||80
hidden states||of||word - level GRUs
word - level GRUs||at||300
",,,,,"Hyperparameters||set||word embedding dimension
Hyperparameters||set||initial learning rate
Hyperparameters||set||character embedding dimension
Hyperparameters||set||dimension
",,,
experiments,We can see that our transfer learning approach consistently improved over the non-transfer results .,"see that
transfer learning approach
consistently improved
over
non-transfer results","consistently improved||over||non-transfer results
","transfer learning approach||has||consistently improved
",,,,"Results||see that||transfer learning approach
",,,
experiments,We also observe that the improvement by transfer learning is more substantial when the labeling rate is lower .,"observe that
improvement
by
transfer learning
is
more substantial
when
labeling rate
is
lower","improvement||by||transfer learning
transfer learning||is||more substantial
more substantial||when||labeling rate
labeling rate||is||lower
",,,,,"Results||observe that||improvement
",,,
experiments,"As shown in and 2 ( e ) , our transfer learning approach can improve the performance on Twitter POS tagging and NER for all labeling rates , and the improvements with 0.1 labels are more than 8 % for both datasets .","our transfer learning approach
improve the performance
on
Twitter POS tagging and NER
for
all labeling rates
improvements
with
0.1 labels
are
more than 8 %
for
both datasets","improve the performance||on||Twitter POS tagging and NER
Twitter POS tagging and NER||for||all labeling rates
improvements||with||0.1 labels
0.1 labels||are||more than 8 %
more than 8 %||for||both datasets
","our transfer learning approach||has||improve the performance
our transfer learning approach||has||improvements
",,,,,,"Results||has||our transfer learning approach
",
experiments,Cross - application transfer also leads to substantial improvement under low - resource conditions .,"Cross - application transfer
leads to
substantial improvement
under
low - resource conditions","Cross - application transfer||leads to||substantial improvement
substantial improvement||under||low - resource conditions
",,,,,,,"Results||has||Cross - application transfer
",
experiments,COMPARISON WITH STATE - OF - THE - ART RESULTS,COMPARISON WITH STATE - OF - THE - ART RESULTS,,,,"Experiments||has||COMPARISON WITH STATE - OF - THE - ART RESULTS
",,,,,"COMPARISON WITH STATE - OF - THE - ART RESULTS||has||Hyperparameters
"
experiments,We use publicly available pretrained word embeddings as initialization .,"use
publicly available pretrained word embeddings
as
initialization","publicly available pretrained word embeddings||as||initialization
",,,,,"Hyperparameters||use||publicly available pretrained word embeddings
",,,
experiments,"On the English datasets , following previous works that are based on neural networks , we experiment with both the 50 - dimensional SENNA embeddings and the 100 - dimensional GloVe embeddings and use the development set to choose the embeddings for different tasks and settings .","On
English datasets
experiment with
50 - dimensional SENNA embeddings
100 - dimensional GloVe embeddings
use
development set
to choose
embeddings
for
different tasks and settings","English datasets||experiment with||50 - dimensional SENNA embeddings
English datasets||experiment with||100 - dimensional GloVe embeddings
English datasets||use||development set
development set||to choose||embeddings
embeddings||for||different tasks and settings
",,,,,"Hyperparameters||On||English datasets
",,,
experiments,"For Spanish and Dutch , we use the 64 - dimensional Polyglot embeddings .","For
Spanish and Dutch
use
64 - dimensional Polyglot embeddings","Spanish and Dutch||use||64 - dimensional Polyglot embeddings
",,,,,"Hyperparameters||For||Spanish and Dutch
",,,
experiments,We set the hidden state dimensions to be 300 for the word - level GRU .,"set
hidden state dimensions
to be
300
for
word - level GRU","hidden state dimensions||to be||300
300||for||word - level GRU
",,,,,"Hyperparameters||set||hidden state dimensions
",,,
experiments,The initial learning rate for AdaGrad is fixed at 0.01 .,"initial learning rate
for
AdaGrad
fixed at
0.01","initial learning rate||for||AdaGrad
AdaGrad||fixed at||0.01
",,,,,,,"Hyperparameters||has||initial learning rate
",
experiments,"First , our transfer learning approach achieves new state - of - the - art results on all the considered benchmark datasets except PTB POS tagging , which indicates that transfer learning can still improve the performance even on datasets with relatively abundant labels .","transfer learning approach
achieves
new state - of - the - art results
on
all the considered benchmark datasets","transfer learning approach||achieves||new state - of - the - art results
new state - of - the - art results||on||all the considered benchmark datasets
",,,,,,,"Results||has||transfer learning approach
",
experiments,"Second , our base model ( w/o transfer ) performs competitively compared to the state - of - the - art systems , which means that the improvements shown in Section 4.2 are obtained over a strong baseline .","our base model ( w/o transfer )
performs
competitively
compared to
state - of - the - art systems","our base model ( w/o transfer )||performs||competitively
competitively||compared to||state - of - the - art systems
",,,,,,,"Results||has||our base model ( w/o transfer )
",
research-problem,End - to - end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,End - to - end Sequence Labeling,,,,,"Contribution||has research problem||End - to - end Sequence Labeling
",,,,
research-problem,State - of - the - art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .,sequence labeling,,,,,"Contribution||has research problem||sequence labeling
",,,,
research-problem,"Linguistic sequence labeling , such as part - ofspeech ( POS ) tagging and named entity recognition ( NER ) , is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community .",Linguistic sequence labeling,,,,,"Contribution||has research problem||Linguistic sequence labeling
",,,,
model,"In this paper , we propose a neural network architecture for sequence labeling .","propose
neural network architecture
for
sequence labeling","neural network architecture||for||sequence labeling
",,"Model||propose||neural network architecture
",,,,,,
model,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .","is
endto - end model
requiring no
task - specific resources
feature engineering
data pre-processing
beyond
pre-trained word embeddings
on
unlabeled corpora","endto - end model||requiring no||task - specific resources
endto - end model||requiring no||feature engineering
endto - end model||requiring no||data pre-processing
endto - end model||beyond||pre-trained word embeddings
pre-trained word embeddings||on||unlabeled corpora
",,"Model||is||endto - end model
",,,,,,
model,"Thus , our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains .","easily applied to
wide range
of
sequence labeling tasks
on
different languages and domains","wide range||of||sequence labeling tasks
sequence labeling tasks||on||different languages and domains
",,"Model||easily applied to||wide range
",,,,,,
model,We first use convolutional neural networks ( CNNs ) to encode character - level information of a word into its character - level representation .,"first use
convolutional neural networks ( CNNs )
to encode
character - level information
of
a word
into
its character - level representation","convolutional neural networks ( CNNs )||to encode||character - level information
character - level information||of||a word
a word||into||its character - level representation
",,"Model||first use||convolutional neural networks ( CNNs )
",,,,,,
model,Then we combine character - and word - level representations and feed them into bi-directional LSTM ( BLSTM ) to model context information of each word .,"combine
character - and word - level representations
feed them into
bi-directional LSTM ( BLSTM )
to model
context information
of
each word","character - and word - level representations||feed them into||bi-directional LSTM ( BLSTM )
bi-directional LSTM ( BLSTM )||to model||context information
context information||of||each word
",,"Model||combine||character - and word - level representations
",,,,,,
model,"On top of BLSTM , we use a sequential CRF to jointly decode labels for the whole sentence .","On top of
BLSTM
use
sequential CRF
to jointly decode
labels
for
whole sentence","BLSTM||use||sequential CRF
sequential CRF||to jointly decode||labels
labels||for||whole sentence
",,"Model||On top of||BLSTM
",,,,,,
hyperparameters,Parameter optimization is performed with minibatch stochastic gradient descent ( SGD ) with batch size 10 and momentum 0.9 .,"Parameter optimization
performed with
minibatch stochastic gradient descent ( SGD )
with
batch size
10
momentum
0.9","Parameter optimization||performed with||minibatch stochastic gradient descent ( SGD )
minibatch stochastic gradient descent ( SGD )||with||batch size
minibatch stochastic gradient descent ( SGD )||with||momentum
","batch size||has||10
momentum||has||0.9
",,"Hyperparameters||has||Parameter optimization
",,,,,
hyperparameters,"We choose an initial learning rate of ? 0 ( ? 0 = 0.01 for POS tagging , and 0.015 for NER , see Section 3.3 . ) , and the learning rate is updated on each epoch of training as ? t = ? 0 / ( 1 + ?t ) , with decay rate ? =","choose
initial learning rate
of
0.01
for
POS tagging
0.015
for
NER
updated on
each epoch
of
training","initial learning rate||updated on||each epoch
each epoch||of||training
initial learning rate||of||0.01
0.01||for||POS tagging
initial learning rate||of||0.015
0.015||for||NER
",,"Hyperparameters||choose||initial learning rate
",,,,,,
hyperparameters,0.05 and t is the number of epoch completed .,,,,,,,,,,
hyperparameters,"To reduce the effects of "" gradient exploding "" , we use a gradient clipping of 5.0 .","reduce
effects
of
gradient exploding
use
gradient clipping
of
5.0","effects||of||gradient exploding
gradient exploding||use||gradient clipping
gradient clipping||of||5.0
",,"Hyperparameters||reduce||effects
",,,,,,
hyperparameters,We use early stopping based on performance on validation sets .,"use
early stopping
based on
performance
on
validation sets","early stopping||based on||performance
performance||on||validation sets
",,"Hyperparameters||use||early stopping
",,,,,,
hyperparameters,"The "" best "" parameters appear at around 50 epochs , according to our experiments .",""" best "" parameters
appear at
around 50 epochs",""" best "" parameters||appear at||around 50 epochs
",,,"Hyperparameters||has||"" best "" parameters
",,,,,
hyperparameters,"For each of the embeddings , we fine - tune initial embeddings , modifying them during gradient updates of the neural network model by back - propagating gradients .","For each of
embeddings
fine - tune
initial embeddings
modifying them during
gradient updates
of
neural network model
by
back - propagating gradients","embeddings||fine - tune||initial embeddings
embeddings||modifying them during||gradient updates
gradient updates||of||neural network model
neural network model||by||back - propagating gradients
",,"Hyperparameters||For each of||embeddings
",,,,,,
hyperparameters,"To mitigate overfitting , we apply the dropout method ( Srivastava et al. , 2014 ) to regularize our model .","To mitigate
overfitting
apply
dropout method
to regularize
model","overfitting||apply||dropout method
dropout method||to regularize||model
",,"Hyperparameters||To mitigate||overfitting
",,,,,,
hyperparameters,"As shown in and 3 , we apply dropout on character embeddings before inputting to CNN , and on both the input and output vectors of BLSTM .","apply
dropout
on
character embeddings
before inputting to
CNN
input and output vectors
of
BLSTM","dropout||on||character embeddings
character embeddings||before inputting to||CNN
dropout||on||input and output vectors
input and output vectors||of||BLSTM
",,"Hyperparameters||apply||dropout
",,,,,,
hyperparameters,We fix dropout rate at 0.5 for all dropout layers through all the experiments .,"fix
dropout rate
at
0.5
for
all dropout layers","dropout rate||at||0.5
0.5||for||all dropout layers
",,"Hyperparameters||fix||dropout rate
",,,,,,
baselines,"We compare the performance with three baseline systems - BRNN , the bi-direction RNN ; BLSTM , the bidirection LSTM , and BLSTM - CNNs , the combination of BLSTM with CNN to model characterlevel information .","three baseline systems
BRNN , the bi-direction RNN
BLSTM , the bidirection LSTM
BLSTM - CNNs , the combination of BLSTM with CNN
to model
characterlevel information","BLSTM - CNNs , the combination of BLSTM with CNN||to model||characterlevel information
","three baseline systems||name||BRNN , the bi-direction RNN
three baseline systems||name||BLSTM , the bidirection LSTM
three baseline systems||name||BLSTM - CNNs , the combination of BLSTM with CNN
",,"Baselines||has||three baseline systems
",,,,,
results,"Finally , by adding CRF layer for joint decoding we achieve significant improvements over BLSTM - CNN models for both POS tagging and NER on all metrics .","adding
CRF layer
for
joint decoding
achieve
significant improvements
over
BLSTM - CNN models
for
POS tagging and NER","CRF layer||achieve||significant improvements
significant improvements||over||BLSTM - CNN models
significant improvements||for||POS tagging and NER
CRF layer||for||joint decoding
",,"Results||adding||CRF layer
",,,,,,
results,"Comparing with traditional statistical models , our system achieves state - of - the - art accuracy , obtaining 0.05 % improvement over the previously best reported results by .","Comparing with
traditional statistical models
our system
achieves
state - of - the - art accuracy
obtaining
0.05 % improvement
over
previously best reported results","our system||achieves||state - of - the - art accuracy
our system||obtaining||0.05 % improvement
0.05 % improvement||over||previously best reported results
","traditional statistical models||has||our system
","Results||Comparing with||traditional statistical models
",,,,,,
results,"Similar to the observations of POS tagging , our model achieves significant improvements over Senna and the other three neural models , namely the LSTM - CRF proposed by , LSTM - CNNs pro- :","our model
achieves
significant improvements
over
Senna
other three neural models","our model||achieves||significant improvements
significant improvements||over||Senna
significant improvements||over||other three neural models
",,,"Results||has||our model
",,,,,
research-problem,Hierarchically - Refined Label Attention Network for Sequence Labeling,Sequence Labeling,,,,,"Contribution||has research problem||Sequence Labeling
",,,,
research-problem,CRF has been used as a powerful model for statistical sequence labeling .,statistical sequence labeling,,,,,"Contribution||has research problem||statistical sequence labeling
",,,,
model,"To this question , we investigate a neural network model for output label sequences .","investigate
neural network model
for
output label sequences","neural network model||for||output label sequences
",,"Model||investigate||neural network model
",,,,,,
model,"In particular , we represent each possible label using an embedding vector , and aim to encode sequences of label distributions using a recurrent neural network .","each possible label
using
embedding vector
aim to encode
sequences
of
label distributions
using
recurrent neural network","sequences||of||label distributions
label distributions||using||recurrent neural network
each possible label||using||embedding vector
",,"Model||aim to encode||sequences
","Model||represent||each possible label
",,,,,
model,This makes our task essentially to represent a full - exponential search space without making Markov assumptions .,"represent
full - exponential search space
without making
Markov assumptions","full - exponential search space||without making||Markov assumptions
",,"Model||represent||full - exponential search space
",,,,,,
results,WSJ . shows the final POS tagging results on WSJ .,WSJ,,,,"Results||has||WSJ
",,,,,"WSJ||has||BiLSTM - LAN
"
results,"BiLSTM - LAN gives significant accuracy improvements over both BiLSTM - CRF and BiLSTM- softmax ( p < 0.01 ) , which is consistent with observations on development experiments .","BiLSTM - LAN
gives
significant accuracy improvements
over
BiLSTM - CRF
BiLSTM- softmax","BiLSTM - LAN||gives||significant accuracy improvements
significant accuracy improvements||over||BiLSTM - CRF
significant accuracy improvements||over||BiLSTM- softmax
",,,,,,,,
results,"Universal Dependencies ( UD ) v 2.2 . We design a multilingual experiment to compare BiLSTMsoftmax , BiLSTM - CRF ( strictly following 1 , which is the state - of - theart on multi-lingual POS tagging ) and BiLSTM - LAN .","Universal Dependencies ( UD ) v 2.2
on",,,,"Results||has||Universal Dependencies ( UD ) v 2.2
",,,,,"Universal Dependencies ( UD ) v 2.2||has||Our model
"
results,Our model outperforms all the baselines on all the languages .,"Our model
outperforms
all the baselines
all the languages","Our model||outperforms||all the baselines
","all the baselines||on||all the languages
",,,,,,,
results,"The improvements are statistically significant for all the languages ( p < 0.01 ) , suggesting that BiLSTM - LAN is generally effective across languages .","improvements
are
statistically significant
for
all the languages ( p < 0.01 )
suggesting that
BiLSTM - LAN
is
generally effective
across
languages","improvements||are||statistically significant
statistically significant||for||all the languages ( p < 0.01 )
all the languages ( p < 0.01 )||suggesting that||BiLSTM - LAN
BiLSTM - LAN||is||generally effective
generally effective||across||languages
",,,,,,,"Universal Dependencies ( UD ) v 2.2||has||improvements
",
results,"OntoNotes 5.0 . In NER , BiLSTM - CRF is widely used , because local dependencies between neighboring labels relatively more important that POS tagging and CCG supertagging .","OntoNotes 5.0
BiLSTM - CRF",,,,"Results||has||OntoNotes 5.0
",,,,"significantly outperforms||has||BiLSTM - CRF
","OntoNotes 5.0||has||BiLSTM - LAN
"
results,BiLSTM - LAN also significantly outperforms BiLSTM - CRF by 1.17 F1-score ( p < 0.01 ) . CCGBank .,"BiLSTM - LAN
significantly outperforms
by
1.17 F1-score","significantly outperforms||by||1.17 F1-score
","BiLSTM - LAN||has||significantly outperforms
",,,,,,,
results,"As shown in , BiLSTM - LAN significantly outperforms both BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 ) , showing the advantage of LAN . and explore BiRNN - softmax and BiLSTM - softmax , respectively .","LAN
both
BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 )
showing
advantage
of","BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 )||showing||advantage
advantage||of||LAN
",,,,,"significantly outperforms||both||BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 )
",,,
results,"Compared with these methods , BiLSTM - LAN obtains new state - of - theart results on CCGBank , matching the tri-training performance of , without training on external data .","obtains
new state - of - theart results
on
CCGBank","new state - of - theart results||on||CCGBank
",,,,,"BiLSTM - LAN||obtains||new state - of - theart results
",,,
research-problem,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,Morphosyntactic Tagging,,,,,"Contribution||has research problem||Morphosyntactic Tagging
",,,,
model,We propose a novel model where we learn context sensitive initial character and word representations through two separate sentence - level recurrent models .,"learn
context sensitive initial character and word representations
through
two separate sentence - level recurrent models","context sensitive initial character and word representations||through||two separate sentence - level recurrent models
",,"Model||learn||context sensitive initial character and word representations
",,,,,,
model,These are then combined via a meta-BiLSTM model that builds a unified representation of each word that is then used for syntactic tagging .,"combined via
meta-BiLSTM model
builds
unified representation
of
each word
used for
syntactic tagging","meta-BiLSTM model||builds||unified representation
unified representation||used for||syntactic tagging
unified representation||of||each word
",,,,,"context sensitive initial character and word representations||combined via||meta-BiLSTM model
",,,
hyperparameters,The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training .,"word embeddings
are
initialized
with
zero values
pre-trained embeddings
not
updated
during
training","word embeddings||are||initialized
initialized||with||zero values
pre-trained embeddings||not||updated
updated||during||training
",,,"Hyperparameters||has||word embeddings
Hyperparameters||has||pre-trained embeddings
",,,,,
hyperparameters,The dropout used on the embeddings is achieved by a RRIE is the relative reduction in error .,"dropout
used on
embeddings
is
achieved by
RRIE
relative reduction in error","dropout||used on||embeddings
embeddings||achieved by||RRIE
RRIE||is||relative reduction in error
",,,"Hyperparameters||has||dropout
",,,,,
results,Part - of - Speech Tagging Results,Part - of - Speech Tagging Results,,,,"Results||has||Part - of - Speech Tagging Results
",,,,,"Part - of - Speech Tagging Results||has||Our model
"
results,Our model outperforms in 32 of the 54 treebanks with 13 ties .,"Our model
outperforms
in
32
of
54 treebanks
with
13 ties","outperforms||with||13 ties
outperforms||in||32
32||of||54 treebanks
","Our model||has||outperforms
",,,,,,,
results,"Our model tends to produce better results , especially for morphologically rich languages ( e.g. Slavic","produce
better results
especially for
morphologically rich languages","better results||especially for||morphologically rich languages
",,,,,"Our model||produce||better results
",,,
results,Morphological Tagging Results,Morphological Tagging Results,,,,"Results||has||Morphological Tagging Results
",,,,,"Morphological Tagging Results||has||Our models
"
results,"Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task ( i.e. , 1.8 % absolute improvement on average , corresponding to a RRIE of 21.20 % ) .","Our models
produce
significantly better results
than
winners of the CoNLL 2017 Shared Task","Our models||produce||significantly better results
significantly better results||than||winners of the CoNLL 2017 Shared Task
",,,,,,,,
ablation-analysis,shows that separately optimized models are significantly more accurate on average than jointly optimized models .,"shows that
separately optimized models
are
significantly more accurate
on
average
than
jointly optimized models","separately optimized models||are||significantly more accurate
significantly more accurate||than||jointly optimized models
significantly more accurate||on||average
",,"Ablation analysis||shows that||separately optimized models
",,,,,,
ablation-analysis,Separate optimization leads to better accuracy for 34 out of 40 treebanks for the morphological features task and for 30 out of 39 treebanks for xpos tagging .,"Separate optimization
leads to
better accuracy
for
34 out of 40 treebanks
for
morphological features task
for
30 out of 39 treebanks
for
xpos tagging","Separate optimization||leads to||better accuracy
better accuracy||for||30 out of 39 treebanks
30 out of 39 treebanks||for||xpos tagging
better accuracy||for||34 out of 40 treebanks
34 out of 40 treebanks||for||morphological features task
",,"Ablation analysis||for||Separate optimization
",,,,,,
ablation-analysis,"Separate optimization outperformed joint optimization by up to 2.1 percent absolute , while joint never out - performed separate by more than 0.5 % absolute .","outperformed
joint optimization
by
up to 2.1 percent absolute
joint
never
out - performed
separate
by
more than 0.5 % absolute","joint optimization||by||up to 2.1 percent absolute
joint||never||out - performed
out - performed||by||more than 0.5 % absolute
","outperformed||has||joint optimization
out - performed||has||separate
",,"Ablation analysis||has||joint
",,,,"Separate optimization||has||outperformed
",
ablation-analysis,The examples show that the combined model has significantly higher accuracy compared with either the character and word models individually .,"combined model
significantly higher accuracy
compared with
character and word models","significantly higher accuracy||compared with||character and word models
","combined model||has||significantly higher accuracy
",,"Ablation analysis||has||combined model
",,,,,
ablation-analysis,"For all of the network sizes in the grid search , we still observed during training that the accuracy reach a high value and degrades with more iterations for the character and word model .","For
all of the network sizes
in
grid search
observed during
training
that
accuracy
reach
high value
degrades with
more iterations
for
character and word model","all of the network sizes||in||grid search
all of the network sizes||observed during||training
training||that||accuracy
accuracy||degrades with||more iterations
more iterations||for||character and word model
accuracy||reach||high value
",,"Ablation analysis||For||all of the network sizes
",,,,,,
research-problem,A Novel Neural Network Model for Joint POS Tagging and Graph - based Dependency Parsing,Joint POS Tagging and Graph - based Dependency Parsing,,,,,"Contribution||has research problem||Joint POS Tagging and Graph - based Dependency Parsing
",,,,
code,Our code is open - source and available together with pre-trained models at : https://github.com/ datquocnguyen/jPTDP .,https://github.com/ datquocnguyen/jPTDP,,,,,"Contribution||Code||https://github.com/ datquocnguyen/jPTDP
",,,,
model,"In this paper , we propose a novel neural architecture for joint POS tagging and graph - based dependency parsing .","propose
novel neural architecture
for
joint POS tagging and graph - based dependency parsing","novel neural architecture||for||joint POS tagging and graph - based dependency parsing
",,"Model||propose||novel neural architecture
",,,,,,
model,Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTMthe bidirectional LSTM .,"learns
latent feature representations
shared for
POS tagging and dependency parsing tasks
using
BiLSTMthe bidirectional LSTM","latent feature representations||shared for||POS tagging and dependency parsing tasks
POS tagging and dependency parsing tasks||using||BiLSTMthe bidirectional LSTM
",,"Model||learns||latent feature representations
",,,,,,
experimental-setup,Our jPTDP is implemented using DYNET v 2.0 .,"jPTDP
implemented using
DYNET v 2.0","jPTDP||implemented using||DYNET v 2.0
",,,"Experimental setup||has||jPTDP
",,,,,
experimental-setup,"We optimize the objective function using Adam ( Kingma and Ba , 2014 ) with default DYNET parameter settings and no mini-batches .","optimize
objective function
using
Adam ( Kingma and Ba , 2014 )
with
default DYNET parameter settings","objective function||using||Adam ( Kingma and Ba , 2014 )
Adam ( Kingma and Ba , 2014 )||with||default DYNET parameter settings
",,"Experimental setup||optimize||objective function
",,,,,,
experimental-setup,"Following Kiperwasser and Goldberg ( 2016 b ) and , we apply a word dropout rate of 0.25 and Gaussian noise with ? = 0.2 .","apply
word dropout rate
of
0.25
Gaussian noise
with
? = 0.2","Gaussian noise||with||? = 0.2
word dropout rate||of||0.25
",,"Experimental setup||apply||Gaussian noise
Experimental setup||apply||word dropout rate
",,,,,,
experimental-setup,"For training , we run for 30 epochs , and evaluate the mixed accuracy of correctly assigning POS tag together with dependency arc and relation type on the development set after each training epoch .","run for
30 epochs",,,"Experimental setup||run for||30 epochs
",,,,,,
experimental-setup,We perform a minimal grid search of hyper - parameters on English .,"perform
minimal grid search
of
hyper - parameters
on
English","minimal grid search||of||hyper - parameters
hyper - parameters||on||English
",,"Experimental setup||perform||minimal grid search
",,,,,,
experimental-setup,"compares the POS tagging and dependency parsing results of our model jPTDP with results reported in prior work , using the same experimental setup .","compares
POS tagging and dependency parsing results
of
our model jPTDP","POS tagging and dependency parsing results||of||our model jPTDP
",,"Experimental setup||compares||POS tagging and dependency parsing results
",,,,,,
results,"In terms of dependency parsing , in most cases , our model jPTDP outperforms Stack - propagation .","In terms of
dependency parsing
our model jPTDP
outperforms
Stack - propagation","our model jPTDP||outperforms||Stack - propagation
","dependency parsing||has||our model jPTDP
","Results||In terms of||dependency parsing
",,,,,,
results,It is somewhat unexpected that our model produces about 7 % absolute lower LAS score than Stack - propagation on Dutch ( nl ) .,"produces
about 7 % absolute lower LAS score
than
Stack - propagation
on
Dutch ( nl )","about 7 % absolute lower LAS score||than||Stack - propagation
Stack - propagation||on||Dutch ( nl )
",,"Results||produces||about 7 % absolute lower LAS score
",,,,,,
results,"Without taking "" nl "" into account , our averaged LAS score over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .","Without taking
nl
into
account
averaged LAS score
over
all remaining languages
is
1.1 % absolute higher
than
Stack - propagation 's","nl||into||account
averaged LAS score||over||all remaining languages
all remaining languages||is||1.1 % absolute higher
1.1 % absolute higher||than||Stack - propagation 's
","nl||has||averaged LAS score
","Results||Without taking||nl
",,,,,,
results,"The last row in shows an absolute LAS improvement of 4.4 % on average when comparing our jPTDP with its simplified version of not using characterbased representations : specifically , morphologically rich languages get an averaged improvement of 9.3 % , vice versa 2.6 % for others .","shows
absolute LAS improvement
of
4.4 %
on
average
of
morphologically rich languages
get
averaged improvement
9.3 %","absolute LAS improvement||of||4.4 %
4.4 %||on||average
morphologically rich languages||get||averaged improvement
averaged improvement||of||9.3 %
",,"Results||shows||absolute LAS improvement
","Results||has||morphologically rich languages
",,,,,
results,"So , our jPDTP is particularly good for morphologically rich languages , with 1.7 % higher averaged LAS than Stack - propagation over these languages .","jPDTP
good for
morphologically rich languages
with
1.7 % higher averaged LAS
than
Stack - propagation","jPDTP||good for||morphologically rich languages
morphologically rich languages||with||1.7 % higher averaged LAS
1.7 % higher averaged LAS||than||Stack - propagation
",,,"Results||has||jPDTP
",,,,,
research-problem,Multilingual Part - of - Speech Tagging with Bidirectional Long Short - Term Memory Models and Auxiliary Loss,Multilingual Part - of - Speech Tagging,,,,,"Contribution||has research problem||Multilingual Part - of - Speech Tagging
",,,,
research-problem,"We address these issues and evaluate bi - LSTMs with word , character , and unicode byte embeddings for POS tagging .",POS tagging,,,,,"Contribution||has research problem||POS tagging
",,,,
model,"Finally , we introduce a novel model , a bi - LSTM trained with auxiliary loss .","introduce
novel model
bi - LSTM trained with auxiliary loss",,"novel model||has||bi - LSTM trained with auxiliary loss
","Model||introduce||novel model
",,,,,,
model,The model jointly predicts the POS and the log frequency of the word .,"jointly predicts
POS and the log frequency of the word",,,"Model||jointly predicts||POS and the log frequency of the word
",,,,,,
hyperparameters,"epochs , default learning rate ( 0.1 ) , 128 dimensions for word embeddings , 100 for character and byte embeddings , 100 hidden states and Gaussian noise with ?= 0.2 .","default learning rate
0.1
128 dimensions
for
word embeddings
100
for
character and byte embeddings
hidden states
Gaussian noise
with
0.2","100||for||character and byte embeddings
Gaussian noise||with||0.2
128 dimensions||for||word embeddings
","100||has||hidden states
default learning rate||has||0.1
",,"Hyperparameters||has||100
Hyperparameters||has||Gaussian noise
Hyperparameters||has||128 dimensions
Hyperparameters||has||default learning rate
",,,,,
hyperparameters,"As training is stochastic in nature , we use a fixed seed throughout .","training
is
stochastic
use
fixed seed","training||is||stochastic
stochastic||use||fixed seed
",,,"Hyperparameters||has||training
",,,,,
hyperparameters,In that case we use offthe - shelf polyglot embeddings .,"use
offthe - shelf polyglot embeddings",,,"Hyperparameters||use||offthe - shelf polyglot embeddings
",,,,,,
code,The code is released at : https : //github.com/bplank/bilstm-aux,https : //github.com/bplank/bilstm-aux,,,,,"Contribution||Code||https : //github.com/bplank/bilstm-aux
",,,,
results,"In an initial investigation , we compared Tnt , HunPos and TreeTagger and found Tnt to be consistently better than Treetagger , Hunpos followed closely but crashed on some languages ( e.g. , Arabic ) .","compared
Tnt , HunPos and TreeTagger
found
Tnt
to be
consistently better
than
Treetagger","Tnt , HunPos and TreeTagger||found||Tnt
Tnt||to be||consistently better
consistently better||than||Treetagger
",,"Results||compared||Tnt , HunPos and TreeTagger
",,,,,,
results,"The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre-trained embeddings .","combined word + character representation model
is
best representation
outperforming
baseline
on
all except one language ( Indonesian )","combined word + character representation model||is||best representation
baseline||on||all except one language ( Indonesian )
","best representation||has||outperforming
outperforming||has||baseline
",,"Results||has||combined word + character representation model
",,,,,
results,This model ( w + c ) reaches the biggest improvement ( more than + 2 % accuracy ) on Hebrew and Slovene .,"reaches
biggest improvement
more than + 2 % accuracy
on
Hebrew and Slovene","biggest improvement||on||Hebrew and Slovene
","biggest improvement||has||more than + 2 % accuracy
",,,,"combined word + character representation model||reaches||biggest improvement
",,,
results,Initializing the word embeddings ( + POLYGLOT ) with off - the - shelf languagespecific embeddings further improves accuracy .,"Initializing
word embeddings ( + POLYGLOT )
with
off - the - shelf languagespecific embeddings
improves
accuracy","word embeddings ( + POLYGLOT )||with||off - the - shelf languagespecific embeddings
off - the - shelf languagespecific embeddings||improves||accuracy
",,"Results||Initializing||word embeddings ( + POLYGLOT )
",,,,,,
results,The over all best system is the multi-task bi - LSTM FREQBIN ( it uses w + c and POLYGLOT initialization for w ) .,"over all best system
is
multi-task bi - LSTM FREQBIN","over all best system||is||multi-task bi - LSTM FREQBIN
",,,"Results||has||over all best system
",,,,,
research-problem,Document Expansion by Query Prediction,Document Expansion,,,,,"Contribution||has research problem||Document Expansion
",,,,
approach,"In this paper , we explore an alternative approach based on enriching the document representation ( prior to indexing ) .","explore
alternative approach
based on
enriching
document representation","alternative approach||based on||enriching
","enriching||has||document representation
","Approach||explore||alternative approach
",,,,,,
approach,"Focusing on question answering , we train a sequence - to - sequence model , that given a document , generates possible questions that the document might answer .","Focusing on
question answering
train
sequence - to - sequence model
that
given
document
generates
possible questions
document
might
answer","sequence - to - sequence model||given||document
document||generates||possible questions
possible questions||that||document
document||might||answer
sequence - to - sequence model||Focusing on||question answering
",,"Approach||train||sequence - to - sequence model
",,,,,,
baselines,BM25 : We use the Anserini open - source IR toolkit 3 to index the original ( non -expanded ) documents and BM25 to rank the passages .,"BM25
use
Anserini open - source IR toolkit
to index
original ( non -expanded ) documents
to rank
passages","BM25||to rank||passages
BM25||use||Anserini open - source IR toolkit
Anserini open - source IR toolkit||to index||original ( non -expanded ) documents
",,,"Baselines||has||BM25
",,,,,
baselines,BM25 + Doc2query :,BM25 + Doc2query,,,,"Baselines||has||BM25 + Doc2query
",,,,,
baselines,We first expand the documents using the proposed Doc2query method .,"expand
documents
using
proposed Doc2query method","documents||using||proposed Doc2query method
",,,,,"BM25 + Doc2query||expand||documents
",,,
baselines,We then index and rank the expanded documents exactly as in the BM25 method above .,"index and rank
expanded documents
exactly as in
BM25 method","expanded documents||exactly as in||BM25 method
",,,,,"BM25 + Doc2query||index and rank||expanded documents
",,,
baselines,RM3 :,RM3,,,,"Baselines||has||RM3
",,,,,
baselines,"To compare document expansion with query expansion , we applied the RM3 query expansion technique .","compare
document expansion
with
query expansion
applied
RM3 query expansion technique","document expansion||with||query expansion
document expansion||applied||RM3 query expansion technique
",,,,,"RM3||compare||document expansion
",,,
baselines,BM25 + BERT : We index and retrieve documents as in the BM25 condition and further re-rank the documents with BERT as described in .,"BM25 + BERT
index and retrieve
documents
as in
BM25 condition
further re-rank
documents
with
BERT","documents||with||BERT
BM25 + BERT||further re-rank||documents
documents||with||BERT
BM25 + BERT||index and retrieve||documents
documents||as in||BM25 condition
",,,"Baselines||has||BM25 + BERT
",,,,,
baselines,"BM25 + Doc2query + BERT : We expand , index , and retrieve documents as in the BM25 + Doc2query condition and further re-rank the documents with BERT .","BM25 + Doc2query + BERT
expand , index , and retrieve
documents
as in
BM25 + Doc2query condition
further re-rank
documents
with
BERT","BM25 + Doc2query + BERT||expand , index , and retrieve||documents
documents||as in||BM25 + Doc2query condition
BM25 + Doc2query + BERT||further re-rank||documents
",,,"Baselines||has||BM25 + Doc2query + BERT
",,,,,
results,Document expansion with our method ( BM25 + Doc2query ) improves retrieval effectiveness by ? 15 % for both datasets .,"Document expansion
with
our method ( BM25 + Doc2query )
improves
retrieval effectiveness
by
15 %
for
both datasets","Document expansion||with||our method ( BM25 + Doc2query )
our method ( BM25 + Doc2query )||improves||retrieval effectiveness
retrieval effectiveness||by||15 %
retrieval effectiveness||for||both datasets
",,,"Results||has||Document expansion
",,,,,
results,"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on TREC CAR ; for MS MARCO , we are near the state of the art .","combine
document expansion
with
state - of - the - art re-ranker ( BM25 + Doc2query + BERT )
achieve
best - known results
on
TREC CAR
for
MS MARCO
near
state of the art","document expansion||with||state - of - the - art re-ranker ( BM25 + Doc2query + BERT )
state - of - the - art re-ranker ( BM25 + Doc2query + BERT )||achieve||best - known results
best - known results||on||TREC CAR
state - of - the - art re-ranker ( BM25 + Doc2query + BERT )||for||MS MARCO
MS MARCO||near||state of the art
",,"Results||combine||document expansion
",,,,,,
results,"Our full re-ranking condition ( BM25 + Doc2query + BERT ) beats BM25 + BERT alone , which verifies that the contribution Input Document : July is the hottest month in Washington DC with an average temperature of 27C ( 80F ) and the coldest is January at 4C ( 38F ) with the most daily sunshine hours at 9 in July .","Our full re-ranking condition ( BM25 + Doc2query + BERT )
beats
BM25 + BERT alone","Our full re-ranking condition ( BM25 + Doc2query + BERT )||beats||BM25 + BERT alone
",,,"Results||has||Our full re-ranking condition ( BM25 + Doc2query + BERT )
",,,,,
results,"We notice that the model tends to copy some words from the input document ( e.g. , Washington DC , River , chromosome ) , meaning that it can effectively perform term re-weighting ( i.e. , increasing the importance of key terms ) .","notice that
model
copy
some words
from
input document","model||copy||some words
some words||from||input document
",,"Results||notice that||model
",,,,,,
results,"Nevertheless , the model also produces words not present in the input document ( e.g. , weather , relationship ) , which can be characterized as expansion by synonyms and other related terms .","produces
words
not present in
input document
characterized as
expansion
by
synonyms and other related terms","words||not present in||input document
words||characterized as||expansion
expansion||by||synonyms and other related terms
",,,,,"model||produces||words
",,,
results,"If we expand MS MARCO documents using only new words and retrieve the development set queries with BM25 , we obtain an MRR@10 of 18.8 ( as opposed to 18.4 when indexing with original documents ) .","expand
MS MARCO documents
using
only new words
retrieve
development set queries
with
BM25
obtain
MRR@10
of
18.8","MS MARCO documents||obtain||MRR@10
MRR@10||of||18.8
MS MARCO documents||using||only new words
MS MARCO documents||retrieve||development set queries
development set queries||with||BM25
",,"Results||expand||MS MARCO documents
",,,,,,
results,Expanding with copied words gives an MRR@10 of 19.7 .,"Expanding with
copied words
gives
MRR@10
of
19.7","copied words||gives||MRR@10
MRR@10||of||19.7
",,"Results||Expanding with||copied words
",,,,,,
results,"We achieve a higher MRR@10 of 21.5 when documents are expanded with both types of words , showing that they are complementary .","achieve
higher MRR@10
of
21.5
when
documents
expanded with
both types of words","higher MRR@10||of||21.5
higher MRR@10||when||documents
documents||expanded with||both types of words
",,"Results||achieve||higher MRR@10
",,,,,,
results,We find that the Recall@1000 of the MS MARCO development set increased from 85.3 ( BM25 ) to 89.3 ( BM25 + Doc2query ) .,"find that
Recall@1000
of
MS MARCO development set
increased from
85.3 ( BM25 )
to
89.3 ( BM25 + Doc2query )","Recall@1000||of||MS MARCO development set
MS MARCO development set||increased from||85.3 ( BM25 )
85.3 ( BM25 )||to||89.3 ( BM25 + Doc2query )
",,"Results||find that||Recall@1000
",,,,,,
results,"As a contrastive condition , we find that query expansion with RM3 hurts in both datasets , whether applied to the unexpanded corpus ( BM25 + RM3 ) or the expanded version ( BM25 + Doc2query + RM3 ) .","query expansion
with
RM3
hurts
in
both datasets","query expansion||with||RM3
hurts||in||both datasets
","RM3||has||hurts
",,"Results||find that||query expansion
",,,,,
results,"This result shows that document expansion can be more effective than query expansion , most likely because there are more signals to exploit as documents are much longer .","shows that
document expansion
more effective than
query expansion","document expansion||more effective than||query expansion
",,"Results||shows that||document expansion
",,,,,,
results,"Our method without a re-ranker ( BM25 + Doc2query ) adds a small latency increase over baseline BM25 ( 50 ms vs. 90 ms ) but is approximately seven times faster than a neural re-ranker that has a three points higher MRR@10 ( Single Duet v2 , which is presented as a baseline in MS MARCO by the organizers ) .","Our method without a re-ranker ( BM25 + Doc2query )
adds
small latency
over
baseline BM25 ( 50 ms vs. 90 ms )
is
seven times faster
than
neural re-ranker
that has
three points higher MRR@10","Our method without a re-ranker ( BM25 + Doc2query )||is||seven times faster
seven times faster||than||neural re-ranker
neural re-ranker||that has||three points higher MRR@10
Our method without a re-ranker ( BM25 + Doc2query )||adds||small latency
small latency||over||baseline BM25 ( 50 ms vs. 90 ms )
",,,"Results||has||Our method without a re-ranker ( BM25 + Doc2query )
",,,,,
research-problem,PASSAGE RE - RANKING WITH BERT,PASSAGE RE - RANKING,,,,,"Contribution||has research problem||PASSAGE RE - RANKING
",,,,
research-problem,"In this paper , we describe a simple re-implementation of BERT for query - based passage re-ranking .",query - based passage re-ranking,,,,,"Contribution||has research problem||query - based passage re-ranking
",,,,
approach,"In this paper , we describe in detail how we have re-purposed BERT as a passage re-ranker and achieved state - of - the - art results on the MS MARCO passage re-ranking task .","re-purposed
BERT
as
passage re-ranker","BERT||as||passage re-ranker
",,"Approach||re-purposed||BERT
",,,,,,
research-problem,PASSAGE RE - RANKING WITH BERT,,,,,,,,,,
experiments,MS MARCO,MS MARCO,,,,"Experiments||has||MS MARCO
",,,,,"MS MARCO||has||Hyperparameters
"
experiments,"We fine - tune the model using TPUs 1 with a batch size of 32 ( 32 sequences * 512 tokens = 16,384 tokens / batch ) for 400 k iterations , which takes approximately 70 hours .","fine - tune
model
using
TPUs
with
batch size
of
32
for
400 k iterations
takes
approximately 70 hours","model||with||batch size
batch size||of||32
model||using||TPUs
model||for||400 k iterations
model||takes||approximately 70 hours
",,,,,"Hyperparameters||fine - tune||model
",,,
experiments,"We use ADAM ( Kingma & Ba , 2014 ) with the initial learning rate set to 3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .","use
ADAM ( Kingma & Ba , 2014 )
with
initial learning rate
set to
3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999
L2 weight decay
of
0.01
learning rate warmup
over
first 10,000 steps
linear decay
of
learning rate","ADAM ( Kingma & Ba , 2014 )||with||initial learning rate
initial learning rate||set to||3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999
ADAM ( Kingma & Ba , 2014 )||with||learning rate warmup
learning rate warmup||over||first 10,000 steps
ADAM ( Kingma & Ba , 2014 )||with||L2 weight decay
L2 weight decay||of||0.01
ADAM ( Kingma & Ba , 2014 )||with||linear decay
linear decay||of||learning rate
",,,,,"Hyperparameters||use||ADAM ( Kingma & Ba , 2014 )
",,,
experiments,We use a dropout probability of 0.1 on all layers .,"dropout probability
of
0.1
on
all layers","dropout probability||of||0.1
0.1||on||all layers
",,,,,,,"Hyperparameters||use||dropout probability
",
,TREC - CAR,TREC - CAR,,,,,,,,,
experiments,"For the fine - tuning data , we generate our query - passage pairs by retrieving the top ten passages from the entire TREC - CAR corpus using BM25 .","For
fine - tuning
data
generate
our query - passage pairs
by retrieving
top ten passages
from
entire TREC - CAR corpus
using
BM25","data||generate||our query - passage pairs
our query - passage pairs||by retrieving||top ten passages
top ten passages||from||entire TREC - CAR corpus
entire TREC - CAR corpus||using||BM25
","fine - tuning||has||data
",,,,"TREC - CAR||For||fine - tuning
",,,
experiments,"We train it for 400 k iterations , or 12.8 M examples ( 400 k iterations * 32 pairs / batch ) , which corresponds to only 40 % of the training set .","train
400 k iterations
12.8 M examples",,,,,,"TREC - CAR||train||400 k iterations
TREC - CAR||train||12.8 M examples
",,,
results,"Despite training on a fraction of the data available , the proposed BERT - based models surpass the previous state - of - the - art models by a large margin on both of the tasks .","Despite
training
on
fraction of the data available
proposed BERT - based models
surpass
previous state - of - the - art models
by
large margin","proposed BERT - based models||surpass||previous state - of - the - art models
previous state - of - the - art models||by||large margin
training||on||fraction of the data available
","training||has||proposed BERT - based models
","Results||Despite||training
",,,,,,
research-problem,Multi - level Multimodal Common Semantic Space for Image - Phrase Grounding,Image - Phrase Grounding,,,,,"Contribution||has research problem||Image - Phrase Grounding
",,,,
research-problem,We address the problem of phrase grounding by learning a multi - level common semantic space shared by the textual and visual modalities .,phrase grounding,,,,,"Contribution||has research problem||phrase grounding
",,,,
model,"In this work , we propose to explicitly learn a non-linear mapping of the visual and textual modalities into a common space , and do so at different granularity for each domain .","explicitly learn
non-linear mapping
of
visual and textual modalities
into
common space
at
different granularity
for
each domain","non-linear mapping||of||visual and textual modalities
visual and textual modalities||into||common space
visual and textual modalities||at||different granularity
different granularity||for||each domain
",,"Model||explicitly learn||non-linear mapping
",,,,,,
model,"This common space mapping is trained with weak supervision and exploited at test - time with a multi - level multimodal attention mechanism , where a natural formalism for computing attention heatmaps at each level , attended features and pertinence scoring , enables us to solve the phrase grounding task elegantly and effectively .","common space mapping
trained with
weak supervision
exploited at
test - time
with
multi - level multimodal attention mechanism
where
natural formalism
for computing
attention heatmaps
at
each level
attended features
pertinence scoring
solve
phrase grounding task
elegantly and effectively","common space mapping||exploited at||test - time
test - time||with||multi - level multimodal attention mechanism
multi - level multimodal attention mechanism||where||natural formalism
natural formalism||solve||phrase grounding task
natural formalism||for computing||attention heatmaps
attention heatmaps||at||each level
natural formalism||for computing||attended features
natural formalism||for computing||pertinence scoring
common space mapping||trained with||weak supervision
","phrase grounding task||has||elegantly and effectively
",,"Model||has||common space mapping
",,,,,
hyperparameters,"We use a batch size of B = 32 , where for a batch of image - caption pairs each image ( caption ) is only related to one caption ( image ) .","use
batch size
of
B = 32","batch size||of||B = 32
",,"Hyperparameters||use||batch size
",,,,,,
hyperparameters,Image - caption pairs are sampled randomly with a uniform distribution .,"Image - caption pairs
sampled
randomly
with
uniform distribution","Image - caption pairs||sampled||randomly
randomly||with||uniform distribution
",,,"Hyperparameters||has||Image - caption pairs
",,,,,
hyperparameters,We train the network for 20 epochs with the Adam optimizer with lr = 0.001 where the learning rate is divided by 2 once at the 10 - th epoch and again at the 15 - th epoch .,"train
network
for
20 epochs
with
Adam optimizer
with
lr = 0.001
where
learning rate
divided by
2
once at
10 - th epoch
again at
15 - th epoch","network||with||Adam optimizer
Adam optimizer||with||lr = 0.001
lr = 0.001||where||learning rate
learning rate||divided by||2
2||again at||15 - th epoch
2||once at||10 - th epoch
network||for||20 epochs
",,"Hyperparameters||train||network
",,,,,,
hyperparameters,We use D = 1024 for common space mapping dimension and ? = 0.25 for Leaky ReLU in the non-linear mappings .,"D = 1024
for
common space mapping dimension
? = 0.25
for
Leaky ReLU
in
non-linear mappings","? = 0.25||for||Leaky ReLU
Leaky ReLU||in||non-linear mappings
D = 1024||for||common space mapping dimension
",,,"Hyperparameters||use||? = 0.25
Hyperparameters||use||D = 1024
",,,,,
hyperparameters,We regularize weights of the mappings with l 2 regularization with reg value = 0.0005 .,"regularize
weights
of
mappings
with
l 2 regularization
with
reg value = 0.0005","weights||with||l 2 regularization
l 2 regularization||with||reg value = 0.0005
weights||of||mappings
",,"Hyperparameters||regularize||weights
",,,,,,
hyperparameters,"For VGG , we take outputs from { conv 4 1 , conv 4 3 , conv5 1 , conv5 3 } and map to semantic feature maps with dimension 18181024 , and for PNAS - Net we take outputs from { Cell 5 , Cell 7 , Cell 9 , Cell 11 } pointing game accuracy attention correctness Ours Ours Ours Ours Class","For
VGG
take
outputs
from
{ conv 4 1 , conv 4 3 , conv5 1 , conv5 3 }
map to
semantic feature maps
with dimension
18181024
for
PNAS - Net
take
outputs
from
{ Cell 5 , Cell 7 , Cell 9 , Cell 11 }","VGG||take||outputs
outputs||map to||semantic feature maps
semantic feature maps||with dimension||18181024
outputs||from||{ conv 4 1 , conv 4 3 , conv5 1 , conv5 3 }
PNAS - Net||take||outputs
outputs||from||{ Cell 5 , Cell 7 , Cell 9 , Cell 11 }
",,"Hyperparameters||For||VGG
Hyperparameters||for||PNAS - Net
",,,,,,
hyperparameters,Both visual and textual networks weights are fixed during training and only common space mapping weights are trainable .,"Both visual and textual networks weights
are
fixed during
training
common space mapping weights
trainable","common space mapping weights||are||trainable
Both visual and textual networks weights||fixed during||training
",,,"Hyperparameters||has||common space mapping weights
Hyperparameters||has||Both visual and textual networks weights
",,,,,
results,The results show that our method significantly outperforms all state - of - the - art methods in all conditions and all datasets .,"show that
method
significantly outperforms
all state - of - the - art methods
in
all conditions and all datasets","significantly outperforms||in||all conditions and all datasets
","method||has||significantly outperforms
significantly outperforms||has||all state - of - the - art methods
","Results||show that||method
",,,,,,
results,"For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .","model
state - of - the - art
on
Flickr30 k",,"Flickr30 k||has||model
","Results||on||Flickr30 k
",,,,"state - of - the - art||on||all categories
state - of - the - art||on||both metrics
","consistently outperforms||has||state - of - the - art
",
results,We observe that our method obtains a higher performance on almost all categories even when VGG16 is used as the visual backbone .,"observe that
our method
obtains
higher performance
on
almost all categories","our method||obtains||higher performance
higher performance||on||almost all categories
",,,,,"Flickr30 k||observe that||our method
",,,
results,The model based on PNASNet consistently outperforms the state - of - the - art on all categories on both metrics .,"based on
PNASNet
consistently outperforms
on
all categories
both metrics",,"PNASNet||has||consistently outperforms
",,,,"model||based on||PNASNet
",,,
results,It shows that the 3rd level dominates the selection while the 4th level is also important for several categories such as scene and animals .,"3rd level
dominates
selection
4th level
important for
several categories
such as
scene and animals","3rd level||dominates||selection
4th level||important for||several categories
several categories||such as||scene and animals
",,,"Results||show that||3rd level
Results||show that||4th level
",,,,,
results,The 1st level is exploited mostly for the animals and people categories .,"1st level
exploited mostly for
animals and people categories","1st level||exploited mostly for||animals and people categories
",,,"Results||show that||1st level
",,,,,
results,"The full sentence selection relies mostly on the 3rd level as well , while for some sentences the 4th model has been selected .","full sentence selection
relies mostly on
3rd level
for
some sentences
4th model
been
selected","4th model||been||selected
full sentence selection||relies mostly on||3rd level
","some sentences||has||4th model
","Results||for||some sentences
","Results||has||full sentence selection
",,,,,
ablation-analysis,"The results in rows 1 , 2 show that using level - attention mechanism based on multi-level feature maps significantly improves the performance over single visual - textual feature comparison .","using
level - attention mechanism
based on
multi-level feature maps
significantly improves
performance
over
single visual - textual feature comparison","level - attention mechanism||based on||multi-level feature maps
level - attention mechanism||significantly improves||performance
performance||over||single visual - textual feature comparison
",,"Ablation analysis||using||level - attention mechanism
",,,,,,
ablation-analysis,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing any mapping with a linear one significantly degrades the performance .","see that
non-linear mapping
in
model
is
really important
replacing
any mapping
with
linear one
significantly degrades
performance","non-linear mapping||in||model
non-linear mapping||is||really important
any mapping||with||linear one
linear one||significantly degrades||performance
",,"Ablation analysis||see that||non-linear mapping
Ablation analysis||replacing||any mapping
",,,,,,
ablation-analysis,"We can also see that non-linear mapping seems more important on the visual side , but best results are obtained with both text and visual non-linear mappings .","also see that
non-linear mapping
seems
more important
on
visual side","non-linear mapping||seems||more important
more important||on||visual side
",,"Ablation analysis||also see that||non-linear mapping
",,,,,,
ablation-analysis,"The results in rows 1 , 3 and 2 , 6 show the importance of using a strong contextualized text embedding as the performance drops significantly .","show
importance
of using
strong contextualized text embedding
as
performance
drops significantly","importance||of using||strong contextualized text embedding
strong contextualized text embedding||as||performance
","performance||has||drops significantly
","Ablation analysis||show||importance
",,,,,,
ablation-analysis,"We also study the use of softmax on the heatmaps , comparing rows 2 , 8 , we see that applying softmax leads to a very negative effect on the performance .","softmax
on
applying
leads to
very negative effect
performance","softmax||leads to||very negative effect
very negative effect||on||performance
",,"Ablation analysis||applying||softmax
",,,,,,
research-problem,Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,Predicting Prosodic Prominence from Text,,,,,"Contribution||has research problem||Predicting Prosodic Prominence from Text
",,,,
research-problem,In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text .,predicting prosodic prominence from written text,,,,,"Contribution||has research problem||predicting prosodic prominence from written text
",,,,
dataset,"In this paper we introduce a new NLP dataset and benchmark for predicting prosodic prominence from text which is based on the recently published Libri TTS corpus , containing automatically generated prosodic prominence labels for over 260 hours or 2.8 million words of English audio books , read by 1230 different speakers .","for predicting
prosodic prominence
from
text
based on
recently published Libri TTS corpus
containing
automatically generated prosodic prominence labels
for
over 260 hours
2.8 million words
of
English audio books
read by
1230 different speakers","prosodic prominence||based on||recently published Libri TTS corpus
prosodic prominence||containing||automatically generated prosodic prominence labels
automatically generated prosodic prominence labels||read by||1230 different speakers
automatically generated prosodic prominence labels||for||over 260 hours
automatically generated prosodic prominence labels||for||2.8 million words
2.8 million words||of||English audio books
prosodic prominence||from||text
",,"Dataset||for predicting||prosodic prominence
",,,,,,
dataset,To our knowledge this will be the largest publicly available dataset with prosodic annotations .,"will be
largest publicly available dataset
with
prosodic annotations","largest publicly available dataset||with||prosodic annotations
",,"Dataset||will be||largest publicly available dataset
",,,,,,
research-problem,Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text .,Prosody prediction,,,,,"Contribution||has research problem||Prosody prediction
",,,,
experimental-setup,We performed experiments with the following models :,models,,,,"Experimental setup||has||models
",,,,,"models||name||BERT - base uncased
models||name||3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM )
models||name||Minitagger ( SVM ) ) + GloVe
models||name||MarMoT ( CRF )
models||name||Majority class per word
"
experimental-setup,"BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word","BERT - base uncased
3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM )
Minitagger ( SVM ) ) + GloVe
MarMoT ( CRF )
Majority class per word",,,,,,,,,
experimental-setup,"We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library , 3 which we further fine - tune during training .","use
Huggingface PyTorch implementation
of
BERT
available in
pytorch transformers library
further
fine - tune
during
training","Huggingface PyTorch implementation||of||BERT
BERT||available in||pytorch transformers library
BERT||further||fine - tune
fine - tune||during||training
",,"Experimental setup||use||Huggingface PyTorch implementation
",,,,,,
experimental-setup,"We take the last hidden layer of BERT and train a single fully - connected classifier layer on top of it , mapping the representation of each word to the labels .","take
last hidden layer
of
BERT
train
single fully - connected classifier layer
on
top
of
mapping
representation
each word
to
labels","representation||of||each word
each word||to||labels
last hidden layer||of||BERT
last hidden layer||train||single fully - connected classifier layer
single fully - connected classifier layer||mapping||representation
representation||of||each word
each word||to||labels
single fully - connected classifier layer||on||top
",,"Experimental setup||take||last hidden layer
",,,,,,
experimental-setup,For our experiments we use the smaller BERT - base model using the uncased alternative .,"smaller BERT - base model
using
uncased alternative","smaller BERT - base model||using||uncased alternative
",,,"Experimental setup||use||smaller BERT - base model
",,,,,
experimental-setup,We use a batch size of 32 and fine - tune the model for 2 epochs .,"batch size
of
32
fine - tune
model
for
2 epochs","batch size||of||32
model||for||2 epochs
",,"Experimental setup||fine - tune||model
","Experimental setup||use||batch size
",,,,,
experimental-setup,For BiLSTM we use pre-trained 300D Glo Ve 840B word embeddings .,"For
BiLSTM
use
pre-trained 300D Glo Ve 840B word embeddings","BiLSTM||use||pre-trained 300D Glo Ve 840B word embeddings
",,"Experimental setup||For||BiLSTM
",,,,,,
experimental-setup,"As with BERT , we add one fullyconnected classifier layer on top of the BiLSTM , mapping the representation of each word to the labels .","add
one fullyconnected classifier layer
on top of
BiLSTM
mapping
representation
of
each word
to
labels","one fullyconnected classifier layer||mapping||representation
one fullyconnected classifier layer||on top of||BiLSTM
",,"Experimental setup||add||one fullyconnected classifier layer
",,,,,,
experimental-setup,We use a dropout of 0.2 between the layers of the BiLSTM .,"dropout
of
0.2
between
layers of the BiLSTM","dropout||of||0.2
0.2||between||layers of the BiLSTM
",,,"Experimental setup||use||dropout
",,,,,
experimental-setup,"For the SVM we use Minitagger 4 implementation by using each dimension of the pre-trained 300D Glo Ve 840B word embeddings as features , with context - size 1 , i.e. including the previous and the next word in the context .","SVM
use
Minitagger 4 implementation
using
each dimension
of
pre-trained 300D Glo Ve 840B word embeddings
as
features
with
context - size 1","SVM||use||Minitagger 4 implementation
Minitagger 4 implementation||using||each dimension
each dimension||of||pre-trained 300D Glo Ve 840B word embeddings
pre-trained 300D Glo Ve 840B word embeddings||with||context - size 1
pre-trained 300D Glo Ve 840B word embeddings||as||features
",,,"Experimental setup||For||SVM
",,,,,
experimental-setup,For the conditional random field ( CRF ) model we use MarMot 5 by with the default configuration .,"conditional random field ( CRF ) model
use
MarMot
with
default configuration","conditional random field ( CRF ) model||use||MarMot
MarMot||with||default configuration
",,,"Experimental setup||For||conditional random field ( CRF ) model
",,,,,
code,All systems except the Minitagger and CRF are our implementations using PyTorch and are made available on GitHub : https://github.com/Helsinki - NLP / prosody .,https://github.com/Helsinki - NLP / prosody,,,,,"Contribution||Code||https://github.com/Helsinki - NLP / prosody
",,,,
results,All models reach over 80 % in the 2 - way classification task while 3 - way classification accuracy stays below 70 % for all of them .,"All models
reach over
80 %
in
2 - way classification task
3 - way classification accuracy
stays below
70 %","3 - way classification accuracy||stays below||70 %
All models||reach over||80 %
80 %||in||2 - way classification task
","All models||has||3 - way classification accuracy
",,"Results||has||All models
",,,,,
results,"The BERTbased model gets the highest accuracy of 83.2 % and 68.6 % in the 2 - way and 3 - way classification tasks , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/","BERTbased model
gets
highest accuracy
of
83.2 % and 68.6 %
in
2 - way and 3 - way classification tasks","BERTbased model||gets||highest accuracy
highest accuracy||of||83.2 % and 68.6 %
83.2 % and 68.6 %||in||2 - way and 3 - way classification tasks
",,,"Results||has||BERTbased model
",,,,,
results,The 3layer BiLSTM achieves 82.1 % in the 2 - way classification and 66.4 % in the 3 - way classification task .,"3layer BiLSTM
achieves
82.1 %
in
2 - way classification
66.4 %
in
3 - way classification task","3layer BiLSTM||achieves||82.1 %
82.1 %||in||2 - way classification
3layer BiLSTM||achieves||66.4 %
66.4 %||in||3 - way classification task
",,,"Results||has||3layer BiLSTM
",,,,,
results,"The traditional feature - based classifiers perform slightly below the neural network models , with the CRF obtaining 81.8 % and 66.4 % for the two classification tasks , respectively .","traditional feature - based classifiers
perform
slightly below
neural network models
with
CRF
obtaining
81.8 % and 66.4 %
for
two classification tasks","traditional feature - based classifiers||perform||slightly below
neural network models||with||CRF
CRF||obtaining||81.8 % and 66.4 %
81.8 % and 66.4 %||for||two classification tasks
","slightly below||has||neural network models
",,"Results||has||traditional feature - based classifiers
",,,,,
results,The Minitagger SVM model 's test accuracies are slightly lower than the CRF 's with 80.8 % and 65.4 % test accuracies .,"Minitagger SVM model 's test accuracies
are
slightly lower
than
CRF 's
with
80.8 % and 65.4 % test accuracies","Minitagger SVM model 's test accuracies||are||slightly lower
slightly lower||than||CRF 's
CRF 's||with||80.8 % and 65.4 % test accuracies
",,,"Results||has||Minitagger SVM model 's test accuracies
",,,,,
results,Finally taking a simple majority class per word gives 80.2 % for the 2 - way classification task and 62.4 % for the 3 - way classification task .,"taking
simple majority class per word
gives
80.2 %
for
2 - way classification task
62.4 %
for
3 - way classification task","simple majority class per word||gives||80.2 %
80.2 %||for||2 - way classification task
simple majority class per word||gives||62.4 %
62.4 %||for||3 - way classification task
",,"Results||taking||simple majority class per word
",,,,,,
results,For most of the models the biggest improvement in performance is achieved when moving from 1 % of the training examples to 5 % .,"For
most of the models
biggest improvement
in
performance
achieved when
moving
from
1 %
of
training examples
to
5 %","biggest improvement||in||performance
biggest improvement||achieved when||moving
moving||from||1 %
1 %||of||training examples
1 %||to||5 %
","most of the models||has||biggest improvement
","Results||For||most of the models
",,,,,,
results,All models have reached close to their full predictive capacity with only 10 % of the training examples .,"reached close to
full predictive capacity
with
only 10 % of the training examples","full predictive capacity||with||only 10 % of the training examples
",,,,,"All models||reached close to||full predictive capacity
",,,
results,"As the proposed dataset has been automatically generated as described in Section 3 , we also tested the best two models , BERT and BiLSTM , with a manually annotated test set from The Boston University radio news corpus . :","with
manually annotated test set from The Boston University radio news corpus",,,"Results||with||manually annotated test set from The Boston University radio news corpus
",,,,,,"manually annotated test set from The Boston University radio news corpus||has||good results
"
results,The good results 6 from this experiment provide further support for the quality of the new dataset .,"good results
provide
further support
for
quality of the new dataset","good results||provide||further support
further support||for||quality of the new dataset
",,,,,,,,
results,Notice also that the difference between BERT and BiLSTM is much bigger with this test set ( + 3.9 % compared to + 1.1 % ) .,"difference
between
BERT and BiLSTM
much bigger","difference||between||BERT and BiLSTM
","difference||has||much bigger
",,,,,,"manually annotated test set from The Boston University radio news corpus||has||difference
",
research-problem,Identifying Well - formed Natural Language Questions,Identifying Well - formed Natural Language Questions,,,,,"Contribution||has research problem||Identifying Well - formed Natural Language Questions
",,,,
model,"Thus , in this paper we present a model to predict whether a given query is a well - formed natural language question .","to predict
given query
is
well - formed natural language question","given query||is||well - formed natural language question
",,"Model||to predict||given query
",,,,,,
dataset,"We construct and publicly release a dataset of 25,100 queries annotated with the probability of being a well - formed natural language question ( 2.1 ) .","construct and publicly release
dataset of 25,100 queries
annotated with
probability
of being
well - formed natural language question","dataset of 25,100 queries||annotated with||probability
probability||of being||well - formed natural language question
",,"Dataset||construct and publicly release||dataset of 25,100 queries
",,,,,,
model,We then train a feed - forward neural network classifier that uses the lexical and syntactic features extracted from the query on this data ( 2.2 ) .,"train
feed - forward neural network classifier
uses
lexical and syntactic features
extracted from
query
on
data","feed - forward neural network classifier||uses||lexical and syntactic features
lexical and syntactic features||extracted from||query
query||on||data
",,"Model||train||feed - forward neural network classifier
",,,,,,
dataset,Our dataset ise available for download at http://goo.gl/language/ query-wellformedness .,"available for download at
http://goo.gl/language/ query-wellformedness",,,"Dataset||available for download at||http://goo.gl/language/ query-wellformedness
",,,,,,
results,"The best performance obtained is 70.7 % while using word - 1 , 2 - grams and POS - 1 , 2 , 3 - grams as features .","best performance
obtained
70.7 %
while using
word - 1 , 2 - grams
POS - 1 , 2 , 3 - grams","best performance||obtained||70.7 %
70.7 %||while using||word - 1 , 2 - grams
70.7 %||while using||POS - 1 , 2 , 3 - grams
",,,"Results||has||best performance
",,,,,
results,Using POS n-grams gave a strong boost of 5.2 points over word unigrams and bigrams .,"Using
POS n-grams
gave
strong boost
of
5.2 points
over
word unigrams and bigrams","POS n-grams||gave||strong boost
strong boost||of||5.2 points
5.2 points||over||word unigrams and bigrams
",,"Results||Using||POS n-grams
",,,,,,
results,"Although character - 3 , 4 grams gave improvement over word unigrams and bigrams , the performance did not sustain when combined with POS tags .","character - 3 , 4 grams
gave improvement over
word unigrams and bigrams
performance
not
sustain
when
combined
with
POS tags","character - 3 , 4 grams||gave improvement over||word unigrams and bigrams
character - 3 , 4 grams||when||combined
combined||with||POS tags
performance||not||sustain
","combined||has||performance
",,"Results||has||character - 3 , 4 grams
",,,,,
baselines,The majority class baseline is 61.5 % which corresponds to all queries being classified non-wellformed .,"majority class baseline
is
61.5 %
corresponds to
all queries being classified non-wellformed","majority class baseline||corresponds to||all queries being classified non-wellformed
majority class baseline||is||61.5 %
",,,"Baselines||has||majority class baseline
",,,,,
baselines,The question word baseline that classifies any query starting with a question word word n-grams char n-grams POS n -grams pwf ( q ) :,"question word baseline
classifies
any query
starting with
question word","question word baseline||classifies||any query
any query||starting with||question word
",,,"Baselines||has||question word baseline
",,,,,
research-problem,Open Question Answering with Weakly Supervised Embedding Models,Open Question Answering,,,,,"Contribution||has research problem||Open Question Answering
",,,,
research-problem,Building computers able to answer questions on any subject is along standing goal of artificial intelligence .,Building computers able to answer questions on any subject,,,,,"Contribution||has research problem||Building computers able to answer questions on any subject
",,,,
research-problem,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",open - domain question answering,,,,,"Contribution||has research problem||open - domain question answering
",,,,
approach,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .","converting questions
to ( uninterpretable ) vectorial representations
which require
no pre-defined grammars or lexicons
can query
any KB
independent of
schema","to ( uninterpretable ) vectorial representations||can query||any KB
any KB||independent of||schema
to ( uninterpretable ) vectorial representations||which require||no pre-defined grammars or lexicons
",,"Approach||converting questions||to ( uninterpretable ) vectorial representations
",,,,,,
approach,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,"learning
low - dimensional vector embeddings
of
words and of KB triples
so that
representations of questions and corresponding answers
end up
similar
in
embedding space","low - dimensional vector embeddings||of||words and of KB triples
words and of KB triples||so that||representations of questions and corresponding answers
representations of questions and corresponding answers||end up||similar
similar||in||embedding space
",,"Approach||learning||low - dimensional vector embeddings
",,,,,,
approach,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .","make use of
weak supervision",,,"Approach||make use of||weak supervision
",,,,,,
approach,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,"end up learning
meaningful vectorial representations
for
questions
involving
up to 800 k words
triples
of
mostly automatically created KB
with
2.4 M entities
600 k relationships","meaningful vectorial representations||for||triples
triples||of||mostly automatically created KB
mostly automatically created KB||with||2.4 M entities
mostly automatically created KB||with||600 k relationships
meaningful vectorial representations||for||questions
questions||involving||up to 800 k words
",,"Approach||end up learning||meaningful vectorial representations
",,,,,,
approach,"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .","propose
fine - tune embedding - based models
optimizing
a matrix
parameterizing
similarity
used in
embedding space
leading to
consistent improvement in performance","fine - tune embedding - based models||optimizing||a matrix
a matrix||parameterizing||similarity
similarity||used in||embedding space
embedding space||leading to||consistent improvement in performance
",,"Approach||propose||fine - tune embedding - based models
",,,,,,
results,Reranking,Reranking,,,,"Results||has||Reranking
",,,,,
results,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .","see that
multitasking with paraphrase data
improves F1
from 0.60 to 0.68","multitasking with paraphrase data||improves F1||from 0.60 to 0.68
",,,,,"Reranking||see that||multitasking with paraphrase data
",,,
results,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,"Fine - tuning the embedding model
to optimize
top of the list
grants a bump of
5 points
of
F1","Fine - tuning the embedding model||to optimize||top of the list
Fine - tuning the embedding model||grants a bump of||5 points
5 points||of||F1
",,,,,,,"Reranking||has||Fine - tuning the embedding model
",
results,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .","All versions of our system
greatly outperform
paralex
improves the F1 - score
by almost 20 points","All versions of our system||greatly outperform||paralex
All versions of our system||improves the F1 - score||by almost 20 points
",,,,,,,"Reranking||has||All versions of our system
",
research-problem,Convolutional Neural Network Architectures for Matching Natural Language Sentences,Matching Natural Language Sentences,,,,,"Contribution||has research problem||Matching Natural Language Sentences
",,,,
research-problem,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",Semantic matching,,,,,"Contribution||has research problem||Semantic matching
",,,,
research-problem,Matching two potentially heterogenous language objects is central to many natural language applications .,Matching two potentially heterogenous language objects,,,,,"Contribution||has research problem||Matching two potentially heterogenous language objects
",,,,
research-problem,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,sentence - matching,,,,,"Contribution||has research problem||sentence - matching
",,,,
model,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .","propose
deep neural network models
adapt
convolutional strategy
to
natural language","deep neural network models||adapt||convolutional strategy
convolutional strategy||to||natural language
",,"Model||propose||deep neural network models
",,,,,,
model,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .","To further explore
relation
between
representing sentences and matching them
devise
novel model
can naturally host
hierarchical composition for sentences
simple - to - comprehensive fusion of matching patterns
with
same convolutional architecture","relation||between||representing sentences and matching them
representing sentences and matching them||devise||novel model
novel model||with||same convolutional architecture
same convolutional architecture||can naturally host||hierarchical composition for sentences
same convolutional architecture||can naturally host||simple - to - comprehensive fusion of matching patterns
",,"Model||To further explore||relation
",,,,,,
hyperparameters,"In other words , We use stochastic gradient descent for the optimization of models .","use
stochastic gradient descent
for
optimization of models","stochastic gradient descent||for||optimization of models
",,"Hyperparameters||use||stochastic gradient descent
",,,,,,
hyperparameters,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,"perform better with
mini-batch ( 100 ? 200 in sizes )
can be
parallelized
on
single machine
with
multi-cores","mini-batch ( 100 ? 200 in sizes )||can be||parallelized
parallelized||on||single machine
single machine||with||multi-cores
",,"Hyperparameters||perform better with||mini-batch ( 100 ? 200 in sizes )
",,,,,,
hyperparameters,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .","For regularization
for
early stopping
models
with
medium size
large training sets
with
over 500K instances","early stopping||for||models
models||with||medium size
models||with||large training sets
large training sets||with||over 500K instances
",,"Hyperparameters||For regularization||early stopping
",,,,,,
hyperparameters,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .","50 - dimensional word embedding
trained with
Word2 Vec","50 - dimensional word embedding||trained with||Word2 Vec
",,,"Hyperparameters||use||50 - dimensional word embedding
",,,,,
hyperparameters,"We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .","3 - word window
throughout
all experiments
test
various numbers of feature maps
typically from
200 to 500
for
optimal performance","various numbers of feature maps||typically from||200 to 500
200 to 500||for||optimal performance
3 - word window||throughout||all experiments
",,"Hyperparameters||test||various numbers of feature maps
","Hyperparameters||use||3 - word window
",,,,,
hyperparameters,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .","ReLu
as
activation function
for
all of models ( convolution and MLP )
yields
comparable or better results
to
sigmoid - like functions
converges
faster","ReLu||as||activation function
activation function||for||all of models ( convolution and MLP )
all of models ( convolution and MLP )||converges||faster
all of models ( convolution and MLP )||yields||comparable or better results
comparable or better results||to||sigmoid - like functions
",,,"Hyperparameters||use||ReLu
",,,,,
baselines,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,"WORDEMBED
represent
each short - text
as
sum of the embedding of the words it contains","WORDEMBED||represent||each short - text
each short - text||as||sum of the embedding of the words it contains
",,,"Baselines||has||WORDEMBED
",,,,,
baselines,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :","matching score
of
two short - texts
calculated
with
MLP
with
embedding of the two documents as input
DEEPMATCH
take
matching model
train it on
our datasets
with
3 hidden layers and 1,000 hidden nodes in the first hidden layer
URAE+ MLP","DEEPMATCH||take||matching model
matching model||train it on||our datasets
our datasets||with||3 hidden layers and 1,000 hidden nodes in the first hidden layer
matching score||of||two short - texts
two short - texts||with||MLP
MLP||with||embedding of the two documents as input
",,,"Baselines||has||URAE+ MLP
Baselines||has||DEEPMATCH
",,"WORDEMBED||calculated||matching score
",,,
baselines,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :","use
Unfolding Recursive Autoencoder
get
100 dimensional vector representation
of
each sentence
put
an MLP
on the top as in
WORDEMBED
SENNA + MLP / SIM","Unfolding Recursive Autoencoder||get||100 dimensional vector representation
100 dimensional vector representation||of||each sentence
Unfolding Recursive Autoencoder||put||an MLP
an MLP||on the top as in||WORDEMBED
",,,"Baselines||has||SENNA + MLP / SIM
",,"URAE+ MLP||use||Unfolding Recursive Autoencoder
","SENNA + MLP / SIM||use||SENNA - type sentence model
",,
baselines,We use the SENNA - type sentence model for sentence representation ;,"use
SENNA - type sentence model
for
sentence representation","SENNA - type sentence model||for||sentence representation
",,,,,,,,
baselines,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .","SENMLP
take
whole sentence
as
input
with
word embedding aligned sequentially
use
MLP
to obtain
score of coherence","SENMLP||take||whole sentence
whole sentence||as||input
input||with||word embedding aligned sequentially
SENMLP||use||MLP
MLP||to obtain||score of coherence
",,,"Baselines||has||SENMLP
",,,,,
results,ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .,"ARC - II
outperforms others significantly when
training instances are relatively abundant","ARC - II||outperforms others significantly when||training instances are relatively abundant
",,,"Results||has||ARC - II
",,,,,
results,"As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .","convolutional models ( ARC - I & II , SENNA + MLP )
perform favorably over
bag - of - words models","convolutional models ( ARC - I & II , SENNA + MLP )||perform favorably over||bag - of - words models
",,,"Results||has||convolutional models ( ARC - I & II , SENNA + MLP )
",,,,,
results,"Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .","ARC - I and ARC - II
trained purely with
random negatives
automatically gain some ability in telling whether
words in a given sentence
in
right sequential order","ARC - I and ARC - II||trained purely with||random negatives
random negatives||automatically gain some ability in telling whether||words in a given sentence
words in a given sentence||in||right sequential order
",,,"Results||has||ARC - I and ARC - II
",,,,,
results,We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .,"simple sum of embedding learned via Word2 Vec
yields
reasonably good results
on
all three tasks","simple sum of embedding learned via Word2 Vec||yields||reasonably good results
reasonably good results||on||all three tasks
",,,"Results||has||simple sum of embedding learned via Word2 Vec
",,,,,
research-problem,Large - scale Simple Question Answering with Memory Networks,Large - scale Simple Question Answering,,,,,"Contribution||has research problem||Large - scale Simple Question Answering
",,,,
research-problem,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,Training large - scale question answering systems,,,,,"Contribution||has research problem||Training large - scale question answering systems
",,,,
research-problem,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",simple question answering,,,,,"Contribution||has research problem||simple question answering
",,,,
dataset,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .","to study
coverage
of
existing systems
to train
jointly
on
different data sources
via
multitasking
collected
first large - scale dataset of questions and answers based on a KB
called
SimpleQuestions","first large - scale dataset of questions and answers based on a KB||called||SimpleQuestions
first large - scale dataset of questions and answers based on a KB||to study||coverage
coverage||of||existing systems
first large - scale dataset of questions and answers based on a KB||to train||jointly
jointly||on||different data sources
different data sources||via||multitasking
",,"Dataset||collected||first large - scale dataset of questions and answers based on a KB
",,,,,,
model,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .","embedding - based QA system
developed under
framework of Memory Networks ( Mem NNs )","embedding - based QA system||developed under||framework of Memory Networks ( Mem NNs )
",,,"Model||has||embedding - based QA system
",,,,,
model,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,"setting of
simple QA
corresponds to
elementary operation
performing
single lookup in the memory","simple QA||corresponds to||elementary operation
elementary operation||performing||single lookup in the memory
",,"Model||setting of||simple QA
",,,,,,
hyperparameters,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?","embedding dimension
learning rate
{ 64 , 128 , 256 }
{ 1 , 0.1 , ... , 1.0e ? 4 }
margin",,,"Hyperparameters||learning rate||{ 1 , 0.1 , ... , 1.0e ? 4 }
Hyperparameters||embedding dimension||{ 64 , 128 , 256 }
",,,,,,"Hyperparameters||margin||0.1
"
hyperparameters,was set to 0.1 .,0.1,,,,,,,,,
results,"On the main benchmark WebQuestions , our best results use all data sources , the bigger extract from Freebase and the CANDS AS NEGS setting .","On
main benchmark WebQuestions
best results
use
all data sources","best results||use||all data sources
","main benchmark WebQuestions||has||best results
","Results||On||main benchmark WebQuestions
",,,,,,
results,Transfer learning on Reverb,Transfer learning on Reverb,,,,"Results||has||Transfer learning on Reverb
",,,,,
results,"In this set of experiments , all Reverb facts are added to the memory , without any retraining , and we test our ability to rerank answers on the companion QA set .","test
our ability
to rerank
answers
on
companion QA set","our ability||to rerank||answers
answers||on||companion QA set
",,,,,"Transfer learning on Reverb||test||our ability
",,,
results,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .",,,,,,,,,,
results,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .","notice
models
trained on
single QA dataset
perform
poorly
on
other datasets","models||trained on||single QA dataset
single QA dataset||perform||poorly
poorly||on||other datasets
",,"Results||notice||models
",,,,,,
results,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .","training on
both datasets
improves
performance
is able to capture
all question patterns
of
two datasets","both datasets||improves||performance
both datasets||is able to capture||all question patterns
all question patterns||of||two datasets
",,"Results||training on||both datasets
",,,,,,
research-problem,Sentence Similarity Learning by Lexical Decomposition and Composition,Sentence Similarity Learning,,,,,"Contribution||has research problem||Sentence Similarity Learning
",,,,
research-problem,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",sentence similarity,,,,,"Contribution||has research problem||sentence similarity
",,,,
model,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .","propose
a novel model
decomposing and composing
lexical semantics
over
sentences","a novel model||decomposing and composing||lexical semantics
lexical semantics||over||sentences
",,"Model||propose||a novel model
",,,,,,
model,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .","Given
sentence pair
represents
each word
as
low -dimensional vector
calculates
semantic matching vector
for
each word
based on
all words in the other sentence","sentence pair||represents||each word
each word||as||low -dimensional vector
sentence pair||calculates||semantic matching vector
semantic matching vector||based on||all words in the other sentence
semantic matching vector||for||each word
",,"Model||Given||sentence pair
",,,,,,
model,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .","based on
semantic matching vector
each word vector
decomposed
into
two components
similar component
dissimilar component","semantic matching vector||decomposed||each word vector
each word vector||into||two components
","two components||name||similar component
two components||name||dissimilar component
","Model||based on||semantic matching vector
",,,,,,
model,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .","use
similar components
of
all the words
to represent
similar parts of the sentence pair
dissimilar components
of
every word
to model
dissimilar parts explicitly","similar components||of||all the words
all the words||to represent||similar parts of the sentence pair
dissimilar components||of||every word
every word||to model||dissimilar parts explicitly
",,"Model||use||similar components
Model||use||dissimilar components
",,,,,,
model,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .","two - channel CNN operation
performed
to compose
similar and dissimilar components
into
feature vector","two - channel CNN operation||to compose||similar and dissimilar components
similar and dissimilar components||into||feature vector
",,"Model||performed||two - channel CNN operation
",,,,,,
model,"Finally , the composed feature vector is utilized to predict the sentence similarity .","composed feature vector
utilized
to predict
sentence similarity","composed feature vector||to predict||sentence similarity
",,"Model||utilized||composed feature vector
",,,,,,
experimental-setup,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .","switched
semantic matching functions
among
{ max , global , local - l}
fixed
other options
as
linear decomposition
filter types
including
{unigram , bigram , trigram }
500
filters","semantic matching functions||among||{ max , global , local - l}
other options||as||linear decomposition
other options||as||filter types
filter types||including||{unigram , bigram , trigram }
filter types||filters||500
",,"Experimental setup||switched||semantic matching functions
Experimental setup||fixed||other options
",,,,,,
experimental-setup,We found that the max function worked better than the global function on both MAP and MRR .,"found
max function
worked better than
global function
on both
MAP and MRR","max function||worked better than||global function
global function||on both||MAP and MRR
",,"Experimental setup||found||max function
",,,,,,
experimental-setup,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .","varied
decomposition operation
among
{ rigid , linear , orthogonal }","decomposition operation||among||{ rigid , linear , orthogonal }
",,"Experimental setup||varied||decomposition operation
",,,,,,
experimental-setup,"Third , we tested the influence of various filter types .","tested
influence of various filter types",,,"Experimental setup||tested||influence of various filter types
",,,,,,
experimental-setup,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .","constructed
5 groups of
filters
win - 1
contains
only the unigram filters
win - 2
contains
both unigram and bigram filters
win - 3
contains
all the filters in win - 2 plus trigram filters
win - 4
extends filters in
win - 3 with 4 - gram filters
win - 5
adds
5 - gram filters into win - 4","5 groups of||filters||win - 5
win - 5||adds||5 - gram filters into win - 4
5 groups of||filters||win - 4
win - 4||extends filters in||win - 3 with 4 - gram filters
5 groups of||filters||win - 3
win - 3||contains||all the filters in win - 2 plus trigram filters
5 groups of||filters||win - 2
win - 2||contains||both unigram and bigram filters
5 groups of||filters||win - 1
win - 1||contains||only the unigram filters
",,,,,"influence of various filter types||constructed||5 groups of
",,,
results,QASent dataset .,QASent dataset,,,,"Results||has||QASent dataset
",,,,,
results,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .","got
best MAP
among
all previous work
comparable MRR
than
dos","comparable MRR||than||dos
best MAP||among||all previous work
",,,,,"QASent dataset||got||comparable MRR
QASent dataset||got||best MAP
",,,
results,Wiki QA dataset .,Wiki QA dataset,,,,"Results||has||Wiki QA dataset
",,,,,
results,The last row of shows that our model is more effective than the other models .,"more effective than
other models",,,,,,"Wiki QA dataset||more effective than||other models
",,,
results,MSRP dataset .,MSRP dataset,,,,"Results||has||MSRP dataset
",,,,,
results,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .","obtained
comparable performance
without using
any sparse features
extra annotated resources
specific training strategies","comparable performance||without using||any sparse features
comparable performance||without using||extra annotated resources
comparable performance||without using||specific training strategies
",,,,,"MSRP dataset||obtained||comparable performance
",,,
research-problem,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,Machine Comprehension on Sparse Data,,,,,"Contribution||has research problem||Machine Comprehension on Sparse Data
",,,,
research-problem,Understanding unstructured text is a major goal within natural language processing .,Understanding unstructured text,,,,,"Contribution||has research problem||Understanding unstructured text
",,,,
research-problem,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",machine comprehension on the challenging MCTest benchmark,,,,,"Contribution||has research problem||machine comprehension on the challenging MCTest benchmark
",,,,
research-problem,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .","Comprehension of unstructured text by machines , at a near- human level",,,,,"Contribution||has research problem||Comprehension of unstructured text by machines , at a near- human level
",,,,
research-problem,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,"Machine comprehension
MC",,,,,"Contribution||has research problem||Machine comprehension
Contribution||has research problem||MC
",,,,
model,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,"to
compares
question and answer candidates
text
using
several distinct perspectives","question and answer candidates||to||text
text||using||several distinct perspectives
",,"Model||compares||question and answer candidates
",,,,,,
model,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .","semantic perspective
compares
hypothesis
to
sentences
in
text
viewed as
single , self - contained thoughts
represented using
sum and transformation of word embedding vectors","semantic perspective||compares||hypothesis
hypothesis||to||sentences
sentences||in||text
text||viewed as||single , self - contained thoughts
single , self - contained thoughts||represented using||sum and transformation of word embedding vectors
",,,"Model||has||semantic perspective
",,,,,
model,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .","word - by - word perspective
focuses on
similarity matches
between
individual words from hypothesis and text , at various scales","word - by - word perspective||focuses on||similarity matches
similarity matches||between||individual words from hypothesis and text , at various scales
",,,"Model||has||word - by - word perspective
",,,,,
model,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .","use
sliding window
implicitly considers
linear distance between matched words","sliding window||implicitly considers||linear distance between matched words
",,"Model||use||sliding window
",,,,,,
model,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .","operates on
two different views of text sentences
sequential view
where
words appear in their natural order
dependency view
where
words are reordered
based on
linearization of the sentence 's dependency graph","sequential view||where||words appear in their natural order
dependency view||where||words are reordered
words are reordered||based on||linearization of the sentence 's dependency graph
","two different views of text sentences||has||sequential view
two different views of text sentences||has||dependency view
",,,,"sliding window||operates on||two different views of text sentences
",,,
hyperparameters,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .","For
word vectors
use
Google 's publicly available embeddings
trained with
word2vec
on
100 - billion - word News corpus","word vectors||use||Google 's publicly available embeddings
Google 's publicly available embeddings||trained with||word2vec
word2vec||on||100 - billion - word News corpus
",,"Hyperparameters||For||word vectors
",,,,,,
hyperparameters,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .","found
dropout
effective at improving
generalization from the training to the test set
used
0.5
as
dropout probability","dropout||effective at improving||generalization from the training to the test set
dropout||used||0.5
0.5||as||dropout probability
",,"Hyperparameters||found||dropout
",,,,,,
hyperparameters,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .","used
Adam optimizer
with
standard settings ( Kingma and Ba , 2014 )
learning rate
0.003","Adam optimizer||with||standard settings ( Kingma and Ba , 2014 )
Adam optimizer||learning rate||0.003
",,"Hyperparameters||used||Adam optimizer
",,,,,,
hyperparameters,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,"To determine
best hyperparameters
performed
grid search
over
150 settings
based on
validation - set accuracy","best hyperparameters||performed||grid search
grid search||over||150 settings
150 settings||based on||validation - set accuracy
",,"Hyperparameters||To determine||best hyperparameters
",,,,,,
results,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ? 1 % ) .","On
MCTest - 500
Parallel Hierarchical model
significantly outperforms
on
single questions
( > 2 % )
slightly outperforms
multi questions ( ? 0.3 % )
over all ( ? 1 % )","Parallel Hierarchical model||slightly outperforms||multi questions ( ? 0.3 % )
Parallel Hierarchical model||slightly outperforms||over all ( ? 1 % )
Parallel Hierarchical model||on||single questions
single questions||( > 2 % )||significantly outperforms
","MCTest - 500||has||Parallel Hierarchical model
","Results||On||MCTest - 500
",,,,,,
ablation-analysis,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .","n-gram functionality
important
contributing
almost 5 % accuracy improvement","n-gram functionality||contributing||almost 5 % accuracy improvement
",,"Ablation analysis||important||n-gram functionality
",,,,,,
ablation-analysis,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .","top N function
contributes very little
suggesting
most multi questions have their evidence distributed across contiguous sentences","top N function||suggesting||most multi questions have their evidence distributed across contiguous sentences
",,"Ablation analysis||contributes very little||top N function
",,,,,,
ablation-analysis,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .","sentential component
made
most significant difference
reducing performance
by more than 5 %","sentential component||made||most significant difference
most significant difference||reducing performance||by more than 5 %
",,,"Ablation analysis||has||sentential component
",,,,,
ablation-analysis,Simple word - by - word matching is obviously useful on MCTest .,"Simple word - by - word matching
obviously useful
on
MCTest","MCTest||obviously useful||Simple word - by - word matching
",,"Ablation analysis||on||MCTest
",,,,,,
ablation-analysis,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .","sequential sliding window
makes
3 % contribution
highlighting
importance of word - distance measures","sequential sliding window||makes||3 % contribution
sequential sliding window||highlighting||importance of word - distance measures
",,,"Ablation analysis||has||sequential sliding window
",,,,,
ablation-analysis,"On the other hand , the dependency - based sliding window makes only a minor contribution .","dependency - based sliding window
only a minor contribution",,,"Ablation analysis||only a minor contribution||dependency - based sliding window
",,,,,,
ablation-analysis,"Finally , the exogenous word weights make a significant contribution of almost 5 % .","exogenous word weights
significant
contribution of
almost 5 %","exogenous word weights||contribution of||almost 5 %
",,"Ablation analysis||significant||exogenous word weights
",,,,,,
research-problem,Iterative Alternating Neural Attention for Machine Reading,Machine Reading,,,,,"Contribution||has research problem||Machine Reading
",,,,
research-problem,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .","machine comprehension
answering Cloze - style queries with respect to a document",,,,,"Contribution||has research problem||machine comprehension
Contribution||has research problem||answering Cloze - style queries with respect to a document
",,,,
model,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .","propose
novel neural attention - based inference model
designed to perform
machine reading comprehension tasks","novel neural attention - based inference model||designed to perform||machine reading comprehension tasks
",,"Model||propose||novel neural attention - based inference model
",,,,,,
model,The model first reads the document and the query using a recurrent neural network .,"reads
document and the query
using
recurrent neural network","document and the query||using||recurrent neural network
",,"Model||reads||document and the query
",,,,,,
model,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .","deploys
iterative inference process
to uncover
inferential links
between
the missing query word , the query , and the document","iterative inference process||to uncover||inferential links
inferential links||between||the missing query word , the query , and the document
",,"Model||deploys||iterative inference process
",,,,,,
model,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .","involves
novel alternating attention mechanism
attends to
some parts of the query
finds
corresponding matches
by attending to
document","novel alternating attention mechanism||finds||corresponding matches
corresponding matches||by attending to||document
novel alternating attention mechanism||attends to||some parts of the query
",,"Model||involves||novel alternating attention mechanism
",,,,,,
model,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,"result
of
alternating search
fed back into
iterative inference process
to seed
next search step","result||of||alternating search
result||fed back into||iterative inference process
iterative inference process||to seed||next search step
",,,"Model||has||result
",,,,,
model,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .","After
fixed number of iterations
uses
summary of its inference process
to predict
answer","fixed number of iterations||uses||summary of its inference process
summary of its inference process||to predict||answer
",,"Model||After||fixed number of iterations
",,,,,,
experimental-setup,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","To train
our model
used
stochastic gradient descent
with
ADAM optimizer
initial learning rate
of
0.001","our model||used||stochastic gradient descent
stochastic gradient descent||with||ADAM optimizer
stochastic gradient descent||with||initial learning rate
initial learning rate||of||0.001
",,"Experimental setup||To train||our model
",,,,,,
experimental-setup,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .","set
batch size
to
32
decay
learning rate
by
0.8","batch size||to||32
learning rate||by||0.8
",,"Experimental setup||set||batch size
Experimental setup||decay||learning rate
",,,,,,
experimental-setup,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .","initialize
weights of our model
by sampling from
normal distribution N ( 0 , 0.05 )","weights of our model||by sampling from||normal distribution N ( 0 , 0.05 )
",,"Experimental setup||initialize||weights of our model
",,,,,,
experimental-setup,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .","GRU recurrent weights
are initialized
to be
orthogonal
biases
to
zero","biases||to||zero
GRU recurrent weights||to be||orthogonal
",,"Experimental setup||are initialized||biases
Experimental setup||are initialized||GRU recurrent weights
",,,,,,
experimental-setup,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .","to stabilize
learning
clip
gradients
if
norm
greater than
5","learning||clip||gradients
gradients||if||norm
norm||greater than||5
",,"Experimental setup||to stabilize||learning
",,,,,,
experimental-setup,"Our model is implemented in Theano , using the Keras library .","implemented in
Theano
using
Keras library","Theano||using||Keras library
",,"Experimental setup||implemented in||Theano
",,,,,,
results,CBT,CBT,,,,"Results||on||CBT
",,,,,
results,Our model ( line 7 ) sets a new stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .,"sets
a new stateof - the - art
on
common noun category
by gaining
3.6 and 5.6 points
in
validation and test
over
best baseline AS Reader","a new stateof - the - art||by gaining||3.6 and 5.6 points
3.6 and 5.6 points||in||validation and test
validation and test||over||best baseline AS Reader
a new stateof - the - art||on||common noun category
",,,,,"CBT||sets||a new stateof - the - art
",,,
results,CNN,CNN,,,,"Results||on||CNN
",,,,,
results,The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .,"improves
state - of - the - art accuracy
by
4 percent absolute on validation and 3.4 on test
with respect to
most recent published result ( AS Reader )","state - of - the - art accuracy||by||4 percent absolute on validation and 3.4 on test
4 percent absolute on validation and 3.4 on test||with respect to||most recent published result ( AS Reader )
",,,,,"CNN||improves||state - of - the - art accuracy
",,,
results,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .","ensembles
achieve
state - of - the - art test performance
of
75.2 and 76.1 on validation and test respectively
outperforming
previously published results","ensembles||of||75.2 and 76.1 on validation and test respectively
ensembles||outperforming||previously published results
","state - of - the - art test performance||has||ensembles
",,,,"CNN||achieve||state - of - the - art test performance
",,,
research-problem,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,QUESTION ANSWERING,,,,,"Contribution||has research problem||QUESTION ANSWERING
",,,,
research-problem,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",question answering when reasoning over multiple facts is required,,,,,"Contribution||has research problem||question answering when reasoning over multiple facts is required
",,,,
model,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .","Query - Reduction Network
is a
single recurrent unit
addresses
long - term dependency problem of most RNN - based models
by simplifying
recurrent update
taking the advantage of
RNN 's capability
to model
sequential data","Query - Reduction Network||is a||single recurrent unit
single recurrent unit||addresses||long - term dependency problem of most RNN - based models
long - term dependency problem of most RNN - based models||by simplifying||recurrent update
long - term dependency problem of most RNN - based models||taking the advantage of||RNN 's capability
RNN 's capability||to model||sequential data
",,,"Model||name||Query - Reduction Network
",,,,,
model,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .","QRN
considers
context sentences
as a sequence of
state - changing triggers
transforms ( reduces )
original query
to
more informed query
as it observes
trigger
through
time","QRN||considers||context sentences
context sentences||as a sequence of||state - changing triggers
QRN||transforms ( reduces )||original query
original query||to||more informed query
more informed query||as it observes||trigger
trigger||through||time
",,,,,,,"Query - Reduction Network||name||QRN
",
hyperparameters,We withhold 10 % of the training for development .,"withhold
10 % of the training
for
development","10 % of the training||for||development
",,"Hyperparameters||withhold||10 % of the training
",,,,,,
hyperparameters,We use the hidden state size of 50 by deafult .,"use
hidden state size
of
50
by
deafult","hidden state size||of||50
50||by||deafult
",,"Hyperparameters||use||hidden state size
",,,,,,
hyperparameters,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .","Batch sizes
of
32
for
bAbI story - based QA 1k
bAb I dialog
DSTC2 dialog
128
for
bAbI QA 10 k
used","Batch sizes||of||128
128||for||bAbI QA 10 k
Batch sizes||of||32
32||for||bAbI story - based QA 1k
32||for||bAb I dialog
32||for||DSTC2 dialog
",,"Hyperparameters||used||Batch sizes
",,,,,,
hyperparameters,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,"L2 weight decay
of
0.001 ( 0.0005 for QA 10 k )","L2 weight decay||of||0.001 ( 0.0005 for QA 10 k )
",,,"Hyperparameters||used||L2 weight decay
",,,,,
hyperparameters,The loss function is the cross entropy betweenv and the one - hot vector of the true answer .,"loss function
is
cross entropy betweenv and the one - hot vector of the true answer","loss function||is||cross entropy betweenv and the one - hot vector of the true answer
",,,"Hyperparameters||has||loss function
",,,,,
hyperparameters,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .","loss
is minimized by
stochastic gradient descent
for
maximally 500 epochs
training
early stopped
if the loss on the development data
does not decrease
for
50 epochs","loss||is minimized by||stochastic gradient descent
stochastic gradient descent||for||maximally 500 epochs
maximally 500 epochs||early stopped||training
training||if the loss on the development data||does not decrease
does not decrease||for||50 epochs
",,,"Hyperparameters||has||loss
",,,,,
hyperparameters,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,"learning rate
controlled
by
AdaGrad
initial learning rate
0.5 ( 0.1 for QA 10 k )","learning rate||by||AdaGrad
AdaGrad||initial learning rate||0.5 ( 0.1 for QA 10 k )
",,"Hyperparameters||controlled||learning rate
",,,,,,
results,Story - based QA .,Story - based QA,,,,"Results||has||Story - based QA
",,,,,
results,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .","In
1 k data
QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )
outperforms
all other models
by
large margin ( 2.8 + % )","QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )||outperforms||all other models
all other models||by||large margin ( 2.8 + % )
","1 k data||has||QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )
",,,,"Story - based QA||In||1 k data
",,,
results,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .","10 k dataset
average accuracy of
QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model
outperforms
all previous models
by
large margin ( 2.5 + % )","10 k dataset||average accuracy of||QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model
QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model||outperforms||all previous models
all previous models||by||large margin ( 2.5 + % )
",,,,,,,"Story - based QA||In||10 k dataset
",
results,Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in in Appendix ) .,"Dialog
previous work",,"Dialog||outperforms||previous work
",,"Results||has||Dialog
",,,,,
results,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,"outperforms
by
large margin ( 2.0 + % )",,,,,,"previous work||by||large margin ( 2.0 + % )
",,,
ablation-analysis,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .","When
number of layers
is
one
model
lacks
reasoning capability","model||lacks||reasoning capability
reasoning capability||When||number of layers
number of layers||is||one
",,,"Ablation analysis||has||model
",,,,,
ablation-analysis,( b ) Adding the reset gate helps .,"Adding the reset gate
helps",,,"Ablation analysis||helps||Adding the reset gate
",,,,,,
ablation-analysis,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .","Including vector gates
hurts
in
1 k datasets","Including vector gates||in||1 k datasets
",,"Ablation analysis||hurts||Including vector gates
",,,,,,
ablation-analysis,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .","vector gates
in
bAbI story - based QA 10 k dataset
sometimes help","vector gates||in||bAbI story - based QA 10 k dataset
",,"Ablation analysis||sometimes help||vector gates
",,,,,,
research-problem,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,natural language understanding,,,,,"Contribution||has research problem||natural language understanding
",,,,
model,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,"propose
a novel class of memory augmented neural networks
called
Neural Semantic Encoders ( NSE )
for
natural language understanding","a novel class of memory augmented neural networks||called||Neural Semantic Encoders ( NSE )
Neural Semantic Encoders ( NSE )||for||natural language understanding
",,"Model||propose||a novel class of memory augmented neural networks
",,,,,,
model,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,"NSE
variable sized encoding memory
to access
entire input sequence
during
reading process
efficiently delivering
long - term dependencies overtime","variable sized encoding memory||efficiently delivering||long - term dependencies overtime
variable sized encoding memory||to access||entire input sequence
entire input sequence||during||reading process
","NSE||has||variable sized encoding memory
",,"Model||name||NSE
",,,,,
model,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .","encoding memory
evolves
overtime
maintains
memory
of
input sequence
through
read , compose and write operations","encoding memory||maintains||memory
memory||of||input sequence
input sequence||through||read , compose and write operations
encoding memory||evolves||overtime
",,,"Model||has||encoding memory
",,,,,
model,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,"sequentially processes
the input
supports
word compositionality",,,,,,"NSE||supports||word compositionality
NSE||sequentially processes||the input
",,,
model,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,"read from and write to
set of relevant encoding memories simultaneously
access
shared encoding memory
supporting
knowledge and representation sharing","shared encoding memory||supporting||knowledge and representation sharing
",,,,,"NSE||access||shared encoding memory
NSE||read from and write to||set of relevant encoding memories simultaneously
",,,
research-problem,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",NLU,,,,,"Contribution||has research problem||NLU
",,,,
experimental-setup,The models are trained using Adam with hyperparameters selected on development set .,"trained using
Adam",,,"Experimental setup||trained using||Adam
",,,,,,
experimental-setup,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,"chose
two one - layer LSTM
for
read / write modules
on
tasks other than QA","two one - layer LSTM||for||read / write modules
read / write modules||on||tasks other than QA
",,"Experimental setup||chose||two one - layer LSTM
",,,,,,
experimental-setup,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,"pre-trained 300 - D Glove 840B vectors
100 - D Glove 6B vectors
obtained for
word embeddings",,"word embeddings||has||pre-trained 300 - D Glove 840B vectors
word embeddings||has||100 - D Glove 6B vectors
","Experimental setup||obtained for||word embeddings
",,,,,,
experimental-setup,We crop or pad the input sequence to a fixed length .,"crop or pad
input sequence
to
fixed length","input sequence||to||fixed length
",,"Experimental setup||crop or pad||input sequence
",,,,,,
experimental-setup,The models were regularized by using dropouts and an l 2 weight decay .,"regularized by
dropouts
l 2 weight decay",,,"Experimental setup||regularized by||dropouts
Experimental setup||regularized by||l 2 weight decay
",,,,,,
tasks,Natural Language Inference,Natural Language Inference,,,,,,,,,"Tasks||Natural Language Inference||Hyperparameters
"
tasks,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .","hidden layer
with
1024 units
ReLU activation
sof tmax layer","1024 units||with||ReLU activation
1024 units||with||sof tmax layer
",,,,,"Hyperparameters||hidden layer||1024 units
",,,
tasks,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .","batch size
128
initial learning rate
3e - 4
l 2 regularizer strength
3 e - 5
train
40 epochs",,,,,,"Hyperparameters||initial learning rate||3e - 4
Hyperparameters||l 2 regularizer strength||3 e - 5
Hyperparameters||batch size||128
Hyperparameters||train||40 epochs
Hyperparameters||initial learning rate||3e - 4
Hyperparameters||l 2 regularizer strength||3 e - 5
Hyperparameters||batch size||128
Hyperparameters||train||40 epochs
Hyperparameters||initial learning rate||3e - 4
Hyperparameters||l 2 regularizer strength||3 e - 5
Hyperparameters||initial learning rate||3e - 4
",,,
tasks,Our MMA - NSE attention model is similar to the LSTM attention model .,MMA - NSE attention model,,,,,,,,"Results||has||MMA - NSE attention model
Results||has||MMA - NSE attention model
",
tasks,This model obtained 85.4 % accuracy score .,"obtained
85.4 % accuracy score",,,,,,"MMA - NSE attention model||obtained||85.4 % accuracy score
",,,
tasks,Answer Sentence Selection,Answer Sentence Selection,,,,,,,,,"Tasks||Answer Sentence Selection||Hyperparameters
"
tasks,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .","batch size
4
initial learning rate
1 e - 5
train the model for
10 epochs",,,,,,"Hyperparameters||initial learning rate||1 e - 5
Hyperparameters||batch size||4
Hyperparameters||train the model for||10 epochs
",,,
tasks,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,"used
40 % dropouts
afterword
embeddings
no
l 2 weight decay","40 % dropouts||no||l 2 weight decay
40 % dropouts||afterword||embeddings
",,,,,"Hyperparameters||used||40 % dropouts
",,,
tasks,The word embeddings are pre-trained 300 - D Glove 840B vectors .,"word embeddings
are
pre-trained 300 - D Glove 840B vectors","word embeddings||are||pre-trained 300 - D Glove 840B vectors
",,,,,,,"Hyperparameters||has||word embeddings
","word embeddings||has||linear mapping layer
"
tasks,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .","linear mapping layer
transforms
300 - D word embeddings
to
512- D LSTM inputs","linear mapping layer||transforms||300 - D word embeddings
300 - D word embeddings||to||512- D LSTM inputs
",,,,,,,,
tasks,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,"MMA - NSE attention model
exceeds
NASM
by approximately
1 %
on
MAP
0.8 %
on
MRR","MMA - NSE attention model||exceeds||NASM
NASM||by approximately||1 %
1 %||on||MAP
NASM||by approximately||0.8 %
0.8 %||on||MRR
",,,,,,,,
tasks,Sentence Classification,Sentence Classification,,,,,,,,,"Tasks||Sentence Classification||Hyperparameters
"
tasks,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,"first layer of the MLP
ReLU activation
1024 or 300 units
for
binary or fine - grained setting","1024 or 300 units||for||binary or fine - grained setting
",,,,,"Hyperparameters||first layer of the MLP||ReLU activation
Hyperparameters||first layer of the MLP||1024 or 300 units
",,,
tasks,The second layer is a sof tmax layer .,"second layer
sof tmax layer",,,,,,"Hyperparameters||second layer||sof tmax layer
",,,
tasks,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,"read / write modules
two one - layer LSTM
with
300 hidden units
word embeddings
pre-trained 300 - D Glove 840B vectors","two one - layer LSTM||with||300 hidden units
",,,,,"Hyperparameters||read / write modules||two one - layer LSTM
Hyperparameters||word embeddings||pre-trained 300 - D Glove 840B vectors
",,,
tasks,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .","batch size
64
initial learning rate
3e - 4
l 2 regularizer strength
3 e - 5
train
25 epochs",,,,,,"Hyperparameters||batch size||64
Hyperparameters||train||25 epochs
",,,
tasks,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,"Our model
outperformed
DMN
set
state - of - the - art results
on
both subtasks","Our model||outperformed||DMN
Our model||set||state - of - the - art results
state - of - the - art results||on||both subtasks
",,,,,,,"Results||has||Our model
",
tasks,Document Sentiment Analysis,Document Sentiment Analysis,,,,,,,,,"Tasks||Document Sentiment Analysis||Hyperparameters
"
tasks,We stack a NSE or LSTM on the top of another NSE for document modeling .,"stack
NSE or LSTM
on the top of
another NSE
for
document modeling","NSE or LSTM||on the top of||another NSE
another NSE||for||document modeling
",,,,,"Hyperparameters||stack||NSE or LSTM
",,,
tasks,The whole network is trained jointly by backpropagating the cross entropy loss .,"The whole network
trained jointly by backpropagating
cross entropy loss","The whole network||trained jointly by backpropagating||cross entropy loss
",,,,,,,"Hyperparameters||has||The whole network
",
tasks,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,"used
one - layer LSTM
with
100 hidden units
for
read / write modules
pre-trained 100 - D Glove 6B vectors","one - layer LSTM||with||100 hidden units
100 hidden units||for||read / write modules
",,,,,"Hyperparameters||used||one - layer LSTM
Hyperparameters||used||pre-trained 100 - D Glove 6B vectors
",,,
tasks,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .","batch size
32
initial learning rate
3e - 4
l 2 regularizer strength
1 e - 5
trained
50 epochs",,,,,,"Hyperparameters||trained||50 epochs
Hyperparameters||l 2 regularizer strength||1 e - 5
Hyperparameters||batch size||32
",,,
tasks,Machine Translation,Machine Translation,,,,,,,,,"Tasks||Machine Translation||Hyperparameters
"
tasks,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,"trained to minimize
word - level cross entropy loss
regularized by
20 % input dropouts
30 % output dropouts",,,,,,"Hyperparameters||regularized by||20 % input dropouts
Hyperparameters||regularized by||30 % output dropouts
Hyperparameters||trained to minimize||word - level cross entropy loss
",,,
tasks,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .","batch size
128
initial learning rate
1e - 3
for
LSTM - LSTM
3e - 4
for
other models
l 2 regularizer strength
3 e - 5
train
40 epochs","3e - 4||for||other models
1e - 3||for||LSTM - LSTM
",,,,,"Hyperparameters||initial learning rate||1e - 3
",,,
research-problem,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,MACHINE COMPREHENSION,,,,,"Contribution||has research problem||MACHINE COMPREHENSION
",,,,
research-problem,Machine comprehension of text is an important problem in natural language processing .,Machine comprehension of text,,,,,"Contribution||has research problem||Machine comprehension of text
",,,,
model,"In this paper , we propose a new end - to - end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset .","propose
end - to - end neural architecture
to address
machine comprehension problem
as defined in
SQuAD dataset","end - to - end neural architecture||to address||machine comprehension problem
machine comprehension problem||as defined in||SQuAD dataset
",,"Model||propose||end - to - end neural architecture
",,,,,,
model,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .","from
original text
adopt
match - LSTM model",,,"Model||adopt||match - LSTM model
",,,"multiple tokens||from||original text
",,,"Model||adopt||Pointer Net ( Ptr - Net ) model
"
model,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .","Pointer Net ( Ptr - Net ) model
enables
predictions of tokens
from
input sequence
generate
answers
consist of
multiple tokens","Pointer Net ( Ptr - Net ) model||generate||answers
answers||consist of||multiple tokens
Pointer Net ( Ptr - Net ) model||enables||predictions of tokens
predictions of tokens||from||input sequence
",,,,,,,,
model,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,"two ways to apply
Ptr - Net model
sequence model
boundary model",,"Ptr - Net model||has||sequence model
Ptr - Net model||has||boundary model
","Model||two ways to apply||Ptr - Net model
",,,,,,
model,We also further extend the boundary model with a search mechanism .,"with
search mechanism",,,,,,"boundary model||with||search mechanism
",,,
hyperparameters,We use word embeddings from GloVe to initialize the model .,"use
word embeddings
GloVe
to initialize the model","word embeddings||GloVe||to initialize the model
",,"Hyperparameters||use||word embeddings
",,,,,,
hyperparameters,The dimensionality l of the hidden layers is set to be 150 or 300 .,"dimensionality l
hidden layers
set
150 or 300","hidden layers||set||150 or 300
",,"Hyperparameters||dimensionality l||hidden layers
",,,,,,
hyperparameters,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,"ADAMAX
with
coefficients ? 1 = 0.9 and ? 2 = 0.999
to optimize
model","ADAMAX||with||coefficients ? 1 = 0.9 and ? 2 = 0.999
coefficients ? 1 = 0.9 and ? 2 = 0.999||to optimize||model
",,,"Hyperparameters||use||ADAMAX
",,,,,
hyperparameters,Each update is computed through a minibatch of 30 instances .,"update
computed through
minibatch
of
30 instances","update||computed through||minibatch
minibatch||of||30 instances
",,,"Hyperparameters||has||update
",,,,,
results,"outperformed the logistic regression model by , which relies on carefully designed features .","outperformed
logistic regression model
relies on
carefully designed features","logistic regression model||relies on||carefully designed features
",,"Results||outperformed||logistic regression model
",,,,,,
results,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .","boundary model
sequence model
achieving
exact match score of 61.1 %
an F1 score of 71.2 %","boundary model||achieving||exact match score of 61.1 %
boundary model||achieving||an F1 score of 71.2 %
","sequence model||by||boundary model
",,"Results||outperformed||sequence model
",,,,,
results,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .","by
adding
Bi - Ans - Ptr with bi-directional pre-processing LSTM
get
1.2 % improvement in F1","Bi - Ans - Ptr with bi-directional pre-processing LSTM||get||1.2 % improvement in F1
",,"Results||adding||Bi - Ans - Ptr with bi-directional pre-processing LSTM
",,,,,,
research-problem,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,EXTRACTIVE QUESTION ANSWERING,,,,,"Contribution||has research problem||EXTRACTIVE QUESTION ANSWERING
",,,,
research-problem,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .",reading comprehension,,,,,"Contribution||has research problem||reading comprehension
",,,,
research-problem,A primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,answer questions about the contents of documents,,,,,"Contribution||has research problem||answer questions about the contents of documents
",,,,
model,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .","called
RASOR
builds
fixed - length span representations
reusing
recurrent computations
for
shared substructures","RASOR||builds||fixed - length span representations
fixed - length span representations||reusing||recurrent computations
recurrent computations||for||shared substructures
",,"Model||called||RASOR
",,,,,,
model,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .","demonstrate
directly classifying
each of the competing spans
training with
global normalization
over all
possible spans
significant increase in performance","significant increase in performance||directly classifying||each of the competing spans
significant increase in performance||training with||global normalization
global normalization||over all||possible spans
",,"Model||demonstrate||significant increase in performance
",,,,,,
experimental-setup,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,"represent
each of the words
in
question and document
using
300 dimensional GloVe embeddings
on
corpus of 840 bn words","each of the words||in||question and document
question and document||using||300 dimensional GloVe embeddings
300 dimensional GloVe embeddings||on||corpus of 840 bn words
",,"Experimental setup||represent||each of the words
",,,,,,
experimental-setup,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .","couple
input and forget gates
in
LSTMs
use
single dropout mask
apply dropout
across all LSTM time - steps","input and forget gates||in||LSTMs
single dropout mask||apply dropout||across all LSTM time - steps
",,"Experimental setup||couple||input and forget gates
Experimental setup||use||single dropout mask
",,,,,,
experimental-setup,Hidden layers in the feed forward neural networks use rectified linear units .,"Hidden layers
feed forward neural networks
use
rectified linear units","feed forward neural networks||use||rectified linear units
",,"Experimental setup||Hidden layers||feed forward neural networks
",,,,,,
experimental-setup,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .","ran
grid searches
dimensionality
LSTM hidden states
width and depth
feed forward neural networks
dropout
LSTMs
decay multiplier
[ 0.9 , 0.95 , 1.0 ]
multiply
learning rate
every
10 k steps","grid searches||width and depth||feed forward neural networks
grid searches||dimensionality||LSTM hidden states
grid searches||dropout||LSTMs
grid searches||decay multiplier||[ 0.9 , 0.95 , 1.0 ]
[ 0.9 , 0.95 , 1.0 ]||multiply||learning rate
learning rate||every||10 k steps
",,"Experimental setup||ran||grid searches
",,,,,,
experimental-setup,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,"best model
uses
50d LSTM states
two - layer BiLSTMs
for
span encoder and the passage - independent question representation
dropout
of
0.1
learning rate decay
of
5 %
every
10 k steps","best model||uses||50d LSTM states
best model||uses||two - layer BiLSTMs
two - layer BiLSTMs||for||span encoder and the passage - independent question representation
best model||uses||dropout
dropout||of||0.1
best model||uses||learning rate decay
learning rate decay||of||5 %
5 %||every||10 k steps
",,,"Experimental setup||has||best model
",,,,,
experimental-setup,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,"implemented using
TensorFlow 3
trained on
SQUAD training set
using
ADAM optimizer
with
mini-batch size
of
4
trained using
10 asynchronous training threads
on
single machine","SQUAD training set||using||ADAM optimizer
ADAM optimizer||with||mini-batch size
mini-batch size||of||4
10 asynchronous training threads||on||single machine
",,"Experimental setup||implemented using||TensorFlow 3
Experimental setup||trained on||SQUAD training set
Experimental setup||trained using||10 asynchronous training threads
",,,,,,
results,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .","RASOR
achieves
error reduction
of more than
50 % over this baseline","RASOR||achieves||error reduction
error reduction||of more than||50 % over this baseline
",,,"Results||has||RASOR
",,,,,
results,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .","efficiently and explicitly model
quadratic number of possible answers
leads to
14 % error reduction
over
best performing Match - LSTM model","quadratic number of possible answers||leads to||14 % error reduction
14 % error reduction||over||best performing Match - LSTM model
",,,,,"RASOR||efficiently and explicitly model||quadratic number of possible answers
",,,
ablation-analysis,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .","passage - aligned question representation
is
crucial","passage - aligned question representation||is||crucial
",,,"Ablation analysis||has||passage - aligned question representation
",,,,,
ablation-analysis,"First , we observe general improvements when using labels that closely align with the task .","observe
general improvements
when using
labels that closely align with the task","general improvements||when using||labels that closely align with the task
",,"Ablation analysis||observe||general improvements
",,,,,,
ablation-analysis,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .","observe the importance of
allowing interactions
between
the endpoints
using
spanlevel FFNN","allowing interactions||between||the endpoints
the endpoints||using||spanlevel FFNN
",,"Ablation analysis||observe the importance of||allowing interactions
",,,,,,
ablation-analysis,"RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .","RASOR
outperforms
endpoint prediction model
by
1.1 in exact match","RASOR||outperforms||endpoint prediction model
endpoint prediction model||by||1.1 in exact match
",,,"Ablation analysis||has||RASOR
",,,,,
research-problem,Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering,Knowledge Base Question Answering,,,,,"Contribution||has research problem||Knowledge Base Question Answering
",,,,
research-problem,Knowledge base question answering ( QA ) is an important natural language processing problem .,Knowledge base question answering ( QA ),,,,,"Contribution||has research problem||Knowledge base question answering ( QA )
",,,,
research-problem,QA requires precise modeling of the question semantics through the entities and relations available in the KB in order to retrieve the correct answer .,QA,,,,,"Contribution||has research problem||QA
",,,,
research-problem,"In this paper , we describe a semantic parsing approach to the problem of KB QA .","semantic parsing
to
problem
of
KB QA","semantic parsing||to||problem
problem||of||KB QA
",,,"Approach||has||semantic parsing
","Contribution||has research problem||KB QA
",,,,
approach,"That is , for each input question , we construct an explicit structural semantic parse ( semantic graph ) , as in .","for
each input question
construct
explicit structural semantic parse ( semantic graph )","each input question||construct||explicit structural semantic parse ( semantic graph )
",,"Approach||for||each input question
",,,,,,
approach,Semantic parses can be deterministically converted to a query to extract the answers from the KB .,"Semantic parses
deterministically converted to
query
to extract
answers
from
KB","Semantic parses||deterministically converted to||query
query||to extract||answers
answers||from||KB
",,,"Approach||has||Semantic parses
",,,,,
approach,"In particular , we adapt Gated Graph Neural Networks ( GGNNs ) , described in , to process and score semantic parses .","adapt
Gated Graph Neural Networks ( GGNNs )
to process and score
semantic parses","Gated Graph Neural Networks ( GGNNs )||to process and score||semantic parses
",,"Approach||adapt||Gated Graph Neural Networks ( GGNNs )
",,,,,,
code,https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering.,,,,,,,,,,
baselines,3 . Pooled Edges model - We use the DCNN to encode the question and the label of each edge in the semantic graph .,"Pooled Edges model
use
DCNN
to encode
question and the label of each edge
in
semantic graph","Pooled Edges model||use||DCNN
DCNN||to encode||question and the label of each edge
question and the label of each edge||in||semantic graph
",,,"Baselines||has||Pooled Edges model
",,,,,
baselines,Graph Neural Network ( GNN ) -,Graph Neural Network ( GNN ),,,,"Baselines||has||Graph Neural Network ( GNN )
",,,,,
baselines,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the hidden state as a combination of the activations ( Eq 1 ) and the previous state .","of
include
model variant
that does not use
gating mechanism
directly computes
hidden state
as
combination
activations ( Eq 1 ) and the previous state","model variant||directly computes||hidden state
hidden state||as||combination
combination||of||activations ( Eq 1 ) and the previous state
model variant||that does not use||gating mechanism
",,,,,"Graph Neural Network ( GNN )||include||model variant
",,,
baselines,"Gated Graph Neural Network ( GGNN ) - We use the GGNN to process semantic parses , as described in Section 3.2 .","Gated Graph Neural Network ( GGNN )
to process
semantic parses","Gated Graph Neural Network ( GGNN )||to process||semantic parses
",,,"Baselines||has||Gated Graph Neural Network ( GGNN )
",,,,,
results,We compare the results on the WebQSP - WD data set in .,"on
WebQSP - WD data set",,,"Results||on||WebQSP - WD data set
",,,,,,"WebQSP - WD data set||has||graph models
"
results,"As can be seen , the graph models outperform all other models across precision , recall and F-score , with GGNN showing the best over all result .","graph models
outperform
all other models
across
precision , recall and F-score
GGNN
showing
best over all result","graph models||outperform||all other models
all other models||across||precision , recall and F-score
GGNN||showing||best over all result
","graph models||has||GGNN
",,,,,,,
results,"The STAGG architecture delivers the worst results in our experiments , the main reason being supposedly that the model had to rely on manually defined features that are less flexible .","STAGG architecture
delivers
worst results","STAGG architecture||delivers||worst results
",,,"Results||has||STAGG architecture
",,,,,
results,The Single Edge model outperforms the more complex Pooled Edges model by a noticeable margin .,"Single Edge model
outperforms
more complex Pooled Edges model
by
noticeable margin","Single Edge model||outperforms||more complex Pooled Edges model
more complex Pooled Edges model||by||noticeable margin
",,,"Results||has||Single Edge model
",,,,,
results,The Single Edge baseline prefers simple graphs that consist of a single edge which is a good strategy to achieve higher recall values .,"Single Edge baseline
prefers
simple graphs
that consist of
single edge
is
good strategy
to achieve
higher recall values","Single Edge baseline||prefers||simple graphs
simple graphs||is||good strategy
good strategy||to achieve||higher recall values
simple graphs||that consist of||single edge
",,,"Results||has||Single Edge baseline
",,,,,
results,"In , we see that for the STAGG and Single Edge baselines the performance on more complex questions drops compared to the results on simpler questions .","for
STAGG and Single Edge baselines
performance
on
more complex questions
drops
compared to
results
on
simpler questions","performance||compared to||results
results||on||simpler questions
performance||on||more complex questions
","STAGG and Single Edge baselines||has||performance
performance||has||drops
","Results||for||STAGG and Single Edge baselines
",,,,,,
results,"The Pooled Edges model maintains a better performance across questions of different complexity , which shows the benefits of encoding all graph edges .","Pooled Edges model
maintains
better performance
across
questions of different complexity
shows
benefits
of encoding
all graph edges","Pooled Edges model||maintains||better performance
better performance||across||questions of different complexity
better performance||shows||benefits
benefits||of encoding||all graph edges
",,,"Results||has||Pooled Edges model
",,,,,
results,"We see that the GGNN model offers the best results both on simple and complex questions , as it effectively encodes the structure of semantic graphs .","see that
GGNN model
offers
best results
on
simple and complex questions","GGNN model||offers||best results
best results||on||simple and complex questions
",,"Results||see that||GGNN model
",,,,,,
research-problem,BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,MACHINE COMPREHENSION,,,,,"Contribution||has research problem||MACHINE COMPREHENSION
",,,,
research-problem,"Machine comprehension ( MC ) , answering a query about a given context paragraph , requires modeling complex interactions between the context and the query .",Machine comprehension ( MC ),,,,,"Contribution||has research problem||Machine comprehension ( MC )
",,,,
research-problem,"Recently , attention mechanisms have been successfully extended to MC .",MC,,,,,"Contribution||has research problem||MC
",,,,
research-problem,The tasks of machine comprehension ( MC ) and question answering ( QA ) have gained significant popularity over the past few years within the natural language processing and computer vision communities .,question answering ( QA ),,,,,"Contribution||has research problem||question answering ( QA )
",,,,
model,"In this paper , we introduce the Bi- Directional Attention Flow ( BIDAF ) network , a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity ) .","introduce
Bi- Directional Attention Flow ( BIDAF ) network
hierarchical multi-stage architecture
for modeling
representations
of
context paragraph
at
different levels of granularity","hierarchical multi-stage architecture||for modeling||representations
representations||at||different levels of granularity
representations||of||context paragraph
","Bi- Directional Attention Flow ( BIDAF ) network||has||hierarchical multi-stage architecture
","Model||introduce||Bi- Directional Attention Flow ( BIDAF ) network
",,,,,,
model,"BIDAF includes character - level , word - level , and contextual embeddings , and uses bi-directional attention flow to obtain a query - aware context representation .","BIDAF
includes
character - level , word - level , and contextual embeddings
uses
bi-directional attention flow
to obtain
query - aware context representation","BIDAF||includes||character - level , word - level , and contextual embeddings
BIDAF||uses||bi-directional attention flow
bi-directional attention flow||to obtain||query - aware context representation
",,,"Model||has||BIDAF
",,,,,
model,"Instead , the attention is computed for every time step , and the attended vector at each time step , along with the representations from previous layers , is allowed to flow through to the subsequent modeling layer .","attention
computed for
every time step
attended vector
at
each time step
allowed to
flow through
to
subsequent modeling layer","attended vector||allowed to||flow through
flow through||to||subsequent modeling layer
attended vector||at||each time step
attention||computed for||every time step
","attention||has||attended vector
",,"Model||has||attention
",,,,,
model,"Second , we use a memory - less attention mechanism .","use
memory - less attention mechanism",,,"Model||use||memory - less attention mechanism
",,,,,,"memory - less attention mechanism||allows||attention
"
model,"It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .","forces
attention layer
to focus on
learning
attention
between
query and the context
enables
modeling layer
to focus on
learning
interaction
within
query - aware context representation
attention","attention layer||to focus on||learning
attention||between||query and the context
modeling layer||to focus on||learning
interaction||within||query - aware context representation
","learning||has||attention
learning||has||interaction
",,,,"memory - less attention mechanism||forces||attention layer
memory - less attention mechanism||enables||modeling layer
",,,
model,It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps .,"allows
at
each time step
to be
unaffected
from
incorrect attendances
at
previous time steps","each time step||to be||unaffected
unaffected||from||incorrect attendances
incorrect attendances||at||previous time steps
",,,,,"attention||at||each time step
",,,
model,"Third , we use attention mechanisms in both directions , query - to - context and context - to - query , which provide complimentary information to each other .","attention mechanisms
in
both directions
query - to - context
context - to - query
which provide
complimentary information
to
each other","attention mechanisms||in||both directions
both directions||which provide||complimentary information
complimentary information||to||each other
","both directions||name||query - to - context
both directions||name||context - to - query
",,"Model||use||attention mechanisms
",,,,,
experimental-setup,Each paragraph and question are tokenized by a regular - expression - based word tokenizer ( PTB Tokenizer ) and fed into the model .,"Each paragraph and question
tokenized by
regular - expression - based word tokenizer ( PTB Tokenizer )
fed into
model","Each paragraph and question||tokenized by||regular - expression - based word tokenizer ( PTB Tokenizer )
Each paragraph and question||fed into||model
",,,"Experimental setup||has||Each paragraph and question
",,,,,
experimental-setup,"We use 100 1D filters for CNN char embedding , each with a width of 5 .","use
100 1D filters
for
CNN char embedding
with
width of 5","100 1D filters||with||width of 5
100 1D filters||for||CNN char embedding
",,"Experimental setup||use||100 1D filters
",,,,,,
experimental-setup,The hidden state size ( d ) of the model is 100 .,"hidden state size ( d )
of
model
is
100","hidden state size ( d )||of||model
model||is||100
",,,"Experimental setup||has||hidden state size ( d )
",,,,,
experimental-setup,The model has about 2.6 million parameters .,"The model
has about
2.6 million parameters","The model||has about||2.6 million parameters
",,,"Experimental setup||has||The model
",,,,,
experimental-setup,"We use the AdaDelta ( Zeiler , 2012 ) optimizer , with a minibatch size of 60 and an initial learning rate of 0.5 , for 12 epochs .","AdaDelta ( Zeiler , 2012 ) optimizer
with
minibatch size
of
60
initial learning rate
of
0.5
for
12 epochs","AdaDelta ( Zeiler , 2012 ) optimizer||with||minibatch size
minibatch size||of||60
AdaDelta ( Zeiler , 2012 ) optimizer||with||initial learning rate
initial learning rate||of||0.5
initial learning rate||for||12 epochs
",,,"Experimental setup||use||AdaDelta ( Zeiler , 2012 ) optimizer
",,,,,
experimental-setup,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the answers .","dropout ) rate
of
0.2
used for
CNN
all LSTM layers
linear transformation
before
softmax
for
answers","dropout ) rate||of||0.2
0.2||used for||CNN
0.2||used for||all LSTM layers
0.2||used for||linear transformation
linear transformation||before||softmax
softmax||for||answers
",,,"Experimental setup||has||dropout ) rate
",,,,,
experimental-setup,"During training , the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999 .","During
training
moving averages
of
all weights
of
model
maintained with
exponential decay rate
of
0.999","moving averages||of||all weights
all weights||of||model
all weights||maintained with||exponential decay rate
exponential decay rate||of||0.999
","training||has||moving averages
","Experimental setup||During||training
",,,,,,
experimental-setup,"At test time , the moving averages instead of the raw weights are used .","At
test time
moving averages
instead of
raw weights
are used","test time||are used||moving averages
moving averages||instead of||raw weights
",,"Experimental setup||At||test time
",,,,,,
experimental-setup,The training process takes roughly 20 hours on a single Titan X GPU .,"training process
takes
roughly 20 hours
on
single Titan X GPU","training process||takes||roughly 20 hours
roughly 20 hours||on||single Titan X GPU
",,,"Experimental setup||has||training process
",,,,,
results,"BIDAF ( ensemble ) achieves an EM score of 73.3 and an F 1 score of 81.1 , outperforming all previous approaches .","BIDAF ( ensemble )
achieves
EM score
of
73.3
F 1 score
of
81.1","BIDAF ( ensemble )||achieves||F 1 score
F 1 score||of||81.1
BIDAF ( ensemble )||achieves||EM score
EM score||of||73.3
",,,"Results||has||BIDAF ( ensemble )
",,,,,
ablation-analysis,Both char - level and word - level embeddings contribute towards the model 's performance .,"char - level and word - level embeddings
contribute towards
model 's performance","char - level and word - level embeddings||contribute towards||model 's performance
",,,"Ablation analysis||has||char - level and word - level embeddings
",,,,,
ablation-analysis,C2Q attention proves to be critical with a drop of more than 10 points on both metrics .,"C2Q attention
proves to be
critical
with
drop
of
more than 10 points","C2Q attention||proves to be||critical
critical||with||drop
drop||of||more than 10 points
",,,"Ablation analysis||has||C2Q attention
",,,,,
ablation-analysis,"Despite being a simpler attention mechanism , our proposed static attention outperforms the dynamically computed attention by more than 3 points .","proposed static attention
outperforms
dynamically computed attention
by
more than 3 points","proposed static attention||outperforms||dynamically computed attention
dynamically computed attention||by||more than 3 points
",,,"Ablation analysis||has||proposed static attention
",,,,,
ablation-analysis,"At the word embedding layer , query words such as When , Where and Who are not well aligned to possible answers in the context , but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer .","At
word embedding layer
query words
such as
When , Where and Who
not well aligned to
possible answers
in
context","query words||not well aligned to||possible answers
possible answers||in||context
query words||such as||When , Where and Who
","word embedding layer||has||query words
","Ablation analysis||At||word embedding layer
",,,,,,
research-problem,Focal Visual - Text Attention for Visual Question Answering,Visual Question Answering,,,,,"Contribution||has research problem||Visual Question Answering
",,,,
research-problem,"Visual question answering ( VQA ) is a successful direction utilizing both computer vision and natural language processing techniques to solve an interesting problem : given a pair of image and a question ( in natural language ) , the goal is to learn an inference model that can the answer questions according to cues discovered from the image .",Visual question answering ( VQA ),,,,,"Contribution||has research problem||Visual question answering ( VQA )
",,,,
research-problem,"Extending from VQA on a single image , this paper considers the following problem :",VQA,,,,,"Contribution||has research problem||VQA
",,,,
model,"To address these two challenges , we propose a focal visual - text attention ( FVTA ) model for sequential data","propose
focal visual - text attention ( FVTA ) model
for
sequential data","focal visual - text attention ( FVTA ) model||for||sequential data
",,"Model||propose||focal visual - text attention ( FVTA ) model
",,,,,,
model,"Inspired by this process , FVTA first learns to localize relevant information within a few , small , temporally consecutive regions over the input sequences , and learns to infer an answer based on the cross-modal statistics pooled from these regions .","FVTA
learns to
localize
relevant information
within
few , small , temporally consecutive regions
over
input sequences
infer
answer
based on
cross-modal statistics
pooled from
these regions","FVTA||learns to||localize
relevant information||within||few , small , temporally consecutive regions
few , small , temporally consecutive regions||over||input sequences
FVTA||learns to||infer
answer||based on||cross-modal statistics
cross-modal statistics||pooled from||these regions
","localize||has||relevant information
infer||has||answer
",,"Model||has||FVTA
",,,,,
model,FVTA proposes a novel kernel to compute the attention tensor that jointly models the latent information in three sources :,"proposes
novel kernel
to compute
attention tensor
jointly models
latent information
in
three sources","novel kernel||to compute||attention tensor
attention tensor||jointly models||latent information
latent information||in||three sources
",,,,,"FVTA||proposes||novel kernel
",,,"three sources||name||answer - signaling words
three sources||name||temporal correlation
three sources||name||cross-modal interaction
"
model,"1 ) answer - signaling words in the question , 2 ) temporal correlation within a sequence , and 3 ) cross-modal interaction between the text and image .","answer - signaling words
in
question
temporal correlation
within
sequence
cross-modal interaction
between
text and image","answer - signaling words||in||question
temporal correlation||within||sequence
cross-modal interaction||between||text and image
",,,,,,,,
model,"FVTA attention allows for collective reasoning by the attention kernel learned over a few , small , consecutive sub-sequences of text and image .","FVTA attention
allows for
collective reasoning
by
attention kernel
learned over
few , small , consecutive sub-sequences
of
text and image","FVTA attention||allows for||collective reasoning
collective reasoning||by||attention kernel
attention kernel||learned over||few , small , consecutive sub-sequences
few , small , consecutive sub-sequences||of||text and image
",,,"Model||has||FVTA attention
",,,,,
model,We propose a novel attention kernel for VQA on visual - text data .,"novel attention kernel
for
VQA
on
visual - text data","novel attention kernel||for||VQA
VQA||on||visual - text data
",,,"Model||propose||novel attention kernel
",,,,,
experiments,Memex QA provides 4 answer choices and only one correct answer for each question .,"Memex QA
answer",,,,"Experiments||has||Memex QA
",,,,,"Memex QA||has||Baselines
"
experiments,"We implement the following methods as baselines : Logistic Regression predicts the answer with concatenated image , question and metadata features as reported in .","Logistic Regression
predicts
with
concatenated image , question and metadata features",,,,,,"answer||with||concatenated image , question and metadata features
","Logistic Regression||predicts||answer
","Baselines||has||Logistic Regression
",
experiments,"Embedding + LSTM utilizes word embeddings and character embeddings , along with the same visual embeddings used in FVTA .","Embedding + LSTM
utilizes
word embeddings and character embeddings
along with
same visual embeddings
used in
FVTA","Embedding + LSTM||utilizes||word embeddings and character embeddings
word embeddings and character embeddings||along with||same visual embeddings
same visual embeddings||used in||FVTA
",,,,,,,"Baselines||has||Embedding + LSTM
",
experiments,Embedding + LSTM + Concat concatenates the last LSTM output from different modalities to produce the final output .,"Embedding + LSTM + Concat
concatenates
last LSTM output
from
different modalities
to produce
final output","Embedding + LSTM + Concat||concatenates||last LSTM output
last LSTM output||from||different modalities
different modalities||to produce||final output
",,,,,,,"Baselines||has||Embedding + LSTM + Concat
",
experiments,Classic Soft Attention uses classic one dimensional question - to - context attention to summarize context for question answering .,"Classic Soft Attention
uses
classic one dimensional question - to - context attention
to summarize
context
for
question answering","Classic Soft Attention||uses||classic one dimensional question - to - context attention
classic one dimensional question - to - context attention||to summarize||context
context||for||question answering
",,,,,,,"Baselines||has||Classic Soft Attention
",
experiments,"DMN + is the improved dynamic memory networks , which is one of the representative architectures that achieve good performance on the VQA Task .","DMN +
is
improved dynamic memory networks
which is one of
representative architectures
that achieve
good performance
on
VQA Task","DMN +||is||improved dynamic memory networks
improved dynamic memory networks||which is one of||representative architectures
representative architectures||that achieve||good performance
good performance||on||VQA Task
",,,,,,,"Baselines||has||DMN +
",
experiments,TGIF Temporal Attention is a recently proposed spatial - temporal reasoning network on sequential animated image QA .,"TGIF Temporal Attention
is
spatial - temporal reasoning network
on
sequential animated image QA","TGIF Temporal Attention||is||spatial - temporal reasoning network
spatial - temporal reasoning network||on||sequential animated image QA
",,,,,,,"Baselines||has||TGIF Temporal Attention
",
experiments,We encode GPS locations using words .,"encode
GPS locations
using
words","GPS locations||using||words
",,,,,"Experimental setup||encode||GPS locations
",,,
experiments,"All questions , textual context and answers are tokenized using the Stanford word tokenizer .","questions , textual context and answers
are
tokenized
using
Stanford word tokenizer","questions , textual context and answers||are||tokenized
tokenized||using||Stanford word tokenizer
",,,,,,,"Experimental setup||has||questions , textual context and answers
",
experiments,"We use pre-trained Glo Ve word embeddings , which is fixed during training .","use
pre-trained Glo Ve word embeddings
fixed during
training","pre-trained Glo Ve word embeddings||fixed during||training
",,,,,"Experimental setup||use||pre-trained Glo Ve word embeddings
",,,
experiments,"For image / video embedding , we extract fixed - size features using the pre-trained CNN model , Inception - ResNet , by concatenating the pool5 layer and classification layer 's output before softmax .","For
image / video embedding
extract
fixed - size features
using
pre-trained CNN model
Inception - ResNet
by concatenating
pool5 layer and classification layer 's output
before
softmax","image / video embedding||extract||fixed - size features
fixed - size features||using||pre-trained CNN model
pre-trained CNN model||by concatenating||pool5 layer and classification layer 's output
pool5 layer and classification layer 's output||before||softmax
","pre-trained CNN model||name||Inception - ResNet
",,,,"Experimental setup||For||image / video embedding
",,,
experiments,We then use a linear transformation to compress the image feature into 100 dimensional .,"linear transformation
to compress
image feature
into
100 dimensional","linear transformation||to compress||image feature
image feature||into||100 dimensional
",,,,,,,"Experimental setup||use||linear transformation
",
experiments,Then a bi-directional LSTM is used for each modality to obtain contextual representations .,"bi-directional LSTM
used for
each modality
to obtain
contextual representations","bi-directional LSTM||used for||each modality
each modality||to obtain||contextual representations
",,,,,,,"Experimental setup||has||bi-directional LSTM
",
experiments,"Given a hidden state size of d , which is set to 50 , we concatenate the output of both directions of the LSTM and get a question matrix Q ?","Given
hidden state size
of
d
set to
50
concatenate
output
of
both directions
of
LSTM
get
question matrix Q","output||of||both directions
both directions||of||LSTM
LSTM||Given||hidden state size
hidden state size||of||d
d||set to||50
",,,,,"Experimental setup||concatenate||output
all media documents||get||question matrix Q
",,,"all media documents||get||context tensor H
question matrix Q||has||R 2 d M
"
experiments,R 2 d M and context tensor H ?,"R 2 d M
context tensor H",,,,,,,,,"context tensor H||has||R 2dV KN 6
"
experiments,R 2dV KN 6 for all media documents .,"R 2dV KN 6
for
all media documents",,,,,,"LSTM||for||all media documents
",,,
experiments,We reshape the context tensor into H ? R 2 d T 6 .,"reshape
context tensor
into
H ? R 2 d T 6","context tensor||into||H ? R 2 d T 6
",,,,,"Experimental setup||reshape||context tensor
",,,
experiments,"To select the best hyperparmeters , we randomly select 20 % of the official training set as the validation set .","To select
best hyperparmeters
randomly select
20 %
of
official training set
as
validation set","best hyperparmeters||randomly select||20 %
20 %||of||official training set
official training set||as||validation set
",,,,,"Experimental setup||To select||best hyperparmeters
",,,
experiments,We use the AdaDelta optimizer and an initial learning rate of 0.5 to train for 200 epochs with a dropout rate of 0.3 ..,"AdaDelta optimizer
initial learning rate
of
0.5
to train for
200 epochs
with
dropout rate
of
0.3","initial learning rate||of||0.5
initial learning rate||of||0.5
0.5||to train for||200 epochs
200 epochs||with||dropout rate
dropout rate||of||0.3
",,,,,,,"Experimental setup||use||AdaDelta optimizer
Experimental setup||use||AdaDelta optimizer
Experimental setup||use||initial learning rate
",
experiments,FVTA outperforms other attention models on finding the relevant photos for the question .,"FVTA
outperforms
other attention models
on finding
relevant photos
for
question","FVTA||outperforms||other attention models
other attention models||on finding||relevant photos
relevant photos||for||question
",,,,,,,"Results||has||FVTA
Results||has||FVTA
",
experiments,"To evaluate the FVTA attention mechanism , we first replace our kernel tensor with simple cosine similarity function .","evaluate
FVTA attention mechanism
first replace
our kernel tensor
with
simple cosine similarity function","FVTA attention mechanism||first replace||our kernel tensor
our kernel tensor||with||simple cosine similarity function
",,,,,"Ablation analysis||evaluate||FVTA attention mechanism
",,,"our kernel tensor||has||Results
"
experiments,Results show that standard cosine similarity is inferior to our similarity function .,"show that
standard cosine similarity
inferior to
our similarity function","standard cosine similarity||inferior to||our similarity function
",,,,,"Results||show that||standard cosine similarity
",,,
experiments,"For ablating intra-sequence dependency , we use the representations from the last timestep of each context document .","For ablating
intra-sequence dependency
use
representations
from
last timestep
of
each context document","intra-sequence dependency||use||representations
representations||from||last timestep
last timestep||of||each context document
",,,,,"Ablation analysis||For ablating||intra-sequence dependency
",,,
experiments,"For ablating cross sequence interaction , we average all attended context representation from different modalities to get the final context vector .","cross sequence interaction
average
all attended context representation
from
different modalities
to get
final context vector","cross sequence interaction||average||all attended context representation
all attended context representation||from||different modalities
different modalities||to get||final context vector
",,,,,,,"Ablation analysis||For ablating||cross sequence interaction
",
experiments,"Both aspects of correlation of the FVTA attention tensor contribute towards the model 's performance , while intra-sequence dependency shows more importance in this experiment .","Both aspects of correlation
of
FVTA attention tensor
contribute towards
model 's performance
while
intra-sequence dependency
shows
more importance","Both aspects of correlation||of||FVTA attention tensor
Both aspects of correlation||while||intra-sequence dependency
intra-sequence dependency||shows||more importance
Both aspects of correlation||contribute towards||model 's performance
",,,,,,,"Ablation analysis||has||Both aspects of correlation
",
experiments,We compare the effectiveness of context - aware question attention by removing the question attention and use the last timestep of the LSTM output from the question as the question representation .,"compare
effectiveness
of
context - aware question attention
by removing
question attention
use
last timestep
of
LSTM output
from
question
as
question representation","effectiveness||of||context - aware question attention
context - aware question attention||by removing||question attention
context - aware question attention||use||last timestep
last timestep||of||LSTM output
LSTM output||as||question representation
LSTM output||from||question
","context - aware question attention||shows||question attention
",,,,"Ablation analysis||compare||effectiveness
",,,
experiments,It shows the question attention provides slight improvement .,"shows
question attention
provides
slight improvement","question attention||provides||slight improvement
",,,,,,,,
experiments,"Finally , we train FVTA without photos to see the contribution of visual information .","train
FVTA without photos
to see
contribution
of
visual information","FVTA without photos||to see||contribution
contribution||of||visual information
",,,,,"Ablation analysis||train||FVTA without photos
",,,"FVTA without photos||has||result
"
experiments,"The result is quite good but it is perhaps not surprising due to the language bias in the questions and answers of the dataset , which is not uncommon in VQA dataset and in Visual7W .","result
is
quite good","result||is||quite good
",,,,,,,,
experiments,"In the MovieQA dataset , each QA is given a set of N movie clips of the same movie , and each clip comes with subtitles .","MovieQA dataset
of
with",,,,"Experiments||has||MovieQA dataset
",,,,,"MovieQA dataset||has||Experimental setup
"
experiments,We implement FVTA network for Movie QA task with modality number of 2 ( video & text ) .,"implement
FVTA network
for
Movie QA task
modality number
2 ( video & text )","FVTA network||for||Movie QA task
","FVTA network||with||modality number
modality number||of||2 ( video & text )
",,,,"Experimental setup||implement||FVTA network
",,,
experiments,"We set the maximum number of movie clips per question to N = 20 , the maximum number of frames to consider to F = 10 , the maximum number of subtitle sentences in a clip to K = 100 and the maximum words to V = 10 .","set
maximum number of movie clips per question
to
N = 20
maximum number of frames to consider
to
F = 10
maximum number of subtitle sentences in a clip
to
K = 100
maximum words
to
V = 10","maximum number of frames to consider||to||F = 10
maximum number of subtitle sentences in a clip||to||K = 100
maximum words||to||V = 10
maximum number of movie clips per question||to||N = 20
",,,,,"Experimental setup||set||maximum number of frames to consider
Experimental setup||set||maximum number of subtitle sentences in a clip
Experimental setup||set||maximum words
Experimental setup||set||maximum number of movie clips per question
",,,
experiments,We use the AdaDelta optimizer with a minibatch of 16 and an initial learning rate of 0.5 to trained for 300 epochs .,"use
AdaDelta optimizer
with
minibatch
of
16
initial learning rate
of
0.5
trained for
300 epochs","AdaDelta optimizer||with||initial learning rate
initial learning rate||trained for||300 epochs
AdaDelta optimizer||with||minibatch
minibatch||of||16
",,,,,,,,
experiments,FVTA model outperforms all baseline methods and achieves comparable performance to the state - of - the - art result 2 on the MovieQA test server .,"FVTA model
outperforms
all baseline methods
achieves
comparable performance
to
state - of - the - art result
on
MovieQA test server","FVTA model||achieves||comparable performance
comparable performance||to||state - of - the - art result
state - of - the - art result||on||MovieQA test server
FVTA model||outperforms||all baseline methods
",,,,,,,"Results||has||FVTA model
",
experiments,Our accuracy is 0.410 ( vs 0.387 by RWMN ) on the validation set and 0.373 ( vs 0.363 ) on the test set .,"accuracy
is
0.410
vs
0.387
by
RWMN
on
validation set
0.373
vs
0.363
on
test set","accuracy||is||0.373
0.373||vs||0.363
0.373||on||test set
accuracy||is||0.410
0.410||vs||0.387
0.387||by||RWMN
0.410||on||validation set
",,,,,,,"Results||has||accuracy
",
experiments,"Benefiting from such modeling ability , FVTA consistently outperforms the classical attention models including soft attention , MCB and TGIF .","FVTA
consistently outperforms
classical attention models
including
soft attention
MCB
TGIF","FVTA||consistently outperforms||classical attention models
classical attention models||including||soft attention
classical attention models||including||MCB
classical attention models||including||TGIF
",,,,,,,,
research-problem,Multi - Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,Reading Comprehension,,,,,"Contribution||has research problem||Reading Comprehension
",,,,
research-problem,This paper presents a new compositional encoder for reading comprehension ( RC ) .,reading comprehension ( RC ),,,,,"Contribution||has research problem||reading comprehension ( RC )
",,,,
research-problem,"We conduct experiments on three RC datasets , showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block .",RC,,,,,"Contribution||has research problem||RC
",,,,
model,"To this end , we propose a new compositional encoder that can either be used in place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .","propose
new compositional encoder
be used
in place
of
standard RNN encoders
serve as
new module
complementary to
existing neural architectures","new compositional encoder||serve as||new module
new module||complementary to||existing neural architectures
new compositional encoder||be used||in place
in place||of||standard RNN encoders
",,"Model||propose||new compositional encoder
",,,,,,
model,Our proposed encoder leverages dilated compositions to model relationships across multiple granularities .,"Our proposed encoder
leverages
dilated compositions
to model
relationships
across
multiple granularities","Our proposed encoder||leverages||dilated compositions
dilated compositions||to model||relationships
relationships||across||multiple granularities
",,,"Model||has||Our proposed encoder
",,,,,
model,"That is , for a given word in the target sequence , our encoder exploits both long - term ( far ) and short - term ( near ) information to decide how much information to retain for it .","for
given word
in
target sequence
our encoder
exploits
long - term ( far ) and short - term ( near ) information
to decide
information
to
retain","given word||in||target sequence
our encoder||exploits||long - term ( far ) and short - term ( near ) information
long - term ( far ) and short - term ( near ) information||to decide||information
information||to||retain
","given word||has||our encoder
","Model||for||given word
",,,,,,
model,"The output of the dilated composition mechanism acts as gating functions , which are then used to learn compositional representations of the input sequence .","output
of
dilated composition mechanism
acts as
gating functions
used to learn
compositional representations
of
input sequence","output||of||dilated composition mechanism
dilated composition mechanism||acts as||gating functions
dilated composition mechanism||used to learn||compositional representations
compositional representations||of||input sequence
",,,"Model||has||output
",,,,,
baselines,RACE,RACE,,,,"Baselines||has||RACE
",,,,,"RACE||has||key competitors
"
baselines,"The key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .","key competitors
are
Stanford Attention Reader ( Stanford AR )
Gated Attention Reader ( GA )
Dynamic Fusion Networks ( DFN )","key competitors||are||Stanford Attention Reader ( Stanford AR )
key competitors||are||Gated Attention Reader ( GA )
key competitors||are||Dynamic Fusion Networks ( DFN )
",,,,,,,,
baselines,SearchQA,SearchQA,,,,"Baselines||has||SearchQA
",,,,,"SearchQA||has||main competitor baseline
"
baselines,The main competitor baseline is the AMANDA model proposed by .,"main competitor baseline
is
AMANDA model","main competitor baseline||is||AMANDA model
",,,,,,,,
baselines,NarrativeQA,NarrativeQA,,,,"Baselines||has||NarrativeQA
",,,,,"NarrativeQA||has||baselines
"
baselines,"We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .","baselines
are
context - less sequence to sequence ( seq2seq ) model
ASR
BiDAF","baselines||are||context - less sequence to sequence ( seq2seq ) model
baselines||are||ASR
baselines||are||BiDAF
",,,,,,,,
experimental-setup,We implement all models in TensorFlow .,"implement
all models
in
TensorFlow","all models||in||TensorFlow
",,"Experimental setup||implement||all models
",,,,,,
experimental-setup,Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,"Word embeddings
initialized with
300d Glo Ve vectors
not fine - tuned during
training","Word embeddings||initialized with||300d Glo Ve vectors
Word embeddings||not fine - tuned during||training
",,,"Experimental setup||has||Word embeddings
",,,,,
experimental-setup,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .","Dropout rate
tuned amongst
{ 0.1 , 0.2 , 0.3 }
on
all layers
including
embedding layer","Dropout rate||tuned amongst||{ 0.1 , 0.2 , 0.3 }
{ 0.1 , 0.2 , 0.3 }||on||all layers
all layers||including||embedding layer
",,,"Experimental setup||has||Dropout rate
",,,,,
experimental-setup,"We adopt the Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / Narrative QA respectively .","adopt
Adam optimizer
with
learning rate
of
0.0003/ 0.001/0.001
for
RACE / SearchQA / Narrative QA","Adam optimizer||with||learning rate
learning rate||of||0.0003/ 0.001/0.001
0.0003/ 0.001/0.001||for||RACE / SearchQA / Narrative QA
",,"Experimental setup||adopt||Adam optimizer
",,,,,,
experimental-setup,The batch size is set to 64/256/32 accordingly .,"batch size
set to
64/256/32","batch size||set to||64/256/32
",,,"Experimental setup||has||batch size
",,,,,
experimental-setup,The maximum sequence lengths are 500/200/1100 respectively .,"maximum sequence lengths
are
500/200/1100","maximum sequence lengths||are||500/200/1100
",,,"Experimental setup||has||maximum sequence lengths
",,,,,
experimental-setup,"For Narrative QA , we use the Rouge - L score to find the best approximate answer relative to the human written answer for training the span model .","For
Narrative QA
use
Rouge - L score
to find
best approximate answer
relative to
human written answer
for training
span model","Narrative QA||use||Rouge - L score
Rouge - L score||to find||best approximate answer
best approximate answer||relative to||human written answer
human written answer||for training||span model
",,"Experimental setup||For||Narrative QA
",,,,,,
experimental-setup,All models are trained and all runtime benchmarks are based on a TitanXP GPU .,"runtime benchmarks
based on
TitanXP GPU","runtime benchmarks||based on||TitanXP GPU
",,,"Experimental setup||has||runtime benchmarks
",,,,,
results,reports our results on the RACE benchmark dataset .,"on
RACE benchmark dataset",,,"Results||on||RACE benchmark dataset
",,,,,,"RACE benchmark dataset||has||Our proposed DCU model
"
results,Our proposed DCU model achieves the best result for both single models and ensemble models .,"Our proposed DCU model
achieves
best result
for
single models
ensemble models","Our proposed DCU model||achieves||best result
best result||for||single models
best result||for||ensemble models
",,,,,,,,
results,We outperform highly complex models such as DFN .,"outperform
highly complex models
such as
DFN","highly complex models||such as||DFN
",,,,,"RACE benchmark dataset||outperform||highly complex models
",,,
results,We also pull ahead of other recent baselines such as ElimiNet and GA by at least 5 % .,"pull ahead of
other recent baselines
such as
ElimiNet
GA
by
at least 5 %","other recent baselines||by||at least 5 %
other recent baselines||such as||ElimiNet
other recent baselines||such as||GA
",,,,,"RACE benchmark dataset||pull ahead of||other recent baselines
",,,
results,The best single model score from RACE - H and RACE - M alternates between Sim - DCU and DCU .,"best single model score
from
RACE - H
RACE - M
alternates between
Sim - DCU
DCU","best single model score||from||RACE - H
best single model score||from||RACE - M
best single model score||alternates between||Sim - DCU
best single model score||alternates between||DCU
",,,,,,,"RACE benchmark dataset||has||best single model score
",
results,Table 2 reports our results on the Search QA dataset .,Search QA dataset,,,,"Results||on||Search QA dataset
",,,,,
results,We achieve the same accuracy as AMANDA without using any LSTM or GRU encoder .,"achieve
same accuracy
as
AMANDA
without using
LSTM or GRU encoder","same accuracy||as||AMANDA
same accuracy||without using||LSTM or GRU encoder
",,,,,"Search QA dataset||achieve||same accuracy
",,,
results,"Finally , the hybrid combination , DCU - LSTM significantly outperforms AMANDA by 3 % .","hybrid combination , DCU - LSTM
significantly outperforms
AMANDA
by
3 %","hybrid combination , DCU - LSTM||significantly outperforms||AMANDA
AMANDA||by||3 %
",,,,,,,"Search QA dataset||has||hybrid combination , DCU - LSTM
",
results,"Contrary to MCQ - based datasets , we found that reports our results on the NarrativeQA benchmark .",NarrativeQA benchmark,,,,"Results||on||NarrativeQA benchmark
",,,,,
results,"First , we observe that 300d DCU can achieve comparable performance with BiDAF .","observe that
300d DCU
achieve
comparable performance
with
BiDAF","300d DCU||achieve||comparable performance
comparable performance||with||BiDAF
",,,,,"NarrativeQA benchmark||observe that||300d DCU
",,,
results,"Finally , DCU - LSTM significantly outperforms all models in terms of ROUGE - L , including BiDAF on this dataset .","DCU - LSTM
significantly outperforms
all models
in terms of
ROUGE - L
including
BiDAF","DCU - LSTM||significantly outperforms||all models
all models||including||BiDAF
all models||in terms of||ROUGE - L
",,,,,,,"NarrativeQA benchmark||has||DCU - LSTM
",
results,"Performance improvement over the vanilla BiLSTM model ranges from 1 % ? 3 % across all metrics , suggesting that DCU encoders are also effective as a complementary neural building block .","Performance improvement
over
vanilla BiLSTM model
ranges from
1 % ? 3 %
across
all metrics","Performance improvement||over||vanilla BiLSTM model
vanilla BiLSTM model||ranges from||1 % ? 3 %
1 % ? 3 %||across||all metrics
",,,,,,,"NarrativeQA benchmark||has||Performance improvement
",
research-problem,Densely Connected Attention Propagation for Reading Comprehension,Reading Comprehension,,,,,"Contribution||has research problem||Reading Comprehension
",,,,
research-problem,"We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) .",reading comprehension ( RC ),,,,,"Contribution||has research problem||reading comprehension ( RC )
",,,,
research-problem,We conduct extensive experiments on four challenging RC benchmarks .,RC,,,,,"Contribution||has research problem||RC
",,,,
model,"Firstly , our network is densely connected , connecting every layer of P with every layer of Q .","network
is
densely connected","network||is||densely connected
",,,"Model||has||network
",,,,,
model,"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect two sequences at arbitrary layers .","propose
efficient Bidirectional Attention Connectors ( BAC )
as
base building block
to connect
two sequences
at
arbitrary layers","efficient Bidirectional Attention Connectors ( BAC )||as||base building block
base building block||to connect||two sequences
two sequences||at||arbitrary layers
",,"Model||propose||efficient Bidirectional Attention Connectors ( BAC )
",,,,,,
model,"The key idea is to compress the attention outputs so that they can be small enough to propagate , yet enabling a connection between two sequences .","is
to
compress
attention outputs
be
small
propagate","attention outputs||be||small
small||to||propagate
",,"Model||compress||attention outputs
",,,,,,
model,"The propagated features are collectively passed into prediction layers , which effectively connect shallow layers to deeper layers .","propagated features
collectively passed into
prediction layers
effectively connect
shallow layers
to
deeper layers","propagated features||collectively passed into||prediction layers
propagated features||effectively connect||shallow layers
shallow layers||to||deeper layers
",,,"Model||has||propagated features
",,,,,
model,"Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .","DECAPROP ( Densely Connected Attention Propagation )
novel architecture
for
reading comprehension","novel architecture||for||reading comprehension
","DECAPROP ( Densely Connected Attention Propagation )||is||novel architecture
",,"Model||propose||DECAPROP ( Densely Connected Attention Propagation )
",,,,,
baselines,NewsQA,NewsQA,,,,"Baselines||has||NewsQA
",,,,,"NewsQA||has||key competitors
"
baselines,"On this dataset , the key competitors are BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .","key competitors
are
BiDAF
Match - LSTM
FastQA / Fast QA - Ext
R2-BiLSTM
AMANDA","key competitors||are||BiDAF
key competitors||are||Match - LSTM
key competitors||are||FastQA / Fast QA - Ext
key competitors||are||R2-BiLSTM
key competitors||are||AMANDA
key competitors||are||BiDAF
",,,,,,,,
baselines,Quasar -T,Quasar -T,,,,"Baselines||has||Quasar -T
",,,,,"Quasar -T||has||key competitors
"
baselines,The key competitors on this dataset are BiDAF and the Reinforced Ranker - Reader ( R 3 ) .,"key competitors
are
BiDAF
Reinforced Ranker - Reader ( R 3 )","key competitors||are||Reinforced Ranker - Reader ( R 3 )
",,,,,,,,
baselines,SearchQA,SearchQA,,,,"Baselines||has||SearchQA
",,,,,"SearchQA||has||competitor baselines
"
baselines,"The competitor baselines on this dataset are Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .","competitor baselines
are
Attention Sum Reader ( ASR )
Focused Hierarchical RNNs ( FH - RNN )
AMANDA
BiDAF
AQA
Reinforced Ranker - Reader ( R 3 )","competitor baselines||are||Attention Sum Reader ( ASR )
competitor baselines||are||Focused Hierarchical RNNs ( FH - RNN )
competitor baselines||are||AMANDA
competitor baselines||are||BiDAF
competitor baselines||are||AQA
competitor baselines||are||Reinforced Ranker - Reader ( R 3 )
",,,,,,,,
baselines,Narrative QA ] is a recent QA dataset that involves comprehension over stories .,Narrative QA,,,,"Baselines||has||Narrative QA
",,,,,
baselines,"We compare with the baselines in the original paper , namely Seq2Seq , Attention Sum Reader and BiDAF .","compare with
baselines
namely
Seq2Seq
Attention Sum Reader
BiDAF","baselines||namely||Seq2Seq
baselines||namely||Attention Sum Reader
baselines||namely||BiDAF
",,,,,"Narrative QA||compare with||baselines
",,,
baselines,We also compare with the recent BiAttention + MRU model .,recent BiAttention + MRU model,,,,,,,,"Narrative QA||compare with||recent BiAttention + MRU model
",
experimental-setup,Our model is implemented in Tensorflow .,"model
implemented in
Tensorflow","model||implemented in||Tensorflow
",,,"Experimental setup||has||model
",,,,,
experimental-setup,"The sequence lengths are capped at 800/700/1500/1100 for News QA , Search QA , Quasar - T and Narrative QA respectively .","sequence lengths
capped at
800/700/1500/1100
for
News QA , Search QA , Quasar - T and Narrative QA","sequence lengths||capped at||800/700/1500/1100
800/700/1500/1100||for||News QA , Search QA , Quasar - T and Narrative QA
",,,"Experimental setup||has||sequence lengths
",,,,,
experimental-setup,"We use Adadelta with ? = 0.5 for News QA , Adam with ? = 0.001 for Search QA , Quasar - T and Narrative QA .","use
Adadelta
with
? = 0.5
for
News QA
Adam
with
? = 0.001
for
Search QA
Quasar - T
Narrative QA","Adam||with||? = 0.001
? = 0.001||for||Search QA
? = 0.001||for||Quasar - T
? = 0.001||for||Narrative QA
Adadelta||with||? = 0.5
? = 0.5||for||News QA
",,"Experimental setup||use||Adam
Experimental setup||use||Adadelta
",,,,,,
experimental-setup,"The choice of the RNN encoder is tuned between GRU and LSTM cells and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .","choice of
RNN encoder
tuned between
GRU and LSTM cells
hidden size
tuned amongst
{ 32 , 50 , 64 , 75 }","hidden size||tuned amongst||{ 32 , 50 , 64 , 75 }
RNN encoder||tuned between||GRU and LSTM cells
",,"Experimental setup||choice of||hidden size
Experimental setup||choice of||RNN encoder
",,,,,,
experimental-setup,We use the CUDNN implementation of the RNN encoder .,"CUDNN implementation
of
RNN encoder","CUDNN implementation||of||RNN encoder
",,,"Experimental setup||use||CUDNN implementation
",,,,,
experimental-setup,"Batch size is tuned amongst { 16 , 32 , 64 } .","Batch size
tuned amongst
{ 16 , 32 , 64 }","Batch size||tuned amongst||{ 16 , 32 , 64 }
",,,"Experimental setup||has||Batch size
",,,,,
experimental-setup,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } and applied to all RNN and fully - connected layers .","Dropout rate
tuned amongst
{ 0.1 , 0.2 , 0.3 }
applied to
all RNN and fully - connected layers","Dropout rate||tuned amongst||{ 0.1 , 0.2 , 0.3 }
Dropout rate||applied to||all RNN and fully - connected layers
",,,"Experimental setup||has||Dropout rate
",,,,,
experimental-setup,We apply variational dropout in - between RNN layers .,"apply
variational dropout
in - between
RNN layers","variational dropout||in - between||RNN layers
",,"Experimental setup||apply||variational dropout
",,,,,,
experimental-setup,We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training .,"initialize
word embeddings
with
300D Glo Ve embeddings
fixed during
training","word embeddings||with||300D Glo Ve embeddings
300D Glo Ve embeddings||fixed during||training
",,"Experimental setup||initialize||word embeddings
",,,,,,
experimental-setup,The size of the character embeddings is set to 8 and the character RNN is set to the same as the word - level RNN encoders .,"size
of
character embeddings
set to
8
character RNN
set to
word - level RNN encoders","size||of||character embeddings
character embeddings||set to||8
size||of||character RNN
character RNN||set to||word - level RNN encoders
",,,"Experimental setup||has||size
",,,,,
experimental-setup,The maximum characters per word is set to 16 .,"maximum characters per word
set to
16","maximum characters per word||set to||16
",,,"Experimental setup||has||maximum characters per word
",,,,,
experimental-setup,The number of layers in DECAENC is set to 3 and the number of factors in the factorization kernel is set to 64 .,"number of layers
in
DECAENC
set to
3
number of factors
in
factorization kernel
set to
64","number of factors||in||factorization kernel
factorization kernel||set to||64
number of layers||in||DECAENC
DECAENC||set to||3
",,,"Experimental setup||has||number of factors
Experimental setup||has||number of layers
",,,,,
experimental-setup,We use a learning rate decay factor of 2 and patience of 3 epochs whenever the EM ( or ROUGE - L ) score on the development set does not increase .,"learning rate decay factor
of
2
patience
of
3 epochs","learning rate decay factor||of||2
patience||of||3 epochs
",,,"Experimental setup||use||learning rate decay factor
Experimental setup||use||patience
",,,,,
results,"Overall , our results are optimistic and promising , with results indicating that DECAPROP achieves state - of - the - art performance 6 on all four datasets . 66.2 75.9 DCN + CoVE 71.3 79.9 R- NET 72.3 80.6 R - NET","DECAPROP
achieves
state - of - the - art performance
on
all four datasets","DECAPROP||achieves||state - of - the - art performance
state - of - the - art performance||on||all four datasets
",,,"Results||has||DECAPROP
",,,,,
results,"On this dataset , DECAPROP outperforms the existing state - of - the - art , i.e. , the recent AMANDA model by ( + 4.7 % EM / + 2.6 % F1 ) .","outperforms
existing state - of - the - art
i.e.
recent AMANDA model
by
+ 4.7 % EM
+ 2.6 % F1","existing state - of - the - art||by||+ 4.7 % EM
existing state - of - the - art||by||+ 2.6 % F1
existing state - of - the - art||i.e.||recent AMANDA model
",,,,,"DECAPROP||outperforms||existing state - of - the - art
",,,
results,"Moreover , our proposed model also outperforms well - established baselines such as Match - LSTM ( + 18 % EM / + 16.3 % F1 ) and BiDAF ( + 16 % EM / + 14 % F1 ) .","our proposed model
outperforms
well - established baselines
such as
Match - LSTM ( + 18 % EM / + 16.3 % F1 )
BiDAF ( + 16 % EM / + 14 % F1 )","our proposed model||outperforms||well - established baselines
well - established baselines||such as||Match - LSTM ( + 18 % EM / + 16.3 % F1 )
well - established baselines||such as||BiDAF ( + 16 % EM / + 14 % F1 )
",,,"Results||has||our proposed model
",,,,,
results,reports the results on Quasar - T .,"on
Quasar - T",,,"Results||on||Quasar - T
",,,,,,"Quasar - T||has||Our model
"
results,"Our model achieves state - of - the - art performance on this dataset , outperforming the state - of - the - art R 3 ( Reinforced Ranker Reader ) by a considerable margin of + 4.4 % EM / + 6 % F1 .","Our model
achieves
state - of - the - art performance
outperforming
state - of - the - art R 3 ( Reinforced Ranker Reader )
by
considerable margin
of
+ 4.4 % EM / + 6 % F1","Our model||achieves||state - of - the - art performance
state - of - the - art performance||outperforming||state - of - the - art R 3 ( Reinforced Ranker Reader )
state - of - the - art R 3 ( Reinforced Ranker Reader )||by||considerable margin
considerable margin||of||+ 4.4 % EM / + 6 % F1
",,,,,,,,
results,"On the original setting , our model outperforms AMANDA by + 15.4 % EM and + 14.2 % in terms of F1 score .","our model
outperforms
AMANDA
by
+ 15.4 % EM and + 14.2 % in terms of F1 score","our model||outperforms||AMANDA
AMANDA||by||+ 15.4 % EM and + 14.2 % in terms of F1 score
",,,,,,,"Quasar - T||has||our model
",
results,"On the over all setting , our model outperforms both AQA ( + 18.1 % EM / + 18 % F1 ) and Reinforced Reader Ranker ( + 7.8 % EM / + 8.3 % F1 ) .","AQA
+ 18.1 % EM / + 18 % F1
Reinforced Reader Ranker
+ 7.8 % EM / + 8.3 % F1",,"AQA||has||+ 18.1 % EM / + 18 % F1
Reinforced Reader Ranker||has||+ 7.8 % EM / + 8.3 % F1
",,,,,,"our model||outperforms||AQA
our model||outperforms||Reinforced Reader Ranker
",
results,SQuAD reports dev scores 8 of our model against several representative models on the popular SQuAD benchmark .,"our model
popular SQuAD benchmark",,"popular SQuAD benchmark||has||our model
",,"Results||on||popular SQuAD benchmark
",,,,,
results,"While our model does not achieve state - of - the - art performance , our model can outperform the base R - NET ( both our implementation as well as the published score ) .","does not achieve
state - of - the - art performance
can outperform
base R - NET",,,,,,"our model||can outperform||base R - NET
our model||does not achieve||state - of - the - art performance
",,,
ablation-analysis,We conduct an ablation study on the New s QA development set .,"on
New s QA development set",,,"Ablation analysis||on||New s QA development set
",,,,,,
ablation-analysis,"Finally , in ( 8 - 9 ) , we varied the FM with linear and nonlinear feed - forward layers . From ( 1 ) , we observe a significant gap in performance between DECAPROP and R - NET .","in
observe
significant gap
performance
between
DECAPROP and R - NET","significant gap||in||performance
performance||between||DECAPROP and R - NET
",,"Ablation analysis||observe||significant gap
",,,,,,
ablation-analysis,"Overall , the key insight is that all model components are crucial to DECAPROP .","key insight
is
all model components
are crucial to
DECAPROP","key insight||is||all model components
all model components||are crucial to||DECAPROP
",,,"Ablation analysis||has||key insight
",,,,,
ablation-analysis,"Notably , the DECAENC seems to contribute the most to the over all performance .","DECAENC
seems to contribute
most
to
over all performance","DECAENC||seems to contribute||most
most||to||over all performance
",,,"Ablation analysis||has||DECAENC
",,,,,
ablation-analysis,We observe that the superiority of DECAPROP over R - NET is consistent and relatively stable .,"superiority
of
DECAPROP
over
R - NET
is
consistent and relatively stable","superiority||of||DECAPROP
DECAPROP||over||R - NET
DECAPROP||is||consistent and relatively stable
",,,"Ablation analysis||observe||superiority
",,,,,
research-problem,EVIDENCE AGGREGATION FOR ANSWER RE - RANKING IN OPEN - DOMAIN QUESTION ANSWERING,OPEN - DOMAIN QUESTION ANSWERING,,,,,"Contribution||has research problem||OPEN - DOMAIN QUESTION ANSWERING
",,,,
research-problem,Open-domain question answering ( QA ) aims to answer questions from a broad range of domains by effectively marshalling evidence from large open - domain knowledge sources .,Open-domain question answering ( QA ),,,,,"Contribution||has research problem||Open-domain question answering ( QA )
",,,,
research-problem,Recent work on open - domain QA has focused on using unstructured text retrieved from the web to build machine comprehension models .,open - domain QA,,,,,"Contribution||has research problem||open - domain QA
",,,,
model,"In this paper , we propose a method to improve open - domain QA by explicitly aggregating evidence from across multiple passages .","propose
method
to improve
open - domain QA
by explicitly aggregating
evidence
from across
multiple passages","method||by explicitly aggregating||evidence
evidence||from across||multiple passages
method||to improve||open - domain QA
",,"Model||propose||method
",,,,,,
model,We formulate the above evidence aggregation as an answer re-ranking problem .,"formulate
evidence aggregation
as
answer re-ranking problem","evidence aggregation||as||answer re-ranking problem
",,"Model||formulate||evidence aggregation
",,,,,,
model,"Here we apply the idea of re-ranking ; for each answer candidate , we efficiently incorporate global information from multiple pieces of textual evidence without significantly increasing the complexity of the prediction of the RC model .","of
for
each answer candidate
efficiently incorporate
global information
from
multiple pieces
of
textual evidence
without
significantly increasing
complexity
of
prediction
RC model","each answer candidate||efficiently incorporate||global information
global information||from||multiple pieces
multiple pieces||of||textual evidence
global information||without||significantly increasing
complexity||of||prediction
prediction||of||RC model
","significantly increasing||has||complexity
","Model||for||each answer candidate
",,,,,,
model,The re-rankers are :,"re-rankers
are",,,,"Model||has||re-rankers
",,,"re-rankers||are||strength - based re-ranker
",,
model,"A strength - based re-ranker , which ranks the answer candidates according to how often their evidence occurs in different passages .","strength - based re-ranker
ranks
answer candidates
according to how often
evidence
occurs in
different passages","strength - based re-ranker||ranks||answer candidates
answer candidates||according to how often||evidence
evidence||occurs in||different passages
",,,,,,,,
model,"A coverage - based re-ranker , which aims to rank an answer candidate higher if the union of all its contexts in different passages could cover more aspects included in the question .","coverage - based re-ranker
aims to rank
answer candidate
higher
if
union
of
all its contexts
in
different passages
could cover
more aspects
included in
question","coverage - based re-ranker||aims to rank||answer candidate
higher||if||union
union||of||all its contexts
all its contexts||in||different passages
all its contexts||could cover||more aspects
more aspects||included in||question
","answer candidate||has||higher
",,,,,,"re-rankers||are||coverage - based re-ranker
",
baselines,"Our baseline models 9 include the following : GA , a reading comprehension model with gated - attention ; BiDAF ) , a RC model with bidirectional attention flow ; AQA ) , a reinforced system learning to aggregate the answers generated by the re-written questions ; R 3 ) , a reinforced model making use of a ranker for selecting passages to train the RC model .","include
GA
reading comprehension model
with
gated - attention
BiDAF
RC model
with
bidirectional attention flow
AQA
reinforced system learning
to aggregate
answers
generated by
re-written questions
R 3
reinforced model
making use of
ranker
for selecting
passages
to train
RC model","RC model||with||bidirectional attention flow
reinforced system learning||to aggregate||answers
answers||generated by||re-written questions
reinforced model||making use of||ranker
ranker||for selecting||passages
passages||to train||RC model
reading comprehension model||with||gated - attention
","BiDAF||has||RC model
AQA||has||reinforced system learning
R 3||has||reinforced model
GA||has||reading comprehension model
","Baselines||include||BiDAF
Baselines||include||AQA
Baselines||include||R 3
Baselines||include||GA
",,,,,,
hyperparameters,"We first use a pre-trained R 3 model , which gets the state - of - the - art performance on the three public datasets we consider , to generate the top 50 candidate spans for the training , development and test datasets , and we use them for further ranking .","use
pre-trained R 3 model
to generate
top 50 candidate spans
for
training , development and test datasets
use them for
further ranking","pre-trained R 3 model||to generate||top 50 candidate spans
top 50 candidate spans||use them for||further ranking
top 50 candidate spans||for||training , development and test datasets
",,"Hyperparameters||use||pre-trained R 3 model
",,,,,,
hyperparameters,"For the coverage - based re-ranker , we use Adam to optimize the model .","For
coverage - based re-ranker
use
Adam
to optimize
model","coverage - based re-ranker||use||Adam
Adam||to optimize||model
",,"Hyperparameters||For||coverage - based re-ranker
",,,,,,
hyperparameters,We set all the words beyond Glove as zero vectors .,"set
all the words
beyond
Glove
as
zero vectors","all the words||as||zero vectors
all the words||beyond||Glove
",,"Hyperparameters||set||all the words
",,,,,,
hyperparameters,"We set l to 300 , batch size to 30 , learning rate to 0.002 .","l
to
300
batch size
to
30
learning rate
to
0.002","learning rate||to||0.002
batch size||to||30
l||to||300
",,,"Hyperparameters||set||learning rate
Hyperparameters||set||batch size
Hyperparameters||set||l
",,,,,
hyperparameters,"We tune the dropout probability from 0 to 0.5 and the number of candidate answers for re-ranking ( K ) in [ 3 , 5 , 10 ] 11 .","tune
dropout probability
from
0 to 0.5
number of candidate answers
for
re-ranking ( K )
in
[ 3 , 5 , 10 ]","dropout probability||from||0 to 0.5
number of candidate answers||for||re-ranking ( K )
re-ranking ( K )||in||[ 3 , 5 , 10 ]
",,"Hyperparameters||tune||dropout probability
Hyperparameters||tune||number of candidate answers
",,,,,,
results,"The results showed that R 3 achieved F1 56.0 , EM 50.9 on Wiki domain and F1 68.5 , EM 63.0 on Web domain , which is competitive to the state - of - the - arts .","showed that
R 3
achieved
F1 56.0 , EM 50.9
on
Wiki domain
F1 68.5 , EM 63.0
on
Web domain
competitive to
state - of - the - arts","R 3||competitive to||state - of - the - arts
R 3||achieved||F1 68.5 , EM 63.0
F1 68.5 , EM 63.0||on||Web domain
R 3||achieved||F1 56.0 , EM 50.9
F1 56.0 , EM 50.9||on||Wiki domain
",,"Results||showed that||R 3
",,,,,,
code,Our code will be released under https://github.com/shuohangwang/mprc.,,,,,,,,,,
results,"From the results , we can clearly see that the full re-ranker , the combination of different re-rankers , significantly outperforms the previous best performance by a large margin , especially on Quasar - T and Search QA .","see that
full re-ranker
combination of
different re-rankers
significantly outperforms
previous best performance
by
large margin
especially on
Quasar - T
Search QA","full re-ranker||combination of||different re-rankers
full re-ranker||significantly outperforms||previous best performance
previous best performance||especially on||Quasar - T
previous best performance||especially on||Search QA
previous best performance||by||large margin
",,"Results||see that||full re-ranker
",,,,,,
results,"Moreover , our model is much better than the human performance on the Search QA dataset .","our model
is
much better
than
human performance
on
Search QA dataset","our model||is||much better
much better||than||human performance
human performance||on||Search QA dataset
",,,"Results||has||our model
",,,,,
results,"In addition , we see that our coverage - based re-ranker achieves consistently good performance on the three datasets , even though its performance is marginally lower than the strength - based re-ranker on the Search QA dataset .","our coverage - based re-ranker
achieves
consistently good performance
on
three datasets","our coverage - based re-ranker||achieves||consistently good performance
consistently good performance||on||three datasets
",,,"Results||see that||our coverage - based re-ranker
",,,,,
research-problem,Neural Question Generation from Text : A Preliminary Study,Neural Question Generation from Text,,,,,"Contribution||has research problem||Neural Question Generation from Text
",,,,
research-problem,Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub- spans of the given passage .,Automatic question generation,,,,,"Contribution||has research problem||Automatic question generation
",,,,
research-problem,"Automatic question generation from natural language text aims to generate questions taking text as input , which has the potential value of education purpose ) .",Automatic question generation from natural language text,,,,,"Contribution||has research problem||Automatic question generation from natural language text
",,,,
research-problem,"As the reverse task of question answering , question generation also has the potential for providing a large scale corpus of question - answer pairs .",question generation,,,,,"Contribution||has research problem||question generation
",,,,
model,"In this work we conduct a preliminary study on question generation from text with neural networks , which is denoted as the Neural Question Generation ( NQG ) framework , to generate natural language questions from text without pre-defined rules .","denoted as
Neural Question Generation ( NQG ) framework
to generate
natural language questions from text
without
pre-defined rules","Neural Question Generation ( NQG ) framework||to generate||natural language questions from text
natural language questions from text||without||pre-defined rules
",,"Model||denoted as||Neural Question Generation ( NQG ) framework
",,,,,,
model,The Neural Question Generation framework extends the sequence - to - sequence models by enriching the encoder with answer and lexical features to generate answer focused questions .,"Neural Question Generation framework
extends
sequence - to - sequence models
by enriching
encoder
with
answer
lexical features
to generate
answer focused questions","Neural Question Generation framework||extends||sequence - to - sequence models
sequence - to - sequence models||by enriching||encoder
encoder||with||answer
encoder||with||lexical features
encoder||to generate||answer focused questions
",,,"Model||has||Neural Question Generation framework
",,,,,
model,"Concretely , the encoder reads not only the input sentence , but also the answer position indicator and lexical features .","encoder
reads
input sentence
answer position indicator
lexical features","encoder||reads||input sentence
encoder||reads||answer position indicator
encoder||reads||lexical features
",,,"Model||has||encoder
",,,,,
model,"The answer position feature denotes the answer span in the input sentence , which is essential to generate answer relevant questions .","answer position feature
denotes
answer span
in
input sentence","answer position feature||denotes||answer span
answer span||in||input sentence
",,,"Model||has||answer position feature
",,,,,
model,The lexical features include part - of - speech ( POS ) and named entity ( NER ) tags to help produce better sentence encoding .,"lexical features
include
part - of - speech ( POS )
named entity ( NER ) tags
to help produce
better sentence encoding","lexical features||to help produce||better sentence encoding
better sentence encoding||include||part - of - speech ( POS )
better sentence encoding||include||named entity ( NER ) tags
",,,"Model||has||lexical features
",,,,,
model,"Lastly , the decoder with attention mechanism generates an answer specific question of the sentence .","decoder
with
attention mechanism
generates
answer specific question
of
sentence","decoder||with||attention mechanism
decoder||generates||answer specific question
answer specific question||of||sentence
",,,"Model||has||decoder
",,,,,
baselines,PCFG - Trans,PCFG - Trans,,,,"Baselines||has||PCFG - Trans
",,,,,"PCFG - Trans||has||rule - based system
"
baselines,The rule - based system 1 modified on the code released by .,rule - based system,,,,,,,,,
baselines,s 2 s+ att,s 2 s+ att,,,,"Baselines||has||s 2 s+ att
",,,,,
baselines,We implement a seq2seq with attention as the baseline method .,"implement
seq2seq with attention",,,,,,"s 2 s+ att||implement||seq2seq with attention
",,,
baselines,NQG,NQG,,,,"Baselines||has||NQG
",,,,,
baselines,We extend the s 2s+ att with our feature - rich encoder to build the NQG system .,"extend
s 2s+ att
with
feature - rich encoder
to build
NQG system","s 2s+ att||with||feature - rich encoder
feature - rich encoder||to build||NQG system
",,,,,"NQG||extend||s 2s+ att
",,,
baselines,"NQG + Based on NQG , we incorporate copy mechanism to deal with rare words problem .","NQG +
incorporate
copy mechanism
deal with
rare words problem","NQG +||incorporate||copy mechanism
copy mechanism||deal with||rare words problem
",,,"Baselines||has||NQG +
",,,,,
baselines,"NQG + Pretrain Based on NQG + , we initialize the word embedding matrix with pre-trained GloVe vectors .","NQG + Pretrain
Based on
NQG +
initialize
word embedding matrix
with
pre-trained GloVe vectors","NQG + Pretrain||Based on||NQG +
NQG +||initialize||word embedding matrix
word embedding matrix||with||pre-trained GloVe vectors
",,,"Baselines||has||NQG + Pretrain
",,,,,
baselines,"NQG + STshare Based on NQG + , we make the encoder and decoder share the same embedding matrix .","NQG + STshare
Based on
NQG +
make
encoder and decoder
share
same embedding matrix","NQG + STshare||Based on||NQG +
NQG + STshare||make||encoder and decoder
encoder and decoder||share||same embedding matrix
",,,"Baselines||has||NQG + STshare
",,,,,
baselines,NQG ++,NQG ++,,,,"Baselines||has||NQG ++
",,,,,
baselines,"Based on NQG + , we use both pre-train word embedding and STshare methods , to further improve the performance .","Based on
NQG +
use
pre-train word embedding and STshare methods
to further improve
performance","NQG +||use||pre-train word embedding and STshare methods
pre-train word embedding and STshare methods||to further improve||performance
",,,,,"NQG ++||Based on||NQG +
",,,
results,Our NQG framework outperforms the PCFG - Trans and s 2s + att baselines by a large margin .,"NQG framework
outperforms
PCFG - Trans and s 2s + att baselines
by
large margin","NQG framework||outperforms||PCFG - Trans and s 2s + att baselines
PCFG - Trans and s 2s + att baselines||by||large margin
",,,"Results||has||NQG framework
",,,,,
results,"With the help of copy mechanism , NQG + has a 2.05 BLEU improvement since it solves the rare words problem .","With the help of
copy mechanism
NQG +
2.05 BLEU improvement",,"copy mechanism||has||NQG +
NQG +||has||2.05 BLEU improvement
","Results||With the help of||copy mechanism
",,,,,,
results,"The extended version , NQG ++ , has 1.11 BLEU score gain over NQG + , which shows that initializing with pre-trained word vectors and sharing them between encoder and decoder help learn better word representation .","NQG ++
1.11 BLEU score gain
over
NQG +","1.11 BLEU score gain||over||NQG +
","NQG ++||has||1.11 BLEU score gain
",,"Results||has||NQG ++
",,,,,
ablation-analysis,"The answer position indicator , as expected , plays a crucial role in answer focused question generation as shown in the NQG ?","answer position indicator
plays
crucial role
in
answer focused question generation","answer position indicator||plays||crucial role
crucial role||in||answer focused question generation
",,,"Ablation analysis||has||answer position indicator
",,,,,
ablation-analysis,"NER , show that word case , POS and NER tag features contributes to question generation .","show that
word case , POS and NER tag features
contributes to
question generation","word case , POS and NER tag features||contributes to||question generation
",,"Ablation analysis||show that||word case , POS and NER tag features
",,,,,,
research-problem,Multimodal Differential Network for Visual Question Generation,Visual Question Generation,,,,,"Contribution||has research problem||Visual Question Generation
",,,,
research-problem,Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .,Generating natural questions from an image,,,,,"Contribution||has research problem||Generating natural questions from an image
",,,,
research-problem,Here the au-thors have proposed the challenging task of generating natural questions for an image .,generating natural questions for an image,,,,,"Contribution||has research problem||generating natural questions for an image
",,,,
approach,"To solve this problem , we use the context obtained by considering exemplars , specifically we use the difference between relevant and irrelevant exemplars .","use
context
by considering
exemplars
difference
between
relevant and irrelevant exemplars","context||by considering||exemplars
difference||between||relevant and irrelevant exemplars
",,"Approach||use||context
Approach||use||difference
",,,,,,
approach,"We consider different contexts in the form of Location , Caption , and Part of Speech tags .","consider
different contexts
in the form of
Location , Caption , and Part of Speech tags","different contexts||in the form of||Location , Caption , and Part of Speech tags
",,"Approach||consider||different contexts
",,,,,,
approach,Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding .,"uses
differential context
obtained through
supporting and contrasting exemplars
to obtain
differentiable embedding","differential context||obtained through||supporting and contrasting exemplars
supporting and contrasting exemplars||to obtain||differentiable embedding
",,"Approach||uses||differential context
",,,,,,
approach,This embedding is used by a question decoder to decode the appropriate question .,"used by
question decoder
to decode
appropriate question","question decoder||to decode||appropriate question
",,,,,"differentiable embedding||used by||question decoder
",,,
approach,"To summarize , we propose a multimodal differential network to solve the task of visual question generation .","propose
multimodal differential network
to solve
task
of
visual question generation","multimodal differential network||to solve||task
task||of||visual question generation
",,"Approach||propose||multimodal differential network
",,,,,,
research-problem,Tha3aroon at NSURL - 2019 Task 8 : Semantic Question Similarity in Arabic,Semantic Question Similarity in Arabic,,,,,"Contribution||has research problem||Semantic Question Similarity in Arabic
",,,,
research-problem,"In this paper , we describe our team 's effort on the semantic text question similarity task of NSURL 2019 .",semantic text question similarity,,,,,"Contribution||has research problem||semantic text question similarity
",,,,
research-problem,Semantic Text Similarity ( STS ) problems are both real - life and challenging .,Semantic Text Similarity ( STS ),,,,,"Contribution||has research problem||Semantic Text Similarity ( STS )
",,,,
research-problem,"For example , in the paraphrase identification task , STS is used to predict if one sentence is a paraphrase of the other or not .",STS,,,,,"Contribution||has research problem||STS
",,,,
research-problem,A new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity ( SQS ) for the Arabic language .,Semantic Question Similarity ( SQS ) for the Arabic language,,,,,"Contribution||has research problem||Semantic Question Similarity ( SQS ) for the Arabic language
",,,,
research-problem,"SQS is a variant of STS , which aims to compare a pair of questions and determine whether they have the same meaning or not .",SQS,,,,,"Contribution||has research problem||SQS
",,,,
model,We then build a neural network model with four components .,"build
neural network model
with
four components","neural network model||with||four components
",,"Model||build||neural network model
",,,,,,
model,The model uses ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings as an input and builds sequence representation vectors that are used to predict the relation between the question pairs .,"uses
ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings
as
input
builds
sequence representation vectors
to predict
relation between the question pairs","sequence representation vectors||to predict||relation between the question pairs
ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings||as||input
",,"Model||builds||sequence representation vectors
Model||uses||ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings
",,,,,,
experimental-setup,All experiments discussed in this work have been done on the Google Colab 7 environment using Tesla T4 GPU accelerator with the following hyperparameters :,"done on
Google Colab 7 environment
using
Tesla T4 GPU accelerator","Google Colab 7 environment||using||Tesla T4 GPU accelerator
",,,,,"Experimental Setup||done on||Google Colab 7 environment
",,,
results,"The tables show that while GRU cells are the most efficient , the ON - LSTM cells ( with chunk size 8 ) are the most effective ( in terms of all considered measures ) .","GRU cells
are
most efficient
ON - LSTM cells
with
chunk size 8
are
most effective
in terms of
all considered measures","GRU cells||are||most efficient
ON - LSTM cells||with||chunk size 8
ON - LSTM cells||are||most effective
most effective||in terms of||all considered measures
",,,"Results||has||GRU cells
Results||has||ON - LSTM cells
",,,,,
results,Effect of Data Augmentation,Effect of Data Augmentation,,,,"Results||has||Effect of Data Augmentation
",,,,,
results,The tables show that each augmentation step affects the model 's efficiency negatively .,"show
each augmentation step
affects
model 's efficiency
negatively","each augmentation step||affects||model 's efficiency
","model 's efficiency||has||negatively
",,,,"Effect of Data Augmentation||show||each augmentation step
",,,
results,"On the other hand , not each increment step has a positive effect on the model 's effectiveness .","not
each increment step
positive effect
on
model 's effectiveness","positive effect||on||model 's effectiveness
","each increment step||has||positive effect
",,,,"Effect of Data Augmentation||not||each increment step
",,,
results,"For example , using pre-trained FastText embeddings as an input to our model yields worse F1score on both public and private leaderboards with 94.254 and 93.118 , respectively , compared with the ELMo contextual embeddings model .","using
pre-trained FastText embeddings
as
input
to
our model
yields
worse F1score
on
public and private leaderboards
with
94.254 and 93.118
compared with
ELMo contextual embeddings model","pre-trained FastText embeddings||as||input
input||to||our model
our model||yields||worse F1score
our model||compared with||ELMo contextual embeddings model
our model||on||public and private leaderboards
public and private leaderboards||with||94.254 and 93.118
",,"Results||using||pre-trained FastText embeddings
",,,,,,
results,"However , the sequence weighted attention gives better results by about 1 point of the F1-score .","sequence weighted attention
gives
better results
by
about 1 point
of
F1-score","sequence weighted attention||gives||better results
better results||by||about 1 point
about 1 point||of||F1-score
",,,"Results||has||sequence weighted attention
",,,,,
results,"Moreover , an attempt to overcome the weakness of the Arabic ELMo model is done by translating the data to English using Google Translate 8 and treating the problem as an English SQS problem instead , but the results are much worse with 88.868 and 87.504 F1 - scores on public and private leaderboards , respectively .","to
overcome
weakness
of
Arabic ELMo model
by translating
data
English
using
Google Translate
treating
problem
as
English SQS problem
results are
much worse
with
88.868 and 87.504 F1 - scores
on
public and private leaderboards","weakness||of||Arabic ELMo model
Arabic ELMo model||by translating||data
data||to||English
English||using||Google Translate
Arabic ELMo model||results are||much worse
much worse||with||88.868 and 87.504 F1 - scores
88.868 and 87.504 F1 - scores||on||public and private leaderboards
Arabic ELMo model||treating||problem
problem||as||English SQS problem
",,"Results||overcome||weakness
",,,,,,
research-problem,End - to - End Relation Extraction using LSTMs on Sequences and Tree Structures,End - to - End Relation Extraction,,,,,"Contribution||has research problem||End - to - End Relation Extraction
",,,,
research-problem,We present a novel end - to - end neural model to extract entities and relations between them .,end - to - end neural model to extract entities and relations between them,,,,,"Contribution||has research problem||end - to - end neural model to extract entities and relations between them
",,,,
research-problem,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,jointly represent both entities and relations with shared parameters in a single model,,,,,"Contribution||has research problem||jointly represent both entities and relations with shared parameters in a single model
",,,,
research-problem,Extracting semantic relations between entities in text is an important and well - studied task in information extraction and natural language processing ( NLP ) .,Extracting semantic relations between entities in text,,,,,"Contribution||has research problem||Extracting semantic relations between entities in text
",,,,
research-problem,"Traditional systems treat this task as a pipeline of two separated tasks , i.e. , named entity recognition ( NER ) ) and relation extraction , but recent studies show that end - to - end ( joint ) modeling of entity and relation is important for high performance since relations interact closely with entity information .",end - to - end ( joint ) modeling of entity and relation,,,,,"Contribution||has research problem||end - to - end ( joint ) modeling of entity and relation
",,,,
model,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,"present
novel end - to - end model
to extract
relations between entities
on
word sequence
dependency tree structures","novel end - to - end model||to extract||relations between entities
relations between entities||on||word sequence
relations between entities||on||dependency tree structures
",,"Model||present||novel end - to - end model
",,,,,,
model,Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,"allows
joint modeling of entities and relations
using
bidirectional sequential ( left - to - right and right - to - left )
bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs","joint modeling of entities and relations||using||bidirectional sequential ( left - to - right and right - to - left )
joint modeling of entities and relations||using||bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs
",,"Model||allows||joint modeling of entities and relations
",,,,,,
model,"Our model first detects entities and then extracts relations between the detected entities using a single incrementally - decoded NN structure , and the NN parameters are jointly updated using both entity and relation labels .","detects
entities
then extracts
relations
between
detected entities
using
single incrementally - decoded NN structure
NN parameters
jointly updated
using
both entity and relation labels","entities||then extracts||relations
relations||using||single incrementally - decoded NN structure
single incrementally - decoded NN structure||jointly updated||NN parameters
NN parameters||using||both entity and relation labels
relations||between||detected entities
",,"Model||detects||entities
",,,,,,
model,"Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .","incorporates
two enhancements
entity pretraining
scheduled sampling",,"two enhancements||name||entity pretraining
two enhancements||name||scheduled sampling
","Model||incorporates||two enhancements
",,,,,,
model,"These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .","alleviate
low - performance entity detection in early stages of training
allow
entity information to further help downstream relation classification",,,,,,"two enhancements||allow||entity information to further help downstream relation classification
two enhancements||alleviate||low - performance entity detection in early stages of training
",,,
,"We evaluate on three datasets : ACE05 and ACE04 for end - to - end relation extraction , and SemEval - 2010 Task 8 for relation classification .",,,,,,,,,,
experimental-setup,We implemented our model using the cnn library .,"implemented
our model
using
cnn library","our model||using||cnn library
",,"Experimental setup||implemented||our model
",,,,,,
experimental-setup,"We parsed the texts using the Stanford neural dependency parser 7 ( Chen and Manning , 2014 ) with the original Stanford Dependencies .","parsed
texts
using
Stanford neural dependency parser 7 ( Chen and Manning , 2014 )
with
original Stanford Dependencies","texts||using||Stanford neural dependency parser 7 ( Chen and Manning , 2014 )
Stanford neural dependency parser 7 ( Chen and Manning , 2014 )||with||original Stanford Dependencies
",,"Experimental setup||parsed||texts
",,,,,,
experimental-setup,"Based on preliminary tuning , we fixed embedding dimensions n w to 200 , n p , n d , n e to 25 , and dimensions of intermediate layers ( n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers ) to 100 .","fixed
embedding dimensions
n w
200
n p , n d , n e
25
dimensions of intermediate layers
n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers
100","dimensions of intermediate layers||n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers||100
embedding dimensions||n p , n d , n e||25
embedding dimensions||n w||200
",,"Experimental setup||fixed||dimensions of intermediate layers
Experimental setup||fixed||embedding dimensions
",,,,,,
experimental-setup,We initialized word vectors via word2 vec trained on Wikipedia 8 and randomly initialized all other parameters .,"initialized
word vectors
via
word2 vec","word vectors||via||word2 vec
",,"Experimental setup||initialized||word vectors
",,,,,,
results,"Table 1 compares our model with the state - of - theart feature - based model of on final test sets , and shows that our model performs better than the state - of - the - art model .","our model
performs better than
state - of - the - art model","our model||performs better than||state - of - the - art model
",,,"Results||has||our model
",,,,,
ablation-analysis,"To analyze the contributions and effects of the various components of our end - to - end relation extraction model , we perform ablation tests on the ACE05 development set ( ) .","on
ACE05 development set",,,"Ablation analysis||on||ACE05 development set
",,,,,,
ablation-analysis,"The performance slightly degraded without scheduled sampling , and the performance significantly degraded when we removed entity pretraining or removed both ( p < 0.05 ) .","performance slightly degraded
without scheduled sampling
performance significantly degraded
removed entity pretraining",,,"Ablation analysis||performance slightly degraded||without scheduled sampling
Ablation analysis||performance significantly degraded||removed entity pretraining
",,,,,,
ablation-analysis,"When we removed all the enhancements , i.e. , scheduled sampling , entity pretraining , label embedding , and shared parameters , the performance is significantly worse than SP - Tree ( p < 0.01 ) , showing that these enhancements provide complementary benefits to end - to - end relation extraction .","removed all the enhancements
scheduled sampling
entity pretraining
label embedding
shared parameters
performance is significantly worse
than SP - Tree ( p < 0.01 )",,"removed all the enhancements||name||scheduled sampling
removed all the enhancements||name||entity pretraining
removed all the enhancements||name||label embedding
removed all the enhancements||name||shared parameters
","Ablation analysis||performance is significantly worse||than SP - Tree ( p < 0.01 )
Ablation analysis||performance is significantly worse||removed all the enhancements
",,,,,,
research-problem,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,Joint Extraction of Entities and Relations,,,,,"Contribution||has research problem||Joint Extraction of Entities and Relations
",,,,
research-problem,"Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text , as shows .",detect entity mentions and recognize their semantic relations simultaneously from unstructured text,,,,,"Contribution||has research problem||detect entity mentions and recognize their semantic relations simultaneously from unstructured text
",,,,
research-problem,"Different from the pipelined methods , joint learning framework is to extract entities together with relations using a single model .",extract entities together with relations using a single model,,,,,"Contribution||has research problem||extract entities together with relations using a single model
",,,,
approach,We propose a novel tagging scheme and an end -toend model with biased objective function to jointly extract entities and their relations .,"propose
novel tagging scheme
end -toend model
with
biased objective function
to jointly extract
entities and their relations","end -toend model||with||biased objective function
biased objective function||to jointly extract||entities and their relations
",,"Approach||propose||novel tagging scheme
Approach||propose||end -toend model
",,,,,,
approach,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .","Tag "" O ""
represents
"" Other "" tag
means
corresponding word is independent of the extracted results","Tag "" O ""||represents||"" Other "" tag
"" Other "" tag||means||corresponding word is independent of the extracted results
",,,"Approach||has||Tag "" O ""
",,,,,
approach,"We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .","use
BIES
Begin , Inside , End , Single
represent
position information
of a
word
in
entity","BIES||represent||position information
position information||of a||word
word||in||entity
","BIES||name||Begin , Inside , End , Single
","Approach||use||BIES
",,,,,,
approach,The End - to - end Model,End - to - end Model,,,,,,,,,
approach,"In this paper , we investigate an end - toend model to produce the tags sequence as shows .","produce
tags sequence",,,,,,"End - to - end Model||produce||tags sequence
",,,
approach,It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .,"contains
bi-directional Long Short Term Memory ( Bi - LSTM ) layer
to encode
input sentence
LSTM - based decoding layer
with
biased loss","LSTM - based decoding layer||with||biased loss
bi-directional Long Short Term Memory ( Bi - LSTM ) layer||to encode||input sentence
",,,,,"End - to - end Model||contains||LSTM - based decoding layer
End - to - end Model||contains||bi-directional Long Short Term Memory ( Bi - LSTM ) layer
",,,
approach,The biased loss can enhance the relevance of entity tags .,"enhance
relevance of entity tags",,,,,,"biased loss||enhance||relevance of entity tags
",,,
hyperparameters,The word embeddings used in the encoding part are initialed by running word2vec 3 on NYT training corpus .,"word embeddings
used in
encoding part
initialed by
running word2vec 3
on
NYT training corpus","word embeddings||used in||encoding part
encoding part||initialed by||running word2vec 3
running word2vec 3||on||NYT training corpus
",,,"Hyperparameters||has||word embeddings
",,,,,
hyperparameters,The dimension of the word embeddings is d = 300 .,"dimension
d = 300",,,,,,"word embeddings||dimension||d = 300
",,,
hyperparameters,We regularize our network using dropout on embedding layer and the dropout ratio is 0.5 .,"regularize
network
using
dropout
on
embedding layer
ratio is
0.5","network||using||dropout
dropout||ratio is||0.5
dropout||on||embedding layer
",,"Hyperparameters||regularize||network
",,,,,,
hyperparameters,The number of lstm units in encoding layer is 300 and the number in decoding layer is 600 .,"number of
lstm units
in encoding layer
300
in decoding layer
600","lstm units||in decoding layer||600
lstm units||in encoding layer||300
",,"Hyperparameters||number of||lstm units
",,,,,,
hyperparameters,The bias parameter ?,bias parameter ?,,,,,,,,,"Hyperparameters||bias parameter ?||10
"
hyperparameters,corresponding to the results in is 10 .,10,,,,,,,,,
baselines,"We compare our method with several classical triplet extraction methods , which can be divided into the following categories : the pipelined methods , the jointly extracting methods and the end - to - end methods based our tagging scheme .","divided into
pipelined methods
jointly extracting methods
end - to - end methods",,,"Baselines||divided into||pipelined methods
Baselines||divided into||jointly extracting methods
Baselines||divided into||end - to - end methods
",,,,,,
baselines,"For the pipelined methods , we follow ) 's settings :",pipelined methods,,,,"Baselines||has||pipelined methods
",,,,,"pipelined methods||has||FCM
pipelined methods||has||DS-logistic
pipelined methods||has||LINE
"
baselines,The NER results are obtained by CoType then several classical relation classification methods are applied to detect the relations .,,,,,,,,,,
baselines,These methods are :,,,,,,,,,,
baselines,"( 1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ;","DS-logistic
is a
distant supervised and feature based method
combines
advantages of supervised IE and unsupervised IE features
LINE
is a
network embedding method
suitable for
arbitrary types of information networks","DS-logistic||is a||distant supervised and feature based method
DS-logistic||combines||advantages of supervised IE and unsupervised IE features
LINE||suitable for||arbitrary types of information networks
LINE||is a||network embedding method
",,,,,,,,
baselines,( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .,"FCM
is a
compositional model
combines
lexicalized linguistic context and word embeddings
for
relation extraction","FCM||is a||compositional model
FCM||combines||lexicalized linguistic context and word embeddings
lexicalized linguistic context and word embeddings||for||relation extraction
",,,,,,,,
baselines,"The jointly extracting methods used in this paper are listed as follows : ( 4 ) DS - Joint ) is a supervised method , which jointly extracts entities and relations using structured perceptron on human - annotated dataset ; ( 5 ) MultiR is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data ; ( 6 ) CoType ) is a domain independent framework by jointly embedding entity mentions , relation mentions , text features and type labels into meaningful representations .","jointly extracting methods
DS - Joint
is a
supervised method
jointly extracts
entities and relations
using
structured perceptron
on
human - annotated dataset
MultiR
is a
typical distant supervised method
based on
multi-instance learning algorithms
to combat
noisy training data
CoType
is a
domain independent framework
by jointly embedding
entity mentions , relation mentions , text features and type labels into meaningful representations","MultiR||is a||typical distant supervised method
typical distant supervised method||based on||multi-instance learning algorithms
multi-instance learning algorithms||to combat||noisy training data
DS - Joint||is a||supervised method
DS - Joint||jointly extracts||entities and relations
entities and relations||using||structured perceptron
structured perceptron||on||human - annotated dataset
CoType||by jointly embedding||entity mentions , relation mentions , text features and type labels into meaningful representations
CoType||is a||domain independent framework
","jointly extracting methods||has||MultiR
jointly extracting methods||has||DS - Joint
jointly extracting methods||has||CoType
",,"Baselines||has||jointly extracting methods
",,,,,
baselines,"In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .","end - to - end tagging models
LSTM- CRF
LSTM - LSTM",,"end - to - end tagging models||has||LSTM- CRF
end - to - end tagging models||has||LSTM - LSTM
",,"Baselines||has||end - to - end tagging models
",,,,,
baselines,LSTM - CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence .,"proposed for
entity recognition
by using
bidirectional LSTM
to encode
input sentence
conditional random fields
to predict
entity tag sequence","entity recognition||by using||conditional random fields
conditional random fields||to predict||entity tag sequence
entity recognition||by using||bidirectional LSTM
bidirectional LSTM||to encode||input sentence
",,,,,"LSTM- CRF||proposed for||entity recognition
",,,
baselines,"Different from LSTM - CRF , LSTM - LSTM uses a LSTM layer to decode the tag sequence instead of CRF .","uses
LSTM layer
to decode
tag sequence instead of CRF","LSTM layer||to decode||tag sequence instead of CRF
",,,,,"LSTM - LSTM||uses||LSTM layer
",,,
results,"It can be seen that our method , LSTM - LSTM - Bias , outperforms all other methods in F 1 score and achieves a 3 % improvement in F 1 over the best method CoType .","LSTM - LSTM - Bias
outperforms
all other methods
in
F 1 score
achieves
3 % improvement
in
F 1
over
best method CoType","LSTM - LSTM - Bias||achieves||3 % improvement
3 % improvement||in||F 1
F 1||over||best method CoType
LSTM - LSTM - Bias||outperforms||all other methods
all other methods||in||F 1 score
",,,"Results||has||LSTM - LSTM - Bias
",,,,,
results,"When compared with the traditional methods , the precisions of the end - to - end models are significantly improved .","compared with
traditional methods
precisions
of
end - to - end models
significantly improved","traditional methods||significantly improved||precisions
precisions||of||end - to - end models
",,"Results||compared with||traditional methods
",,,,,,
results,We also find that the LSTM - LSTM model is better than LSTM - CRF model based on our tagging scheme .,"LSTM - LSTM model
is better than
LSTM - CRF model
based on
our tagging scheme","LSTM - LSTM model||is better than||LSTM - CRF model
LSTM - CRF model||based on||our tagging scheme
",,,"Results||has||LSTM - LSTM model
",,,,,
research-problem,Joint entity recognition and relation extraction as a multi-head selection problem,Joint entity recognition and relation extraction,,,,,"Contribution||has research problem||Joint entity recognition and relation extraction
",,,,
research-problem,"In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool .",entity recognition and relation extraction simultaneously,,,,,"Contribution||has research problem||entity recognition and relation extraction simultaneously
",,,,
research-problem,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .",joint models to detect entities and their relations,,,,,"Contribution||has research problem||joint models to detect entities and their relations
",,,,
research-problem,"In this work , we focus on a new general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously , and that can handle multiple relations together .",joint model that performs the two tasks of entity recognition and relation extraction simultaneously,,,,,"Contribution||has research problem||joint model that performs the two tasks of entity recognition and relation extraction simultaneously
",,,,
model,"In this section , we present our multi-head joint model illustrated in .","present
multi-head joint model",,,"Model||present||multi-head joint model
",,,,,,
model,"The input of our model is a sequence of tokens ( i.e. , words of the sentence ) which are then represented as word vectors ( i.e. , word embeddings ) .","input
sequence of tokens ( i.e. , words of the sentence )
represented as
word vectors ( i.e. , word embeddings )","sequence of tokens ( i.e. , words of the sentence )||represented as||word vectors ( i.e. , word embeddings )
",,"Model||input||sequence of tokens ( i.e. , words of the sentence )
",,,,,,
model,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .","outputs for
each token
are twofold
entity recognition label
set of tuples
comprising
head tokens of the entity and the types of relations between them","each token||are twofold||entity recognition label
each token||are twofold||set of tuples
set of tuples||comprising||head tokens of the entity and the types of relations between them
",,"Model||outputs for||each token
",,,,,,
experimental-setup,We have developed our joint model by using Python with the TensorFlow machine learning library .,"developed
our joint model
by using
Python
with
TensorFlow machine learning library","our joint model||with||TensorFlow machine learning library
our joint model||by using||Python
",,"Experimental setup||developed||our joint model
",,,,,,
experimental-setup,"Training is performed using the Adam optimizer ( Kingma & Ba , 2015 ) with a learning rate of 10 ?3 .","Training
performed using
Adam optimizer ( Kingma & Ba , 2015 )
with
learning rate
of
10 ?3","Training||performed using||Adam optimizer ( Kingma & Ba , 2015 )
Adam optimizer ( Kingma & Ba , 2015 )||with||learning rate
learning rate||of||10 ?3
",,,"Experimental setup||has||Training
",,,,,
experimental-setup,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,"fix
size of the LSTM
to
d = 64
layer width
of
neural network
to
l = 64","size of the LSTM||to||d = 64
layer width||of||neural network
neural network||to||l = 64
",,"Experimental setup||fix||size of the LSTM
Experimental setup||fix||layer width
",,,,,,
experimental-setup,We use dropout to regularize our network .,"use
dropout
to regularize
our network","dropout||to regularize||our network
",,"Experimental setup||use||dropout
",,,,,,
experimental-setup,The hidden dimension for the characterbased LSTMs is 25 ( for each direction ) .,"hidden dimension
for
characterbased LSTMs
is
25
for
each direction","hidden dimension||for||characterbased LSTMs
characterbased LSTMs||is||25
25||for||each direction
",,,"Experimental setup||has||hidden dimension
",,,,,
experimental-setup,We employ the technique of early stopping based on the validation set .,"employ
early stopping
based on
validation set","early stopping||based on||validation set
",,"Experimental setup||employ||early stopping
",,,,,,
experimental-setup,"In all the datasets examined in this study , we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset .","obtain
best hyperparameters
after
60 to 200 epochs
depending on
size of the dataset","best hyperparameters||after||60 to 200 epochs
60 to 200 epochs||depending on||size of the dataset
",,"Experimental setup||obtain||best hyperparameters
",,,,,,
results,We observe that our model outperforms all previous models that do not rely on complex hand - crafted features by a large margin ( > 4 % for both tasks ) .,"observe
our model
outperforms
all previous models
that do not rely on
complex hand - crafted features
by a large margin
> 4 % for both tasks","our model||outperforms||all previous models
all previous models||that do not rely on||complex hand - crafted features
all previous models||by a large margin||> 4 % for both tasks
",,"Results||observe||our model
",,,,,,
results,"We also report results for the DREC dataset , with two different evaluation settings .","for
DREC dataset",,,"Results||for||DREC dataset
",,,,,,"DREC dataset||has||boundaries evaluation
"
results,"Specifically , we use the boundaries and the strict settings .","use
boundaries and the strict settings",,,,,,"DREC dataset||use||boundaries and the strict settings
",,,
results,"In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .","boundaries evaluation
achieve
? 3 % improvement
for
both tasks","boundaries evaluation||achieve||? 3 % improvement
? 3 % improvement||for||both tasks
",,,,,,,,
ablation-analysis,We conduct ablation tests on the ACE04 dataset reported in to analyze the effectiveness of the various parts of our joint model .,"on
ACE04 dataset
to analyze
effectiveness of the various parts of our joint model","ACE04 dataset||to analyze||effectiveness of the various parts of our joint model
",,"Ablation analysis||on||ACE04 dataset
",,,,,,
ablation-analysis,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,"performance of
RE task
decreases
when
remove the label embeddings layer
only use the LSTM hidden states
as
inputs","decreases||when||remove the label embeddings layer
decreases||when||only use the LSTM hidden states
only use the LSTM hidden states||as||inputs
","RE task||has||decreases
","Ablation analysis||performance of||RE task
",,,,,,
ablation-analysis,Removing character embeddings also degrades the performance of both NER ( ? 1 % ) and RE ( ? 2 % ) tasks by a relatively large margin .,"Removing
character embeddings
also degrades the performance of
both NER ( ? 1 % ) and RE ( ? 2 % ) tasks
by
relatively large margin","character embeddings||also degrades the performance of||both NER ( ? 1 % ) and RE ( ? 2 % ) tasks
both NER ( ? 1 % ) and RE ( ? 2 % ) tasks||by||relatively large margin
",,"Ablation analysis||Removing||character embeddings
",,,,,,
ablation-analysis,"Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .","for
NER task
by removing
CRF loss layer
substituting it with
softmax","NER task||by removing||CRF loss layer
NER task||substituting it with||softmax
",,"Ablation analysis||for||NER task
",,,,,,
ablation-analysis,"Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .","leads to
a slight decrease
in
F 1 performance
of
NER module
? 2 % decrease
in
performance
of
RE task","? 2 % decrease||in||performance
performance||of||RE task
a slight decrease||in||F 1 performance
F 1 performance||of||NER module
",,,,,"softmax||leads to||? 2 % decrease
softmax||leads to||a slight decrease
",,,
research-problem,Adversarial training for multi-context joint entity and relation extraction,multi-context joint entity and relation extraction,,,,,"Contribution||has research problem||multi-context joint entity and relation extraction
",,,,
research-problem,We show how to use AT for the tasks of entity recognition and relation extraction .,entity recognition and relation extraction,,,,,"Contribution||has research problem||entity recognition and relation extraction
",,,,
research-problem,"In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",jointly extracting entities and relations,,,,,"Contribution||has research problem||jointly extracting entities and relations
",,,,
model,"The baseline model , described in detail in , is illustrated in .",baseline model,,,,"Model||has||baseline model
",,,,,
model,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,"aims to detect
type and the boundaries of the entities
relations between them",,,,,,"baseline model||aims to detect||type and the boundaries of the entities
baseline model||aims to detect||relations between them
",,,
model,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .","input
is a
sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n","input||is a||sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n
",,,"Model||has||input
",,,,,
model,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .","use
character level embeddings
to implicitly capture
morphological features ( e.g. , prefixes and suffixes )
representing each
character by a vector ( embedding )","character level embeddings||to implicitly capture||morphological features ( e.g. , prefixes and suffixes )
morphological features ( e.g. , prefixes and suffixes )||representing each||character by a vector ( embedding )
",,"Model||use||character level embeddings
",,,,,,
model,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,"character embeddings
fed to
bidirectional LSTM ( BiLSTM )
to obtain
character - based representation of the word","character embeddings||fed to||bidirectional LSTM ( BiLSTM )
bidirectional LSTM ( BiLSTM )||to obtain||character - based representation of the word
",,,"Model||has||character embeddings
",,,,,
model,We also use pre-trained word embeddings .,pre-trained word embeddings,,,,"Model||use||pre-trained word embeddings
",,,,,
model,"Word and character embeddings are concatenated to form the final token representation , which is then fed to a BiLSTM layer to extract sequential information .","Word and character embeddings
concatenated to form
final token representation
then fed to
BiLSTM layer
to extract
sequential information","Word and character embeddings||concatenated to form||final token representation
final token representation||then fed to||BiLSTM layer
BiLSTM layer||to extract||sequential information
",,,"Model||has||Word and character embeddings
",,,,,
model,"For the NER task , we adopt the BIO ( Beginning , Inside , Outside ) encoding scheme .","For
NER task
adopt
BIO ( Beginning , Inside , Outside ) encoding scheme","NER task||adopt||BIO ( Beginning , Inside , Outside ) encoding scheme
",,"Model||For||NER task
",,,,,,
model,We model the relation extraction task as a multi-label head selection problem .,"model
relation extraction task
as a
multi-label head selection problem","relation extraction task||as a||multi-label head selection problem
",,"Model||model||relation extraction task
",,,,,,
model,Adversarial training ( AT ),Adversarial training ( AT ),,,,"Model||has||Adversarial training ( AT )
",,,,,
model,We exploit the idea of AT as a regularization method to make our model robust to input perturbations .,"exploit
idea of AT
as a
regularization method
to make
our model robust to input perturbations","idea of AT||as a||regularization method
regularization method||to make||our model robust to input perturbations
",,,,,"Adversarial training ( AT )||exploit||idea of AT
",,,
experimental-setup,"We evaluate our models on four datasets , using the code as available from our github codebase .","evaluate
our models
on
four datasets","our models||on||four datasets
",,"Experimental setup||evaluate||our models
",,,,,,
experimental-setup,We also evaluate our models on the NER task similar to in the same dataset using 10 - fold cross validation .,"NER task
using
10 - fold cross validation","NER task||using||10 - fold cross validation
",,,,,,,"our models||on||NER task
",
experimental-setup,We employ early stopping in all of the experiments .,"employ
early stopping",,,"Experimental setup||employ||early stopping
",,,,,,
experimental-setup,"We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .","use
Adam optimizer
fix
hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate )
on
validation sets","hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate )||on||validation sets
",,"Experimental setup||fix||hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate )
Experimental setup||use||Adam optimizer
",,,,,,
experimental-setup,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .","three types of evaluation
namely
S( trict )
score
an entity
as
correct
if
both the entity boundaries and the entity type are correct
B ( oundaries )
score
an entity
as
correct
if
only the entity boundaries are correct while the entity type is not taken into account
R( elaxed )
multi-token entity
considered correct if
at least one correct type is assigned to the tokens comprising the entity","three types of evaluation||namely||R( elaxed )
multi-token entity||considered correct if||at least one correct type is assigned to the tokens comprising the entity
three types of evaluation||namely||S( trict )
S( trict )||score||an entity
an entity||as||correct
correct||if||both the entity boundaries and the entity type are correct
three types of evaluation||namely||B ( oundaries )
B ( oundaries )||score||an entity
an entity||as||correct
correct||if||only the entity boundaries are correct while the entity type is not taken into account
","R( elaxed )||has||multi-token entity
",,"Experimental setup||use||three types of evaluation
",,,,,
results,"For ACE04 , the baseline outperforms by ? 2 % in both tasks .","For
ACE04
baseline
outperforms by
? 2 %
in
both tasks","baseline||outperforms by||? 2 %
baseline||in||both tasks
","ACE04||has||baseline
","Results||For||ACE04
",,,,,,
results,"For the CoNLL04 dataset , we use two evaluation settings .",CoNLL04 dataset,,,,"Results||For||CoNLL04 dataset
",,,,,"CoNLL04 dataset||has||baseline model
"
results,"The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .","baseline model
outperforms
state - of - the - art models that do not rely on manually extracted features
> 4 % improvement
for
both tasks","baseline model||outperforms||state - of - the - art models that do not rely on manually extracted features
> 4 % improvement||for||both tasks
","state - of - the - art models that do not rely on manually extracted features||has||> 4 % improvement
",,,,,,,
results,"For the DREC dataset , we use two evaluation methods .",DREC dataset,,,,"Results||For||DREC dataset
",,,,,
results,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .","In
boundaries evaluation
baseline
has an improvement of
? 3 %
on
both tasks","baseline||has an improvement of||? 3 %
? 3 %||on||both tasks
","boundaries evaluation||has||baseline
",,,,"DREC dataset||In||boundaries evaluation
",,,
results,and show the effectiveness of the adversarial training on top of the baseline model .,"show
adversarial training on top of the baseline model",,,"Results||show||adversarial training on top of the baseline model
",,,,,,
results,"In all of the experiments , AT improves the predictive performance of the baseline model in the joint setting .","In
all of the experiments
AT
improves
predictive performance
of
baseline model
in
joint setting","AT||improves||predictive performance
predictive performance||of||baseline model
baseline model||in||joint setting
","all of the experiments||has||AT
","Results||In||all of the experiments
",,,,,,
results,"Specifically , for ACE04 , there is an improvement in both tasks as well as in the over all F 1 performance ( 0.4 % ) .","for
ACE04
improvement in
both tasks
over all F 1 performance ( 0.4 % )","ACE04||improvement in||both tasks
ACE04||improvement in||over all F 1 performance ( 0.4 % )
",,,,,"adversarial training on top of the baseline model||for||ACE04
",,,
results,"For CoNLL04 , we note an improvement in the over all F 1 of 0.4 % for the EC and 0.8 % for the NER tasks , respectively .","For
CoNLL04
note an improvement
over all F 1 of 0.4 %
for
EC
0.8 %
for
NER tasks","CoNLL04||note an improvement||over all F 1 of 0.4 %
over all F 1 of 0.4 %||for||EC
CoNLL04||note an improvement||0.8 %
0.8 %||for||NER tasks
",,,,,"adversarial training on top of the baseline model||For||CoNLL04
",,,
results,"For the DREC dataset , in both settings , there is an over all improvement of ? 1 % .","DREC dataset
there is
over all improvement of ? 1 %","DREC dataset||there is||over all improvement of ? 1 %
",,,,,,,"adversarial training on top of the baseline model||For||DREC dataset
",
results,"Finally , for ADE , our AT model beats the baseline F 1 by 0.7 % .","ADE
AT model
beats
baseline F 1
by
0.7 %","AT model||beats||baseline F 1
baseline F 1||by||0.7 %
","ADE||has||AT model
",,,,,,"adversarial training on top of the baseline model||for||ADE
",
research-problem,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,Relation Extraction,,,,,"Contribution||has research problem||Relation Extraction
",,,,
research-problem,Dependency trees help relation extraction models capture long - range relations between words .,capture long - range relations between words,,,,,"Contribution||has research problem||capture long - range relations between words
",,,,
research-problem,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",capture long - range syntactic relations,,,,,"Contribution||has research problem||capture long - range syntactic relations
",,,,
model,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .","encodes
dependency structure
over
input sentence
with
efficient graph convolution operations
then extracts
entity - centric representations
to make
robust relation predictions","dependency structure||over||input sentence
input sentence||with||efficient graph convolution operations
efficient graph convolution operations||then extracts||entity - centric representations
entity - centric representations||to make||robust relation predictions
",,"Model||encodes||dependency structure
",,,,,,
model,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .","apply
novel path - centric pruning technique
to remove
irrelevant information from the tree
while maximally keeping
relevant content","novel path - centric pruning technique||to remove||irrelevant information from the tree
irrelevant information from the tree||while maximally keeping||relevant content
",,"Model||apply||novel path - centric pruning technique
",,,,,,
baselines,Dependency - based models .,Dependency - based models,,,,"Baselines||has||Dependency - based models
",,,,,"Dependency - based models||has||logistic regression ( LR ) classifier
"
baselines,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,"logistic regression ( LR ) classifier
combines
dependencybased features
with
other lexical features","logistic regression ( LR ) classifier||combines||dependencybased features
dependencybased features||with||other lexical features
",,,,,,,,
baselines,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .","Shortest Dependency Path LSTM ( SDP - LSTM )
applies
neural sequence model
on
shortest path
between
subject and object entities
in
dependency tree","Shortest Dependency Path LSTM ( SDP - LSTM )||applies||neural sequence model
neural sequence model||on||shortest path
shortest path||between||subject and object entities
subject and object entities||in||dependency tree
",,,,,,,"Dependency - based models||has||Shortest Dependency Path LSTM ( SDP - LSTM )
",
baselines,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .","Tree - LSTM
is a
recursive model
that generalizes
LSTM to arbitrary tree structures","Tree - LSTM||is a||recursive model
recursive model||that generalizes||LSTM to arbitrary tree structures
",,,,,,,"Dependency - based models||has||Tree - LSTM
",
baselines,Neural sequence model .,Neural sequence model,,,,"Baselines||has||Neural sequence model
",,,,,
baselines,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .","presented
competitive sequence model
that employs
position - aware attention mechanism
over
LSTM outputs ( PA - LSTM )
it outperforms
several CNN and dependency - based models
by
substantial margin","competitive sequence model||it outperforms||several CNN and dependency - based models
several CNN and dependency - based models||by||substantial margin
competitive sequence model||that employs||position - aware attention mechanism
position - aware attention mechanism||over||LSTM outputs ( PA - LSTM )
",,,,,"Neural sequence model||presented||competitive sequence model
",,,
results,Results on the TACRED Dataset,"on
TACRED Dataset",,,"Results||on||TACRED Dataset
",,,,,,
results,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,"observe
our GCN model
outperforms
all dependency - based models
by
at least 1.6 F 1","our GCN model||outperforms||all dependency - based models
all dependency - based models||by||at least 1.6 F 1
",,,,,"TACRED Dataset||observe||our GCN model
",,,
results,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .","using
contextualized word representations
C - GCN model
further outperforms
strong PA - LSTM model
by
1.3 F 1","C - GCN model||further outperforms||strong PA - LSTM model
strong PA - LSTM model||by||1.3 F 1
","contextualized word representations||has||C - GCN model
",,,,"TACRED Dataset||using||contextualized word representations
",,,
results,"In addition , we find our model improves upon other dependencybased models in both precision and recall .","improves
upon other dependencybased models
in both
precision and recall","upon other dependencybased models||in both||precision and recall
",,,,,"TACRED Dataset||improves||upon other dependencybased models
",,,
results,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .","Comparing
C - GCN model
with
GCN model
find that
gain
mainly comes from
improved recall","C - GCN model||with||GCN model
GCN model||find that||gain
gain||mainly comes from||improved recall
",,,,,"TACRED Dataset||Comparing||C - GCN model
",,,
results,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .","find that
our GCN models
have
complementary strengths
when compared to
PA - LSTM","our GCN models||have||complementary strengths
complementary strengths||when compared to||PA - LSTM
",,,,,"TACRED Dataset||find that||our GCN models
",,,
results,Results on the SemEval Dataset,SemEval Dataset,,,,"Results||on||SemEval Dataset
",,,,,
results,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .","under
conventional with- entity evaluation
C - GCN model
outperforms
all existing dependency - based neural models","C - GCN model||outperforms||all existing dependency - based neural models
","conventional with- entity evaluation||has||C - GCN model
",,,,"SemEval Dataset||under||conventional with- entity evaluation
",,,
results,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .","by properly incorporating
off - path information
our model outperforms
previous shortest dependency path - based model ( SDP - LSTM )","previous shortest dependency path - based model ( SDP - LSTM )||by properly incorporating||off - path information
",,,,,"SemEval Dataset||our model outperforms||previous shortest dependency path - based model ( SDP - LSTM )
",,,
results,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .","Under
mask - entity evaluation
C - GCN model
also outperforms
PA - LSTM
by
substantial margin","C - GCN model||also outperforms||PA - LSTM
PA - LSTM||by||substantial margin
","mask - entity evaluation||has||C - GCN model
",,,,"SemEval Dataset||Under||mask - entity evaluation
",,,
results,Effect of Path - centric Pruning,Effect of Path - centric Pruning,,,,"Results||has||Effect of Path - centric Pruning
",,,,,
results,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .","compare
two GCN models
Tree - LSTM
when
pruning distance K
is
varied","Tree - LSTM||when||pruning distance K
pruning distance K||is||varied
",,,,,"Effect of Path - centric Pruning||compare||two GCN models
Effect of Path - centric Pruning||compare||Tree - LSTM
",,,
results,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .","performance of
all three models
peaks when
K = 1
outperforming
respective dependency path - based counterpart ( K = 0 )","all three models||peaks when||K = 1
K = 1||outperforming||respective dependency path - based counterpart ( K = 0 )
",,,,,"Effect of Path - centric Pruning||performance of||all three models
",,,
results,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .","find
all three models
are
less effective
when
entire dependency tree
is
present","all three models||are||less effective
less effective||when||entire dependency tree
entire dependency tree||is||present
",,,,,"Effect of Path - centric Pruning||find||all three models
",,,
results,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .","contextualizing
GCN
makes it
less sensitive
to changes in
tree structures provided","GCN||makes it||less sensitive
less sensitive||to changes in||tree structures provided
",,,,,"Effect of Path - centric Pruning||contextualizing||GCN
",,,
ablation-analysis,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .","on
TACRED dev set",,,"Ablation analysis||on||TACRED dev set
",,,,,,
ablation-analysis,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,"find
entity representations and feedforward layers
contribute
1.0 F 1","entity representations and feedforward layers||contribute||1.0 F 1
",,,,,"TACRED dev set||find||entity representations and feedforward layers
",,,
ablation-analysis,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .","remove
dependency structure
score drops by
3.2 F 1","dependency structure||score drops by||3.2 F 1
",,,,,"TACRED dev set||remove||dependency structure
",,,
ablation-analysis,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .","F 1 drops by
10.3
feedforward layers , the LSTM component and the dependency structure altogether","feedforward layers , the LSTM component and the dependency structure altogether||F 1 drops by||10.3
",,,,,,,"TACRED dev set||remove||feedforward layers , the LSTM component and the dependency structure altogether
",
ablation-analysis,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .","Removing
pruning
using
full trees as input
hurts the result by
another 9.7 F 1","pruning||using||full trees as input
pruning||hurts the result by||another 9.7 F 1
",,,,,"TACRED dev set||Removing||pruning
",,,
research-problem,End - to - end neural relation extraction using deep biaffine attention,End - to - end neural relation extraction,,,,,"Contribution||has research problem||End - to - end neural relation extraction
",,,,
research-problem,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .",joint extraction of named entities and relations between them,,,,,"Contribution||has research problem||joint extraction of named entities and relations between them
",,,,
research-problem,Extracting entities and their semantic relations from raw text is a key information extraction task .,Extracting entities and their semantic relations from raw text,,,,,"Contribution||has research problem||Extracting entities and their semantic relations from raw text
",,,,
research-problem,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",jointly learn to extract entities and relations,,,,,"Contribution||has research problem||jointly learn to extract entities and relations
",,,,
model,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .","present
novel end - to - end neural model
for
joint entity and relation extraction","novel end - to - end neural model||for||joint entity and relation extraction
",,"Model||present||novel end - to - end neural model
",,,,,,
model,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .","mixture of
named entity recognition ( NER ) component
relation classification ( RC ) component",,,"Model||mixture of||named entity recognition ( NER ) component
Model||mixture of||relation classification ( RC ) component
",,,,,,
model,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,"NER component
employs
BiLSTM - CRF architecture
to predict
entities
from
input word tokens","NER component||employs||BiLSTM - CRF architecture
BiLSTM - CRF architecture||to predict||entities
entities||from||input word tokens
",,,"Model||has||NER component
",,,,,
model,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .","RC component
uses
another BiLSTM
to learn
latent features relevant for relation classification","RC component||uses||another BiLSTM
another BiLSTM||to learn||latent features relevant for relation classification
",,,"Model||has||RC component
",,,,,
model,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .","takes into account
second - order interactions
over
latent features
via
tensor","second - order interactions||over||latent features
latent features||via||tensor
",,,,,"RC component||takes into account||second - order interactions
",,,
model,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .","for
relation classification
propose a novel use of
deep biaffine attention mechanism","relation classification||propose a novel use of||deep biaffine attention mechanism
",,"Model||for||relation classification
",,,,,,
experimental-setup,Our model is implemented using DYNET v 2.0 .,"using
DYNET v 2.0",,,"Experimental setup||using||DYNET v 2.0
",,,,,,
experimental-setup,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .","optimize
objective loss
using
Adam","objective loss||using||Adam
",,"Experimental setup||optimize||objective loss
",,,,,,
code,Our code is available at : https : //github.com/datquocnguyen/jointRE,https : //github.com/datquocnguyen/jointRE,,,,,"Contribution||Code||https : //github.com/datquocnguyen/jointRE
",,,,
ablation-analysis,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .","provide
results of a pipeline approach
where we treat
NER and RC components as independent networks
train them
separately","results of a pipeline approach||where we treat||NER and RC components as independent networks
NER and RC components as independent networks||train them||separately
",,"Ablation analysis||provide||results of a pipeline approach
",,,,,,
ablation-analysis,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .","find that
joint approach does slightly better
than
pipeline approach
in
relation classification","joint approach does slightly better||than||pipeline approach
pipeline approach||in||relation classification
",,"Ablation analysis||find that||joint approach does slightly better
",,,,,,
research-problem,Semantic Relation Classification via Bidirectional LSTM Networks with Entity - aware Attention using Latent Entity Typing,Semantic Relation Classification,,,,,"Contribution||has research problem||Semantic Relation Classification
",,,,
research-problem,Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) .,Classifying semantic relations between entity pairs in sentences,,,,,"Contribution||has research problem||Classifying semantic relations between entity pairs in sentences
",,,,
research-problem,A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,predicting a semantic relationship between two tagged entities in a sentence,,,,,"Contribution||has research problem||predicting a semantic relationship between two tagged entities in a sentence
",,,,
model,"In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail .","introduce
novel recurrent neural model
incorporate
entity - aware attention mechanism
with
LET method","novel recurrent neural model||incorporate||entity - aware attention mechanism
entity - aware attention mechanism||with||LET method
",,"Model||introduce||novel recurrent neural model
",,,,,,
model,"As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET .","consists of
four main components
Word Representation
maps
each word in a sentence
into
vector representations
Self Attention
captures
meaning of the correlation between words
based on
multi-head attention
BLSTM
sequentially encodes
representations of self attention layer
Entity - aware Attention
calculates
attention weights
with respect to
entity pairs , word positions relative to these pairs , and their latent types obtained by LET","BLSTM||sequentially encodes||representations of self attention layer
Self Attention||captures||meaning of the correlation between words
meaning of the correlation between words||based on||multi-head attention
Entity - aware Attention||calculates||attention weights
attention weights||with respect to||entity pairs , word positions relative to these pairs , and their latent types obtained by LET
Word Representation||maps||each word in a sentence
each word in a sentence||into||vector representations
","four main components||has||BLSTM
four main components||has||Self Attention
four main components||has||Entity - aware Attention
four main components||has||Word Representation
","Model||consists of||four main components
",,,,,,
results,"Non Our proposed model achieves an F1-score of 85.2 % which outperforms all competing state - of - theart approaches except depLCNN + NS , DRNNs , and Attention - CNN .","proposed model
achieves
F1-score
of
85.2 %
outperforms
all competing state - of - theart approaches
except
depLCNN + NS
DRNNs
Attention - CNN","proposed model||achieves||F1-score
F1-score||of||85.2 %
proposed model||outperforms||all competing state - of - theart approaches
all competing state - of - theart approaches||except||depLCNN + NS
all competing state - of - theart approaches||except||DRNNs
all competing state - of - theart approaches||except||Attention - CNN
",,,"Results||has||proposed model
",,,,,
research-problem,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,pre-trained biomedical language representation,,,,,"Contribution||has research problem||pre-trained biomedical language representation
",,,,
research-problem,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",extracting valuable information from biomedical literature,,,,,"Contribution||has research problem||extracting valuable information from biomedical literature
",,,,
research-problem,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora,,,,,"Contribution||has research problem||biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora
",,,,
research-problem,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",pre-trained language model BERT can be adapted for biomedical corpora,,,,,"Contribution||has research problem||pre-trained language model BERT can be adapted for biomedical corpora
",,,,
code,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",https://github. com/naver/biobert-pretrained,,,,,"Contribution||Code||https://github. com/naver/biobert-pretrained
",,,,
research-problem,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",word distributions of general and biomedical corpora are quite different,,,,,"Contribution||has research problem||word distributions of general and biomedical corpora are quite different
",,,,
approach,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .","introduce
BioBERT
is
pre-trained language representation model
for
biomedical domain","BioBERT||is||pre-trained language representation model
pre-trained language representation model||for||biomedical domain
",,"Approach||introduce||BioBERT
",,,,,,
approach,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .","initialize
BioBERT
with
weights
from
BERT
pretrained on
general domain corpora ( English Wikipedia and Books Corpus )","BioBERT||with||weights
weights||from||BERT
BERT||pretrained on||general domain corpora ( English Wikipedia and Books Corpus )
",,"Approach||initialize||BioBERT
",,,,,,
approach,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .","BioBERT
pre-trained on
biomedical domain corpora ( PubMed abstracts and PMC full - text articles )","BioBERT||pre-trained on||biomedical domain corpora ( PubMed abstracts and PMC full - text articles )
",,,"Approach||has||BioBERT
",,,,,
approach,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .","fine - tuned and evaluated on
three popular biomedical text mining tasks",,,,,,"BioBERT||fine - tuned and evaluated on||three popular biomedical text mining tasks
",,,
experimental-setup,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,"used
BERT BASE model
pre-trained on
English Wikipedia and Books Corpus
for
1 M steps","BERT BASE model||pre-trained on||English Wikipedia and Books Corpus
English Wikipedia and Books Corpus||for||1 M steps
",,"Experimental setup||used||BERT BASE model
",,,,,,
experimental-setup,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,"BioBERT v1.0 ( PubMed PMC )
version of
BioBERT ( PubMed PMC )
trained for
470 K steps","BioBERT v1.0 ( PubMed PMC )||version of||BioBERT ( PubMed PMC )
BioBERT ( PubMed PMC )||trained for||470 K steps
",,,"Experimental setup||has||BioBERT v1.0 ( PubMed PMC )
",,,,,
experimental-setup,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,"eight NVIDIA V100 ( 32GB ) GPUs
for
pre-training","eight NVIDIA V100 ( 32GB ) GPUs||for||pre-training
",,,"Experimental setup||used||eight NVIDIA V100 ( 32GB ) GPUs
",,,,,
experimental-setup,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .","maximum sequence length
fixed to
512
mini-batch size
set to
192","mini-batch size||set to||192
maximum sequence length||fixed to||512
",,,"Experimental setup||has||mini-batch size
Experimental setup||has||maximum sequence length
",,,,,
experimental-setup,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,"single NVIDIA Titan Xp ( 12GB ) GPU
to fine - tune
BioBERT
on
each task","single NVIDIA Titan Xp ( 12GB ) GPU||to fine - tune||BioBERT
BioBERT||on||each task
",,,"Experimental setup||used||single NVIDIA Titan Xp ( 12GB ) GPU
",,,,,
results,The results of NER are shown in .,"of
NER",,,"Results||of||NER
",,,,,,"NER||has||BioBERT
"
results,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .","BioBERT
achieves
higher scores
than
BERT","BioBERT||achieves||higher scores
higher scores||than||BERT
",,,,,,,,
results,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .","outperformed
state - of - the - art models
on
six out of nine datasets
BioBERT v 1.1 ( PubMed )
outperformed
state - of - the - art models
by
0.62
in terms of
micro averaged F1 score","BioBERT v 1.1 ( PubMed )||outperformed||state - of - the - art models
state - of - the - art models||by||0.62
0.62||in terms of||micro averaged F1 score
state - of - the - art models||on||six out of nine datasets
",,,,,"BioBERT||outperformed||state - of - the - art models
",,"NER||has||BioBERT v 1.1 ( PubMed )
",
results,The RE results of each model are shown in .,RE,,,,"Results||of||RE
",,,,,"RE||has||BioBERT v1.0 ( PubMed )
"
results,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .","BioBERT v1.0 ( PubMed )
obtained
higher F1 score ( 2.80 higher )
than
state - of - the - art models","BioBERT v1.0 ( PubMed )||obtained||higher F1 score ( 2.80 higher )
higher F1 score ( 2.80 higher )||than||state - of - the - art models
",,,,,,,,
results,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .","BioBERT
achieved
highest F 1 scores
on
2 out of 3 biomedical datasets","BioBERT||achieved||highest F 1 scores
highest F 1 scores||on||2 out of 3 biomedical datasets
",,,,,,,"RE||has||BioBERT
",
results,The QA results are shown in .,QA,,,,"Results||of||QA
",,,,,"QA||has||BioBERT v1.1 ( PubMed )
QA||has||BioBERT
"
results,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .","of
BioBERT
significantly outperformed
BERT
state - of - the - art models
BioBERT v1.1 ( PubMed )
obtained
strict accuracy
of
38.77
lenient accuracy
of
53.81
mean reciprocal rank score
44.77","BioBERT v1.1 ( PubMed )||obtained||mean reciprocal rank score
mean reciprocal rank score||of||44.77
BioBERT v1.1 ( PubMed )||obtained||lenient accuracy
lenient accuracy||of||53.81
BioBERT v1.1 ( PubMed )||obtained||strict accuracy
strict accuracy||of||38.77
BioBERT||significantly outperformed||BERT
BioBERT||significantly outperformed||state - of - the - art models
",,,,,,,,
results,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .","biomedical QA datasets
BioBERT
achieved
new state - of - the - art performance
in terms of
MRR","BioBERT||achieved||new state - of - the - art performance
new state - of - the - art performance||in terms of||MRR
","biomedical QA datasets||has||BioBERT
",,,,,,"QA||has||biomedical QA datasets
",
research-problem,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,Extracting Multiple - Relations in One - Pass,,,,,"Contribution||has research problem||Extracting Multiple - Relations in One - Pass
",,,,
research-problem,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,extracting multiple entity - relations from an input paragraph,,,,,"Contribution||has research problem||extracting multiple entity - relations from an input paragraph
",,,,
research-problem,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",multiple entityrelations extraction task with only one - pass,,,,,"Contribution||has research problem||multiple entityrelations extraction task with only one - pass
",,,,
research-problem,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,"Relation extraction
RE",,,,,"Contribution||has research problem||Relation extraction
Contribution||has research problem||RE
",,,,
research-problem,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,"multiplerelations extraction
MRE
recognize relations of multiple pairs of entity mentions from an input paragraph",,,,,"Contribution||has research problem||multiplerelations extraction
Contribution||has research problem||MRE
Contribution||has research problem||recognize relations of multiple pairs of entity mentions from an input paragraph
",,,,
approach,This section describes the proposed one - pass encoding MRE solution .,"proposed
one - pass encoding MRE solution",,,"Approach||proposed||one - pass encoding MRE solution
",,,,,,
approach,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .","built upon
BERT
with
structured prediction layer
to predict
multiple relations
with
onepass encoding
entity - aware self - attention mechanism
to infuse
relational information with regard to multiple entities
at each layer of
hidden states","BERT||with||structured prediction layer
structured prediction layer||to predict||multiple relations
multiple relations||with||onepass encoding
entity - aware self - attention mechanism||to infuse||relational information with regard to multiple entities
relational information with regard to multiple entities||at each layer of||hidden states
",,"Approach||built upon||BERT
Approach||built upon||entity - aware self - attention mechanism
",,,,,,
baselines,"We compare our solution with previous works that predict a single relation per pass , our model that predicts single relation per pass for MRE , and with the following naive modifications of BERT that could achieve MRE in one - pass .","previous works
that predict
single relation per pass","previous works||that predict||single relation per pass
",,,"Baselines||has||previous works
",,,,,
baselines,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .","BERT SP
BERT
with
structured prediction only","BERT||with||structured prediction only
","BERT SP||has||BERT
",,"Baselines||has||BERT SP
",,,,,
baselines,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .","Entity - Aware BERT SP
our full model",,"Entity - Aware BERT SP||has||our full model
",,"Baselines||has||Entity - Aware BERT SP
",,,,,
baselines,BERT SP with position embedding on the final attention layer .,BERT SP with position embedding on the final attention layer,,,,"Baselines||has||BERT SP with position embedding on the final attention layer
",,,,,
baselines,"In this method , the BERT model encode the paragraph to the last attention - layer .","encode
paragraph
to
last attention - layer","paragraph||to||last attention - layer
",,,,,"BERT SP with position embedding on the final attention layer||encode||paragraph
",,,
baselines,"Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .","for
each entity pair
takes
hidden states
adds
relative position embeddings
corresponding to
target entities
makes
relation prediction","each entity pair||makes||relation prediction
each entity pair||takes||hidden states
each entity pair||adds||relative position embeddings
relative position embeddings||corresponding to||target entities
",,,,,"BERT SP with position embedding on the final attention layer||for||each entity pair
",,,
results,Results on ACE 2005,"on
ACE 2005",,,"Results||on||ACE 2005
",,,,,,
results,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,"first observation
our model architecture
achieves
much better results
compared to
previous state - of - the - art methods","our model architecture||achieves||much better results
much better results||compared to||previous state - of - the - art methods
",,,,,"ACE 2005||first observation||our model architecture
",,,
results,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .","Our full model
with
structured fine - tuning of attention layers
brings
further improvement
of
about 5.5 %
in
MRE one - pass setting","Our full model||with||structured fine - tuning of attention layers
structured fine - tuning of attention layers||brings||further improvement
further improvement||in||MRE one - pass setting
further improvement||of||about 5.5 %
",,,,,,,"ACE 2005||has||Our full model
",
results,The results on SemEval 2018 Task 7 are shown in .,SemEval 2018 Task 7,,,,"Results||on||SemEval 2018 Task 7
",,,,,"SemEval 2018 Task 7||has||Our Entity - Aware BERT SP
"
results,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .","Our Entity - Aware BERT SP
gives comparable results to
top - ranked system in the shared task
with
slightly lower Macro - F1","Our Entity - Aware BERT SP||gives comparable results to||top - ranked system in the shared task
top - ranked system in the shared task||with||slightly lower Macro - F1
",,,,,,,,
results,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .","When predicting multiple relations in one - pass
0.9 % drop
on
Macro - F1","0.9 % drop||on||Macro - F1
",,,,,"Our Entity - Aware BERT SP||When predicting multiple relations in one - pass||0.9 % drop
",,,
results,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .","compared to
top singlemodel result
which makes use of
additional word and entity embeddings
pretrained on
in - domain data
our methods
demonstrate
clear advantage as a
single model","top singlemodel result||demonstrate||our methods
our methods||clear advantage as a||single model
top singlemodel result||which makes use of||additional word and entity embeddings
additional word and entity embeddings||pretrained on||in - domain data
",,,,,"SemEval 2018 Task 7||compared to||top singlemodel result
",,,
research-problem,SCIBERT : A Pretrained Language Model for Scientific Text,Pretrained Language Model,,,,,"Contribution||has research problem||Pretrained Language Model
",,,,
research-problem,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,Obtaining large - scale annotated data for NLP tasks in the scientific domain,,,,,"Contribution||has research problem||Obtaining large - scale annotated data for NLP tasks in the scientific domain
",,,,
research-problem,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .","pretrained language model based on BERT ( Devlin et al. , 2019 )
address the lack of high - quality , large - scale labeled scientific data",,,,,"Contribution||has research problem||pretrained language model based on BERT ( Devlin et al. , 2019 )
Contribution||has research problem||address the lack of high - quality , large - scale labeled scientific data
",,,,
code,The code and pretrained models are available at https://github.com/allenai/scibert/.,,,,,,,,,,
research-problem,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",annotated data is difficult and expensive to collect due to the expertise required for quality annotation,,,,,"Contribution||has research problem||annotated data is difficult and expensive to collect due to the expertise required for quality annotation
",,,,
approach,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,"SCIB - ERT
follows
same architecture as BERT
pretrained on
scientific text","SCIB - ERT||follows||same architecture as BERT
same architecture as BERT||pretrained on||scientific text
",,,"Approach||name||SCIB - ERT
",,,,,
approach,"We construct SCIVOCAB , a new WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .","construct
SCIVOCAB
a new WordPiece vocabulary
on
our scientific corpus
using
Sen - tencePiece 1 library","a new WordPiece vocabulary||on||our scientific corpus
our scientific corpus||using||Sen - tencePiece 1 library
","SCIVOCAB||has||a new WordPiece vocabulary
","Approach||construct||SCIVOCAB
",,,,,,
approach,Corpus,Corpus,,,,"Approach||has||Corpus
",,,,,
approach,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,"train
SCIBERT
on
random sample
of
1.14 M papers
from
Semantic Scholar","SCIBERT||on||random sample
random sample||of||1.14 M papers
1.14 M papers||from||Semantic Scholar
",,,,,"Corpus||train||SCIBERT
",,,
approach,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,"consists
18 % papers
from
computer science domain
82 %
from
broad biomedical domain","82 %||from||broad biomedical domain
18 % papers||from||computer science domain
",,,,,"Corpus||consists||82 %
Corpus||consists||18 % papers
",,,
tasks,Named Entity Recognition ( NER ),"Named Entity Recognition
NER",,"Named Entity Recognition||name||NER
",,"Tasks||has||Named Entity Recognition
",,,,,
tasks,2 . PICO Extraction ( PICO ),"PICO Extraction
PICO",,"PICO Extraction||name||PICO
",,"Tasks||has||PICO Extraction
",,,,,
tasks,3 . Text Classification ( CLS ),"Text Classification
CLS",,"Text Classification||name||CLS
",,"Tasks||has||Text Classification
",,,,,
tasks,4 . Relation Classification ( REL ),"Relation Classification
REL",,"Relation Classification||name||REL
",,"Tasks||has||Relation Classification
",,,,,
,5 . Dependency Parsing ( DEP ) ,"Dependency Parsing
DEP",,,,,,,,,
results,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),"observe
SCIBERT
outperforms
BERT - Base
on
scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without )","SCIBERT||outperforms||BERT - Base
SCIBERT||outperforms||BERT - Base
SCIBERT||outperforms||BERT - Base
SCIBERT||outperforms||BERT - Base
BERT - Base||on||scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without )
",,"Results||observe||SCIBERT
",,,,,,
results,Biomedical Domain,Biomedical Domain,,,,"Results||has||Biomedical Domain
",,,,,
results,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,"observe
SCIBERT
outperforms
BERT - Base
on
biomedical tasks
+ 1.92 F1
with finetuning
+ 3.59 F1
without","BERT - Base||on||biomedical tasks
biomedical tasks||with finetuning||+ 1.92 F1
biomedical tasks||without||+ 3.59 F1
",,,,,"Biomedical Domain||observe||SCIBERT
",,,
results,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .","SCIB - ERT
achieves
new SOTA results
on
BC5 CDR and ChemProt
EBM - NLP","SCIB - ERT||achieves||new SOTA results
new SOTA results||on||BC5 CDR and ChemProt
new SOTA results||on||EBM - NLP
",,,,,,,"Biomedical Domain||has||SCIB - ERT
",
results,Computer Science Domain,Computer Science Domain,,,,"Results||has||Computer Science Domain
",,,,,
results,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,"observe
SCIBERT
outperforms
BERT - Base
on
computer science tasks
+ 3.55 F1
with finetuning
+ 1.13 F1
without","BERT - Base||on||computer science tasks
computer science tasks||with finetuning||+ 3.55 F1
computer science tasks||without||+ 1.13 F1
",,,,,"Computer Science Domain||observe||SCIBERT
",,,
results,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .","SCIBERT
achieves
new SOTA results
on
ACL - ARC
NER part of SciERC","SCIBERT||achieves||new SOTA results
new SOTA results||on||ACL - ARC
new SOTA results||on||NER part of SciERC
",,,,,,,"Computer Science Domain||has||SCIBERT
",
results,Multiple Domains,Multiple Domains,,,,"Results||has||Multiple Domains
",,,,,
results,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,"observe
SCIBERT
outperforms
BERT - Base
on
multidomain tasks
+ 0.49 F1
with finetuning
+ 0.93 F1
without","BERT - Base||on||multidomain tasks
multidomain tasks||with finetuning||+ 0.49 F1
multidomain tasks||without||+ 0.93 F1
",,,,,"Multiple Domains||observe||SCIBERT
",,,
results,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .","SCIBERT
outperforms
SOTA
on
Sci - Cite","SCIBERT||outperforms||SOTA
SOTA||on||Sci - Cite
",,,,,,,"Multiple Domains||has||SCIBERT
",
research-problem,Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees,Joint Extraction of Entity Mentions and Relations,,,,,"Contribution||has research problem||Joint Extraction of Entity Mentions and Relations
",,,,
research-problem,Extraction of entities and their relations from text belongs to a very well - studied family of structured prediction tasks in NLP .,Extraction of entities and their relations from text,,,,,"Contribution||has research problem||Extraction of entities and their relations from text
",,,,
research-problem,Several methods have been proposed for entity mention and relation extraction at the sentencelevel .,entity mention and relation extraction at the sentencelevel,,,,,"Contribution||has research problem||entity mention and relation extraction at the sentencelevel
",,,,
model,"In this paper , we propose a novel RNN - based model for the joint extraction of entity mentions and relations .","propose
novel RNN - based model
for
joint extraction of entity mentions and relations","novel RNN - based model||for||joint extraction of entity mentions and relations
",,"Model||propose||novel RNN - based model
",,,,,,
model,"Unlike other models , our model does not depend on any dependency tree information .","does not
depend
on
any dependency tree information","depend||on||any dependency tree information
",,"Model||does not||depend
",,,,,,
model,Our RNN - based model is a multi - layer bidirectional LSTM over a sequence .,"Our RNN - based model
is
multi - layer bidirectional LSTM
over
sequence","Our RNN - based model||is||multi - layer bidirectional LSTM
multi - layer bidirectional LSTM||over||sequence
",,,"Model||has||Our RNN - based model
",,,,,
model,We encode the output sequence from left - to - right .,"encode
output sequence
from
left - to - right","output sequence||from||left - to - right
",,"Model||encode||output sequence
",,,,,,
model,"At each time step , we use an attention - like model on the previously decoded time steps , to identify the tokens in a specified relation with the current token .","At
each time step
use
attention - like model
on
previously decoded time steps
to identify
tokens
in
specified relation
with
current token","each time step||use||attention - like model
attention - like model||to identify||tokens
tokens||in||specified relation
specified relation||with||current token
attention - like model||on||previously decoded time steps
",,"Model||At||each time step
",,,,,,
model,We also add an additional layer to our network to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,"add
additional layer
to
our network
to encode
output sequence
from
right - to - left","additional layer||to encode||output sequence
output sequence||from||right - to - left
additional layer||to||our network
",,"Model||add||additional layer
",,,,,,
baselines,The model proposed by is a feature - based structured perceptron model with efficient beam - search .,"feature - based structured perceptron model
with
efficient beam - search","feature - based structured perceptron model||with||efficient beam - search
",,,"Baselines||has||feature - based structured perceptron model
",,,,,
baselines,They employ a segment - based decoder instead of token - based decoding .,"employ
segment - based decoder
instead of
token - based decoding","segment - based decoder||instead of||token - based decoding
",,,,,"feature - based structured perceptron model||employ||segment - based decoder
",,,
baselines,"( SPTree ) recently proposed a LSTM - based model with a sequence layer for entity identification , and a tree - based dependency layer which identifies relations between pairs of candidate entities using the shortest dependency path between them .","SPTree
proposed
LSTM - based model
with
sequence layer
for
entity identification
tree - based dependency layer
which identifies
relations between pairs of candidate entities","SPTree||proposed||LSTM - based model
LSTM - based model||with||sequence layer
sequence layer||for||entity identification
LSTM - based model||with||tree - based dependency layer
tree - based dependency layer||which identifies||relations between pairs of candidate entities
",,,"Baselines||has||SPTree
",,,,,
baselines,We also employed our previous approach for extraction of opinion entities and relations to this task .,"employed
our previous approach
for
extraction
of
opinion entities and relations
to
this task","our previous approach||for||extraction
extraction||of||opinion entities and relations
opinion entities and relations||to||this task
",,"Baselines||employed||our previous approach
",,,,,,
hyperparameters,We train our model using Adadelta with gradient clipping .,"train
our model
using
Adadelta
with
gradient clipping","our model||using||Adadelta
Adadelta||with||gradient clipping
",,"Hyperparameters||train||our model
",,,,,,
hyperparameters,We regularize our network using dropout with the drop - out rate tuned using development set .,"regularize
our network
using
dropout
with
drop - out rate
tuned using
development set","our network||using||dropout
dropout||with||drop - out rate
drop - out rate||tuned using||development set
",,"Hyperparameters||regularize||our network
",,,,,,
hyperparameters,We have 3 hidden layers in our network and the dimensionality of the hidden units is 100 .,"have
3 hidden layers
in
our network
dimensionality
of
hidden units
is
100","dimensionality||of||hidden units
hidden units||is||100
3 hidden layers||in||our network
",,"Hyperparameters||have||dimensionality
Hyperparameters||have||3 hidden layers
",,,,,,
hyperparameters,All the weights in the network are initialized from small random uniform noise .,"weights
in
network
initialized from
random uniform noise","weights||in||network
network||initialized from||random uniform noise
",,,"Hyperparameters||has||weights
",,,,,
hyperparameters,We tune our hyperparameters based on ACE05 development set and use them for training on ACE04 dataset .,"tune
hyperparameters
based on
ACE05 development set
use them for
training
on
ACE04 dataset","hyperparameters||use them for||training
training||on||ACE04 dataset
hyperparameters||based on||ACE05 development set
",,"Hyperparameters||tune||hyperparameters
",,,,,,
results,Multiple Relations,,,,,,,,,,
results,"We find that modifying our objective to include multiple relations improves the recall of our system on relations , leading to slight improvement on the over all performance on relations .","find that
modifying our objective
to include
multiple relations
improves
recall
of
our system
on
relations
leading to
slight improvement
on
over all performance on relations","modifying our objective||to include||multiple relations
multiple relations||improves||recall
recall||leading to||slight improvement
slight improvement||on||over all performance on relations
recall||of||our system
recall||on||relations
",,,,,"Multiple Relations||find that||modifying our objective
",,,
results,"By adding bidirectional encoding to our system , we find that we can significantly improve the performance of our system compared to left - to - right encoding .","adding
bidirectional encoding
to
our system
find
significantly improve
performance
of
our system
compared to
left - to - right encoding","bidirectional encoding||find||significantly improve
performance||of||our system
performance||compared to||left - to - right encoding
bidirectional encoding||to||our system
","significantly improve||has||performance
","Results||adding||bidirectional encoding
",,,,,,
results,It also improves precision compared to left - toright decoding combined with multiple relations objective .,"improves
precision
compared to
left - toright decoding
combined with
multiple relations objective","precision||compared to||left - toright decoding
left - toright decoding||combined with||multiple relations objective
",,"Results||improves||precision
",,,,,,
results,We find that for some relations it is easier to detect them with respect to one of the entities in the entity pair .,"find that for
some relations
is
easier
to detect
with respect to
one of the entities
in
entity pair","some relations||is||easier
easier||to detect||with respect to
one of the entities||in||entity pair
","with respect to||has||one of the entities
","Results||find that for||some relations
",,,,,,
results,PHYS relation is easier identified with respect to GPE entity than PER entity .,"PHYS relation
is
easier identified
with respect to
GPE entity
than
PER entity","PHYS relation||is||easier identified
easier identified||with respect to||GPE entity
GPE entity||than||PER entity
",,,"Results||has||PHYS relation
",,,,,
research-problem,Simple BERT Models for Relation Extraction and Semantic Role Labeling,"Relation Extraction
Semantic Role Labeling",,,,,"Contribution||has research problem||Relation Extraction
Contribution||has research problem||Semantic Role Labeling
",,,,
research-problem,Relation extraction and semantic role labeling ( SRL ) are two fundamental tasks in natural language understanding .,semantic role labeling ( SRL ),,,,,"Contribution||has research problem||semantic role labeling ( SRL )
",,,,
research-problem,"For SRL , the task is to extract the predicate - argument structure of a sentence , determining "" who did what to whom "" , "" when "" , "" where "" , etc .",SRL,,,,,"Contribution||has research problem||SRL
",,,,
model,We show that simple neural architectures built on top of BERT yields state - of - the - art performance on a variety of benchmark datasets for these two tasks .,"show
simple neural architectures
built on top of
BERT
yields
state - of - the - art performance
on
variety of benchmark datasets","simple neural architectures||built on top of||BERT
simple neural architectures||yields||state - of - the - art performance
state - of - the - art performance||on||variety of benchmark datasets
",,"Model||show||simple neural architectures
",,,,,,
hyperparameters,We conduct experiments on two SRL tasks : and the predicate indicator embedding size is 10 .,"predicate indicator embedding size
is
10","predicate indicator embedding size||is||10
",,,"Hyperparameters||has||predicate indicator embedding size
",,,,,
hyperparameters,The learning rate is 5 10 ?5 . BERT base - cased and large - cased models are used in our experiments .,"learning rate
5 10 ?5
BERT base - cased and large - cased models
used in
our experiments","BERT base - cased and large - cased models||used in||our experiments
","learning rate||has||5 10 ?5
",,"Hyperparameters||has||BERT base - cased and large - cased models
Hyperparameters||has||learning rate
",,,,,
hyperparameters,The position embeddings are randomly initialized and fine - tuned during the training process .,"position embeddings
are
randomly initialized and fine - tuned
during
training process","position embeddings||are||randomly initialized and fine - tuned
randomly initialized and fine - tuned||during||training process
",,,"Hyperparameters||has||position embeddings
",,,,,
results,We see that the BERT - LSTM - large model achieves the state - of - the - art F 1 score among single models and outperforms the ensemble model on the CoNLL 2005 in - domain and out - of - domain tests .,"see
BERT - LSTM - large model
achieves
state - of - the - art F 1 score
among
single models
outperforms
ensemble model
on
CoNLL 2005
in - domain and out - of - domain tests","BERT - LSTM - large model||achieves||state - of - the - art F 1 score
state - of - the - art F 1 score||among||single models
BERT - LSTM - large model||outperforms||ensemble model
ensemble model||on||CoNLL 2005
","ensemble model||has||in - domain and out - of - domain tests
","Results||see||BERT - LSTM - large model
",,,,,,
results,"However , it falls short on the CoNLL 2012 benchmark because the model of obtains very high precision .","falls short on
CoNLL 2012 benchmark",,,"Results||falls short on||CoNLL 2012 benchmark
",,,,,,
research-problem,Span - Level Model for Relation Extraction,Relation Extraction,,,,,"Contribution||has research problem||Relation Extraction
",,,,
research-problem,"This paper focuses on Relation Extraction ( RE ) , which is the task of entity mention detection and classifying the relations between each pair of those mentions .",Relation Extraction ( RE ),,,,,"Contribution||has research problem||Relation Extraction ( RE )
",,,,
research-problem,"Since , work on RE has revolved around end - to - end systems : single models which first perform entity mention detection and then relation extraction .",RE,,,,,"Contribution||has research problem||RE
",,,,
model,We propose a simple bi - LSTM based model which generates span representations for each possible span .,"propose
simple bi - LSTM based model
which generates
span representations
for
each possible span","simple bi - LSTM based model||which generates||span representations
span representations||for||each possible span
",,"Model||propose||simple bi - LSTM based model
",,,,,,
model,The span representations are used to perform entity mention detection on all spans in parallel .,"span representations
to perform
entity mention detection
on
all spans
in
parallel","span representations||to perform||entity mention detection
entity mention detection||on||all spans
all spans||in||parallel
",,,"Model||has||span representations
",,,,,
model,The same span representations are then used to perform relation extraction on all pairs of detected entity mentions .,"relation extraction
on
all pairs
of
detected entity mentions","relation extraction||on||all pairs
all pairs||of||detected entity mentions
",,,,,,,"span representations||to perform||relation extraction
",
hyperparameters,"The learned character embeddings are of size 8 . 1 - dimensional convolutions of window size 3 , 4,5 are applied per-token with 50 filters of each window size .","learned character embeddings
of size
8
1 - dimensional convolutions
of window size
3","learned character embeddings||of size||8
1 - dimensional convolutions||of window size||3
",,,"Hyperparameters||has||learned character embeddings
Hyperparameters||has||1 - dimensional convolutions
",,,,,
hyperparameters,Our stacked bi - LSTMs ( Section 3.1 ) has 3 layers with 200 - dimensional hidden states and highway connections .,"stacked bi - LSTMs
3 layers
with
200 - dimensional hidden states
highway connections","3 layers||with||200 - dimensional hidden states
3 layers||with||highway connections
","stacked bi - LSTMs||has||3 layers
",,"Hyperparameters||has||stacked bi - LSTMs
",,,,,
hyperparameters,"All Multi Layer Perceptrons ( MLP ) has two hidden layers with 500 dimensions , each followed by ReLU activation .","All Multi Layer Perceptrons ( MLP )
two hidden layers
with
500 dimensions
followed by
ReLU activation","two hidden layers||with||500 dimensions
two hidden layers||followed by||ReLU activation
","All Multi Layer Perceptrons ( MLP )||has||two hidden layers
",,"Hyperparameters||has||All Multi Layer Perceptrons ( MLP )
",,,,,
hyperparameters,We only consider spans that are entirely within a sentence and limit spans to a max length of L = 10 .,"consider
spans
entirely within
sentence
limit
spans
to
max length
of
L = 10","spans||to||max length
max length||of||L = 10
spans||entirely within||sentence
",,"Hyperparameters||limit||spans
Hyperparameters||consider||spans
",,,,,,
hyperparameters,"Regularization Dropout is applied with dropout rate 0.2 to all hidden layers of all MLPs and feature encodings , with dropout rate 0.5 to all word and character embeddings and with dropout rate 0.4 to all LSTM layer outputs .","Regularization Dropout
applied with
dropout rate 0.2
to
all hidden layers
of
all MLPs and feature encodings
dropout rate 0.5
to
all word and character embeddings
dropout rate 0.4
to
all LSTM layer outputs","Regularization Dropout||applied with||dropout rate 0.2
dropout rate 0.2||to||all hidden layers
all hidden layers||of||all MLPs and feature encodings
Regularization Dropout||applied with||dropout rate 0.4
dropout rate 0.4||to||all LSTM layer outputs
Regularization Dropout||applied with||dropout rate 0.5
dropout rate 0.5||to||all word and character embeddings
",,,"Hyperparameters||has||Regularization Dropout
",,,,,
hyperparameters,"Learning Learning is done with Adam ( Kingma and Ba , 2015 ) with default parameters .","Learning
done with
Adam ( Kingma and Ba , 2015 )
with
default parameters","Learning||done with||Adam ( Kingma and Ba , 2015 )
Adam ( Kingma and Ba , 2015 )||with||default parameters
",,,"Hyperparameters||has||Learning
",,,,,
hyperparameters,The learning rate is annealed by 1 % every 100 iterations .,"learning rate
annealed by
1 %
every
100 iterations","learning rate||annealed by||1 %
1 %||every||100 iterations
",,,"Hyperparameters||has||learning rate
",,,,,
hyperparameters,Minibatch Size is 1 .,"Minibatch Size
is
1","Minibatch Size||is||1
",,,"Hyperparameters||has||Minibatch Size
",,,,,
hyperparameters,Early Stopping of 20 evaluations on the dev set is used .,"Early Stopping
of
20 evaluations
on
dev set","Early Stopping||of||20 evaluations
20 evaluations||on||dev set
",,,"Hyperparameters||has||Early Stopping
",,,,,
results,"Our proposed model achieves a new SOTA on RE with a F 1 of 62. 83 , more than 2.3 F 1 above the previous SOTA .","Our proposed model
achieves
new SOTA
on
RE
with
F 1
of
62. 83","Our proposed model||achieves||new SOTA
new SOTA||with||F 1
F 1||of||62. 83
new SOTA||on||RE
",,,"Results||has||Our proposed model
",,,,,
results,Our proposed model also beats a multitask model which uses signals from additional tasks by more than 1.5 F 1 points .,"also beats
multitask model
which uses
signals
from
additional tasks
by more than
1.5 F 1 points","multitask model||by more than||1.5 F 1 points
multitask model||which uses||signals
signals||from||additional tasks
",,,,,"Our proposed model||also beats||multitask model
",,,
results,"For both tasks , our model 's Precision is close to and Recall is significantly higher than previous works .","For
both tasks
our model 's Precision
close to
Recall
significantly higher
than
previous works","both tasks||than||previous works
previous works||close to||our model 's Precision
previous works||significantly higher||Recall
",,"Results||For||both tasks
",,,,,,
results,The Recall gains for RE ( 4.3 absolute points ) are much higher than for EMD ( 0.6 absolute points ) .,"Recall gains
for
RE
4.3 absolute points
much higher than for
EMD
0.6 absolute points","Recall gains||much higher than for||EMD
Recall gains||for||RE
","EMD||has||0.6 absolute points
RE||has||4.3 absolute points
",,"Results||has||Recall gains
",,,,,
results,"Thus , our large gains in RE Recall ( and F 1 ) showcase the effectiveness of our simple modeling of ordered span pairs for relation extraction ( Section 3.3 ) .","our large gains
in
RE Recall ( and F 1 )
showcase
effectiveness
of
our simple modeling of ordered span pairs
for
relation extraction","our large gains||in||RE Recall ( and F 1 )
RE Recall ( and F 1 )||showcase||effectiveness
effectiveness||of||our simple modeling of ordered span pairs
our simple modeling of ordered span pairs||for||relation extraction
",,,"Results||has||our large gains
",,,,,
research-problem,RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side Information,Distantly - Supervised Neural Relation Extraction,,,,,"Contribution||has research problem||Distantly - Supervised Neural Relation Extraction
",,,,
research-problem,Distantly - supervised Relation Extraction ( RE ) methods train an extractor by automatically aligning relation instances in a Knowledge Base ( KB ) with unstructured text .,Distantly - supervised Relation Extraction ( RE ),,,,,"Contribution||has research problem||Distantly - supervised Relation Extraction ( RE )
",,,,
research-problem,RE models usually ignore such readily available side information .,RE,,,,,"Contribution||has research problem||RE
",,,,
research-problem,Relation Extraction ( RE ) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text .,Relation Extraction ( RE ),,,,,"Contribution||has research problem||Relation Extraction ( RE )
",,,,
model,"In this paper , we propose RESIDE , a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture .","propose
RESIDE
novel distant supervised relation extraction method
utilizes
additional supervision
from
KB
through
neural network based architecture","novel distant supervised relation extraction method||utilizes||additional supervision
additional supervision||through||neural network based architecture
additional supervision||from||KB
","RESIDE||has||novel distant supervised relation extraction method
","Model||propose||RESIDE
",,,,,,
model,"RESIDE makes principled use of entity type and relation alias information from KBs , to impose soft constraints while predicting the relation .","RESIDE
makes
principled use
of
entity type and relation alias information
from
KBs
to impose
soft constraints
while predicting
relation","RESIDE||makes||principled use
principled use||of||entity type and relation alias information
entity type and relation alias information||to impose||soft constraints
soft constraints||while predicting||relation
entity type and relation alias information||from||KBs
",,,"Model||has||RESIDE
",,,,,
model,"It uses encoded syntactic information obtained from Graph Convolution Networks ( GCN ) , along with embedded side information , to improve neural relation extraction .","uses
encoded syntactic information
obtained from
Graph Convolution Networks ( GCN )
along with
embedded side information
to improve
neural relation extraction","encoded syntactic information||obtained from||Graph Convolution Networks ( GCN )
encoded syntactic information||to improve||neural relation extraction
encoded syntactic information||along with||embedded side information
",,"Model||uses||encoded syntactic information
",,,,,,
code,RESIDE 's source code and datasets used in the paper are available at http://github.com / malllabiisc / RESIDE .,http://github.com / malllabiisc / RESIDE,,,,,"Contribution||Code||http://github.com / malllabiisc / RESIDE
",,,,
baselines,Mintz : Multi-class logistic regression model proposed by for distant supervision paradigm .,"Mintz
Multi-class logistic regression model
for
distant supervision paradigm","Multi-class logistic regression model||for||distant supervision paradigm
","Mintz||has||Multi-class logistic regression model
",,"Baselines||has||Mintz
",,,,,
baselines,MultiR : Probabilistic graphical model for multi instance learning by MIMLRE :,"MultiR
Probabilistic graphical model
for
multi instance learning
MIMLRE","Probabilistic graphical model||for||multi instance learning
","MultiR||has||Probabilistic graphical model
",,"Baselines||has||MIMLRE
Baselines||has||MultiR
",,,,,"MIMLRE||has||graphical model
"
baselines,A graphical model which jointly models multiple instances and multiple labels .,"graphical model
jointly models
multiple instances and multiple labels","graphical model||jointly models||multiple instances and multiple labels
",,,,,,,,
baselines,More details in . PCNN : A CNN based relation extraction model by which uses piecewise max - pooling for sentence representation .,"PCNN
CNN based relation extraction model
uses
piecewise max - pooling
for
sentence representation","CNN based relation extraction model||uses||piecewise max - pooling
piecewise max - pooling||for||sentence representation
","PCNN||has||CNN based relation extraction model
",,"Baselines||has||PCNN
",,,,,
baselines,PCNN + ATT : A piecewise max - pooling over CNN based model which is used by to get sentence representation followed by attention over sentences .,"PCNN + ATT
piecewise max - pooling
over
CNN based model
to get
sentence representation
followed by
attention over sentences","piecewise max - pooling||over||CNN based model
CNN based model||to get||sentence representation
sentence representation||followed by||attention over sentences
","PCNN + ATT||has||piecewise max - pooling
",,"Baselines||has||PCNN + ATT
",,,,,
baselines,BGWA : Bi - GRU based relation extraction model with word and sentence level attention ) .,"BGWA
Bi - GRU based relation extraction model
with
word and sentence level attention","Bi - GRU based relation extraction model||with||word and sentence level attention
","BGWA||has||Bi - GRU based relation extraction model
",,"Baselines||has||BGWA
",,,,,
results,"Overall , we find that RESIDE achieves higher precision over the entire recall range on both the datasets .","find that
RESIDE
achieves
higher precision
over
entire recall range
on
both the datasets","RESIDE||achieves||higher precision
higher precision||over||entire recall range
entire recall range||on||both the datasets
",,"Results||find that||RESIDE
",,,,,,
results,All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous .,"non-neural baselines
could not perform
well","non-neural baselines||could not perform||well
",,,"Results||has||non-neural baselines
",,,,,
results,RESIDE outperforms PCNN + ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model .,"RESIDE
outperforms
PCNN + ATT and BGWA","RESIDE||outperforms||PCNN + ATT and BGWA
",,,"Results||has||RESIDE
",,,,,
results,The higher performance of BGWA and PCNN + ATT over PCNN shows that attention helps in distant supervised RE .,"higher performance
of
BGWA and PCNN + ATT
over
PCNN
shows that
attention
helps in
distant supervised RE","higher performance||of||BGWA and PCNN + ATT
BGWA and PCNN + ATT||over||PCNN
higher performance||shows that||attention
attention||helps in||distant supervised RE
",,,"Results||has||higher performance
",,,,,
ablation-analysis,The results validate that GCNs are effective at encoding syntactic information .,"validate that
GCNs
effective at encoding
syntactic information","GCNs||effective at encoding||syntactic information
",,"Ablation analysis||validate that||GCNs
",,,,,,
ablation-analysis,"Further , the improvement from side information shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that inducing side information leads to improved relation extraction .","side information
inducing
leads to
improved relation extraction","side information||leads to||improved relation extraction
",,"Ablation analysis||inducing||side information
",,,,,,
ablation-analysis,We find that the model performs best when aliases are provided by the KB itself .,"model
performs
best
when
aliases
provided by
KB","model||performs||best
best||when||aliases
aliases||provided by||KB
",,,"Ablation analysis||has||model
",,,,,
ablation-analysis,"Overall , we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available .","RESIDE
gives
competitive performance
when
very limited amount of relation alias information
is
available","RESIDE||gives||competitive performance
competitive performance||when||very limited amount of relation alias information
very limited amount of relation alias information||is||available
",,,"Ablation analysis||has||RESIDE
",,,,,
ablation-analysis,We observe that performance improves further with the availability of more alias information .,"observe
performance
improves further with
availability of more alias information","performance||improves further with||availability of more alias information
",,"Ablation analysis||observe||performance
",,,,,,
research-problem,Attention Guided Graph Convolutional Networks for Relation Extraction,Relation Extraction,,,,,"Contribution||has research problem||Relation Extraction
",,,,
model,"In this paper , we propose the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) , which operate directly on the full tree .","propose
novel Attention Guided Graph Convolutional Networks ( AGGCNs )
operate directly on
full tree","novel Attention Guided Graph Convolutional Networks ( AGGCNs )||operate directly on||full tree
",,"Model||propose||novel Attention Guided Graph Convolutional Networks ( AGGCNs )
",,,,,,
model,"Intuitively , we develop a "" soft pruning "" strategy that transforms the original dependency tree into a fully connected edgeweighted graph .","develop
"" soft pruning "" strategy
that transforms
original dependency tree
into
fully connected edgeweighted graph",""" soft pruning "" strategy||that transforms||original dependency tree
original dependency tree||into||fully connected edgeweighted graph
",,"Model||develop||"" soft pruning "" strategy
",,,,,,
model,"These weights can be viewed as the strength of relatedness between nodes , which can be learned in an end - to - end fashion by using self - attention mechanism .","weights
viewed as
strength of relatedness
between
nodes
can be learned in
end - to - end fashion
by using
self - attention mechanism","weights||viewed as||strength of relatedness
strength of relatedness||can be learned in||end - to - end fashion
end - to - end fashion||by using||self - attention mechanism
strength of relatedness||between||nodes
",,,"Model||has||weights
",,,,,
model,we next introduce dense connections ) to the GCN model following .,"introduce
dense connections
to
GCN model","dense connections||to||GCN model
",,"Model||introduce||dense connections
",,,,,,
model,"For GCNs , L layers will be needed in order to capture neighborhood information that is L hops away .","For
GCNs
L layers
to capture
neighborhood information
that is
L hops away","L layers||to capture||neighborhood information
neighborhood information||that is||L hops away
","GCNs||has||L layers
","Model||For||GCNs
",,,,,,
model,"With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .","With the help of
dense connections
able to
train
AGGCN model
with
large depth","dense connections||able to||train
AGGCN model||with||large depth
","train||has||AGGCN model
","Model||With the help of||dense connections
",,,,,,
code,Our code is available at https://github.com/Cartus / AGGCN_TACRED,https://github.com/Cartus / AGGCN_TACRED,,,,,"Contribution||Code||https://github.com/Cartus / AGGCN_TACRED
",,,,
hyperparameters,"We choose the number of heads N for attention guided layer from { 1 , 2 , 3 , 4 } , the block number M from { 1 , 2 , 3 } , the number of sub - layers L in each densely connected layer from { 2 , 3 , 4 }.","choose
number of heads N
for
attention guided layer
from
{ 1 , 2 , 3 , 4 }
block number M
from
{ 1 , 2 , 3 }
number of sub - layers L
in
each densely connected layer
from","number of heads N||for||attention guided layer
attention guided layer||from||{ 1 , 2 , 3 , 4 }
number of sub - layers L||in||each densely connected layer
block number M||from||{ 1 , 2 , 3 }
",,"Hyperparameters||choose||number of heads N
Hyperparameters||choose||number of sub - layers L
Hyperparameters||choose||block number M
",,,,"each densely connected layer||from||{ 2 , 3 , 4 }
",,
hyperparameters,Glo Ve vectors are used as the initialization for word embeddings .,"Glo Ve vectors
used as
initialization
for
word embeddings","Glo Ve vectors||used as||initialization
initialization||for||word embeddings
",,,"Hyperparameters||has||Glo Ve vectors
",,,,,
baselines,"For cross - sentence n- ary relation extraction task , we consider three kinds of models as baselines :","For
cross - sentence n- ary relation extraction task",,,"Baselines||For||cross - sentence n- ary relation extraction task
",,,,,,"cross - sentence n- ary relation extraction task||has||feature - based classifier
cross - sentence n- ary relation extraction task||has||Graph - structured LSTM methods
"
baselines,"1 ) a feature - based classifier based on shortest dependency paths between all entity pairs , 2 ) Graph - structured LSTM methods , including Graph LSTM , bidirectional DAG LSTM ( Bidir DAG LSTM ) and Graph State LSTM ( GS GLSTM ) .","feature - based classifier
based on
shortest dependency paths
between
all entity pairs
Graph - structured LSTM methods
including
Graph LSTM
bidirectional DAG LSTM ( Bidir DAG LSTM )
Graph State LSTM ( GS GLSTM )","feature - based classifier||based on||shortest dependency paths
shortest dependency paths||between||all entity pairs
Graph - structured LSTM methods||including||Graph LSTM
Graph - structured LSTM methods||including||bidirectional DAG LSTM ( Bidir DAG LSTM )
Graph - structured LSTM methods||including||Graph State LSTM ( GS GLSTM )
",,,,,,,,
baselines,"These methods extend LSTM to encode graphs constructed from input sentences with dependency edges , 3 ) Graph convolutional networks ( GCN ) with pruned trees , 6 https://nlp.stanford.edu/projects/","with
Graph convolutional networks ( GCN )
pruned trees","Graph convolutional networks ( GCN )||with||pruned trees
",,,,,,,"cross - sentence n- ary relation extraction task||has||Graph convolutional networks ( GCN )
",
results,"For ternary relation extraction ( first two columns in ) , our AGGCN model achieves accuracies of 87.1 and 87.0 on instances within single sentence ( Single ) and on all instances ( Cross ) , respectively , which outperform all the baselines .","For
ternary relation extraction
our AGGCN model
achieves
accuracies
of
87.1 and 87.0
on
instances
within
single sentence ( Single )
all instances ( Cross )
outperform
all the baselines","our AGGCN model||achieves||accuracies
accuracies||of||87.1 and 87.0
87.1 and 87.0||outperform||all the baselines
87.1 and 87.0||on||instances
instances||within||single sentence ( Single )
87.1 and 87.0||on||all instances ( Cross )
","ternary relation extraction||has||our AGGCN model
","Results||For||ternary relation extraction
",,,,,,
results,"More specifically , our AG - GCN model surpasses the state - of - the - art Graphstructured LSTM model ( GS GLSTM ) by 6.8 and 3.8 points for the Single and Cross settings , respectively .","AG - GCN model
surpasses
state - of - the - art Graphstructured LSTM model ( GS GLSTM )
by
6.8 and 3.8 points
for
Single and Cross settings","AG - GCN model||surpasses||state - of - the - art Graphstructured LSTM model ( GS GLSTM )
state - of - the - art Graphstructured LSTM model ( GS GLSTM )||by||6.8 and 3.8 points
state - of - the - art Graphstructured LSTM model ( GS GLSTM )||for||Single and Cross settings
",,,"Results||has||AG - GCN model
",,,,,
results,"Compared to GCN models , our model obtains 1.3 and 1.2 points higher than the best performing model with pruned tree ( K=1 ) .","Compared to
GCN models
our model
obtains
1.3 and 1.2 points higher
than
best performing model
with
pruned tree ( K=1 )","our model||obtains||1.3 and 1.2 points higher
1.3 and 1.2 points higher||than||best performing model
best performing model||with||pruned tree ( K=1 )
","GCN models||has||our model
","Results||Compared to||GCN models
",,,,,,
results,"For binary relation extraction ( third and fourth columns in ) , AGGCN consistently outperforms GS GLSTM and GCN as well .","binary relation extraction
AGGCN
consistently outperforms
GS GLSTM
GCN","AGGCN||consistently outperforms||GS GLSTM
AGGCN||consistently outperforms||GCN
","binary relation extraction||has||AGGCN
",,"Results||For||binary relation extraction
",,,,,
results,"AGGCN also performs better than GCNs , although its performance can be boosted via pruned trees .","AGGCN
performs
better
than
GCNs","AGGCN||performs||better
better||than||GCNs
",,,"Results||has||AGGCN
",,,,,
results,"However , our AGGCN model still obtains 8.0 and 5.7 points higher than the GS GLSTM model for ternary and binary relations , respectively .","our AGGCN model
obtains
8.0 and 5.7 points
higher than
GS GLSTM model
for
ternary and binary relations","our AGGCN model||obtains||8.0 and 5.7 points
8.0 and 5.7 points||higher than||GS GLSTM model
GS GLSTM model||for||ternary and binary relations
",,,"Results||has||our AGGCN model
",,,,,
results,"We also notice that our AGGCN achieves a better test accuracy than all GCN models , which further demonstrates its ability to learn better representations from full trees .","our AGGCN
achieves
better test accuracy
than
all GCN models","our AGGCN||achieves||better test accuracy
better test accuracy||than||all GCN models
",,,"Results||has||our AGGCN
",,,,,
results,"Our C - AGGCN model achieves an F1 score of 68.2 , which outperforms the state - ofart C - GCN model by 1.8 points .","C - AGGCN model
achieves
F1 score
of
68.2
outperforms
state - ofart C - GCN model
by
1.8 points","C - AGGCN model||achieves||F1 score
F1 score||of||68.2
68.2||outperforms||state - ofart C - GCN model
state - ofart C - GCN model||by||1.8 points
",,,"Results||has||C - AGGCN model
",,,,,
results,"We also notice that AGGCN and C - AGGCN achieve better precision and recall scores than GCN and C - GCN , respectively .","notice
AGGCN and C - AGGCN
achieve
better precision and recall scores
than
GCN and C - GCN","AGGCN and C - AGGCN||achieve||better precision and recall scores
better precision and recall scores||than||GCN and C - GCN
",,"Results||notice||AGGCN and C - AGGCN
",,,,,,
results,The performance gap between GCNs with pruned trees and AGGCNs with full trees empirically show that the AGGCN model is better at distinguishing relevant from irrelevant information for learning a better graph representation .,"performance gap
between
GCNs with pruned trees and AGGCNs with full trees
empirically show
AGGCN model
better at distinguishing
relevant from irrelevant information
for learning
better graph representation","performance gap||between||GCNs with pruned trees and AGGCNs with full trees
GCNs with pruned trees and AGGCNs with full trees||empirically show||AGGCN model
AGGCN model||better at distinguishing||relevant from irrelevant information
relevant from irrelevant information||for learning||better graph representation
",,,"Results||has||performance gap
",,,,,
results,We also evaluate our model on the SemEval dataset under the same settings as .,"on
SemEval dataset",,,"Results||on||SemEval dataset
",,,,,,"SemEval dataset||has||Our C - AGGCN model ( 85.7 )
"
results,"Our C - AGGCN model ( 85.7 ) consistently outperforms the C - GCN model ( 84.8 ) , showing the good generalizability .","Our C - AGGCN model ( 85.7 )
consistently outperforms
C - GCN model ( 84.8 )
showing
good generalizability","Our C - AGGCN model ( 85.7 )||consistently outperforms||C - GCN model ( 84.8 )
Our C - AGGCN model ( 85.7 )||showing||good generalizability
",,,,,,,,
ablation-analysis,We can observe that adding either attention guided layers or densely connected layers improves the performance of the model .,"observe that
adding
either attention guided layers or densely connected layers
improves
performance
of
model","either attention guided layers or densely connected layers||improves||performance
performance||of||model
","adding||has||either attention guided layers or densely connected layers
","Ablation analysis||observe that||adding
",,,,,,
ablation-analysis,We also notice that the feed - forward layer is effective in our model .,"notice that
feed - forward layer
effective in
our model","feed - forward layer||effective in||our model
",,"Ablation analysis||notice that||feed - forward layer
",,,,,,
ablation-analysis,"Without the feed - forward layer , the result drops to an F1 score of 67.8 .","Without
feed - forward layer
result
drops to
F1 score of 67.8","result||drops to||F1 score of 67.8
","feed - forward layer||has||result
","Ablation analysis||Without||feed - forward layer
",,,,,,
ablation-analysis,We can observe that all the C - AGGCN models with varied values of K are able to outperform the state - of - the - art C - GCN model ( reported in ) .,"all the C - AGGCN models
with
varied values of K
outperform
state - of - the - art C - GCN model","all the C - AGGCN models||with||varied values of K
varied values of K||outperform||state - of - the - art C - GCN model
",,,"Ablation analysis||observe that||all the C - AGGCN models
",,,,,
ablation-analysis,"In addition , we notice that the performance of C - AGGCN with full trees outperforms all C - AGGCNs with pruned trees .","performance
of
C - AGGCN with full trees
outperforms
all C - AGGCNs with pruned trees","performance||of||C - AGGCN with full trees
C - AGGCN with full trees||outperforms||all C - AGGCNs with pruned trees
",,,"Ablation analysis||notice that||performance
",,,,,
ablation-analysis,"In general , C - AGGCN with full trees outperforms C - AGGCN with pruned trees and C - GCN against various sentence lengths .","C - AGGCN with full trees
outperforms
C - AGGCN with pruned trees and C - GCN
against
various sentence lengths","C - AGGCN with full trees||outperforms||C - AGGCN with pruned trees and C - GCN
C - AGGCN with pruned trees and C - GCN||against||various sentence lengths
",,,"Ablation analysis||has||C - AGGCN with full trees
",,,,,
ablation-analysis,"Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the sentence length increases .","improvement
achieved by
C - AGGCN with pruned trees
decays
when
sentence length
increases","improvement||achieved by||C - AGGCN with pruned trees
decays||when||sentence length
","C - AGGCN with pruned trees||has||decays
sentence length||has||increases
",,"Ablation analysis||has||improvement
",,,,,
ablation-analysis,This suggests that C - AGGCN can benefit more from larger graphs ( full tree ) .,"suggests that
C - AGGCN
benefit more from
larger graphs ( full tree )","C - AGGCN||benefit more from||larger graphs ( full tree )
",,"Ablation analysis||suggests that||C - AGGCN
",,,,,,
ablation-analysis,C - AGGCN consistently outperforms C - GCN under the same amount of training data .,"C - AGGCN
consistently outperforms
C - GCN
under
same amount of training data","C - AGGCN||consistently outperforms||C - GCN
C - GCN||under||same amount of training data
",,,"Ablation analysis||has||C - AGGCN
",,,,,
ablation-analysis,"When the size of training data increases , we can observe that the performance gap becomes more obvious .","When
size
of
training data
increases
observe that
performance gap
becomes
more obvious","size||of||training data
training data||observe that||performance gap
performance gap||becomes||more obvious
","training data||has||increases
","Ablation analysis||When||size
",,,,,,
ablation-analysis,"Particularly , using 80 % of the training data , the C - AGGCN model is able to achieve a F 1 score of 66.5 , higher than C - GCN trained on the whole dataset .","using
80 %
of
training data
C - AGGCN model
able to achieve
F 1 score
of
66.5
higher than
C - GCN
trained on
whole dataset","80 %||of||training data
C - AGGCN model||able to achieve||F 1 score
F 1 score||of||66.5
66.5||higher than||C - GCN
C - GCN||trained on||whole dataset
","training data||has||C - AGGCN model
","Ablation analysis||using||80 %
",,,,,,
research-problem,Matching the Blanks : Distributional Similarity for Relation Learning,Relation Learning,,,,,"Contribution||has research problem||Relation Learning
",,,,
research-problem,Reading text to identify and extract relations between entities has been along standing goal in natural language processing .,identify and extract relations between entities,,,,,"Contribution||has research problem||identify and extract relations between entities
",,,,
research-problem,Typically efforts in relation extraction fall into one of three groups .,relation extraction,,,,,"Contribution||has research problem||relation extraction
",,,,
model,"First , we study the ability of the Transformer neural network architecture to encode relations between entity pairs , and we identify a method of representation that outperforms previous work in supervised relation extraction .","study the ability of
Transformer neural network architecture
to encode
relations
between
entity pairs
identify
representation
that outperforms
previous work
in
supervised relation extraction","Transformer neural network architecture||identify||representation
representation||that outperforms||previous work
previous work||in||supervised relation extraction
Transformer neural network architecture||to encode||relations
relations||between||entity pairs
",,"Model||study the ability of||Transformer neural network architecture
",,,,,,
model,"Then , we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks .","present
method
of
training
relation representation
without
any supervision
from
knowledge graph or human annotators
by matching
blanks","method||of||training
training||by matching||blanks
relation representation||without||any supervision
any supervision||from||knowledge graph or human annotators
","training||has||relation representation
","Model||present||method
",,,,,,
results,shows that the task agnostic BERT EM and BERT EM + MTB models outperform the previous published state of the art on FewRel task even when they have not seen any FewRel training data .,"task agnostic BERT EM and BERT EM + MTB models
outperform
previous published state of the art
on
FewRel task
when they have not seen
any FewRel training data","task agnostic BERT EM and BERT EM + MTB models||outperform||previous published state of the art
previous published state of the art||on||FewRel task
FewRel task||when they have not seen||any FewRel training data
",,,"Results||has||task agnostic BERT EM and BERT EM + MTB models
",,,,,
results,"For BERT EM + MTB , the increase over 's supervised approach is very significant - 8.8 % on the 5 - way - 1 - shot task and 12.7 % on the 10 - way - 1 - shot task .","For
BERT EM + MTB
increase
very significant
8.8 %
on
5 - way - 1 - shot task
12.7 %
on
10 - way - 1 - shot task","BERT EM + MTB||increase||very significant
12.7 %||on||10 - way - 1 - shot task
8.8 %||on||5 - way - 1 - shot task
","very significant||has||12.7 %
very significant||has||8.8 %
","Results||For||BERT EM + MTB
",,,,,,
results,"BERT EM + MTB also significantly outperforms BERT EM in this unsupervised setting , which is to be expected since there is no relation - specific loss during BERT EM 's training .","BERT EM
significantly outperforms",,,,,,"BERT EM + MTB||significantly outperforms||BERT EM
",,,
results,"When given access to all of the training data , BERT EM approaches BERT EM + MTB 's performance .","given
access
to
all of the training data
BERT EM
approaches
BERT EM + MTB 's performance","access||to||all of the training data
BERT EM||approaches||BERT EM + MTB 's performance
","all of the training data||has||BERT EM
","Results||given||access
",,,,,,
results,The results in show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system .,"show that
MTB training
could be used to
significantly reduce effort
in implementing
exemplar based relation extraction system","MTB training||could be used to||significantly reduce effort
significantly reduce effort||in implementing||exemplar based relation extraction system
",,"Results||show that||MTB training
",,,,,,
results,The additional MTB based training further increases F 1 scores for all tasks .,"additional MTB based training
further increases
F 1 scores
for
all tasks","additional MTB based training||further increases||F 1 scores
F 1 scores||for||all tasks
",,,"Results||has||additional MTB based training
",,,,,
results,"For all tasks , we see that MTB based training is even more effective for low - resource cases , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .","see that
MTB based training
even more effective for
low - resource cases","MTB based training||even more effective for||low - resource cases
",,"Results||see that||MTB based training
",,,,,,
results,"This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors , and populate a knowledge base .","training by
matching the blanks
can
significantly reduce
amount of
human input
to create
relation extractors
populate
knowledge base","matching the blanks||can||significantly reduce
significantly reduce||amount of||human input
human input||to create||relation extractors
human input||populate||knowledge base
",,"Results||training by||matching the blanks
",,,,,,
research-problem,Enriching Pre-trained Language Model with Entity Information for Relation Classification,Relation Classification,,,,,"Contribution||has research problem||Relation Classification
",,,,
model,"In this paper , we apply the pretrained BERT model for relation classification .","apply
pretrained BERT model
for
relation classification","pretrained BERT model||for||relation classification
",,"Model||apply||pretrained BERT model
",,,,,,
model,"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .","insert
special tokens
before and after
target entities
before feeding
text
to
BERT
for
fine - tuning
to identify
locations
of
two target entities
transfer
information
into
BERT model","special tokens||before feeding||text
text||to||BERT
BERT||for||fine - tuning
special tokens||before and after||target entities
target entities||to identify||locations
locations||transfer||information
information||into||BERT model
locations||of||two target entities
",,"Model||insert||special tokens
",,,,,,
model,We then locate the positions of the two target entities in the output embedding from BERT model .,"locate
positions
of
two target entities
in
output embedding
from
BERT model","positions||in||output embedding
output embedding||from||BERT model
positions||of||two target entities
",,"Model||locate||positions
",,,,,,
model,We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .,"use
embeddings
sentence encoding
embedding
of
special first token
in the setting of
BERT
as the input to
multi - layer neural network
for
classification","multi - layer neural network||for||classification
classification||use||embeddings
classification||use||sentence encoding
embedding||of||special first token
special first token||in the setting of||BERT
","sentence encoding||has||embedding
","Model||as the input to||multi - layer neural network
",,,,,,
hyperparameters,We add dropout before each add - on layer .,"add
dropout
before
each add - on layer","dropout||before||each add - on layer
",,"Hyperparameters||add||dropout
",,,,,,
hyperparameters,"For the pre-trained BERT model , we use the uncased basic model .","For
pre-trained BERT model
use
uncased basic model","pre-trained BERT model||use||uncased basic model
",,"Hyperparameters||For||pre-trained BERT model
",,,,,,
baselines,"We compare our method , R - BERT , against results by multiple methods recently published for the SemEval - 2010 Task 8 dataset , including SVM , RNN , MVRNN , CNN + Softmax , FCM , CR - CNN , Attention - CNN , Entity Attention Bi-LSTM .","SVM
RNN
MVRNN
CNN + Softmax
FCM
CR - CNN
Attention - CNN
Entity Attention Bi-LSTM",,,,"Baselines||has||SVM
Baselines||has||RNN
Baselines||has||MVRNN
Baselines||has||CNN + Softmax
Baselines||has||FCM
Baselines||has||CR - CNN
Baselines||has||Attention - CNN
Baselines||has||Entity Attention Bi-LSTM
",,,,,
results,We can see that R - BERT significantly beats all the baseline methods .,"see that
R - BERT
significantly beats
baseline methods","R - BERT||significantly beats||baseline methods
",,"Results||see that||R - BERT
",,,,,,
results,"The MACRO F1 value of R - BERT is 89. 25 , which is much better than the previous best solution on this dataset .","MACRO F1 value
of
R - BERT
is
89. 25
much better than
previous best solution","MACRO F1 value||of||R - BERT
R - BERT||is||89. 25
89. 25||much better than||previous best solution
",,,"Results||has||MACRO F1 value
",,,,,
ablation-analysis,We observe that the three methods all perform worse than R - BERT .,"observe
three methods
perform
worse
than
R - BERT","three methods||perform||worse
worse||than||R - BERT
",,"Ablation analysis||observe||three methods
",,,,,,
ablation-analysis,"Of the methods , BERT - NO - SEP - NO - ENT performs worst , with its F1 8.16 absolute points worse than R - BERT .","BERT - NO - SEP - NO - ENT
performs
worst
with
F1 8.16 absolute points
worse than
R - BERT","BERT - NO - SEP - NO - ENT||performs||worst
worst||with||F1 8.16 absolute points
F1 8.16 absolute points||worse than||R - BERT
",,,"Ablation analysis||has||BERT - NO - SEP - NO - ENT
",,,,,
ablation-analysis,This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach .,"demonstrates
special separate tokens and the hidden entity vectors
make
important contributions","special separate tokens and the hidden entity vectors||make||important contributions
",,"Ablation analysis||demonstrates||special separate tokens and the hidden entity vectors
",,,,,,
ablation-analysis,BERT without special separate tokens can not locate the target entities and lose this key information .,"BERT
without
special separate tokens
can not locate
target entities","BERT||can not locate||target entities
BERT||without||special separate tokens
",,,"Ablation analysis||has||BERT
",,,,,
ablation-analysis,"On the other hand , incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction .","incorporating
output
of
target entity vectors
further enriches
information
to make
more accurate prediction","output||of||target entity vectors
target entity vectors||to make||more accurate prediction
target entity vectors||further enriches||information
",,"Ablation analysis||incorporating||output
",,,,,,
research-problem,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,Extracting Multiple - Relations,,,,,"Contribution||has research problem||Extracting Multiple - Relations
",,,,
research-problem,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,extracting multiple entity - relations,,,,,"Contribution||has research problem||extracting multiple entity - relations
",,,,
research-problem,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",multiple entityrelations extraction,,,,,"Contribution||has research problem||multiple entityrelations extraction
",,,,
research-problem,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,Relation extraction ( RE ),,,,,"Contribution||has research problem||Relation extraction ( RE )
",,,,
research-problem,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,"RE
multiplerelations extraction ( MRE )",,,,,"Contribution||has research problem||RE
Contribution||has research problem||multiplerelations extraction ( MRE )
",,,,
research-problem,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",MRE,,,,,"Contribution||has research problem||MRE
",,,,
model,"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .","presents
solution
resolve
inefficient multiple - passes issue
of
existing solutions
for
MRE
by encoding
input only once","solution||resolve||inefficient multiple - passes issue
inefficient multiple - passes issue||of||existing solutions
existing solutions||for||MRE
MRE||by encoding||input only once
",,"Model||presents||solution
",,,,,,
model,"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .","proposed solution
built on top of
existing transformer - based , pretrained general - purposed language encoders","proposed solution||built on top of||existing transformer - based , pretrained general - purposed language encoders
",,,"Model||has||proposed solution
",,,,,
model,"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .","use
Bidirectional Encoder Representations from Transformers ( BERT )
as
transformer - based encoder","Bidirectional Encoder Representations from Transformers ( BERT )||as||transformer - based encoder
",,"Model||use||Bidirectional Encoder Representations from Transformers ( BERT )
",,,,,,
model,The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,"introduce
structured prediction layer
for predicting
multiple relations
for
different entity pairs
make
selfattention layers
aware of
positions
of
all en-tities
in
input paragraph","structured prediction layer||for predicting||multiple relations
multiple relations||for||different entity pairs
selfattention layers||aware of||positions
positions||of||all en-tities
all en-tities||in||input paragraph
",,"Model||introduce||structured prediction layer
Model||make||selfattention layers
",,,,,,
baselines,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .","BERT SP
BERT with structured prediction only",,"BERT SP||has||BERT with structured prediction only
",,"Baselines||has||BERT SP
",,,,,
baselines,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .","Entity - Aware BERT SP
our full model",,"Entity - Aware BERT SP||is||our full model
",,"Baselines||has||Entity - Aware BERT SP
",,,,,
baselines,BERT SP with position embedding on the final attention layer .,BERT SP with position embedding on the final attention layer,,,,"Baselines||has||BERT SP with position embedding on the final attention layer
",,,,,"BERT SP with position embedding on the final attention layer||has||more straightforward way
"
baselines,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,"is
more straightforward way
to achieve
MRE
in
one - pass
using
position embeddings","more straightforward way||to achieve||MRE
MRE||using||position embeddings
MRE||in||one - pass
",,,,,,,,
baselines,"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )","BERT SP with entity indicators on input layer
replaces
our structured attention layer
adds
indicators
of
entities ( transformed to embeddings )","BERT SP with entity indicators on input layer||replaces||our structured attention layer
BERT SP with entity indicators on input layer||adds||indicators
indicators||of||entities ( transformed to embeddings )
",,,"Baselines||has||BERT SP with entity indicators on input layer
",,,,,
results,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,"first observation
is that
our model architecture
achieves
much better results
compared to
previous state - of - the - art methods","first observation||is that||our model architecture
our model architecture||achieves||much better results
much better results||compared to||previous state - of - the - art methods
",,,"Results||has||first observation
",,,,,
results,"Note that our method was not designed for domain adaptation , it still outperforms those methods with domain adaptation .","Note
our method
not designed for
domain adaptation
still outperforms
methods
with
domain adaptation","our method||not designed for||domain adaptation
our method||still outperforms||methods
methods||with||domain adaptation
",,"Results||Note||our method
",,,,,,
results,"Among all the BERT - based approaches , finetuning the off - the - shelf BERT does not give a satisfying result , because the sentence embeddings can not distinguish different entity pairs .","Among
all the BERT - based approaches
finetuning
off - the - shelf BERT
does not give
satisfying result","all the BERT - based approaches||finetuning||off - the - shelf BERT
off - the - shelf BERT||does not give||satisfying result
",,"Results||Among||all the BERT - based approaches
",,,,,,
results,"The simpler version of our approach , BERT SP , can successfully adapt the pre-trained BERT to the MRE task , and achieves comparable performance at the 3 Note the usage of relative position embeddings does notwork for one - pass MRE , since each word corresponds to a varying number of position embedding vectors .","BERT SP
successfully adapt
pre-trained BERT
to
MRE task
achieves
comparable performance","BERT SP||successfully adapt||pre-trained BERT
pre-trained BERT||to||MRE task
BERT SP||achieves||comparable performance
",,,"Results||has||BERT SP
",,,,,
results,"It works for the singlerelation per pass setting , but the performance lags behind using only indicators of the two target entities .","works for
singlerelation per pass setting
performance
lags behind using
only indicators
of
two target entities","performance||lags behind using||only indicators
only indicators||of||two target entities
","singlerelation per pass setting||has||performance
","Results||works for||singlerelation per pass setting
",,,,,,
results,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .","Our full model
with
structured fine - tuning
of
attention layers
brings
further improvement
of
about 5.5 %
in
MRE one - pass setting
achieves
new state - of - the - art performance
compared to
methods with domain adaptation","Our full model||with||structured fine - tuning
structured fine - tuning||of||attention layers
Our full model||brings||further improvement
further improvement||in||MRE one - pass setting
further improvement||of||about 5.5 %
further improvement||achieves||new state - of - the - art performance
new state - of - the - art performance||compared to||methods with domain adaptation
",,,"Results||has||Our full model
",,,,,
results,"For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .","For
BERT SP
with
entity indicators
on
inputs","BERT SP||with||entity indicators
entity indicators||on||inputs
",,"Results||For||BERT SP
",,,,,,
results,A 2 % gap is observed as expected .,"2 % gap
observed",,,,,,"BERT SP||observed||2 % gap
",,,
results,"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .","BERT SP with position embeddings
on
final attention layer
train
model
in
single - relation setting
test with
two different settings
so
results
are
same","BERT SP with position embeddings||test with||two different settings
two different settings||so||results
results||are||same
BERT SP with position embeddings||on||final attention layer
BERT SP with position embeddings||train||model
model||in||single - relation setting
",,,"Results||For||BERT SP with position embeddings
",,,,,
results,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .","Our Entity - Aware BERT SP
gives
comparable results
to
top - ranked system
in
shared task
with
slightly lower Macro - F1
slightly higher Micro - F1","Our Entity - Aware BERT SP||gives||comparable results
comparable results||with||slightly lower Macro - F1
comparable results||with||slightly higher Micro - F1
comparable results||to||top - ranked system
top - ranked system||in||shared task
",,,"Results||has||Our Entity - Aware BERT SP
",,,,,
results,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .","When predicting
multiple relations
in
one - pass
have
0.9 % drop
on
Macro - F1
further 0.8 % improvement
on
Micro - F1","multiple relations||in||one - pass
multiple relations||have||further 0.8 % improvement
further 0.8 % improvement||on||Micro - F1
multiple relations||have||0.9 % drop
0.9 % drop||on||Macro - F1
",,"Results||When predicting||multiple relations
",,,,,,
results,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .","compared to
top singlemodel result
which makes use of
additional word and entity embeddings
pretrained on
in - domain data
our methods
demonstrate
clear advantage
as
single model","top singlemodel result||which makes use of||additional word and entity embeddings
additional word and entity embeddings||pretrained on||in - domain data
our methods||demonstrate||clear advantage
clear advantage||as||single model
","additional word and entity embeddings||has||our methods
","Results||compared to||top singlemodel result
",,,,,,
research-problem,"However , the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly .",populate knowledge bases with facts,,,,,"Contribution||has research problem||populate knowledge bases with facts
",,,,
research-problem,A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text .,populate a knowledge base with relational facts,,,,,"Contribution||has research problem||populate a knowledge base with relational facts
",,,,
research-problem,"Existing work on relation extraction ( e.g. , has been unable to achieve sufficient recall or precision for the results to be usable versus hand - constructed knowledge bases .",relation extraction,,,,,"Contribution||has research problem||relation extraction
",,,,
model,"We propose a new , effective neural network sequence model for relation classification .","propose
new , effective neural network sequence model
for
relation classification","new , effective neural network sequence model||for||relation classification
",,"Model||propose||new , effective neural network sequence model
",,,,,,
model,Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,"architecture
better customized for
slot filling task
word representations
augmented by
extra distributed representations of word position
relative to
subject and object
of
putative relation","architecture||better customized for||slot filling task
word representations||augmented by||extra distributed representations of word position
extra distributed representations of word position||relative to||subject and object
subject and object||of||putative relation
","architecture||has||word representations
",,"Model||has||architecture
",,,,,
model,This means that the neural attention model can effectively exploit the combination of semantic similarity - based attention and positionbased attention .,"means that
neural attention model
effectively exploit
combination of semantic similarity - based attention and positionbased attention","neural attention model||effectively exploit||combination of semantic similarity - based attention and positionbased attention
",,"Model||means that||neural attention model
",,,,,,
dataset,"Secondly , we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset , suitable for the common relations between people , organizations and locations which are used in the TAC KBP evaluations .","markedly improve
availability of supervised training data
by using
Mechanical Turk crowd annotation
to produce
large supervised training dataset
suitable for
common relations
between
people , organizations and locations
used in
TAC KBP evaluations","availability of supervised training data||by using||Mechanical Turk crowd annotation
Mechanical Turk crowd annotation||to produce||large supervised training dataset
large supervised training dataset||suitable for||common relations
common relations||used in||TAC KBP evaluations
common relations||between||people , organizations and locations
",,"Dataset||markedly improve||availability of supervised training data
",,,,,,
dataset,"We name this dataset the TAC Relation Extraction Dataset ( TACRED ) , and will make it available through the Linguistic Data Consortium ( LDC ) in order to respect copyrights on the underlying text .","TAC Relation Extraction Dataset ( TACRED )
make it available through
Linguistic Data Consortium ( LDC )
to respect
copyrights
on
underlying text","TAC Relation Extraction Dataset ( TACRED )||make it available through||Linguistic Data Consortium ( LDC )
Linguistic Data Consortium ( LDC )||to respect||copyrights
copyrights||on||underlying text
",,,"Dataset||name||TAC Relation Extraction Dataset ( TACRED )
",,,,,
hyperparameters,We map words that occur less than 2 times in the training set to a special < UNK > token .,"map
words
occur less than
2 times
in
training set
to
special < UNK > token","words||occur less than||2 times
2 times||in||training set
words||to||special < UNK > token
",,"Hyperparameters||map||words
",,,,,,
hyperparameters,We use the pre-trained GloVe vectors to initialize word embeddings .,"use
pre-trained GloVe vectors
to initialize
word embeddings","pre-trained GloVe vectors||to initialize||word embeddings
",,"Hyperparameters||use||pre-trained GloVe vectors
",,,,,,
hyperparameters,"For all the LSTM layers , we find that 2 - layer stacked LSTMs generally work better than one - layer LSTMs .","For
all the LSTM layers
find that
2 - layer stacked LSTMs
work better than
one - layer LSTMs","all the LSTM layers||find that||2 - layer stacked LSTMs
2 - layer stacked LSTMs||work better than||one - layer LSTMs
",,"Hyperparameters||For||all the LSTM layers
",,,,,,
hyperparameters,We minimize cross - entropy loss over all 42 relations using AdaGrad .,"minimize
cross - entropy loss
over
all 42 relations
using
AdaGrad","cross - entropy loss||over||all 42 relations
cross - entropy loss||using||AdaGrad
",,"Hyperparameters||minimize||cross - entropy loss
",,,,,,
hyperparameters,We apply Dropout with p = 0.5 to CNNs and LSTMs .,"apply
Dropout
with
p = 0.5
to
CNNs and LSTMs","Dropout||with||p = 0.5
p = 0.5||to||CNNs and LSTMs
",,"Hyperparameters||apply||Dropout
",,,,,,
hyperparameters,During training we also find a word dropout strategy to be very effective : we randomly set a token to be < UNK > with a probability p.,"During
training
find
word dropout strategy
to be
very effective
randomly set
token
to be
< UNK >
with","training||find||word dropout strategy
word dropout strategy||randomly set||token
token||to be||< UNK >
word dropout strategy||to be||very effective
",,"Hyperparameters||During||training
",,,,"< UNK >||with||probability p
",,
hyperparameters,We set p to be 0.06 for the SDP - LSTM model and 0.04 for all other models .,"set
p
to be
0.06
for
SDP - LSTM model
0.04
for
all other models","p||to be||0.06
0.06||for||SDP - LSTM model
p||to be||0.04
0.04||for||all other models
",,"Hyperparameters||set||p
",,,,,,
results,"We observe that all neural models achieve higher F 1 scores than the logistic regression and patterns systems , which demonstrates the effectiveness of neural models for relation extraction .","observe
all neural models
achieve
higher F 1 scores
than
logistic regression and patterns systems
demonstrates
effectiveness
of
neural models
for
relation extraction","all neural models||achieve||higher F 1 scores
higher F 1 scores||demonstrates||effectiveness
effectiveness||of||neural models
effectiveness||for||relation extraction
higher F 1 scores||than||logistic regression and patterns systems
",,"Results||observe||all neural models
",,,,,,
results,"Although positional embeddings help increase the F 1 by around 2 % over the plain CNN model , a simple ( 2 - layer ) LSTM model performs surprisingly better than CNN and dependency - based models .","positional embeddings
help increase
F 1
by around
2 %
over
plain CNN model","positional embeddings||help increase||F 1
F 1||by around||2 %
2 %||over||plain CNN model
",,,"Results||has||positional embeddings
",,,,,
results,"Lastly , our proposed position - aware mechanism is very effective and achieves an F 1 score of 65.4 % , with an absolute increase of 3.9 % over the best baseline neural model ( LSTM ) and 7.9 % over the baseline logistic regression system .","proposed position - aware mechanism
is
very effective
achieves
F 1 score
of
65.4 %
with
absolute increase
of
3.9 %
over
best baseline neural model ( LSTM )
7.9 %
over
baseline logistic regression system","proposed position - aware mechanism||achieves||F 1 score
F 1 score||of||65.4 %
65.4 %||with||absolute increase
absolute increase||of||3.9 %
3.9 %||over||best baseline neural model ( LSTM )
absolute increase||of||7.9 %
7.9 %||over||baseline logistic regression system
proposed position - aware mechanism||is||very effective
",,,"Results||has||proposed position - aware mechanism
",,,,,
results,We also run an ensemble of our position - aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F 1 score up by 1.6 % .,"run
ensemble
of
position - aware attention model
takes
majority votes
from
5 runs
with
random initializations
further pushes
F 1 score
by
1.6 %","ensemble||of||position - aware attention model
position - aware attention model||further pushes||F 1 score
F 1 score||by||1.6 %
position - aware attention model||takes||majority votes
majority votes||from||5 runs
5 runs||with||random initializations
",,"Results||run||ensemble
",,,,,,
results,CNN - based models tend to have higher precision ; RNN - based models have better recall .,"CNN - based models
tend to have
higher precision
RNN - based models
have
better recall","CNN - based models||tend to have||higher precision
RNN - based models||have||better recall
",,,"Results||has||CNN - based models
Results||has||RNN - based models
",,,,,
results,"Evaluating relation extraction systems on slot filling is particularly challenging in that : ( 1 ) Endto - end cold start slot filling scores conflate the performance of all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor ) .","Endto - end cold start
slot filling scores
conflate
performance
of
all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor )","slot filling scores||conflate||performance
performance||of||all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor )
","Endto - end cold start||has||slot filling scores
",,"Results||has||Endto - end cold start
",,,,,
results,( 2 ) Errors in hop - 0 predictions can easily propagate to hop - 1 predictions .,"Errors
in
hop - 0 predictions
easily propagate to
hop - 1 predictions","Errors||in||hop - 0 predictions
hop - 0 predictions||easily propagate to||hop - 1 predictions
",,,"Results||has||Errors
",,,,,
results,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -","find
training
our logistic regression model
on
TACRED
2 million bootstrapped examples
used in
2015 Stanford system
combining it with
patterns","our logistic regression model||on||TACRED
our logistic regression model||on||2 million bootstrapped examples
2 million bootstrapped examples||used in||2015 Stanford system
2015 Stanford system||combining it with||patterns
","training||has||our logistic regression model
","Results||find||training
",,,,,,
ablation-analysis,presents the results of an ablation test of our position - aware attention model on the development set of TACRED .,"presents
results of an ablation test
of
our position - aware attention model
on
development set of TACRED","results of an ablation test||of||our position - aware attention model
our position - aware attention model||on||development set of TACRED
",,"Ablation analysis||presents||results of an ablation test
",,,,,,
ablation-analysis,"The entire attention mechanism contributes about 1.5 % F 1 , where the position - aware term in Eq.","contributes
about 1.5 % F 1
where
position - aware term","about 1.5 % F 1||where||position - aware term
",,,,,"our position - aware attention model||contributes||about 1.5 % F 1
","position - aware term||contributes||about 1 % F 1 score
",,
ablation-analysis,( 3 ) alone contributes about 1 % F 1 score .,"contributes
about 1 % F 1 score",,,,,,,,,
ablation-analysis,Impact of negative examples .,,,,,,,,,,
ablation-analysis,"shows how the slot filling evaluation scores change as we change the amount of negative ( i.e. , no relation ) training data provided to our proposed model .","shows how
slot filling evaluation scores
change
as
amount of negative ( i.e. , no relation ) training data
provided to
our proposed model","change||as||amount of negative ( i.e. , no relation ) training data
amount of negative ( i.e. , no relation ) training data||provided to||our proposed model
","slot filling evaluation scores||has||change
","Ablation analysis||shows how||slot filling evaluation scores
",,,,,,
ablation-analysis,"We find that : ( 1 ) At hop - 0 level , precision increases as we provide more negative examples , while recall stays almost unchanged .","At
hop - 0 level
precision
increases
provide more
negative examples
while
recall
stays
almost unchanged","hop - 0 level||provide more||negative examples
hop - 0 level||while||recall
recall||stays||almost unchanged
","hop - 0 level||has||precision
precision||has||increases
","Ablation analysis||At||hop - 0 level
",,,,,,"hop - 0 level||has||F 1 score
"
ablation-analysis,F 1 score keeps increasing .,"F 1 score
keeps
increasing","F 1 score||keeps||increasing
",,,,,,,,
ablation-analysis,"( 2 ) At hop - all level , F 1 score increases by Performance by sentence length .","hop - all level
F 1 score
increases by
Performance
by
sentence length","F 1 score||increases by||Performance
Performance||by||sentence length
","hop - all level||has||F 1 score
",,"Ablation analysis||At||hop - all level
",,,,,
ablation-analysis,We find that : ( 1 ) Performance of all models degrades substantially as the sentences get longer .,"find that
Performance
of
all models
degrades
substantially
as
sentences
get
longer","Performance||of||all models
Performance||degrades||substantially
substantially||as||sentences
sentences||get||longer
",,"Ablation analysis||find that||Performance
",,,,,,
ablation-analysis,"When compared with the CNN - PE model , our position - aware attention model achieves improved F 1 scores on 30 out of the 41 slot types , with the top 5 slot types being org : members , per: country of death , org : shareholders , per:children and per:religion .","compared with
CNN - PE model
position - aware attention model
achieves
improved F 1 scores
on
30 out of the 41 slot types
with
top 5 slot types
being
org : members
per: country of death
org : shareholders
per:children
per:religion","position - aware attention model||achieves||improved F 1 scores
improved F 1 scores||with||top 5 slot types
top 5 slot types||being||org : members
top 5 slot types||being||per: country of death
top 5 slot types||being||org : shareholders
top 5 slot types||being||per:children
top 5 slot types||being||per:religion
improved F 1 scores||on||30 out of the 41 slot types
","CNN - PE model||has||position - aware attention model
","Ablation analysis||compared with||CNN - PE model
",,,,,,
ablation-analysis,"When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .","with
SDP - LSTM model
our model
achieves
improved F 1 scores
on
26 out of the 41 slot types
top 5 slot
being
org : political / religious affiliation
per: country of death
org : alternate names
per:religion
per: alternate names","our model||achieves||improved F 1 scores
improved F 1 scores||with||top 5 slot
top 5 slot||being||org : political / religious affiliation
top 5 slot||being||per: country of death
top 5 slot||being||org : alternate names
top 5 slot||being||per:religion
top 5 slot||being||per: alternate names
improved F 1 scores||on||26 out of the 41 slot types
","SDP - LSTM model||has||our model
",,"Ablation analysis||compared with||SDP - LSTM model
",,,,,
ablation-analysis,We observe that slot types with relatively sparse training examples tend to be improved by using the position - aware attention model .,"observe that
slot types
with
relatively sparse training examples
improved by
position - aware attention model","slot types||with||relatively sparse training examples
slot types||improved by||position - aware attention model
",,"Ablation analysis||observe that||slot types
",,,,,,
ablation-analysis,"We find that the model learns to pay more attention to words that are informative for the relation ( e.g. , "" graduated from "" , "" niece "" and "" chairman "" ) , though it still makes mistakes ( e.g. , "" refused to name the three "" ) .","find
model
to pay
more attention
to
words
informative for
relation","model||to pay||more attention
more attention||to||words
words||informative for||relation
",,"Ablation analysis||find||model
",,,,,,
ablation-analysis,"We also observe that the model tends to put a lot of weight onto object entities , as the object NER signatures are very informative to the classification of relations .","observe
model
tends to put
lot of weight
onto
object entities","model||tends to put||lot of weight
lot of weight||onto||object entities
",,"Ablation analysis||observe||model
",,,,,,
research-problem,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,Relation Extraction,,,,,"Contribution||has research problem||Relation Extraction
",,,,
model,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .","propose
novel extension of the graph convolutional network
tailored for
relation extraction","novel extension of the graph convolutional network||tailored for||relation extraction
",,"Model||propose||novel extension of the graph convolutional network
",,,,,,
model,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .","encodes
dependency structure
over
input sentence
with
efficient graph convolution operations
extracts
entity - centric representations
to make
robust relation predictions","dependency structure||over||input sentence
input sentence||with||efficient graph convolution operations
dependency structure||extracts||entity - centric representations
entity - centric representations||to make||robust relation predictions
",,"Model||encodes||dependency structure
",,,,,,
model,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .","apply
novel path - centric pruning technique
to remove
irrelevant information
from
the tree
maximally keeping
relevant content","novel path - centric pruning technique||to remove||irrelevant information
irrelevant information||from||the tree
irrelevant information||maximally keeping||relevant content
",,"Model||apply||novel path - centric pruning technique
",,,,,,
baselines,Dependency - based models .,Dependency - based models,,,,"Baselines||has||Dependency - based models
",,,,,"Dependency - based models||has||A logistic regression ( LR ) classifier
"
baselines,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,"A logistic regression ( LR ) classifier
combines
dependencybased features
with
other lexical features","A logistic regression ( LR ) classifier||combines||dependencybased features
dependencybased features||with||other lexical features
",,,,,,,,
baselines,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .","Shortest Dependency Path LSTM ( SDP - LSTM )
applies
neural sequence model
on
shortest path
between
subject and object entities
in
dependency tree","Shortest Dependency Path LSTM ( SDP - LSTM )||applies||neural sequence model
neural sequence model||on||shortest path
shortest path||between||subject and object entities
subject and object entities||in||dependency tree
",,,,,,,"Dependency - based models||has||Shortest Dependency Path LSTM ( SDP - LSTM )
",
baselines,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .","Tree - LSTM
is
recursive model
generalizes
LSTM
to
arbitrary tree structures","Tree - LSTM||is||recursive model
recursive model||generalizes||LSTM
LSTM||to||arbitrary tree structures
",,,,,,,"Dependency - based models||has||Tree - LSTM
",
baselines,Neural sequence model .,Neural sequence model,,,,"Baselines||has||Neural sequence model
",,,,,"Neural sequence model||has||competitive sequence model
"
baselines,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .","competitive sequence model
employs
position - aware attention mechanism
over
LSTM outputs ( PA - LSTM )","competitive sequence model||employs||position - aware attention mechanism
position - aware attention mechanism||over||LSTM outputs ( PA - LSTM )
",,,,,,,,
results,Results on the TACRED Dataset,"on
TACRED Dataset",,,"Results||on||TACRED Dataset
",,,,,,
results,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,"observe
GCN model
outperforms
all dependency - based models
by
at least 1.6 F 1","GCN model||outperforms||all dependency - based models
all dependency - based models||by||at least 1.6 F 1
",,,,,"TACRED Dataset||observe||GCN model
",,,
results,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .","using
contextualized word representations
C - GCN model
outperforms
strong PA - LSTM model
by
1.3 F 1
achieves
new state of the art","C - GCN model||outperforms||strong PA - LSTM model
strong PA - LSTM model||by||1.3 F 1
strong PA - LSTM model||achieves||new state of the art
","contextualized word representations||has||C - GCN model
",,,,"TACRED Dataset||using||contextualized word representations
",,,
results,"In addition , we find our model improves upon other dependencybased models in both precision and recall .","find
our model
improves upon
other dependencybased models
in
both precision and recall","our model||improves upon||other dependencybased models
other dependencybased models||in||both precision and recall
",,,,,"TACRED Dataset||find||our model
",,,
results,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .","Comparing
C - GCN model
with
GCN model
find that
gain
mainly comes from
improved recall","C - GCN model||with||GCN model
GCN model||find that||gain
gain||mainly comes from||improved recall
",,,,,"TACRED Dataset||Comparing||C - GCN model
",,,
results,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .","our GCN models
have
complementary strengths
when compared to
PA - LSTM","our GCN models||have||complementary strengths
complementary strengths||when compared to||PA - LSTM
",,,,,,,"TACRED Dataset||find||our GCN models
",
results,"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .","simple interpolation
between
GCN and a PA - LSTM
achieves
F 1 score
of
67.1
outperforming
each model alone
by
at least 2.0 F 1","simple interpolation||between||GCN and a PA - LSTM
GCN and a PA - LSTM||achieves||F 1 score
F 1 score||of||67.1
GCN and a PA - LSTM||outperforming||each model alone
each model alone||by||at least 2.0 F 1
",,,,,,,"TACRED Dataset||has||simple interpolation
",
results,An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .,"interpolation
between
C - GCN and a PA - LSTM
further improves
result
to
68.2","interpolation||between||C - GCN and a PA - LSTM
C - GCN and a PA - LSTM||further improves||result
result||to||68.2
",,,,,,,"TACRED Dataset||has||interpolation
",
results,Results on the SemEval Dataset,SemEval Dataset,,,,"Results||on||SemEval Dataset
",,,,,
results,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .","find that under
conventional with- entity evaluation
our C - GCN model
outperforms
all existing dependency - based neural models","our C - GCN model||outperforms||all existing dependency - based neural models
","conventional with- entity evaluation||has||our C - GCN model
",,,,"SemEval Dataset||find that under||conventional with- entity evaluation
",,,
results,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .","by properly incorporating
off - path information
our model
outperforms
previous shortest dependency path - based model ( SDP - LSTM )","our model||outperforms||previous shortest dependency path - based model ( SDP - LSTM )
","off - path information||has||our model
",,,,"SemEval Dataset||by properly incorporating||off - path information
",,,
results,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .","Under
mask - entity evaluation
our C - GCN model
outperforms
PA - LSTM
by
substantial margin","our C - GCN model||outperforms||PA - LSTM
PA - LSTM||by||substantial margin
","mask - entity evaluation||has||our C - GCN model
",,,,"SemEval Dataset||Under||mask - entity evaluation
",,,
results,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .","show
effectiveness
of
path - centric pruning
compare
two GCN models and the Tree - LSTM
when","effectiveness||of||path - centric pruning
path - centric pruning||compare||two GCN models and the Tree - LSTM
",,"Results||show||effectiveness
",,,,,,
results,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .","performance
peaks
K = 1
outperforming
respective dependency path - based counterpart ( K = 0 )","peaks||outperforming||respective dependency path - based counterpart ( K = 0 )
","peaks||when||K = 1
",,,,"two GCN models and the Tree - LSTM||performance||peaks
",,,
results,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .","find that
all three models
are
less effective
when
entire dependency tree
is
present","all three models||are||less effective
less effective||when||entire dependency tree
entire dependency tree||is||present
",,"Results||find that||all three models
",,,,,,
results,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .","contextualizing
GCN
makes it
less sensitive
to
changes
in
tree structures","GCN||makes it||less sensitive
less sensitive||to||changes
changes||in||tree structures
",,"Results||contextualizing||GCN
",,,,,,
ablation-analysis,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,"entity representations and feedforward layers
contribute
1.0 F 1","entity representations and feedforward layers||contribute||1.0 F 1
",,,"Ablation analysis||has||entity representations and feedforward layers
",,,,,
ablation-analysis,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .","remove
dependency structure
score
drops by
3.2 F 1","score||drops by||3.2 F 1
","dependency structure||has||score
","Ablation analysis||remove||dependency structure
",,,,,,
ablation-analysis,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .","F 1
drops by
10.3
when we remove
feedforward layers
LSTM component
dependency structure","F 1||drops by||10.3
10.3||when we remove||feedforward layers
10.3||when we remove||LSTM component
10.3||when we remove||dependency structure
",,,"Ablation analysis||has||F 1
",,,,,
ablation-analysis,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .","Removing
pruning
hurts
result
by another
9.7 F 1","pruning||hurts||result
result||by another||9.7 F 1
",,"Ablation analysis||Removing||pruning
",,,,,,
research-problem,Context - Aware Representations for Knowledge Base Relation Extraction,Knowledge Base Relation Extraction,,,,,"Contribution||has research problem||Knowledge Base Relation Extraction
",,,,
research-problem,We demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .,sentence - level relation extraction,,,,,"Contribution||has research problem||sentence - level relation extraction
",,,,
model,Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,"architecture
uses
LSTM - based encoder
to jointly learn
representations
for
all relations
in
single sentence","architecture||uses||LSTM - based encoder
LSTM - based encoder||to jointly learn||representations
representations||for||all relations
all relations||in||single sentence
",,,"Model||has||architecture
",,,,,
research-problem,The main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text .,relation extraction,,,,,"Contribution||has research problem||relation extraction
",,,,
research-problem,"In this paper , we consider the sentential relation extraction task : to each occurrence of the target entity pair e 1 , e 2 in some sentence s one has to assign a relation type r from a given set R. A triple e 1 , r , e 2 is called a relation instance and we refer to the relation of the target entity pair as target relation .",sentential relation extraction,,,,,"Contribution||has research problem||sentential relation extraction
",,,,
model,We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation .,"present
novel architecture
considers
other relations
in
sentence
as
context
for predicting
label
of
target relation","novel architecture||considers||other relations
other relations||as||context
other relations||in||sentence
other relations||for predicting||label
label||of||target relation
",,"Model||present||novel architecture
",,,,,,
model,Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,,,,,,,,,,
model,The representation of the target relation and representations of the context relations are combined to make the final prediction .,"representation of
target relation
representations of
context relations
are
combined
to make
final prediction","combined||representations of||context relations
combined||to make||final prediction
combined||representation of||target relation
",,"Model||are||combined
",,,,,,
hyperparameters,All models were trained using the Adam optimizer with categorical crossentropy as the loss function .,"trained using
Adam optimizer
with
categorical crossentropy
as
loss function","Adam optimizer||with||categorical crossentropy
categorical crossentropy||as||loss function
",,"Hyperparameters||trained using||Adam optimizer
",,,,,,
hyperparameters,We use an early stopping criterion on the validation data to determine the number of training epochs .,"use
early stopping criterion
on
validation data
to determine
number of training epochs","early stopping criterion||on||validation data
validation data||to determine||number of training epochs
",,"Hyperparameters||use||early stopping criterion
",,,,,,
hyperparameters,"The learning rate is fixed to 0.01 and the rest of the optimization parameters are set as recommended in : ? 1 = 0.9 , ? 2 = 0.999 , ? = 1e ? 08 . The training is performed in batches of 128 instances .","learning rate
fixed to
0.01
of
training
performed in
batches
128 instances","learning rate||fixed to||0.01
training||performed in||batches
batches||of||128 instances
",,,"Hyperparameters||has||learning rate
Hyperparameters||has||training
",,,,,
hyperparameters,We apply Dropout on the penultimate layer as well as on the embeddings layer with a probability of 0.5 .,"apply
Dropout
on
penultimate layer
as well as on
embeddings layer
with
probability
of
0.5","Dropout||on||penultimate layer
penultimate layer||as well as on||embeddings layer
embeddings layer||with||probability
probability||of||0.5
",,"Hyperparameters||apply||Dropout
",,,,,,
hyperparameters,We choose the size of the layers ( RNN layer size o = 256 ) and entity marker embeddings ( d = 3 ) with a random search on the validation set .,"choose
size
of
layers ( RNN layer size o = 256 )
with
random search
on
validation set","size||with||random search
random search||on||validation set
size||of||layers ( RNN layer size o = 256 )
",,"Hyperparameters||choose||size
",,,,,,
results,"The models that take the context into account perform similar to the baselines at the smallest recall numbers , but start to positively deviate from them at higher recall rates .","models
that take
context
into
account
perform
similar
to
baselines
at
smallest recall numbers
start
to
positively deviate
at
higher recall rates","models||that take||context
context||into||account
models||perform||similar
similar||to||baselines
baselines||at||smallest recall numbers
models||perform||start
start||to||positively deviate
positively deviate||at||higher recall rates
",,,"Results||has||models
",,,,,
results,"In particular , the ContextAtt model performs better than any other system in our study over the entire recall range .","ContextAtt model
performs
better
than
any other system
over
entire recall range","ContextAtt model||performs||better
better||over||entire recall range
better||than||any other system
",,,"Results||has||ContextAtt model
",,,,,
results,"Compared to the competitive LSTM - baseline that uses the same relation encoder , the ContextAtt model achieves a 24 % reduction of the average error : from 0.2096 0.002 to 0.1590 0.002 .","Compared to
competitive LSTM - baseline
ContextAtt model
achieves
24 % reduction
of
average error","ContextAtt model||achieves||24 % reduction
24 % reduction||of||average error
","competitive LSTM - baseline||has||ContextAtt model
","Results||Compared to||competitive LSTM - baseline
",,,,,,
results,shows that the ContextAtt model performs best over all relation types .,"shows
ContextAtt model
performs
best
over
all relation types","ContextAtt model||over||all relation types
ContextAtt model||performs||best
",,"Results||shows||ContextAtt model
",,,,,,
results,One can also see that the ContextSum does n't universally outperforms the LSTM - baseline .,"see that
ContextSum
does n't universally outperforms
LSTM - baseline","ContextSum||does n't universally outperforms||LSTM - baseline
",,"Results||see that||ContextSum
",,,,,,
results,It demonstrates again that using attention is crucial to extract relevant information from the context relations .,"using
attention
crucial to extract
relevant information
from
context relations","attention||crucial to extract||relevant information
relevant information||from||context relations
",,"Results||using||attention
",,,,,,
results,"On the relation - specific results we observe that the context - enabled model demonstrates the most improvement on precision and seems to be especially useful for taxonomy relations ( see SUBCLASS OF , PART OF ) .","observe
context - enabled model
demonstrates
most improvement
on
precision
useful for
taxonomy relations","context - enabled model||demonstrates||most improvement
most improvement||on||precision
context - enabled model||useful for||taxonomy relations
",,"Results||observe||context - enabled model
",,,,,,
research-problem,Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer Learning,Neural Relation Extraction,,,,,"Contribution||has research problem||Neural Relation Extraction
",,,,
research-problem,Relation extraction aims to extract relations between pairs of marked entities in raw texts .,Relation extraction,,,,,"Contribution||has research problem||Relation extraction
",,,,
research-problem,"In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words .","propose
novel word - level approach
for
distant supervised relation extraction
by reducing
inner-sentence noise
improving
robustness
against
noisy words","novel word - level approach||improving||robustness
robustness||against||noisy words
novel word - level approach||for||distant supervised relation extraction
novel word - level approach||by reducing||inner-sentence noise
",,"Approach||propose||novel word - level approach
",,"Contribution||has research problem||distant supervised relation extraction
",,,,
approach,"To reduce innersentence noise , we utilize a novel Sub - Tree Parse ( STP ) method to remove irrelevant words by intercepting a subtree under the parent of entities ' lowest common ancestor .","To reduce
innersentence noise
utilize
novel Sub - Tree Parse ( STP ) method
to remove
irrelevant words
by intercepting
subtree
under
parent of entities ' lowest common ancestor","innersentence noise||utilize||novel Sub - Tree Parse ( STP ) method
novel Sub - Tree Parse ( STP ) method||by intercepting||subtree
subtree||under||parent of entities ' lowest common ancestor
novel Sub - Tree Parse ( STP ) method||to remove||irrelevant words
",,"Approach||To reduce||innersentence noise
",,,,,,
approach,"Furthermore , the entity - wise attention is adopted to alleviate the influence of noisy words in the subtree and emphasize the task - relevant features .","entity - wise attention
to alleviate
influence of noisy words
in
subtree
emphasize
task - relevant features","entity - wise attention||to alleviate||influence of noisy words
influence of noisy words||in||subtree
entity - wise attention||emphasize||task - relevant features
",,,"Approach||has||entity - wise attention
",,,,,
approach,"To tackle the second challenge , we initialize our model parameters with a priori knowledge learned from the entity type classification task by transfer learning .","initialize
model parameters
with
a priori knowledge
learned from
entity type classification task
by
transfer learning","model parameters||with||a priori knowledge
a priori knowledge||learned from||entity type classification task
entity type classification task||by||transfer learning
",,"Approach||initialize||model parameters
",,,,,,
experimental-setup,"In the experiment , we utilize word2vec 2 to train word embeddings on NYT corpus .","utilize
word2vec
to train
word embeddings
on
NYT corpus","word2vec||to train||word embeddings
word embeddings||on||NYT corpus
",,"Experimental setup||utilize||word2vec
",,,,,,
experimental-setup,"The grid search approach is used to select optimal learning rate lr for Adam optimizer among { 0.1 , 0.001 , 0.0005 , 0.0001 } , GRU size m ? { 100 , 160 , 230 , 400 } , position embedding size l ? { 5 , 10 , 15 , 20}. shows all parameters for our task .","grid search approach
to select
optimal learning rate lr
for
Adam optimizer
among
0.1 , 0.001 , 0.0005 , 0.0001
GRU size m ?
100 , 160 , 230 , 400
position embedding size l ?","grid search approach||to select||position embedding size l ?
grid search approach||to select||GRU size m ?
grid search approach||to select||optimal learning rate lr
optimal learning rate lr||among||0.1 , 0.001 , 0.0005 , 0.0001
optimal learning rate lr||for||Adam optimizer
","GRU size m ?||has||100 , 160 , 230 , 400
",,"Experimental setup||has||grid search approach
",,,,,"position embedding size l ?||has||5 , 10 , 15 , 20
"
experimental-setup,GRU size m 230,"GRU size m
230",,,"Experimental setup||GRU size m||230
",,,,,,
experimental-setup,Word embedding dimension k 50 POS embedding dimension l 5 Batch size n 50 Entity - Task weights ( ?,"Word embedding dimension k
50
POS embedding dimension l
5
Batch size n
50
Entity - Task weights",,,"Experimental setup||Word embedding dimension k||50
Experimental setup||POS embedding dimension l||5
Experimental setup||Batch size n||50
",,,,,,"Experimental setup||Entity - Task weights||0.5
"
experimental-setup,"head , ? tail ) 0.5,0.5 Entity - Relation Task weight ?",Entity - Relation Task weight,,,,,,,,,"Experimental setup||Entity - Relation Task weight||0.3
"
experimental-setup,0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ?,"0.3
Learning rate lr
0.001
Dropout probability p
0.5
l 2 penalty",,,"Experimental setup||Learning rate lr||0.001
Experimental setup||Dropout probability p||0.5
",,,,,,"Experimental setup||l 2 penalty||0.0001
"
experimental-setup,0.0001,0.0001,,,,,,,,,
results,"From , we can observe that the model with the STP performs best , and the SDP model obtains an even worse result than the pure one .","observe
model
with
STP
performs
best
SDP model
obtains
even worse result
than
pure one","SDP model||obtains||even worse result
even worse result||than||pure one
model||with||STP
model||performs||best
",,"Results||observe||model
","Results||has||SDP model
",,,,,
results,"The PR curve areas of BGRU + SDP and BGRU are about 0.332 and 0.337 respectively , while BGRU + STP increases it to 0.366 .","PR curve areas
of
BGRU + SDP and BGRU
are about
0.332 and 0.337
BGRU + STP
increases it to
0.366","PR curve areas||of||BGRU + SDP and BGRU
BGRU + SDP and BGRU||are about||0.332 and 0.337
PR curve areas||of||BGRU + STP
BGRU + STP||increases it to||0.366
",,,"Results||has||PR curve areas
",,,,,
results,The result indicates : ( 1 ) Our STP can get rid of irrelevant words in each instance and obtain more precise sentence representation for relation extraction .,"Our STP
get rid of
irrelevant words
in
each instance
obtain
more precise sentence representation","Our STP||obtain||more precise sentence representation
Our STP||get rid of||irrelevant words
irrelevant words||in||each instance
",,,"Results||has||Our STP
",,,,,
results,( 2 ) The SDP method is not appropriate to handle low - quality sentences where key relation words are not in the SDP .,"SDP method
not appropriate to handle
low - quality sentences
where
key relation words
not in
SDP","SDP method||not appropriate to handle||low - quality sentences
low - quality sentences||where||key relation words
key relation words||not in||SDP
",,,"Results||has||SDP method
",,,,,
results,"From and , we can obtain : ( 1 ) Regardless of the dataset that we employ , BGRU - WLA ( + STP ) + EWA outperforms BGRU (+ STP ) .","BGRU - WLA ( + STP ) + EWA
outperforms
BGRU (+ STP )","BGRU - WLA ( + STP ) + EWA||outperforms||BGRU (+ STP )
",,,"Results||has||BGRU - WLA ( + STP ) + EWA
",,,,,
results,"To be more specific , the PR curve area has a relative improvement of over 2.3 % , which demonstrates that entity - wise hidden states in the BGRU present more precise relational features than other word states .","PR curve area
relative improvement
of
over 2.3 %","relative improvement||of||over 2.3 %
","PR curve area||has||relative improvement
",,"Results||has||PR curve area
",,,,,
results,"EWA achieves further improvements and outperforms the baseline by over 4.6 % , because it considers more information than entity or relational words alone .","EWA
achieves
further improvements
outperforms
baseline
by over
4.6 %","EWA||achieves||further improvements
EWA||outperforms||baseline
baseline||by over||4.6 %
",,,"Results||has||EWA
",,,,,
results,"( 1 ) Regardless of the dataset that we use , models with TL achieve better performance , which improve the PR curve area by over 4.7 % .","models with TL
achieve
better performance
improve
PR curve area
by
over 4.7 %","models with TL||achieve||better performance
models with TL||improve||PR curve area
PR curve area||by||over 4.7 %
",,,"Results||has||models with TL
",,,,,
results,"( 2 ) BGRU + STP + TL achieves the best performance and increases the area to 0.383 , while areas of BGRU , BGRU + STP and BGRU + TL are 0.337 , 0.366 and 0.372 respectively .","BGRU + STP + TL
achieves
best performance
increases
area
to
0.383","BGRU + STP + TL||increases||area
area||to||0.383
BGRU + STP + TL||achieves||best performance
",,,"Results||has||BGRU + STP + TL
",,,,,
baselines,Mintz proposes the humandesigned feature model .,"Mintz
proposes
humandesigned feature model","Mintz||proposes||humandesigned feature model
",,,"Baselines||has||Mintz
",,,,,
baselines,MultiR puts forward a graphical model .,"MultiR
puts forward
graphical model","MultiR||puts forward||graphical model
",,,"Baselines||has||MultiR
",,,,,
baselines,MIML proposes a multi -instance multi-label model .,"MIML
proposes
multi -instance multi-label model","MIML||proposes||multi -instance multi-label model
",,,"Baselines||has||MIML
",,,,,
baselines,PCNN puts forward a piecewise CNN for relation extraction .,"PCNN
puts forward
piecewise CNN for relation extraction","PCNN||puts forward||piecewise CNN for relation extraction
",,,"Baselines||has||PCNN
",,,,,
baselines,PCNN + ATT proposes the selective attention mechanism with PCNN .,"PCNN + ATT
proposes
selective attention mechanism
with
PCNN","PCNN + ATT||proposes||selective attention mechanism
selective attention mechanism||with||PCNN
",,,"Baselines||has||PCNN + ATT
",,,,,
baselines,BGRU proposes a BGRU with the word - level attention mechanism .,"BGRU
proposes
with
word - level attention mechanism","BGRU||with||word - level attention mechanism
",,"Baselines||proposes||BGRU
",,,,,,
research-problem,Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,Distant Supervision for Relation Extraction,,,,,"Contribution||has research problem||Distant Supervision for Relation Extraction
",,,,
research-problem,"In relation extraction , one challenge that is faced when building a machine learning system is the generation of training examples .",relation extraction,,,,,"Contribution||has research problem||relation extraction
",,,,
model,"In this paper , we propose a novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ) with multi-instance learning to address the two problems described above .","propose
novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs )
with
multi-instance learning","novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs )||with||multi-instance learning
",,"Model||propose||novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs )
",,,,,,
research-problem,"To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies .","distant supervised relation extraction
treated as
multi-instance problem","distant supervised relation extraction||treated as||multi-instance problem
",,,"Model||has||distant supervised relation extraction
","Contribution||has research problem||distant supervised relation extraction
",,,,
model,We design an objective function at the bag level .,"design
objective function
at
bag level","objective function||at||bag level
",,"Model||design||objective function
",,,,,,
model,"In the learning process , the uncertainty of instance labels can be taken into account ; this alleviates the wrong label problem .","In
learning process
uncertainty
of
instance labels
alleviates
wrong label problem","uncertainty||alleviates||wrong label problem
uncertainty||of||instance labels
","learning process||has||uncertainty
","Model||In||learning process
",,,,,,
model,"To address the second problem , we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by .","adopt
convolutional architecture
to automatically learn
relevant features
without
complicated NLP preprocessing","convolutional architecture||to automatically learn||relevant features
relevant features||without||complicated NLP preprocessing
",,"Model||adopt||convolutional architecture
",,,,,,
model,"To capture structural and other latent information , we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer .","To capture
structural and other latent information
divide
convolution results
into
three segments
based on
positions
of
two given entities
devise
piecewise max pooling layer
instead of
single max pooling layer","structural and other latent information||divide||convolution results
convolution results||into||three segments
three segments||based on||positions
positions||of||two given entities
structural and other latent information||devise||piecewise max pooling layer
piecewise max pooling layer||instead of||single max pooling layer
",,"Model||To capture||structural and other latent information
",,,,,,
model,The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence .,"piecewise max pooling procedure
returns
maximum value
in
each segment
instead of
single maximum value
over
entire sentence","piecewise max pooling procedure||returns||maximum value
maximum value||in||each segment
each segment||instead of||single maximum value
single maximum value||over||entire sentence
",,,"Model||has||piecewise max pooling procedure
",,,,,
experimental-setup,"In this paper , we use the Skip - gram model ( word2 vec ) 5 to train the word embeddings on the NYT corpus .","use
Skip - gram model ( word2 vec )
to train
word embeddings
on
NYT corpus","Skip - gram model ( word2 vec )||to train||word embeddings
word embeddings||on||NYT corpus
",,"Experimental setup||use||Skip - gram model ( word2 vec )
",,,,,,
experimental-setup,"Following , we tune all of the models using three - fold validation on the training set .","tune
models
using
three - fold validation
on
training set","models||using||three - fold validation
three - fold validation||on||training set
",,"Experimental setup||tune||models
",,,,,,
experimental-setup,"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .","grid search
to determine
optimal parameters
manually specify
subsets
of
parameter spaces
w ? { 1 , 2 , 3 , , 7 }","grid search||to determine||optimal parameters
grid search||manually specify||subsets
subsets||of||parameter spaces
","parameter spaces||has||w ? { 1 , 2 , 3 , , 7 }
",,"Experimental setup||use||grid search
",,,,,"parameter spaces||has||n ? { 50 , 60 , , 300}
"
experimental-setup,"Because the position dimension has little effect on the result , we heuristically choose d p = 5 .","heuristically choose
d p
=
5","d p||=||5
",,"Experimental setup||heuristically choose||d p
",,,,,,
experimental-setup,The batch size is fixed to 50 .,"batch size
fixed to
50","batch size||fixed to||50
",,,"Experimental setup||has||batch size
",,,,,
experimental-setup,"We use Adadelta in the update procedure ; it relies on two main parameters , ? and ? , which do not significantly affect the performance .","Adadelta
in
update procedure","Adadelta||in||update procedure
",,,"Experimental setup||use||Adadelta
",,,,,
experimental-setup,"In the dropout operation , we randomly set the hidden unit activities to zero with a probability of 0.5 during training .","In
dropout operation
randomly set
hidden unit activities
to
zero
with
probability
of
0.5
during
training","dropout operation||randomly set||hidden unit activities
hidden unit activities||with||probability
probability||of||0.5
hidden unit activities||to||zero
hidden unit activities||during||training
",,"Experimental setup||In||dropout operation
",,,,,,
baselines,Mintz represents a traditional distantsupervision - based model that was proposed by .,"Mintz
represents
traditional distantsupervision - based model","Mintz||represents||traditional distantsupervision - based model
",,,"Baselines||has||Mintz
",,,,,
baselines,MultiR is a multi-instance learning method that was proposed by .,"MultiR
is
multi-instance learning method","MultiR||is||multi-instance learning method
",,,"Baselines||has||MultiR
",,,,,
baselines,MIML is a multi-instance multilabel model that was proposed by .,"MIML
is
multi-instance multilabel model","MIML||is||multi-instance multilabel model
",,,"Baselines||has||MIML
",,,,,
results,"shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and demonstrates that PCNNs + MIL achieves higher precision over the entire range of recall .","PCNNs + MIL
demonstrates
achieves
higher precision
over
entire range of recall","PCNNs + MIL||achieves||higher precision
higher precision||over||entire range of recall
",,"Results||demonstrates||PCNNs + MIL
",,,,,,
results,PCNNs + MIL enhances the recall to ap - proximately 34 % without any loss of precision .,"PCNNs + MIL
enhances
recall
to
ap - proximately 34 %
without any loss of
precision","PCNNs + MIL||enhances||recall
recall||to||ap - proximately 34 %
PCNNs + MIL||without any loss of||precision
",,,"Results||has||PCNNs + MIL
",,,,,
results,"In terms of both precision and recall , PCNNs + MIL outperforms all other evaluated approaches .","In terms of both
precision and recall
PCNNs + MIL
outperforms
all other evaluated approaches","PCNNs + MIL||outperforms||all other evaluated approaches
","precision and recall||has||PCNNs + MIL
","Results||In terms of both||precision and recall
",,,,,,
results,Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction .,"Automatically learning
features
via
PCNNs
alleviate
error propagation","features||via||PCNNs
PCNNs||alleviate||error propagation
",,"Results||Automatically learning||features
",,,,,,
results,Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem .,"Incorporating
multi-instance learning
into
convolutional neural network
effective means of addressing
wrong label problem","multi-instance learning||into||convolutional neural network
convolutional neural network||effective means of addressing||wrong label problem
",,"Results||Incorporating||multi-instance learning
",,,,,,
research-problem,Relation Classification via Multi - Level Attention CNNs,Relation Classification,,,,,"Contribution||has research problem||Relation Classification
",,,,
model,We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches .,"propose
novel CNN architecture",,,"Model||propose||novel CNN architecture
",,,,,,
model,"Our CNN architecture relies on a novel multi-level attention mechanism to capture both entity - specific attention ( primary attention at the input level , with respect to the target entities ) and relation - specific pooling attention ( secondary attention with respect to the target relations ) .","CNN architecture
relies on
novel multi-level attention mechanism
to capture
entity - specific attention
primary attention
at
input level
with respect to
target entities
relation - specific pooling attention
secondary attention
with respect to
target relations","CNN architecture||relies on||novel multi-level attention mechanism
novel multi-level attention mechanism||to capture||entity - specific attention
primary attention||at||input level
input level||with respect to||target entities
novel multi-level attention mechanism||to capture||relation - specific pooling attention
secondary attention||with respect to||target relations
","entity - specific attention||has||primary attention
relation - specific pooling attention||has||secondary attention
",,"Model||has||CNN architecture
",,,,,
model,2 . We introduce a novel pair - wise margin - based objective function that proves superior to standard loss functions .,"introduce
novel pair - wise margin - based objective function
proves
superior
to
standard loss functions","novel pair - wise margin - based objective function||proves||superior
superior||to||standard loss functions
",,"Model||introduce||novel pair - wise margin - based objective function
",,,,,,
experimental-setup,We use the word2 vec skip - gram model to learn initial word representations on Wikipedia .,"use
word2 vec skip - gram model
to learn
initial word representations
on
Wikipedia","word2 vec skip - gram model||to learn||initial word representations
initial word representations||on||Wikipedia
",,"Experimental setup||use||word2 vec skip - gram model
",,,,,,
experimental-setup,Other matrices are initialized with random values following a Gaussian distribution .,"Other matrices
initialized with
random values
following
Gaussian distribution","Other matrices||initialized with||random values
random values||following||Gaussian distribution
",,,"Experimental setup||has||Other matrices
",,,,,
experimental-setup,We apply a cross-validation procedure on the training data to select suitable hyperparameters .,"apply
cross-validation procedure
on
training data
to select
suitable hyperparameters","cross-validation procedure||to select||suitable hyperparameters
cross-validation procedure||on||training data
",,"Experimental setup||apply||cross-validation procedure
",,,,,,
results,We observe that our novel attentionbased architecture achieves new state - of - the - art results on this relation classification dataset .,"observe that
novel attentionbased architecture
achieves
new state - of - the - art results","novel attentionbased architecture||achieves||new state - of - the - art results
",,"Results||observe that||novel attentionbased architecture
",,,,,,
results,"Att - Input - CNN relies only on the primal attention at the input level , performing standard max - pooling after the convolution layer to generate the network output w O , in which the new objective function is utilized .","Att - Input - CNN
relies only on
primal attention
at
input level
performing
standard max - pooling
after
convolution layer
to generate
network output w O","Att - Input - CNN||relies only on||primal attention
primal attention||at||input level
Att - Input - CNN||performing||standard max - pooling
standard max - pooling||after||convolution layer
convolution layer||to generate||network output w O
",,,"Results||has||Att - Input - CNN
",,,,,
results,"With Att - Input - CNN , we achieve an F1-score of 87.5 % , thus already outperforming not only the original winner of the SemEval task , an SVM - based approach ( 82.2 % ) , but also the wellknown CR - CNN model ( 84.1 % ) with a relative improvement of 4.04 % , and the newly released DRNNs ( 85.8 % ) with a relative improvement of 2.0 % , although the latter approach depends on the Stanford parser to obtain dependency parse information .","With
Att - Input - CNN
achieve
F1-score
of
87.5 %
outperforming
an SVM - based approach ( 82.2 % )
wellknown CR - CNN model ( 84.1 % )
newly released DRNNs ( 85.8 % )","Att - Input - CNN||achieve||F1-score
F1-score||of||87.5 %
87.5 %||outperforming||an SVM - based approach ( 82.2 % )
87.5 %||outperforming||wellknown CR - CNN model ( 84.1 % )
87.5 %||outperforming||newly released DRNNs ( 85.8 % )
",,"Results||With||Att - Input - CNN
",,,,,,
results,Our full dual attention model Att - Pooling - CNN achieves an even more favorable F1- score of 88 % .,"Our full dual attention model Att - Pooling - CNN
achieves
even more favorable F1- score
of
88 %","Our full dual attention model Att - Pooling - CNN||achieves||even more favorable F1- score
even more favorable F1- score||of||88 %
",,,"Results||has||Our full dual attention model Att - Pooling - CNN
",,,,,
ablation-analysis,"To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .","model
evaluating
several simplified models",,,"Ablation analysis||evaluating||several simplified models
",,,,,,"several simplified models||has||first simplification
several simplified models||has||third
several simplified models||has||second
"
ablation-analysis,The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .,"first simplification
to use
without
input attention mechanism
with
pooling attention layer",,,,,,"model||with||pooling attention layer
model||without||input attention mechanism
","first simplification||to use||model
",,
ablation-analysis,The second removes both attention mechanisms .,"second
removes
both attention mechanisms","second||removes||both attention mechanisms
",,,,,,,,
ablation-analysis,The third removes both forms of attention and additionally uses a regular objective function based on the inner product s = r w for a sentence representation r and relation class embedding w.,"third
removes
both forms of attention
uses
regular objective function
based on
inner product s = r w
for
sentence representation r","third||uses||regular objective function
regular objective function||based on||inner product s = r w
inner product s = r w||for||sentence representation r
third||removes||both forms of attention
",,,,,,"inner product s = r w||for||relation class embedding w
",,
ablation-analysis,We observe that all three of our components lead to noticeable improvements over these baselines .,"observe that
all three of our components
lead to
noticeable improvements
over
baselines","all three of our components||lead to||noticeable improvements
noticeable improvements||over||baselines
",,"Ablation analysis||observe that||all three of our components
",,,,,,
research-problem,"We introduce the Self - Annotated Reddit Corpus ( SARC ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection .",sarcasm detection,,,,,"Contribution||has research problem||sarcasm detection
",,,,
dataset,"In this work , we make available the first corpus 1 for sarcasm detection that has both unbalanced and self - annotated labels and does not consist of short text snippets from Twitter 2 .","make available
first corpus
for
sarcasm detection
unbalanced and self - annotated labels","first corpus||for||sarcasm detection
","first corpus||has||unbalanced and self - annotated labels
","Dataset||make available||first corpus
",,,,,,
dataset,"With more than a million examples of sarcastic statements , each provided with author , topic , and contex information , the dataset exceeds all previous sarcasm corpora by an order of magnitude in size .","With
more than a million examples
of
sarcastic statements
each provided with
author , topic , and contex information
exceeds
previous sarcasm corpora
by
an order of magnitude in size","more than a million examples||exceeds||previous sarcasm corpora
previous sarcasm corpora||by||an order of magnitude in size
more than a million examples||of||sarcastic statements
sarcastic statements||each provided with||author , topic , and contex information
",,"Dataset||With||more than a million examples
",,,,,,
code,Code to reproduce our results is provided at https://github.com/NLPrinceton/,https://github.com/NLPrinceton/,,,,,"Contribution||Code||https://github.com/NLPrinceton/
",,,,
results,"The baselines in perform reasonably well and much better than the random baseline , but none of them match human performance on either dataset .",,,,,,,,,,
results,"There is clear scope for improvement for machine learning methods , starting with the use of context provided to make better decisions about sarcasm .","clear scope
for
improvement
machine learning methods","clear scope||for||improvement
clear scope||for||machine learning methods
",,,,,,,"Baselines||has||clear scope
",
research-problem,CASCADE : Contextual Sarcasm Detection in Online Discussion Forums,Contextual Sarcasm Detection,,,,,"Contribution||has research problem||Contextual Sarcasm Detection
",,,,
research-problem,"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .",automated sarcasm detection,,,,,"Contribution||has research problem||automated sarcasm detection
",,,,
research-problem,"In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .",sarcasm detection,,,,,"Contribution||has research problem||sarcasm detection
",,,,
model,"Particularly , we propose a hybrid network , named CASCADE , that utilizes both content and contextual - information required for sarcasm detection .","propose
hybrid network
named
CASCADE
utilizes
content and contextual - information
required for
sarcasm detection","hybrid network||named||CASCADE
hybrid network||utilizes||content and contextual - information
content and contextual - information||required for||sarcasm detection
",,"Model||propose||hybrid network
",,,,,,
model,"First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .","performs
user profiling
to create
user embeddings
that capture
indicative behavioral traits
for
sarcasm","user profiling||to create||user embeddings
user embeddings||that capture||indicative behavioral traits
indicative behavioral traits||for||sarcasm
",,"Model||performs||user profiling
",,,,,,
model,"It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .","makes use of
users ' historical posts
to model
writing style ( stylometry ) and personality indicators
fused into
comprehensive user embeddings
using
multi-view fusion approach
Canonical Correlation Analysis ( CCA )","users ' historical posts||to model||writing style ( stylometry ) and personality indicators
writing style ( stylometry ) and personality indicators||fused into||comprehensive user embeddings
comprehensive user embeddings||using||multi-view fusion approach
","multi-view fusion approach||name||Canonical Correlation Analysis ( CCA )
","Model||makes use of||users ' historical posts
",,,,,,
model,"Second , it extracts contextual information from the discourse of comments in the discussion forums .","extracts
contextual information
from
discourse
of
comments
in
discussion forums","contextual information||from||discourse
discourse||in||discussion forums
discourse||of||comments
",,"Model||extracts||contextual information
",,,,,,
model,This is done by document modeling of these consolidated comments belonging to the same forum .,"done by
document modeling
of
consolidated comments
belonging to
same forum","document modeling||of||consolidated comments
consolidated comments||belonging to||same forum
",,"Model||done by||document modeling
",,,,,,
model,"After the contextual modeling phase , CASCADE is provided with a comment for sarcasm detection .","After
contextual modeling phase
CASCADE
provided with
comment
for
sarcasm detection","CASCADE||provided with||comment
comment||for||sarcasm detection
","contextual modeling phase||has||CASCADE
","Model||After||contextual modeling phase
",,,,,,
model,It performs content - modeling using a Convolutional Neural Network ( CNN ) to extract its syntactic features .,"content - modeling
using
Convolutional Neural Network ( CNN )
to extract
syntactic features","content - modeling||using||Convolutional Neural Network ( CNN )
Convolutional Neural Network ( CNN )||to extract||syntactic features
",,,"Model||performs||content - modeling
",,,,,
model,This CNN representation is then concatenated with the relevant user embedding and discourse features to get the final representation which is used for classification .,"CNN representation
concatenated with
relevant user embedding and discourse features
to get
final representation
used for
classification","CNN representation||concatenated with||relevant user embedding and discourse features
CNN representation||to get||final representation
final representation||used for||classification
",,,"Model||has||CNN representation
",,,,,
hyperparameters,We holdout 10 % of the training data for validation .,"holdout
10 %
of
training data
for
validation","10 %||of||training data
10 %||for||validation
",,"Hyperparameters||holdout||10 %
",,,,,,
hyperparameters,"To optimize the parameters , Adam optimizer ( Kingma and Ba , 2014 ) is used , starting with an initial learning rate of 1e ? 4 .","To optimize
parameters
Adam optimizer
starting with
initial learning rate
of
1e ? 4","Adam optimizer||starting with||initial learning rate
initial learning rate||of||1e ? 4
","parameters||has||Adam optimizer
","Hyperparameters||To optimize||parameters
",,,,,,
hyperparameters,Training termination is decided using early stopping technique with a patience of 12 .,"Training termination
decided using
early stopping technique
with
patience
of
12","Training termination||decided using||early stopping technique
early stopping technique||with||patience
patience||of||12
",,,"Hyperparameters||has||Training termination
",,,,,
hyperparameters,"For the batched - modeling of comments in CNNs , each comment is either restricted or padded to 100 words for uniformity .","For
batched - modeling
of
comments
in
CNNs
each comment
is
restricted or padded
to
100 words
for
uniformity","batched - modeling||of||comments
comments||in||CNNs
each comment||is||restricted or padded
restricted or padded||for||uniformity
restricted or padded||to||100 words
","comments||has||each comment
","Hyperparameters||For||batched - modeling
",,,,,,
baselines,Bag - of - Words :,Bag - of - Words,,,,"Baselines||has||Bag - of - Words
",,,,,
baselines,This model uses a comment 's word - counts as features in a vector .,"uses
comment 's
word - counts
as
features
in
vector","word - counts||as||features
features||in||vector
","comment 's||has||word - counts
",,,,"Bag - of - Words||uses||comment 's
",,,
baselines,CNN : We compare our model with this individual CNN version .,"CNN
individual CNN version",,"CNN||has||individual CNN version
",,"Baselines||has||CNN
",,,,,
baselines,CNN - SVM :,CNN - SVM,,,,"Baselines||has||CNN - SVM
",,,,,
baselines,"This model proposed by consists of a CNN for content modeling and other pre-trained CNNs for extracting sentiment , emotion and personality features from the given comment .","consists of
CNN
for
content modeling
other pre-trained CNNs
for extracting
sentiment , emotion and personality features
from
given comment","CNN||for||content modeling
other pre-trained CNNs||for extracting||sentiment , emotion and personality features
sentiment , emotion and personality features||from||given comment
",,,,,"CNN - SVM||consists of||CNN
CNN - SVM||consists of||other pre-trained CNNs
",,,
baselines,CUE - CNN :,CUE - CNN,,,,"Baselines||has||CUE - CNN
",,,,,
baselines,This method proposed by also models user embeddings with a method akin to ParagraphVector .,"models
user embeddings
method akin to
ParagraphVector","user embeddings||method akin to||ParagraphVector
",,,,,"CUE - CNN||models||user embeddings
",,,
results,CASCADE manages to achieve major improvement across all datasets with statistical significance .,"CASCADE
achieve
major improvement
across
all datasets
with
statistical significance","CASCADE||achieve||major improvement
major improvement||across||all datasets
major improvement||with||statistical significance
",,,"Results||has||CASCADE
",,,,,
results,The lowest performance is obtained by the Bag - of - words approach whereas all neural architectures outperform it .,"lowest performance
obtained by
Bag - of - words approach","lowest performance||obtained by||Bag - of - words approach
",,,"Results||has||lowest performance
",,,,,
results,"Amongst the neural networks , the CNN baseline receives the least performance .","Amongst
neural networks
CNN baseline
receives
least performance","CNN baseline||receives||least performance
","neural networks||has||CNN baseline
","Results||Amongst||neural networks
",,,,,,
results,CASCADE comfortably beats the state - of - the - art neural models CNN - SVM and CUE - CNN .,"comfortably beats
state - of - the - art neural models
CNN - SVM
CUE - CNN",,"state - of - the - art neural models||name||CNN - SVM
state - of - the - art neural models||name||CUE - CNN
",,,,"CASCADE||comfortably beats||state - of - the - art neural models
",,,
results,Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .,"improved performance
on
Main imbalanced dataset
reflects
robustness
towards
class imbalance
establishes it as
real - world deployable network","improved performance||on||Main imbalanced dataset
Main imbalanced dataset||establishes it as||real - world deployable network
Main imbalanced dataset||reflects||robustness
robustness||towards||class imbalance
",,,"Results||has||improved performance
",,,,,
results,"Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .","CUE - CNN
generates
user embeddings
using
method
similar to
ParagraphVector","CUE - CNN||generates||user embeddings
user embeddings||using||method
method||similar to||ParagraphVector
",,,"Results||has||CUE - CNN
",,,,,
ablation-analysis,"First , we test performance for the content - based CNN only ( row 1 ) .","test
performance
for
content - based CNN only","performance||for||content - based CNN only
",,"Ablation analysis||test||performance
",,,,,,
ablation-analysis,This setting provides the worst relative performance with almost 10 % lesser accuracy than optimal .,"provides
worst relative performance
with
almost 10 % lesser accuracy
than
optimal","worst relative performance||with||almost 10 % lesser accuracy
almost 10 % lesser accuracy||than||optimal
",,,,,"content - based CNN only||provides||worst relative performance
",,,
ablation-analysis,"Next , we include contextual features to this network .","include
contextual features",,,,,,"content - based CNN only||include||contextual features
",,,"contextual features||has||effect
"
ablation-analysis,"Here , the effect of discourse features is primarily seen in the Pol dataset getting an increase of 3 % in F1 ( row 2 ) .","effect
of
discourse features
primarily seen in
Pol dataset
getting
increase
of
3 % in F1","effect||of||discourse features
discourse features||getting||increase
increase||of||3 % in F1
discourse features||primarily seen in||Pol dataset
",,,,,,,,
ablation-analysis,A major boost in performance is observed ( 8 ? 12 % accuracy and F1 ) when user embeddings are introduced ( row 5 ) .,"major boost
in
performance
observed
user embeddings
introduced","user embeddings||observed||major boost
major boost||in||performance
",,,,,"content - based CNN only||introduced||user embeddings
",,,
ablation-analysis,"Overall , CASCADE consisting of CNN with user embeddings and contextual discourse features provide the best performance in all three datasets ( row 6 ) .","CASCADE
consisting of
CNN
with
user embeddings and contextual discourse features
provide
best performance
in
all three datasets","CASCADE||consisting of||CNN
CNN||with||user embeddings and contextual discourse features
user embeddings and contextual discourse features||provide||best performance
best performance||in||all three datasets
",,,"Ablation analysis||has||CASCADE
",,,,,
ablation-analysis,We challenge the use of CCA for the generation of user embeddings and thus replace it with simple concatenation .,"use of
CCA
for
generation
of
user embeddings
replace it with
simple concatenation","CCA||for||generation
generation||of||user embeddings
user embeddings||replace it with||simple concatenation
",,"Ablation analysis||use of||CCA
",,,,,,
ablation-analysis,This however causes a significant drop in performance ( row 3 ) .,"causes
significant drop
in
performance","significant drop||in||performance
",,,,,"simple concatenation||causes||significant drop
",,,
research-problem,Spider : A Large - Scale Human - Labeled Dataset for Complex and Cross - Domain Semantic Parsing and Text - to - SQL Task,"Complex and Cross - Domain Semantic Parsing
Text - to - SQL",,,,,"Contribution||has research problem||Complex and Cross - Domain Semantic Parsing
Contribution||has research problem||Text - to - SQL
",,,,
code,Our dataset and task are publicly available at https://yale-lily. github.io/ spider .,https://yale-lily. github.io/ spider,,,,,"Contribution||Code||https://yale-lily. github.io/ spider
",,,,
research-problem,Semantic parsing ( SP ) is one of the most important tasks in natural language processing ( NLP ) .,Semantic parsing ( SP ),,,,,"Contribution||has research problem||Semantic parsing ( SP )
",,,,
research-problem,Existing datasets for SP have two shortcomings .,SP,,,,,"Contribution||has research problem||SP
",,,,
dataset,"To address the need for a large and high - quality dataset for a new complex and cross-domain semantic parsing task , we introduce Spider , which consists of 200 databases with multiple tables , 10,181 questions , and 5,693 corresponding complex SQL queries , all written by 11 college students spending a total of 1,000 man-hours .","for
new complex and cross-domain semantic parsing task
introduce
Spider
consists of
200 databases
with
multiple tables
10,181 questions
5,693 corresponding complex SQL queries","new complex and cross-domain semantic parsing task||introduce||Spider
Spider||consists of||200 databases
200 databases||with||multiple tables
200 databases||with||10,181 questions
200 databases||with||5,693 corresponding complex SQL queries
",,"Dataset||for||new complex and cross-domain semantic parsing task
",,,,,,
dataset,"As illustrates , given a database with multiple tables including foreign keys , our corpus creates and annotates complex questions and SQL queries including different SQL clauses such as joining and nested query .","including
creates and annotates
questions
SQL queries
different SQL clauses
such as
joining and nested query","SQL queries||including||different SQL clauses
different SQL clauses||such as||joining and nested query
",,"Dataset||creates and annotates||questions
Dataset||creates and annotates||SQL queries
",,,,,,
results,"The performances of the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are very low .","performances
of
Seq2Seq - based basic models
including
Seq2Seq
Seq2Seq + Attention
Seq2Seq + Copying
are
very low","performances||of||Seq2Seq - based basic models
Seq2Seq - based basic models||including||Seq2Seq
Seq2Seq - based basic models||including||Seq2Seq + Attention
Seq2Seq - based basic models||including||Seq2Seq + Copying
Seq2Seq - based basic models||are||very low
",,,"Results||has||performances
",,,,,
results,"In contrast , SQLNet and TypeSQL that utilize SQL structure information to guide the SQL generation process significantly outperform other Seq2Seq models .","SQLNet and TypeSQL
utilize
SQL structure information
to guide
SQL generation process
significantly outperform
other Seq2Seq models","SQLNet and TypeSQL||utilize||SQL structure information
SQL structure information||to guide||SQL generation process
SQLNet and TypeSQL||significantly outperform||other Seq2Seq models
",,,,,,,"performances||of||SQLNet and TypeSQL
",
results,"As Component Matching results in shows , all models struggle with WHERE clause prediction the most .","all models
struggle with
WHERE clause prediction","all models||struggle with||WHERE clause prediction
",,,"Results||has||all models
",,,,,
results,"In general , the over all performances of all models are low , indicating that our task is challenging and there is still a large room for improvement .","over all performances
of
all models
are
low","over all performances||of||all models
all models||are||low
",,,"Results||has||over all performances
",,,,,
research-problem,TRANX : A Transition - based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation,Semantic Parsing and Code Generation,,,,,"Contribution||has research problem||Semantic Parsing and Code Generation
",,,,
model,"Inspired by this existing research , we have developed TRANX , a TRANsition - based abstract syntaX parser for semantic parsing and code generation .","developed
TRANX
TRANsition - based abstract syntaX parser
for
semantic parsing and code generation","TRANX||for||semantic parsing and code generation
","TRANX||name||TRANsition - based abstract syntaX parser
","Model||developed||TRANX
",,,,,,
model,TRANX is designed with the following principles in mind :,"TRANX
designed with
following principles","TRANX||designed with||following principles
",,,"Model||has||TRANX
",,,,,"following principles||has||Generalization ability
"
model,"Generalization ability TRANX employs ASTs as a general - purpose intermediate meaning representation , and the task - dependent grammar is provided to the system as external knowledge to guide the parsing process , therefore decoupling the semantic parsing procedure with specificities of grammars .","Generalization ability
employs
ASTs
as
general - purpose intermediate meaning representation
task - dependent grammar
provided to
system
as
external knowledge
to guide
parsing process","Generalization ability||employs||ASTs
ASTs||as||general - purpose intermediate meaning representation
task - dependent grammar||provided to||system
system||as||external knowledge
external knowledge||to guide||parsing process
","Generalization ability||has||task - dependent grammar
",,,,,,,
model,Extensibility TRANX uses a simple transition system to parse NL utterances into tree -,"Extensibility
uses
simple transition system
to parse
NL utterances","Extensibility||uses||simple transition system
simple transition system||to parse||NL utterances
",,,,,,,"following principles||has||Extensibility
",
model,Effectiveness,Effectiveness,,,,,,,,"following principles||has||Effectiveness
",
model,"We test TRANX on four semantic parsing ( ATIS , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .","test
TRANX
on
semantic parsing
ATIS
GEO
code generation
DJANGO
WIKISQL","TRANX||on||code generation
TRANX||on||semantic parsing
","code generation||name||DJANGO
code generation||name||WIKISQL
semantic parsing||name||ATIS
semantic parsing||name||GEO
",,,,"Effectiveness||test||TRANX
",,,
results,Semantic Parsing Tab.,Semantic Parsing,,,,"Results||has||Semantic Parsing
",,,,,"Semantic Parsing||has||Our system
"
results,Our system outperforms existing neural network - based approaches .,"Our system
outperforms
existing neural network - based approaches","Our system||outperforms||existing neural network - based approaches
",,,,,,,,
results,"Interestingly , we found the model without parent feeding achieves slightly better accuracy on GEO , probably because that its relative simple grammar does not require extra handling of parent information .","model
without
parent feeding
achieves
slightly better accuracy
on
GEO","model||achieves||slightly better accuracy
slightly better accuracy||on||GEO
model||without||parent feeding
",,,,,,,"Semantic Parsing||has||model
",
results,Code Generation Tab.,Code Generation,,,,"Results||has||Code Generation
",,,,,"Code Generation||has||TRANX
"
results,2 lists the results on DJANGO .,"on
DJANGO",,,,,,"state - of - the - art results||on||DJANGO
",,,
results,TRANX achieves state - of - the - art results on DJANGO .,"TRANX
achieves
state - of - the - art results","TRANX||achieves||state - of - the - art results
",,,,,,,,
results,"We also find parent feeding yields + 1 point gain in accuracy , suggesting the importance of modeling parental connections in ASTs with complex domain grammars ( e.g. , Python ) .","find
parent feeding
yields
+ 1 point gain
in
accuracy","parent feeding||yields||+ 1 point gain
+ 1 point gain||in||accuracy
",,,,,"Code Generation||find||parent feeding
",,,
results,"3 . We find TRANX , although just with simple extensions to adapt to this dataset , achieves impressive results and outperforms many task - specific methods .","TRANX
achieves
impressive results
outperforms
many task - specific methods","TRANX||achieves||impressive results
TRANX||outperforms||many task - specific methods
",,,,,,,"Code Generation||find||TRANX
",
research-problem,Coarse - to - Fine Decoding for Neural Semantic Parsing,Neural Semantic Parsing,,,,,"Contribution||has research problem||Neural Semantic Parsing
",,,,
research-problem,Semantic parsing aims at mapping natural language utterances into structured meaning representations .,Semantic parsing,,,,,"Contribution||has research problem||Semantic parsing
",,,,
model,"In this work , we propose to decompose the decoding process into two stages .","propose to decompose
decoding process
into
two stages","decoding process||into||two stages
",,"Model||propose to decompose||decoding process
",,,,,,
model,"The first decoder focuses on predicting a rough sketch of the meaning representation , which omits low - level details , such as arguments and variable names .","first decoder
focuses on
predicting
rough sketch
of
meaning representation","first decoder||focuses on||predicting
rough sketch||of||meaning representation
","predicting||has||rough sketch
",,"Model||has||first decoder
",,,,,
model,"Then , a second decoder fills in missing details by conditioning on the natural language input and the sketch itself .","second decoder
fills in
missing details
by conditioning on
natural language input
sketch","second decoder||fills in||missing details
missing details||by conditioning on||natural language input
missing details||by conditioning on||sketch
",,,"Model||has||second decoder
",,,,,
model,"Specifically , the sketch constrains the generation process and is encoded into vectors to guide decoding .","sketch
constrains
generation process
encoded into
vectors
to guide
decoding","sketch||encoded into||vectors
vectors||to guide||decoding
sketch||constrains||generation process
",,,"Model||has||sketch
",,,,,
model,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model meaning at different levels of granularity .","decomposition
disentangles
high - level from low - level semantic information
enables
decoders
to model
meaning
at
different levels of granularity","decomposition||disentangles||high - level from low - level semantic information
high - level from low - level semantic information||enables||decoders
decoders||to model||meaning
meaning||at||different levels of granularity
",,,"Model||has||decomposition
",,,,,
model,"Secondly , the model can explicitly share knowledge of coarse structures for the examples that have the same sketch ( i.e. , basic meaning ) , even though their actual meaning representations are different ( e.g. , due to different details ) .","explicitly share
knowledge
of
coarse structures
for
examples that have the same sketch ( i.e. , basic meaning )","knowledge||of||coarse structures
coarse structures||for||examples that have the same sketch ( i.e. , basic meaning )
","explicitly share||has||knowledge
",,"Model||has||explicitly share
",,,,,
model,"Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .","generating
sketch
decoder
knows
basic meaning of the utterance
model
use it
as
global context
to improve
prediction
of
final details","decoder||knows||basic meaning of the utterance
sketch||use it||model
model||as||global context
global context||to improve||prediction
prediction||of||final details
","generating||has||sketch
sketch||has||decoder
",,"Model||has||generating
",,,,,
code,Our implementation and pretrained models are available at https :// github.com/donglixp/coarse2fine.,,,,,,,,,,
hyperparameters,"Dimensions of hidden vectors and word embeddings were selected from { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .","Dimensions
of
hidden vectors and word embeddings
selected from
{ 250 , 300 } and { 150 , 200 , 250 , 300 }","Dimensions||of||hidden vectors and word embeddings
hidden vectors and word embeddings||selected from||{ 250 , 300 } and { 150 , 200 , 250 , 300 }
",,,"Hyperparameters||has||Dimensions
",,,,,
hyperparameters,"The dropout rate was selected from { 0.3 , 0.5 } .","dropout rate
selected from
{ 0.3 , 0.5 }","dropout rate||selected from||{ 0.3 , 0.5 }
",,,"Hyperparameters||has||dropout rate
",,,,,
hyperparameters,Label smoothing was employed for GEO and ATIS .,"Label smoothing
employed for
GEO
ATIS","Label smoothing||employed for||GEO
Label smoothing||employed for||ATIS
",,,"Hyperparameters||has||Label smoothing
",,,,,
hyperparameters,The smoothing parameter was set to 0.1 .,"smoothing parameter
set to
0.1","smoothing parameter||set to||0.1
",,,"Hyperparameters||has||smoothing parameter
",,,,,
hyperparameters,"Word embeddings were initialized by GloVe , and were shared by table encoder and input encoder in Section 4.3 .","Word embeddings
initialized by
GloVe
shared by
table encoder
input encoder","Word embeddings||shared by||table encoder
Word embeddings||shared by||input encoder
Word embeddings||initialized by||GloVe
",,,"Hyperparameters||has||Word embeddings
",,,,,
hyperparameters,We appended 10 - dimensional part - of - speech tag vectors to embeddings of the question words in WIKISQL .,"appended
10 - dimensional part - of - speech tag vectors
to
embeddings
of
question words
in
WIKISQL","10 - dimensional part - of - speech tag vectors||to||embeddings
embeddings||of||question words
question words||in||WIKISQL
",,"Hyperparameters||appended||10 - dimensional part - of - speech tag vectors
",,,,,,
hyperparameters,The part - of - speech tags were obtained by the spaCy toolkit .,"part - of - speech tags
obtained by
spaCy toolkit","part - of - speech tags||obtained by||spaCy toolkit
",,,"Hyperparameters||has||part - of - speech tags
",,,,,
hyperparameters,We used the RMSProp optimizer to train the models .,"used
RMSProp optimizer
to train
models","RMSProp optimizer||to train||models
",,"Hyperparameters||used||RMSProp optimizer
",,,,,,
hyperparameters,"The learning rate was selected from { 0.002 , 0.005 } .","learning rate
selected from
{ 0.002 , 0.005 }","learning rate||selected from||{ 0.002 , 0.005 }
",,,"Hyperparameters||has||learning rate
",,,,,
hyperparameters,"The batch size was 200 for WIKISQL , and was 64 for other datasets .","batch size
was
200
for
WIKISQL
64
for
other datasets","batch size||was||200
200||for||WIKISQL
batch size||was||64
64||for||other datasets
",,,"Hyperparameters||has||batch size
",,,,,
hyperparameters,Early stopping was used to determine the number of epochs .,"Early stopping
to determine
number of epochs","Early stopping||to determine||number of epochs
",,,"Hyperparameters||has||Early stopping
",,,,,
results,"Overall , we observe that COARSE2FINE outperforms ONESTAGE , which suggests that disentangling high - level from low - level information dur - 62.3 SNM + COPY 71 and .","observe
COARSE2FINE
outperforms
ONESTAGE
suggests
disentangling
high - level
from
low - level information","COARSE2FINE||suggests||disentangling
high - level||from||low - level information
COARSE2FINE||outperforms||ONESTAGE
","disentangling||has||high - level
","Results||observe||COARSE2FINE
",,,,,,
results,"Compared with previous neural models that utilize syntax or grammatical information ( SEQ2 TREE , ASN ; the second block in ) , our method performs competitively despite the use of relatively simple decoders .","our method
performs
competitively
despite the use of
relatively simple decoders","our method||performs||competitively
competitively||despite the use of||relatively simple decoders
",,,"Results||has||our method
",,,,,
results,"As can be seen , predicting the sketch correctly boosts performance .","predicting
sketch
correctly
boosts
performance","sketch||boosts||performance
","sketch||has||correctly
","Results||predicting||sketch
",,,,,,
results,Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle .,"sketch encoder
is
beneficial
there is
8.9 point difference
in accuracy between
COARSE2FINE and the oracle","sketch encoder||there is||8.9 point difference
8.9 point difference||in accuracy between||COARSE2FINE and the oracle
sketch encoder||is||beneficial
",,,"Results||observe||sketch encoder
",,,,,
results,Our model is superior to ONESTAGE as well as to previous best performing systems .,"Our model
superior to
ONESTAGE
previous best performing systems","Our model||superior to||ONESTAGE
Our model||superior to||previous best performing systems
",,,"Results||has||Our model
",,,,,
results,"COARSE2FINE 's accuracies on aggregation agg op and agg col are 90.2 % and 92.0 % , respectively , which is comparable to SQLNET .","COARSE2FINE 's accuracies
on
aggregation agg op and agg col
are
90.2 % and 92.0 %
comparable to
SQLNET","COARSE2FINE 's accuracies||on||aggregation agg op and agg col
aggregation agg op and agg col||are||90.2 % and 92.0 %
90.2 % and 92.0 %||comparable to||SQLNET
",,,"Results||has||COARSE2FINE 's accuracies
",,,,,
results,So the most gain is obtained by the improved decoder of the WHERE clause .,"most gain
obtained by
improved decoder
of
WHERE clause","most gain||obtained by||improved decoder
improved decoder||of||WHERE clause
",,,"Results||has||most gain
",,,,,
results,"We also find that a tableaware input encoder is critical for doing well on this task , since the same question might lead to different SQL queries depending on the table schemas .","find
tableaware input encoder
is
critical
for
doing well
on
this task","tableaware input encoder||is||critical
critical||for||doing well
doing well||on||this task
",,"Results||find||tableaware input encoder
",,,,,,
results,Sketches produced by COARSE2FINE are more accurate across the board .,"Sketches
produced by
COARSE2FINE
are
more accurate","Sketches||produced by||COARSE2FINE
COARSE2FINE||are||more accurate
",,,"Results||has||Sketches
",,,,,
results,"On WIKISQL , the sketches predicted by COARSE2FINE are marginally better compared with ONESTAGE .","On
WIKISQL
sketches
predicted by
COARSE2FINE
are
marginally better
compared with
ONESTAGE","sketches||are||marginally better
marginally better||compared with||ONESTAGE
sketches||predicted by||COARSE2FINE
","WIKISQL||has||sketches
","Results||On||WIKISQL
",,,,,,
research-problem,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,Neural Semantic Role Labeling,,,,,"Contribution||has research problem||Neural Semantic Role Labeling
",,,,
research-problem,"Semantic role labeling ( SRL ) captures predicateargument relations , such as "" who did what to whom . """,Semantic role labeling ( SRL ),,,,,"Contribution||has research problem||Semantic role labeling ( SRL )
",,,,
research-problem,"Recent high - performing SRL models are BIO - taggers , labeling argument spans for a single predicate at a time ( as shown in .",SRL,,,,,"Contribution||has research problem||SRL
",,,,
model,We propose an end - to - end approach for predicting all the predicates and their argument spans in one forward pass .,"propose
end - to - end approach
for predicting
all the predicates
argument spans
in
one forward pass","end - to - end approach||in||one forward pass
one forward pass||for predicting||all the predicates
one forward pass||for predicting||argument spans
",,"Model||propose||end - to - end approach
",,,,,,
model,"Our model builds on a recent coreference resolution model , by making central use of learned , contextualized span representations .","builds on
recent coreference resolution model
by making
central
use of
learned , contextualized span representations","recent coreference resolution model||by making||central
central||use of||learned , contextualized span representations
",,"Model||builds on||recent coreference resolution model
",,,,,,
model,We use these representations to predict SRL graphs directly over text spans .,"use
representations
to predict
SRL graphs
directly over
text spans","representations||to predict||SRL graphs
SRL graphs||directly over||text spans
",,"Model||use||representations
",,,,,,
model,"Each edge is identified by independently predicting which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .","Each edge
identified by
independently predicting
which
role
holds between
every possible pair of text spans
https://github.com/luheng/lsgn","Each edge||identified by||independently predicting
independently predicting||which||role
role||holds between||every possible pair of text spans
",,,"Model||has||Each edge
","Contribution||Code||https://github.com/luheng/lsgn
",,,,
model,The final graph is simply the union of predicted SRL roles ( edges ) and their associated text spans ( nodes ) .,"final graph
union of
predicted SRL roles ( edges )
associated text spans ( nodes )","final graph||union of||predicted SRL roles ( edges )
final graph||union of||associated text spans ( nodes )
",,,"Model||has||final graph
",,,,,
model,"The span representations also generalize the token - level representations in BIObased models , letting the model dynamically decide which spans and roles to include , without using previously standard syntactic features .","span representations
generalize
token - level representations
in
BIObased models
letting
model
dynamically decide
which spans and roles to include
without using
previously standard syntactic features","span representations||generalize||token - level representations
token - level representations||in||BIObased models
token - level representations||letting||model
model||dynamically decide||which spans and roles to include
which spans and roles to include||without using||previously standard syntactic features
",,,"Model||has||span representations
",,,,,
model,"To the best of our knowledge , this is the first span - based SRL model that does not assume that predicates are given .","is
first span - based SRL model
that does not assume
predicates are given","first span - based SRL model||that does not assume||predicates are given
",,"Model||is||first span - based SRL model
",,,,,,
results,"As shown in , 2 our joint model outperforms the previous best pipeline system by an F1 difference of anywhere between 1.3 and 6.0 in every setting .","joint model
outperforms
previous best pipeline system
by
F1 difference
of
anywhere between 1.3 and 6.0","joint model||by||F1 difference
F1 difference||of||anywhere between 1.3 and 6.0
joint model||outperforms||previous best pipeline system
",,,"Results||has||joint model
",,,,,
results,"On all datasets , our model is able to predict over 40 % of the sentences completely correctly .","On
all datasets
our model
able to
predict
over
40 %
of
sentences
completely correctly","our model||able to||predict
predict||over||40 %
40 %||of||sentences
","all datasets||has||our model
40 %||has||completely correctly
","Results||On||all datasets
",,,,,,
research-problem,Linguistically - Informed Self - Attention for Semantic Role Labeling,Semantic Role Labeling,,,,,"Contribution||has research problem||Semantic Role Labeling
",,,,
research-problem,Current state - of - the - art semantic role labeling ( SRL ) uses a deep neural network with no explicit linguistic features .,semantic role labeling ( SRL ),,,,,"Contribution||has research problem||semantic role labeling ( SRL )
",,,,
research-problem,"However , prior work has shown that gold syntax trees can dramatically improve SRL decoding , suggesting the possibility of increased accuracy from explicit modeling of syntax .",SRL,,,,,"Contribution||has research problem||SRL
",,,,
model,"In response , we propose linguistically - informed self - attention ( LISA ) : a model that combines multi-task learning with stacked layers of multi-head self - attention ; the model is trained to : ( 1 ) jointly predict parts of speech and predicates ; ( 2 ) perform parsing ; and ( 3 ) attend to syntactic parse parents , while ( 4 ) assigning semantic role labels .","propose
linguistically - informed self - attention ( LISA )
trained to
jointly predict
parts of speech and predicates
perform parsing
attend
to
syntactic parse parents
assigning
semantic role labels","attend||to||syntactic parse parents
","jointly predict||has||parts of speech and predicates
assigning||has||semantic role labels
","Model||trained to||jointly predict
Model||trained to||perform parsing
Model||trained to||attend
Model||trained to||assigning
Model||propose||linguistically - informed self - attention ( LISA )
",,,,,,
model,"Whereas prior work typically requires separate models to provide linguistic analysis , including most syntaxfree neural models which still rely on external predicate detection , our model is truly end - to - end : earlier layers are trained to predict prerequisite parts - of - speech and predicates , the latter of which are supplied to later layers for scoring .","end - to - end
earlier layers
trained to predict
prerequisite parts - of - speech and predicates
latter
supplied to
later layers
for
scoring","earlier layers||trained to predict||prerequisite parts - of - speech and predicates
latter||supplied to||later layers
later layers||for||scoring
","end - to - end||has||earlier layers
end - to - end||has||latter
",,"Model||has||end - to - end
",,,,,
model,"Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL , we more efficiently encode each sentence only once , predict its predicates , part - of - speech tags and labeled syntactic parse , then predict the semantic roles for all predicates in the sentence in parallel .","each sentence
predict
encode
only once
predicates
part - of - speech tags
labeled syntactic parse
then predict
semantic roles","each sentence||predict||predicates
predicates||then predict||semantic roles
each sentence||predict||part - of - speech tags
each sentence||predict||labeled syntactic parse
","each sentence||has||only once
","Model||encode||each sentence
",,,,,,
hyperparameters,"We train the model using Nadam ( Dozat , 2016 ) SGD combined with the learning rate schedule in .","train
model
using
Nadam ( Dozat , 2016 ) SGD","model||using||Nadam ( Dozat , 2016 ) SGD
",,"Hyperparameters||train||model
",,,,,,
hyperparameters,"In addition to MTL , we regularize our model using dropout .","regularize
our model
using
dropout","our model||using||dropout
",,"Hyperparameters||regularize||our model
",,,,,,
hyperparameters,We use gradient clipping to avoid exploding gradients .,"use
gradient clipping
to avoid
exploding gradients","gradient clipping||to avoid||exploding gradients
",,"Hyperparameters||use||gradient clipping
",,,,,,
results,"We present results on the CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0 , achieving state - of - the - art results for a single model with predicted predicates on both corpora .","on
CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0
achieving
state - of - the - art results
for
single model
with
predicted predicates
on
both corpora","CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0||achieving||state - of - the - art results
state - of - the - art results||for||single model
single model||with||predicted predicates
single model||on||both corpora
",,"Results||on||CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0
",,,,,,
results,"We demonstrate that our models benefit from injecting state - of - the - art predicted parses at test time ( + D&M ) by fixing the attention to parses predicted by Dozat and Manning ( 2017 ) , the winner of the 2017 CoNLL shared task which we re-train using ELMo embeddings .","demonstrate that
our models
benefit from
injecting
state - of - the - art predicted parses
at
test time","our models||benefit from||injecting
injecting||at||test time
","injecting||has||state - of - the - art predicted parses
","Results||demonstrate that||our models
",,,,,,
results,"For models using GloVe embeddings , our syntax - free SA model already achieves a new state - of - the - art by jointly predicting predicates , POS and SRL .","For
models
using
GloVe embeddings
our syntax - free SA model
achieves
new state - of - the - art
by jointly predicting
predicates
POS
SRL","models||using||GloVe embeddings
our syntax - free SA model||achieves||new state - of - the - art
new state - of - the - art||by jointly predicting||predicates
new state - of - the - art||by jointly predicting||POS
new state - of - the - art||by jointly predicting||SRL
","models||has||our syntax - free SA model
","Results||For||models
",,,,,,
results,"LISA with it s own parses performs comparably to SA , but when supplied with D&M parses LISA out - performs the previous state - of - the - art by 2.5 F1 points .","LISA
with
own parses
performs
comparably
to
SA
when supplied with
D&M parses
out - performs
previous state - of - the - art
by
2.5 F1 points","LISA||with||own parses
own parses||performs||comparably
comparably||to||SA
LISA||when supplied with||D&M parses
D&M parses||out - performs||previous state - of - the - art
previous state - of - the - art||by||2.5 F1 points
",,,"Results||has||LISA
",,,,,
results,"On the out - ofdomain Brown test set , LISA also performs comparably to its syntax - free counterpart with its own parses , but with D&M parses LISA performs exceptionally well , more than 3.5 F1 points higher than He et al ..","On
out - ofdomain Brown test set
LISA
performs
comparably
to
syntax - free counterpart
with
its own parses
with
D&M parses
performs
exceptionally well","LISA||performs||comparably
comparably||with||its own parses
comparably||to||syntax - free counterpart
LISA||with||D&M parses
D&M parses||performs||exceptionally well
","out - ofdomain Brown test set||has||LISA
","Results||On||out - ofdomain Brown test set
",,,,,,
results,Incorporating ELMo em-beddings improves all scores .,"Incorporating
ELMo em-beddings
improves
all scores","ELMo em-beddings||improves||all scores
",,"Results||Incorporating||ELMo em-beddings
",,,,,,
research-problem,Deep Semantic Role Labeling : What Works and What 's Next,Deep Semantic Role Labeling,,,,,"Contribution||has research problem||Deep Semantic Role Labeling
",,,,
research-problem,"We introduce a new deep learning model for semantic role labeling ( SRL ) that significantly improves the state of the art , along with detailed analyses to reveal its strengths and limitations .",semantic role labeling ( SRL ),,,,,"Contribution||has research problem||semantic role labeling ( SRL )
",,,,
research-problem,Recently breakthroughs involving end - to - end deep models for SRL without syntactic input seem to overturn the long - held belief that syntactic parsing is a prerequisite for this task .,SRL,,,,,"Contribution||has research problem||SRL
",,,,
model,"In this paper , we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding , again significantly moving the state of the art ( another 2 points on CoNLL 2005 ) .","using
deep highway bidirectional LSTMs
with
constrained decoding","deep highway bidirectional LSTMs||with||constrained decoding
",,"Model||using||deep highway bidirectional LSTMs
",,,,,,
model,"Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .","treat
SRL
as
BIO tagging problem
use
deep bidirectional LSTMs","SRL||as||BIO tagging problem
",,"Model||treat||SRL
Model||use||deep bidirectional LSTMs
",,,,,,
model,"However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .","differ by
simplifying
input and output layers
introducing
highway connections
using
recurrent dropout
decoding
with
BIOconstraints
ensembling
with
product of experts","ensembling||with||product of experts
decoding||with||BIOconstraints
","using||has||recurrent dropout
introducing||has||highway connections
simplifying||has||input and output layers
",,,,"deep bidirectional LSTMs||differ by||using
deep bidirectional LSTMs||differ by||ensembling
deep bidirectional LSTMs||differ by||introducing
deep bidirectional LSTMs||differ by||decoding
deep bidirectional LSTMs||differ by||simplifying
",,,
hyperparameters,"Our network consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with 300 dimensional hidden units , and a softmax layer for predicting the output distribution .","network
consists of
8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs )
with
300 dimensional hidden units
softmax layer
for predicting
output distribution","network||consists of||8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs )
8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs )||with||300 dimensional hidden units
network||consists of||softmax layer
softmax layer||for predicting||output distribution
",,,"Hyperparameters||has||network
",,,,,
hyperparameters,"All the weight matrices in BiL - STMs are initialized with random orthonormal matrices as described in. ,","weight matrices
in
BiL - STMs
initialized with
random orthonormal matrices","weight matrices||in||BiL - STMs
BiL - STMs||initialized with||random orthonormal matrices
",,,"Hyperparameters||has||weight matrices
",,,,,
hyperparameters,All tokens are lower - cased and initialized with 100 - dimensional GloVe embeddings pre-trained on 6B tokens and updated during training .,"tokens
are
lower - cased
initialized with
100 - dimensional GloVe embeddings
pre-trained on
6B tokens
updated during
training","tokens||are||lower - cased
tokens||initialized with||100 - dimensional GloVe embeddings
100 - dimensional GloVe embeddings||pre-trained on||6B tokens
100 - dimensional GloVe embeddings||updated during||training
",,,"Hyperparameters||has||tokens
",,,,,
hyperparameters,Tokens that are not covered by GloVe are replaced with a randomly initialized UNK embedding .,"Tokens
not covered by
GloVe
replaced with
randomly initialized UNK embedding","Tokens||not covered by||GloVe
GloVe||replaced with||randomly initialized UNK embedding
",,,"Hyperparameters||has||Tokens
",,,,,
hyperparameters,"Training We use Adadelta ( Zeiler , 2012 ) with = 1e ?6 and ? = 0.95 and mini-batches of size 80 .","use
Adadelta
with
1e ?6 and ? = 0.95
mini-batches
size
80","Adadelta||with||1e ?6 and ? = 0.95
mini-batches||size||80
",,"Hyperparameters||use||Adadelta
Hyperparameters||use||mini-batches
",,,,,,
hyperparameters,We set RNN - dropout probability to 0.1 and clip gradients with norm larger than 1 .,"set
RNN - dropout probability
to
0.1
clip
gradients
with
norm
larger than
1","RNN - dropout probability||to||0.1
gradients||with||norm
norm||larger than||1
",,"Hyperparameters||set||RNN - dropout probability
Hyperparameters||clip||gradients
",,,,,,
hyperparameters,All the models are trained for 500 epochs with early stopping based on development results .,"models
trained for
500 epochs
with
early stopping
based on
development results","models||trained for||500 epochs
500 epochs||with||early stopping
early stopping||based on||development results
",,,"Hyperparameters||has||models
",,,,,
results,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 on both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,"Our ensemble ( PoE )
an absolute improvement
of
2.1 F1
on
CoNLL 2005
CoNLL 2012
over
previous state of the art","an absolute improvement||over||previous state of the art
previous state of the art||of||2.1 F1
previous state of the art||on||CoNLL 2005
previous state of the art||on||CoNLL 2012
","Our ensemble ( PoE )||has||an absolute improvement
",,"Results||has||Our ensemble ( PoE )
",,,,,
results,Our single model also achieves more than a 0.4 improvement on both datasets .,"Our single model
achieves
more than a 0.4 improvement
on
both datasets","Our single model||achieves||more than a 0.4 improvement
more than a 0.4 improvement||on||both datasets
",,,"Results||has||Our single model
",,,,,
results,"In comparison with the best reported results , our percentage of completely correct predicates improves by 5.9 points .","In comparison with
best reported results
our percentage of completely correct predicates
improves by
5.9 points","our percentage of completely correct predicates||improves by||5.9 points
","best reported results||has||our percentage of completely correct predicates
","Results||In comparison with||best reported results
",,,,,,
results,"While the continuing trend of improving SRL without syntax seems to suggest that neural end - to - end systems no longer needs parsers , our analysis in Section 4.4 will show that accurate syntactic information can improve these deep models .","show
accurate syntactic information
improve
deep models","accurate syntactic information||improve||deep models
",,"Results||show||accurate syntactic information
",,,,,,
ablation-analysis,"Without dropout , the model overfits at around 300 epochs at 78 F1 .","Without dropout
model
overfits
at
around 300 epochs
78 F1","model||overfits||around 300 epochs
around 300 epochs||at||78 F1
","Without dropout||has||model
",,"Ablation analysis||has||Without dropout
",,,,,
ablation-analysis,"Orthonormal parameter initialization is surprisingly important - without this , the model achieves only 65 F1 within the first 50 epochs .","Orthonormal parameter initialization
is
surprisingly important
without this
model
achieves
only 65 F1
within
first 50 epochs","Orthonormal parameter initialization||is||surprisingly important
Orthonormal parameter initialization||without this||model
model||achieves||only 65 F1
only 65 F1||within||first 50 epochs
",,,"Ablation analysis||has||Orthonormal parameter initialization
",,,,,
ablation-analysis,All 8 layer ablations suffer a loss of more than 1.7 in absolute F 1 compared to the full model .,"8 layer ablations
suffer
loss
more than
1.7
in
absolute F 1
compared to
full model","8 layer ablations||suffer||loss
loss||more than||1.7
loss||in||absolute F 1
loss||compared to||full model
",,,"Ablation analysis||has||8 layer ablations
",,,,,
research-problem,Deep Semantic Role Labeling with Self - Attention,Deep Semantic Role Labeling,,,,,"Contribution||has research problem||Deep Semantic Role Labeling
",,,,
research-problem,Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .,Semantic Role Labeling ( SRL ),,,,,"Contribution||has research problem||Semantic Role Labeling ( SRL )
",,,,
research-problem,"In this paper , we present a simple and effective architecture for SRL which aims to address these problems .",SRL,,,,,"Contribution||has research problem||SRL
",,,,
research-problem,"Semantic Role Labeling is a shallow semantic parsing task , whose goal is to determine essentially "" who did what to whom "" , "" when "" and "" where "" .",Semantic Role Labeling,,,,,"Contribution||has research problem||Semantic Role Labeling
",,,,
model,"To address these problems above , we present a deep attentional neural network ( DEEPATT ) for the task of SRL 1 .","present
deep attentional neural network ( DEEPATT )
for
SRL","deep attentional neural network ( DEEPATT )||for||SRL
",,"Model||present||deep attentional neural network ( DEEPATT )
",,,,,,
model,Our models rely on the self - attention mechanism which directly draws the global dependencies of the inputs .,"rely on
self - attention mechanism
directly draws
global dependencies
of
inputs","self - attention mechanism||directly draws||global dependencies
global dependencies||of||inputs
",,"Model||rely on||self - attention mechanism
",,,,,,
model,"In contrast to RNNs , a major advantage of self - attention is that it conducts direct connections between two arbitrary tokens in a sentence .","major advantage of
self - attention
conducts
direct connections
between
two arbitrary tokens
in
sentence","self - attention||conducts||direct connections
direct connections||between||two arbitrary tokens
two arbitrary tokens||in||sentence
",,"Model||major advantage of||self - attention
",,,,,,
model,"Therefore , distant elements can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .","distant elements
can interact with
each other
by
shorter paths
allows
unimpeded information flow
through
network","distant elements||can interact with||each other
each other||allows||unimpeded information flow
unimpeded information flow||through||network
each other||by||shorter paths
",,,,,,,"self - attention||has||distant elements
",
model,"Self - attention also provides a more flexible way to select , represent and synthesize the information of the inputs and is complementary to RNN based models .","provides
more flexible way
to select , represent and synthesize
information
of
inputs
complementary to
RNN based models","more flexible way||to select , represent and synthesize||information
information||of||inputs
information||complementary to||RNN based models
",,,,,"self - attention||provides||more flexible way
",,,
model,"Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .","Along with
self - attention
DEEP - ATT
comes with
three variants
uses
recurrent ( RNN )
convolutional ( CNN )
feed - forward ( FFN ) neural network
to further enhance
representations","DEEP - ATT||comes with||three variants
three variants||to further enhance||representations
representations||uses||recurrent ( RNN )
representations||uses||convolutional ( CNN )
representations||uses||feed - forward ( FFN ) neural network
","self - attention||has||DEEP - ATT
","Model||Along with||self - attention
",,,,,,
hyperparameters,We initialize the weights of all sub-layers as random orthogonal matrices .,"initialize
weights
of
all sub-layers
as
random orthogonal matrices","weights||of||all sub-layers
all sub-layers||as||random orthogonal matrices
",,"Hyperparameters||initialize||weights
",,,,,,
hyperparameters,"For other parameters , we initialize them by sampling each element from a Gaussian distribution with mean 0 and variance 1 ? d .","other parameters
by sampling
each element
from
Gaussian distribution
with
mean 0 and variance 1 ? d","other parameters||by sampling||each element
each element||from||Gaussian distribution
Gaussian distribution||with||mean 0 and variance 1 ? d
",,,"Hyperparameters||initialize||other parameters
",,,,,
hyperparameters,The embedding layer can be initialized randomly or using pre-trained word embeddings .,"embedding layer
can be
initialized randomly
pre-trained word embeddings","embedding layer||can be||initialized randomly
embedding layer||can be||pre-trained word embeddings
",,,"Hyperparameters||has||embedding layer
",,,,,
hyperparameters,The dimension of word embeddings and predicate mask embeddings is set to 100 and the number of hidden layers is set to 10 .,"dimension
of
word embeddings and predicate mask embeddings
set to
100
number of hidden layers
set to
10","number of hidden layers||set to||10
dimension||of||word embeddings and predicate mask embeddings
word embeddings and predicate mask embeddings||set to||100
",,,"Hyperparameters||has||number of hidden layers
Hyperparameters||has||dimension
",,,,,
hyperparameters,We set the number of hidden units d to 200 .,"set
number of hidden units d
to
200","number of hidden units d||to||200
",,"Hyperparameters||set||number of hidden units d
",,,,,,
hyperparameters,The number of heads h is set to 8 .,"number of heads h
set to
8","number of heads h||set to||8
",,,"Hyperparameters||has||number of heads h
",,,,,
hyperparameters,Dropout layers are added before residual connections with a keep probability of 0.8 .,"Dropout layers
added before
residual connections
with
keep probability
of
0.8","Dropout layers||added before||residual connections
residual connections||with||keep probability
keep probability||of||0.8
",,,"Hyperparameters||has||Dropout layers
",,,,,
hyperparameters,"Dropout is also applied before the attention softmax layer and the feed - froward ReLU hidden layer , and the keep probabilities are set to 0.9 .","Dropout
applied before
attention softmax layer
feed - froward ReLU hidden layer
keep probabilities
set to
0.9","Dropout||applied before||attention softmax layer
Dropout||applied before||feed - froward ReLU hidden layer
Dropout||applied before||keep probabilities
keep probabilities||set to||0.9
",,,"Hyperparameters||has||Dropout
",,,,,
hyperparameters,We also employ label smoothing technique with a smoothing value of 0.1 during training .,"employ
label smoothing technique
with
smoothing value
of
0.1
during
training","label smoothing technique||with||smoothing value
smoothing value||of||0.1
0.1||during||training
",,"Hyperparameters||employ||label smoothing technique
",,,,,,
hyperparameters,Learning Parameter optimization is performed using stochastic gradient descent .,"Learning Parameter optimization
performed using
stochastic gradient descent","Learning Parameter optimization||performed using||stochastic gradient descent
",,,"Hyperparameters||has||Learning Parameter optimization
",,,,,
hyperparameters,We adopt Adadelta ) ( = 10 6 and ? = 0.95 ) as the optimizer .,"adopt
Adadelta
as
optimizer","Adadelta||as||optimizer
",,"Hyperparameters||adopt||Adadelta
",,,,,,
hyperparameters,"To avoid exploding gradients problem , we clip the norm of gradients with a predefined threshold 1.0 .","To avoid
exploding gradients problem
clip
norm
of
gradients
with
predefined threshold 1.0","norm||of||gradients
gradients||with||predefined threshold 1.0
norm||To avoid||exploding gradients problem
",,"Hyperparameters||clip||norm
",,,,,,
hyperparameters,Each SGD contains a mini-batch of approximately 4096 tokens for the CoNLL - 2005 dataset and 8192 tokens for the CoNLL - 2012 dataset .,"SGD
contains
mini-batch
of approximately
4096 tokens
for
CoNLL - 2005 dataset
8192 tokens
for
CoNLL - 2012 dataset","SGD||contains||mini-batch
mini-batch||of approximately||4096 tokens
4096 tokens||for||CoNLL - 2005 dataset
mini-batch||of approximately||8192 tokens
8192 tokens||for||CoNLL - 2012 dataset
",,,"Hyperparameters||has||SGD
",,,,,
hyperparameters,The learning rate is initialized to 1.0 .,"learning rate
initialized to
1.0","learning rate||initialized to||1.0
",,,"Hyperparameters||has||learning rate
",,,,,
hyperparameters,"After training 400 k steps , we halve the learning rate every 100 K steps .","After training
400 k steps
halve
learning rate
every
100 K steps","400 k steps||halve||learning rate
learning rate||every||100 K steps
",,"Hyperparameters||After training||400 k steps
",,,,,,
hyperparameters,We train all models for 600 K steps .,"train
all models
for
600 K steps","all models||for||600 K steps
",,"Hyperparameters||train||all models
",,,,,,
hyperparameters,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .","For
DEEP - ATT
with
FFN sub - layers
whole training stage
takes
two days
to
finish
on
single Titan X GPU
which is
2.5 times faster","DEEP - ATT||with||FFN sub - layers
whole training stage||takes||two days
two days||to||finish
finish||on||single Titan X GPU
two days||which is||2.5 times faster
","DEEP - ATT||has||whole training stage
","Hyperparameters||For||DEEP - ATT
",,,,,,
results,"In Remarkably , we get 74.1 F 1 score on the out - of - domain dataset , which outperforms the previous state - of - the - art system by 2.0 F 1 score .","get
74.1 F 1 score
on
out - of - domain dataset","74.1 F 1 score||on||out - of - domain dataset
",,"Results||get||74.1 F 1 score
",,,,,,
results,"When ensembling 5 models with FFN nonlinear sub - layers , our approach achieves an F 1 score of 84.6 and 83.9 on the two datasets respectively , which has an absolute improvement of 1.4 and 0.5 over the previous state - of - the - art .","When ensembling
5 models
with
FFN nonlinear sub - layers
our approach
achieves
F 1 score
of
84.6 and 83.9
absolute improvement
of
1.4 and 0.5
over
previous state - of - the - art","5 models||with||FFN nonlinear sub - layers
our approach||achieves||F 1 score
F 1 score||of||84.6 and 83.9
absolute improvement||over||previous state - of - the - art
absolute improvement||of||1.4 and 0.5
","FFN nonlinear sub - layers||has||our approach
our approach||has||absolute improvement
","Results||When ensembling||5 models
",,,,,,
results,These results are consistent with our intuition that the self - attention layers is helpful to capture structural information and long distance dependencies .,"results
that
self - attention layers
helpful to capture
structural information
long distance dependencies","results||that||self - attention layers
self - attention layers||helpful to capture||structural information
self - attention layers||helpful to capture||long distance dependencies
",,,"Results||has||results
",,,,,
research-problem,A Span Selection Model for Semantic Role Labeling,Semantic Role Labeling,,,,,"Contribution||has research problem||Semantic Role Labeling
",,,,
research-problem,We present a simple and accurate span - based model for semantic role labeling ( SRL ) .,semantic role labeling ( SRL ),,,,,"Contribution||has research problem||semantic role labeling ( SRL )
",,,,
model,"At decoding time , we greedily select higher scoring labeled spans .","At
decoding time
greedily select
higher scoring labeled spans","decoding time||greedily select||higher scoring labeled spans
",,"Model||At||decoding time
",,,,,,
research-problem,"Given a sentence and a target predicate , SRL systems have to predict semantic arguments of the predicate .",SRL,,,,,"Contribution||has research problem||SRL
",,,,
model,"To fill this gap , this paper presents a simple and accurate span - based model .","presents
simple and accurate span - based model",,,"Model||presents||simple and accurate span - based model
",,,,,,
model,"Inspired by recent span - based models in syntactic parsing and coreference resolution , our model directly scores all possible labeled spans based on span representations induced from neural networks .","directly scores
all possible labeled spans
based on
span representations
induced from
neural networks","all possible labeled spans||based on||span representations
span representations||induced from||neural networks
",,"Model||directly scores||all possible labeled spans
",,,,,,
model,"At decoding time , we greedily select higher scoring labeled spans .",,,,,,,,,,
model,The model parameters are learned by optimizing loglikelihood of correct labeled spans .,"model parameters
learned by
optimizing loglikelihood
of
correct labeled spans","model parameters||learned by||optimizing loglikelihood
optimizing loglikelihood||of||correct labeled spans
",,,"Model||has||model parameters
",,,,,
baselines,"For comparison , as a model based on BIO tagging approaches , we use the BiLSTM - CRF model proposed by .","use
BiLSTM - CRF model",,,"Baselines||use||BiLSTM - CRF model
",,,,,,
hyperparameters,"As the base function f base , we use 4 BiLSTM layers with 300 dimensional hidden units .","As
base function f base
use
4 BiLSTM layers
with
300 dimensional hidden units","base function f base||use||4 BiLSTM layers
4 BiLSTM layers||with||300 dimensional hidden units
",,"Hyperparameters||As||base function f base
",,,,,,
hyperparameters,"To optimize the model parameters , we use Adam .","optimize
model parameters
use
Adam","model parameters||use||Adam
",,"Hyperparameters||optimize||model parameters
",,,,,,
hyperparameters,"To validate the model performance , we use two types of word embeddings .","To validate
model performance
use
two types of word embeddings","model performance||use||two types of word embeddings
",,"Hyperparameters||To validate||model performance
",,,,,,
hyperparameters,"Typical word embeddings , SENNA 6 ( Collobert et al. , 2011 ) Contextualized word embeddings , ELMo 7 SENNA and ELMo can be regarded as different types of embeddings in terms of the context sensitivity .","Typical word embeddings
SENNA
ELMo",,"Typical word embeddings||has||SENNA
Typical word embeddings||has||ELMo
",,"Hyperparameters||has||Typical word embeddings
",,,,,
hyperparameters,These embeddings are fixed during training .,"embeddings
fixed during
training","embeddings||fixed during||training
",,,"Hyperparameters||has||embeddings
",,,,,
hyperparameters,"As the objective function , we use the crossentropy L ? in Eq. 3 with L2 weight decay ,","objective function
use
crossentropy L
with
L2 weight decay","objective function||use||crossentropy L
crossentropy L||with||L2 weight decay
",,,"Hyperparameters||As||objective function
",,,,,
results,We report averaged scores across five different runs of the model training .,"report
averaged scores
across
five different runs
of
model","averaged scores||across||five different runs
five different runs||of||model
",,"Results||report||averaged scores
",,,,,,
results,"Overall , our span - based ensemble model using ELMo achieved the best F1 scores , 87.4 F1 and 87.0 F1 on the CoNLL - 2005 and CoNLL - 2012 datasets , respectively .","span - based ensemble model
using
ELMo
achieved
best F1 scores
87.4 F1 and 87.0 F1
on
CoNLL - 2005 and CoNLL - 2012 datasets","span - based ensemble model||using||ELMo
ELMo||achieved||best F1 scores
best F1 scores||on||CoNLL - 2005 and CoNLL - 2012 datasets
","best F1 scores||has||87.4 F1 and 87.0 F1
",,"Results||has||span - based ensemble model
",,,,,
results,"In comparison with the CRF - based single model , our span - based single model consistently yielded better F 1 scores regardless of the word embeddings , SENNA and ELMO .","In comparison with
CRF - based single model
our span - based single model
consistently yielded
better F 1 scores
regardless of
word embeddings , SENNA and ELMO","our span - based single model||consistently yielded||better F 1 scores
better F 1 scores||regardless of||word embeddings , SENNA and ELMO
","CRF - based single model||has||our span - based single model
","Results||In comparison with||CRF - based single model
",,,,,,
results,Our single and ensemble models using ELMO achieved the best F 1 scores on all the test sets except the Brown test set .,"Our single and ensemble models
achieved
best F 1 scores
on
all the test sets
except
Brown test set","Our single and ensemble models||achieved||best F 1 scores
best F 1 scores||on||all the test sets
all the test sets||except||Brown test set
",,,"Results||has||Our single and ensemble models
",,,,,
research-problem,Structural Scaffolds for Citation Intent Classification in Scientific Publications,Citation Intent Classification in Scientific Publications,,,,,"Contribution||has research problem||Citation Intent Classification in Scientific Publications
",,,,
research-problem,"Identifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature .",Identifying the intent of a citation in scientific papers,,,,,"Contribution||has research problem||Identifying the intent of a citation in scientific papers
",,,,
code,Our code and data are available at : https://github.com/ allenai/scicite .,https://github.com/ allenai/scicite,,,,,"Contribution||Code||https://github.com/ allenai/scicite
",,,,
research-problem,"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .",citation intent classification,,,,,"Contribution||has research problem||citation intent classification
",,,,
model,"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .","propose
neural multitask learning framework
to incorporate
knowledge
into
citations
from
structure of scientific papers","neural multitask learning framework||to incorporate||knowledge
knowledge||into||citations
citations||from||structure of scientific papers
",,"Model||propose||neural multitask learning framework
",,,,,,
model,"In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .","two auxiliary tasks
as
structural scaffolds
to improve
citation intent prediction","two auxiliary tasks||as||structural scaffolds
structural scaffolds||to improve||citation intent prediction
",,,"Model||propose||two auxiliary tasks
",,,,,
model,"On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .","On
two datasets
show that
proposed neural scaffold model
outperforms
existing methods
by
large margins","two datasets||show that||proposed neural scaffold model
proposed neural scaffold model||by||large margins
proposed neural scaffold model||outperforms||existing methods
",,"Model||On||two datasets
",,,,,,
model,"Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .","neural scaffold framework
for
citation intent classification
to incorporate into
citations
knowledge
from
structure of scientific papers","neural scaffold framework||for||citation intent classification
citation intent classification||to incorporate into||citations
knowledge||from||structure of scientific papers
","citations||has||knowledge
",,"Model||propose||neural scaffold framework
",,,,,
dataset,"To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .","introduce
Sci - Cite
new dataset
of
citation intents
that is
significantly larger , more coarse - grained and generaldomain
compared with
existing datasets","new dataset||that is||significantly larger , more coarse - grained and generaldomain
significantly larger , more coarse - grained and generaldomain||compared with||existing datasets
new dataset||of||citation intents
","Sci - Cite||has||new dataset
","Dataset||introduce||Sci - Cite
",,,,,,
dataset,"We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .","consider
three intent categories
BACK - GROUND
METHOD
RESULTCOMPARISON",,"three intent categories||name||BACK - GROUND
three intent categories||name||METHOD
three intent categories||name||RESULTCOMPARISON
","Dataset||consider||three intent categories
",,,,,,
dataset,Citation intent of sentence extractions was labeled through the crowdsourcing platform .,"Citation intent
of
sentence extractions
labeled through
crowdsourcing platform","Citation intent||of||sentence extractions
sentence extractions||labeled through||crowdsourcing platform
",,,"Dataset||has||Citation intent
",,,,,
dataset,"Citation contexts were annotated by 850 crowdsource workers who made a total of 29,926 annotations and individually made between 4 and 240 annotations .","Citation contexts
annotated by
850 crowdsource workers
made
total of 29,926 annotations
individually made
4 and 240 annotations","Citation contexts||annotated by||850 crowdsource workers
850 crowdsource workers||individually made||4 and 240 annotations
850 crowdsource workers||made||total of 29,926 annotations
",,,"Dataset||has||Citation contexts
",,,,,
dataset,"Each sentence was annotated , on average , 3.74 times .","sentence
was
annotated
on average
3.74 times","sentence||was||annotated
annotated||on average||3.74 times
",,,,,,,"Citation contexts||has||sentence
",
dataset,"This resulted in a total 9,159 crowdsourced instances which were divided to training and validation sets with 90 % of the data used for the training set .","resulted in
total 9,159
crowdsourced instances
divided to
training and validation sets
with
90 %
of
data
used for
training set","crowdsourced instances||divided to||training and validation sets
training and validation sets||with||90 %
90 %||of||data
data||used for||training set
","total 9,159||has||crowdsourced instances
",,,,"Citation contexts||resulted in||total 9,159
",,,
hyperparameters,We implement our proposed scaffold framework using the AllenNLP library .,"implement
proposed scaffold framework
using
AllenNLP library","proposed scaffold framework||using||AllenNLP library
",,"Hyperparameters||implement||proposed scaffold framework
",,,,,,
hyperparameters,"For word representations , we use 100 - dimensional GloVe vectors trained on a corpus of 6B tokens from Wikipedia and Gigaword .","For
word representations
use
100 - dimensional GloVe vectors
trained on
corpus
of
6B tokens
from
Wikipedia and Gigaword","word representations||use||100 - dimensional GloVe vectors
100 - dimensional GloVe vectors||trained on||corpus
corpus||of||6B tokens
corpus||from||Wikipedia and Gigaword
",,"Hyperparameters||For||word representations
",,,,,,
hyperparameters,"For contextual representations , we use ELMo vectors released by with output dimension size of 1,024 which have been trained on a dataset of 5.5 B tokens .","contextual representations
use
ELMo vectors
with
output dimension size
of
1,024
trained on
dataset
of
5.5 B tokens","contextual representations||use||ELMo vectors
ELMo vectors||with||output dimension size
output dimension size||of||1,024
ELMo vectors||trained on||dataset
dataset||of||5.5 B tokens
",,,"Hyperparameters||For||contextual representations
",,,,,
hyperparameters,We use a single - layer BiLSTM with a hidden dimension size of 50 for each direction 11 .,"use
single - layer BiLSTM
with
hidden dimension size
of
50","single - layer BiLSTM||with||hidden dimension size
hidden dimension size||of||50
",,"Hyperparameters||use||single - layer BiLSTM
",,,,,,
hyperparameters,"For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .","each of scaffold tasks
use
single - layer MLP
with
20 hidden nodes
ReLU activation
Dropout rate
of
0.2
between
hidden and input layers","each of scaffold tasks||use||single - layer MLP
single - layer MLP||between||hidden and input layers
hidden and input layers||with||20 hidden nodes
hidden and input layers||with||ReLU activation
hidden and input layers||with||Dropout rate
Dropout rate||of||0.2
",,,"Hyperparameters||For||each of scaffold tasks
",,,,,
hyperparameters,Batch size is 8 for ACL - ARC dataset and 32 for SciCite dataset ( recall that SciCite is larger than ACL - ARC ) .,"Batch size
is
8
for
ACL - ARC dataset
32
for
SciCite dataset","Batch size||is||8
8||for||ACL - ARC dataset
Batch size||is||32
32||for||SciCite dataset
",,,"Hyperparameters||has||Batch size
",,,,,
hyperparameters,We use Beaker 12 for running the experiments .,"Beaker
for
running the experiments","Beaker||for||running the experiments
",,,"Hyperparameters||use||Beaker
",,,,,
baselines,BiLSTM Attention ( with and without ELMo ) .,BiLSTM Attention ( with and without ELMo ),,,,"Baselines||has||BiLSTM Attention ( with and without ELMo )
",,,,,
baselines,"This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .","uses
similar architecture
to
proposed neural multitask learning framework
optimizes
network
for
main loss
regarding
citation intent classification ( L 1 )","network||for||main loss
main loss||regarding||citation intent classification ( L 1 )
similar architecture||to||proposed neural multitask learning framework
",,,,,"BiLSTM Attention ( with and without ELMo )||optimizes||network
BiLSTM Attention ( with and without ELMo )||uses||similar architecture
",,,
results,We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .,"observe
scaffold - enhanced models
achieve
clear improvements
over
state - of - the - art approach","scaffold - enhanced models||achieve||clear improvements
clear improvements||over||state - of - the - art approach
",,"Results||observe||scaffold - enhanced models
",,,,,,
results,"Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .","Starting with
BiLSTM - Attn
with
macro F1 score
of
51.8
adding
first scaffold task in ' BiLSTM - Attn + section title scaffold '
improves
F1 score
to
56.9 (?= 5.1 )","BiLSTM - Attn||with||macro F1 score
macro F1 score||of||51.8
BiLSTM - Attn||adding||first scaffold task in ' BiLSTM - Attn + section title scaffold '
first scaffold task in ' BiLSTM - Attn + section title scaffold '||improves||F1 score
F1 score||to||56.9 (?= 5.1 )
",,"Results||Starting with||BiLSTM - Attn
",,,,,,
results,Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : 56.3 (?= 4.5 ) .,"Adding
second scaffold
in
BiLSTM - Attn + citation worthiness scaffold
results in
similar improvements
56.3 (?= 4.5 )","second scaffold||in||BiLSTM - Attn + citation worthiness scaffold
BiLSTM - Attn + citation worthiness scaffold||results in||similar improvements
","similar improvements||has||56.3 (?= 4.5 )
","Results||Adding||second scaffold
",,,,,,
results,"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .","both scaffolds
used simultaneously in
BiLSTM - Attn + both scaffolds
F1 score
improves to
63.1 ( ?= 11.3 )","both scaffolds||used simultaneously in||BiLSTM - Attn + both scaffolds
F1 score||improves to||63.1 ( ?= 11.3 )
","both scaffolds||has||F1 score
",,"Results||has||both scaffolds
",,,,,
results,"The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .","add
ELMo vectors
to
input representations
in
BiLSTM - Attn w / ELMo + both scaffolds
achieving
F1
of
67.9
major improvement
from
previous state - of - the - art results
of
54.6 ( ?= 13.3 )","ELMo vectors||achieving||F1
F1||of||67.9
major improvement||from||previous state - of - the - art results
previous state - of - the - art results||of||54.6 ( ?= 13.3 )
ELMo vectors||to||input representations
input representations||in||BiLSTM - Attn w / ELMo + both scaffolds
","67.9||has||major improvement
",,,,"best results||add||ELMo vectors
",,,
results,"We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .","note
scaffold tasks
provide
major contributions
on top of
ELMo - enabled baseline ( ?= 13.6 )","scaffold tasks||provide||major contributions
major contributions||on top of||ELMo - enabled baseline ( ?= 13.6 )
",,"Results||note||scaffold tasks
",,,,,,
results,"We also experimented with adding features used in to our best model and not only we did not see any improvements , but we observed at least 1.7 % decline in performance .","experimented with
adding features
used in
our best model
observed
at least 1.7 % decline
in
performance","adding features||used in||our best model
adding features||observed||at least 1.7 % decline
at least 1.7 % decline||in||performance
",,"Results||experimented with||adding features
",,,,,,
results,Each scaffold task improves model performance .,"Each scaffold task
improves
model performance","Each scaffold task||improves||model performance
",,,"Results||has||Each scaffold task
",,,,,
results,Adding both scaffolds results in further improvements .,"both scaffolds
results in
further improvements","both scaffolds||results in||further improvements
",,,"Results||Adding||both scaffolds
",,,,,
results,And the best results are obtained by using ELMo representation in addition to both scaffolds .,"best results
by using
ELMo representation
in addition to
both scaffolds","best results||by using||ELMo representation
ELMo representation||in addition to||both scaffolds
",,,,,,,,
results,Generally we observe that results on categories with more number of instances are higher .,"results
on
categories
with
more number of instances
are
higher","results||on||categories
categories||with||more number of instances
more number of instances||are||higher
",,,"Results||observe||results
",,,,,
results,"For example on ACL - ARC , the results on the BACKGROUND category are the highest as this category is the most common .","on
ACL - ARC
results
on
BACKGROUND category
are
highest
as
category
is
most common","results||on||BACKGROUND category
BACKGROUND category||are||highest
highest||as||category
category||is||most common
","ACL - ARC||has||results
","Results||on||ACL - ARC
",,,,,,
results,"Conversely , the results on the FUTUREWORK category are the lowest .","FUTUREWORK category
are
lowest","FUTUREWORK category||are||lowest
",,,,,,,"results||on||FUTUREWORK category
",
results,This category has the fewest data points ( see distribution of the categories in ) and thus it is harder for the model to learn the optimal parameters for correct classification in this category .,"fewest data points
thus
harder
for
model
to learn
optimal parameters
for
correct classification","fewest data points||thus||harder
harder||for||model
model||to learn||optimal parameters
optimal parameters||for||correct classification
",,,,,,,"FUTUREWORK category||has||fewest data points
",
research-problem,Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,Sequential Sentence Classification in Medical Scientific Abstracts,,,,,"Contribution||has research problem||Sequential Sentence Classification in Medical Scientific Abstracts
",,,,
research-problem,"This hampers the traditional sentence classification approaches to the problem of sequential sentence classification , where structured prediction is needed for better over all classification performance .",sequential sentence classification,,,,,"Contribution||has research problem||sequential sentence classification
",,,,
model,"In this work , we present a hierarchical neural network model for the sequential sentence classification task , which we call a hierarchical sequential labeling network ( HSLN ) .","present
hierarchical neural network model
for
sequential sentence classification task
call
hierarchical sequential labeling network ( HSLN )","hierarchical neural network model||call||hierarchical sequential labeling network ( HSLN )
hierarchical neural network model||for||sequential sentence classification task
",,"Model||present||hierarchical neural network model
",,,,,,
model,"Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings , then uses another bi - LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation , subsequently uses a single - hidden - layer feed - forward network to transform the sentence representation to the probability vector , and finally optimizes the predicted label sequence jointly via a CRF layer .","first uses
RNN or CNN layer
to individually encode
sentence representation
from
sequence of word embeddings
uses
another bi - LSTM layer
to
take as input
individual sentence representation
output
contextualized sentence representation
subsequently uses
single - hidden - layer feed - forward network
to transform
sentence representation
probability vector
optimizes
predicted label sequence jointly
via
CRF layer","single - hidden - layer feed - forward network||to transform||sentence representation
sentence representation||to||probability vector
predicted label sequence jointly||via||CRF layer
another bi - LSTM layer||output||contextualized sentence representation
another bi - LSTM layer||take as input||individual sentence representation
RNN or CNN layer||to individually encode||sentence representation
sentence representation||from||sequence of word embeddings
",,"Model||subsequently uses||single - hidden - layer feed - forward network
Model||optimizes||predicted label sequence jointly
Model||uses||another bi - LSTM layer
Model||first uses||RNN or CNN layer
",,,,,,
hyperparameters,"The token embeddings were pre-trained on a large corpus combining Wikipedia , PubMed , and PMC texts ( Moen and Ananiadou , 2013 ) using the word2vec tool 4 ( denoted as "" Word2vec- wiki+P.M. "" ) .","token embeddings
pre-trained on
large corpus
combining
Wikipedia , PubMed , and PMC texts
using
word2vec tool","token embeddings||using||word2vec tool
token embeddings||pre-trained on||large corpus
large corpus||combining||Wikipedia , PubMed , and PMC texts
",,,"Hyperparameters||has||token embeddings
",,,,,
hyperparameters,They are fixed during the training phase to avoid over-fitting .,"fixed during
training phase
to avoid
over-fitting","training phase||to avoid||over-fitting
",,"Hyperparameters||fixed during||training phase
",,,,,,
hyperparameters,"The model is trained using the Adam optimization method ( Kingma and Ba , 2014 ) .","trained using
Adam optimization method",,,"Hyperparameters||trained using||Adam optimization method
",,,,,,
hyperparameters,The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch .,"learning rate
initially set
0.003
decayed by
0.9
after
each epoch","learning rate||initially set||0.003
learning rate||decayed by||0.9
0.9||after||each epoch
",,,"Hyperparameters||has||learning rate
",,,,,
hyperparameters,"For regularization , dropout ( Srivastava et al. , 2014 ) is applied to each layer .","For
regularization
dropout
applied to
each layer","dropout||applied to||each layer
","regularization||has||dropout
","Hyperparameters||For||regularization
",,,,,,
hyperparameters,"To reduce this gap , we adopted the dropout with expectation - linear regularization introduced by to explicitly control the inference gap and thus improve the generaliza - tion performance .","adopted
dropout
with
expectation - linear regularization
to explicitly control
inference gap
improve
generaliza - tion performance","dropout||with||expectation - linear regularization
expectation - linear regularization||improve||generaliza - tion performance
expectation - linear regularization||to explicitly control||inference gap
",,"Hyperparameters||adopted||dropout
",,,,,,
hyperparameters,Hyperparameters were optimized via grid search based on the validation set and the best configuration is shown in .,"optimized via
grid search
based on
validation set","grid search||based on||validation set
",,"Hyperparameters||optimized via||grid search
",,,,,,
hyperparameters,"The window sizes of the CNN encoder in the sentence encoding layer are 2 , 3 , 4 and 5 .","window sizes
of
CNN encoder
in
sentence encoding layer
are
2 , 3 , 4 and 5","window sizes||of||CNN encoder
CNN encoder||in||sentence encoding layer
sentence encoding layer||are||2 , 3 , 4 and 5
",,,"Hyperparameters||has||window sizes
",,,,,
ablation-analysis,"As can be seen from , our HSLN - CNN model uni-formly suffers a little more from the component removal than the HSLN - RNN model , indicating that the HSLN - RNN model is more robust .","HSLN - CNN model
suffers a little more from
component removal
than
HSLN - RNN model","HSLN - CNN model||suffers a little more from||component removal
HSLN - CNN model||than||HSLN - RNN model
",,,"Ablation analysis||has||HSLN - CNN model
",,,,,
ablation-analysis,"When the context enriching layer is removed , both models experience the most significant performance drop and can only be on par with the previous stateof - the - art results , strongly demonstrating that this proposed component is the key to the performance improvement of our model .","When
context enriching layer
is
removed
both models
experience
most significant performance drop
on par with
previous stateof - the - art results","context enriching layer||is||removed
both models||experience||most significant performance drop
both models||on par with||previous stateof - the - art results
","removed||has||both models
","Ablation analysis||When||context enriching layer
",,,,,,
ablation-analysis,"Furthermore , even without the label sequence optimization layer , our model still significantly outperforms the best published methods that are empowered by this layer , indicating that the context enriching layer we propose can help optimize the label sequence by considering the context information from the surrounding sentences .","even without
label sequence optimization layer
our model
significantly outperforms
best published methods
empowered by
this layer","our model||significantly outperforms||best published methods
best published methods||empowered by||this layer
","label sequence optimization layer||has||our model
","Ablation analysis||even without||label sequence optimization layer
",,,,,,
ablation-analysis,"Last but not the least , the dropout regularization and attention - based pooling components we add to our system can help further improve the model in a limited extent . :","dropout regularization and attention - based pooling components
add to
our system
further improve
model
in
limited extent","dropout regularization and attention - based pooling components||further improve||model
model||in||limited extent
dropout regularization and attention - based pooling components||add to||our system
",,,"Ablation analysis||has||dropout regularization and attention - based pooling components
",,,,,
research-problem,Translations as Additional Contexts for Sentence Classification,Sentence Classification,,,,,"Contribution||has research problem||Sentence Classification
",,,,
approach,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .","propose
usage of translations
as
compelling and effective domain - free contexts","usage of translations||as||compelling and effective domain - free contexts
",,"Approach||propose||usage of translations
",,,,,,
approach,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .","method
to mitigate
possible problems
when using
translated sentences
as
context","method||to mitigate||possible problems
possible problems||when using||translated sentences
translated sentences||as||context
",,,"Approach||propose||method
",,,,,
approach,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .","present
neural attentionbased multiple context fixing attachment ( MCFA )",,,"Approach||present||neural attentionbased multiple context fixing attachment ( MCFA )
",,,,,,
approach,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .","MCFA
is
series of modules
uses
all the sentence vectors
as
context
to fix
a sentence vector","MCFA||is||series of modules
series of modules||uses||all the sentence vectors
all the sentence vectors||as||context
all the sentence vectors||to fix||a sentence vector
",,,"Approach||has||MCFA
",,,,,
approach,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,"computes
two sentence usability metrics
to control
noise when fixing vectors
self usability
weighs
confidence
of using
sentence a
in solving
task","two sentence usability metrics||to control||noise when fixing vectors
self usability||weighs||confidence
confidence||of using||sentence a
sentence a||in solving||task
","two sentence usability metrics||has||self usability
",,,,"MCFA||computes||two sentence usability metrics
",,,
approach,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.","relative usability
weighs
confidence of
using
sentence a
fixing","relative usability||weighs||confidence of
confidence of||using||sentence a
",,,,,,"sentence a||fixing||sentence b
","two sentence usability metrics||has||relative usability
",
approach,"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .","attached after
encoding the sentence
makes it widely adaptable
to other models","encoding the sentence||makes it widely adaptable||to other models
",,,,,"MCFA||attached after||encoding the sentence
",,,
approach,"( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .","moves
vectors
inside
same space
preserves
meaning
of
vector dimensions","vectors||preserves||meaning
meaning||of||vector dimensions
vectors||inside||same space
",,,,,"MCFA||moves||vectors
",,,
hyperparameters,Tokenization is done using the polyglot library 7 .,"Tokenization
done using
polyglot library","Tokenization||done using||polyglot library
",,,"Hyperparameters||has||Tokenization
",,,,,
hyperparameters,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,"experiment on using
only one additional context ( N = 1 )
all ten languages at once ( N = 10 )",,,"Hyperparameters||experiment on using||only one additional context ( N = 1 )
Hyperparameters||experiment on using||all ten languages at once ( N = 10 )
",,,,,,
hyperparameters,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .","For
our CNN",,,"Hyperparameters||For||our CNN
",,,,,,"our CNN||use||rectified linear units
our CNN||use||three filters
"
hyperparameters,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .","final sentence vector
concatenate
feature maps
to get
300 - dimension vector","final sentence vector||concatenate||feature maps
feature maps||to get||300 - dimension vector
",,,"Hyperparameters||For||final sentence vector
",,,,,
hyperparameters,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,"use
dropout
on
all nonlinear connections
with
dropout rate
of
0.5","dropout||with||dropout rate
dropout rate||of||0.5
dropout||on||all nonlinear connections
",,"Hyperparameters||use||dropout
",,,,,,
hyperparameters,"We also use an l 2 constraint of 3 , following for accurate comparisons .","l 2 constraint
of
3","l 2 constraint||of||3
",,,"Hyperparameters||use||l 2 constraint
",,,,,
hyperparameters,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,"FastText pre-trained vectors
for
all our data sets","FastText pre-trained vectors||for||all our data sets
",,,"Hyperparameters||use||FastText pre-trained vectors
",,,,,
hyperparameters,"During training , we use mini-batch size of 50 .","During
training
use
mini-batch size
of
50","training||use||mini-batch size
mini-batch size||of||50
",,"Hyperparameters||During||training
",,,,,,
hyperparameters,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,"Training
done via
stochastic gradient descent
over
shuffled mini-batches
with
Adadelta update rule","Training||done via||stochastic gradient descent
stochastic gradient descent||over||shuffled mini-batches
shuffled mini-batches||with||Adadelta update rule
",,,"Hyperparameters||has||Training
",,,,,
hyperparameters,We perform early stopping using a random 10 % of the training set as the development set .,"perform
early stopping
using
random 10 %
of
training set
as
development set","early stopping||using||random 10 %
random 10 %||as||development set
random 10 %||of||training set
",,"Hyperparameters||perform||early stopping
",,,,,,
results,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,"show
CNN + MCFA
achieves
state of the art performance
on
three of the four data sets
performs
competitively
on
one data set","CNN + MCFA||performs||competitively
competitively||on||one data set
CNN + MCFA||achieves||state of the art performance
state of the art performance||on||three of the four data sets
",,"Results||show||CNN + MCFA
",,,,,,
results,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .","When
N = 1
MCFA
increases
performance
of
normal CNN
from
85.0
to
87.6
beating
current state of the art
on
CR data set","MCFA||increases||performance
performance||of||normal CNN
normal CNN||beating||current state of the art
current state of the art||on||CR data set
normal CNN||from||85.0
85.0||to||87.6
","N = 1||has||MCFA
","Results||When||N = 1
",,,,,,
results,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .","N = 10
MCFA
additionally beats
state of the art
on
TREC data set","MCFA||additionally beats||state of the art
state of the art||on||TREC data set
","N = 10||has||MCFA
",,"Results||When||N = 10
",,,,,
results,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .","our ensemble classifier
additionally outperforms
all competing models
on
MR data set","our ensemble classifier||additionally outperforms||all competing models
all competing models||on||MR data set
",,,"Results||has||our ensemble classifier
",,,,,
results,"On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .","On
all data sets
except
SUBJ
accuracy
of
CNN + B1
decreases from
base CNN accuracy","all data sets||except||SUBJ
accuracy||of||CNN + B1
CNN + B1||decreases from||base CNN accuracy
","all data sets||has||accuracy
","Results||On||all data sets
",,,,,,
results,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .","topics
translations",,,,"Results||conclude that||translations
",,,,,
results,"Overall , we conclude that translations are better additional contexts than topics .","conclude that
are
better additional contexts
than",,,,,,"translations||are||better additional contexts
","better additional contexts||than||topics
",,
results,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .","using
single context
translations
always outperform
topics","translations||always outperform||topics
","single context||has||translations
","Results||using||single context
",,,,,,
results,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .","Using
topics
as
additional context
decreases
performance
of
CNN classifier
on
most data sets","topics||as||additional context
topics||decreases||performance
performance||of||CNN classifier
CNN classifier||on||most data sets
",,"Results||Using||topics
",,,,,,
research-problem,Can Syntax Help ? Improving an LSTM - based Sentence Compression Model for New Domains,Sentence Compression,,,,,"Contribution||has research problem||Sentence Compression
",,,,
model,"To this end , we extend the deletionbased LSTM model for sentence compression by .","extend
deletionbased LSTM model
for
sentence compression","deletionbased LSTM model||for||sentence compression
",,"Model||extend||deletionbased LSTM model
",,,,,,
model,"Specifically , we propose two major changes to the model by : We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model .","propose
two major changes
explicitly introduce
POS embeddings and dependency relation embeddings
into
neural network model","two major changes||explicitly introduce||POS embeddings and dependency relation embeddings
POS embeddings and dependency relation embeddings||into||neural network model
",,"Model||propose||two major changes
",,,,,,
model,"( 2 ) Inspired by a previous method , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences .","formulate
final predictions
as
Integer Linear Programming problem
to incorporate
constraints
based on
syntactic relations
between
words and expected lengths
of
compressed sentences","final predictions||as||Integer Linear Programming problem
Integer Linear Programming problem||to incorporate||constraints
constraints||based on||syntactic relations
syntactic relations||between||words and expected lengths
words and expected lengths||of||compressed sentences
",,,,,"two major changes||formulate||final predictions
",,,
model,"In addition to the two major changes above , we also use bi-directional LSTM to include contextual information from both directions into the model .","use
bi-directional LSTM
to include
contextual information
from
both directions into the model","bi-directional LSTM||to include||contextual information
contextual information||from||both directions into the model
",,"Model||use||bi-directional LSTM
",,,,,,
hyperparameters,"In the experiments , our model was trained using the Adam algorithm with a learning rate initialized at 0.001 .","our model
trained using
Adam algorithm
with
learning rate
initialized at
0.001","our model||trained using||Adam algorithm
Adam algorithm||with||learning rate
learning rate||initialized at||0.001
",,,"Hyperparameters||has||our model
",,,,,
hyperparameters,The dimension of the hidden layers of bi - LSTM is 100 .,"dimension
of
hidden layers
of
bi - LSTM
is
100","dimension||of||hidden layers
hidden layers||of||bi - LSTM
bi - LSTM||is||100
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,Word embeddings are initialized from GloVe 100 dimensional pre-trained embeddings .,"Word embeddings
initialized from
GloVe 100 dimensional pre-trained embeddings","Word embeddings||initialized from||GloVe 100 dimensional pre-trained embeddings
",,,"Hyperparameters||has||Word embeddings
",,,,,
hyperparameters,POS and dependency embeddings are randomly initialized with 40 - dimensional vectors .,"POS and dependency embeddings
randomly initialized with
40 - dimensional vectors","POS and dependency embeddings||randomly initialized with||40 - dimensional vectors
",,,"Hyperparameters||has||POS and dependency embeddings
",,,,,
hyperparameters,The embeddings are all updated during training .,"embeddings
updated during
training","embeddings||updated during||training
",,,"Hyperparameters||has||embeddings
",,,,,
hyperparameters,Dropping probability for dropout layers between stacked LSTM layers is 0.5 .,"Dropping probability
for
dropout layers
between
stacked LSTM layers
is
0.5","Dropping probability||for||dropout layers
dropout layers||between||stacked LSTM layers
stacked LSTM layers||is||0.5
",,,"Hyperparameters||has||Dropping probability
",,,,,
hyperparameters,The batch size is set as 30 .,"batch size
set as
30","batch size||set as||30
",,,"Hyperparameters||has||batch size
",,,,,
hyperparameters,We utilize an open source ILP solver 4 in our method .,"utilize
open source ILP solver",,,"Hyperparameters||utilize||open source ILP solver
",,,,,,
baselines,LSTM : This is the basic LSTM - based deletion method proposed by .,"LSTM
is
basic LSTM - based deletion method","LSTM||is||basic LSTM - based deletion method
",,,"Baselines||has||LSTM
",,,,,
baselines,"LSTM + : This is advanced version of the model proposed by , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word .","LSTM +
incorporated
dependency parse tree information
into
LSTM model
used
prediction
on
previous word
to help
prediction
on
current word","LSTM +||incorporated||dependency parse tree information
dependency parse tree information||into||LSTM model
LSTM +||used||prediction
prediction||on||previous word
previous word||to help||prediction
prediction||on||current word
",,,"Baselines||has||LSTM +
",,,,,
baselines,Traditional ILP :,Traditional ILP,,,,"Baselines||has||Traditional ILP
",,,,,
baselines,This is the ILP - based method proposed by .,"is
ILP - based method",,,,,,"Traditional ILP||is||ILP - based method
",,,
baselines,Abstractive seq2seq :,Abstractive seq2seq,,,,"Baselines||has||Abstractive seq2seq
",,,,,
baselines,This is an abstractive sequence - to - sequence model trained on 3.8 million Gigaword title - article pairs as described in Section 1 .,"is
abstractive sequence - to - sequence model
trained on
3.8 million Gigaword title - article pairs","abstractive sequence - to - sequence model||trained on||3.8 million Gigaword title - article pairs
",,,,,"Abstractive seq2seq||is||abstractive sequence - to - sequence model
",,,
results,We can see that indeed this abstractive method performed poorly in cross - domain settings .,"see that
abstractive method
performed
poorly
in
cross - domain settings","abstractive method||performed||poorly
poorly||in||cross - domain settings
",,"Results||see that||abstractive method
",,,,,,
results,"( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .","In
in - domain setting
with
our BiLSTM method
syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP )
performs
similarly to or better
than
LSTM + method
in terms of
both F1 and accuracy","our BiLSTM method||with||syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP )
our BiLSTM method||performs||similarly to or better
similarly to or better||than||LSTM + method
similarly to or better||in terms of||both F1 and accuracy
","in - domain setting||has||our BiLSTM method
","Results||In||in - domain setting
",,,,,,
results,This shows that our method is comparable to the LSTM + method in the in - domain setting .,"our method
comparable to
LSTM + method
in
in - domain setting","our method||comparable to||LSTM + method
LSTM + method||in||in - domain setting
",,,"Results||has||our method
",,,,,
results,"( 4 ) In the out - of - domain setting , our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM + methods .","out - of - domain setting
our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods
clearly outperform
LSTM and LSTM + methods","our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods||clearly outperform||LSTM and LSTM + methods
","out - of - domain setting||has||our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods
",,"Results||In||out - of - domain setting
",,,,,
results,( 5 ) The Traditional ILP method also works better than the LSTM and LSTM + methods in the out - of - domain setting .,"Traditional ILP method
works
better
than
LSTM and LSTM + methods
in
out - of - domain setting","Traditional ILP method||works||better
better||than||LSTM and LSTM + methods
LSTM and LSTM + methods||in||out - of - domain setting
",,,"Results||has||Traditional ILP method
",,,,,
results,But the Traditional ILP method performs worse in the in - domain setting than both the LSTM and LSTM + methods and our methods .,"performs
worse
in
in - domain setting
than both
LSTM and LSTM + methods and our methods","worse||in||in - domain setting
worse||than both||LSTM and LSTM + methods and our methods
",,,,,"Traditional ILP method||performs||worse
",,,
results,"Therefore , our method works reasonably well for both in - domain and out - ofdomain data .","works
reasonably well
for both
in - domain and out - ofdomain data","reasonably well||for both||in - domain and out - ofdomain data
",,,,,"our method||works||reasonably well
",,,
results,"We also notice that on Google News , adding the ILP layer decreased the sentence compression performance .","on
Google News
adding
ILP layer
decreased
sentence compression performance","Google News||adding||ILP layer
ILP layer||decreased||sentence compression performance
",,"Results||on||Google News
",,,,,,
results,"We can see that in the in - domain setting , our method does not have any advantage over the LSTM + method .","in
in - domain setting
our method
does not have
any advantage
over
LSTM + method","our method||does not have||any advantage
any advantage||over||LSTM + method
",,"Results||in||in - domain setting
",,,,,,
results,"But in the cross - domain setting , our method that uses ILP to impose syntax - based constraints clearly performs better than LSTM + when the amount of training data is relatively small .","cross - domain setting
our method
uses
ILP
to impose
syntax - based constraints
performs
better
than
LSTM +
when
amount of training data
is
relatively small","our method||performs||better
better||than||LSTM +
better||when||amount of training data
amount of training data||is||relatively small
our method||uses||ILP
ILP||to impose||syntax - based constraints
","cross - domain setting||has||our method
",,"Results||in||cross - domain setting
",,,,,
research-problem,Sentence Compression by Deletion with LSTMs,Sentence Compression by Deletion,,,,,"Contribution||has research problem||Sentence Compression by Deletion
",,,,
research-problem,"We present an LSTM approach to deletion - based sentence compression where the task is to translate a sentence into a sequence of zeros and ones , corresponding to token deletion decisions .",deletion - based sentence compression,,,,,"Contribution||has research problem||deletion - based sentence compression
",,,,
research-problem,Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence .,Sentence compression,,,,,"Contribution||has research problem||Sentence compression
",,,,
model,"In particular , we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models ( LSTMs ) to output surprisingly readable and informative compressions .","benefits from
very recent advances in deep learning
uses
word embeddings and Long Short Term Memory models ( LSTMs )
to output
surprisingly readable and informative compressions","word embeddings and Long Short Term Memory models ( LSTMs )||to output||surprisingly readable and informative compressions
",,"Model||benefits from||very recent advances in deep learning
Model||uses||word embeddings and Long Short Term Memory models ( LSTMs )
",,,,,,
model,"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings , in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges .","Trained on
corpus
of
less than two million
automatically extracted
parallel sentences
using
standard tool
to obtain
word embeddings","corpus||using||standard tool
standard tool||to obtain||word embeddings
corpus||of||less than two million
less than two million||automatically extracted||parallel sentences
",,"Model||Trained on||corpus
",,,,,,
results,"There is a significant difference in performance of the MIRA baseline and the LSTM models , both in terms of F1 - score and in accuracy .","significant difference
in
performance
of
MIRA baseline and the LSTM models
in terms of
F1 - score and in accuracy","significant difference||in||performance
performance||of||MIRA baseline and the LSTM models
performance||in terms of||F1 - score and in accuracy
",,,"Results||has||significant difference
",,,,,
results,More than 30 % of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20 % of MIRA .,"More than 30 %
of
golden compressions
could be
regenerated
by
LSTM systems
which is in
sharp contrast
with
20 %
of
MIRA","More than 30 %||which is in||sharp contrast
sharp contrast||with||20 %
20 %||of||MIRA
More than 30 %||of||golden compressions
golden compressions||could be||regenerated
regenerated||by||LSTM systems
",,,"Results||has||More than 30 %
",,,,,
results,"The differences in F- score between the three versions of LSTM are not significant , all scores are close to 0.81 .","differences
in
F- score
between
three versions of LSTM
are
not significant
scores
close to
0.81","scores||close to||0.81
differences||in||F- score
F- score||between||three versions of LSTM
three versions of LSTM||are||not significant
",,,"Results||has||scores
Results||has||differences
",,,,,
research-problem,Improving sentence compression by learning to predict gaze,sentence compression,,,,,"Contribution||has research problem||sentence compression
",,,,
model,We go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .,"suggesting that
eye - tracking recordings
to induce
better models
for
sentence compression
for
text simplification","eye - tracking recordings||to induce||better models
better models||for||sentence compression
sentence compression||for||text simplification
",,"Model||suggesting that||eye - tracking recordings
",,,,,,
model,"Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .","show how to use
existing eye - tracking recordings
to improve
induction
of
Long Short - Term Memory models ( LSTMs )
for
sentence compression","existing eye - tracking recordings||to improve||induction
induction||of||Long Short - Term Memory models ( LSTMs )
Long Short - Term Memory models ( LSTMs )||for||sentence compression
",,"Model||show how to use||existing eye - tracking recordings
",,,,,,
model,Our proposed model does not require that the gaze data and the compression data come from the same source .,"Our proposed model
does not require
gaze data and the compression data
come from
same source","Our proposed model||does not require||gaze data and the compression data
gaze data and the compression data||come from||same source
",,,"Model||has||Our proposed model
",,,,,
model,"Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .","use
gaze data
from
readers
of
Dundee Corpus
to improve
sentence compression results
on
several datasets","gaze data||to improve||sentence compression results
sentence compression results||on||several datasets
gaze data||from||readers
readers||of||Dundee Corpus
",,"Model||use||gaze data
",,,,,,
model,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .","intriguing potential
of
this work
in deriving
sentence simplification models
that are
personalized
for
individual users
based on
their reading behavior","intriguing potential||of||this work
this work||in deriving||sentence simplification models
sentence simplification models||that are||personalized
personalized||based on||their reading behavior
personalized||for||individual users
",,,"Model||has||intriguing potential
",,,,,
hyperparameters,Both the baseline and our systems are three - layer bi - LSTM models trained for 30 iterations with pretrained ( SENNA ) embeddings .,"Both the baseline and our systems
are
three - layer bi - LSTM models
trained for
30 iterations
with
pretrained ( SENNA ) embeddings","Both the baseline and our systems||are||three - layer bi - LSTM models
three - layer bi - LSTM models||trained for||30 iterations
30 iterations||with||pretrained ( SENNA ) embeddings
",,,"Hyperparameters||has||Both the baseline and our systems
",,,,,
hyperparameters,"The input and hidden layers are 50 dimensions , and at the output layer we predict sequences of two labels , indicating whether to delete the labeled word or not .","input and hidden layers
are
50 dimensions
output layer
predict
sequences
of
two labels","input and hidden layers||are||50 dimensions
output layer||predict||sequences
sequences||of||two labels
",,,"Hyperparameters||has||input and hidden layers
Hyperparameters||has||output layer
",,,,,
baselines,Our baseline ( BASELINE - LSTM ) is a multi - task learning 1 http://groups.inf.ed.ac.uk/ccg/,"BASELINE - LSTM
is
multi - task learning","BASELINE - LSTM||is||multi - task learning
",,,"Baselines||has||BASELINE - LSTM
",,,,,
baselines,bi -LSTM predicting both CCG supertags and sentence compression ( word deletion ) at the outer layer .,"predicting both
CCG supertags and sentence compression
at
outer layer","CCG supertags and sentence compression||at||outer layer
",,,,,"multi - task learning||predicting both||CCG supertags and sentence compression
",,,
results,"We observe that across all three datasets , including all three annotations of BROADCAST , gaze features lead to improvements over our baseline 3 - layer bi - LSTM .","across
all three datasets
including
all three annotations of BROADCAST
gaze features
lead to
improvements
over
baseline 3 - layer bi - LSTM","all three datasets||including||all three annotations of BROADCAST
gaze features||lead to||improvements
improvements||over||baseline 3 - layer bi - LSTM
","all three datasets||has||gaze features
","Results||across||all three datasets
",,,,,,
results,"Also , CASCADED - LSTM is consistently better than MULTITASK - LSTM . : Results ( F1 ) .","CASCADED - LSTM
is
consistently better
than
MULTITASK - LSTM","CASCADED - LSTM||is||consistently better
consistently better||than||MULTITASK - LSTM
",,,,,,,"all three datasets||has||CASCADED - LSTM
",
results,"For all three datasets , the inclusion of gaze measures ( first pass duration ( FP ) and regression duration ( Regr. ) ) leads to improvements over the baseline .","For
all three datasets
inclusion
of
gaze measures
first pass duration ( FP )
regression duration ( Regr. )
leads to
improvements
over
baseline","inclusion||of||gaze measures
gaze measures||leads to||improvements
improvements||over||baseline
","all three datasets||has||inclusion
gaze measures||name||first pass duration ( FP )
gaze measures||name||regression duration ( Regr. )
","Results||For||all three datasets
",,,,,,
results,"With the harder datasets , the impact of the gaze information becomes stronger , consistently favouring the cascaded architecture , and with improvements using both first pass duration and regression duration , the late measure associated with interpretation of content .","With
harder datasets
impact of
gaze information
becomes
stronger
favouring
cascaded architecture
with
improvements
using
first pass duration
regression duration","harder datasets||impact of||gaze information
gaze information||with||improvements
improvements||using||first pass duration
improvements||using||regression duration
gaze information||favouring||cascaded architecture
gaze information||becomes||stronger
",,"Results||With||harder datasets
",,,,,,
research-problem,A Language Model based Evaluator for Sentence Compression,Sentence Compression,,,,,"Contribution||has research problem||Sentence Compression
",,,,
research-problem,"We herein present a language - modelbased evaluator for deletion - based sentence compression , and viewed this task as a series of deletion - and - evaluation operations using the evaluator .",deletion - based sentence compression,,,,,"Contribution||has research problem||deletion - based sentence compression
",,,,
model,"To answer the above questions , a syntax - based neural language model is trained on large - scale datasets as a readability evaluator .","syntax - based neural language model
trained on
large - scale datasets
as
readability evaluator","syntax - based neural language model||trained on||large - scale datasets
large - scale datasets||as||readability evaluator
",,,"Model||has||syntax - based neural language model
",,,,,
model,The neural language model is supposed to learn the correct word collocations in terms of both syntax and semantics .,"neural language model
to learn
correct word collocations
in terms of
syntax
semantics","neural language model||to learn||correct word collocations
correct word collocations||in terms of||syntax
correct word collocations||in terms of||semantics
",,,"Model||has||neural language model
",,,,,
model,"Subsequently , we formulate the deletionbased sentence compression as a series of trialand - error deletion operations through a reinforcement learning framework .","formulate
deletionbased sentence compression
series of
trialand - error deletion operations
through
reinforcement learning framework","deletionbased sentence compression||series of||trialand - error deletion operations
trialand - error deletion operations||through||reinforcement learning framework
",,"Model||formulate||deletionbased sentence compression
",,,,,,
model,"The policy network performs either RETAIN or REMOVE action to form a compression , and receives a reward ( e.g. , readability score ) to update the network .","policy network
performs either
RETAIN
REMOVE
to form
compression
receives
reward ( e.g. , readability score )
to update
network","policy network||to form||compression
compression||performs either||RETAIN
compression||performs either||REMOVE
policy network||receives||reward ( e.g. , readability score )
reward ( e.g. , readability score )||to update||network
",,,"Model||has||policy network
",,,,,
baselines,We choose several strong baselines ; the first one is the dependency - tree - based method that considers the sentence compression task as an optimization problem by using integer linear programming 5 .,"dependency - tree - based method
considers
sentence compression task
as
optimization problem
by using
integer linear programming","dependency - tree - based method||considers||sentence compression task
sentence compression task||as||optimization problem
optimization problem||by using||integer linear programming
",,,"Baselines||has||dependency - tree - based method
",,,,,
baselines,The second method is the long short - term memory networks ( LSTMs ) which showed strong promise in sentence compression by .,"long short - term memory networks ( LSTMs )
showed
strong promise
in
sentence compression","long short - term memory networks ( LSTMs )||showed||strong promise
strong promise||in||sentence compression
",,,"Baselines||has||long short - term memory networks ( LSTMs )
",,,,,
hyperparameters,"The embedding size for word , part - of - speech tag , and the dependency relation is 128 .","embedding size
for
word
part - of - speech tag
dependency relation
is
128","embedding size||for||word
embedding size||for||part - of - speech tag
embedding size||for||dependency relation
embedding size||is||128
",,,"Hyperparameters||has||embedding size
",,,,,
hyperparameters,We employed the vanilla RNN with a hidden size of 512 for both the policy network and neural language model .,"employed
vanilla RNN
with
hidden size
of
512
for
policy network
neural language model","vanilla RNN||with||hidden size
hidden size||of||512
hidden size||for||policy network
hidden size||for||neural language model
",,"Hyperparameters||employed||vanilla RNN
",,,,,,
hyperparameters,"The mini - batch size was chosen from [ 5 , 50 , 100 ] .","mini - batch size
chosen from
[ 5 , 50 , 100 ]","mini - batch size||chosen from||[ 5 , 50 , 100 ]
",,,"Hyperparameters||has||mini - batch size
",,,,,
hyperparameters,"Vocabulary size was 50,000 .","Vocabulary size
was
50,000","Vocabulary size||was||50,000
",,,"Hyperparameters||has||Vocabulary size
",,,,,
hyperparameters,"The learning rate for neural language model is 2.5 e - 4 , and 1e - 05 for the policy network .","learning rate
for
neural language model
is
2.5 e - 4
1e - 05
for
policy network","learning rate||for||neural language model
neural language model||is||2.5 e - 4
1e - 05||for||policy network
","learning rate||has||1e - 05
",,"Hyperparameters||has||learning rate
",,,,,
hyperparameters,"For policy learning , we used the REINFORCE algorithm to update the parameters of the policy network and find an policy that maximizes the reward .","For
policy learning
used
REINFORCE algorithm
to update
parameters
of
policy network
find
policy
maximizes
reward","policy learning||find||policy
policy||maximizes||reward
policy learning||used||REINFORCE algorithm
REINFORCE algorithm||to update||parameters
parameters||of||policy network
",,"Hyperparameters||For||policy learning
",,,,,,
code,6 https://github.com/code4conference/code4sc,https://github.com/code4conference/code4sc,,,,,"Contribution||Code||https://github.com/code4conference/code4sc
",,,,
results,"( 1 ) As shown in , our Evaluator - SLMbased method yields a large improvement over the baselines , demonstrating that the language - modelbased evaluator is effective as a post-hoc grammar checker for the compressed sentences .","Evaluator - SLMbased method
yields
large improvement
over
baselines","Evaluator - SLMbased method||yields||large improvement
large improvement||over||baselines
",,,"Results||has||Evaluator - SLMbased method
",,,,,
results,"( 3 ) As for Google news dataset , LSTMs ( LSTM + pos+dep ) ( & 3 ) is a relatively strong baseline , suggesting that incorporating dependency relations and part - of - speech tags may help model learn the syntactic relations and thus make a better prediction .","for
Google news dataset
LSTMs ( LSTM + pos+dep )
is
relatively strong baseline
incorporating
dependency relations
part - of - speech tags","LSTMs ( LSTM + pos+dep )||is||relatively strong baseline
relatively strong baseline||incorporating||dependency relations
relatively strong baseline||incorporating||part - of - speech tags
","Google news dataset||has||LSTMs ( LSTM + pos+dep )
","Results||for||Google news dataset
",,,,,,
results,"When further applying Evaluator - SLM , only a tiny improvement is observed ( &3 vs & 4 ) , not comparable to the improvement between # 3 and # 5 .","applying
Evaluator - SLM
only a tiny improvement
observed","Evaluator - SLM||observed||only a tiny improvement
",,"Results||applying||Evaluator - SLM
",,,,,,
results,"For Gigaword dataset with 1.02 million instances , the perplexity of the language model is 20.3 , while for the Google news dataset with 0.2 million instances , the perplexity is 76.5 .","For
Gigaword dataset
with
1.02 million instances
perplexity
of
language model
is
20.3
with
0.2 million instances
perplexity
is
76.5","perplexity||is||76.5
Gigaword dataset||with||1.02 million instances
perplexity||of||language model
language model||is||20.3
","Gigaword dataset||has||perplexity
","Results||For||Gigaword dataset
",,,"Google news dataset||with||0.2 million instances
",,"Google news dataset||has||perplexity
",
results,"The results shows that small improvements are observed on two datasets ( # 4 vs # 5 ; & 4 vs & 5 ) , suggesting that incorporating syntactic knowledge may help evaluator to encourage more unseen but reasonable word collocations .","shows that
small improvements
observed on
two datasets","small improvements||observed on||two datasets
",,"Results||shows that||small improvements
",,,,,,
research-problem,MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXT,SPEECH EMOTION RECOGNITION,,,,,"Contribution||has research problem||SPEECH EMOTION RECOGNITION
",,,,
model,"To overcome these limitations , we propose a model that uses high - level text transcription , as well as low - level audio signals , to utilize the information contained within low - resource datasets to a greater degree .","uses
high - level text transcription
as well as
low - level audio signals
to utilize
information
contained within
low - resource datasets","high - level text transcription||to utilize||information
information||contained within||low - resource datasets
high - level text transcription||as well as||low - level audio signals
",,"Model||uses||high - level text transcription
",,,,,,
model,"In this paper , we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech .","propose
novel deep dual recurrent encoder model
simultaneously utilizes
audio and text data
in recognizing
emotions
from
speech","novel deep dual recurrent encoder model||simultaneously utilizes||audio and text data
audio and text data||in recognizing||emotions
emotions||from||speech
",,"Model||propose||novel deep dual recurrent encoder model
",,,,,,
hyperparameters,"Among the variants of the RNN function , we use GRUs as they yield comparable performance to that of the LSTM and include a smaller number of weight parameters .","of
use
GRUs
as they yield
comparable performance
to that of
LSTM
include
smaller number
weight parameters","GRUs||include||smaller number
smaller number||of||weight parameters
GRUs||as they yield||comparable performance
comparable performance||to that of||LSTM
",,"Hyperparameters||use||GRUs
",,,,,,
hyperparameters,"We use a max encoder step of 750 for the audio input , based on the implementation choices presented in and 128 for the text input because it covers the maximum length of the transcripts .","max encoder step
of
750
for
audio input
128
for
text input
covers
maximum length
of
transcripts","max encoder step||of||750
750||for||audio input
max encoder step||of||128
128||for||text input
text input||covers||maximum length
maximum length||of||transcripts
",,,"Hyperparameters||use||max encoder step
",,,,,
hyperparameters,"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .","vocabulary size
of
dataset
is
3,747
including
"" UNK "" token
represents
unknown words
"" PAD "" token
used to indicate
padding information
added while preparing
mini-batch data","vocabulary size||of||dataset
dataset||is||3,747
3,747||including||"" PAD "" token
"" PAD "" token||used to indicate||padding information
padding information||added while preparing||mini-batch data
3,747||including||"" UNK "" token
"" UNK "" token||represents||unknown words
",,,"Hyperparameters||has||vocabulary size
",,,,,
hyperparameters,"The number of hidden units and the number of layers in the RNN for each model ( ARE , TRE , MDRE and MDREA ) are selected based on extensive hyperparameter search experiments .","number of hidden units and the number of layers
in
RNN
for
each model ( ARE , TRE , MDRE and MDREA )
selected based on
extensive hyperparameter search experiments","number of hidden units and the number of layers||in||RNN
RNN||for||each model ( ARE , TRE , MDRE and MDREA )
each model ( ARE , TRE , MDRE and MDREA )||selected based on||extensive hyperparameter search experiments
",,,"Hyperparameters||has||number of hidden units and the number of layers
",,,,,
,The weights of the hidden units are initialized using orthogonal ,"weights
of
hidden units
initialized using
orthogonal",,,,,,,,,
hyperparameters,"weights ] , and the text embedding layer is initialized from pretrained word - embedding vectors .","text embedding layer
initialized from
pretrained word - embedding vectors","text embedding layer||initialized from||pretrained word - embedding vectors
",,,"Hyperparameters||has||text embedding layer
",,,,,
hyperparameters,"In preparing the textual dataset , we first use the released transcripts of the IEMOCAP dataset for simplicity .","released transcripts
of
IEMOCAP dataset
for
simplicity","released transcripts||of||IEMOCAP dataset
IEMOCAP dataset||for||simplicity
",,,"Hyperparameters||use||released transcripts
",,,,,
results,"First , our ARE model shows the baseline performance because we use minimal audio features , such as the MFCC and prosodic features with simple architectures .","ARE model
shows
baseline performance
use
minimal audio features
such as
MFCC and prosodic features
with
simple architectures","ARE model||shows||baseline performance
baseline performance||use||minimal audio features
minimal audio features||such as||MFCC and prosodic features
MFCC and prosodic features||with||simple architectures
",,,"Results||has||ARE model
",,,,,
results,"On the other hand , the TRE model shows higher performance gain compared to the ARE .","TRE model
shows
higher performance gain
compared to
ARE","TRE model||shows||higher performance gain
higher performance gain||compared to||ARE
",,,"Results||has||TRE model
",,,,,
results,"From this result , we note that textual data are informative in emotion prediction tasks , and the recurrent encoder model is effective in understanding these types of sequential data .","note
textual data
are
informative
in
emotion prediction tasks
recurrent encoder model
is
effective
in understanding
types of sequential data","recurrent encoder model||is||effective
effective||in understanding||types of sequential data
textual data||are||informative
informative||in||emotion prediction tasks
",,"Results||note||recurrent encoder model
Results||note||textual data
",,,,,,
results,"Second , the newly proposed model , MDRE , shows a substantial performance gain .","MDRE
shows
substantial performance gain","MDRE||shows||substantial performance gain
",,,"Results||has||MDRE
",,,,,
results,It thus achieves the state - of - the - art performance with a WAP value of 0.718 .,"achieves
state - of - the - art performance
with
WAP value
of
0.718","state - of - the - art performance||with||WAP value
WAP value||of||0.718
",,,,,"MDRE||achieves||state - of - the - art performance
",,,
results,"Lastly , the attention model , MDREA , also outperforms the best existing research results ( WAP 0.690 to 0.688 ) .","MDREA
outperforms
best existing research results ( WAP 0.690 to 0.688 )","MDREA||outperforms||best existing research results ( WAP 0.690 to 0.688 )
",,,"Results||has||MDREA
",,,,,
results,The label accuracy of the processed transcripts is 5.53 % WER .,"label accuracy
of
processed transcripts
is
5.53 % WER","label accuracy||of||processed transcripts
processed transcripts||is||5.53 % WER
",,,"Results||has||label accuracy
",,,,,
results,"The TRE - ASR , MDRE - ASR and MDREA - ASR models reflect degraded performance compared to that of the TRE , MDRE and MDREA models .","TRE - ASR , MDRE - ASR and MDREA - ASR models
reflect
degraded performance
compared to
TRE , MDRE and MDREA models","TRE - ASR , MDRE - ASR and MDREA - ASR models||reflect||degraded performance
degraded performance||compared to||TRE , MDRE and MDREA models
",,,"Results||has||TRE - ASR , MDRE - ASR and MDREA - ASR models
",,,,,
ablation-analysis,"The ARE model ( ) incorrectly classifies most instances of happy as neutral ( 43.51 % ) ; thus , it shows reduced accuracy ( 35.15 % ) in predicting the the happy class .","ARE model
incorrectly classifies
most instances of happy
as
neutral ( 43.51 % )","ARE model||incorrectly classifies||most instances of happy
most instances of happy||as||neutral ( 43.51 % )
",,,"Ablation analysis||has||ARE model
",,,,,
ablation-analysis,"Overall , most of the emotion classes are frequently confused with the neutral class .","emotion classes
frequently confused with
neutral class","emotion classes||frequently confused with||neutral class
",,,,,,,"ARE model||has||emotion classes
",
ablation-analysis,"Interestingly , the TRE model ( ) shows greater prediction gains in predicting the happy class when compared to the ARE model ( 35.15 % to 75. 73 % ) .","TRE model
shows
greater prediction gains
in predicting
happy class
compared to
ARE model ( 35.15 % to 75. 73 % )","TRE model||shows||greater prediction gains
greater prediction gains||in predicting||happy class
",,,"Ablation analysis||has||TRE model
",,,"happy class||compared to||ARE model ( 35.15 % to 75. 73 % ) 
",,
ablation-analysis,The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .,"MDRE model
compensates for
weaknesses
of
previous two models ( ARE and TRE )
benefits from
strengths
to
surprising degree","MDRE model||benefits from||strengths
strengths||to||surprising degree
MDRE model||compensates for||weaknesses
weaknesses||of||previous two models ( ARE and TRE )
",,,"Ablation analysis||has||MDRE model
",,,,,
ablation-analysis,"Furthermore , the occurrence of the incorrect "" sad - to - happy "" cases in the TRE model is reduced from 16 . 20 % to 9.15 % .","occurrence
of
incorrect "" sad - to - happy "" cases
in
TRE model
reduced from
16 . 20 % to 9.15 %","occurrence||of||incorrect "" sad - to - happy "" cases
incorrect "" sad - to - happy "" cases||in||TRE model
TRE model||reduced from||16 . 20 % to 9.15 %
",,,,,,,"MDRE model||has||occurrence
",
research-problem,EEG - Based Emotion Recognition Using Regularized Graph Neural Networks,EEG - Based Emotion Recognition,,,,,"Contribution||has research problem||EEG - Based Emotion Recognition
",,,,
research-problem,"E MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc .",E MOTION recognition,,,,,"Contribution||has research problem||E MOTION recognition
",,,,
model,"In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .","propose
regularized graph neural network ( RGNN )",,,"Model||propose||regularized graph neural network ( RGNN )
",,,,,,
model,"Inspired by , , we consider each channel in EEG signals as a node in our graph .","consider
each channel
in
EEG signals
as
node
in
our graph","each channel||in||EEG signals
EEG signals||as||node
node||in||our graph
",,"Model||consider||each channel
",,,,,,
model,"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .","RGNN model
extends
simple graph convolution network ( SGC )
leverages
topological structure
of
EEG signals
according to
economy of brain network organization
propose
biologically supported sparse adjacency matrix
to capture
local and global inter-channel relations","RGNN model||extends||simple graph convolution network ( SGC )
RGNN model||leverages||topological structure
topological structure||of||EEG signals
EEG signals||according to||economy of brain network organization
economy of brain network organization||propose||biologically supported sparse adjacency matrix
biologically supported sparse adjacency matrix||to capture||local and global inter-channel relations
",,,"Model||has||RGNN model
",,,,,
model,"Local interchannel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale , .","Local interchannel relations
connect
nearby groups of neurons
reveal
anatomical connectivity
at
macroscale","Local interchannel relations||reveal||anatomical connectivity
anatomical connectivity||at||macroscale
Local interchannel relations||connect||nearby groups of neurons
",,,"Model||has||Local interchannel relations
",,,,,
model,"Global inter-channel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotion - related functional connectivity , .","Global inter-channel relations
connect
distant groups of neurons
between
left and right hemispheres
may reveal
emotion - related functional connectivity","Global inter-channel relations||connect||distant groups of neurons
distant groups of neurons||between||left and right hemispheres
Global inter-channel relations||may reveal||emotion - related functional connectivity
",,,"Model||has||Global inter-channel relations
",,,,,
hyperparameters,"For our RGNN in all experiments , we empirically set the number of convolutional layers L = 2 , dropout rate of 0.7 at the output fully - connected layer , and batch size of 16 .","RGNN
empirically
set
number
of
convolutional layers L = 2
dropout rate
of
0.7
at
output fully - connected layer
batch size
of
16","RGNN||empirically||set
number||of||convolutional layers L = 2
batch size||of||16
dropout rate||of||0.7
0.7||at||output fully - connected layer
","set||has||number
set||has||batch size
set||has||dropout rate
",,"Hyperparameters||has||RGNN
",,,,,
hyperparameters,"We use Adam optimization with default values , i.e. , ? 1 = 0.9 and ? 2 = 0.999 .","use
Adam optimization
with
default values
i.e.
? 1 = 0.9 and ? 2 = 0.999","Adam optimization||with||default values
default values||i.e.||? 1 = 0.9 and ? 2 = 0.999
",,"Hyperparameters||use||Adam optimization
",,,,,,
hyperparameters,"We only tune the output feature dimension d , label noise level , learning rate ? , L1 regularization factor ? , and L2 regularization for each experiment .","tune
output feature dimension d
label noise level
learning rate ?
L1 regularization factor ?
L2 regularization",,,"Hyperparameters||tune||output feature dimension d
Hyperparameters||tune||label noise level
Hyperparameters||tune||learning rate ?
Hyperparameters||tune||L1 regularization factor ?
Hyperparameters||tune||L2 regularization
",,,,,,
results,It is encouraging to see that our model achieves superior performance on both datasets as compared to all baselines including the stateof - the - art BiHDM when DE features from all frequency bands are used .,"our model
achieves
superior performance
on
both datasets
compared to
all baselines
including
stateof - the - art BiHDM when DE features from all frequency bands are used","our model||achieves||superior performance
superior performance||on||both datasets
both datasets||compared to||all baselines
all baselines||including||stateof - the - art BiHDM when DE features from all frequency bands are used
",,,"Results||has||our model
",,,,,
results,It is worth noting that our model improves the accuracy of the state - of - the - art model on SEED - IV by around 5 % .,"improves
accuracy
of
state - of - the - art model
on
SEED - IV
by
around 5 %","accuracy||of||state - of - the - art model
state - of - the - art model||on||SEED - IV
SEED - IV||by||around 5 %
",,,,,"our model||improves||accuracy
",,,
results,"In particular , our model performs better than DGCNN , which is another GNN - based model that leverages the topological structure in EEG signals .","performs
better
than
DGCNN
another
GNN - based model
leverages
topological structure
in
EEG signals","better||than||DGCNN
DGCNN||another||GNN - based model
GNN - based model||leverages||topological structure
topological structure||in||EEG signals
",,,,,"our model||performs||better
",,,
results,"In subject - dependent experiments on SEED , STRNN achieves the highest accuracy in delta , theta and alpha bands , BiDANN performs best in beta band , and our model performs best in gamma band .","In
subject - dependent experiments
on
SEED
STRNN
achieves
highest accuracy
in
delta , theta and alpha bands
BiDANN
performs
best
in
beta band
our model
performs
best
in
gamma band","our model||performs||best
subject - dependent experiments||on||SEED
BiDANN||performs||best
best||in||beta band
STRNN||achieves||highest accuracy
highest accuracy||in||delta , theta and alpha bands
our model||performs||best
best||in||gamma band
","SEED||has||our model
SEED||has||BiDANN
SEED||has||STRNN
SEED||has||our model
SEED||has||our model
","Results||In||subject - dependent experiments
",,,,,,
results,"In subject - independent experiments on SEED , BiDANN - S achieves the highest accuracy in theta and alpha bands , and our model performs best in delta , beta and gamma bands .","subject - independent experiments
on
SEED
BiDANN - S
achieves
highest accuracy
in
theta and alpha bands
our model
performs
best
in
delta , beta and gamma bands","subject - independent experiments||on||SEED
BiDANN - S||achieves||highest accuracy
highest accuracy||in||theta and alpha bands
best||in||delta , beta and gamma bands
","SEED||has||BiDANN - S
",,"Results||In||subject - independent experiments
",,,,,
results,"For both subject - dependent and subjectindependent settings on SEED , we compare the performance of each model across different frequency bands .","For
subject - dependent and subjectindependent settings
on
SEED","subject - dependent and subjectindependent settings||on||SEED
",,"Results||For||subject - dependent and subjectindependent settings
",,,,,,
results,"In general , most models including our model achieve better performance on beta and gamma bands than delta , theta and alpha bands , with one exception of STRNN , which performs the worst on gamma band .","our model
achieve
better performance
on
beta and gamma bands
than
delta , theta and alpha bands
with one exception of
STRNN
performs
worst
on
gamma band","our model||achieve||better performance
better performance||with one exception of||STRNN
STRNN||performs||worst
worst||on||gamma band
better performance||on||beta and gamma bands
beta and gamma bands||than||delta , theta and alpha bands
",,,,,,,,
results,"One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band , whereas other models perform comparably in both bands , indicating that gamma band maybe the most discriminative band for our model .","consistently better
in
gamma band
than
beta band","consistently better||in||gamma band
gamma band||than||beta band
",,,,,,,"our model||performs||consistently better
",
ablation-analysis,"The two major designs in our adjacency matrix A , i.e. , global connection and symmetric adjacency matrix designs , are helpful in recognizing emotions .","two major designs
in
adjacency matrix A
i.e.
global connection and symmetric adjacency matrix designs
helpful in
recognizing emotions","two major designs||in||adjacency matrix A
adjacency matrix A||helpful in||recognizing emotions
adjacency matrix A||i.e.||global connection and symmetric adjacency matrix designs
",,,"Ablation analysis||has||two major designs
",,,,,
ablation-analysis,"The global connection models the asymmetric difference between neuronal activities in the left and right hemispheres and have been shown to reveal certain emotions , , .","global connection
models
asymmetric difference
between
neuronal activities
in
left and right hemispheres
shown to reveal
certain emotions","global connection||models||asymmetric difference
asymmetric difference||between||neuronal activities
neuronal activities||in||left and right hemispheres
global connection||shown to reveal||certain emotions
",,,"Ablation analysis||has||global connection
",,,,,
ablation-analysis,"Our NodeDAT regularizer has a noticeable positive impact on the performance of our model , which demonstrates that domain adaptation is significantly helpful in crosssubject classification .","NodeDAT regularizer
noticeable positive impact
on
performance
of
our model","noticeable positive impact||on||performance
performance||of||our model
performance||of||our model
performance||of||our model
","NodeDAT regularizer||has||noticeable positive impact
",,"Ablation analysis||has||NodeDAT regularizer
",,,,,
ablation-analysis,"In addition , if NodeDAT is removed , the performance of our model has a greater variance , demonstrating the importance of NodeDAT in improving the robustness of our model against cross - subject variations .","if
NodeDAT
is
removed
performance
of
our model
greater variance","NodeDAT||is||removed
","removed||has||performance
our model||has||greater variance
","Ablation analysis||if||NodeDAT
",,,,,,
ablation-analysis,DL regularizer improves performance of our model by around 3 % in accuracy on both datasets .,"DL regularizer
improves
performance
of
our model
by
around 3 %
in
accuracy
on
both datasets","DL regularizer||improves||performance
our model||by||around 3 %
around 3 %||in||accuracy
accuracy||on||both datasets
",,,"Ablation analysis||has||DL regularizer
",,,,,
research-problem,DialogueRNN : An Attentive RNN for Emotion Detection in Conversations,Emotion Detection in Conversations,,,,,"Contribution||has research problem||Emotion Detection in Conversations
",,,,
model,Our proposed DialogueRNN system employs three gated recurrent units ( GRU ) to model these aspects .,"DialogueRNN system
employs
three gated recurrent units ( GRU )","DialogueRNN system||employs||three gated recurrent units ( GRU )
",,,"Model||has||DialogueRNN system
",,,,,
model,"The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .","incoming utterance
fed into
two GRUs
called
global GRU and party GRU","incoming utterance||fed into||two GRUs
two GRUs||called||global GRU and party GRU
",,,"Model||has||incoming utterance
",,,,,
model,All rights reserved .,,,,,,,,,,
model,"state , respectively .",,,,,,,,,,
model,The global GRU encodes corresponding party information while encoding an utterance .,"global GRU
encodes
corresponding party information
while encoding
utterance","global GRU||encodes||corresponding party information
corresponding party information||while encoding||utterance
",,,"Model||has||global GRU
",,,,,
model,Attending over this GRU gives contextual representation that has information of all preceding utterances by different parties in the conversation .,"Attending over
gives contextual representation
has information of
all preceding utterances
by
different parties
in
conversation","gives contextual representation||has information of||all preceding utterances
all preceding utterances||by||different parties
different parties||in||conversation
",,,,,"global GRU||Attending over||gives contextual representation
",,,
model,The speaker state depends on this context through attention and the speaker 's previous state .,"speaker state
depends on
context
through
attention and the speaker 's previous state","speaker state||depends on||context
context||through||attention and the speaker 's previous state
",,,"Model||has||speaker state
",,,,,
model,"This ensures that at time t , the speaker state directly gets information from the speaker 's previous state and global GRU which has information on the preceding parties .","at time t
speaker state
directly gets information from
speaker 's previous state
global GRU
which has information on
preceding parties","speaker state||directly gets information from||speaker 's previous state
speaker state||directly gets information from||global GRU
global GRU||which has information on||preceding parties
",,"Model||at time t||speaker state
",,,,,,
model,"Finally , the updated speaker state is fed into the emotion GRU to decode the emotion representation of the given utterance , which is used for emotion classification .","updated speaker state
fed into
emotion GRU
to decode
emotion representation
of
given utterance
used for
emotion classification","updated speaker state||fed into||emotion GRU
emotion GRU||to decode||emotion representation
emotion representation||of||given utterance
given utterance||used for||emotion classification
",,,"Model||has||updated speaker state
",,,,,
model,"At time t , the emotion GRU cell gets the emotion representation of t ? 1 and the speaker state of t .","At time t
emotion GRU cell
gets
emotion representation of t ? 1 and the speaker state of t","emotion GRU cell||gets||emotion representation of t ? 1 and the speaker state of t
",,"Model||At time t||emotion GRU cell
",,,,,,
baselines,c - LSTM : Biredectional LSTM is used to capture the context from the surrounding utterances to generate contextaware utterance representation .,"c - LSTM
Biredectional LSTM
is
used to capture
context
from
surrounding utterances
to generate
contextaware utterance representation","c - LSTM||is||Biredectional LSTM
Biredectional LSTM||used to capture||context
context||from||surrounding utterances
surrounding utterances||to generate||contextaware utterance representation
",,,"Baselines||has||c - LSTM
",,,,,
baselines,c- LSTM+ Att :,c- LSTM+ Att,,,,"Baselines||has||c- LSTM+ Att
",,,,,"c- LSTM+ Att||has||variant attention
"
baselines,In this variant attention is applied applied to the c - LSTM output at each timestamp by following Eqs. and .,"variant attention
applied to
c - LSTM output
at
each timestamp","variant attention||at||each timestamp
variant attention||applied to||c - LSTM output
",,,,,,,,
baselines,TFN :,TFN,,,,"Baselines||has||TFN
",,,,,"TFN||has||Tensor outer product
"
baselines,This is specific to multimodal scenario .,"specific to
multimodal scenario",,,,,,"TFN||specific to||multimodal scenario
",,,
baselines,Tensor outer product is used to capture intermodality and intra-modality interactions .,"Tensor outer product
used to capture
intermodality and intra-modality interactions","Tensor outer product||used to capture||intermodality and intra-modality interactions
",,,,,,,,
baselines,This model does not capture context from surrounding utterances .,,,,,,,,,,
baselines,"MFN ) : Specific to multimodal scenario , this model utilizes multi-view learning by modeling view - specific and cross - view interactions .","MFN
Specific to
multimodal scenario
utilizes
multi-view learning
by modeling
view - specific and cross - view interactions","MFN||Specific to||multimodal scenario
MFN||utilizes||multi-view learning
multi-view learning||by modeling||view - specific and cross - view interactions
",,,"Baselines||has||MFN
",,,,,
baselines,CNN : This is identical to our textual feature extractor network ( Section 3.2 ) and it does not use contextual information from the surrounding utterances .,"CNN
identical to
our textual feature extractor network
does not use
contextual information","CNN||identical to||our textual feature extractor network
CNN||does not use||contextual information
",,,"Baselines||has||CNN
",,,,,
baselines,"Memnet : As described in , the current utterance is fed to a memory network , where the memories correspond to preceding utterances .","Memnet
current utterance
fed to
memory network
where
memories
correspond to
preceding utterances","current utterance||fed to||memory network
memory network||where||memories
memories||correspond to||preceding utterances
","Memnet||has||current utterance
",,"Baselines||has||Memnet
",,,,,"Memnet||has||output
"
baselines,The output from the memory network is used as the final utterance representation for emotion classification .,"output
from
memory network
used as
final utterance representation
for
emotion classification","output||from||memory network
memory network||used as||final utterance representation
final utterance representation||for||emotion classification
",,,,,,,,
baselines,CMN : This state - of - the - art method models utterance context from dialogue history using two distinct GRUs for two speakers .,"CMN
state - of - the - art method
models
utterance context
from
dialogue history
using
two distinct GRUs
for
two speakers","state - of - the - art method||models||utterance context
utterance context||from||dialogue history
dialogue history||using||two distinct GRUs
two distinct GRUs||for||two speakers
","CMN||has||state - of - the - art method
",,"Baselines||has||CMN
",,,,,
results,"As expected , on average Di - alogue RNN outperforms all the baseline methods , including the state - of - the - art CMN , on both of the datasets .","on average
Di - alogue RNN
outperforms
all the baseline methods
including
state - of - the - art CMN","Di - alogue RNN||outperforms||all the baseline methods
all the baseline methods||including||state - of - the - art CMN
",,"Results||on average||Di - alogue RNN
",,,,,,
results,"As evidenced by , for IEMOCAP dataset , our model surpasses the state - of - the - art method CMN by 2.77 % accuracy and 3.76 % f 1 - score on average .","by
for
IEMOCAP dataset
our model
surpasses
state - of - the - art method CMN
2.77 % accuracy
3.76 % f 1 - score","our model||surpasses||state - of - the - art method CMN
state - of - the - art method CMN||by||2.77 % accuracy
state - of - the - art method CMN||by||3.76 % f 1 - score
","IEMOCAP dataset||has||our model
","Results||for||IEMOCAP dataset
",,,,,,
results,"AVEC DialogueRNN outperforms CMN for valence , arousal , expectancy , and power attributes ; see .","AVEC
DialogueRNN
outperforms
CMN
for
valence , arousal , expectancy , and power attributes","DialogueRNN||outperforms||CMN
CMN||for||valence , arousal , expectancy , and power attributes
","AVEC||has||DialogueRNN
",,"Results||has||AVEC
",,,,,
results,DialogueRNN vs. DialogueRNN Variants,DialogueRNN vs. DialogueRNN Variants,,,,"Results||has||DialogueRNN vs. DialogueRNN Variants
",,,,,"DialogueRNN vs. DialogueRNN Variants||has||DialogueRNN l
"
results,DialogueRNN l :,DialogueRNN l,,,,,,,,,
results,"Following , using explicit listener state update yields slightly worse performance than regular DialogueRNN .","using
explicit listener state update
yields
slightly worse performance
than
regular DialogueRNN","explicit listener state update||yields||slightly worse performance
slightly worse performance||than||regular DialogueRNN
",,,,,"DialogueRNN l||using||explicit listener state update
",,,
results,"BiDialogueRNN : Since BiDialogueRNN captures context from the future utterances , we expect improved performance from it over DialogueRNN .",BiDialogueRNN,,,,,,,,"DialogueRNN vs. DialogueRNN Variants||has||BiDialogueRNN
",
results,"This is confirmed in , where BiDialogueRNN outperforms Dialogue RNN on average on both datasets .","outperforms
Dialogue RNN
on
both datasets","Dialogue RNN||on||both datasets
",,,,,"BiDialogueRNN||outperforms||Dialogue RNN
",,,
research-problem,Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,Emotion Recognition in Dyadic Dialogue Videos,,,,,"Contribution||has research problem||Emotion Recognition in Dyadic Dialogue Videos
",,,,
research-problem,Emotion recognition in conversations is crucial for the development of empathetic machines .,Emotion recognition in conversations,,,,,"Contribution||has research problem||Emotion recognition in conversations
",,,,
research-problem,"Emotion detection from such resources can benefit numerous fields like counseling , public opinion mining , financial forecasting , and intelligent systems such as smart homes and chatbots .",Emotion detection,,,,,"Contribution||has research problem||Emotion detection
",,,,
research-problem,"In this paper , we analyze emotion detection in videos of dyadic conversations .",emotion detection in videos of dyadic conversations,,,,,"Contribution||has research problem||emotion detection in videos of dyadic conversations
",,,,
model,"We propose a conversational memory network ( CMN ) , which uses a multimodal approach for emotion detection in utterances ( a unit of speech bound by breathes or pauses ) of such conversational videos .","propose
conversational memory network ( CMN )
uses
multimodal approach
for
emotion detection
in
utterances ( a unit of speech bound by breathes or pauses )
of
conversational videos","conversational memory network ( CMN )||uses||multimodal approach
multimodal approach||for||emotion detection
emotion detection||in||utterances ( a unit of speech bound by breathes or pauses )
utterances ( a unit of speech bound by breathes or pauses )||of||conversational videos
",,"Model||propose||conversational memory network ( CMN )
",,,,,,
model,Our proposed CMN incorporates these factors by using emotional context information present in the conversation history .,"proposed CMN
by using
emotional context information
present in
conversation history","proposed CMN||by using||emotional context information
emotional context information||present in||conversation history
",,,"Model||has||proposed CMN
",,,,,
model,It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long - term dependencies and summarizing task - specific details using attention models .,"improves
speakerbased emotion modeling
by using
memory networks
efficient in capturing
long - term dependencies
summarizing
task - specific details","speakerbased emotion modeling||by using||memory networks
memory networks||efficient in capturing||long - term dependencies
",,,,,"proposed CMN||improves||speakerbased emotion modeling
","memory networks||summarizing||task - specific details 
",,
model,"Specifically , the memory cells of CMN are continuous vectors that store the context information found in the utterance histories .","memory cells
of
CMN
are
continuous vectors
store
context information
found in
utterance histories","memory cells||of||CMN
CMN||are||continuous vectors
continuous vectors||store||context information
context information||found in||utterance histories
",,,"Model||has||memory cells
",,,,,
model,CMN also models interplay of these memories to capture interspeaker dependencies .,"CMN
models
interplay
of
memories
to capture
interspeaker dependencies","CMN||models||interplay
interplay||of||memories
memories||to capture||interspeaker dependencies
",,,"Model||has||CMN
",,,,,
model,"CMN first extracts multimodal features ( audio , visual , and text ) for all utterances in a video .","extracts
multimodal features ( audio , visual , and text )
for
all utterances
in
video","multimodal features ( audio , visual , and text )||for||all utterances
all utterances||in||video
",,,,,"CMN||extracts||multimodal features ( audio , visual , and text )
",,,
hyperparameters,We use 10 % of the training set as a held - out validation set for hyperparameter tuning .,"use
10 %
of
training set
as
held - out validation set
for
hyperparameter tuning","10 %||of||training set
training set||as||held - out validation set
held - out validation set||for||hyperparameter tuning
",,"Hyperparameters||use||10 %
",,,,,,
hyperparameters,"To optimize the parameters , we use Stochastic Gradient Descent ( SGD ) optimizer , starting with an initial learning Utterances whose history has atleast 3 similar emotion labels in either own history or the history of the other person , is counted in case 1 or 2 , respectively .","Stochastic Gradient Descent ( SGD ) optimizer
starting with
initial learning Utterances
whose history has
atleast 3 similar emotion labels","Stochastic Gradient Descent ( SGD ) optimizer||starting with||initial learning Utterances
initial learning Utterances||whose history has||atleast 3 similar emotion labels
",,,"Hyperparameters||use||Stochastic Gradient Descent ( SGD ) optimizer
",,,,,
hyperparameters,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,"annealing approach
halves
lr
every
20 epochs
termination
decided using
early - stop measure
with
patience
of
12
by monitoring
validation loss","annealing approach||halves||lr
lr||every||20 epochs
termination||decided using||early - stop measure
early - stop measure||with||patience
patience||of||12
12||by monitoring||validation loss
","annealing approach||has||termination
",,"Hyperparameters||has||annealing approach
",,,,,
hyperparameters,Gradient clipping is used for regularization with a norm set to 40 .,"Gradient clipping
used for
regularization
with
norm
set to
40","Gradient clipping||used for||regularization
regularization||with||norm
norm||set to||40
",,,"Hyperparameters||has||Gradient clipping
",,,,,
hyperparameters,Hyperparameters are decided using a Random Search .,"decided using
Random Search",,,"Hyperparameters||decided using||Random Search
",,,,,,
hyperparameters,"Based on validation performance , context window length K is set to be 40 and the number of hops R is fixed at 3 hops .","Based on
validation performance
context window length K
set to
40
number of hops R
fixed at
3 hops","number of hops R||fixed at||3 hops
context window length K||set to||40
","validation performance||has||number of hops R
validation performance||has||context window length K
","Hyperparameters||Based on||validation performance
",,,,,,
hyperparameters,The dimension size of the memory cells d is set as 50 .,"dimension size
of
memory cells d
set as
50","dimension size||of||memory cells d
memory cells d||set as||50
",,,"Hyperparameters||has||dimension size
",,,,,
baselines,SVM - ensemble :,SVM - ensemble,,,,"Baselines||has||SVM - ensemble
",,,,,"SVM - ensemble||has||strong context - free benchmark model
"
baselines,A strong context - free benchmark model which uses similar multimodal approach on an ensemble of trees .,"strong context - free benchmark model
which uses
similar multimodal approach
on
ensemble of trees","strong context - free benchmark model||which uses||similar multimodal approach
similar multimodal approach||on||ensemble of trees
",,,,,,,,
baselines,bc - LSTM :,bc - LSTM,,,,"Baselines||has||bc - LSTM
",,,,,"bc - LSTM||has||bi-directional LSTM
"
baselines,"A bi-directional LSTM equipped with hierarchical fusion , proposed by .","bi-directional LSTM
equipped with
hierarchical fusion","bi-directional LSTM||equipped with||hierarchical fusion
",,,,,,,,
baselines,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .","Memn2n
generates
memory representations
for
each historical utterance
using
embedding matrix B
without
sequential modeling","Memn2n||generates||memory representations
memory representations||for||each historical utterance
each historical utterance||using||embedding matrix B
embedding matrix B||without||sequential modeling
",,,"Baselines||has||Memn2n
",,,,,
baselines,"[ 1 , K ] } for ? ? {a , b}. CMN Self :",CMN Self,,,,"Baselines||has||CMN Self
",,,,,
baselines,"In this baseline , we use only self history for classifying emotion of utterance u i .","use
only self history
for classifying
emotion
of
utterance u i","only self history||for classifying||emotion
emotion||of||utterance u i
",,,,,"CMN Self||use||only self history
",,,
baselines,CMN N A : Single layer variant of the CMN with no attention module .,"CMN N A
Single layer variant
of
CMN
with
no attention module","Single layer variant||of||CMN
CMN||with||no attention module
","CMN N A||has||Single layer variant
",,"Baselines||has||CMN N A
",,,,,
results,This suggests that gathering contexts temporally through sequential processing is indeed a superior method over non-temporal memory representations .,"suggests
gathering contexts temporally
through
sequential processing
is
superior method
over
non-temporal memory representations","gathering contexts temporally||through||sequential processing
sequential processing||is||superior method
superior method||over||non-temporal memory representations
",,"Results||suggests||gathering contexts temporally
",,,,,,
results,CMN self which uses only single history channel also provides lesser performance when compared to CMN .,"CMN self
uses only
single history channel
provides
lesser performance
compared to
CMN","CMN self||provides||lesser performance
lesser performance||compared to||CMN
CMN self||uses only||single history channel
",,,"Results||has||CMN self
",,,,,
results,"Overall , predictions on valence and arousal levels also show similar results which reinforce our hypothesis of CMN 's ability to model emotional dynamics .","predictions
on
valence and arousal levels
show
similar results","predictions||on||valence and arousal levels
valence and arousal levels||show||similar results
",,,"Results||has||predictions
",,,,,
research-problem,Progressive Self - Supervised Attention Learning for Aspect - Level Sentiment Analysis,Aspect - Level Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - Level Sentiment Analysis
",,,,
research-problem,"In aspect - level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect .",aspect - level sentiment classification ( ASC ),,,,,"Contribution||has research problem||aspect - level sentiment classification ( ASC )
",,,,
research-problem,"In this paper , we propose a progressive self - supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms .",neural ASC,,,,,"Contribution||has research problem||neural ASC
",,,,
research-problem,"Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .","Aspect - level sentiment classification ( ASC )
sentiment analysis",,,,,"Contribution||has research problem||Aspect - level sentiment classification ( ASC )
Contribution||has research problem||sentiment analysis
",,,,
research-problem,"However , the existing attention mechanism in ASC suffers from a major drawback .",ASC,,,,,"Contribution||has research problem||ASC
",,,,
approach,"In this paper , we propose a novel progressive self - supervised attention learning approach for neural ASC models .","propose
novel progressive self - supervised attention learning approach
for
neural ASC models","novel progressive self - supervised attention learning approach||for||neural ASC models
",,"Approach||propose||novel progressive self - supervised attention learning approach
",,,,,,
approach,"Our method is able to automatically and incrementally mine attention supervision information from a training corpus , which can be exploited to guide the training of attention mechanisms in ASC models .","able to automatically and incrementally mine
attention supervision information
from
training corpus
exploited to guide
training
of
attention mechanisms
in
ASC models","attention supervision information||exploited to guide||training
training||of||attention mechanisms
attention mechanisms||in||ASC models
attention supervision information||from||training corpus
",,"Approach||able to automatically and incrementally mine||attention supervision information
",,,,,,
approach,The basic idea behind our approach roots in the following fact : the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence .,"roots in
context word
with
maximum attention weight
greatest impact
on
sentiment prediction
of
input sentence","context word||with||maximum attention weight
greatest impact||on||sentiment prediction
sentiment prediction||of||input sentence
","maximum attention weight||has||greatest impact
","Approach||roots in||context word
",,,,,,
hyperparameters,We used pre-trained Glo Ve vectors to initialize the word embeddings with vector dimension 300 .,"used
pre-trained Glo Ve vectors
to initialize
word embeddings
with
vector dimension
300","pre-trained Glo Ve vectors||to initialize||word embeddings
word embeddings||with||vector dimension
","vector dimension||has||300
","Hyperparameters||used||pre-trained Glo Ve vectors
",,,,,,
hyperparameters,"For out - of - vocabulary words , we randomly sampled their embeddings from the uniform distribution , as implemented in .","For
out - of - vocabulary words
randomly sampled
embeddings
from
uniform distribution","out - of - vocabulary words||randomly sampled||embeddings
embeddings||from||uniform distribution
",,"Hyperparameters||For||out - of - vocabulary words
",,,,,,
hyperparameters,"To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .","To alleviate
overfitting
employed
dropout strategy ( Hinton et al. , 2012 )
on
input word embeddings
of
LSTM","overfitting||employed||dropout strategy ( Hinton et al. , 2012 )
dropout strategy ( Hinton et al. , 2012 )||on||input word embeddings
input word embeddings||of||LSTM
",,"Hyperparameters||To alleviate||overfitting
",,,,,,
hyperparameters,"Adam ( Kingma and Ba , 2015 ) was adopted as the optimizer with the learning rate 0.001 .","Adam ( Kingma and Ba , 2015 )
adopted
optimizer
with
learning rate
0.001","Adam ( Kingma and Ba , 2015 )||adopted||optimizer
optimizer||with||learning rate
","learning rate||has||0.001
",,"Hyperparameters||has||Adam ( Kingma and Ba , 2015 )
",,,,,
hyperparameters,"When implementing our approach , we empirically set the maximum iteration number K as 5 , ? in Equation 3 as 0.1 on LAPTOP data set , 0.5 on REST data set and 0.1 on TWITTER data set , respectively .","When implementing
our approach
empirically set
maximum iteration number K
as
5
0.1
on
LAPTOP data set
0.5
on
REST data set
0.1
on
TWITTER data set","our approach||empirically set||maximum iteration number K
maximum iteration number K||as||5
maximum iteration number K||as||0.1
0.1||on||LAPTOP data set
maximum iteration number K||as||0.5
0.5||on||REST data set
maximum iteration number K||as||0.1
0.1||on||TWITTER data set
",,"Hyperparameters||When implementing||our approach
",,,,,,
hyperparameters,All hyper - parameters were tuned on 20 % randomly held - out training data .,"All hyper - parameters
tuned on
20 % randomly held - out training data","All hyper - parameters||tuned on||20 % randomly held - out training data
",,,"Hyperparameters||has||All hyper - parameters
",,,,,
results,"First , both of our reimplemented MN and TNet are comparable to their original models reported in .","both of our reimplemented MN and TNet
comparable to
original models","both of our reimplemented MN and TNet||comparable to||original models
",,,"Results||has||both of our reimplemented MN and TNet
",,,,,
results,"When we replace the CNN of TNet with an attention mechanism , TNet - ATT is slightly inferior to TNet .","replace
CNN
of
TNet
with
attention mechanism
TNet - ATT
slightly inferior to
TNet","TNet - ATT||slightly inferior to||TNet
TNet - ATT||replace||CNN
CNN||of||TNet
TNet||with||attention mechanism
",,,"Results||has||TNet - ATT
",,,,,
results,"Moreover , when we perform additional K+1 - iteration of training on these models , their performance has not changed significantly , suggesting simply increasing training time is unable to enhance the performance of the neural ASC models .","perform
additional K+1 - iteration
of
training
on
performance
not changed significantly
neural ASC models","additional K+1 - iteration||of||training
training||on||neural ASC models
neural ASC models||performance||not changed significantly
",,"Results||perform||additional K+1 - iteration
",,,,,,
results,"Finally , when we use both kinds of attention supervision information , no matter for which metric , MN ( + AS ) remarkably outperforms MN on all test sets .","use
both kinds of attention supervision information
MN ( + AS )
remarkably outperforms
MN
on
all test sets","MN ( + AS )||remarkably outperforms||MN
MN||on||all test sets
","both kinds of attention supervision information||has||MN ( + AS )
","Results||use||both kinds of attention supervision information
",,,,,,
research-problem,BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis,Aspect - based Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - based Sentiment Analysis
",,,,
research-problem,"To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .","aspect extraction
aspect sentiment classification
aspect - based sentiment analysis",,,,,"Contribution||has research problem||aspect extraction
Contribution||has research problem||aspect sentiment classification
Contribution||has research problem||aspect - based sentiment analysis
",,,,
dataset,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .","first builds
RRC dataset
called
ReviewRC
using
reviews
from
SemEval 2016 Task 5
is a
popular dataset
for
aspect - based sentiment analysis ( ABSA )
in
domains of laptop and restaurant","RRC dataset||called||ReviewRC
ReviewRC||using||reviews
reviews||from||SemEval 2016 Task 5
SemEval 2016 Task 5||is a||popular dataset
popular dataset||for||aspect - based sentiment analysis ( ABSA )
aspect - based sentiment analysis ( ABSA )||in||domains of laptop and restaurant
",,"Dataset||first builds||RRC dataset
",,,,,,
model,This work adopts BERT ) as the base model as it achieves the state - of the - art performance on MRC .,"adopts
BERT
as
base model","BERT||as||base model
",,"Model||adopts||BERT
",,,,,,
model,"To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .","propose
novel joint post - training technique
takes
BERT 's pre-trained weights
as
initialization
for
basic language understanding
adapt
BERT
with
both domain knowledge and task ( MRC ) knowledge
before
fine - tuning
using
domain end task annotated data
for
domain RRC","BERT||with||both domain knowledge and task ( MRC ) knowledge
both domain knowledge and task ( MRC ) knowledge||before||fine - tuning
fine - tuning||using||domain end task annotated data
domain end task annotated data||for||domain RRC
novel joint post - training technique||takes||BERT 's pre-trained weights
BERT 's pre-trained weights||as||initialization
initialization||for||basic language understanding
",,"Model||adapt||BERT
Model||propose||novel joint post - training technique
",,,,,,
model,"This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness .","leverages
knowledge
from
two sources
unsupervised domain reviews
supervised ( yet out - of - domain ) MRC data","knowledge||from||two sources
","two sources||name||unsupervised domain reviews
two sources||name||supervised ( yet out - of - domain ) MRC data
","Model||leverages||knowledge
",,,,,,
hyperparameters,We adopt BERT BASE ( uncased ) as the basis for all experiments,"adopt
BERT BASE ( uncased )
as
basis
for
all experiments","BERT BASE ( uncased )||as||basis
basis||for||all experiments
",,"Hyperparameters||adopt||BERT BASE ( uncased )
",,,,,,
hyperparameters,"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .","leverage
FP16 computation
to reduce
size
of both
model
hidden representations
of
data","FP16 computation||to reduce||size
size||of both||model
size||of both||hidden representations
hidden representations||of||data
",,"Hyperparameters||leverage||FP16 computation
",,,,,,
hyperparameters,"We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation .","set
static loss scale
of
2
in","static loss scale||of||2
",,"Hyperparameters||set||static loss scale
",,,,"static loss scale||in||FP
",,
hyperparameters,The maximum length of post -training is set to 320 with a batch size of 16 for each type of knowledge .,"maximum length
of
post -training
set to
320
with
batch size
of
16
for each type of
knowledge","maximum length||of||post -training
post -training||set to||320
320||with||batch size
batch size||of||16
16||for each type of||knowledge
",,,"Hyperparameters||has||maximum length
",,,,,
hyperparameters,"The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G .","number of subbatch u
set to
2
good enough to store
sub - batch iteration
into
GPU memory
of
11G","number of subbatch u||set to||2
2||good enough to store||sub - batch iteration
sub - batch iteration||into||GPU memory
GPU memory||of||11G
",,,"Hyperparameters||has||number of subbatch u
",,,,,
hyperparameters,We use Adam optimizer and set the learning rate to be 3e - 5 .,"use
Adam optimizer
set
learning rate
to be
3e - 5","Adam optimizer||set||learning rate
learning rate||to be||3e - 5
",,"Hyperparameters||use||Adam optimizer
",,,,,,
hyperparameters,"We train 70,000 steps for the laptop domain and 140,000 steps for the restaurant domain , which roughly have one pass over the preprocessed data on the respective domain .","train
70,000 steps
for
laptop domain
140,000 steps
for
restaurant domain","140,000 steps||for||restaurant domain
70,000 steps||for||laptop domain
",,"Hyperparameters||train||140,000 steps
Hyperparameters||train||70,000 steps
",,,,,,
results,"To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge .","observed
proposed joint post - training ( BERT - PT )
best performance
over
all tasks
in
all domains
show
benefits
of having
two types of knowledge","best performance||over||all tasks
all tasks||in||all domains
best performance||show||benefits
benefits||of having||two types of knowledge
","proposed joint post - training ( BERT - PT )||has||best performance
","Results||observed||proposed joint post - training ( BERT - PT )
",,,,,,
results,"Rest. Methods EM F1 EM F1 DrQA 38.26 50.99 49.52 63.73 DrQA+MRC 40 To answer RQ2 , to our surprise we found that the vanilla pre-trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks .","found that
vanilla pre-trained weights
of
BERT
do not work
well
for
review - based tasks","vanilla pre-trained weights||of||BERT
BERT||do not work||well
well||for||review - based tasks
",,"Results||found that||vanilla pre-trained weights
",,,,,,
results,"To answer RQ3 , we noticed that the roles of domain knowledge and task knowledge vary for different tasks and domains .","noticed that
roles
of
domain knowledge and task knowledge
vary for
different tasks and domains","roles||of||domain knowledge and task knowledge
domain knowledge and task knowledge||vary for||different tasks and domains
",,"Results||noticed that||roles
",,,,,,
results,"For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post -training ( as indicated by BERT - MRC ) .","For
RRC
found that
performance gain
of
BERT - PT
mostly comes from
task - awareness ( MRC ) post -training","RRC||found that||performance gain
performance gain||of||BERT - PT
BERT - PT||mostly comes from||task - awareness ( MRC ) post -training
",,"Results||For||RRC
",,,,,,
results,We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .,"further investigated
examples
improved
BERT - MRC
found that
boundaries of spans ( especially short spans )
were
greatly improved","BERT - MRC||improved||examples
examples||found that||boundaries of spans ( especially short spans )
boundaries of spans ( especially short spans )||were||greatly improved
",,"Results||further investigated||BERT - MRC
",,,,,,
results,"For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .","AE
found that
great performance boost
comes mostly from
domain knowledge posttraining","AE||found that||great performance boost
great performance boost||comes mostly from||domain knowledge posttraining
",,,"Results||For||AE
",,,,,
results,"BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .","BERT - MRC
has almost no
improvement
on
restaurant","BERT - MRC||has almost no||improvement
improvement||on||restaurant
",,,"Results||has||BERT - MRC
",,,,,
results,"For ASC , we observed that large - scale annotated MRC data is very useful .","ASC
observed that
large - scale annotated MRC data
is
very useful","ASC||observed that||large - scale annotated MRC data
large - scale annotated MRC data||is||very useful
",,,"Results||For||ASC
",,,,,
results,The errors on RRC mainly come from boundaries of spans that are not concise enough and incorrect location of spans that may have certain nearby words related to the question .,"errors
on
RRC
come from
boundaries
of
spans
that are not
concise enough
incorrect location
of
spans
that may have
certain nearby words
related to
question","errors||on||RRC
RRC||come from||incorrect location
incorrect location||of||spans
spans||that may have||certain nearby words
certain nearby words||related to||question
RRC||come from||boundaries
boundaries||of||spans
spans||that are not||concise enough
",,,"Results||has||errors
",,,,,
results,"For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) .","errors
mostly come from
annotation inconsistency
boundaries
of
aspects","errors||mostly come from||annotation inconsistency
errors||mostly come from||boundaries
boundaries||of||aspects
",,,,,,,"AE||has||errors
",
results,"ASC tends to have more errors as the decision boundary between the negative and neutral examples is unclear ( e.g. , even annotators may not sure whether the reviewer shows no opinion or slight negative opinion when mentioning an aspect ) .","ASC
tends to have
errors
as
decision boundary
between
negative and neutral examples
is
unclear","ASC||tends to have||errors
errors||as||decision boundary
decision boundary||between||negative and neutral examples
negative and neutral examples||is||unclear
",,,"Results||has||ASC
",,,,,
research-problem,Emo2 Vec : Learning Generalized Emotion Representation by Multi- task Training,Learning Generalized Emotion Representation,,,,,"Contribution||has research problem||Learning Generalized Emotion Representation
",,,,
model,This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment - related tasks compared to other word embeddings .,"demonstrates
effectiveness
of incorporating
sentiment labels
in
wordlevel information
for
sentiment - related tasks
compared to
other word embeddings","effectiveness||of incorporating||sentiment labels
sentiment labels||in||wordlevel information
wordlevel information||for||sentiment - related tasks
sentiment - related tasks||compared to||other word embeddings
",,"Model||demonstrates||effectiveness
",,,,,,
model,"1 ) We propose Emo2Vec 1 which are word - level representations that encode emotional semantics into fixed - sized , real - valued vectors .","propose
Emo2Vec
are
word - level representations
that encode
emotional semantics
into
fixed - sized , real - valued vectors","Emo2Vec||are||word - level representations
word - level representations||that encode||emotional semantics
emotional semantics||into||fixed - sized , real - valued vectors
",,"Model||propose||Emo2Vec
",,,,,,
model,2 ) We propose to learn Emo2Vec with a multi-task learning framework by including six different emotion - related tasks .,"propose to learn
Emo2Vec
with
multi-task learning framework
by including
six different emotion - related tasks","Emo2Vec||with||multi-task learning framework
multi-task learning framework||by including||six different emotion - related tasks
",,"Model||propose to learn||Emo2Vec
",,,,,,
hyperparameters,Pre-training Emo2Vec,Pre-training Emo2Vec,,,,"Hyperparameters||has||Pre-training Emo2Vec
",,,,,"Pre-training Emo2Vec||has||Emo2 Vec embedding matrix and the CNN model
"
hyperparameters,Emo2 Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone .,"Emo2 Vec embedding matrix and the CNN model
pre-trained using
hashtag corpus","Emo2 Vec embedding matrix and the CNN model||pre-trained using||hashtag corpus
",,,,,,,,
hyperparameters,Parameters of T and CNN are randomly initialized and Adam is used for optimization .,"Parameters of T and CNN
are
randomly initialized
Adam
used for
optimization","Adam||used for||optimization
Parameters of T and CNN||are||randomly initialized
",,,,,,,"Pre-training Emo2Vec||has||Adam
Pre-training Emo2Vec||has||Parameters of T and CNN
",
hyperparameters,"For the best model , we use the batch size of 16 , embedding size of 100 , 1024 filters and filter sizes are 1 , 3 ,5 and 7 respectively .","For
best model
use
batch size
of
16
embedding size
of
100
1024 filters
filter sizes
are
1 , 3 ,5 and 7","best model||use||batch size
batch size||of||16
best model||use||embedding size
embedding size||of||100
filter sizes||are||1 , 3 ,5 and 7
","best model||has||1024 filters
1024 filters||has||filter sizes
",,,,"Pre-training Emo2Vec||For||best model
",,,
hyperparameters,Multi - task training,Multi - task training,,,,"Hyperparameters||has||Multi - task training
",,,,,
hyperparameters,"We tune our parameters of learning rate , L2 regularization , whether to pre-train our model and batch size with the average accuracy of the development set of all datasets .","tune
our parameters
of
learning rate
L2 regularization","our parameters||of||learning rate
our parameters||of||L2 regularization
",,,,,"Multi - task training||tune||our parameters
",,,
hyperparameters,We early stop our model when the averaged dev accuracy stop increasing .,"early stop
our model
when
averaged dev accuracy
stop
increasing","our model||when||averaged dev accuracy
averaged dev accuracy||stop||increasing
",,,,,"Multi - task training||early stop||our model
",,,
hyperparameters,"Our best model uses learning rate of 0.001 , L2 regularization of 1.0 , batch size of 32 .","best model
uses
learning rate
of
0.001
L2 regularization
of
1.0
batch size
of
32","best model||uses||L2 regularization
L2 regularization||of||1.0
best model||uses||learning rate
learning rate||of||0.001
best model||uses||batch size
batch size||of||32
",,,,,,,"Multi - task training||has||best model
",
hyperparameters,We save the best model and take the embedding layer as Emo2Vec vectors .,"save
best model
take
embedding layer
as
Emo2Vec vectors","best model||take||embedding layer
embedding layer||as||Emo2Vec vectors
",,,,,"Multi - task training||save||best model
",,,
results,"Compared with CNN embedding : Emo2 Vec works better than CNN embedding on 14 / 18 datasets , giving 2.6 % absolute accuracy improvement for the sentiment task and 1.6 % absolute f1score improvement on the other tasks .","CNN embedding
Emo2 Vec
works better than
on
14 / 18 datasets
giving
2.6 % absolute accuracy improvement
for
sentiment task
1.6 % absolute f1score improvement
on
other tasks","Emo2 Vec||works better than||CNN embedding
CNN embedding||giving||2.6 % absolute accuracy improvement
2.6 % absolute accuracy improvement||for||sentiment task
CNN embedding||giving||1.6 % absolute f1score improvement
1.6 % absolute f1score improvement||on||other tasks
CNN embedding||on||14 / 18 datasets
",,,"Results||has||Emo2 Vec
",,,,,
results,It shows multi-task training helps to create better generalized word emotion representations than just using a single task .,"shows
multi-task training
helps to create
better generalized word emotion representations
than just using
single task","multi-task training||helps to create||better generalized word emotion representations
better generalized word emotion representations||than just using||single task
",,"Results||shows||multi-task training
",,,,,,
results,"Compared with SSWE : Emo2 Vec works much better on all datasets except SS - T datasets , which gives 3.3 % accuracy improvement and 4.7 % f 1 score improvement respectively on sentiment and other tasks .","works
much better
on
all datasets
except
SS - T datasets
gives
3.3 % accuracy improvement
4.7 % f 1 score improvement
on
sentiment and other tasks","much better||on||all datasets
all datasets||except||SS - T datasets
all datasets||on||sentiment and other tasks
sentiment and other tasks||gives||3.3 % accuracy improvement
sentiment and other tasks||gives||4.7 % f 1 score improvement
",,,,,"Emo2 Vec||works||much better
",,,
results,"On average , it gives 1.3 % improvement in accuracy for the sentiment task and 1.1 % improvement of f 1 - score on the other tasks .","gives
1.3 % improvement
in
accuracy
for
sentiment task
1.1 % improvement
of
f 1 - score
on
other tasks","1.3 % improvement||in||accuracy
accuracy||for||sentiment task
1.1 % improvement||of||f 1 - score
f 1 - score||on||other tasks
",,"Results||gives||1.3 % improvement
Results||gives||1.1 % improvement
",,,,,,
results,"Since Emo2 Vec is not trained by predicting contextual words , it is weak on capturing synthetic and semantic meaning .","is
not trained by
predicting contextual words
weak
on capturing
synthetic and semantic meaning","predicting contextual words||is||weak
weak||on capturing||synthetic and semantic meaning
",,,,,"Emo2 Vec||not trained by||predicting contextual words
",,,
results,"Here , we want to highlight that solely using a simple classifier with good word representation can achieve promising results .","solely using
simple classifier
with
good word representation
can achieve
promising results","simple classifier||with||good word representation
good word representation||can achieve||promising results
",,"Results||solely using||simple classifier
",,,,,,
results,"Compared with GloVe+ DeepMoji , GloVe + Emo2 Vec achieves same or better results on 11 / 14 datasets , which on average gives 1.0 % improvement .","Compared with
GloVe+ DeepMoji , GloVe + Emo2 Vec
achieves
same or better results
on
11 / 14 datasets
on average gives
1.0 % improvement","GloVe+ DeepMoji , GloVe + Emo2 Vec||achieves||same or better results
same or better results||on average gives||1.0 % improvement
same or better results||on||11 / 14 datasets
",,"Results||Compared with||GloVe+ DeepMoji , GloVe + Emo2 Vec
",,,,,,
results,"GloVe + Emo2 Vec achieves better performances on SOTA results on three datasets ( SE0714 , stress and tube tablet ) and comparable result to SOTA on dataset Previous SOTA results","GloVe + Emo2 Vec
achieves
better performances
on
SOTA results
on
three datasets ( SE0714 , stress and tube tablet )
comparable result
to
SOTA
on
dataset","GloVe + Emo2 Vec||achieves||comparable result
comparable result||to||SOTA
SOTA||on||dataset
GloVe + Emo2 Vec||achieves||better performances
better performances||on||SOTA results
SOTA results||on||three datasets ( SE0714 , stress and tube tablet )
",,,"Results||has||GloVe + Emo2 Vec
",,,,,
results,"Thus , to detect the corresponding emotion , more attention needs to be paid to words .","to detect
corresponding emotion
more attention needs to be paid to
words","corresponding emotion||more attention needs to be paid to||words
",,"Results||to detect||corresponding emotion
",,,,,,
research-problem,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,Sentiment Treebank,,,,,"Contribution||has research problem||Sentiment Treebank
",,,,
research-problem,Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .,richer supervised training and evaluation resources,,,,,"Contribution||has research problem||richer supervised training and evaluation resources
",,,,
dataset,The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language .,"Stanford Sentiment Treebank
is
first corpus
with
fully labeled parse trees
allows for
complete analysis
of
compositional effects
of
sentiment in language","Stanford Sentiment Treebank||is||first corpus
first corpus||with||fully labeled parse trees
fully labeled parse trees||allows for||complete analysis
complete analysis||of||compositional effects
compositional effects||of||sentiment in language
",,,"Dataset||name||Stanford Sentiment Treebank
",,,,,
dataset,"The corpus is based on the dataset introduced by and consists of 11,855 single sentences extracted from movie reviews .","corpus
based on
dataset
consists of
11,855 single sentences
extracted from
movie reviews","corpus||based on||dataset
dataset||consists of||11,855 single sentences
11,855 single sentences||extracted from||movie reviews
",,,"Dataset||has||corpus
",,,,,
dataset,"It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees , each annotated by 3 human judges .","parsed
with
Stanford parser
total of
215,154 unique phrases
from
parse trees
annotated by
3 human judges","parsed||with||Stanford parser
parsed||total of||215,154 unique phrases
215,154 unique phrases||from||parse trees
parse trees||annotated by||3 human judges
",,,"Dataset||has||parsed
",,,,,
dataset,This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena .,"new dataset
to analyze
intricacies
of
sentiment
to capture
complex linguistic phenomena","new dataset||to analyze||intricacies
intricacies||of||sentiment
new dataset||to capture||complex linguistic phenomena
",,,"Dataset||has||new dataset
",,,,,
dataset,The granularity and size of this dataset will enable the community to train compositional models that are based on supervised and structured machine learning techniques .,"granularity and size
enable
community
to train
compositional models
based on
supervised and structured machine learning techniques","granularity and size||enable||community
community||to train||compositional models
compositional models||based on||supervised and structured machine learning techniques
",,,"Dataset||has||granularity and size
",,,,,
model,"In order to capture the compositional effects with higher accuracy , we propose a new model called the Recursive Neural Tensor Network ( RNTN ) .","called
Recursive Neural Tensor Network ( RNTN )",,,"Model||called||Recursive Neural Tensor Network ( RNTN )
",,,,,,
model,Recursive Neural Tensor Networks take as input phrases of any length .,"Recursive Neural Tensor Networks
take as input
phrases
of
any length","Recursive Neural Tensor Networks||take as input||phrases
phrases||of||any length
",,,"Model||has||Recursive Neural Tensor Networks
",,,,,
model,They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .,"represent
phrase
through
word vectors and a parse tree
compute
vectors
for
higher nodes
in
tree
using
same tensor - based composition function","vectors||for||higher nodes
higher nodes||in||tree
tree||using||same tensor - based composition function
phrase||through||word vectors and a parse tree
",,,,,"Recursive Neural Tensor Networks||compute||vectors
Recursive Neural Tensor Networks||represent||phrase
",,,
hyperparameters,Optimal performance for all models was achieved at word vector sizes between 25 and 35 dimensions and batch sizes between 20 and 30 .,"Optimal performance
for
all models
achieved at
word vector sizes
between
25 and 35 dimensions
batch sizes
between
20 and 30","Optimal performance||for||all models
all models||achieved at||word vector sizes
word vector sizes||between||25 and 35 dimensions
all models||achieved at||batch sizes
batch sizes||between||20 and 30
",,,"Hyperparameters||has||Optimal performance
",,,,,
hyperparameters,The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours .,"RNTN
usually achieve
best performance
on
dev set
after training for
3 - 5 hours","RNTN||usually achieve||best performance
best performance||on||dev set
dev set||after training for||3 - 5 hours
",,,"Hyperparameters||has||RNTN
",,,,,
hyperparameters,We use f = tanh in all experiments .,"use
f = tanh
in
all experiments","f = tanh||in||all experiments
",,"Hyperparameters||use||f = tanh
",,,,,,
baselines,"We compare to commonly used methods that use bag of words features with Naive Bayes and SVMs , as well as Naive Bayes with bag of bigram features .","compare to
commonly used methods
that use
bag of words features
with
Naive Bayes and SVMs
Naive Bayes
with
bag of bigram features","commonly used methods||that use||bag of words features
bag of words features||with||Naive Bayes and SVMs
bag of words features||with||Naive Bayes
Naive Bayes||with||bag of bigram features
",,"Baselines||compare to||commonly used methods
",,,,,,
baselines,We also compare to a model that averages neural word vectors and ignores word order ( VecAvg ) .,"model
averages
neural word vectors
ignores
word order ( VecAvg )","model||ignores||word order ( VecAvg )
model||averages||neural word vectors
",,,"Baselines||compare to||model
",,,,,
hyperparameters,"The sentences in the treebank were split into a train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) and these splits are made available with the data release .","sentences
in
treebank
were split into
train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 )","sentences||in||treebank
treebank||were split into||train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 )
",,,"Hyperparameters||has||sentences
",,,,,
results,showed that a fine grained classification into 5 classes is a reasonable approximation to capture most of the data variation .,"showed
fine grained classification
into
5 classes
is
reasonable approximation
to capture
most of the data variation","fine grained classification||into||5 classes
5 classes||is||reasonable approximation
reasonable approximation||to capture||most of the data variation
",,"Results||showed||fine grained classification
",,,,,,
results,"The RNTN gets the highest performance , followed by the MV - RNN and RNN .","RNTN
gets
highest performance
followed by
MV - RNN and RNN","RNTN||followed by||MV - RNN and RNN
RNTN||gets||highest performance
",,,"Results||has||RNTN
",,,,,
results,"The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .","recursive models
work
very well
on
shorter phrases
where
negation and composition
are
important
bag of features baselines
perform
well
with
longer sentences","recursive models||work||very well
very well||on||shorter phrases
shorter phrases||where||negation and composition
negation and composition||are||important
bag of features baselines||perform||well
well||with||longer sentences
",,,"Results||has||recursive models
Results||has||bag of features baselines
",,,,,
results,The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4 % .,"combination of
new sentiment treebank and the RNTN
pushes
state of the art
on
short phrases
up to
85.4 %","new sentiment treebank and the RNTN||pushes||state of the art
state of the art||on||short phrases
short phrases||up to||85.4 %
",,"Results||combination of||new sentiment treebank and the RNTN
",,,,,,
ablation-analysis,"The RNTN has the highest reversal accuracy , showing its ability to structurally learn negation of positive sentences .","RNTN
highest reversal accuracy
showing its ability to
structurally learn negation of positive sentences","highest reversal accuracy||showing its ability to||structurally learn negation of positive sentences
","RNTN||has||highest reversal accuracy
",,"Ablation analysis||has||RNTN
",,,,,
ablation-analysis,shows a typical case in which sentiment was made more positive by switching the main class from negative to neutral even though both not and dull were negative .,"shows
typical case
in which
sentiment
made
more positive
by switching
main class
from
negative to neutral","typical case||in which||sentiment
sentiment||made||more positive
more positive||by switching||main class
main class||from||negative to neutral
",,"Ablation analysis||shows||typical case
",,,,,,
ablation-analysis,Therefore we can conclude that the RNTN is best able to identify the effect of negations upon both positive and negative sentiment sentences . :,"conclude
RNTN
best able to identify
effect of negations
upon
positive and negative sentiment sentences","RNTN||best able to identify||effect of negations
effect of negations||upon||positive and negative sentiment sentences
",,"Ablation analysis||conclude||RNTN
",,,,,,
research-problem,Target - Sensitive Memory Networks for Aspect Sentiment Classification,Aspect Sentiment Classification,,,,,"Contribution||has research problem||Aspect Sentiment Classification
",,,,
research-problem,Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .,"Aspect sentiment classification ( ASC )
sentiment analysis",,,,,"Contribution||has research problem||Aspect sentiment classification ( ASC )
Contribution||has research problem||sentiment analysis
",,,,
research-problem,"However , we found an important problem with the current MNs in performing the ASC task .",ASC,,,,,"Contribution||has research problem||ASC
",,,,
model,"To address this problem , we propose target - sensitive memory networks ( TMNs ) , which can capture the sentiment interaction between targets and contexts .","propose
target - sensitive memory networks ( TMNs )
can capture
sentiment interaction
between
targets and contexts","target - sensitive memory networks ( TMNs )||can capture||sentiment interaction
sentiment interaction||between||targets and contexts
",,"Model||propose||target - sensitive memory networks ( TMNs )
",,,,,,
baselines,AMN : A state - of - the - art memory network used for ASC .,"AMN
state - of - the - art memory network
used for
ASC","state - of - the - art memory network||used for||ASC
","AMN||has||state - of - the - art memory network
",,"Baselines||has||AMN
",,,,,
baselines,"BL - MN : Our basic memory network presented in Section 2 , which does not use the proposed techniques for capturing target - sensitive sentiments .","BL - MN
Our basic memory network
does not use
proposed techniques
for capturing
target - sensitive sentiments","Our basic memory network||does not use||proposed techniques
proposed techniques||for capturing||target - sensitive sentiments
","BL - MN||has||Our basic memory network
",,"Baselines||has||BL - MN
",,,,,
baselines,AE - LSTM : RNN / LSTM is another popular attention based neural model .,AE - LSTM,,,,"Baselines||has||AE - LSTM
",,,,,
baselines,"Here we compare with a state - of - the - art attention - based LSTM for ASC , AE - LSTM .","compare with
state - of - the - art attention - based LSTM
for
ASC","state - of - the - art attention - based LSTM||for||ASC
",,,,,"AE - LSTM||compare with||state - of - the - art attention - based LSTM
",,,
baselines,ATAE - LSTM :,ATAE - LSTM,,,,"Baselines||has||ATAE - LSTM
",,,,,"ATAE - LSTM||has||attention - based LSTM
"
baselines,Another attention - based LSTM for ASC reported in .,"attention - based LSTM
for
ASC","attention - based LSTM||for||ASC
",,,,,,,,
baselines,Target - sensitive Memory Networks ( TMNs ) :,Target - sensitive Memory Networks ( TMNs ),,,,"Baselines||has||Target - sensitive Memory Networks ( TMNs )
",,,,,"Target - sensitive Memory Networks ( TMNs )||has||six proposed techniques
"
baselines,"The six proposed techniques , NP , CNP , IT , CI , JCI , and JPI give six target - sensitive memory networks .","six proposed techniques
NP
CNP
IT
CI
JCI
JPI
give
six target - sensitive memory networks","six proposed techniques||give||six target - sensitive memory networks
","six proposed techniques||name||NP
six proposed techniques||name||CNP
six proposed techniques||name||IT
six proposed techniques||name||CI
six proposed techniques||name||JCI
six proposed techniques||name||JPI
",,,,,,,
hyperparameters,We use the open - domain word embeddings 1 for the initialization of word vectors .,"use
open - domain word embeddings
for
initialization
of
word vectors","open - domain word embeddings||for||initialization
initialization||of||word vectors
",,"Hyperparameters||use||open - domain word embeddings
",,,,,,
hyperparameters,"We initialize other model parameters from a uniform distribution U ( - 0.05 , 0.05 ) .","initialize
other model parameters
from
uniform distribution U ( - 0.05 , 0.05 )","other model parameters||from||uniform distribution U ( - 0.05 , 0.05 )
",,"Hyperparameters||initialize||other model parameters
",,,,,,
hyperparameters,The dimension of the word embedding and the size of the hidden layers are 300 .,"dimension
of
word embedding
size of the hidden layers
are
300","dimension||are||300
300||of||word embedding
300||of||size of the hidden layers
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,The learning rate is set to 0.01 and the dropout rate is set to 0.1 .,"learning rate
set to
0.01
dropout rate
set to
0.1","learning rate||set to||0.01
dropout rate||set to||0.1
",,,"Hyperparameters||has||learning rate
Hyperparameters||has||dropout rate
",,,,,
hyperparameters,Stochastic gradient descent is used as our optimizer .,"Stochastic gradient descent
used as
our optimizer","Stochastic gradient descent||used as||our optimizer
",,,"Hyperparameters||has||Stochastic gradient descent
",,,,,
hyperparameters,"We also compare the memory networks in their multiple computational layers version ( i.e. , multiple hops ) and the number of hops is set to 3 as used in the mentioned previous studies .","compare
memory networks
in
multiple computational layers version ( i.e. , multiple hops )
number of hops
set to
3","memory networks||in||multiple computational layers version ( i.e. , multiple hops )
number of hops||set to||3
","memory networks||has||number of hops
","Hyperparameters||compare||memory networks
",,,,,,
hyperparameters,"We implemented all models in the TensorFlow environment using same input , embedding size , dropout rate , optimizer , etc.","implemented
all models
in
TensorFlow environment
using
same input , embedding size , dropout rate , optimizer , etc.","all models||in||TensorFlow environment
TensorFlow environment||using||same input , embedding size , dropout rate , optimizer , etc.
",,"Hyperparameters||implemented||all models
",,,,,,
results,"Comparing the 1 - hop memory networks ( first nine rows ) , we see significant performance gains achieved by CNP , CI , JCI , and JPI on both datasets , where each of them has p < 0.01 over the strongest baseline ( BL - MN ) from paired t- test using F1 - Macro .","Comparing
1 - hop memory networks
see
significant performance gains
achieved by
CNP , CI , JCI , and JPI
on
both datasets
each of them has
p < 0.01
over
strongest baseline ( BL - MN )
from
paired t- test
using
F1 - Macro","1 - hop memory networks||see||significant performance gains
significant performance gains||achieved by||CNP , CI , JCI , and JPI
significant performance gains||on||both datasets
both datasets||each of them has||p < 0.01
p < 0.01||over||strongest baseline ( BL - MN )
strongest baseline ( BL - MN )||from||paired t- test
p < 0.01||using||F1 - Macro
",,"Results||Comparing||1 - hop memory networks
",,,,,,
results,"2 . In the 3 - hop setting , TMNs achieve much better results on Restaurant .","In
3 - hop setting
TMNs
achieve
much better results
on
Restaurant","TMNs||achieve||much better results
much better results||on||Restaurant
","3 - hop setting||has||TMNs
","Results||In||3 - hop setting
",,,,,,
results,"JCI , IT , and CI achieve the best scores , outperforming the strongest baseline AMN by 2.38 % , 2.18 % , and 2.03 % .","JCI , IT , and CI
achieve
best scores
outperforming
strongest baseline AMN
by
2.38 % , 2.18 % , and 2.03 %","JCI , IT , and CI||achieve||best scores
JCI , IT , and CI||outperforming||strongest baseline AMN
strongest baseline AMN||by||2.38 % , 2.18 % , and 2.03 %
",,,,,,,"3 - hop setting||has||JCI , IT , and CI
",
results,"On Laptop , BL - MN and most TMNs ( except CNP and JPI ) perform similarly .","On
Laptop
BL - MN and most TMNs
perform
similarly","BL - MN and most TMNs||perform||similarly
","Laptop||has||BL - MN and most TMNs
","Results||On||Laptop
",,,,,,
results,"3 . Comparing all TMNs , we see that JCI works the best as it always obtains the top - three scores on two datasets and in two settings .","all TMNs
see that
JCI
works
best","all TMNs||see that||JCI
JCI||works||best
",,,"Results||Comparing||all TMNs
",,,,,
results,CI and JPI also perform well in most cases .,"CI and JPI
perform
well
in
most cases","CI and JPI||perform||well
well||in||most cases
",,,"Results||has||CI and JPI
",,,,,
results,"IT , NP , and CNP can achieve very good scores in some cases but are less stable .","IT , NP , and CNP
achieve
very good scores
in
some cases
are
less stable","IT , NP , and CNP||achieve||very good scores
very good scores||in||some cases
very good scores||are||less stable
",,,"Results||has||IT , NP , and CNP
",,,,,
research-problem,Improved Semantic Representations From Tree - Structured Long Short - Term Memory Networks,Improved Semantic Representations,,,,,"Contribution||has research problem||Improved Semantic Representations
",,,,
research-problem,"Tree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) .","predicting the semantic relatedness of two sentences
sentiment classification",,,,,"Contribution||has research problem||predicting the semantic relatedness of two sentences
Contribution||has research problem||sentiment classification
",,,,
model,"In this paper , we introduce a generalization of the standard LSTM architecture to tree - structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM .","introduce
generalization of the standard LSTM architecture
to
tree - structured network topologies
show
superiority
for representing
sentence meaning
over
sequential LSTM","generalization of the standard LSTM architecture||to||tree - structured network topologies
tree - structured network topologies||show||superiority
superiority||for representing||sentence meaning
sentence meaning||over||sequential LSTM
",,"Model||introduce||generalization of the standard LSTM architecture
",,,,,,
model,"While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step , the tree - structured LSTM , or Tree - LSTM , composes its state from an input vector and the hidden states of arbitrarily many child units .","composes
state
from
of
tree - structured LSTM , or Tree - LSTM
input vector
hidden states
arbitrarily many child units","tree - structured LSTM , or Tree - LSTM||composes||state
state||from||input vector
state||from||hidden states
hidden states||of||arbitrarily many child units
",,,"Model||has||tree - structured LSTM , or Tree - LSTM
",,,,,
code,Implementations of our models and experiments are available at https :// github.com/stanfordnlp/treelstm.,,,,,,,,,,
hyperparameters,The hyperparameters for our models were tuned on the development set for each task .,"tuned on
development set",,,"Hyperparameters||tuned on||development set
",,,,,,
hyperparameters,We initialized our word representations using publicly available 300 - dimensional Glove vectors,"initialized
our word representations
using
publicly available 300 - dimensional Glove vectors","our word representations||using||publicly available 300 - dimensional Glove vectors
",,"Hyperparameters||initialized||our word representations
",,,,,,
hyperparameters,"For the sentiment classification task , word representations were updated during training with a learning rate of 0.1 .","For
sentiment classification task
word representations
updated
during training
with
learning rate
of
0.1","word representations||updated||during training
during training||with||learning rate
learning rate||of||0.1
","sentiment classification task||has||word representations
","Hyperparameters||For||sentiment classification task
",,,,,,
hyperparameters,"For the semantic relatedness task , word representations were held fixed as we did not observe any significant improvement when the representations were tuned .","semantic relatedness task
word representations
held
fixed","word representations||held||fixed
","semantic relatedness task||has||word representations
",,"Hyperparameters||For||semantic relatedness task
",,,,,
hyperparameters,Our models were trained using AdaGrad with a learning rate of 0.05 and a minibatch size of 25 .,"Our models
trained using
AdaGrad
with
learning rate
of
0.05
minibatch size
of
25","Our models||trained using||AdaGrad
AdaGrad||with||minibatch size
minibatch size||of||25
AdaGrad||with||learning rate
learning rate||of||0.05
",,,"Hyperparameters||has||Our models
",,,,,
hyperparameters,The model parameters were regularized with a per-minibatch L2 regularization strength of 10 ?4 .,"model parameters
regularized with
per-minibatch L2 regularization strength
of
10 ?4","model parameters||regularized with||per-minibatch L2 regularization strength
per-minibatch L2 regularization strength||of||10 ?4
",,,"Hyperparameters||has||model parameters
",,,,,
hyperparameters,The sentiment classifier was additionally regularized using dropout with a dropout rate of 0.5 .,"sentiment classifier
additionally regularized using
dropout
with
dropout rate
of
0.5","sentiment classifier||additionally regularized using||dropout
dropout||with||dropout rate
dropout rate||of||0.5
",,,"Hyperparameters||has||sentiment classifier
",,,,,
research-problem,Effective Attention Modeling for Aspect - Level Sentiment Classification,Aspect - Level Sentiment Classification,,,,,"Contribution||has research problem||Aspect - Level Sentiment Classification
",,,,
research-problem,Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .,fine - grained sentiment analysis,,,,,"Contribution||has research problem||fine - grained sentiment analysis
",,,,
approach,We propose two novel approaches for improving the effectiveness of attention models .,"propose
two novel approaches
for improving
effectiveness of attention models","two novel approaches||for improving||effectiveness of attention models
",,"Approach||propose||two novel approaches
",,,,,,
approach,The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .,"first approach
of",,,,"Approach||has||first approach
",,,,,
approach,"To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .","model
each target
as
mixture
K aspect embeddings","each target||as||mixture
","mixture||of||K aspect embeddings
",,,,"first approach||model||each target
",,,
approach,We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the aspect embeddings .,"use
autoencoder structure
to learn
both the aspect embeddings as well as the representation of the target
as
weighted combination
of
aspect embeddings","autoencoder structure||to learn||both the aspect embeddings as well as the representation of the target
both the aspect embeddings as well as the representation of the target||as||weighted combination
weighted combination||of||aspect embeddings
",,,,,"first approach||use||autoencoder structure
",,,
approach,The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sentiment .,"autoencoder structure
jointly trained with
neural attention - based sentiment classifier
to provide
good target representation
as well as
high accuracy
on
predicted sentiment","autoencoder structure||jointly trained with||neural attention - based sentiment classifier
neural attention - based sentiment classifier||to provide||good target representation
good target representation||as well as||high accuracy
high accuracy||on||predicted sentiment
",,,,,,,"first approach||has||autoencoder structure
",
approach,Our second approach exploits syntactic information to construct a syntax - based attention model .,"second approach
exploits
syntactic information
to construct
syntax - based attention model","second approach||exploits||syntactic information
syntactic information||to construct||syntax - based attention model
",,,"Approach||has||second approach
",,,,,
approach,"Instead , our syntax - based attention mechanism selectively focuses on a small subset of context words that are close to the target on the syntactic path which is obtained by applying a dependency parser on the review sentence .","syntax - based attention mechanism
selectively focuses on
small subset of context words
close to
target
on
syntactic path
obtained by
applying a dependency parser
on
review sentence","syntax - based attention mechanism||selectively focuses on||small subset of context words
small subset of context words||close to||target
target||on||syntactic path
syntactic path||obtained by||applying a dependency parser
applying a dependency parser||on||review sentence
",,,,,,,"second approach||has||syntax - based attention mechanism
",
baselines,( 1 ) Feature - based SVM :,Feature - based SVM,,,,"Baselines||has||Feature - based SVM
",,,,,
baselines,We compare with the reported results of a top system in SemEval 2014 .,"compare with
reported results
of
top system
in
SemEval 2014","reported results||of||top system
top system||in||SemEval 2014
",,,,,"Feature - based SVM||compare with||reported results
",,,
baselines,( 2 ) LSTM : An LSTM network is built on top of word embeddings .,"LSTM
built on
top of word embeddings","LSTM||built on||top of word embeddings
",,,"Baselines||has||LSTM
",,,,,
results,"1 ) Feature - based SVM is still a strong baseline , our best model achieves competitive results on D1 and D2 without relying on so many manually - designed features and external resources .","our best model
achieves
competitive results
on
D1 and D2
without relying on
so many manually - designed features and external resources","our best model||achieves||competitive results
competitive results||on||D1 and D2
D1 and D2||without relying on||so many manually - designed features and external resources
",,,"Results||has||our best model
",,,,,
results,"2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .","Compared with
all other neural baselines
our full model
achieves
statistically significant improvements ( p < 0.05 )
on
both accuracies and macro - F1 scores
for
D1 , D3 , D4","our full model||achieves||statistically significant improvements ( p < 0.05 )
statistically significant improvements ( p < 0.05 )||on||both accuracies and macro - F1 scores
both accuracies and macro - F1 scores||for||D1 , D3 , D4
","all other neural baselines||has||our full model
","Results||Compared with||all other neural baselines
",,,,,,
results,"3 ) Compared with LSTM + ATT , all three settings of our model are able to achieve statistically significant improvements ( p < 0.05 ) on all datasets .","LSTM + ATT
all three settings
of
our model
are able to achieve
statistically significant improvements ( p < 0.05 )
on
all datasets","all three settings||of||our model
our model||are able to achieve||statistically significant improvements ( p < 0.05 )
statistically significant improvements ( p < 0.05 )||on||all datasets
","LSTM + ATT||has||all three settings
",,"Results||Compared with||LSTM + ATT
",,,,,
results,4 ) The integrated full model over all achieves the best performance compared to using only one of the two proposed approaches .,"integrated full model over all
achieves
best performance
compared to using
only one of the two proposed approaches","integrated full model over all||achieves||best performance
best performance||compared to using||only one of the two proposed approaches
",,,"Results||has||integrated full model over all
",,,,,
results,"5 ) The proposed target representation is more helpful on restaurant domain ( D1 , D3 , and D4 ) than laptop domain ( D2 ) .","proposed target representation
is
more helpful
on
restaurant domain ( D1 , D3 , and D4 )
than
laptop domain ( D2 )","proposed target representation||is||more helpful
more helpful||than||laptop domain ( D2 )
more helpful||on||restaurant domain ( D1 , D3 , and D4 )
",,,"Results||has||proposed target representation
",,,,,
research-problem,Improved Sentence Modeling using Suffix Bidirectional LSTM,Improved Sentence Modeling,,,,,"Contribution||has research problem||Improved Sentence Modeling
",,,,
research-problem,"Recurrent neural networks have become ubiquitous in computing representations of sequential data , especially textual data in natural language processing .",computing representations of sequential data,,,,,"Contribution||has research problem||computing representations of sequential data
",,,,
research-problem,Using SuBiLSTM we achieve new state - of - the - art results for fine - grained sentiment classification and question classification .,"fine - grained sentiment classification
question classification",,,,,"Contribution||has research problem||fine - grained sentiment classification
Contribution||has research problem||question classification
",,,,
research-problem,Recurrent Neural Networks ( RNN ) ) have emerged as a powerful tool for modeling sequential data .,modeling sequential data,,,,,"Contribution||has research problem||modeling sequential data
",,,,
model,"In this paper , we propose a simple , general and effective technique to compute contextual representations that capture long range dependencies .","propose
simple , general and effective technique
to compute
contextual representations
that capture
long range dependencies","simple , general and effective technique||to compute||contextual representations
contextual representations||that capture||long range dependencies
",,"Model||propose||simple , general and effective technique
",,,,,,
model,"For each token t , we encode both its prefix and suffix in both the forward and reverse direction .","For
each token t
encode both
prefix and suffix
in both
forward and reverse direction","each token t||encode both||prefix and suffix
prefix and suffix||in both||forward and reverse direction
",,"Model||For||each token t
",,,,,,
model,"Further , we combine the prefix and suffix representations by a simple max - pooling operation to produce a richer contextual representation of t in both the forward and reverse direction .","combine
prefix and suffix representations
by
simple max - pooling operation
to produce
richer contextual representation
in both
forward and reverse direction","prefix and suffix representations||by||simple max - pooling operation
simple max - pooling operation||to produce||richer contextual representation
richer contextual representation||in both||forward and reverse direction
",,"Model||combine||prefix and suffix representations
",,,,,,
model,We call our model Suffix BiLSTM or SuBiLSTM in short .,,,,,,,,,,
baselines,"For each of the tasks , we compare SuBiLSTM and SuBiLSTM - Tied with a single - layer BiLSTM and a 2 - layer BiLSTM encoder with the same hidden dimension .","compare
SuBiLSTM and SuBiLSTM - Tied
with
single - layer BiLSTM and a 2 - layer BiLSTM encoder
with
same hidden dimension","SuBiLSTM and SuBiLSTM - Tied||with||single - layer BiLSTM and a 2 - layer BiLSTM encoder
single - layer BiLSTM and a 2 - layer BiLSTM encoder||with||same hidden dimension
",,"Baselines||compare||SuBiLSTM and SuBiLSTM - Tied
",,,,,,
results,"The relative performance of SuBiL - STM and SuBiLSTM - Tied are fairly close to each other , as shown by the relative gains in .","relative performance
of
SuBiL - STM and SuBiLSTM - Tied
are
fairly close","relative performance||of||SuBiL - STM and SuBiLSTM - Tied
SuBiL - STM and SuBiLSTM - Tied||are||fairly close
",,,"Results||has||relative performance
",,,,,
results,"SuBiLSTM - Tied works better on small datasets ( SST and TREC ) , probably owing to the regularizing effect of using the same LSTM to encode both suffixes and prefixes .","SuBiLSTM - Tied
works
better
on
small datasets ( SST and TREC )","SuBiLSTM - Tied||works||better
better||on||small datasets ( SST and TREC )
",,,"Results||has||SuBiLSTM - Tied
",,,,,
results,"For the larger datasets ( SNLI and QUORA ) , SuBILSTM slightly edges out the tied version owing to its larger capacity .","For
larger datasets ( SNLI and QUORA )
SuBILSTM
edges out
tied version
owing to
larger capacity","SuBILSTM||edges out||tied version
SuBILSTM||owing to||larger capacity
","larger datasets ( SNLI and QUORA )||has||SuBILSTM
","Results||For||larger datasets ( SNLI and QUORA )
",,,,,,
results,"The training complexity for both the models is similar and hence , with half the parameters , SuBILSTM - Tied should be the more favored model for sentence modeling tasks .","training complexity
for both
models
is
similar
with
half the parameters
SuBILSTM - Tied
should be
more favored model
for
sentence modeling tasks","training complexity||for both||models
models||is||similar
SuBILSTM - Tied||with||half the parameters
half the parameters||should be||more favored model
more favored model||for||sentence modeling tasks
","models||has||SuBILSTM - Tied
",,"Results||has||training complexity
",,,,,
research-problem,A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis,Aspect - level Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - level Sentiment Analysis
",,,,
research-problem,"Sentiment analysis , also known as opinion mining , is a vital task in Natural Language Processing ( NLP ) .",Sentiment analysis,,,,,"Contribution||has research problem||Sentiment analysis
",,,,
model,"Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .","propose
position - aware bidirectional attention network ( PBAN )
based on
bidirectional Gated Recurrent Units ( Bi - GRU )","position - aware bidirectional attention network ( PBAN )||based on||bidirectional Gated Recurrent Units ( Bi - GRU )
",,"Model||propose||position - aware bidirectional attention network ( PBAN )
",,,,,,
,"In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism .","PBAN
mutually models
relationship
between
sentence and different words in the aspect term
by adopting
bidirectional attention mechanism",,,,,,,,,
model,"1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding .","Obtaining position information
of
each word
in
corresponding sentence
based on
current aspect term
converting
position information
into
position embedding","Obtaining position information||of||each word
each word||in||corresponding sentence
corresponding sentence||based on||current aspect term
current aspect term||converting||position information
position information||into||position embedding
",,,"Model||has||Obtaining position information
",,,,,
model,2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .,"PBAN
composes of
two Bi - GRU networks
focusing on extracting
aspectlevel features
sentence - level features","PBAN||composes of||two Bi - GRU networks
two Bi - GRU networks||focusing on extracting||aspectlevel features
two Bi - GRU networks||focusing on extracting||sentence - level features
",,,"Model||has||PBAN
",,,,,
model,3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence .,"bidirectional attention mechanism
to model
mutual relation
between
aspect term and its corresponding sentence","bidirectional attention mechanism||to model||mutual relation
mutual relation||between||aspect term and its corresponding sentence
",,,"Model||has||bidirectional attention mechanism
",,,,,
hyperparameters,"In our experiments , all word embedding are initialized by the pre-trained Glove vector 2 .","word embedding
initialized by
pre-trained Glove vector","word embedding||initialized by||pre-trained Glove vector
",,,"Hyperparameters||has||word embedding
",,,,,
hyperparameters,"All the weight matrices are given the initial value by sampling from the uniform distribution U ( ?0.1 , 0.1 ) , and all the biases are set to zero .","weight matrices
given
initial value
by
sampling
from
uniform distribution U ( ?0.1 , 0.1 )
biases
are set to
zero","biases||are set to||zero
weight matrices||given||initial value
initial value||by||sampling
sampling||from||uniform distribution U ( ?0.1 , 0.1 )
",,,"Hyperparameters||has||biases
Hyperparameters||has||weight matrices
",,,,,
hyperparameters,"The dimension of the word embedding and aspect term embedding are set to 300 , and the number of the hidden units are set to 200 .","word embedding and aspect term embedding
set to
300
hidden units
set to
200","word embedding and aspect term embedding||set to||300
hidden units||set to||200
",,,"Hyperparameters||has||hidden units
",,,,"dimension||of||word embedding and aspect term embedding
",
hyperparameters,"The dimension of position embedding is set to 100 , which is randomly initialized and updated during the training process .","dimension
of
position embedding
set to
100
which is
randomly initialized and updated
during
training process","dimension||of||position embedding
position embedding||set to||100
100||which is||randomly initialized and updated
randomly initialized and updated||during||training process
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,"We use Tensorflow to implement our proposed model and employ the Momentum as the training method , whose momentum parameter ? is set to 0.9 , ? is set to 10 ? 6 , and the initial learning rate is set to 0.01 .","use
Tensorflow
to implement
our proposed model
employ
Momentum
as
training method
whose
momentum parameter ?
set to
0.9
set to
initial learning rate
0.01","Momentum||as||training method
Momentum||whose||momentum parameter ?
momentum parameter ?||set to||0.9
Momentum||whose||initial learning rate
initial learning rate||set to||0.01
Tensorflow||to implement||our proposed model
",,"Hyperparameters||employ||Momentum
Hyperparameters||use||Tensorflow
",,,,,,
baselines,LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .,"LSTM
takes
sentence
as
input
as
to get
hidden representation
of
each word","LSTM||takes||sentence
sentence||as||input
input||to get||hidden representation
hidden representation||of||each word
",,,"Baselines||has||LSTM
",,,,,
baselines,"Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .","regards
average value
of
hidden states
representation of sentence
puts it into
softmax layer
to predict
probability
of
each sentiment polarity","average value||of||hidden states
representation of sentence||puts it into||softmax layer
softmax layer||to predict||probability
probability||of||each sentiment polarity
","hidden states||as||representation of sentence
",,,,"LSTM||regards||average value
",,,
baselines,"AE - LSTM : AE - LSTM first models the words in sentence via LSTM network and concatenate the aspect embedding to the hidden contextual representation for calculating the attention weights , which are employed to produce the final representation for the input sentence to judge the sentiment polarity .","AE - LSTM
models
words
in
sentence
via
LSTM network
concatenate
aspect embedding
to
hidden contextual representation
for calculating
attention weights
employed to produce
final representation
for
input sentence
to judge
sentiment polarity","AE - LSTM||models||words
words||in||sentence
sentence||via||LSTM network
AE - LSTM||concatenate||aspect embedding
aspect embedding||to||hidden contextual representation
hidden contextual representation||for calculating||attention weights
attention weights||employed to produce||final representation
final representation||for||input sentence
input sentence||to judge||sentiment polarity
",,,"Baselines||has||AE - LSTM
",,,,,
baselines,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .","ATAE - LSTM
extended
AE - LSTM
by appending
aspect embedding
to
each word embedding
to represent
input sentence","ATAE - LSTM||extended||AE - LSTM
AE - LSTM||by appending||aspect embedding
aspect embedding||to||each word embedding
each word embedding||to represent||input sentence
",,,"Baselines||has||ATAE - LSTM
",,,,,
baselines,IAN : IAN considers the separate modeling of aspect terms and sentences respectively .,"IAN
considers
separate modeling
of
aspect terms and sentences","IAN||considers||separate modeling
separate modeling||of||aspect terms and sentences
",,,"Baselines||has||IAN
",,,,,
baselines,"MemNet : MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory .","MemNet
applies
attention
multiple times
on
word embedding
so that
more abstractive evidences
could be selected from
external memory","MemNet||applies||attention
multiple times||so that||more abstractive evidences
more abstractive evidences||could be selected from||external memory
multiple times||on||word embedding
","attention||has||multiple times
",,"Baselines||has||MemNet
",,,,,
results,shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .,"on
datasets Restaurant and Laptop",,,"Results||on||datasets Restaurant and Laptop
",,,,,,
results,We can observe that our proposed PBAN model achieves the best performance among all methods .,"observe
our proposed PBAN model
achieves
best performance
among
methods","our proposed PBAN model||achieves||best performance
best performance||among||methods
",,,,,"datasets Restaurant and Laptop||observe||our proposed PBAN model
",,,
results,"Generally speaking , by integrating the position information and the bidirectional attention mechanism , PBAN achieves the state - of - the - art performances , and it can effectively judge the sentiment polarity of different aspect term in its corresponding sentence so as to improve the classification accuracy .","by integrating
position information and the bidirectional attention mechanism
PBAN
achieves
state - of - the - art performances
effectively judge
sentiment polarity
of
different aspect term
in
corresponding sentence
to improve
classification accuracy","PBAN||effectively judge||sentiment polarity
sentiment polarity||of||different aspect term
different aspect term||in||corresponding sentence
corresponding sentence||to improve||classification accuracy
PBAN||achieves||state - of - the - art performances
","position information and the bidirectional attention mechanism||has||PBAN
",,,,"datasets Restaurant and Laptop||by integrating||position information and the bidirectional attention mechanism
",,,
research-problem,DataStories at SemEval-2017 Task 4 : Deep LSTM with Attention for Message - level and Topic - based Sentiment Analysis,Message - level and Topic - based Sentiment Analysis,,,,,"Contribution||has research problem||Message - level and Topic - based Sentiment Analysis
",,,,
research-problem,"In this paper we present two deep - learning systems that competed at SemEval - 2017 Task 4 "" Sentiment Analysis in Twitter "" .",Sentiment Analysis in Twitter,,,,,"Contribution||has research problem||Sentiment Analysis in Twitter
",,,,
research-problem,"Sentiment analysis is an area in Natural Language Processing ( NLP ) , studying the identification and quantification of the sentiment expressed in text .",Sentiment analysis,,,,,"Contribution||has research problem||Sentiment analysis
",,,,
model,"In this paper , we present two deep - learning systems that competed at SemEval - 2017 Task 4 .","present
two deep - learning systems
competed at
SemEval - 2017 Task 4","two deep - learning systems||competed at||SemEval - 2017 Task 4
",,"Model||present||two deep - learning systems
",,,,,,
model,Our first model is designed for addressing the problem of messagelevel sentiment analysis .,"first model
designed for addressing
problem of messagelevel sentiment analysis","first model||designed for addressing||problem of messagelevel sentiment analysis
",,,"Model||has||first model
",,,,,
model,"We employ a 2 - layer Bidirectional LSTM , equipped with an attention mechanism .","employ
2 - layer Bidirectional LSTM
equipped with
attention mechanism","2 - layer Bidirectional LSTM||equipped with||attention mechanism
",,,,,"first model||employ||2 - layer Bidirectional LSTM
",,,
model,"For the topic - based sentiment analysis tasks , we propose a Siamese Bidirectional LSTM with a contextaware attention mechanism .","For
topic - based sentiment analysis tasks
propose
Siamese Bidirectional LSTM
with
contextaware attention mechanism","topic - based sentiment analysis tasks||propose||Siamese Bidirectional LSTM
Siamese Bidirectional LSTM||with||contextaware attention mechanism
",,"Model||For||topic - based sentiment analysis tasks
",,,,,,
hyperparameters,MSA Model ( message - level ),MSA Model,,,,"Hyperparameters||has||MSA Model
",,,,,"MSA Model||has||LSTM layers 150
"
hyperparameters,TSA Model ( topic - based ),TSA Model,,,,"Hyperparameters||has||TSA Model
",,,,,
hyperparameters,"The size of the embedding layer is 300 , and the LSTM layers 150 ( 300 for BiLSTM ) .","size of
embedding layer
is
300
LSTM layers 150","embedding layer||is||300
embedding layer||is||300
",,,,,"MSA Model||size of||embedding layer
",,,
hyperparameters,"We add Gaussian noise with ? = 0.2 and dropout of 0.3 at the embedding layer , dropout of 0.5 at the LSTM layers and dropout of 0.25 at the recurrent connections of the LSTM .","add
Gaussian noise
with
? = 0.2
dropout
of
0.3
at
embedding layer
of
0.5
at
LSTM layers
0.25
at
recurrent connections
LSTM","Gaussian noise||with||? = 0.2
dropout||of||0.3
0.3||at||embedding layer
dropout||of||0.5
0.5||at||LSTM layers
dropout||of||0.25
0.25||at||recurrent connections
recurrent connections||of||LSTM
dropout||of||0.3
Gaussian noise||with||? = 0.2
",,,,,"MSA Model||add||Gaussian noise
MSA Model||add||dropout
",,,
hyperparameters,"Finally , we add L 2 regularization of 0.0001 at the loss function .","L 2 regularization
of
0.0001
at
loss function","L 2 regularization||of||0.0001
0.0001||at||loss function
",,,,,,,"MSA Model||add||L 2 regularization
",
hyperparameters,"The size of the embedding layer is 300 , and the LSTM layers 64 ( 128 for BiLSTM ) .","size of
embedding layer
is
300
LSTM layers
64",,"LSTM layers||has||64
",,,,"TSA Model||size of||LSTM layers
TSA Model||size of||embedding layer
",,,
hyperparameters,"We insert Gaussian noise with ? = 0.2 at the embedding layer of both inputs and dropout of 0.3 at the embedding layer of the message , dropout of 0.2 at the LSTM layer and the recurrent connection of the LSTM layer and dropout of 0.3 at the attention layer and the Maxout layer .","insert
Gaussian noise
with
? = 0.2
at
embedding layer
of
both inputs
dropout
of
0.3
at
embedding layer
of
message
of
0.2
at
LSTM layer
recurrent connection
LSTM layer
0.3
at
attention layer and the Maxout layer","Gaussian noise||with||dropout
dropout||of||0.2
0.2||at||LSTM layer
0.2||at||recurrent connection
recurrent connection||of||LSTM layer
0.3||at||attention layer and the Maxout layer
? = 0.2||at||embedding layer
embedding layer||of||both inputs
",,,,,"TSA Model||insert||Gaussian noise
",,,
hyperparameters,"Finally , we add L 2 regularization of 0.001 at the loss function .","add
L 2 regularization
of
0.001
at
loss function","L 2 regularization||of||0.001
0.001||at||loss function
",,,,,"TSA Model||add||L 2 regularization
",,,
results,"Our official ranking is 1/38 ( tie ) in Subtask A , 2/24 in Subtask B , 2/16 in Subtask C , 2/16 in Subtask D and 11/12 in Subtask E.","Our official ranking
is
1/38 ( tie )
in
Subtask A
2/24
in
Subtask B
2/16
in
Subtask C
in
Subtask D
11/12","Our official ranking||is||1/38 ( tie )
1/38 ( tie )||in||Subtask A
Our official ranking||is||2/16
2/16||in||Subtask C
2/16||in||Subtask D
Our official ranking||is||2/24
2/24||in||Subtask B
Our official ranking||is||11/12
",,,"Results||has||Our official ranking
",,,"11/12||in||Subtask E
",,
research-problem,Sentiment Classification using Document Embeddings trained with Cosine Similarity,Sentiment Classification,,,,,"Contribution||has research problem||Sentiment Classification
",,,,
research-problem,"In document - level sentiment classification , each document must be mapped to a fixed length vector .",document - level sentiment classification,,,,,"Contribution||has research problem||document - level sentiment classification
",,,,
research-problem,"In document classification tasks such as sentiment classification ( in this paper we focus on binary sentiment classification of long movie reviews , i.e. determining whether each review is positive or negative ) , the choice of document representation is usually more important than the choice of classifier .","sentiment classification
binary sentiment classification of long movie reviews",,,,,"Contribution||has research problem||sentiment classification
Contribution||has research problem||binary sentiment classification of long movie reviews
",,,,
model,This paper aims to improve existing document embedding models by training document embeddings using cosine similarity instead of dot product .,"aims to improve
document embedding models
by training
document embeddings
using
cosine similarity","document embedding models||by training||document embeddings
document embeddings||using||cosine similarity
",,"Model||aims to improve||document embedding models
",,,,,,
model,"For example , in the basic model of trying to predict given a document - the words / n - grams in the document , instead of trying to maximize the dot product between a document vector and vectors of the words / n - grams in the document over the training set , we 'll be trying to maximize the cosine similarity instead .","in
trying to
predict
given
document
words / n - grams
document
maximize
cosine similarity","predict||given||document
words / n - grams||in||document
","maximize||has||cosine similarity
predict||has||words / n - grams
","Model||trying to||maximize
Model||trying to||predict
",,,,,,
code,Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.,,,,,,,,,,
hyperparameters,Grid search was performed using 20 % of the training data as a validation set in order to determine the optimal hyperparameters as well as whether to use a constant learning rate or learning rate annealing .,"Grid search
performed using
20 % of the training data
as
validation set
to determine
optimal hyperparameters
to use
constant learning rate or learning rate annealing","Grid search||performed using||20 % of the training data
20 % of the training data||as||validation set
validation set||to determine||optimal hyperparameters
validation set||to use||constant learning rate or learning rate annealing
",,,"Hyperparameters||has||Grid search
",,,,,
hyperparameters,"We did however tune the number of iterations from , learning rate from [ 0.25 , 0.025 , 0.0025 , 0.001 ] and ? from .","tune
number of iterations
from
learning rate
[ 0.25 , 0.025 , 0.0025 , 0.001 ]","learning rate||from||[ 0.25 , 0.025 , 0.0025 , 0.001 ]
",,"Hyperparameters||tune||number of iterations
","Hyperparameters||has||learning rate
",,,,,
hyperparameters,"In the case of using L2 regularized dot product , ? ( regularization strength ) was chosen from [ 1 , 0.1 , 0.01 ] .","using
L2 regularized dot product
chosen from
[ 1 , 0.1 , 0.01 ]","L2 regularized dot product||chosen from||[ 1 , 0.1 , 0.01 ]
",,"Hyperparameters||using||L2 regularized dot product
",,,,,,
hyperparameters,"The optimal learning rate in the case of cosine similarity is extremely small , suggesting a chaotic error surface .","optimal learning rate
in the case of
cosine similarity
is
extremely small
suggesting
chaotic error surface","optimal learning rate||in the case of||cosine similarity
cosine similarity||is||extremely small
extremely small||suggesting||chaotic error surface
",,,"Hyperparameters||has||optimal learning rate
",,,,,
hyperparameters,The model in turn requires a larger number of epochs for convergence .,"requires
larger number of epochs
for
convergence","larger number of epochs||for||convergence
",,"Hyperparameters||requires||larger number of epochs
",,,,,,
hyperparameters,"For the distribution for sampling negative words , we used the n-gram distribution raised to the 3 / 4 th power in accordance with .","For
distribution
for
sampling negative words
used
n-gram distribution
raised to
3 / 4 th power","distribution||for||sampling negative words
sampling negative words||used||n-gram distribution
n-gram distribution||raised to||3 / 4 th power
",,"Hyperparameters||For||distribution
",,,,,,
hyperparameters,"The weights of the networks were initialized from a uniform distribution in the range of [ - 0.001 , 0.001 ] .","weights
of
networks
were
initialized
from
uniform distribution
in
range
of
[ - 0.001 , 0.001 ]","weights||of||networks
networks||were||initialized
initialized||from||uniform distribution
uniform distribution||in||range
range||of||[ - 0.001 , 0.001 ]
",,,"Hyperparameters||has||weights
",,,,,
results,From here we see that using cosine similarity instead of dot product improves accuracy across the board .,"see that using
cosine similarity
instead of
dot product
improves
accuracy","cosine similarity||instead of||dot product
dot product||improves||accuracy
",,"Results||see that using||cosine similarity
",,,,,,
results,However it is not to suggest that switching from dot product to cosine similarity alone improves accuracy as other minor ad - justments and hyperparameter tuning as explained was done .,"to
suggest that
switching
from
dot product
cosine similarity
improves
accuracy","switching||improves||accuracy
switching||from||dot product
dot product||to||cosine similarity
",,"Results||suggest that||switching
",,,,,,
results,"As seen during grid search , whenever the initial learning rate was 0.25 , accuracy was always poor .","during
grid search
initial learning rate
was
0.25","initial learning rate||was||0.25
","grid search||has||initial learning rate
","Results||during||grid search
",,,,,,
results,"Introducing L2 regularization to dot product improves accuracy for all cases except a depreciation in the case of using unigrams only , lucikily cosine similarity does not suffer from this same depreciation .","Introducing
L2 regularization
to
dot product
improves
accuracy
for
all cases
except
depreciation
in the case of using
unigrams","L2 regularization||improves||accuracy
accuracy||for||all cases
all cases||except||depreciation
depreciation||in the case of using||unigrams
L2 regularization||to||dot product
",,"Results||Introducing||L2 regularization
",,,,,,
research-problem,Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis,Aspect-level Sentiment Analysis,,,,,"Contribution||has research problem||Aspect-level Sentiment Analysis
",,,,
research-problem,"Aspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .",sentiment analysis,,,,,"Contribution||has research problem||sentiment analysis
",,,,
model,"Based on the analysis above , in this paper , we propose a hierarchical attention based positionaware network ( HAPN ) for aspect - level sentiment classification .","propose
hierarchical attention based positionaware network ( HAPN )
for
aspect - level sentiment classification","hierarchical attention based positionaware network ( HAPN )||for||aspect - level sentiment classification
",,"Model||propose||hierarchical attention based positionaware network ( HAPN )
",,,,,,
model,A position - aware encoding layer is introduced for modelling the sentence to achieve the position - aware abstract representation of each word .,"position - aware encoding layer
for modelling
sentence
to achieve
position - aware abstract representation
of
each word","position - aware encoding layer||for modelling||sentence
position - aware encoding layer||to achieve||position - aware abstract representation
position - aware abstract representation||of||each word
",,,"Model||has||position - aware encoding layer
",,,,,
model,"On this basis , a succinct fusion mechanism is further proposed to fuse the information of aspects and the contexts , achieving the final sentence representation .","succinct fusion mechanism
proposed to
fuse
information of
aspects and the contexts
achieving
final sentence representation","succinct fusion mechanism||proposed to||fuse
fuse||information of||aspects and the contexts
aspects and the contexts||achieving||final sentence representation
",,,"Model||has||succinct fusion mechanism
",,,,,
model,"Finally , we feed the achieved sentence representation into a softmax layer to predict the sentiment polarity .","feed
achieved sentence representation
into
softmax layer
to predict
sentiment polarity","achieved sentence representation||into||softmax layer
softmax layer||to predict||sentiment polarity
",,"Model||feed||achieved sentence representation
",,,,,,
code,We make our source code public at https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis.,,,,,,,,,,
hyperparameters,"We use 300 - dimension word vectors pre-trained by GloVe ( whose vocabulary size is 1.9M ) for our experiments , as previous works did .","use
300 - dimension word vectors
pre-trained by
GloVe","300 - dimension word vectors||pre-trained by||GloVe
",,"Hyperparameters||use||300 - dimension word vectors
",,,,,,
hyperparameters,"All out - of - vocabulary words are initialized as zero vectors , and all biases are set to zero .","All out - of - vocabulary words
initialized as
zero vectors
all biases
set to
zero","All out - of - vocabulary words||initialized as||zero vectors
all biases||set to||zero
",,,"Hyperparameters||has||All out - of - vocabulary words
Hyperparameters||has||all biases
",,,,,
hyperparameters,The dimensions of hidden states and fused embeddings are set to 300 .,"dimensions
of
hidden states and fused embeddings
set to
300","dimensions||set to||300
300||of||hidden states and fused embeddings
",,,"Hyperparameters||has||dimensions
",,,,,
hyperparameters,The dimension of position embeddings is set to 50 .,"dimension
of
position embeddings
set to
50","dimension||of||position embeddings
position embeddings||set to||50
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,Keras is used for implementing our neural network model .,"Keras
for implementing
neural network model","Keras||for implementing||neural network model
",,,"Hyperparameters||has||Keras
",,,,,
hyperparameters,"In model training , we set the learning rate to 0.001 , the batch size to 64 , and dropout rate to 0.5 .","In
model training
set
learning rate
to
0.001
batch size
to
64
dropout rate
to
0.5","model training||set||learning rate
learning rate||to||0.001
model training||set||batch size
batch size||to||64
model training||set||dropout rate
dropout rate||to||0.5
",,"Hyperparameters||In||model training
",,,,,,
hyperparameters,The paired t- test is used for the significance testing .,"paired t- test
used for
significance testing","paired t- test||used for||significance testing
",,,"Hyperparameters||has||paired t- test
",,,,,
baselines,Majority assigns the sentiment polarity with most frequent occurrences in the training set to each sample in test set .,"Majority
assigns
sentiment polarity
with
most frequent occurrences
in
training set
to
each sample
in
test set","Majority||assigns||sentiment polarity
sentiment polarity||with||most frequent occurrences
most frequent occurrences||in||training set
training set||to||each sample
each sample||in||test set
",,,"Baselines||has||Majority
",,,,,
baselines,Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the final word for prediction respectively .,"Bi - LSTM and Bi - GRU
adopt
Bi - LSTM and a Bi - GRU network
to model
sentence
use
hidden state
of
final word
for
prediction","Bi - LSTM and Bi - GRU||adopt||Bi - LSTM and a Bi - GRU network
Bi - LSTM and a Bi - GRU network||use||hidden state
hidden state||of||final word
final word||for||prediction
Bi - LSTM and a Bi - GRU network||to model||sentence
",,,"Baselines||has||Bi - LSTM and Bi - GRU
",,,,,
baselines,TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; It takes the hidden states of LSTM at last time - step to represent the sentence for prediction .,"TD - LSTM
adopts
two LSTMs
to model
left context
with
target
right context
with
target
takes
hidden states
of
LSTM
at
last time - step
to represent
sentence
for
prediction","TD - LSTM||adopts||two LSTMs
two LSTMs||to model||right context
right context||with||target
two LSTMs||to model||left context
left context||with||target
TD - LSTM||takes||hidden states
hidden states||of||LSTM
LSTM||at||last time - step
last time - step||to represent||sentence
sentence||for||prediction
",,,"Baselines||has||TD - LSTM
",,,,,
baselines,"MemNet applies attention multiple times on the word embeddings , and the output of last attention is fed to softmax for prediction .","MemNet
applies
attention
multiple times
on
word embeddings
output
of
last attention
fed to
softmax
for
prediction","MemNet||applies||attention
multiple times||on||word embeddings
output||of||last attention
output||fed to||softmax
softmax||for||prediction
","attention||has||multiple times
MemNet||has||output
",,"Baselines||has||MemNet
",,,,,
baselines,"IAN interactively learns attentions in the contexts and targets , and generates the representations for targets and contexts separately .","IAN
learns
attentions
in
contexts and targets
generates
representations
for
targets and contexts","IAN||generates||representations
representations||for||targets and contexts
IAN||learns||attentions
attentions||in||contexts and targets
",,,"Baselines||has||IAN
",,,,,
baselines,RAM ) is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .,"RAM
is
multilayer architecture
where
each layer
consists of
attention - based aggregation
of
word features
GRU cell
to learn
sentence representation","RAM||is||multilayer architecture
multilayer architecture||where||each layer
each layer||consists of||GRU cell
GRU cell||to learn||sentence representation
each layer||consists of||attention - based aggregation
attention - based aggregation||of||word features
",,,"Baselines||has||RAM
",,,,,
baselines,"LCR - Rot employs three Bi- LSTMs to model the left context , the target and the right context .","LCR - Rot
employs
three Bi- LSTMs
to model
left context
target
right context","LCR - Rot||employs||three Bi- LSTMs
three Bi- LSTMs||to model||left context
three Bi- LSTMs||to model||target
three Bi- LSTMs||to model||right context
",,,"Baselines||has||LCR - Rot
",,,,,
baselines,AOA - LSTM introduces an attention - over- attention ( AOA ) based network to model aspects and sentences in a joint way and explicitly capture the interaction between aspects and context sentences .,"AOA - LSTM
introduces
attention - over- attention ( AOA ) based network
to model
aspects and sentences
in
joint way
explicitly capture
interaction
between
aspects and context sentences","AOA - LSTM||introduces||attention - over- attention ( AOA ) based network
attention - over- attention ( AOA ) based network||explicitly capture||interaction
interaction||between||aspects and context sentences
attention - over- attention ( AOA ) based network||to model||aspects and sentences
aspects and sentences||in||joint way
",,,"Baselines||has||AOA - LSTM
",,,,,
results,"( 2 ) The TD - LSTM model , which has been shown to be better than LSTM , gets the worst performance of all RNN based models and the accuracy achieved by TD - LSTM is 2.94 % and 2.4 % lower than those by Bi - LSTM on the two datasets respectively .","TD - LSTM model
shown to be better than
LSTM
gets
worst performance
of
all RNN based models
accuracy
achieved by
TD - LSTM
is
2.94 % and 2.4 %
lower than
Bi - LSTM","accuracy||achieved by||TD - LSTM
TD - LSTM||is||2.94 % and 2.4 %
2.94 % and 2.4 %||lower than||Bi - LSTM
TD - LSTM model||gets||worst performance
worst performance||of||all RNN based models
TD - LSTM model||shown to be better than||LSTM
","TD - LSTM model||has||accuracy
",,"Results||has||TD - LSTM model
",,,,,
results,"( 3 ) Compared with the state - of - the - art methods , our model achieves the best performance , which illustrates the effectiveness of the proposed approach .","Compared with
state - of - the - art methods
our model
achieves
best performance","our model||achieves||best performance
","state - of - the - art methods||has||our model
","Results||Compared with||state - of - the - art methods
",,,,,,
results,"Our method achieves accuracies of 82.23 % as well as 77 . 27 % on the Restaurant and Laptop dataset respectively , which are 0.89 % and 2.03 % higher than the current best method .","Our method
achieves
accuracies
of
82.23 %
as well as
77 . 27 %
on
Restaurant and Laptop dataset","Our method||achieves||accuracies
accuracies||of||82.23 %
82.23 %||as well as||77 . 27 %
77 . 27 %||on||Restaurant and Laptop dataset
",,,"Results||has||Our method
",,,,,
results,"After introducing the position embeddings , the accuracy has an increase of 0.62 % and 2.67 % on two datasets .","introducing
position embeddings
accuracy
has an
increase
of
0.62 % and 2.67 %
on
two datasets","accuracy||has an||increase
increase||of||0.62 % and 2.67 %
0.62 % and 2.67 %||on||two datasets
","position embeddings||has||accuracy
","Results||introducing||position embeddings
",,,,,,
results,"In addition , another observation is that Bi - GRU - PW performs even worse than Bi - GRU .","Bi - GRU - PW
performs
even worse
than
Bi - GRU","Bi - GRU - PW||performs||even worse
even worse||than||Bi - GRU
",,,"Results||has||Bi - GRU - PW
",,,,,
results,The accuracy achieved by Bi - GRU - PW is 0.72 % as well as 1.41 % lower than that by Bi - GRU on the Restaurant and Laptop dataset respectively .,"accuracy
achieved by
Bi - GRU - PW
is
0.72 %
as well as
1.41 %
lower than
Bi - GRU
on
Restaurant and Laptop dataset","accuracy||achieved by||Bi - GRU - PW
Bi - GRU - PW||is||0.72 %
0.72 %||as well as||1.41 %
1.41 %||lower than||Bi - GRU
Bi - GRU||on||Restaurant and Laptop dataset
",,,"Results||has||accuracy
",,,,,
results,HAPN achieves improvement of 0.35 % and 0.78 % on accuracy respectively on the two dataset .,"HAPN
achieves
improvement
of
0.35 % and 0.78 %
on
accuracy","HAPN||achieves||improvement
improvement||of||0.35 % and 0.78 %
0.35 % and 0.78 %||on||accuracy
",,,"Results||has||HAPN
",,,,,
results,( 1 ) The information fusion operation is only used to calculate the Source2context attention value .,"information fusion operation
used to
calculate
Source2context attention value","information fusion operation||used to||calculate
","calculate||has||Source2context attention value
",,"Results||has||information fusion operation
",,,,,
results,The output of Source2aspect attention is only used for information fusion .,"output
of
Source2aspect attention
used for
information fusion","output||of||Source2aspect attention
Source2aspect attention||used for||information fusion
",,,"Results||has||output
",,,,,
results,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the accuracies of 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .","Bi - GRU - PE
achieving
accuracies
of
80.89 % and 76.02 %
on
two datasets","Bi - GRU - PE||achieving||accuracies
accuracies||of||80.89 % and 76.02 %
80.89 % and 76.02 %||on||two datasets
",,,"Results||has||Bi - GRU - PE
",,,,,
research-problem,Discriminative Neural Sentence Modeling by Tree - Based Convolution,Discriminative Neural Sentence Modeling,,,,,"Contribution||has research problem||Discriminative Neural Sentence Modeling
",,,,
model,"In this paper , we propose a novel neural architecture for discriminative sentence modeling , called the Tree - Based Convolutional Neural Network ( TBCNN ) .","propose
novel neural architecture
for
discriminative sentence modeling
called
Tree - Based Convolutional Neural Network ( TBCNN )","novel neural architecture||for||discriminative sentence modeling
",,"Model||called||Tree - Based Convolutional Neural Network ( TBCNN )
Model||propose||novel neural architecture
",,,,,,
model,"Our models can leverage different sentence parsing trees , e.g. , constituency trees and dependency trees .","leverage
different sentence parsing trees",,,"Model||leverage||different sentence parsing trees
",,,,,,
model,"The model variants are denoted as c- TBCNN and d - TBCNN , respectively .","variants
denoted as
c- TBCNN
d - TBCNN","variants||denoted as||c- TBCNN
variants||denoted as||d - TBCNN
",,,"Model||has||variants
",,,,,
model,"The idea of tree - based convolution is to apply a set of subtree feature detectors , sliding over the entire parsing tree of a sentence ; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension .","of
tree - based convolution
apply
set of subtree feature detectors
sliding over
entire parsing tree
sentence
pooling
aggregates these extracted feature vectors
taking
maximum value
in
each dimension","tree - based convolution||apply||set of subtree feature detectors
set of subtree feature detectors||sliding over||entire parsing tree
entire parsing tree||of||sentence
set of subtree feature detectors||pooling||aggregates these extracted feature vectors
aggregates these extracted feature vectors||taking||maximum value
maximum value||in||each dimension
",,,"Model||has||tree - based convolution
",,,,,
hyperparameters,"In our d-TBCNN model , the number of units is 300 for convolution and 200 for the last hidden layer .","In
our d-TBCNN model
number of units
is
300
for
convolution
200
for
last hidden layer","number of units||is||200
200||for||last hidden layer
number of units||is||300
300||for||convolution
","our d-TBCNN model||has||number of units
","Hyperparameters||In||our d-TBCNN model
",,,,,,
hyperparameters,"Word embeddings are 300 dimensional , pretrained ourselves using word2vec To train our model , we compute gradient by back - propagation and apply stochastic gradient descent with mini-batch 200 .","Word embeddings
are
300 dimensional
pretrained ourselves using
word2vec
To train
our model
compute
gradient
by
back - propagation
apply
stochastic gradient descent
with
mini-batch 200","our model||compute||gradient
gradient||by||back - propagation
our model||apply||stochastic gradient descent
stochastic gradient descent||with||mini-batch 200
Word embeddings||are||300 dimensional
Word embeddings||pretrained ourselves using||word2vec
",,"Hyperparameters||To train||our model
","Hyperparameters||has||Word embeddings
",,,,,
hyperparameters,We use ReLU as the activation function .,"use
ReLU
as
activation function","ReLU||as||activation function
",,"Hyperparameters||use||ReLU
",,,,,,
hyperparameters,"For regularization , we add 2 penalty for weights with a coefficient of 10 ?5 .","For
regularization
add
2 penalty
for
weights
with
coefficient
of
10 ?5","regularization||add||2 penalty
2 penalty||for||weights
weights||with||coefficient
coefficient||of||10 ?5
",,"Hyperparameters||For||regularization
",,,,,,
hyperparameters,Dropout is further applied to both weights and embeddings .,"Dropout
applied to
both weights and embeddings","Dropout||applied to||both weights and embeddings
",,,"Hyperparameters||has||Dropout
",,,,,
hyperparameters,"All hidden layers are dropped out by 50 % , and embeddings 40 % .","All hidden layers
dropped out by
50 %
embeddings
40 %","All hidden layers||dropped out by||50 %
","embeddings||has||40 %
",,"Hyperparameters||has||embeddings
Hyperparameters||has||All hidden layers
",,,,,
results,"Nonetheless , our d-TBCNN model achieves","d-TBCNN model
achieves",,,,"Results||has||d-TBCNN model
",,,"d-TBCNN model||achieves||87.9 % accuracy
",,
results,"87.9 % accuracy , ranking third in the list .",87.9 % accuracy,,,,,,,,,
results,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .","In
more controlled comparison
with
shallow architectures
basic interaction ( linearly transformed and non-linearly squashed )
TBCNNs
consistently outperform
RNNs
to
large extent ( 50.4 - 51.4 % versus 43.2 % )
"" flat "" CNNs
by more than
10 %","more controlled comparison||with||shallow architectures
more controlled comparison||with||basic interaction ( linearly transformed and non-linearly squashed )
TBCNNs||consistently outperform||RNNs
RNNs||to||large extent ( 50.4 - 51.4 % versus 43.2 % )
TBCNNs||consistently outperform||"" flat "" CNNs
"" flat "" CNNs||by more than||10 %
","more controlled comparison||has||TBCNNs
","Results||In||more controlled comparison
",,,,,,
results,We also observe d- TBCNN achieves higher performance than c - TBCNN .,"observe
d- TBCNN
achieves
higher performance
than
c - TBCNN","d- TBCNN||achieves||higher performance
higher performance||than||c - TBCNN
",,"Results||observe||d- TBCNN
",,,,,,
research-problem,An Interactive Multi - Task Learning Network for End - to - End Aspect - Based Sentiment Analysis,Aspect - Based Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - Based Sentiment Analysis
",,,,
research-problem,Aspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .,Aspect - based sentiment analysis ( ABSA ),,,,,"Contribution||has research problem||Aspect - based sentiment analysis ( ABSA )
",,,,
research-problem,"This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) .","aspect term extraction ( AE )
aspect - level sentiment classification ( AS )",,,,,"Contribution||has research problem||aspect term extraction ( AE )
Contribution||has research problem||aspect - level sentiment classification ( AS )
",,,,
model,"In this work , we propose an interactive multitask learning network ( IMN ) , which solves both tasks simultaneously , enabling the interactions between both tasks to be better exploited .","propose
interactive multitask learning network ( IMN )",,,"Model||propose||interactive multitask learning network ( IMN )
",,,,,,
model,"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .","IMN
allows
AE and AS
to be trained together with
related document - level tasks
exploiting
knowledge
from
larger document - level corpora","IMN||allows||AE and AS
AE and AS||to be trained together with||related document - level tasks
related document - level tasks||exploiting||knowledge
knowledge||from||larger document - level corpora
",,,"Model||has||IMN
",,,,,
model,IMN introduces a novel message passing mechanism that allows informative interactions between tasks .,"introduces
novel message passing mechanism
allows
informative interactions
between
tasks","novel message passing mechanism||allows||informative interactions
informative interactions||between||tasks
",,"Model||introduces||novel message passing mechanism
",,,,,,
model,"Specifically , it sends useful information from different tasks back to a shared latent representation .","sends
useful information
from
different tasks
back to
shared latent representation","useful information||from||different tasks
different tasks||back to||shared latent representation
",,"Model||sends||useful information
",,,,,,
model,The information is then combined with the shared latent representation and made available to all tasks for further processing .,"information
combined with
shared latent representation
made available to
all tasks
for
further processing","information||combined with||shared latent representation
information||made available to||all tasks
all tasks||for||further processing
",,,"Model||has||information
",,,,,
model,"In contrast to most multi-task learning schemes which share information through learning a common feature representation , IMN not only allows shared features , but also explicitly models the interactions between tasks through the message passing mechanism , allowing different tasks to better influence each other .","through
explicitly models
interactions
between
tasks
message passing mechanism
allowing
different tasks
to better influence
each other","interactions||through||message passing mechanism
message passing mechanism||allowing||different tasks
different tasks||to better influence||each other
interactions||between||tasks
",,,,,"IMN||explicitly models||interactions
",,,
model,"In addition , IMN allows fined - grained tokenlevel classification tasks to be trained together with document - level classification tasks .","fined - grained tokenlevel classification tasks
to be trained together with
document - level classification tasks","fined - grained tokenlevel classification tasks||to be trained together with||document - level classification tasks
",,,,,,,"IMN||allows||fined - grained tokenlevel classification tasks
",
model,"We incorporated two document - level classification tasks - sentiment classification ( DS ) and domain classification ( DD ) - to be jointly trained with AE and AS , allowing the aspect - level tasks to benefit from document - level information .","incorporated
two document - level classification tasks
sentiment classification ( DS )
domain classification ( DD )
to be jointly trained with
AE and AS
allowing
aspect - level tasks
to benefit from
document - level information","two document - level classification tasks||to be jointly trained with||AE and AS
AE and AS||allowing||aspect - level tasks
aspect - level tasks||to benefit from||document - level information
","two document - level classification tasks||name||sentiment classification ( DS )
two document - level classification tasks||name||domain classification ( DD )
","Model||incorporated||two document - level classification tasks
",,,,,,
hyperparameters,We adopt the multi - layer - CNN structure from as the CNN - based encoders in our proposed network .,"adopt
multi - layer - CNN structure
from
CNN - based encoders","multi - layer - CNN structure||from||CNN - based encoders
",,"Hyperparameters||adopt||multi - layer - CNN structure
",,,,,,
hyperparameters,"For word embedding initialization , we concatenate a general - purpose embedding matrix and a domain - specific embedding matrix 7 following .","For
word embedding initialization
concatenate
general - purpose embedding matrix
domain - specific embedding matrix","word embedding initialization||concatenate||general - purpose embedding matrix
word embedding initialization||concatenate||domain - specific embedding matrix
",,"Hyperparameters||For||word embedding initialization
",,,,,,
hyperparameters,"We adopt their released domainspecific embeddings for restaurant and laptop domains with 100 dimensions , which are trained on a large domain - specific corpus using fast Text .","released domainspecific embeddings
for
restaurant and laptop domains
with
100 dimensions
trained on
large domain - specific corpus
using
fast Text","released domainspecific embeddings||for||restaurant and laptop domains
restaurant and laptop domains||with||100 dimensions
released domainspecific embeddings||trained on||large domain - specific corpus
large domain - specific corpus||using||fast Text
",,,"Hyperparameters||adopt||released domainspecific embeddings
",,,,,
hyperparameters,The general - purpose embeddings are pre-trained Glove vectors with 300 dimensions .,"general - purpose embeddings
are
pre-trained Glove vectors
with
300 dimensions","general - purpose embeddings||are||pre-trained Glove vectors
pre-trained Glove vectors||with||300 dimensions
",,,"Hyperparameters||has||general - purpose embeddings
",,,,,
hyperparameters,We tune the maximum number of iterations T in the message passing mechanism by training IMN ?d via cross validation on D1 .,"tune
maximum number of iterations T
in
message passing mechanism
by training
IMN ?d
via
cross validation","maximum number of iterations T||in||message passing mechanism
message passing mechanism||by training||IMN ?d
IMN ?d||via||cross validation
",,"Hyperparameters||tune||maximum number of iterations T
",,,,,,
hyperparameters,It is set to 2 .,"set to
2",,,,,,"maximum number of iterations T||set to||2
",,,
hyperparameters,"We use Adam optimizer with learning rate set to 10 ? 4 , and we set batch size to 32 .","use
Adam optimizer
with
learning rate
set to
10 ? 4
set
batch size
to
32","Adam optimizer||with||learning rate
learning rate||set to||10 ? 4
Adam optimizer||set||batch size
batch size||to||32
",,"Hyperparameters||use||Adam optimizer
",,,,,,
hyperparameters,Learning rate and batch size are set to conventional values without specific tuning for our task .,"Learning rate and batch size
set to
conventional values
without specific tuning for
our task","Learning rate and batch size||set to||conventional values
conventional values||without specific tuning for||our task
",,,"Hyperparameters||has||Learning rate and batch size
",,,,,
hyperparameters,"At training phase , we randomly sample 20 % of the training data from the aspect - level dataset as the development set and only use the remaining 80 % for training .","At
training phase
randomly sample
20 %
of
training data
from
aspect - level dataset
as
development set
use
remaining 80 %
for
training","training phase||randomly sample||20 %
20 %||as||development set
20 %||of||training data
training data||from||aspect - level dataset
training phase||use||remaining 80 %
remaining 80 %||for||training
",,"Hyperparameters||At||training phase
",,,,,,
results,"From , we observe that IMN ?d is able to significantly outperform other baselines on F1 - I .","observe that
IMN ?d
able to
significantly outperform
other
baselines
on
F1","IMN ?d||able to||significantly outperform
significantly outperform||other||baselines
significantly outperform||on||F1
",,"Results||observe that||IMN ?d
",,,,,,
results,"IMN further boosts the performance and outperforms the best F1 - I results from the baselines by 2.29 % , 1.77 % , and 2.61 % on D1 , D2 , and D3 .","IMN
boosts
performance
outperforms
best F1
from
baselines
by
2.29 % , 1.77 % , and 2.61 %
on
D1 , D2 , and D3","IMN||boosts||performance
IMN||outperforms||best F1
best F1||from||baselines
baselines||by||2.29 % , 1.77 % , and 2.61 %
2.29 % , 1.77 % , and 2.61 %||on||D1 , D2 , and D3
",,,"Results||has||IMN
",,,,,
results,"Specifically , for AE ( F1 - a and F1 - o ) , IMN ?d performs the best in most cases .","for
AE ( F1 - a and F1 - o )
IMN ?d
performs
best
in
most cases","IMN ?d||performs||best
best||in||most cases
","AE ( F1 - a and F1 - o )||has||IMN ?d
","Results||for||AE ( F1 - a and F1 - o )
",,,,,,
results,"For AS ( acc - s and F1 - s ) , IMN outperforms other methods by large margins .","For
AS ( acc - s and F1 - s )
IMN
outperforms
other methods
by
large margins","IMN||outperforms||other methods
other methods||by||large margins
","AS ( acc - s and F1 - s )||has||IMN
","Results||For||AS ( acc - s and F1 - s )
",,,,,,
results,IMN wo DE performs only marginally below IMN .,"IMN wo DE
performs
only marginally
below
IMN","IMN wo DE||performs||only marginally
only marginally||below||IMN
",,,"Results||has||IMN wo DE
",,,,,
results,"IMN ?d is more affected without domain - specific embeddings , while it still outperforms all other baselines except DECNN - d Trans .","IMN ?d
more affected without
domain - specific embeddings
outperforms
all other baselines
except
DECNN - d Trans","IMN ?d||more affected without||domain - specific embeddings
IMN ?d||outperforms||all other baselines
all other baselines||except||DECNN - d Trans
",,,"Results||has||IMN ?d
",,,,,
results,DECNN - dTrans is a very strong baseline as it exploits additional knowledge from larger corpora for both tasks .,"DECNN - dTrans
is
very strong baseline
exploits
additional knowledge
from
larger corpora
for
both tasks","DECNN - dTrans||is||very strong baseline
very strong baseline||exploits||additional knowledge
additional knowledge||from||larger corpora
larger corpora||for||both tasks
",,,"Results||has||DECNN - dTrans
",,,,,
results,"IMN ?d wo DE is competitive with DECNN - dTrans even without utilizing additional knowledge , which suggests the effectiveness of the proposed network structure .","IMN ?d wo DE
competitive with
DECNN - dTrans","IMN ?d wo DE||competitive with||DECNN - dTrans
",,,"Results||has||IMN ?d wo DE
",,,,,
ablation-analysis,"We observe that + Message passing - a and + Message passing - d contribute to the performance gains the most , which demonstrates the effectiveness of the proposed message passing mechanism .","observe
+ Message passing - a and + Message passing - d
contribute to
performance gains
demonstrates
effectiveness
of
proposed message passing mechanism","+ Message passing - a and + Message passing - d||contribute to||performance gains
+ Message passing - a and + Message passing - d||demonstrates||effectiveness
effectiveness||of||proposed message passing mechanism
",,"Ablation analysis||observe||+ Message passing - a and + Message passing - d
",,,,,,
ablation-analysis,We also observe that simply adding documentlevel tasks ( + DS / DD ) with parameter sharing only marginally improves the performance of IMN ?d .,"adding documentlevel tasks ( + DS / DD )
with
parameter sharing
marginally improves
performance
of
IMN ?d","adding documentlevel tasks ( + DS / DD )||with||parameter sharing
parameter sharing||marginally improves||performance
performance||of||IMN ?d
",,,"Ablation analysis||observe||adding documentlevel tasks ( + DS / DD )
",,,,,
ablation-analysis,"However , + Message passing -d is still helpful with considerable performance gains , showing that aspect - level tasks can benefit from knowing predictions of the relevant document - level tasks .","Message passing -d
still helpful with
considerable performance gains","Message passing -d||still helpful with||considerable performance gains
",,,"Ablation analysis||has||Message passing -d
",,,,,
research-problem,Aspect Based Sentiment Analysis with Gated Convolutional Networks,Aspect Based Sentiment Analysis,,,,,"Contribution||has research problem||Aspect Based Sentiment Analysis
",,,,
research-problem,"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .",Aspect based sentiment analysis ( ABSA ),,,,,"Contribution||has research problem||Aspect based sentiment analysis ( ABSA )
",,,,
research-problem,We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .,"aspect - category sentiment analysis ( ACSA )
aspect - term sentiment analysis ( ATSA )",,,,,"Contribution||has research problem||aspect - category sentiment analysis ( ACSA )
Contribution||has research problem||aspect - term sentiment analysis ( ATSA )
",,,,
research-problem,"A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .",ABSA,,,,,"Contribution||has research problem||ABSA
",,,,
research-problem,"The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories .",ACSA,,,,,"Contribution||has research problem||ACSA
",,,,
research-problem,"On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word .",ATSA,,,,,"Contribution||has research problem||ATSA
",,,,
model,"In this paper , we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .","propose
fast and effective neural network
for
ACSA and ATSA
based on
convolutions and gating mechanisms","fast and effective neural network||for||ACSA and ATSA
ACSA and ATSA||based on||convolutions and gating mechanisms
",,"Model||propose||fast and effective neural network
",,,,,,
model,"For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .","For
ACSA task
our model
two separate convolutional layers
on the top of
embedding layer
whose
outputs
are combined by
novel gating units","two separate convolutional layers||on the top of||embedding layer
embedding layer||whose||outputs
outputs||are combined by||novel gating units
","ACSA task||has||our model
our model||has||two separate convolutional layers
","Model||For||ACSA task
",,,,,,
model,"The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .","proposed
gating units
have
two nonlinear gates
connected to
one convolutional layer","gating units||have||two nonlinear gates
two nonlinear gates||connected to||one convolutional layer
",,"Model||proposed||gating units
",,,,,,
model,"For ATSA task , where the aspect terms consist of multiple words , we extend our model to include another convolutional layer for the target expressions .","ATSA task
extend
our model
to include
another convolutional layer
for
target expressions","ATSA task||extend||our model
our model||to include||another convolutional layer
another convolutional layer||for||target expressions
",,,"Model||For||ATSA task
",,,,,
hyperparameters,"In our experiments , word embedding vectors are initialized with 300 - dimension GloVe vectors which are pre-trained on unlabeled data of 840 billion tokens .","word embedding vectors
initialized with
300 - dimension GloVe vectors
pre-trained on
unlabeled data
of
840 billion tokens","word embedding vectors||initialized with||300 - dimension GloVe vectors
300 - dimension GloVe vectors||pre-trained on||unlabeled data
unlabeled data||of||840 billion tokens
",,,"Hyperparameters||has||word embedding vectors
",,,,,
hyperparameters,"Words out of the vocabulary of Glo Ve are randomly initialized with a uniform distribution U ( ? 0.25 , 0.25 ) .","Words out of the vocabulary
of
Glo Ve
are
randomly initialized
with
uniform distribution U ( ? 0.25 , 0.25 )","Words out of the vocabulary||of||Glo Ve
Glo Ve||are||randomly initialized
randomly initialized||with||uniform distribution U ( ? 0.25 , 0.25 )
",,,"Hyperparameters||has||Words out of the vocabulary
",,,,,
hyperparameters,"We use Adagrad with a batch size of 32 instances , default learning rate of 1 e ? 2 , and maximal epochs of 30 .","use
Adagrad
with
batch size
of
32 instances
default learning rate
of
1 e ? 2
maximal epochs
of
30","Adagrad||with||batch size
batch size||of||32 instances
Adagrad||with||default learning rate
default learning rate||of||1 e ? 2
Adagrad||with||maximal epochs
maximal epochs||of||30
",,"Hyperparameters||use||Adagrad
",,,,,,
hyperparameters,We only fine tune early stopping with 5 - fold cross validation on training datasets .,"fine tune
early stopping
with
5 - fold cross validation
on
training datasets","early stopping||with||5 - fold cross validation
5 - fold cross validation||on||training datasets
",,"Hyperparameters||fine tune||early stopping
",,,,,,
hyperparameters,All neural models are implemented in PyTorch .,"neural models
implemented in
PyTorch","neural models||implemented in||PyTorch
",,,"Hyperparameters||has||neural models
",,,,,
baselines,NRC - Canada is the top method in SemEval 2014 Task 4 for ACSA and ATSA task .,"NRC - Canada
is
top method
in
SemEval 2014 Task 4
for
ACSA and ATSA task","NRC - Canada||is||top method
top method||in||SemEval 2014 Task 4
top method||for||ACSA and ATSA task
",,,"Baselines||has||NRC - Canada
",,,,,
baselines,CNN is widely used on text classification task .,"CNN
widely used on
text classification task","CNN||widely used on||text classification task
",,,"Baselines||has||CNN
",,,,,
baselines,TD - LSTM uses two LSTM networks to model the preceding and following contexts of the target to generate target - dependent representation for sentiment prediction .,"TD - LSTM
uses
two LSTM networks
to model
preceding and following contexts
of
target
to generate
target - dependent representation
for
sentiment prediction","TD - LSTM||uses||two LSTM networks
two LSTM networks||to generate||target - dependent representation
target - dependent representation||for||sentiment prediction
two LSTM networks||to model||preceding and following contexts
preceding and following contexts||of||target
",,,"Baselines||has||TD - LSTM
",,,,,
baselines,ATAE - LSTM is an attention - based LSTM for ACSA task .,"ATAE - LSTM
is
attention - based LSTM
for
ACSA task","ATAE - LSTM||is||attention - based LSTM
attention - based LSTM||for||ACSA task
",,,"Baselines||has||ATAE - LSTM
",,,,,
baselines,"IAN stands for interactive attention network for ATSA task , which is also based on LSTM and attention mechanisms .","IAN
stands for
interactive attention network
for
ATSA task
based on
LSTM and attention mechanisms","IAN||based on||LSTM and attention mechanisms
IAN||stands for||interactive attention network
interactive attention network||for||ATSA task
",,,"Baselines||has||IAN
",,,,,
baselines,"RAM is a recurrent attention network for ATSA task , which uses LSTM and multiple attention mechanisms .","RAM
is
recurrent attention network
for
ATSA task
uses
LSTM and multiple attention mechanisms","RAM||is||recurrent attention network
recurrent attention network||for||ATSA task
RAM||uses||LSTM and multiple attention mechanisms
",,,"Baselines||has||RAM
",,,,,
baselines,"GCN stands for gated convolutional neural network , in which GTRU does not have the aspect embedding as an additional input .","GCN
stands for
gated convolutional neural network
in which
GTRU
does not have
aspect embedding
as
additional input","GCN||in which||GTRU
GTRU||does not have||aspect embedding
aspect embedding||as||additional input
GCN||stands for||gated convolutional neural network
",,,"Baselines||has||GCN
",,,,,
results,LSTM based model ATAE - LSTM has the worst performance of all neural networks .,"LSTM based model ATAE - LSTM
worst performance
of
all neural networks","worst performance||of||all neural networks
","LSTM based model ATAE - LSTM||has||worst performance
",,,,,,"ACSA||has||LSTM based model ATAE - LSTM
",
results,GCAE improves the performance by 1.1 % to 2.5 % compared with ATAE - LSTM .,"GCAE
improves
performance
by
1.1 % to 2.5 %
compared with
ATAE - LSTM","GCAE||improves||performance
performance||by||1.1 % to 2.5 %
1.1 % to 2.5 %||compared with||ATAE - LSTM
",,,,,,,"ACSA||has||GCAE
",
results,"Without the large amount of sentiment lexicons , SVM perform worse than neural methods .","Without the large amount of
sentiment lexicons
SVM
perform
worse
than
neural methods","SVM||perform||worse
worse||than||neural methods
","sentiment lexicons||has||SVM
",,,,"ACSA||Without the large amount of||sentiment lexicons
",,,
results,"With multiple sentiment lexicons , the performance is increased by 7.6 % .","performance
increased
by
7.6 %","increased||by||7.6 %
",,,,,"SVM||performance||increased
",,,
results,GCAE achieves 4 % higher accuracy than ATAE - LSTM on Restaurant - Large and 5 % higher on SemEval - 2014 on ACSA task .,"GCAE
achieves
4 % higher accuracy
than
ATAE - LSTM
on
Restaurant - Large
5 % higher
on
SemEval - 2014 on ACSA task","GCAE||achieves||5 % higher
5 % higher||on||SemEval - 2014 on ACSA task
GCAE||achieves||4 % higher accuracy
4 % higher accuracy||than||ATAE - LSTM
ATAE - LSTM||on||Restaurant - Large
",,,,,,,,
results,"However , GCN , which does not have aspect modeling part , has higher score than GCAE on the original restaurant dataset .","GCN
higher score
than
GCAE
on
original restaurant dataset","higher score||than||GCAE
GCAE||on||original restaurant dataset
","GCN||has||higher score
",,,,,,"ACSA||has||GCN
",
results,ATSA,ATSA,,,,"Results||has||ATSA
",,,,,"ATSA||has||IAN
"
results,"IAN has better performance than TD - LSTM and ATAE - LSTM , because two attention layers guides the representation learning of the context and the entity interactively .","IAN
better performance
than
TD - LSTM and ATAE - LSTM","better performance||than||TD - LSTM and ATAE - LSTM
","IAN||has||better performance
",,,,,,,
results,"RAM also achieves good accuracy by combining multiple attentions with a recurrent neural network , but it needs more training time as shown in the following section .","RAM
achieves
good accuracy
by combining
multiple attentions
with
recurrent neural network","RAM||achieves||good accuracy
good accuracy||by combining||multiple attentions
multiple attentions||with||recurrent neural network
",,,,,,,"ATSA||has||RAM
",
results,"On the hard test dataset , GCAE has 1 % higher accuracy than RAM on restaurant data and 1.7 % higher on laptop data .","On
hard test dataset
GCAE
1 % higher accuracy
than
RAM
on
restaurant data
1.7 % higher
on
laptop data","1.7 % higher||on||laptop data
1 % higher accuracy||than||RAM
1 % higher accuracy||on||restaurant data
","hard test dataset||has||GCAE
GCAE||has||1.7 % higher
GCAE||has||1 % higher accuracy
",,,,"ATSA||On||hard test dataset
",,,
results,"Because of the gating mechanisms and the convolutional layer over aspect terms , GCAE outperforms other neural models and basic SVM .","GCAE
outperforms
other neural models and basic SVM","GCAE||outperforms||other neural models and basic SVM
",,,,,,,"ATSA||has||GCAE
",
research-problem,A Helping Hand : Transfer Learning for Deep Sentiment Analysis,Deep Sentiment Analysis,,,,,"Contribution||has research problem||Deep Sentiment Analysis
",,,,
research-problem,"Over the past decades , sentiment analysis has grown from an academic endeavour to an essential analytics tool .",sentiment analysis,,,,,"Contribution||has research problem||sentiment analysis
",,,,
research-problem,"In recent years , deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification .",supervised sentiment polarity classification,,,,,"Contribution||has research problem||supervised sentiment polarity classification
",,,,
model,"In this paper , we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis .","investigate
extrinsic signals
incorporated into
deep neural networks
for
sentiment analysis","extrinsic signals||incorporated into||deep neural networks
deep neural networks||for||sentiment analysis
",,"Model||investigate||extrinsic signals
",,,,,,
model,"In our paper , we instead consider word embeddings specifically specialized for the task of sentiment analysis , studying how they can lead to stronger and more consistent gains , despite the fact that the embeddings were obtained using out - of - domain data .","consider
word embeddings
specialized for
sentiment analysis
lead to
stronger and more consistent gains
obtained using
out - of - domain data","word embeddings||lead to||stronger and more consistent gains
stronger and more consistent gains||obtained using||out - of - domain data
word embeddings||specialized for||sentiment analysis
",,"Model||consider||word embeddings
",,,,,,
model,We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings .,"propose
bespoke convolutional neural network architecture
with
separate memory module
dedicated to
sentiment embeddings","bespoke convolutional neural network architecture||with||separate memory module
separate memory module||dedicated to||sentiment embeddings
",,"Model||propose||bespoke convolutional neural network architecture
",,,,,,
experimental-setup,Embeddings .,Embeddings,,,,,,,,"Experimental Setup||has||Embeddings
","Embeddings||has||standard pre-trained word vectors
"
experimental-setup,"The standard pre-trained word vectors used for English are the GloVe ones trained on 840 billion tokens of Common Crawl data 1 , while for other languages , we rely on the Facebook fastText Wikipedia embeddings as input representations .","standard pre-trained word vectors
for
English
are
GloVe ones
trained on
840 billion tokens
of
Common Crawl data
other languages
rely on
Facebook fastText Wikipedia embeddings
as
input representations","standard pre-trained word vectors||for||English
English||are||GloVe ones
GloVe ones||trained on||840 billion tokens
840 billion tokens||of||Common Crawl data
standard pre-trained word vectors||for||other languages
other languages||rely on||Facebook fastText Wikipedia embeddings
Facebook fastText Wikipedia embeddings||as||input representations
",,,,,,"standard pre-trained word vectors||are||300 - dimensional
",,
experimental-setup,All of these are 300 - dimensional .,"are
300 - dimensional",,,,,,,,,
experimental-setup,"The vectors are either fed to the CNN , or to the convolutional module of the DM - MCNN during initialization , while unknown words are initialized with zeros .","vectors
fed to
CNN
convolutional module
of
DM - MCNN
during
initialization
unknown words
initialized with
zeros","vectors||fed to||CNN
vectors||fed to||convolutional module
convolutional module||of||DM - MCNN
DM - MCNN||during||initialization
unknown words||initialized with||zeros
",,,,,,,"Embeddings||has||vectors
Embeddings||has||unknown words
",
experimental-setup,"All words , including the unknown ones , are fine - tuned during the training process .","All words
including
unknown ones
are
fine - tuned
during
training process","All words||including||unknown ones
All words||are||fine - tuned
fine - tuned||during||training process
",,,,,,,"Embeddings||has||All words
",
experimental-setup,"For our transfer learning approach , our experiments rely on the multi-domain sentiment dataset by , collected from Amazon customers reviews .","For
transfer learning approach
rely on
multi-domain sentiment dataset
collected from
Amazon customers reviews","transfer learning approach||rely on||multi-domain sentiment dataset
multi-domain sentiment dataset||collected from||Amazon customers reviews
",,,,,"Embeddings||For||transfer learning approach
",,,
experimental-setup,"Specifically , we train linear SVMs using scikit - learn to extract word coefficients in each domain and also for the union of all domains together , yielding a 26 - dimensional sentiment embedding .","train
linear SVMs
using
scikit - learn
to extract
word coefficients
in
each domain
also for
union of all domains
yielding
26 - dimensional sentiment embedding","linear SVMs||using||scikit - learn
scikit - learn||to extract||word coefficients
word coefficients||yielding||26 - dimensional sentiment embedding
word coefficients||in||each domain
word coefficients||also for||union of all domains
",,,,,"Embeddings||train||linear SVMs
",,,
experimental-setup,"For comparison and analysis , we also consider several alternative forms of infusing external cues .","consider
several alternative forms
of infusing
external cues","several alternative forms||of infusing||external cues
",,,,,"Embeddings||consider||several alternative forms
",,,
experimental-setup,We consider a recent sentiment lexicon called VADER .,"sentiment lexicon
called
VADER","sentiment lexicon||called||VADER
",,,,,,,"Embeddings||consider||sentiment lexicon
",
experimental-setup,"These contain separate domain - specific scores for 250 different Reddit communities , and hence result in 250 - dimensional embeddings .","contain
separate domain - specific scores
for
250 different Reddit communities
result in
250 - dimensional embeddings","separate domain - specific scores||for||250 different Reddit communities
250 different Reddit communities||result in||250 - dimensional embeddings
",,,,,"VADER||contain||separate domain - specific scores
",,,
experimental-setup,"For cross - lingual projection , we extract links between words from a 2017 dump of the English edition of Wiktionary .","cross - lingual projection
extract
links
between
words
from
2017 dump of the English edition of Wiktionary","cross - lingual projection||extract||links
links||between||words
words||from||2017 dump of the English edition of Wiktionary
",,,,,,,"Embeddings||For||cross - lingual projection
",
experimental-setup,"We restrict the vocabulary link set to include the languages in , mining corresponding translation , synonymy , derivation , and etymological links from Wiktionary .","restrict
vocabulary link
set to include
languages
mining
corresponding
translation
synonymy
derivation
etymological links
Wiktionary","vocabulary link||set to include||languages
vocabulary link||mining||Wiktionary
Wiktionary||corresponding||translation
Wiktionary||corresponding||synonymy
Wiktionary||corresponding||derivation
Wiktionary||corresponding||etymological links
",,,,,"Embeddings||restrict||vocabulary link
",,,
experimental-setup,"For CNNs , we make use of the well - known CNN - non-static architecture and hyperparameters proposed by , with a learning rate of 0.0006 , obtained by tuning on the validation data .","For
CNNs
make use of
well - known CNN - non-static architecture and hyperparameters
with
learning rate
of
0.0006
obtained by
tuning
on
validation data","CNNs||make use of||well - known CNN - non-static architecture and hyperparameters
well - known CNN - non-static architecture and hyperparameters||with||learning rate
learning rate||of||0.0006
0.0006||obtained by||tuning
tuning||on||validation data
",,,,,"Experimental Setup||For||CNNs
",,,
experimental-setup,"For our DM - MCNN models , the configuration of the convolutional module is the same as for CNNs , and the remaining hyperparameter values were as well tuned on the validation sets .","DM - MCNN models
configuration
of
convolutional module
same as for
CNNs
remaining hyperparameter values
tuned on
validation sets","configuration||of||convolutional module
convolutional module||same as for||CNNs
remaining hyperparameter values||tuned on||validation sets
","DM - MCNN models||has||configuration
DM - MCNN models||has||remaining hyperparameter values
",,,,,,"Experimental Setup||For||DM - MCNN models
",
experimental-setup,"For greater efficiency and better convergence properties , the training relies on mini-batches .","greater efficiency and better convergence properties
training
relies on
mini-batches","training||relies on||mini-batches
","greater efficiency and better convergence properties||has||training
",,,,,,"Experimental Setup||For||greater efficiency and better convergence properties
",
experimental-setup,"Our implementation considers the maximal sentence length in each mini-batch and zero - pads all other sentences to this length under convolutional module , thus enabling uniform and fast processing of each mini-batch .","implementation
considers
maximal sentence length
in
each mini-batch
zero - pads
all other sentences
to
this length
under
convolutional module
enabling
uniform and fast processing
of
each mini-batch","implementation||considers||maximal sentence length
maximal sentence length||in||each mini-batch
implementation||zero - pads||all other sentences
all other sentences||to||this length
this length||under||convolutional module
convolutional module||enabling||uniform and fast processing
uniform and fast processing||of||each mini-batch
",,,,,,,"Experimental Setup||has||implementation
",
experimental-setup,All neural network architectures are implemented using the PyTorch framework 2 .,"neural network architectures
implemented using
PyTorch","neural network architectures||implemented using||PyTorch
",,,,,,,"Experimental Setup||has||neural network architectures
",
results,"Comparing this to CNNs with GloVe / fastText embeddings , where Glo Ve is used for English , and fastText is used for all other languages , we observe substantial improvements across all datasets .","Comparing this to
CNNs
with
GloVe / fastText embeddings
where
Glo Ve
used for
English
fastText
used for
all other languages
observe
substantial improvements
across all
datasets","CNNs||with||GloVe / fastText embeddings
GloVe / fastText embeddings||where||fastText
fastText||used for||all other languages
GloVe / fastText embeddings||where||Glo Ve
Glo Ve||used for||English
CNNs||observe||substantial improvements
substantial improvements||across all||datasets
",,"Results||Comparing this to||CNNs
",,,,,,
results,This shows that word vectors do tend to convey pertinent word semantics signals that enable models to generalize better .,"shows
word vectors
tend to convey
pertinent word semantics signals
that enable
models
to generalize
better","word vectors||tend to convey||pertinent word semantics signals
pertinent word semantics signals||that enable||models
models||to generalize||better
",,"Results||shows||word vectors
",,,,,,
results,Note also that the accuracy using GloVe on the English movies review dataset is consistent with numbers reported in previous work .,"Note
accuracy
using
GloVe
on
English movies review dataset
consistent with
numbers
reported in
previous work","accuracy||using||GloVe
GloVe||on||English movies review dataset
English movies review dataset||consistent with||numbers
numbers||reported in||previous work
",,"Results||Note||accuracy
",,,,,,
results,"Next , we consider our DM - MCNNs with their dual - module mechanism to take advantage of transfer learning .","consider
our DM - MCNNs
with
dual - module mechanism","our DM - MCNNs||with||dual - module mechanism
",,"Results||consider||our DM - MCNNs
",,,,,,
results,We observe fairly consistent and sometimes quite substan - tial gains over CNNs with just the GloVe / fastText vectors .,"observe
fairly consistent and sometimes quite substan - tial gains
over
CNNs
with just
GloVe / fastText vectors","fairly consistent and sometimes quite substan - tial gains||over||CNNs
CNNs||with just||GloVe / fastText vectors
",,"Results||observe||fairly consistent and sometimes quite substan - tial gains
",,,,,,
results,"Although the automatically projected cross - lingual embeddings are very noisy and limited in their coverage , particularly with respect to inflected forms , our model succeeds in exploiting them to obtain substantial gains in several different languages and domains .","automatically projected cross - lingual embeddings
are
very noisy and limited in their coverage
particularly with respect to
inflected forms
our model
succeeds in exploiting them to obtain
substantial gains
in
several different languages and domains","automatically projected cross - lingual embeddings||are||very noisy and limited in their coverage
very noisy and limited in their coverage||particularly with respect to||inflected forms
our model||succeeds in exploiting them to obtain||substantial gains
substantial gains||in||several different languages and domains
","automatically projected cross - lingual embeddings||has||our model
",,"Results||has||automatically projected cross - lingual embeddings
",,,,,
research-problem,Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect - level Sentiment Classification,Aspect - level Sentiment Classification,,,,,"Contribution||has research problem||Aspect - level Sentiment Classification
",,,,
research-problem,"It is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context .",sentiment analysis,,,,,"Contribution||has research problem||sentiment analysis
",,,,
model,"In this paper , we propose a novel method to model Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ) for aspect - level sentiment classification .","propose
novel method
to model
Sentiment Dependencies with Graph Convolutional Networks ( SDGCN )
for
aspect - level sentiment classification","novel method||to model||Sentiment Dependencies with Graph Convolutional Networks ( SDGCN )
Sentiment Dependencies with Graph Convolutional Networks ( SDGCN )||for||aspect - level sentiment classification
",,"Model||propose||novel method
",,,,,,
model,"GCN is a simple and effective convolutional neural network operating on graphs , which can catch inter-dependent information from rich relational data .","GCN
is
simple and effective convolutional neural network
operating on
graphs
can catch
inter-dependent information
from
rich relational data","GCN||is||simple and effective convolutional neural network
simple and effective convolutional neural network||operating on||graphs
graphs||can catch||inter-dependent information
inter-dependent information||from||rich relational data
",,,"Model||has||GCN
",,,,,
model,"For every node in graph , GCN encodes relevant information about its neighborhoods as a new feature representation vector .","For
every node
in
graph
GCN
encodes
relevant information
about
neighborhoods
as
new feature representation vector","every node||in||graph
GCN||encodes||relevant information
relevant information||as||new feature representation vector
relevant information||about||neighborhoods
","every node||has||GCN
","Model||For||every node
",,,,,,
model,"In our case , an aspect is treated as a node , and an edge represents the sentiment dependency relation of two nodes .","aspect
treated as
node
edge
represents
sentiment dependency relation
of
two nodes","edge||represents||sentiment dependency relation
sentiment dependency relation||of||two nodes
aspect||treated as||node
",,,"Model||has||edge
Model||has||aspect
",,,,,
model,Our model learns the sentiment dependencies of aspects via this graph structure .,"learns
sentiment dependencies
of
aspects
via
graph structure","sentiment dependencies||of||aspects
aspects||via||graph structure
",,"Model||learns||sentiment dependencies
",,,,,,
model,"As far as we know , our work is the first to consider the sentiment dependencies between aspects in one sentence for aspect - level sentiment classification task .","first to consider
sentiment dependencies
between
aspects
in
one sentence
for
aspect - level sentiment classification task","sentiment dependencies||between||aspects
aspects||in||one sentence
one sentence||for||aspect - level sentiment classification task
",,"Model||first to consider||sentiment dependencies
",,,,,,
model,"Furthermore , in order to capture the aspect - specific representations , our model applies bidirectional attention mechanism with position encoding before GCN .","applies
bidirectional attention mechanism
with
position encoding
before
GCN","bidirectional attention mechanism||with||position encoding
position encoding||before||GCN
",,"Model||applies||bidirectional attention mechanism
",,,,,,
hyperparameters,"In our implementation , we respectively use the GloVe 3 word vector and the pre-trained language model word representation BERT 4 to initialize the word embeddings .","use
GloVe 3 word vector
pre-trained language model word representation BERT
to initialize
word embeddings","word embeddings||use||GloVe 3 word vector
word embeddings||use||pre-trained language model word representation BERT
",,"Hyperparameters||to initialize||word embeddings
",,,,,,
hyperparameters,The dimension of each word vector is 300 for GloVe and 768 for BERT .,"dimension
of
each word vector
is
300
for
GloVe
768
for
BERT","dimension||of||each word vector
each word vector||is||300
300||for||GloVe
each word vector||is||768
768||for||BERT
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,"The number of LSTM hidden units is set to 300 , and the output dimension of GCN layer is set to 600 .","number
of
LSTM hidden units
set to
300
output dimension
of
GCN layer
set to
600","number||of||LSTM hidden units
LSTM hidden units||set to||300
output dimension||of||GCN layer
GCN layer||set to||600
",,,"Hyperparameters||has||number
Hyperparameters||has||output dimension
",,,,,
hyperparameters,"The weight matrix of last fully connect layer is randomly initialized by a normal distribution N ( 0 , 1 ) .","weight matrix
of
last fully connect layer
randomly initialized by
normal distribution N ( 0 , 1 )","weight matrix||of||last fully connect layer
last fully connect layer||randomly initialized by||normal distribution N ( 0 , 1 )
",,,"Hyperparameters||has||weight matrix
",,,,,
hyperparameters,"Besides the last fully connect layer , all the weight matrices are randomly initialized by a uniform distribution U ( ? 0.01 , 0.01 ) .","Besides
last fully connect layer
all the weight matrices
randomly initialized by
uniform distribution U ( ? 0.01 , 0.01 )","all the weight matrices||randomly initialized by||uniform distribution U ( ? 0.01 , 0.01 )
all the weight matrices||Besides||last fully connect layer
",,,"Hyperparameters||has||all the weight matrices
",,,,,
hyperparameters,"In addition , we add L2-regularization to the last fully connect layer with a weight of 0.01 .","add
L2-regularization
to
last fully connect layer
with
weight
of
0.01","L2-regularization||to||last fully connect layer
last fully connect layer||with||weight
weight||of||0.01
",,"Hyperparameters||add||L2-regularization
",,,,,,
hyperparameters,"During training , we set dropout to 0.5 , the batch size is set to 32 and the optimizer is Adam Optimizer with a learning rate of 0.001 .","During
training
set
dropout
to
0.5
batch size
is
set to
32
optimizer
Adam Optimizer
with
learning rate
of
0.001","training||set||dropout
dropout||to||0.5
optimizer||is||Adam Optimizer
Adam Optimizer||with||learning rate
learning rate||of||0.001
batch size||set to||32
","training||has||optimizer
training||has||batch size
","Hyperparameters||During||training
",,,,,,
hyperparameters,We implement our proposed model using Tensorflow 5 .,"implement
proposed model
using
Tensorflow","proposed model||using||Tensorflow
",,"Hyperparameters||implement||proposed model
",,,,,,
baselines,"TD - LSTM constructs aspect-specific representation by the left context with aspect and the right context with aspect , then employs two LSTMs to model them respectively .","TD - LSTM
constructs
aspect-specific representation
by
left context
with
aspect
right context
with
aspect
employs
two LSTMs","TD - LSTM||employs||two LSTMs
TD - LSTM||constructs||aspect-specific representation
aspect-specific representation||by||right context
right context||with||aspect
aspect-specific representation||by||left context
left context||with||aspect
",,,"Baselines||has||TD - LSTM
",,,,,
baselines,The last hidden states of the two LSTMs are finally concatenated for predicting the sentiment polarity of the aspect .,"last hidden states
of
two LSTMs
finally concatenated
for predicting
sentiment polarity
of
aspect","last hidden states||of||two LSTMs
finally concatenated||for predicting||sentiment polarity
sentiment polarity||of||aspect
","last hidden states||has||finally concatenated
",,,,,,"TD - LSTM||has||last hidden states
",
baselines,"ATAE - LSTM first attaches the aspect embedding to each word embedding to capture aspect - dependent information , and then employs attention mechanism to get the sentence representation for final classification .","ATAE - LSTM
attaches
aspect embedding
to
each word embedding
to capture
aspect - dependent information
employs
attention mechanism
to get
sentence representation
for
final classification","ATAE - LSTM||employs||attention mechanism
attention mechanism||to get||sentence representation
sentence representation||for||final classification
ATAE - LSTM||attaches||aspect embedding
aspect embedding||to||each word embedding
each word embedding||to capture||aspect - dependent information
",,,"Baselines||has||ATAE - LSTM
",,,,,
baselines,Mem Net uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect .,"Mem Net
uses
deep memory network
on
context word embeddings
for
sentence representation
to capture
relevance
between
each context word and the aspect","Mem Net||uses||deep memory network
deep memory network||on||context word embeddings
context word embeddings||for||sentence representation
sentence representation||to capture||relevance
relevance||between||each context word and the aspect
relevance||between||each context word and the aspect
",,,"Baselines||has||Mem Net
",,,,,
baselines,IAN generates the representations for aspect terms and contexts with two attention - based LSTM network separately .,"IAN
generates
representations
for
aspect terms and contexts
with
two attention - based LSTM network","IAN||generates||representations
representations||for||aspect terms and contexts
aspect terms and contexts||with||two attention - based LSTM network
",,,"Baselines||has||IAN
",,,,,
baselines,"RAM [ 10 ] employs a gated recurrent unit network to model a multiple attention mechanism , and captures the relevance between each context word and the aspect .","RAM
employs
gated recurrent unit network
to model
multiple attention mechanism
captures
relevance
between
each context word and the aspect","RAM||captures||relevance
RAM||employs||gated recurrent unit network
gated recurrent unit network||to model||multiple attention mechanism
",,,"Baselines||has||RAM
",,,,,
baselines,PBAN appends the position embedding into each word embedding .,"PBAN
appends
position embedding
into
each word embedding","PBAN||appends||position embedding
position embedding||into||each word embedding
",,,"Baselines||has||PBAN
",,,,,
baselines,TSN is a two - stage framework for aspect - level sentiment analysis .,"TSN
is
two - stage framework
for
aspect - level sentiment analysis","TSN||is||two - stage framework
two - stage framework||for||aspect - level sentiment analysis
",,,"Baselines||has||TSN
",,,,,
baselines,"AEN mainly consists of an embedding layer , an attentional encoder layer , an aspect - specific attention layer , and an output layer .","AEN
consists of
embedding layer
attentional encoder layer
aspect - specific attention layer
output layer","AEN||consists of||embedding layer
AEN||consists of||attentional encoder layer
AEN||consists of||aspect - specific attention layer
AEN||consists of||output layer
",,,"Baselines||has||AEN
",,,,,
baselines,AEN - BERT is AEN with BERT embedding .,"AEN - BERT
is
AEN
with
BERT embedding","AEN - BERT||is||AEN
AEN||with||BERT embedding
",,,"Baselines||has||AEN - BERT
",,,,,
results,"Among all the GloVe - based methods , the TD - LSTM approach performs worst because it takes the aspect information into consideration in a very coarse way .","Among
all the GloVe - based methods
TD - LSTM approach
performs
worst","TD - LSTM approach||performs||worst
","all the GloVe - based methods||has||TD - LSTM approach
","Results||Among||all the GloVe - based methods
",,,,,,
results,"After taking the importance of the aspect into account with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .","After taking
importance
of
aspect
into
account
with
attention mechanism
achieve
stable improvement
comparing to
TD - LSTM","importance||with||attention mechanism
importance||achieve||stable improvement
stable improvement||comparing to||TD - LSTM
importance||of||aspect
aspect||into||account
",,"Results||After taking||importance
",,,,,,
results,"RAM achieves a better performance than other basic attention - based models , because it combines multiple attentions with a recurrent neural network to capture aspect - specific representations .","RAM
achieves
better performance
than
other basic attention - based models","RAM||achieves||better performance
better performance||than||other basic attention - based models
",,,"Results||has||RAM
",,,,,
results,PBAN achieves a similar performance as RAM by employing a position embedding .,"PBAN
achieves
similar performance
as
RAM
by employing
position embedding","PBAN||achieves||similar performance
similar performance||as||RAM
RAM||by employing||position embedding
",,,"Results||has||PBAN
",,,,,
results,"To be specific , PBAN is better than RAM on Restaurant dataset , but worse than RAN on Laptop dataset .","better than
RAM
on
Restaurant dataset
worse than
RAN
on
Laptop dataset","RAN||on||Laptop dataset
RAM||on||Restaurant dataset
",,,,,"PBAN||worse than||RAN
PBAN||better than||RAM
",,,
results,"Compared with RAM and PBAN , the over all performance of TSN is not perform well on both Restaurant dataset and Laptop dataset , which might because the framework of TSN is too simple to model the representations of context and aspect effectively .","Compared with
RAM and PBAN
over all performance
of
TSN
not perform well
on both
Restaurant dataset and Laptop dataset","over all performance||of||TSN
not perform well||on both||Restaurant dataset and Laptop dataset
","RAM and PBAN||has||over all performance
over all performance||has||not perform well
","Results||Compared with||RAM and PBAN
",,,,,,
results,"AEN is slightly better than TSN , but still worse than RAM and PBAN .","AEN
slightly better than
TSN
still worse than
RAM and PBAN","AEN||still worse than||RAM and PBAN
AEN||slightly better than||TSN
",,,"Results||has||AEN
",,,,,
results,"Comparing the results of SDGCN - A w/o position and SDGCN - G w/o position , SDGCN - A and SDGCN - G , respectively , we observe that the GCN built with global - relation is slightly higher than built with adjacent - relation in both accuracy and Macro - F1 measure .","Comparing
results
of
SDGCN - A w/o position and SDGCN - G w/o position
SDGCN - A and SDGCN - G
observe
GCN
built with
global - relation
slightly higher than
built with adjacent - relation
in both
accuracy and Macro - F1 measure","results||of||SDGCN - A w/o position and SDGCN - G w/o position
results||of||SDGCN - A and SDGCN - G
results||observe||GCN
GCN||slightly higher than||built with adjacent - relation
built with adjacent - relation||in both||accuracy and Macro - F1 measure
GCN||built with||global - relation
",,"Results||Comparing||results
",,,,,,
results,"Moreover , the two models ( SDGCN - A and SDGCN - G ) with position information gain a significant improvement compared to the two models without position information .","two models ( SDGCN - A and SDGCN - G )
with
position information
gain
significant improvement
compared to
two models
without
position information","two models ( SDGCN - A and SDGCN - G )||with||position information
position information||gain||significant improvement
significant improvement||compared to||two models
two models||without||position information
",,,"Results||has||two models ( SDGCN - A and SDGCN - G )
",,,,,
results,"Benefits from the power of pre-trained BERT , BERT - based models have shown huge superiority over GloVe - based models .","Benefits from
power
of
pre-trained BERT
BERT - based models
shown
huge superiority
over
GloVe - based models","power||of||pre-trained BERT
BERT - based models||shown||huge superiority
huge superiority||over||GloVe - based models
",,"Results||Benefits from||power
Results||Benefits from||BERT - based models
",,,,,,
results,"Furthermore , compared with AEN - BERT , on the Restaurant dataset , SDGCN - BERT achieves absolute increases of 1.09 % and 1.86 % in accuracy and Macro - F1 measure respectively , and gains absolute increases of 1.42 % and 2.03 % in accuracy and Macro - F1 measure respectively on the Laptop dataset .","compared with
AEN - BERT
on
Restaurant dataset
SDGCN - BERT
achieves
absolute increases
of
1.09 % and 1.86 %
in
accuracy and Macro - F1 measure
gains
absolute increases
of
1.42 % and 2.03 %
in
accuracy and Macro - F1 measure
on
Laptop dataset","AEN - BERT||on||Restaurant dataset
SDGCN - BERT||achieves||absolute increases
absolute increases||in||accuracy and Macro - F1 measure
absolute increases||of||1.09 % and 1.86 %
SDGCN - BERT||gains||absolute increases
absolute increases||in||accuracy and Macro - F1 measure
absolute increases||of||1.42 % and 2.03 %
absolute increases||on||Laptop dataset
",,"Results||compared with||AEN - BERT
Results||compared with||SDGCN - BERT
",,,,,,
research-problem,Attentional Encoder Network for Targeted Sentiment Classification,Targeted Sentiment Classification,,,,,"Contribution||has research problem||Targeted Sentiment Classification
",,,,
research-problem,"Targeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over "" opinion targets "" that explicitly appear in the sentence .",fine - grained sentiment analysis,,,,,"Contribution||has research problem||fine - grained sentiment analysis
",,,,
research-problem,"However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .",fine - grained targeted sentiment classification,,,,,"Contribution||has research problem||fine - grained targeted sentiment classification
",,,,
model,This paper propose an attention based model to solve the problems above .,"propose
attention based model",,,"Model||propose||attention based model
",,,,,,
model,"Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .","our model
eschews
recurrence
employs
attention
as
competitive alternative
to draw
introspective and interactive semantics
between
target and context words","our model||employs||attention
attention||as||competitive alternative
competitive alternative||to draw||introspective and interactive semantics
introspective and interactive semantics||between||target and context words
our model||eschews||recurrence
",,,"Model||has||our model
",,,,,
model,"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .","To deal with
label unreliability issue
employ
label smoothing regularization
to encourage
model
to be
less confident
with
fuzzy labels","label unreliability issue||employ||label smoothing regularization
label smoothing regularization||to encourage||model
model||with||fuzzy labels
model||to be||less confident
",,"Model||To deal with||label unreliability issue
",,,,,,
model,We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .,"apply
pre-trained BERT",,,"Model||apply||pre-trained BERT
",,,,,,
hyperparameters,shows the number of training and test instances in each category .,"shows
number of training and test instances
in
each category","number of training and test instances||in||each category
",,"Hyperparameters||shows||number of training and test instances
",,,,,,
hyperparameters,"Word embeddings in AEN - Glo Ve do not get updated in the learning process , but we fine - tune pre-trained BERT 3 in AEN - BERT .","Word embeddings
in
AEN - Glo Ve
do not get updated in
learning process
fine - tune
pre-trained BERT
in
AEN - BERT","Word embeddings||in||AEN - Glo Ve
AEN - Glo Ve||do not get updated in||learning process
Word embeddings||fine - tune||pre-trained BERT
pre-trained BERT||in||AEN - BERT
",,,"Hyperparameters||has||Word embeddings
",,,,,
hyperparameters,Embedding dimension d dim is 300 for GloVe and is 768 for pretrained BERT .,"Embedding dimension d dim
is
300
for
GloVe
768
for
pretrained BERT","Embedding dimension d dim||is||300
300||for||GloVe
Embedding dimension d dim||is||768
768||for||pretrained BERT
",,,"Hyperparameters||has||Embedding dimension d dim
",,,,,
hyperparameters,Dimension of hidden states d hid is set to 300 .,"Dimension
of
hidden states d hid
set to
300","Dimension||of||hidden states d hid
hidden states d hid||set to||300
",,,"Hyperparameters||has||Dimension
",,,,,
hyperparameters,The weights of our model are initialized with Glorot initialization .,"weights
of
our model
initialized with
Glorot initialization","weights||of||our model
weights||initialized with||Glorot initialization
",,,"Hyperparameters||has||weights
",,,,,
hyperparameters,"During training , we set label smoothing parameter to 0.2 , the coefficient ? of L 2 regularization item is 10 ? 5 and dropout rate is 0.1 .","During
training
set
label smoothing parameter
to
0.2
coefficient ? of L 2 regularization item
is
10 ? 5
dropout rate
is
0.1","training||set||label smoothing parameter
label smoothing parameter||to||0.2
training||set||dropout rate
dropout rate||is||0.1
training||set||coefficient ? of L 2 regularization item
coefficient ? of L 2 regularization item||is||10 ? 5
",,"Hyperparameters||During||training
",,,,,,
hyperparameters,"Adam optimizer ( Kingma and Ba , 2014 ) is applied to update all the parameters .","Adam optimizer ( Kingma and Ba , 2014 )
applied to
update
all the parameters","Adam optimizer ( Kingma and Ba , 2014 )||applied to||update
","update||has||all the parameters
",,"Hyperparameters||has||Adam optimizer ( Kingma and Ba , 2014 )
",,,,,
baselines,We also design a basic BERT - based model to evaluate the performance of AEN - BERT .,"design
basic BERT - based model
to evaluate
performance
of
AEN - BERT","basic BERT - based model||to evaluate||performance
performance||of||AEN - BERT
",,"Baselines||design||basic BERT - based model
",,,,,,
baselines,Non - RNN based baselines :,Non - RNN based baselines,,,,"Baselines||has||Non - RNN based baselines
",,,,,"Non - RNN based baselines||has||Feature - based SVM
"
baselines,Feature - based SVM is a traditional support vector machine based model with extensive feature engineering .,"Feature - based SVM
is
traditional support vector machine based model
with
extensive feature engineering","Feature - based SVM||is||traditional support vector machine based model
traditional support vector machine based model||with||extensive feature engineering
",,,,,,,,
baselines,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .","Rec - NN
uses
rules
to transform
dependency tree
put
opinion target
at
root
learns
sentence representation
toward
target
via
semantic composition
using
Recursive NNs","Rec - NN||learns||sentence representation
sentence representation||toward||target
sentence representation||via||semantic composition
semantic composition||using||Recursive NNs
Rec - NN||uses||rules
rules||to transform||dependency tree
Rec - NN||put||opinion target
opinion target||at||root
",,,,,,,"Non - RNN based baselines||has||Rec - NN
",
baselines,MemNet uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word .,"MemNet
uses
multi-hops of attention layers
on
context word embeddings
for
sentence representation
to explicitly captures
importance
of
each context word","MemNet||uses||multi-hops of attention layers
multi-hops of attention layers||to explicitly captures||importance
importance||of||each context word
multi-hops of attention layers||on||context word embeddings
context word embeddings||for||sentence representation
",,,,,,,"Non - RNN based baselines||has||MemNet
",
baselines,RNN based baselines :,RNN based baselines,,,,"Baselines||has||RNN based baselines
",,,,,"RNN based baselines||has||TD - LSTM
"
baselines,TD - LSTM extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively .,"TD - LSTM
extends
LSTM
by using
two LSTM networks
to model
left context
with
target
right context
with
target","TD - LSTM||extends||LSTM
LSTM||by using||two LSTM networks
LSTM||to model||right context
right context||with||target
LSTM||to model||left context
left context||with||target
",,,,,,,,
baselines,ATAE - LSTM,ATAE - LSTM,,,,,,,,"RNN based baselines||has||ATAE - LSTM
",
baselines,"( Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .","strengthens
effect of target embeddings
which appends
target embeddings
with
each word embeddings
use
LSTM
with
attention
to get
final representation
for
classification","LSTM||with||attention
attention||to get||final representation
final representation||for||classification
effect of target embeddings||which appends||target embeddings
target embeddings||with||each word embeddings
",,,,,"ATAE - LSTM||use||LSTM
ATAE - LSTM||strengthens||effect of target embeddings
",,,
baselines,"IAN learns the representations of the target and context with two LSTMs and attentions interactively , which generates the representations for targets and contexts with respect to each other .","IAN
learns
representations
of
target and context
with
two LSTMs and attentions
which generates
representations
for
targets and contexts
with respect to
each other","IAN||learns||representations
representations||of||target and context
target and context||with||two LSTMs and attentions
target and context||which generates||representations
representations||for||targets and contexts
targets and contexts||with respect to||each other
",,,,,,,"RNN based baselines||has||IAN
",
baselines,RAM strengthens Mem - Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation .,"RAM
strengthens
Mem - Net
by representing
memory
with
bidirectional LSTM
using
gated recurrent unit network
to combine
multiple attention outputs
for
sentence representation","RAM||using||gated recurrent unit network
gated recurrent unit network||to combine||multiple attention outputs
multiple attention outputs||for||sentence representation
RAM||strengthens||Mem - Net
Mem - Net||by representing||memory
memory||with||bidirectional LSTM
",,,,,,,"RNN based baselines||has||RAM
",
baselines,AEN - Glo Ve ablations :,AEN - Glo Ve ablations,,,,"Baselines||has||AEN - Glo Ve ablations
",,,,,"AEN - Glo Ve ablations||has||AEN - GloVe w/ o PCT
"
baselines,AEN - GloVe w/ o PCT ablates PCT module .,"AEN - GloVe w/ o PCT
ablates
PCT module","AEN - GloVe w/ o PCT||ablates||PCT module
",,,,,,,,
baselines,AEN - GloVe w/ o MHA ablates MHA module .,"AEN - GloVe w/ o MHA
ablates
MHA module","AEN - GloVe w/ o MHA||ablates||MHA module
",,,,,,,"AEN - Glo Ve ablations||has||AEN - GloVe w/ o MHA
",
baselines,AEN - GloVe w/ o LSR ablates label smoothing regularization .,"AEN - GloVe w/ o LSR
ablates
label smoothing regularization","AEN - GloVe w/ o LSR||ablates||label smoothing regularization
",,,,,,,"AEN - Glo Ve ablations||has||AEN - GloVe w/ o LSR
",
baselines,AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM .,"AEN-GloVe-BiLSTM
replaces
attentional encoder layer
with
two bidirectional LSTM","AEN-GloVe-BiLSTM||replaces||attentional encoder layer
attentional encoder layer||with||two bidirectional LSTM
",,,,,,,"AEN - Glo Ve ablations||has||AEN-GloVe-BiLSTM
",
baselines,Basic BERT - based model :,Basic BERT - based model,,,,"Baselines||has||Basic BERT - based model
",,,,,"Basic BERT - based model||has||BERT - SPC
"
baselines,"BERT - SPC feeds sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] "" into the basic BERT model for sentence pair classification task .","BERT - SPC
feeds
sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""
into
basic BERT model
for
sentence pair classification task","BERT - SPC||feeds||sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""
sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""||into||basic BERT model
sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] ""||for||sentence pair classification task
",,,,,,,,
results,The over all performance of TD - LSTM is not good since it only makes a rough treatment of the target words .,"over all performance
of
TD - LSTM
is
not good
makes
rough treatment
of
target words","over all performance||of||TD - LSTM
TD - LSTM||makes||rough treatment
rough treatment||of||target words
TD - LSTM||is||not good
",,,"Results||has||over all performance
",,,,,
results,"ATAE - LSTM , IAN and RAM are attention based models , they stably exceed the TD - LSTM method on Restaurant and Laptop datasets .","ATAE - LSTM , IAN and RAM
are
attention based models
stably exceed
TD - LSTM method
on
Restaurant and Laptop datasets","ATAE - LSTM , IAN and RAM||stably exceed||TD - LSTM method
TD - LSTM method||on||Restaurant and Laptop datasets
ATAE - LSTM , IAN and RAM||are||attention based models
",,,"Results||has||ATAE - LSTM , IAN and RAM
",,,,,
results,"RAM is better than other RNN based models , but it does not perform well on Twitter dataset , which might because bidirectional LSTM is not good at modeling small and ungrammatical text .","RAM
better than
other RNN based models
does not perform
well
on
Twitter dataset","RAM||does not perform||well
well||on||Twitter dataset
RAM||better than||other RNN based models
",,,"Results||has||RAM
",,,,,
results,"Feature - based SVM is still a competitive baseline , but relying on manually - designed features .","Feature - based SVM
still
competitive baseline
relying on
manually - designed features","Feature - based SVM||still||competitive baseline
Feature - based SVM||relying on||manually - designed features
",,,"Results||has||Feature - based SVM
",,,,,
results,Rec - NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments .,"Rec - NN
gets
worst performances
among
all neural network baselines","Rec - NN||gets||worst performances
worst performances||among||all neural network baselines
",,,"Results||has||Rec - NN
",,,,,
results,"Like AEN , Mem Net also eschews recurrence , but its over all performance is not good since it does not model the hidden semantic of embeddings , and the result of the last attention is essentially a linear combination of word embeddings .","Like
AEN
Mem Net
eschews
recurrence
over all performance
is
not good","Mem Net||eschews||recurrence
over all performance||is||not good
","AEN||has||Mem Net
Mem Net||has||over all performance
","Results||Like||AEN
",,,,,,
ablation-analysis,"Comparing the results of AEN - GloVe and AEN - Glo Ve w / o LSR , we observe that the accuracy of AEN - Glo Ve w / o LSR drops significantly on all three datasets .","Comparing the results of
AEN - GloVe and AEN - Glo Ve w / o LSR
observe
accuracy
of
AEN - Glo Ve w / o LSR
drops
significantly
on
all three datasets","AEN - GloVe and AEN - Glo Ve w / o LSR||observe||accuracy
accuracy||drops||significantly
significantly||on||all three datasets
accuracy||of||AEN - Glo Ve w / o LSR
",,"Ablation analysis||Comparing the results of||AEN - GloVe and AEN - Glo Ve w / o LSR
",,,,,,
ablation-analysis,"The over all performance of AEN - GloVe and AEN - Glo Ve - BiLSTM is relatively close , AEN - Glo Ve performs better on the Restaurant dataset .","over all performance
of
AEN - GloVe and AEN - Glo Ve - BiLSTM
is
relatively close
AEN - Glo Ve
performs
better
on
Restaurant dataset","over all performance||of||AEN - Glo Ve
AEN - Glo Ve||performs||better
better||on||Restaurant dataset
over all performance||of||AEN - GloVe and AEN - Glo Ve - BiLSTM
AEN - GloVe and AEN - Glo Ve - BiLSTM||is||relatively close
",,,"Ablation analysis||has||over all performance
",,,,,
ablation-analysis,"More importantly , AEN - Glo Ve has fewer parameters and is easier to parallelize .","AEN - Glo Ve
fewer parameters
easier to
parallelize","AEN - Glo Ve||easier to||parallelize
","AEN - Glo Ve||has||fewer parameters
",,"Ablation analysis||has||AEN - Glo Ve
",,,,,
ablation-analysis,"AEN - Glo Ve 's lightweight level ranks second , since it takes some more parameters than MemNet in modeling hidden states of sequences .","AEN - Glo Ve 's lightweight level
ranks
second","AEN - Glo Ve 's lightweight level||ranks||second
",,,"Ablation analysis||has||AEN - Glo Ve 's lightweight level
",,,,,
ablation-analysis,"As a comparison , the model size of AEN - Glo Ve - BiLSTM is more than twice that of AEN - GloVe , but does not bring any performance improvements .","model size
of
AEN - Glo Ve - BiLSTM
more than
twice
of
AEN - GloVe
does not bring
any performance improvements","model size||of||AEN - Glo Ve - BiLSTM
AEN - Glo Ve - BiLSTM||more than||twice
twice||of||AEN - GloVe
AEN - Glo Ve - BiLSTM||does not bring||any performance improvements
",,,"Ablation analysis||has||model size
",,,,,
research-problem,Aspect Level Sentiment Classification with Attention - over - Attention Neural Networks,Aspect Level Sentiment Classification,,,,,"Contribution||has research problem||Aspect Level Sentiment Classification
",,,,
research-problem,Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .,Aspect - level sentiment classification,,,,,"Contribution||has research problem||Aspect - level sentiment classification
",,,,
approach,"Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .","based on
long short - term memory ( LSTM ) neural networks",,,"Approach||based on||long short - term memory ( LSTM ) neural networks
",,,,,,
approach,"Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models aspects and texts simultaneously using LSTMs .","models
aspects and texts simultaneously
using
LSTMs","aspects and texts simultaneously||using||LSTMs
",,"Approach||models||aspects and texts simultaneously
",,,,,,
approach,"Furthermore , the target representation and text representation generated from LSTMs interact with each other by an attention - over - attention ( AOA ) module .","target representation and text representation
generated from
LSTMs
interact with
each other
by
attention - over - attention ( AOA ) module","target representation and text representation||generated from||LSTMs
target representation and text representation||interact with||each other
each other||by||attention - over - attention ( AOA ) module
",,,"Approach||has||target representation and text representation
",,,,,
approach,AOA automatically generates mutual attentions not only from aspect - to - text but also text - to - aspect .,"AOA
automatically generates
mutual attentions
not only from
aspect - to - text
but also
text - to - aspect","AOA||automatically generates||mutual attentions
mutual attentions||but also||text - to - aspect
mutual attentions||not only from||aspect - to - text
",,,"Approach||has||AOA
",,,,,
approach,That is why we choose AOA to attend to the most important parts in both aspect and sentence .,"choose
AOA
to attend
most important parts
in both
aspect and sentence","AOA||to attend||most important parts
most important parts||in both||aspect and sentence
",,"Approach||choose||AOA
",,,,,,
hyperparameters,"In experiments , we first randomly select 20 % of training data as validation set to tune the hyperparameters .","randomly select
20 %
of
training data
as
validation set
to tune
hyperparameters","20 %||of||training data
training data||as||validation set
training data||to tune||hyperparameters
",,"Hyperparameters||randomly select||20 %
",,,,,,
hyperparameters,"All weight matrices are randomly initialized from uniform distribution U ( ?10 ?4 , 10 ?4 ) and all bias terms are set to zero .","weight matrices
randomly initialized from
uniform distribution U ( ?10 ?4 , 10 ?4 )
bias terms
set to
zero","weight matrices||randomly initialized from||uniform distribution U ( ?10 ?4 , 10 ?4 )
bias terms||set to||zero
",,,"Hyperparameters||has||weight matrices
Hyperparameters||has||bias terms
",,,,,
hyperparameters,The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .,"L 2 regularization coefficient
set to
10 ? 4
dropout keep rate
set to
0.2","L 2 regularization coefficient||set to||10 ? 4
dropout keep rate||set to||0.2
",,,"Hyperparameters||has||L 2 regularization coefficient
Hyperparameters||has||dropout keep rate
",,,,,
hyperparameters,The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,"word embeddings
initialized with
300 - dimensional Glove vectors
fixed during
training","word embeddings||fixed during||training
word embeddings||initialized with||300 - dimensional Glove vectors
",,,"Hyperparameters||has||word embeddings
",,,,,
hyperparameters,"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .","For
out of vocabulary words
initialize them
randomly
from
uniform distribution U ( ? 0.01 , 0.01 )","out of vocabulary words||initialize them||randomly
randomly||from||uniform distribution U ( ? 0.01 , 0.01 )
",,"Hyperparameters||For||out of vocabulary words
",,,,,,
hyperparameters,The dimension of LSTM hidden states is set to 150 .,"dimension
of
LSTM hidden states
set to
150","dimension||of||LSTM hidden states
LSTM hidden states||set to||150
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,The initial learning rate is 0.01 for the Adam optimizer .,"initial learning rate
is
0.01
for
Adam optimizer","initial learning rate||is||0.01
0.01||for||Adam optimizer
",,,"Hyperparameters||has||initial learning rate
",,,,,
hyperparameters,"If the training loss does not drop after every three epochs , we decrease the learning rate by half .","training loss
does not
drop
after
every three epochs
decrease
learning rate
by
half","training loss||does not||drop
drop||after||every three epochs
drop||decrease||learning rate
learning rate||by||half
",,,"Hyperparameters||has||training loss
",,,,,
hyperparameters,The batch size is set as 25 .,"batch size
set as
25","batch size||set as||25
",,,"Hyperparameters||has||batch size
",,,,,
baselines,"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .","Majority
is
basic baseline method
assigns
largest sentiment polarity
in
training set
to
each sample
in
test set","Majority||is||basic baseline method
Majority||assigns||largest sentiment polarity
largest sentiment polarity||in||training set
training set||to||each sample
each sample||in||test set
",,,"Baselines||has||Majority
",,,,,
baselines,"LSTM uses one LSTM network to model the sentence , and the last hidden state is used as the sentence representation for the final classification .","LSTM
uses
one LSTM network
to model
sentence
last hidden state
used as
sentence representation
for
final classification","LSTM||uses||one LSTM network
one LSTM network||to model||sentence
last hidden state||used as||sentence representation
sentence representation||for||final classification
","LSTM||has||last hidden state
",,"Baselines||has||LSTM
",,,,,
baselines,TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,"TD - LSTM
uses
two LSTM networks
to model
preceding and following contexts
surrounding
aspect term","TD - LSTM||uses||two LSTM networks
two LSTM networks||to model||preceding and following contexts
preceding and following contexts||surrounding||aspect term
",,,"Baselines||has||TD - LSTM
",,,,,
baselines,AT - LSTM first models the sentence via a LSTM model .,"AT - LSTM
first models
sentence
via
LSTM model","AT - LSTM||first models||sentence
sentence||via||LSTM model
",,,"Baselines||has||AT - LSTM
",,,,,
baselines,ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,"ATAE - LSTM
further extends
AT - LSTM
by appending
aspect embedding
into
each word vector","ATAE - LSTM||further extends||AT - LSTM
AT - LSTM||by appending||aspect embedding
aspect embedding||into||each word vector
",,,"Baselines||has||ATAE - LSTM
",,,,,
baselines,IAN uses two LSTM networks to model the sentence and aspect term respectively .,"IAN
uses
two LSTM networks
to model
sentence and aspect term","IAN||uses||two LSTM networks
two LSTM networks||to model||sentence and aspect term
",,,"Baselines||has||IAN
",,,,,
results,"In our implementation , we found that the performance fluctuates with different random initialization , which is a well - known issue in training neural networks .","found that
performance fluctuates
with
different random initialization","performance fluctuates||with||different random initialization
",,"Results||found that||performance fluctuates
",,,,,,
results,"On average , our algorithm is better than these baseline methods and our best trained model outperforms them in a large margin .","On average
our algorithm
better than
baseline methods
our best trained model
outperforms them in
large margin","our best trained model||outperforms them in||large margin
our algorithm||better than||baseline methods
",,"Results||On average||our best trained model
Results||On average||our algorithm
",,,,,,
research-problem,Knowledge - Enriched Transformer for Emotion Detection in Textual Conversations,Emotion Detection in Textual Conversations,,,,,"Contribution||has research problem||Emotion Detection in Textual Conversations
",,,,
research-problem,The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks .,detecting emotions in textual conversations,,,,,"Contribution||has research problem||detecting emotions in textual conversations
",,,,
research-problem,"This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context .","detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations",,,,,"Contribution||has research problem||detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations
",,,,
model,"To this end , we propose a Knowledge - Enriched Transformer ( KET ) to effectively incorporate contextual information and external knowledge bases to address the aforementioned challenges .","propose
Knowledge - Enriched Transformer ( KET )
incorporate
contextual information and external knowledge bases","Knowledge - Enriched Transformer ( KET )||incorporate||contextual information and external knowledge bases
",,"Model||propose||Knowledge - Enriched Transformer ( KET )
",,,,,,
,"The self - attention and cross-attention modules in the Transformer capture the intra-sentence and inter-sentence correlations , respectively .","self - attention and cross-attention modules
Transformer
capture
intra-sentence and inter-sentence correlations",,,,,,,,,
,The shorter path of information flow in these two modules compared to gated RNNs and CNNs allows KET to model contextual information more efficiently .,"shorter path
of
information flow
allows
KET
to model
contextual information more efficiently",,,,,,,,,
model,"In addition , we propose a hierarchical self - attention mechanism allowing KET to model the hierarchical structure of conversations .","hierarchical self - attention mechanism
allowing
KET
to model
hierarchical structure of conversations","hierarchical self - attention mechanism||allowing||KET
KET||to model||hierarchical structure of conversations
",,,"Model||propose||hierarchical self - attention mechanism
",,,,,
model,"Our model separates context and response into the encoder and decoder , respectively , which is different from other Transformer - based models , e.g. , BERT , which directly concatenate context and response , and then train language models using only the encoder part .","separates
context and response
into
encoder and decoder","context and response||into||encoder and decoder
",,"Model||separates||context and response
",,,,,,
model,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word in the utterances by referring to related knowledge entities .","exploit
commonsense knowledge
leverage
external knowledge bases
to facilitate
understanding of each word
in
utterances
by referring to
related knowledge entities","commonsense knowledge||leverage||external knowledge bases
external knowledge bases||to facilitate||understanding of each word
understanding of each word||in||utterances
understanding of each word||by referring to||related knowledge entities
",,"Model||exploit||commonsense knowledge
",,,,,,
model,The referring process is dynamic and balances between relatedness and affectiveness of the retrieved knowledge entities using a context - aware affective graph attention mechanism .,"referring process
is
dynamic
balances between
relatedness and affectiveness
of
retrieved knowledge entities
using
context - aware affective graph attention mechanism","referring process||balances between||relatedness and affectiveness
relatedness and affectiveness||of||retrieved knowledge entities
retrieved knowledge entities||using||context - aware affective graph attention mechanism
referring process||is||dynamic
",,,"Model||has||referring process
",,,,,
,c LSTM : A contextual LSTM model .,"c LSTM
contextual LSTM model",,,,,,,,,
baselines,An utterance - level bidirectional LSTM is used to encode each utterance .,"utterance - level bidirectional LSTM
to encode
each utterance","utterance - level bidirectional LSTM||to encode||each utterance
",,,,,,,"c LSTM||has||utterance - level bidirectional LSTM
",
baselines,A context - level unidirectional LSTM is used to encode the context .,"context - level unidirectional LSTM
to encode
context","context - level unidirectional LSTM||to encode||context
",,,,,,,"c LSTM||has||context - level unidirectional LSTM
",
baselines,CNN+cLSTM : An CNN is used to extract utterance features .,"CNN+cLSTM
CNN
to extract
utterance features","CNN||to extract||utterance features
","CNN+cLSTM||has||CNN
",,"Baselines||has||CNN+cLSTM
",,,,,"CNN+cLSTM||has||c LSTM
"
baselines,An c LSTM is then applied to learn context representations .,"c LSTM
applied to learn
context representations","c LSTM||applied to learn||context representations
",,,,,,,,
baselines,BERT BASE :,BERT BASE,,,,"Baselines||has||BERT BASE
",,,,,
baselines,We treat each utterance with its context as a single document .,"treat
each utterance
with
context
as
single document","each utterance||with||context
context||as||single document
",,,,,"BERT BASE||treat||each utterance
",,,
baselines,We limit the document length to the last 100 tokens to allow larger batch size .,"limit
document length
to
last 100 tokens
to allow
larger batch size","document length||to||last 100 tokens
last 100 tokens||to allow||larger batch size
",,,,,"BERT BASE||limit||document length
",,,
baselines,DialogueRNN : The stateof - the - art model for emotion detection in textual conversations .,"DialogueRNN
stateof - the - art model
for
emotion detection
in
textual conversations","stateof - the - art model||for||emotion detection
emotion detection||in||textual conversations
","DialogueRNN||has||stateof - the - art model
",,"Baselines||has||DialogueRNN
",,,,,
baselines,It models both context and speakers information .,"models both
context and speakers information",,,,,,"DialogueRNN||models both||context and speakers information
",,,
baselines,KET SingleSelfAttn :,KET SingleSelfAttn,,,,"Baselines||has||KET SingleSelfAttn
",,,,,
baselines,We replace the hierarchical self - attention by a single self - attention layer to learn context representations .,"replace
hierarchical self - attention
by
single self - attention layer
to learn
context representations","hierarchical self - attention||by||single self - attention layer
single self - attention layer||to learn||context representations
",,,,,"KET SingleSelfAttn||replace||hierarchical self - attention
",,,
baselines,Contextual utterances are concatenated together prior to the single self - attention layer .,"Contextual utterances
are
concatenated together
prior to
single self - attention layer","Contextual utterances||are||concatenated together
concatenated together||prior to||single self - attention layer
",,,,,,,"KET SingleSelfAttn||has||Contextual utterances
",
baselines,KET StdAttn :,KET StdAttn,,,,"Baselines||has||KET StdAttn
",,,,,
baselines,We replace the dynamic contextaware affective graph attention by the standard graph attention .,"replace
dynamic contextaware affective graph attention
by
standard graph attention","dynamic contextaware affective graph attention||by||standard graph attention
",,,,,"KET StdAttn||replace||dynamic contextaware affective graph attention
",,,
experimental-setup,We preprocessed all datasets by lower - casing and tokenization using Spacy 2 .,"preprocessed
all datasets
by
lower - casing
tokenization
using
Spacy","all datasets||by||lower - casing
all datasets||by||tokenization
tokenization||using||Spacy
",,"Experimental setup||preprocessed||all datasets
",,,,,,
experimental-setup,We use the released code for BERT BASE and DialogueRNN .,"use
released code
for
BERT BASE and DialogueRNN","released code||for||BERT BASE and DialogueRNN
",,"Experimental setup||use||released code
",,,,,,
experimental-setup,"For each dataset , all models are fine - tuned based on their performance on the validation set .","For
each dataset
all models
are
fine - tuned
based on
performance
on
validation set","all models||are||fine - tuned
fine - tuned||based on||performance
performance||on||validation set
","each dataset||has||all models
","Experimental setup||For||each dataset
",,,,,,
experimental-setup,"For our model in all datasets , we use Adam optimization ( Kingma and Ba , 2014 ) with a batch size of 64 and learning rate of 0.0001 throughout the training process .","our model
in
all datasets
use
Adam optimization ( Kingma and Ba , 2014 )
with
batch size
of
64
learning rate
of
0.0001","our model||in||all datasets
all datasets||use||Adam optimization ( Kingma and Ba , 2014 )
Adam optimization ( Kingma and Ba , 2014 )||with||learning rate
learning rate||of||0.0001
Adam optimization ( Kingma and Ba , 2014 )||with||batch size
batch size||of||64
",,,"Experimental setup||For||our model
",,,,,
experimental-setup,We use Glo Ve embedding for initialization in the word and concept embedding layers,"Glo Ve embedding
for
initialization
in
word and concept embedding layers","Glo Ve embedding||for||initialization
initialization||in||word and concept embedding layers
",,,"Experimental setup||use||Glo Ve embedding
",,,,,
experimental-setup,"For the class weights in cross - entropy loss for each dataset , we set them as the ratio of the class distribution in the validation set to the class distribution in the training set .","class weights
in
cross - entropy loss
for
each dataset
set them as
ratio
of
class distribution
in
validation set
to
class distribution
in
training set","class weights||set them as||ratio
ratio||of||class distribution
class distribution||in||validation set
class distribution||to||class distribution
class distribution||in||training set
class weights||in||cross - entropy loss
cross - entropy loss||for||each dataset
",,,"Experimental setup||For||class weights
",,,,,
results,"c LSTM performs reasonably well on short conversations ( i.e. , EC and DailyDialog ) , but the worst on long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP ) .","c LSTM
performs
reasonably well
on
short conversations ( i.e. , EC and DailyDialog )
worst
on
long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP )","c LSTM||performs||worst
worst||on||long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP )
c LSTM||performs||reasonably well
reasonably well||on||short conversations ( i.e. , EC and DailyDialog )
",,,"Results||has||c LSTM
",,,,,
results,"In contrast , when the utterance - level LSTM in c LSTM is replaced by features extracted by CNN , i.e. , the CNN + c LSTM , the model performs significantly better than c LSTM on long conversations , which further validates that modelling long conversations using only RNN models may not be sufficient .","when
utterance - level LSTM
in
c LSTM
replaced by
features
extracted by
CNN
c LSTM
model
performs
significantly better
than
on
long conversations","utterance - level LSTM||in||c LSTM
c LSTM||replaced by||features
features||extracted by||CNN
model||performs||significantly better
significantly better||than||c LSTM
c LSTM||on||long conversations
","features||has||model
","Results||when||utterance - level LSTM
",,,,,,
results,BERT BASE achieves very competitive performance on all datasets except EC due to its strong representational power via bi-directional context modelling using the Transformer .,"BERT BASE
achieves
very competitive performance
on
all datasets
except
EC
due to
strong representational power
via
bi-directional context modelling
using
Transformer","BERT BASE||achieves||very competitive performance
very competitive performance||on||all datasets
all datasets||except||EC
EC||due to||strong representational power
strong representational power||via||bi-directional context modelling
bi-directional context modelling||using||Transformer
all datasets||except||EC
",,,"Results||has||BERT BASE
",,,,,
results,"In particular , DialogueRNN performs better than our model on IEMOCAP , which maybe attributed to its detailed speaker information for modelling the emotion dynamics in each speaker as the conversation flows .","DialogueRNN
performs better than
our model
on
IEMOCAP","DialogueRNN||performs better than||our model
our model||on||IEMOCAP
",,,"Results||has||DialogueRNN
",,,,,
results,"This finding indicates that our model is robust across datasets with varying training sizes , context lengths and domains .","indicates that
our model
is
robust
across
datasets
with
varying training sizes , context lengths and domains","our model||is||robust
robust||across||datasets
datasets||with||varying training sizes , context lengths and domains
",,"Results||indicates that||our model
",,,,,,
results,Our KET variants KET SingleSelfAttn and KET StdAttn perform comparably with the best baselines on all datasets except IEMOCAP .,"KET variants
KET SingleSelfAttn and KET StdAttn
perform
comparably
with
best baselines
on
all datasets
except
IEMOCAP","KET variants||perform||comparably
comparably||with||best baselines
best baselines||on||all datasets
all datasets||except||IEMOCAP
","KET variants||name||KET SingleSelfAttn and KET StdAttn
",,"Results||has||KET variants
",,,,,
results,"However , both variants perform noticeably worse than KET on all datasets except EC , validating the importance of our proposed hierarchical self - attention and dynamic context - aware affective graph attention mechanism .","noticeably worse
than
KET
on
all datasets
except
EC","noticeably worse||than||KET
KET||on||all datasets
",,,,,,,"KET variants||perform||noticeably worse
",
results,One observation worth mentioning is that these two variants perform on a par with the KET model on EC .,"on a par
with
KET model
on
EC","on a par||with||KET model
KET model||on||EC
",,,,,,,"KET variants||perform||on a par
",
ablation-analysis,It is clear that both context and knowledge are essential to the strong performance of KET on all datasets .,"both context and knowledge
essential to
strong performance
of
KET
on
all datasets","both context and knowledge||essential to||strong performance
strong performance||of||KET
KET||on||all datasets
",,,"Ablation analysis||has||both context and knowledge
",,,,,
ablation-analysis,"Note that removing context has a greater impact on long conversations than short conversations , which is expected because more contextual information is lost in long conversations .","removing
context
greater impact
on
long conversations
than
short conversations","greater impact||on||long conversations
long conversations||than||short conversations
","context||has||greater impact
","Ablation analysis||removing||context
",,,,,,
research-problem,Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment Analysis,Targeted Aspect - based Sentiment Analysis,,,,,"Contribution||has research problem||Targeted Aspect - based Sentiment Analysis
",,,,
research-problem,"While neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task .",targeted aspect - based sentiment analysis ( TABSA ),,,,,"Contribution||has research problem||targeted aspect - based sentiment analysis ( TABSA )
",,,,
model,"In this work , we propose a novel model architecture for TABSA , augmented with multiple "" memory chains "" , and equipped with a delayed memory update mechanism , to keep track of numerous entities independently .","propose
novel model architecture
for
TABSA
augmented with
multiple "" memory chains ""
equipped with
delayed memory update mechanism
to keep track of
numerous entities independently","novel model architecture||augmented with||multiple "" memory chains ""
novel model architecture||for||TABSA
delayed memory update mechanism||to keep track of||numerous entities independently
",,"Model||propose||novel model architecture
Model||equipped with||delayed memory update mechanism
",,,,,,
hyperparameters,"We initialise our model with GloVe ( 300 - D , trained on 42B tokens , 1.9 M vocab , not updated during training : ) 4 and pre-process the corpus with tokenisation using NLTK ) and case folding .","initialise
our model
with
GloVe
300 - D
trained on
42B tokens , 1.9 M vocab
not updated during
training
pre-process
corpus
with
tokenisation
using
NLTK
case folding","corpus||with||tokenisation
tokenisation||using||NLTK
tokenisation||using||case folding
our model||with||GloVe
GloVe||not updated during||training
GloVe||trained on||42B tokens , 1.9 M vocab
","GloVe||has||300 - D
","Hyperparameters||pre-process||corpus
Hyperparameters||initialise||our model
",,,,,,
hyperparameters,Training is carried out over 800 epochs with the FTRL optimiser and a batch size of 128 and learning rate of 0.05 .,"Training
carried out over
800 epochs
with
FTRL optimiser
batch size
of
128
learning rate
of
0.05","Training||with||FTRL optimiser
Training||carried out over||800 epochs
learning rate||of||0.05
batch size||of||128
",,,"Hyperparameters||has||Training
Hyperparameters||has||learning rate
Hyperparameters||has||batch size
",,,,,
hyperparameters,"We use the following hyper - parameters for weight matrices in both directions : R ? R 3003 , H , U , V , Ware all matrices of size R 300300 , v ? R 300 , and hidden size of the GRU in Equation is 300 .","use
following hyper - parameters
for
weight matrices
in
both directions
of
300
hidden size
GRU
is","following hyper - parameters||for||weight matrices
weight matrices||in||both directions
following hyper - parameters||for||hidden size
hidden size||of||GRU
GRU||is||300
",,"Hyperparameters||use||following hyper - parameters
",,,,,,
hyperparameters,Dropout is applied to the output of ? in the final classifier ( Equation ) with a rate of 0.2 .,"Dropout
applied to
output
of
in
final classifier
with
rate
0.2","Dropout||with||rate
rate||of||0.2
Dropout||applied to||output
output||in||final classifier
",,,"Hyperparameters||has||Dropout
",,,,,
hyperparameters,"Lastly , to curb overfitting , we regularise the last layer ( Equation ) with an L 2 penalty on its weights : ?","to curb
overfitting
regularise
last layer
with
L 2 penalty
on
weights","overfitting||regularise||last layer
last layer||with||L 2 penalty
L 2 penalty||on||weights
",,"Hyperparameters||to curb||overfitting
",,,,,,
hyperparameters,"We empirically set the number of memory chains to 6 , with the keys of two of them set to the same embeddings as the target words LOC1 and LOC2 , resp. , and the other 4 chains with free key embeddings which are updated during training , and therefore free to capture any entities .","empirically set
number of memory chains
to
6
with
keys of two of them
set to
same embeddings
as
target words LOC1 and LOC2","number of memory chains||with||keys of two of them
keys of two of them||set to||same embeddings
same embeddings||as||target words LOC1 and LOC2
number of memory chains||to||6
",,"Hyperparameters||empirically set||number of memory chains
",,,,,,
results,Our model achieves state - of - the - art results for both aspect detection and sentiment classification .,"Our model
achieves
state - of - the - art results
for both
aspect detection
sentiment classification","Our model||achieves||state - of - the - art results
state - of - the - art results||for both||aspect detection
state - of - the - art results||for both||sentiment classification
",,,"Results||has||Our model
",,,,,
results,"It is impressive that the proposed model , equipped only with domainindependent general - purpose GloVe embeddings , outperforms Sentic LSTM , an approach heavily reliant on external knowledge bases and domainspecific embeddings .","proposed model
equipped only with
domainindependent general - purpose GloVe embeddings
outperforms
Sentic LSTM
heavily reliant on
external knowledge bases and domainspecific embeddings","proposed model||equipped only with||domainindependent general - purpose GloVe embeddings
proposed model||outperforms||Sentic LSTM
Sentic LSTM||heavily reliant on||external knowledge bases and domainspecific embeddings
",,,"Results||has||proposed model
",,,,,
results,Ent Net vs. our model .,Ent Net vs. our model,,,,"Results||has||Ent Net vs. our model
",,,,,
results,"We see consistent performance gains for our model in both aspect detection and sentiment classification , compared to EntNet , esp. for aspect detection , underlining the benefit of delayed update gate activation .","see
consistent performance gains
for
our model
in both
aspect detection and sentiment classification
compared to
EntNet","consistent performance gains||for||our model
our model||in both||aspect detection and sentiment classification
our model||compared to||EntNet
",,,,,"Ent Net vs. our model||see||consistent performance gains
",,,
research-problem,Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,Aspect Level Sentiment Classification,,,,,"Contribution||has research problem||Aspect Level Sentiment Classification
",,,,
research-problem,Continuous growing of user generated text in social media platforms such as Twitter drives sentiment classification increasingly popular .,sentiment classification,,,,,"Contribution||has research problem||sentiment classification
",,,,
research-problem,"Differing from general sentiment classification , aspect level sentiment classification identifies opinions from text about specific entities and their aspects .",general sentiment classification,,,,,"Contribution||has research problem||general sentiment classification
",,,,
model,"In the present work , we propose two simple yet effective convolutional neural networks with aspect information incorporated .","propose
two simple yet effective convolutional neural networks
with
aspect information","two simple yet effective convolutional neural networks||with||aspect information
",,"Model||propose||two simple yet effective convolutional neural networks
",,,,,,
model,"Specifically , we design two novel neural units that take target aspects into account .","design
two novel neural units
take
target aspects
into
account","two novel neural units||take||target aspects
target aspects||into||account
",,"Model||design||two novel neural units
",,,,,,
model,"One is parameterized filter , the other is parameterized gate .","One is
parameterized filter
other is
parameterized gate",,,,,,"two novel neural units||other is||parameterized gate
two novel neural units||One is||parameterized filter
",,,
model,These units both are generated from aspect - specific features and are further applied on the sentence .,"units
generated from
aspect - specific features","units||generated from||aspect - specific features
",,,"Model||has||units
",,,,,
hyperparameters,"We use rectifier as non-linear function f in the CNN g , CNN t and sigmoid in the CNN s , filter window sizes of 1 , 2 , 3 , 4 with 100 feature maps each , l 2 regularization term of 0.001 and minibatch size of 25 .","use
rectifier
as
non-linear function f
in
CNN g , CNN t
sigmoid
in
CNN s
filter window sizes
of
1 , 2 , 3 , 4 with 100 feature maps each
l 2 regularization term
of
0.001
minibatch size
of
25","minibatch size||of||25
rectifier||as||non-linear function f
non-linear function f||in||CNN g , CNN t
rectifier||as||sigmoid
sigmoid||in||CNN s
l 2 regularization term||of||0.001
filter window sizes||of||1 , 2 , 3 , 4 with 100 feature maps each
",,"Hyperparameters||use||minibatch size
Hyperparameters||use||rectifier
Hyperparameters||use||l 2 regularization term
Hyperparameters||use||filter window sizes
",,,,,,
hyperparameters,Parameterized filters and gates have the same size and number as normal filters .,"Parameterized filters and gates
have
same size and number
as
normal filters","Parameterized filters and gates||have||same size and number
same size and number||as||normal filters
",,,"Hyperparameters||has||Parameterized filters and gates
",,,,,
hyperparameters,"They are generated uniformly by CNN with window sizes of 1 , 2 , 3 , 4 , eg. among 100 parameterized filters with size 3 , 25 of them are generated by aspect CNN with filter size 1 , 2 , 3 , 4 respectively .","generated uniformly by
CNN
with
window sizes
of
1 , 2 , 3 , 4","CNN||with||window sizes
window sizes||of||1 , 2 , 3 , 4
",,,,,"Parameterized filters and gates||generated uniformly by||CNN
",,,
hyperparameters,The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,"word embeddings
initialized with
300 - dimensional Glove vectors
fixed during
training","word embeddings||initialized with||300 - dimensional Glove vectors
300 - dimensional Glove vectors||fixed during||training
",,,"Hyperparameters||has||word embeddings
",,,,,
hyperparameters,"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .","For
out of vocabulary words
initialize them
randomly
from
uniform distribution U ( ? 0.01 , 0.01 )","out of vocabulary words||initialize them||randomly
out of vocabulary words||from||uniform distribution U ( ? 0.01 , 0.01 )
",,"Hyperparameters||For||out of vocabulary words
",,,,,,
hyperparameters,We apply dropout on the final classification features of PG - CNN .,"apply
dropout
on
final classification features
of
PG - CNN","dropout||on||final classification features
final classification features||of||PG - CNN
",,"Hyperparameters||apply||dropout
",,,,,,
hyperparameters,The dropout rate is chosen as 0.3 .,"dropout rate
chosen as
0.3","dropout rate||chosen as||0.3
",,,"Hyperparameters||has||dropout rate
",,,,,
hyperparameters,Training is done through mini-batch stochastic gradient descent with Adam update rule .,"Training
done through
mini-batch stochastic gradient descent
with
Adam update rule","Training||done through||mini-batch stochastic gradient descent
mini-batch stochastic gradient descent||with||Adam update rule
",,,"Hyperparameters||has||Training
",,,,,
hyperparameters,The initial learning rate is 0.001 .,"initial learning rate
is
0.001","initial learning rate||is||0.001
",,,"Hyperparameters||has||initial learning rate
",,,,,
hyperparameters,"If the training loss does not drop after every three epochs , we decrease the learning rate by half .","training loss
does not
drop
after
every three epochs
decrease
learning rate
by
half","training loss||does not||drop
drop||after||every three epochs
every three epochs||decrease||learning rate
learning rate||by||half
",,,"Hyperparameters||has||training loss
",,,,,
hyperparameters,We adopt early stopping based on the validation loss on development sets .,"adopt
early stopping
based on
validation loss
on
development sets","early stopping||based on||validation loss
validation loss||on||development sets
",,"Hyperparameters||adopt||early stopping
",,,,,,
baselines,TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,"TD - LSTM
uses
two LSTM networks
to model
preceding and following contexts
surrounding
aspect term","TD - LSTM||uses||two LSTM networks
two LSTM networks||to model||preceding and following contexts
preceding and following contexts||surrounding||aspect term
",,,"Baselines||has||TD - LSTM
",,,,,
baselines,AT - LSTM combines the sentence hidden states from a LSTM with the aspect term embedding to generate the attention vector .,"AT - LSTM
combines
sentence hidden states
from
LSTM
with
aspect term embedding
to generate
attention vector","AT - LSTM||combines||sentence hidden states
sentence hidden states||with||aspect term embedding
aspect term embedding||to generate||attention vector
sentence hidden states||from||LSTM
",,,"Baselines||has||AT - LSTM
",,,,,
baselines,ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,"ATAE - LSTM
extends
AT - LSTM
by appending
aspect embedding
into
each word vector","ATAE - LSTM||extends||AT - LSTM
AT - LSTM||by appending||aspect embedding
aspect embedding||into||each word vector
",,,"Baselines||has||ATAE - LSTM
",,,,,
baselines,AF - LSTM introduces a word - aspect fusion attention to learn associative relationships between aspect and context words .,"AF - LSTM
introduces
word - aspect fusion attention
to learn
associative relationships
between
aspect and context words","AF - LSTM||introduces||word - aspect fusion attention
word - aspect fusion attention||to learn||associative relationships
associative relationships||between||aspect and context words
",,,"Baselines||has||AF - LSTM
",,,,,
baselines,CNN uses the architecture proposed in without explicitly considering aspect .,"CNN
uses
architecture
without explicitly considering
aspect","CNN||uses||architecture
architecture||without explicitly considering||aspect
",,,"Baselines||has||CNN
",,,,,
results,"Our two models achieve the best performance when compared to these baselines as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .","Our two models
achieve
best performance
compared to
baselines","Our two models||achieve||best performance
best performance||compared to||baselines
",,,"Results||has||Our two models
",,,,,
results,"Compared to one recently proposed model AF - LSTM , our method achieve 2 % - 5 % improvements .","Compared to
recently proposed model AF - LSTM
our method
achieve
2 % - 5 % improvements","our method||achieve||2 % - 5 % improvements
","recently proposed model AF - LSTM||has||our method
","Results||Compared to||recently proposed model AF - LSTM
",,,,,,
results,"Surprisingly , a vanilla CNN works quite well on this problem .","vanilla CNN
works
quite well","vanilla CNN||works||quite well
",,,"Results||has||vanilla CNN
",,,,,
results,"It even beats these welldesigned LSTM models , which further proves that using CNN - based methods is a direction worth exploring .","beats
welldesigned LSTM models",,,,,,"vanilla CNN||beats||welldesigned LSTM models
",,,
research-problem,Interactive Attention Networks for Aspect - Level Sentiment Classification,Aspect - Level Sentiment Classification,,,,,"Contribution||has research problem||Aspect - Level Sentiment Classification
",,,,
research-problem,Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .,sentiment classification,,,,,"Contribution||has research problem||sentiment classification
",,,,
model,"Based on the two points analyzed above , we propose an interactive attention network ( IAN ) model which is based on long - short term memory networks ( LSTM ) and attention mechanism .","propose
interactive attention network ( IAN ) model
based on
long - short term memory networks ( LSTM )
attention mechanism","interactive attention network ( IAN ) model||based on||long - short term memory networks ( LSTM )
interactive attention network ( IAN ) model||based on||attention mechanism
",,"Model||propose||interactive attention network ( IAN ) model
",,,,,,
model,IAN utilizes the attention mechanism associated with a target to get important information from the context and compute context representation for sentiment classification .,"IAN
utilizes
attention mechanism
associated with
target
to get
important information
from
context
compute
context representation
for
sentiment classification","IAN||compute||context representation
context representation||for||sentiment classification
IAN||utilizes||attention mechanism
attention mechanism||associated with||target
attention mechanism||to get||important information
important information||from||context
",,,"Model||has||IAN
",,,,,
model,"Further , IAN makes use of the interactive information from context to supervise the modeling of the target which is helpful to judging sentiment .","makes use of
interactive information
from
context
to supervise
modeling
of
target","interactive information||to supervise||modeling
modeling||of||target
interactive information||from||context
",,,,,"IAN||makes use of||interactive information
",,,
model,"Finally , with both target representation and context representation concatenated , IAN predicts the sentiment polarity for the target within its context .","with
both target representation and context representation concatenated
predicts
sentiment polarity
for
target
within
its context","sentiment polarity||with||both target representation and context representation concatenated
sentiment polarity||for||target
target||within||its context
",,,,,"IAN||predicts||sentiment polarity
",,,
hyperparameters,"In our experiments , all word embeddings from context and target are initialized by GloVe 2 , and all out - of - vocabulary words are initialized by sampling from the uniform distribution U ( ?0.1 , 0.1 ) .","all word embeddings
from
context and target
initialized by
GloVe
all out - of - vocabulary words
initialized by
sampling
from
uniform distribution U ( ?0.1 , 0.1 )","all word embeddings||from||context and target
all word embeddings||initialized by||GloVe
all out - of - vocabulary words||initialized by||sampling
sampling||from||uniform distribution U ( ?0.1 , 0.1 )
",,,"Hyperparameters||has||all word embeddings
Hyperparameters||has||all out - of - vocabulary words
",,,,,
hyperparameters,"All weight matrices are given their initial values by sampling from uniform distribution U ( ?0.1 , 0.1 ) , and all biases are set to zeros .","weight matrices
given their
initial values
sampling from
uniform distribution U ( ?0.1 , 0.1 )
biases
set to
zeros","biases||set to||zeros
weight matrices||given their||initial values
initial values||sampling from||uniform distribution U ( ?0.1 , 0.1 )
",,,"Hyperparameters||has||biases
Hyperparameters||has||weight matrices
",,,,,
hyperparameters,"The dimensions of word embeddings , attention vectors and LSTM hidden states are set to 300 as in .","dimensions
of
word embeddings , attention vectors and LSTM hidden states
set to
300","dimensions||of||word embeddings , attention vectors and LSTM hidden states
word embeddings , attention vectors and LSTM hidden states||set to||300
",,,"Hyperparameters||has||dimensions
",,,,,
hyperparameters,"To train the parameters of IAN , we employ the Momentum , which adds a fraction ? of the update vector in the prior step to the current update vector .","train
parameters
of
IAN
employ
Momentum
adds
fraction
of
update vector
in
prior step
to
current update vector","parameters||employ||Momentum
Momentum||adds||fraction
fraction||of||update vector
update vector||in||prior step
update vector||to||current update vector
parameters||of||IAN
",,"Hyperparameters||train||parameters
",,,,,,
hyperparameters,"The coefficient of L 2 normalization in the objective function is set to 10 ?5 , and the dropout rate is set to 0.5 .","coefficient
of
L 2 normalization
in
objective function
set to
10 ?5
dropout rate
set to
0.5","coefficient||of||L 2 normalization
L 2 normalization||in||objective function
L 2 normalization||set to||10 ?5
dropout rate||set to||0.5
",,,"Hyperparameters||has||coefficient
Hyperparameters||has||dropout rate
",,,,,
baselines,"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .","Majority
is
basic baseline method
assigns
largest sentiment polarity
in
training set
to
each sample
in
test set","Majority||is||basic baseline method
Majority||assigns||largest sentiment polarity
largest sentiment polarity||in||training set
training set||to||each sample
each sample||in||test set
",,,"Baselines||has||Majority
",,,,,
baselines,LSTM only uses one LSTM network to model the context and get the hidden state of each word .,"LSTM
uses
one LSTM network
to model
context
get
hidden state
of
each word","LSTM||get||hidden state
hidden state||of||each word
LSTM||uses||one LSTM network
one LSTM network||to model||context
",,,"Baselines||has||LSTM
",,,,,
baselines,TD - LSTM adopts two long short - term memory ( LSTM ) networks to model the left context with target and the right context with target respectively .,"TD - LSTM
adopts
two long short - term memory ( LSTM ) networks
to model
left context
with
target
right context
with
target","TD - LSTM||adopts||two long short - term memory ( LSTM ) networks
two long short - term memory ( LSTM ) networks||to model||right context
right context||with||target
two long short - term memory ( LSTM ) networks||to model||left context
left context||with||target
",,,"Baselines||has||TD - LSTM
",,,,,
baselines,AE - LSTM represents targets with aspect embeddings .,"AE - LSTM
represents
targets
with
aspect embeddings","AE - LSTM||represents||targets
targets||with||aspect embeddings
",,,"Baselines||has||AE - LSTM
",,,,,
baselines,ATAE - LSTM is developed based on AE - LSTM .,"ATAE - LSTM
based on
AE - LSTM","ATAE - LSTM||based on||AE - LSTM
",,,"Baselines||has||ATAE - LSTM
",,,,,
results,"All the other methods are based on LSTM models and better than the Majority method , showing that LSTM has potentials in automatically generating representations and can all bring performance improvement for sentiment classification .","All the other methods
based on
LSTM models
better than
Majority method","All the other methods||based on||LSTM models
LSTM models||better than||Majority method
",,,"Results||has||All the other methods
",,,,,
results,"The LSTM method gets the worst performance of all the neural network baseline methods , because it treats targets equally with other context words and does not make full use of the target information .","LSTM method
gets
worst performance
of
all the neural network baseline methods","LSTM method||gets||worst performance
worst performance||of||all the neural network baseline methods
",,,"Results||has||LSTM method
",,,,,
results,"TD - LSTM outperforms LSTM over 1 percent and 2 percent on the Restaurant and Laptop category respectively , since it develops from the standard LSTM and processes the left and right contexts with targets .","TD - LSTM
outperforms
LSTM
over
1 percent and 2 percent
on
Restaurant and Laptop category","TD - LSTM||outperforms||LSTM
LSTM||over||1 percent and 2 percent
1 percent and 2 percent||on||Restaurant and Laptop category
",,,"Results||has||TD - LSTM
",,,,,
results,"Further , both AE - LSTM and ATAE - LSTM stably exceed the TD - LSTM method because of the introduction of attention mechanism .","both AE - LSTM and ATAE - LSTM
stably exceed
TD - LSTM method","both AE - LSTM and ATAE - LSTM||stably exceed||TD - LSTM method
",,,"Results||has||both AE - LSTM and ATAE - LSTM
",,,,,
results,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .","For
AE - LSTM and ATAE - LSTM
capture
important information
in
context
with
supervision
of
target
generate
more reasonable representations
for
aspect - level sentiment classification","AE - LSTM and ATAE - LSTM||capture||important information
important information||in||context
context||with||supervision
supervision||of||target
AE - LSTM and ATAE - LSTM||generate||more reasonable representations
more reasonable representations||for||aspect - level sentiment classification
",,"Results||For||AE - LSTM and ATAE - LSTM
",,,,,,
results,"We can also see that AE - LSTM and ATAE - LSTM further emphasize the modeling of targets via the addition of the aspect embedding , which is also the reason of performance improvement .","see
AE - LSTM and ATAE - LSTM
emphasize
modeling
of
targets
via
addition of the aspect embedding","AE - LSTM and ATAE - LSTM||emphasize||modeling
modeling||of||targets
modeling||via||addition of the aspect embedding
",,"Results||see||AE - LSTM and ATAE - LSTM
",,,,,,
results,"Compared with AE - LSTM , ATAE - LSTM especially enhance the interaction between the context words and target and thus has a better performance than AE - LSTM .","Compared with
AE - LSTM
ATAE - LSTM
especially enhance
interaction
between
context words and target
better performance
than
AE - LSTM","ATAE - LSTM||especially enhance||interaction
interaction||between||context words and target
better performance||than||AE - LSTM
ATAE - LSTM||Compared with||AE - LSTM
","ATAE - LSTM||has||better performance
",,"Results||has||ATAE - LSTM
",,,,,
results,We can see that IAN achieves the best performance among all baselines .,"IAN
achieves
best performance
among
all baselines","IAN||achieves||best performance
best performance||among||all baselines
",,,"Results||see||IAN
",,,,,
results,"Compared with ATAE - LSTM model , IAN improves the performance about 1.4 % and 3.2 % on the Restaurant and Laptop categories respectively .","Compared with
ATAE - LSTM model
IAN
improves
performance
about
1.4 % and 3.2 %
on
Restaurant and Laptop categories","IAN||improves||performance
performance||about||1.4 % and 3.2 %
1.4 % and 3.2 %||on||Restaurant and Laptop categories
","ATAE - LSTM model||has||IAN
","Results||Compared with||ATAE - LSTM model
",,,,,,
results,"The more attentions are paid to targets , the higher accuracy the system achieves .","more attentions
paid to
targets
higher accuracy
achieves","more attentions||achieves||higher accuracy
more attentions||paid to||targets
",,,"Results||has||more attentions
",,,,,
research-problem,Convolutional Neural Networks with Recurrent Neural Filters,Convolutional Neural Networks with Recurrent Neural Filters,,,,,"Contribution||has research problem||Convolutional Neural Networks with Recurrent Neural Filters
",,,,
research-problem,"In this work , we model convolution filters with RNNs that naturally capture compositionality and long - term dependencies in language .",model convolution filters with RNNs,,,,,"Contribution||has research problem||model convolution filters with RNNs
",,,,
model,"To overcome this , we propose to employ recurrent neural networks ( RNNs ) as convolution filters of CNN systems for various NLP tasks .","propose to employ
recurrent neural networks ( RNNs )
as
convolution filters
of
CNN systems","recurrent neural networks ( RNNs )||as||convolution filters
convolution filters||of||CNN systems
",,"Model||propose to employ||recurrent neural networks ( RNNs )
",,,,,,
model,"Our recurrent neural filters ( RNFs ) can naturally deal with language compositionality with a recurrent function that models word relations , and they are also able to implicitly model long - term dependencies .","recurrent neural filters ( RNFs )
deal with
language compositionality
with
recurrent function
that models
word relations
to implicitly model
long - term dependencies","recurrent neural filters ( RNFs )||deal with||language compositionality
language compositionality||with||recurrent function
recurrent function||that models||word relations
recurrent neural filters ( RNFs )||to implicitly model||long - term dependencies
",,,"Model||has||recurrent neural filters ( RNFs )
",,,,,
model,"RNFs are typically applied to word sequences of moderate lengths , which alleviates some well - known drawbacks of RNNs , including their vulnerability to the gradient vanishing and exploding problems .","RNFs
applied to
word sequences
of
moderate lengths
alleviates
some well - known drawbacks
of
RNNs
including
vulnerability
to
gradient vanishing and exploding problems","RNFs||alleviates||some well - known drawbacks
some well - known drawbacks||including||vulnerability
vulnerability||to||gradient vanishing and exploding problems
some well - known drawbacks||of||RNNs
RNFs||applied to||word sequences
word sequences||of||moderate lengths
",,,"Model||has||RNFs
",,,,,
model,"As in conventional CNNs , the computation of the convolution operation with RNFs can be easily parallelized .","As in
conventional CNNs
computation of
convolution operation
with
RNFs
can be
parallelized","convolution operation||with||RNFs
RNFs||can be||parallelized
parallelized||As in||conventional CNNs
",,"Model||computation of||convolution operation
",,,,,,
model,"As a result , RNF - based CNN models can be 3 - 8 x faster than their RNN counterparts .","RNF - based CNN models
can be
3 - 8 x faster
than
RNN counterparts","RNF - based CNN models||can be||3 - 8 x faster
3 - 8 x faster||than||RNN counterparts
",,,"Model||has||RNF - based CNN models
",,,,,
model,We present two RNF - based CNN architectures for sentence classification and answer sentence selection problems .,"present
two RNF - based CNN architectures
for
sentence classification and answer sentence selection problems","two RNF - based CNN architectures||for||sentence classification and answer sentence selection problems
",,"Model||present||two RNF - based CNN architectures
",,,,,,
baselines,We consider CNN variants with linear filters and RNFs.,"consider
CNN variants
with",,,"Baselines||consider||CNN variants
",,,,"CNN variants||with||linear filters and RNFs
",,
baselines,"For RNFs , we adopt two implementations based on GRUs and LSTMs respectively .","For
RNFs
adopt
two implementations
based on
GRUs and LSTMs","RNFs||adopt||two implementations
two implementations||based on||GRUs and LSTMs
",,"Baselines||For||RNFs
",,,,,,
baselines,"We also compare against the following RNN variants : GRU , LSTM , GRU with max pooling , and LSTM with max pooling .","compare against
RNN variants
GRU
LSTM
GRU
with
max pooling
LSTM
with
max pooling","GRU||with||max pooling
LSTM||with||max pooling
","RNN variants||name||GRU
RNN variants||name||LSTM
RNN variants||name||GRU
RNN variants||name||LSTM
","Baselines||compare against||RNN variants
",,,,,,
results,"In particular , CNN - RNF - LSTM achieves 53.4 % and 90.0 % accuracies on the fine - grained and binary sentiment classification tasks respectively , which match the state - of the - art results on the Stanford Sentiment Treebank .","CNN - RNF - LSTM
achieves
53.4 % and 90.0 % accuracies
on
fine - grained and binary sentiment classification tasks
match
state - of the - art results
on
Stanford Sentiment Treebank","CNN - RNF - LSTM||achieves||53.4 % and 90.0 % accuracies
53.4 % and 90.0 % accuracies||match||state - of the - art results
state - of the - art results||on||Stanford Sentiment Treebank
53.4 % and 90.0 % accuracies||on||fine - grained and binary sentiment classification tasks
",,,"Results||has||CNN - RNF - LSTM
",,,,,
results,"CNN - RNF - LSTM also obtains competitive results on answer sentence selection datasets , despite the simple model architecture compared to state - of - the - art systems .","obtains
competitive results
on
answer sentence selection datasets
despite
simple model architecture
compared to
state - of - the - art systems","competitive results||despite||simple model architecture
simple model architecture||compared to||state - of - the - art systems
competitive results||on||answer sentence selection datasets
",,,,,"CNN - RNF - LSTM||obtains||competitive results
",,,
results,"Conventional RNN models clearly benefit from max pooling , especially on the task of answer sentence selection .","Conventional RNN models
benefit from
max pooling
especially on
answer sentence selection","Conventional RNN models||benefit from||max pooling
max pooling||especially on||answer sentence selection
",,,"Results||has||Conventional RNN models
",,,,,
results,"As a result , RNF - based CNN models perform consistently better than max - pooled RNN models .","RNF - based CNN models
perform
consistently better
than
max - pooled RNN models","RNF - based CNN models||perform||consistently better
consistently better||than||max - pooled RNN models
",,,"Results||has||RNF - based CNN models
",,,,,
research-problem,Mazajak : An Online Arabic Sentiment Analyser,Arabic Sentiment Analyser,,,,,"Contribution||has research problem||Arabic Sentiment Analyser
",,,,
research-problem,Sentiment analysis ( SA ) is one of the most useful natural language processing applications .,Sentiment analysis ( SA ),,,,,"Contribution||has research problem||Sentiment analysis ( SA )
",,,,
research-problem,Sentiment analysis is one of the vital approaches to extract public opinion from large corpora of text .,Sentiment analysis,,,,,"Contribution||has research problem||Sentiment analysis
",,,,
research-problem,"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .",SA,,,,,"Contribution||has research problem||SA
",,,,
model,"In this paper , we present Mazajak 2 , an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings .","present
Mazajak
Online Arabic sentiment analysis system
utilises
deep learning
massive Arabic word embeddings","Mazajak||utilises||deep learning
Mazajak||utilises||massive Arabic word embeddings
","Mazajak||has||Online Arabic sentiment analysis system
","Model||present||Mazajak
",,,,,,
model,The system is available as an online API that can be used by other researchers .,"available as
online API
used by
other researchers","online API||used by||other researchers
",,"Model||available as||online API
",,,,,,
baselines,The best performing system in the SemEval 2017 task is the one described in which achieved an F P N of 0.61 .,"best performing system
in
SemEval 2017 task","best performing system||in||SemEval 2017 task
",,,"Baselines||has||best performing system
",,,,,
baselines,"For the ASTD , the best reported results are by who used an ensemble system combining output of CNN and Bi - LSTM architectures , which achieved an F P N of 0.71 .","For
ASTD
best reported results
by
ensemble system
combining output of
CNN and Bi - LSTM architectures","best reported results||by||ensemble system
ensemble system||combining output of||CNN and Bi - LSTM architectures
","ASTD||has||best reported results
","Baselines||For||ASTD
",,,,,,
results,"As shown in the table , Mazajak model outperformed the current state - of - the - art models on the SemEval and ASTD datasets .","Mazajak model
outperformed
current state - of - the - art models
on
SemEval and ASTD datasets","Mazajak model||outperformed||current state - of - the - art models
current state - of - the - art models||on||SemEval and ASTD datasets
",,,"Results||has||Mazajak model
",,,,,
results,"In addition , it achieved a high performance on the ArSAS dataset .","achieved
high performance
on
ArSAS dataset","high performance||on||ArSAS dataset
",,,,,"Mazajak model||achieved||high performance
",,,
results,"Our reported scores are higher than current top systems for all the evaluation scores , including average recall , F P N , and accuracy .","reported scores
higher than
current top systems
for
all the evaluation scores
including
average recall , F P N , and accuracy","reported scores||higher than||current top systems
reported scores||for||all the evaluation scores
all the evaluation scores||including||average recall , F P N , and accuracy
",,,"Results||has||reported scores
",,,,,
results,These results confirm that our model choice for our tool represents the current state - of - the - art for Arabic SA .,"for
our tool
represents
current state - of - the - art
for
Arabic SA","our tool||represents||current state - of - the - art
current state - of - the - art||for||Arabic SA
",,"Results||for||our tool
",,,,,,
research-problem,Exploiting Coarse - to - Fine Task Transfer for Aspect - level Sentiment Classification,Aspect - level Sentiment Classification,,,,,"Contribution||has research problem||Aspect - level Sentiment Classification
",,,,
research-problem,"Aspect - level sentiment classification ( ASC ) aims at identifying sentiment polarities towards aspects in a sentence , where the aspect can behave as a general Aspect Category ( AC ) or a specific Aspect Term ( AT ) .",Aspect - level sentiment classification ( ASC ),,,,,"Contribution||has research problem||Aspect - level sentiment classification ( ASC )
",,,,
research-problem,"To model aspect - oriented sentiment analysis , equipping Recurrent Neural Networks ( RNNs ) with the attention Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",aspect - oriented sentiment analysis,,,,,"Contribution||has research problem||aspect - oriented sentiment analysis
",,,,
model,"To resolve the challenges , we propose a novel framework named Multi - Granularity Alignment Network ( MGAN ) to simultaneously align aspect granularity and aspect- specific feature representations across domains .","propose
novel framework named Multi - Granularity Alignment Network ( MGAN )
simultaneously align
aspect granularity and aspect- specific feature representations
across
domains","novel framework named Multi - Granularity Alignment Network ( MGAN )||simultaneously align||aspect granularity and aspect- specific feature representations
aspect granularity and aspect- specific feature representations||across||domains
",,"Model||propose||novel framework named Multi - Granularity Alignment Network ( MGAN )
",,,,,,
model,"Specifically , the MGAN consists of two networks for learning aspect - specific representations for the two domains , respectively .","MGAN
consists of
two networks
for learning
aspect - specific representations
for
two domains","MGAN||consists of||two networks
two networks||for learning||aspect - specific representations
aspect - specific representations||for||two domains
",,,"Model||has||MGAN
",,,,,
model,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .","to reduce
task discrepancy
between
domains
modeling
two tasks
at
same fine - grained level
propose
novel Coarse2 Fine ( C2F ) attention module
to help
source task
automatically capture
corresponding aspect term
in
context
towards
given aspect category","task discrepancy||modeling||two tasks
two tasks||at||same fine - grained level
task discrepancy||propose||novel Coarse2 Fine ( C2F ) attention module
novel Coarse2 Fine ( C2F ) attention module||to help||source task
source task||automatically capture||corresponding aspect term
corresponding aspect term||in||context
corresponding aspect term||towards||given aspect category
task discrepancy||between||domains
",,"Model||to reduce||task discrepancy
",,,,,,
model,"To prevent false alignment , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align aspect - specific representations .","To prevent
false alignment
adopt
Contrastive Feature Alignment ( CFA )
to semantically align
aspect - specific representations","false alignment||adopt||Contrastive Feature Alignment ( CFA )
Contrastive Feature Alignment ( CFA )||to semantically align||aspect - specific representations
",,"Model||To prevent||false alignment
",,,,,,
model,"The CFA considers both semantic alignment by maximally ensuring the equivalent distributions from different domains but the same class , and semantic separation by guaranteeing distributions from both different classes and domains to be as dissimilar as possible .","CFA
considers
semantic alignment
by maximally ensuring
equivalent distributions
from
different domains
same
class
semantic separation
by guaranteeing
distributions
from both
different classes and domains
to be
as dissimilar as possible","CFA||considers||semantic separation
semantic separation||by guaranteeing||distributions
distributions||from both||different classes and domains
distributions||to be||as dissimilar as possible
CFA||considers||semantic alignment
semantic alignment||by maximally ensuring||equivalent distributions
equivalent distributions||from||different domains
different domains||same||class
",,,"Model||has||CFA
",,,,,
hyperparameters,The word embeddings are initialized with 200 - dimension GloVE vectors and fine - tuned during the training .,"word embeddings
initialized with
200 - dimension GloVE vectors
fine - tuned during
training","word embeddings||fine - tuned during||training
word embeddings||initialized with||200 - dimension GloVE vectors
",,,"Hyperparameters||has||word embeddings
",,,,,
hyperparameters,The fc layer size is 300 .,"fc layer size
is
300","fc layer size||is||300
",,,"Hyperparameters||has||fc layer size
",,,,,
hyperparameters,The Adam ( Kingma and Ba 2014 ) is used as the optimizer with the initial learning rate 10 ? 4 .,"Adam ( Kingma and Ba 2014 )
used as
optimizer
with
initial learning rate 10 ? 4","Adam ( Kingma and Ba 2014 )||used as||optimizer
optimizer||with||initial learning rate 10 ? 4
",,,"Hyperparameters||has||Adam ( Kingma and Ba 2014 )
",,,,,
hyperparameters,Gradients with the 2 norm larger than 40 are normalized to be 40 .,"Gradients
with
2 norm
larger than
40
are
normalized
to be
40","Gradients||with||2 norm
2 norm||larger than||40
Gradients||are||normalized
normalized||to be||40
",,,"Hyperparameters||has||Gradients
",,,,,
hyperparameters,"All weights in networks are randomly initialized from a uniform distribution U ( ? 0.01 , 0.01 ) .","All weights
in
networks
randomly initialized from
uniform distribution U ( ? 0.01 , 0.01 )","All weights||randomly initialized from||uniform distribution U ( ? 0.01 , 0.01 )
All weights||in||networks
",,,"Hyperparameters||has||All weights
",,,,,
hyperparameters,"The batch sizes are 64 and 32 for source and target domains , respectively .","batch sizes
are
64 and 32
for
source and target domains","batch sizes||are||64 and 32
64 and 32||for||source and target domains
",,,"Hyperparameters||has||batch sizes
",,,,,
hyperparameters,"To alleviate overfitting , we apply dropout on the word embeddings of the context with dropout rate 0.5 .","To alleviate
overfitting
apply
dropout
on
word embeddings
of
context
with
dropout rate 0.5","overfitting||apply||dropout
dropout||on||word embeddings
word embeddings||with||dropout rate 0.5
word embeddings||of||context
",,"Hyperparameters||To alleviate||overfitting
",,,,,,
hyperparameters,We also perform early stopping on the validation set during the training process .,"perform
early stopping
on
validation set
during
training process","early stopping||on||validation set
validation set||during||training process
",,"Hyperparameters||perform||early stopping
",,,,,,
hyperparameters,The hyperparameters are tuned on 10 % randomly held - out training data of the target domain in R1?L task and are fixed to be used in all transfer pairs .,"tuned on
10 % randomly held - out training data
of
target domain
in
R1?L task
fixed to be used in all
transfer pairs","10 % randomly held - out training data||fixed to be used in all||transfer pairs
10 % randomly held - out training data||of||target domain
target domain||in||R1?L task
",,"Hyperparameters||tuned on||10 % randomly held - out training data
",,,,,,
baselines,Non-Transfer,Non-Transfer,,,,"Baselines||has||Non-Transfer
",,,,,"Non-Transfer||has||Target Network ( TN )
"
baselines,"To demonstrate the benefits from coarse - tofine task transfer , we compare with the following state - of the - art AT - level methods without transfer : Target Network ( TN ) :",Target Network ( TN ),,,,,,,,,
baselines,It is our proposed base model ( BiLSTM + C2A + Pas ) trained on D t for the target task .,"is
proposed base model ( BiLSTM + C2A + Pas )
trained on
D t
for
target task","proposed base model ( BiLSTM + C2A + Pas )||trained on||D t
D t||for||target task
",,,,,"Target Network ( TN )||is||proposed base model ( BiLSTM + C2A + Pas )
",,,
baselines,Transfer,Transfer,,,,"Baselines||has||Transfer
",,,,,"Transfer||has||Source- only ( SO )
"
baselines,Source- only ( SO ) :,Source- only ( SO ),,,,,,,,,
baselines,It uses a source network trained on D s to initialize a target network and then tests it on D t .,"uses
source network
trained on
D s
to initialize
target network
tests it on
D t","source network||tests it on||D t
source network||trained on||D s
D s||to initialize||target network
",,,,,"Source- only ( SO )||uses||source network
",,,
baselines,Fine-tuning ( FT ) : It advances SO with further finetuning the target network on D t .,"Fine-tuning ( FT )
advances
SO
with further finetuning
target network
on
D t","Fine-tuning ( FT )||advances||SO
SO||with further finetuning||target network
target network||on||D t
",,,,,,,"Transfer||has||Fine-tuning ( FT )
",
baselines,M- DAN : It is a multi-adversarial version of Domain Adversarial Network ( DAN ) ) based on multiple domain discriminators .,"M- DAN
is
multi-adversarial version
of
Domain Adversarial Network ( DAN )
based on
multiple domain discriminators","M- DAN||is||multi-adversarial version
multi-adversarial version||based on||multiple domain discriminators
multi-adversarial version||of||Domain Adversarial Network ( DAN )
",,,,,,,"Transfer||has||M- DAN
",
baselines,"M - MMD : Similar with M - DAN , M - MMD aligns different class distributions between domains based on multiple Maximum Mean Discrepancy ( MMD ) ) .","M - MMD
aligns
different class distributions
between
domains
based on
multiple Maximum Mean Discrepancy ( MMD )","M - MMD||aligns||different class distributions
different class distributions||between||domains
domains||based on||multiple Maximum Mean Discrepancy ( MMD )
",,,,,,,"Transfer||has||M - MMD
",
results,Comparison with Non - Transfer,Comparison with Non - Transfer,,,,"Results||has||Comparison with Non - Transfer
",,,,,"Comparison with Non - Transfer||has||MGAN
"
results,"( 2 ) MGAN consistently outperforms the MGAN w / o C2 F , where C2F module of the source network is removed and the source position information is missed ( we set all p s i to 1 ) , by 1.41 % , 1.03 % , 1.09 % for accuracy and 1.79 % , 3.62 % and 1.16 % for Macro - F1 on average .","MGAN
consistently outperforms
MGAN w / o C2 F
where
C2F module
of
source network
is
removed
source position information
is
missed
by
1.41 % , 1.03 % , 1.09 %
for
accuracy
1.79 % , 3.62 % and 1.16 %
for
Macro - F1","MGAN||consistently outperforms||MGAN w / o C2 F
MGAN w / o C2 F||where||source position information
source position information||is||missed
missed||by||1.41 % , 1.03 % , 1.09 %
1.41 % , 1.03 % , 1.09 %||for||accuracy
missed||by||1.79 % , 3.62 % and 1.16 %
1.79 % , 3.62 % and 1.16 %||for||Macro - F1
MGAN w / o C2 F||where||C2F module
C2F module||of||source network
C2F module||is||removed
",,,,,,,,
results,"The MGAN w / o PI , which does not utilize the position information , performs very poorly .","MGAN w / o PI
does not utilize
position information
performs
very poorly","MGAN w / o PI||performs||very poorly
MGAN w / o PI||does not utilize||position information
",,,,,,,"Comparison with Non - Transfer||has||MGAN w / o PI
",
results,Comparison with Transfer,Comparison with Transfer,,,,"Results||has||Comparison with Transfer
",,,,,"Comparison with Transfer||has||SO
"
results,SO performs poorly due to no adaptation applied .,"SO
performs
poorly","SO||performs||poorly
",,,,,,,,
results,The popular technique FT can not achieve satisfactory results since fine - tuning may cause the oblivion of useful knowledge from the source task .,"popular technique FT
can not achieve
satisfactory results","popular technique FT||can not achieve||satisfactory results
",,,,,,,"Comparison with Transfer||has||popular technique FT
",
results,"The full model MGAN outperforms M - DAN and M - MMD by 1.80 % and 1.33 % for accuracy and 1.90 % and 1.66 % for Marco - F1 on average , respectively .","full model MGAN
outperforms
M - DAN and M - MMD
by
1.80 % and 1.33 %
for
accuracy
1.90 % and 1.66 %
for
Marco - F1 on average","full model MGAN||outperforms||M - DAN and M - MMD
M - DAN and M - MMD||by||1.90 % and 1.66 %
1.90 % and 1.66 %||for||Marco - F1 on average
M - DAN and M - MMD||by||1.80 % and 1.33 %
1.80 % and 1.33 %||for||accuracy
",,,,,,,"Comparison with Transfer||has||full model MGAN
",
results,"Remarkably , MGAN considers both of them in a point - wise surrogate , which altogether improves the performance of our method .","MGAN
considers both of them in
point - wise surrogate
improves
performance
of
our method","MGAN||considers both of them in||point - wise surrogate
point - wise surrogate||improves||performance
performance||of||our method
",,,,,,,"Comparison with Transfer||has||MGAN
",
results,"Besides , MGAN outperforms its ablation MGAN w/ o SS removing the semantic separation loss of the CFA by 0.81 % for accuracy and 1.00 % for Macro - F1 on average , which implies that the semantic separation plays an important role in alleviating false alignment .","outperforms
ablation MGAN w/ o SS
removing
semantic separation loss
of
CFA
by
0.81 %
for
accuracy
1.00 %
for
Macro - F1 on average","ablation MGAN w/ o SS||removing||semantic separation loss
semantic separation loss||of||CFA
CFA||by||0.81 %
0.81 %||for||accuracy
CFA||by||1.00 %
1.00 %||for||Macro - F1 on average
",,,,,"MGAN||outperforms||ablation MGAN w/ o SS
",,,
results,Effect of C2F Attention Module,Effect of C2F Attention Module,,,,"Results||has||Effect of C2F Attention Module
",,,,,"Effect of C2F Attention Module||has||MGAN
"
results,"Then , compared with MGAN w / o C2F , MGAN further uses C2F to capture more specific aspect terms from the context towards the aspect category , such as "" shells "" to food seafood sea , which helps the source task capture more fine - grained semantics of aspect category and detailed position information like the target task , such that the sentiment attention can be positionaware and identify more relevant sentiment features towards the aspect .","compared with
MGAN
C2F
uses
to capture
more specific aspect terms
from
context
towards
aspect category
helps
source task
capture
more fine - grained semantics
of
aspect category
detailed position information
like
target task","MGAN||uses||C2F
C2F||to capture||more specific aspect terms
more specific aspect terms||from||context
context||towards||aspect category
more specific aspect terms||helps||source task
source task||capture||more fine - grained semantics
more fine - grained semantics||of||aspect category
source task||capture||detailed position information
detailed position information||like||target task
",,,,,,"MGAN||compared with||MGAN w / o C2F
",,
results,While MGAN w / o C2F locates wrong sentiment contexts and fails in ( c ) .,"MGAN w / o C2F
locates
wrong sentiment contexts","MGAN w / o C2F||locates||wrong sentiment contexts
",,,,,,,,
results,"As such , benefited from distilled knowledge from the source task , MGAN can better model the complicated relatedness between the context and aspect term for the target domain L , but MGAN w / o C2F performs poorly though it make true predictions in ( d ) and ( e ) .","benefited from
distilled knowledge
from
source task
MGAN
can better model
complicated relatedness
between
context and aspect term
for
target domain L
MGAN w / o C2F
performs
poorly","distilled knowledge||from||source task
MGAN||can better model||complicated relatedness
complicated relatedness||between||context and aspect term
context and aspect term||for||target domain L
MGAN w / o C2F||performs||poorly
","distilled knowledge||has||MGAN
distilled knowledge||has||MGAN w / o C2F
",,,,"Effect of C2F Attention Module||benefited from||distilled knowledge
",,,
research-problem,Transformation Networks for Target - Oriented Sentiment Classification,Target - Oriented Sentiment Classification,,,,,"Contribution||has research problem||Target - Oriented Sentiment Classification
",,,,
model,"We propose a new architecture , named Target - Specific Transformation Networks ( TNet ) , to solve the above issues in the task of target sentiment classification .","propose
new architecture
named
Target - Specific Transformation Networks ( TNet )","new architecture||named||Target - Specific Transformation Networks ( TNet )
",,"Model||propose||new architecture
",,,,,,
model,TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs .,"TNet
firstly encodes
context information
into
word embeddings
generates
contextualized word representations
with
LSTMs","TNet||generates||contextualized word representations
contextualized word representations||with||LSTMs
TNet||firstly encodes||context information
context information||into||word embeddings
",,,"Model||has||TNet
",,,,,
model,"To integrate the target information into the word representations , TNet introduces a novel Target - Specific Transformation ( TST ) component for generating the target - specific word representations .","To integrate
target information
into
word representations
introduces
novel Target - Specific Transformation ( TST ) component
for generating
target - specific word representations","novel Target - Specific Transformation ( TST ) component||for generating||target - specific word representations
novel Target - Specific Transformation ( TST ) component||To integrate||target information
target information||into||word representations
",,,,,"TNet||introduces||novel Target - Specific Transformation ( TST ) component
",,,
model,"Contrary to the previous attention - based approaches which apply the same target representation to determine the attention scores of individual context words , TST firstly generates different representations of the target conditioned on individual context words , then it consolidates each context word with its tailor - made target representation to obtain the transformed word representation .","target
of
individual context words
TST
generates
different representations
conditioned on
consolidates
each context word
with
tailor - made target representation
to obtain
transformed word representation","TST||consolidates||each context word
each context word||with||tailor - made target representation
tailor - made target representation||to obtain||transformed word representation
TST||generates||different representations
different representations||of||target
target||conditioned on||individual context words
",,,"Model||has||TST
",,,,,
model,"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we design a contextpreserving mechanism to contextualize the generated target - specific word representations .","design
contextpreserving mechanism
to contextualize
generated target - specific word representations","contextpreserving mechanism||to contextualize||generated target - specific word representations
",,"Model||design||contextpreserving mechanism
",,,,,,
model,"To help the CNN feature extractor locate sentiment indicators more accurately , we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target .","To help
CNN feature extractor
locate
sentiment indicators
more accurately
adopt
proximity strategy
to scale
input
of
convolutional layer
with
positional relevance
between
a word and the target","proximity strategy||To help||CNN feature extractor
CNN feature extractor||locate||sentiment indicators
proximity strategy||to scale||input
input||of||convolutional layer
convolutional layer||with||positional relevance
positional relevance||between||a word and the target
","sentiment indicators||has||more accurately
","Model||adopt||proximity strategy
",,,,,,
baselines,SVM : It is a traditional support vector machine based model with extensive feature engineering ;,"SVM
is
traditional support vector machine based model
with
extensive feature engineering","SVM||is||traditional support vector machine based model
traditional support vector machine based model||with||extensive feature engineering
",,,"Baselines||has||SVM
",,,,,
baselines,AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ;,"AdaRNN
learns
sentence representation
toward
target
for
sentiment prediction
via
semantic composition
over
dependency tree","AdaRNN||learns||sentence representation
sentence representation||toward||target
sentence representation||for||sentiment prediction
sentiment prediction||via||semantic composition
semantic composition||over||dependency tree
",,,"Baselines||has||AdaRNN
",,,,,
baselines,"AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ;","AE - LSTM
ATAE - LSTM
AE - LSTM
is
simple LSTM model
incorporating
target embedding
as
input
extends
with
attention","AE - LSTM||is||simple LSTM model
simple LSTM model||incorporating||target embedding
target embedding||as||input
ATAE - LSTM||extends||AE - LSTM
AE - LSTM||with||attention
",,,"Baselines||has||AE - LSTM
Baselines||has||ATAE - LSTM
",,,,,
baselines,IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ;,"IAN
employs
two LSTMs
to learn
representations
of
context and the target phrase","IAN||employs||two LSTMs
two LSTMs||to learn||representations
representations||of||context and the target phrase
",,,"Baselines||has||IAN
",,,,,
baselines,CNN - ASP :,CNN - ASP,,,,"Baselines||has||CNN - ASP
",,,,,
baselines,It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ;,"is
CNN - based model
which directly concatenates
target representation
to
each word embedding","CNN - based model||which directly concatenates||target representation
target representation||to||each word embedding
",,,,,"CNN - ASP||is||CNN - based model
",,,
baselines,TD - LSTM :,TD - LSTM,,,,"Baselines||has||TD - LSTM
",,,,,
baselines,"It employs two LSTMs to model the left and right contexts of the target separately , then performs predictions based on concatenated context representations ;","employs
two LSTMs
to model
left and right contexts
of
target
performs
predictions
based on
concatenated context representations","predictions||based on||concatenated context representations
two LSTMs||to model||left and right contexts
left and right contexts||of||target
",,,,,"TD - LSTM||performs||predictions
TD - LSTM||employs||two LSTMs
",,,
baselines,MemNet : It applies attention mechanism over the word embeddings multiple times and predicts sentiments based on the top - most sentence representations ;,"MemNet
applies
attention mechanism
over
word embeddings multiple times
predicts
sentiments
based on
top - most sentence representations","MemNet||applies||attention mechanism
attention mechanism||over||word embeddings multiple times
MemNet||predicts||sentiments
sentiments||based on||top - most sentence representations
",,,"Baselines||has||MemNet
",,,,,
baselines,"BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and introduces gates to measure the importance of left context , right context , and the entire sentence for the prediction ;","BILSTM - ATT -G
models
left and right contexts
using
two attention - based LSTMs
introduces
gates
to measure
importance of left context , right context , and the entire sentence
for
prediction","BILSTM - ATT -G||models||left and right contexts
left and right contexts||using||two attention - based LSTMs
BILSTM - ATT -G||introduces||gates
gates||to measure||importance of left context , right context , and the entire sentence
importance of left context , right context , and the entire sentence||for||prediction
",,,"Baselines||has||BILSTM - ATT -G
",,,,,
baselines,RAM : RAM is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .,"RAM
is
multilayer architecture
where
each layer
consists of
attention - based aggregation of word features
GRU cell
to learn
sentence representation","RAM||is||multilayer architecture
multilayer architecture||where||each layer
each layer||consists of||attention - based aggregation of word features
each layer||consists of||GRU cell
GRU cell||to learn||sentence representation
",,,"Baselines||has||RAM
",,,,,
results,LSTM - based models relying on sequential information can perform well for formal sentences by capturing more useful context features ;,"LSTM - based models
relying on
sequential information
can perform
well
for
formal sentences
by capturing
more useful context features","LSTM - based models||relying on||sequential information
LSTM - based models||can perform||well
well||for||formal sentences
well||by capturing||more useful context features
",,,"Results||has||LSTM - based models
",,,,,
results,"For ungrammatical text , CNN - based models may have some advantages because CNN aims to extract the most informative n-gram features and is thus less sensitive to informal texts without strong sequential patterns .","For
ungrammatical text
CNN - based models
may have
some advantages","CNN - based models||may have||some advantages
","ungrammatical text||has||CNN - based models
","Results||For||ungrammatical text
",,,,,,
ablation-analysis,"After removing the deep transformation ( i.e. , the techniques introduced in Section 2.2 ) , both TNet - LF and TNet - AS are reduced to TNet w/o transformation ( where position relevance is kept ) , and their results in both accuracy and F 1 measure are incomparable with those of TNet .","removing
deep transformation
TNet - LF and TNet - AS
reduced to
TNet w/o transformation
results
in both
accuracy and F 1 measure
incomparable with
TNet","TNet - LF and TNet - AS||reduced to||TNet w/o transformation
results||incomparable with||TNet
results||in both||accuracy and F 1 measure
","deep transformation||has||TNet - LF and TNet - AS
TNet - LF and TNet - AS||has||results
","Ablation analysis||removing||deep transformation
",,,,,,
ablation-analysis,It shows that the integration of target information into the word - level representations is crucial for good performance .,"shows that
integration
of
target information
into
word - level representations
crucial for
good performance","integration||of||target information
target information||into||word - level representations
target information||crucial for||good performance
",,,,,"deep transformation||shows that||integration
",,,
ablation-analysis,"Comparing the results of TNet and TNet w/o context ( where TST and position relevance are kept ) , we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST 7 , while on TWITTER , TNet w/o context performs very competitive ( p- values with TNet - LF and TNet - AS are 0.066 and 0.053 respectively for Accuracy ) .","Comparing
results
of
TNet and TNet w/o context
observe
performance
of
TNet w/o context
drops
significantly
on
LAPTOP and REST
on
TWITTER
TNet w/o context
performs
very competitive","results||of||TNet and TNet w/o context
TNet and TNet w/o context||observe||performance
performance||drops||significantly
significantly||on||LAPTOP and REST
performance||of||TNet w/o context
TNet and TNet w/o context||on||TWITTER
TNet w/o context||performs||very competitive
","TWITTER||has||TNet w/o context
","Ablation analysis||Comparing||results
",,,,,,
ablation-analysis,"TNet w/o context performs consistently better than TNet w/o transformation , which verifies the efficacy of the target specific transformation ( TST ) , before applying context - preserving .","TNet w/o context
performs
consistently better
than
TNet w/o transformation","TNet w/o context||performs||consistently better
consistently better||than||TNet w/o transformation
",,,"Ablation analysis||has||TNet w/o context
",,,,,
ablation-analysis,"All of the produced p-values are less than 0.05 , suggesting that the improvements brought in by position information are significant .","produced p-values
are less than
0.05
suggesting that
improvements
brought in by
position information
are
significant","produced p-values||are less than||0.05
0.05||suggesting that||improvements
improvements||are||significant
improvements||brought in by||position information
",,,"Ablation analysis||has||produced p-values
",,,,,
research-problem,IARM : Inter-Aspect Relation Modeling with Memory Networks in Aspect - Based Sentiment Analysis,Aspect - Based Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - Based Sentiment Analysis
",,,,
research-problem,Sentiment analysis has immense implications in modern businesses through user-feedback mining .,Sentiment analysis,,,,,"Contribution||has research problem||Sentiment analysis
",,,,
research-problem,Aspect - based sentiment analysis ( ABSA ) caters to these needs .,Aspect - based sentiment analysis ( ABSA ),,,,,"Contribution||has research problem||Aspect - based sentiment analysis ( ABSA )
",,,,
research-problem,The aim of the ABSA classifier is to learn these connections between the aspects and their sentiment bearing phrases .,ABSA,,,,,"Contribution||has research problem||ABSA
",,,,
model,"To model these scenarios , firstly , following , we independently generate aspect - aware sentence representations for all the aspects using gated recurrent unit ( GRU ) and attention mechanism .","independently generate
aspect - aware sentence representations
for
all the aspects
using
gated recurrent unit ( GRU )
attention mechanism","aspect - aware sentence representations||using||gated recurrent unit ( GRU )
aspect - aware sentence representations||using||attention mechanism
aspect - aware sentence representations||for||all the aspects
",,"Model||independently generate||aspect - aware sentence representations
",,,,,,
model,"Then , we employ memory networks to repeatedly match the target aspect representation with the other aspects to generate more accurate representation of the target aspect .","employ
memory networks
to repeatedly match
target aspect representation
with
other aspects
to generate
more accurate representation
of
target aspect","memory networks||to repeatedly match||target aspect representation
target aspect representation||with||other aspects
target aspect representation||to generate||more accurate representation
more accurate representation||of||target aspect
",,"Model||employ||memory networks
",,,,,,"more accurate representation||name||refined representation
"
model,This refined representation is fed to a softmax classifier for final classification .,"refined representation
fed to
softmax classifier
for
final classification","refined representation||fed to||softmax classifier
softmax classifier||for||final classification
",,,,,,,,
,LSTM,LSTM,,,,,,,,,
baselines,"Following , the sentence is fed to along short - term memory ( LSTM ) network to propagate context among the constituent words .","sentence
fed to
short - term memory ( LSTM ) network
to propagate
context
among
constituent words","sentence||fed to||short - term memory ( LSTM ) network
short - term memory ( LSTM ) network||to propagate||context
context||among||constituent words
",,,,,,,"LSTM||has||sentence
",
baselines,TD- LSTM,TD- LSTM,,,,"Baselines||has||TD- LSTM
",,,,,"TD- LSTM||has||sequence
"
baselines,"Following , sequence of words preceding ( left context ) and succeeding ( right context ) target aspect term are fed to two different LSTMs .","sequence
of
words preceding ( left context ) and succeeding ( right context ) target aspect term
fed to
two different LSTMs","sequence||of||words preceding ( left context ) and succeeding ( right context ) target aspect term
words preceding ( left context ) and succeeding ( right context ) target aspect term||fed to||two different LSTMs
",,,,,,,,
baselines,"This representation is fed to softmax classifier. , ATAE - LSTM is identical to AE - LSTM , except the LSTM is fed with the concatenation of aspect - term representation and word representation . , target - aspect and its context are sent to two distinct LSTMs and the means of the hidden outputs are taken as intermediate aspect representation and context representation respectively .","ATAE - LSTM
identical to
AE - LSTM
except
LSTM
fed with
concatenation
of
aspect - term representation and word representation","ATAE - LSTM||identical to||AE - LSTM
AE - LSTM||except||LSTM
LSTM||fed with||concatenation
concatenation||of||aspect - term representation and word representation
",,,"Baselines||has||ATAE - LSTM
",,,,,
results,"It is evident from the results that our IARM model outperforms all the baseline models , including the state of the art , in both of the domains .","evident
our IARM model
outperforms
all the baseline models
including
state of the art
in both of
domains","our IARM model||outperforms||all the baseline models
all the baseline models||in both of||domains
all the baseline models||including||state of the art
",,"Results||evident||our IARM model
",,,,,,
results,"We obtained bigger improvement in laptop domain , of 1.7 % , compared to restaurant domain , of 1.4 % .","obtained
bigger improvement
in
laptop domain
of
1.7 %
compared to
restaurant domain
of
1.4 %","bigger improvement||in||laptop domain
laptop domain||of||1.7 %
laptop domain||compared to||restaurant domain
restaurant domain||of||1.4 %
",,"Results||obtained||bigger improvement
",,,,,,
research-problem,Representation learning ) plays a critical role in many modern machine learning systems .,Representation learning,,,,,"Contribution||has research problem||Representation learning
",,,,
research-problem,We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .,"sentiment analysis
attempt to learn
unsupervised representation
accurately contains","unsupervised representation||accurately contains||sentiment analysis
",,"Approach||attempt to learn||unsupervised representation
",,"Contribution||has research problem||sentiment analysis
",,,,
approach,"As an approach , we consider the popular research benchmark of byte ( character ) level language modelling due to its further simplicity and generality .","consider
research benchmark of byte ( character ) level language modelling
due to
its further simplicity and generality","research benchmark of byte ( character ) level language modelling||due to||its further simplicity and generality
",,"Approach||consider||research benchmark of byte ( character ) level language modelling
",,,,,,
approach,We train on a very large corpus picked to have a similar distribution as our task of interest .,"train on
very large corpus
picked to have
similar distribution
as
our task of interest","very large corpus||picked to have||similar distribution
similar distribution||as||our task of interest
",,"Approach||train on||very large corpus
",,,,,,
results,Review Sentiment Analysis,Review Sentiment Analysis,,,,"Results||has||Review Sentiment Analysis
",,,,,"Review Sentiment Analysis||has||The representation learned by our model
"
results,The representation learned by our model achieves 91.8 % significantly outperforming the state of the art of 90.2 % by a 30 model ensemble .,"The representation learned by our model
achieves
91.8 %
significantly outperforming
state of the art
of
90.2 %
by
30 model ensemble","The representation learned by our model||achieves||91.8 %
91.8 %||significantly outperforming||state of the art
state of the art||of||90.2 %
state of the art||by||30 model ensemble
",,,,,,,,
results,It matches the performance of baselines using as few as a dozen labeled examples and outperforms all previous results with only a few hundred labeled examples .,"matches
performance
of
baselines
using
dozen labeled examples
outperforms
all previous results
with
few hundred labeled examples","performance||using||dozen labeled examples
performance||of||baselines
all previous results||with||few hundred labeled examples
",,,,,"The representation learned by our model||matches||performance
The representation learned by our model||outperforms||all previous results
",,,
results,"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of 53.6 % on the fine - grained subtask , achieving 52.9 % .","on
does not reach
state of the art
of
53.6 %
fine - grained subtask
achieving
52.9 %","state of the art||achieving||52.9 %
state of the art||of||53.6 %
state of the art||on||fine - grained subtask
",,,,,"The representation learned by our model||does not reach||state of the art
",,,
results,L1 regularization is known to reduce sample complexity when there are many irrelevant features .,L1 regularization,,,,,,,,"Review Sentiment Analysis||has||L1 regularization
",
results,"Fitting a threshold to this single unit achieves a test accuracy of 92.30 % which outperforms a strong supervised results on the dataset , the 91.87 % of NB - SVM trigram , but is still below the semi-supervised state of the art of 94.09 % .","Fitting
threshold
achieves
test accuracy
of
92.30 %
outperforms
strong supervised results
91.87 %
of
NB - SVM trigram
still below
semi-supervised state of the art
of
94.09 %","threshold||achieves||test accuracy
test accuracy||of||92.30 %
test accuracy||outperforms||strong supervised results
91.87 %||of||NB - SVM trigram
91.87 %||still below||semi-supervised state of the art
semi-supervised state of the art||of||94.09 %
","threshold||has||91.87 %
",,,,"L1 regularization||Fitting||threshold
",,,
results,Using the full 4096 unit representation achieves 92.88 % .,"Using
full 4096 unit representation
achieves
92.88 %","full 4096 unit representation||achieves||92.88 %
",,,,,"Review Sentiment Analysis||Using||full 4096 unit representation
",,,
results,Capacity Ceiling,Capacity Ceiling,,,,"Results||has||Capacity Ceiling
",,,,,
results,We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in .,"try
approach
on
binary version
of
Yelp Dataset Challenge in 2015","approach||on||binary version
binary version||of||Yelp Dataset Challenge in 2015
",,,,,"Capacity Ceiling||try||approach
",,,
results,"Using the full dataset , we achieve 95 . 22 % test accuracy .","Using
full dataset
achieve
95 . 22 % test accuracy","full dataset||achieve||95 . 22 % test accuracy
",,,,,"Capacity Ceiling||Using||full dataset
",,,
results,The observed capacity ceiling is an interesting phenomena and stumbling point for scaling our unsupervised representations .,"observed
capacity ceiling
is
interesting phenomena and stumbling point
for
scaling our unsupervised representations","capacity ceiling||is||interesting phenomena and stumbling point
interesting phenomena and stumbling point||for||scaling our unsupervised representations
",,,,,"Capacity Ceiling||observed||capacity ceiling
",,,
results,"Additionally , there is a notable drop in the relative performance of our approach transitioning from sentence to document datasets .","there is
notable drop
in
relative performance
of
our approach
transitioning from
sentence to document datasets","notable drop||in||relative performance
relative performance||of||our approach
our approach||transitioning from||sentence to document datasets
",,,,,"Capacity Ceiling||there is||notable drop
",,,
results,"Finally , as the amount of labeled data increases , the performance of the simple linear model we train on top of our static representation will eventually saturate .","of
labeled data
increases
performance
simple linear model
train on top of
static representation
will eventually
saturate","performance||of||simple linear model
simple linear model||train on top of||static representation
performance||will eventually||saturate
","labeled data||has||increases
increases||has||performance
",,,,,,"Capacity Ceiling||has||labeled data
",
research-problem,SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,Targeted Aspect Based Sentiment Analysis,,,,,"Contribution||has research problem||Targeted Aspect Based Sentiment Analysis
",,,,
research-problem,"In this paper , we introduce the task of targeted aspect - based sentiment analysis .",targeted aspect - based sentiment analysis,,,,,"Contribution||has research problem||targeted aspect - based sentiment analysis
",,,,
research-problem,Sentiment analysis is an important task in natural language processing .,Sentiment analysis,,,,,"Contribution||has research problem||Sentiment analysis
",,,,
research-problem,"Aspect - based sentiment analysis ( ABSA ) relates to the task of extracting fine - grained information by identifying the polarity towards different aspects of an entity in the same unit of text , and recognizing the polarity associated with each aspect separately .",Aspect - based sentiment analysis ( ABSA ),,,,,"Contribution||has research problem||Aspect - based sentiment analysis ( ABSA )
",,,,
research-problem,Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences ( often a tweet ) .,Targeted sentiment analysis,,,,,"Contribution||has research problem||Targeted sentiment analysis
",,,,
dataset,SentiHood currently contains annotated sentences containing one or two location entity mentions .,"SentiHood
contains
annotated sentences
containing
one or two location entity mentions","SentiHood||contains||annotated sentences
annotated sentences||containing||one or two location entity mentions
",,,"Dataset||name||SentiHood
",,,,,
dataset,2 Sen-tiHood contains 5215 sentences with 3862 sentences containing a single location and 1353 sentences containing multiple ( two ) locations .,"5215 sentences
with
3862 sentences
containing
single location
1353 sentences
containing
multiple ( two ) locations","5215 sentences||with||3862 sentences
3862 sentences||containing||single location
5215 sentences||with||1353 sentences
1353 sentences||containing||multiple ( two ) locations
",,,,,,,"SentiHood||contains||5215 sentences
",
dataset,""" Positive "" sentiment is dominant for aspects such as dining and shopping .",""" Positive "" sentiment
dominant for
aspects
such as
dining and shopping",""" Positive "" sentiment||dominant for||aspects
aspects||such as||dining and shopping
",,,"Dataset||has||"" Positive "" sentiment
",,,,,
dataset,The general aspect is the most frequent aspect with over 2000 sentences while aspect touristy has occurred in less than 100 sentences .,"general aspect
is
most frequent aspect
with
over 2000 sentences
aspect touristy
occurred in
less than 100 sentences","aspect touristy||occurred in||less than 100 sentences
general aspect||is||most frequent aspect
most frequent aspect||with||over 2000 sentences
",,,"Dataset||has||aspect touristy
Dataset||has||general aspect
",,,,,
dataset,"Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .","since
each sentence
can contain
one or more opinions
total number of opinions ( 5920 )
in
dataset
higher than
number of sentences","total number of opinions ( 5920 )||higher than||number of sentences
number of sentences||since||each sentence
each sentence||can contain||one or more opinions
total number of opinions ( 5920 )||in||dataset
",,,"Dataset||has||total number of opinions ( 5920 )
",,,,,
dataset,"Location entity names are masked by location1 and location 2 in the whole dataset , so the task does not involve identification and segmentation of the named entities .","Location entity names
masked by
location1 and location 2
in
whole dataset","Location entity names||masked by||location1 and location 2
location1 and location 2||in||whole dataset
",,,"Dataset||has||Location entity names
",,,,,
baselines,Logistic Regression,Logistic Regression,,,,"Baselines||has||Logistic Regression
",,,,,
baselines,"Many existing works in the aspect - based sentiment analysis task , 3 use a classifier , such as logistic regression or SVM , based on linguistic features such as n-grams , POS information or more hand - engineered features .","such as
based on
linguistic features
n-grams
POS information
more hand - engineered features","linguistic features||such as||n-grams
linguistic features||such as||POS information
linguistic features||such as||more hand - engineered features
",,,,,"Logistic Regression||based on||linguistic features
",,,
baselines,"More concretely , we define the following sparse representations of locations :","define
sparse representations of locations",,,,,,"Logistic Regression||define||sparse representations of locations
",,,"sparse representations of locations||name||Mask target entity n-grams
sparse representations of locations||name||Left - right n- grams
sparse representations of locations||name||Left right pooling
"
baselines,Mask target entity n-grams :,Mask target entity n-grams,,,,,,,,,
baselines,Left - right n- grams : we create an n-gram representation for both the right and the left context around each location mention .,Left - right n- grams,,,,,,,,,
baselines,Left right pooling :,Left right pooling,,,,,,,,,
baselines,Long Short - Term Memory ( LSTM ),Long Short - Term Memory ( LSTM ),,,,"Baselines||has||Long Short - Term Memory ( LSTM )
",,,,,
baselines,"Inspired by the recent success of applying deep neural networks on language tasks , we use a bidirectional LSTM to learn a classifier for each of the aspects .","use
bidirectional LSTM
to learn
classifier
for
each of the aspects","bidirectional LSTM||to learn||classifier
classifier||for||each of the aspects
",,,,,"Long Short - Term Memory ( LSTM )||use||bidirectional LSTM
",,,
baselines,Representations for a location ( e l ) are obtained using one of the following two approaches :,"Representations
for
location ( e l )
using","Representations||for||location ( e l )
",,,,,,"location ( e l )||using||Final output state ( LSTM - Final )
location ( e l )||using||Location output state ( LSTM - Location )
","Long Short - Term Memory ( LSTM )||has||Representations
",
baselines,Final output state ( LSTM - Final ) : e l is the output embedding of the bidirectional LSTM .,Final output state ( LSTM - Final ),,,,,,,,,
baselines,Location output state ( LSTM - Location ) :,Location output state ( LSTM - Location ),,,,,,,,,
results,"As we can see , the n-gram representation with location masking achieves slightly better results over the left - right context .","see
n-gram representation
with
location masking
achieves
slightly better results
over
left - right context","n-gram representation||with||location masking
n-gram representation||achieves||slightly better results
slightly better results||over||left - right context
",,"Results||see||n-gram representation
",,,,,,
results,"Also , by adding POS information , we gain an increase in the performance .","by adding
POS information
gain
increase
in
performance","POS information||gain||increase
increase||in||performance
",,"Results||by adding||POS information
",,,,,,
results,"Separating the left and the right context ( LR - Left - Right ) for BoW representation , does not improve the performance .","Separating
left and the right context ( LR - Left - Right )
for
BoW representation
does not improve
performance","left and the right context ( LR - Left - Right )||does not improve||performance
left and the right context ( LR - Left - Right )||for||BoW representation
",,"Results||Separating||left and the right context ( LR - Left - Right )
",,,,,,
results,"Amongst the two variations of LSTM , the model with final state embeddings does slightly better than the model where we use the embeddings at the location index , however they are not significantly different ( with a p valueless than 0.01 ) .","Amongst
two variations of LSTM
model
with
final state embeddings
does
slightly better
than
model
use
embeddings at the location index","model||with||final state embeddings
final state embeddings||does||slightly better
slightly better||than||model
model||use||embeddings at the location index
","two variations of LSTM||has||model
","Results||Amongst||two variations of LSTM
",,,,,,
results,"It is interesting to note that the best LSTM model is not superior to logistic regression model , especially in terms of AUC .","interesting to note
best LSTM model
not superior to
logistic regression model
in terms of
AUC","best LSTM model||not superior to||logistic regression model
best LSTM model||in terms of||AUC
",,"Results||interesting to note||best LSTM model
",,,,,,
results,Another interesting observation is that the F 1 measure for logistic regression model with n-grams and POS information is very low while this model 's performance is superior to other models in terms of AUC .,"interesting observation is that
F 1 measure
for
logistic regression model
with
n-grams and POS information
is
very low
performance
is
superior
to
other models
in terms of
AUC","logistic regression model||with||n-grams and POS information
logistic regression model||interesting observation is that||performance
performance||is||superior
superior||to||other models
other models||in terms of||AUC
logistic regression model||interesting observation is that||F 1 measure
F 1 measure||is||very low
",,"Results||for||logistic regression model
",,,,,,
research-problem,ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection,Multimodal Emotion Detection,,,,,"Contribution||has research problem||Multimodal Emotion Detection
",,,,
research-problem,Emotion recognition in conversations is crucial for building empathetic machines .,Emotion recognition in conversations,,,,,"Contribution||has research problem||Emotion recognition in conversations
",,,,
research-problem,"Analyzing emotional dynamics in conversations , however , poses complex challenges .",Analyzing emotional dynamics in conversations,,,,,"Contribution||has research problem||Analyzing emotional dynamics in conversations
",,,,
model,"We propose Interactive COnversational memory Network ( ICON ) , a multimodal network for identifying emotions in utterance - videos .","propose
Interactive COnversational memory Network ( ICON )
multimodal network
for identifying
emotions
in
utterance - videos","multimodal network||for identifying||emotions
emotions||in||utterance - videos
","Interactive COnversational memory Network ( ICON )||has||multimodal network
","Model||propose||Interactive COnversational memory Network ( ICON )
",,,,,,
model,"First , it extracts multimodal features from all utterancevideos .","extracts
multimodal features
from
all utterancevideos","multimodal features||from||all utterancevideos
",,"Model||extracts||multimodal features
",,,,,,
model,"Next , given a test utterance to be classified , ICON considers the preceding utterances of both speakers falling within a context - window and models their self - emotional influences using local gated recurrent units .","given
test utterance
to be
classified
ICON
considers
preceding utterances
of
both speakers
falling within
context - window
models
self - emotional influences
using
local gated recurrent units","ICON||considers||preceding utterances
preceding utterances||of||both speakers
both speakers||falling within||context - window
ICON||models||self - emotional influences
self - emotional influences||using||local gated recurrent units
test utterance||to be||classified
","test utterance||has||ICON
","Model||given||test utterance
",,,,,,
model,"Furthermore , to incorporate inter -speaker influences , a global representation is generated using a GRU that intakes output of the local GRUs .","to incorporate
inter -speaker influences
global representation
is
generated
using
GRU
intakes
output
of
local GRUs","global representation||is||generated
generated||using||GRU
GRU||intakes||output
output||of||local GRUs
","inter -speaker influences||has||global representation
","Model||to incorporate||inter -speaker influences
",,,,,,
model,"For each instance in the context - window , the output of this global GRU is stored as a memory cell .","For
each instance
in
context - window
output
of
global GRU
stored as
memory cell","each instance||in||context - window
output||of||global GRU
global GRU||stored as||memory cell
","context - window||has||output
","Model||For||each instance
",,,,,,
model,These memories are then subjected to multiple read / write cycles that include attention mechanism for generating contextual summaries of the conversational history .,"memories
subjected to
multiple read / write cycles
that include
attention mechanism
for generating
contextual summaries
of
conversational history","memories||subjected to||multiple read / write cycles
multiple read / write cycles||that include||attention mechanism
attention mechanism||for generating||contextual summaries
contextual summaries||of||conversational history
",,,"Model||has||memories
",,,,,
model,"At each iteration , the representation of the test utterance is improved with this summary representation and finally used for prediction .","At
each iteration
representation of the test utterance
improved with
summary representation
used for
prediction","representation of the test utterance||improved with||summary representation
representation of the test utterance||used for||prediction
","each iteration||has||representation of the test utterance
","Model||At||each iteration
",,,,,,
experimental-setup,20 % of the training set is used as validation set for hyper - parameter tuning .,"20 % of the training set
used as
validation set
for
hyper - parameter tuning","20 % of the training set||used as||validation set
validation set||for||hyper - parameter tuning
",,,"Experimental setup||has||20 % of the training set
",,,,,
experimental-setup,"We use the Adam optimizer ( Kingma and Ba , 2014 ) for training the parameters starting with an initial learning rate of 0.001 .","use
Adam optimizer ( Kingma and Ba , 2014 )
for
training
parameters
starting with
initial learning rate
of
0.001","Adam optimizer ( Kingma and Ba , 2014 )||for||training
parameters||starting with||initial learning rate
initial learning rate||of||0.001
","training||has||parameters
","Experimental setup||use||Adam optimizer ( Kingma and Ba , 2014 )
",,,,,,
experimental-setup,Termination of the training - phase is decided by early - stopping with a patience of 10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs .,"Termination
of
training - phase
decided by
early - stopping
with
patience
of
10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs","Termination||of||training - phase
training - phase||decided by||early - stopping
early - stopping||with||patience
patience||of||10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs
",,,"Experimental setup||has||Termination
",,,,,
experimental-setup,The network is subjected to regularization in the form of Dropout and Gradient - clipping for a norm of 40 .,"network
subjected to
regularization
in the form of
Dropout and Gradient - clipping
for
norm
of
40","network||subjected to||regularization
regularization||for||norm
norm||of||40
regularization||in the form of||Dropout and Gradient - clipping
",,,"Experimental setup||has||network
",,,,,
experimental-setup,"Finally , the best hyper - parameters are decided using a gridsearch .","best hyper - parameters
decided using
gridsearch","best hyper - parameters||decided using||gridsearch
",,,"Experimental setup||has||best hyper - parameters
",,,,,
experimental-setup,"For multimodal feature extraction , we explore different designs for the employed CNNs .","For
multimodal feature extraction
explore
different designs
for
employed CNNs","multimodal feature extraction||explore||different designs
different designs||for||employed CNNs
",,"Experimental setup||For||multimodal feature extraction
",,,,,,
experimental-setup,"For text , we find the single layer CNN to perform at par with deeper variants .","text
find
single layer CNN
to perform at par with
deeper variants","text||find||single layer CNN
single layer CNN||to perform at par with||deeper variants
",,,"Experimental setup||For||text
",,,,,
experimental-setup,"For visual features , however , a deeper CNN provides better representations .","visual features
deeper CNN
provides
better representations","deeper CNN||provides||better representations
","visual features||has||deeper CNN
",,"Experimental setup||For||visual features
",,,,,
experimental-setup,We also find that contextually conditioned features perform better than context - less features .,"find
contextually conditioned features
perform better than
context - less features","contextually conditioned features||perform better than||context - less features
",,"Experimental setup||find||contextually conditioned features
",,,,,,
baselines,memnet is an end - toend memory network .,"memnet
is
end - toend memory network","memnet||is||end - toend memory network
",,,"Baselines||has||memnet
",,,,,
baselines,cLSTM 4 classifies utterances using neighboring utterances ( of same speaker ) as context .,"cLSTM
classifies
utterances
using
neighboring utterances ( of same speaker )
as
context","cLSTM||classifies||utterances
utterances||using||neighboring utterances ( of same speaker )
neighboring utterances ( of same speaker )||as||context
",,,"Baselines||has||cLSTM
",,,,,
baselines,"TFN 5 models intra-and intermodality dynamics by explicitly aggregating uni - , bi- and trimodal interactions .","TFN
models
intra-and intermodality dynamics
by explicitly aggregating
uni - , bi- and trimodal interactions","TFN||models||intra-and intermodality dynamics
intra-and intermodality dynamics||by explicitly aggregating||uni - , bi- and trimodal interactions
",,,"Baselines||has||TFN
",,,,,
baselines,"MFN performs multi-view learning by using Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .","MFN
performs
multi-view learning
by using
Delta - memory Attention Network
fusion mechanism
to learn
cross - view interactions","MFN||performs||multi-view learning
multi-view learning||by using||Delta - memory Attention Network
fusion mechanism||to learn||cross - view interactions
","Delta - memory Attention Network||has||fusion mechanism
",,"Baselines||has||MFN
",,,,,
baselines,CMN models separate contexts for both speaker and listener to an utterance .,"CMN models
separate
contexts
for
speaker and listener
to an
utterance","CMN models||separate||contexts
contexts||to an||utterance
contexts||for||speaker and listener
",,,"Baselines||has||CMN models
",,,,,
results,ICON performs better than the compared models with significant performance increase in emotions ( ? 2.1 % acc. ) .,"ICON
performs better than
compared models
with
significant performance increase
in
emotions ( ? 2.1 % acc. )","ICON||performs better than||compared models
compared models||with||significant performance increase
significant performance increase||in||emotions ( ? 2.1 % acc. )
",,,"Results||has||ICON
",,,,,
results,"For each emotion , ICON outperforms all the compared models except for happiness emotion .","For
each emotion
ICON
outperforms
all the compared models
except for
happiness emotion","ICON||outperforms||all the compared models
all the compared models||except for||happiness emotion
","each emotion||has||ICON
","Results||For||each emotion
",,,,,,"ICON||has||performance
"
results,"However , its performance is still at par with c LSTM without a significant gap .","performance
at par with
c LSTM
without
significant gap","performance||at par with||c LSTM
performance||without||significant gap
",,,,,,,,
results,"Also , ICON manages to correctly identify the relatively similar excitement emotion by a large margin .","manages to
correctly
identify
relatively similar excitement emotion
by
large margin","identify||correctly||relatively similar excitement emotion
relatively similar excitement emotion||by||large margin
",,,,,"ICON||manages to||identify
",,,
results,"In all the labels , ICON attains improved performance over its counterparts , suggesting the efficacy of its context - modeling scheme .","In
labels
ICON
attains
improved performance
over
counterparts","ICON||attains||improved performance
improved performance||over||counterparts
","labels||has||ICON
","Results||In||labels
",,,,,,
results,presents the results for different combinations of modes used by ICON on IEMOCAP .,"presents
different combinations of modes
used by
ICON
on
IEMOCAP","different combinations of modes||used by||ICON
ICON||on||IEMOCAP
",,"Results||presents||different combinations of modes
",,,,,,
results,"As seen , the trimodal network provides the best performance which is preceded by the bimodal variants .","trimodal network
provides
best performance
preceded by
bimodal variants","trimodal network||provides||best performance
best performance||preceded by||bimodal variants
",,,"Results||has||trimodal network
",,,,,
results,"Among unimodals , language modality performs the best , reaffirming its significance in multimodal systems .","Among
unimodals
language modality
performs
best","language modality||performs||best
","unimodals||has||language modality
","Results||Among||unimodals
",,,,,,
results,"Interestingly , the audio and visual modality , on their own , do not provide good performance , but when used with text , complementary data is shared to improve over all performance .","audio and visual modality
when used with
text
complementary data
shared
to improve
over all performance","audio and visual modality||when used with||text
text||shared||complementary data
complementary data||to improve||over all performance
",,,"Results||has||audio and visual modality
",,,,,
ablation-analysis,Self vs Dual History :,Self vs Dual History,,,,"Ablation analysis||has||Self vs Dual History
",,,,,
ablation-analysis,"Compared to the dual - history variants ( variants 3 , 5 , and 7 ) , these models provide lesser performance .","Compared to
dual - history variants ( variants 3 , 5 , and 7 )
these models
provide
lesser performance","these models||provide||lesser performance
","dual - history variants ( variants 3 , 5 , and 7 )||has||these models
",,,,"Self vs Dual History||Compared to||dual - history variants ( variants 3 , 5 , and 7 )
",,,
ablation-analysis,DGIM prevents the storage of dynamic influences between speakers at each historical time step and leads to performance deterioration .,"DGIM
prevents
storage of dynamic influences
between
speakers
at each
historical time step
leads to
performance deterioration","DGIM||prevents||storage of dynamic influences
storage of dynamic influences||between||speakers
speakers||leads to||performance deterioration
speakers||at each||historical time step
",,,"Ablation analysis||has||DGIM
",,,,,
ablation-analysis,"Multi - hop vs No - hop : Variants 2 and 3 represent cases where multi-hop is omitted , i.e. , R = 1 . Performance for them are poorer than variants having multi-hop mechanism ( variants 4 - 7 ) .","Multi - hop vs No - hop
multi-hop
than",,"Multi - hop vs No - hop||removal of||multi-hop
",,"Ablation analysis||has||Multi - hop vs No - hop
",,,,,
ablation-analysis,"Also , removal of multi-hop leads to worse performance than the removal of DGIM .","removal of
leads to
worse performance
removal of DGIM",,"worse performance||than||removal of DGIM
",,,,"multi-hop||leads to||worse performance
",,,
ablation-analysis,"However , best performance is achieved by variant 6 which contains all the proposed modules in its pipeline .","best performance
achieved by
variant 6
which contains
all the proposed modules
in
its pipeline","best performance||achieved by||variant 6
variant 6||which contains||all the proposed modules
all the proposed modules||in||its pipeline
",,,,,,,"Multi - hop vs No - hop||has||best performance
",
research-problem,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,Aspect Sentiment Analysis,,,,,"Contribution||has research problem||Aspect Sentiment Analysis
",,,,
model,"In this paper , we propose a novel framework to solve the above problems in target sentiment analysis .","propose
novel framework
to solve
problems
in
target sentiment analysis","novel framework||to solve||problems
problems||in||target sentiment analysis
",,"Model||propose||novel framework
",,,,,,
model,"Specifically , our framework first adopts a bidirectional LSTM ( BLSTM ) to produce the memory ( i.e. the states of time steps generated by LSTM ) from the input , as bidirectional recurrent neural networks ( RNNs ) were found effective for a similar purpose in machine translation .","framework
first adopts
bidirectional LSTM ( BLSTM )
to produce
memory ( i.e. the states of time steps generated by LSTM )
from
input","framework||first adopts||bidirectional LSTM ( BLSTM )
bidirectional LSTM ( BLSTM )||to produce||memory ( i.e. the states of time steps generated by LSTM )
memory ( i.e. the states of time steps generated by LSTM )||from||input
",,,"Model||has||framework
",,,,,
model,"The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .","memory slices
weighted according to
relative positions
to
target","memory slices||weighted according to||relative positions
relative positions||to||target
",,,"Model||has||memory slices
",,,,,
model,"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .","pay
multiple attentions
on
position - weighted memory
nonlinearly combine
attention results
with
recurrent network , i.e. GRUs","attention results||with||recurrent network , i.e. GRUs
multiple attentions||on||position - weighted memory
",,"Model||nonlinearly combine||attention results
Model||pay||multiple attentions
",,,,,,
model,"Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .","apply
softmax
on
output
of
GRU network
to predict
sentiment
on
target","softmax||to predict||sentiment
sentiment||on||target
softmax||on||output
output||of||GRU network
",,"Model||apply||softmax
",,,,,,
model,Our framework introduces a novel way of applying multiple - attention mechanism to synthesize important features in difficult sentence structures .,"Our framework
introduces
novel way
of applying
multiple - attention mechanism
to synthesize
important features
in
difficult sentence structures","Our framework||introduces||novel way
novel way||of applying||multiple - attention mechanism
multiple - attention mechanism||to synthesize||important features
important features||in||difficult sentence structures
",,,"Model||has||Our framework
",,,,,
baselines,Average Context :,Average Context,,,,"Baselines||has||Average Context
",,,,,"Average Context||has||first one
Average Context||has||second one
"
baselines,There are two versions of this method .,of,,,,,,,,,"word vectors||of||full context
"
baselines,"The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .","first one
named
AC - S
averages
word vectors
before
target
word vectors
after
target","first one||named||AC - S
first one||averages||word vectors
word vectors||before||target
word vectors||after||target
",,,,,"second one||averages||word vectors
",,,
baselines,"The second one , named AC , averages the word vectors of the full context .","second one
named
AC
averages
full context","second one||named||AC
",,,,,,,,
baselines,"SVM : The traditional state - of - the - art method using SVMs on surface features , lexicon features and parsing features , which is the best team in SemEval 2014 .","SVM
on
surface features , lexicon features and parsing features","SVM||on||surface features , lexicon features and parsing features
",,,"Baselines||has||SVM
",,,,,
baselines,"Rec - NN : It firstly uses rules to transform the dependency tree and put the opinion target at the root , and then performs semantic composition with Recursive NNs for sentiment prediction .","Rec - NN
firstly uses
rules
to transform
dependency tree
put
opinion target
at
root
performs
semantic composition
with
Recursive NNs
for
sentiment prediction","Rec - NN||performs||semantic composition
semantic composition||with||Recursive NNs
Recursive NNs||for||sentiment prediction
Rec - NN||firstly uses||rules
rules||to transform||dependency tree
Rec - NN||put||opinion target
opinion target||at||root
",,,"Baselines||has||Rec - NN
",,,,,
baselines,TD- LSTM : It uses a forward LSTM and a backward LSTM to abstract the information before and after the target .,"TD- LSTM
uses
forward LSTM and a backward LSTM
to abstract
information
before and after
target","TD- LSTM||uses||forward LSTM and a backward LSTM
forward LSTM and a backward LSTM||to abstract||information
information||before and after||target
",,,"Baselines||has||TD- LSTM
",,,,,
baselines,TD - LSTM - A : We developed TD - LSTM to make it have one attention on the outputs of 3 https://github.com/svn2github/word2vec,"TD - LSTM - A
developed
TD - LSTM
have
one attention
on
outputs","TD - LSTM - A||developed||TD - LSTM
TD - LSTM||have||one attention
one attention||on||outputs
",,,"Baselines||has||TD - LSTM - A
",,,,,
baselines,"forward and backward LSTMs , respectively .",,,,,,,,,,
baselines,"MemNet : It applies attention multiple times on the word embeddings , and the last attention 's output is fed to softmax for prediction , without combining the results of different attentions .","MemNet
applies
attention
multiple times
on
word embeddings
last attention 's output
fed to
softmax
for
prediction","MemNet||applies||attention
attention||on||word embeddings
last attention 's output||fed to||softmax
softmax||for||prediction
","attention||has||multiple times
MemNet||has||last attention 's output
",,"Baselines||has||MemNet
",,,,,
results,"As shown by the results in , our RAM consistently outperforms all compared methods on these four datasets .","our RAM
consistently outperforms
compared methods","our RAM||consistently outperforms||compared methods
",,,"Results||has||our RAM
",,,,,
results,"AC and AC - S perform poorly , because averaging context is equivalent to paying identical attention to each word which would hide the true sentiment word .","AC and AC - S
perform
poorly","AC and AC - S||perform||poorly
",,,"Results||has||AC and AC - S
",,,,,
results,Rec - NN is better than TD - LSTM but not as good as our method .,"Rec - NN
better than
TD - LSTM
not as good as
our method","Rec - NN||better than||TD - LSTM
Rec - NN||not as good as||our method
",,,"Results||has||Rec - NN
",,,,,
results,"TD - LSTM performs less competitive than our method on all the datasets , particularly on the tweet dataset , because in this dataset sentiment words are usually far from person names , for which case the multiple - attention mechanism is designed to work .","TD - LSTM
performs
less competitive
than
our method","TD - LSTM||performs||less competitive
less competitive||than||our method
",,,"Results||has||TD - LSTM
",,,,,
results,"TD - LSTM - A also performs worse than our method , because it s two attentions , i.e. one for the text before the target and the other for the after , can not tackle some cases where more than one features being attended are at the same side of the target .","TD - LSTM - A
performs
worse
than
our method","TD - LSTM - A||performs||worse
worse||than||our method
",,,"Results||has||TD - LSTM - A
",,,,,
results,"MemNet adopts multiple attentions in order to improve the attention results , given the assumption that the result of an attention at a later hop should be better than that at the beginning .","MemNet
adopts
multiple attentions
in order to improve
attention results","MemNet||adopts||multiple attentions
multiple attentions||in order to improve||attention results
",,,"Results||has||MemNet
",,,,,
research-problem,Attention - based LSTM for Aspect - level Sentiment Classification,Aspect - level Sentiment Classification,,,,,"Contribution||has research problem||Aspect - level Sentiment Classification
",,,,
research-problem,Aspect - level sentiment classification is a finegrained task in sentiment analysis .,sentiment analysis,,,,,"Contribution||has research problem||sentiment analysis
",,,,
research-problem,"In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect .",aspect - level sentiment classification,,,,,"Contribution||has research problem||aspect - level sentiment classification
",,,,
model,"In this paper , we propose an attention mechanism to enforce the model to attend to the important part of a sentence , in response to a specific aspect .","propose
attention mechanism
to enforce
model
to attend
important part of a sentence","attention mechanism||to attend||important part of a sentence
attention mechanism||to enforce||model
",,"Model||propose||attention mechanism
",,,,,,
model,We design an aspect - tosentence attention mechanism that can concentrate on the key part of a sentence given the aspect .,"design
aspect - tosentence attention mechanism
can concentrate on
key part
of
sentence
given
aspect","aspect - tosentence attention mechanism||can concentrate on||key part
key part||given||aspect
key part||of||sentence
",,"Model||design||aspect - tosentence attention mechanism
",,,,,,
model,We explore the potential correlation of aspect and sentiment polarity in aspect - level sentiment classification .,"explore
potential correlation
of
aspect and sentiment polarity
in
aspect - level sentiment classification","potential correlation||of||aspect and sentiment polarity
aspect and sentiment polarity||in||aspect - level sentiment classification
",,"Model||explore||potential correlation
",,,,,,
model,"In order to capture important information in response to a given aspect , we design an attentionbased LSTM .","to capture
important information
in response to
given aspect
design
attentionbased LSTM","important information||in response to||given aspect
","design||has||attentionbased LSTM
","Model||to capture||important information
Model||to capture||design
",,,,,,
hyperparameters,We apply the proposed model to aspect - level sentiment classification .,"apply
proposed model
to
aspect - level sentiment classification","proposed model||to||aspect - level sentiment classification
",,"Hyperparameters||apply||proposed model
",,,,,,
hyperparameters,"In our experiments , all word vectors are initialized by Glove 1 .","In
our experiments
all word vectors
are
initialized
by
Glove","all word vectors||are||initialized
initialized||by||Glove
","our experiments||has||all word vectors
","Hyperparameters||In||our experiments
",,,,,,
hyperparameters,The word embedding vectors are pre-trained on an unlabeled corpus whose size is about 840 billion .,"word embedding vectors
pre-trained on
unlabeled corpus
whose
size
is
about 840 billion","word embedding vectors||pre-trained on||unlabeled corpus
unlabeled corpus||whose||size
size||is||about 840 billion
",,,"Hyperparameters||has||word embedding vectors
",,,,,
hyperparameters,"The other parameters are initialized by sampling from a uniform distribution U (?? , ? ) .","other parameters
initialized by
sampling
from
uniform distribution U (?? , ? )","other parameters||initialized by||sampling
sampling||from||uniform distribution U (?? , ? )
",,,"Hyperparameters||has||other parameters
",,,,,
hyperparameters,"The dimension of word vectors , aspect embeddings and the size of hidden layer are 300 .","dimension
of
word vectors
aspect embeddings
size of hidden layer
are
300","dimension||are||300
300||of||word vectors
300||of||aspect embeddings
300||of||size of hidden layer
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,The length of attention weights is the same as the length of sentence .,"length
of
attention weights
same as
length
of
sentence","length||of||attention weights
attention weights||same as||length
length||of||sentence
",,,"Hyperparameters||has||length
",,,,,
hyperparameters,Theano is used for implementing our neural network models .,"Theano
used for
implementing
neural network models","Theano||used for||implementing
","implementing||has||neural network models
",,"Hyperparameters||has||Theano
",,,,,
hyperparameters,"We trained all models with a batch size of 25 examples , and a momentum of 0.9 , L 2 - regularization weight of 0.001 and initial learning rate of 0.01 for AdaGrad .","trained
all models
with
batch size
of
25 examples
momentum
of
0.9
L 2 - regularization weight
of
0.001
initial learning rate
of
0.01
for
AdaGrad","all models||with||L 2 - regularization weight
L 2 - regularization weight||of||0.001
all models||with||initial learning rate
initial learning rate||of||0.01
0.01||for||AdaGrad
all models||with||batch size
batch size||of||25 examples
all models||with||momentum
momentum||of||0.9
",,"Hyperparameters||trained||all models
",,,,,,
results,"LSTM : Standard LSTM can not capture any aspect information in sentence , so it must get the same ( a ) the aspect of this sentence : service ( b ) the aspect of this sentence : food : Attention Visualizations .","LSTM
Standard LSTM
can not capture
aspect information
in
sentence","Standard LSTM||can not capture||aspect information
aspect information||in||sentence
","LSTM||has||Standard LSTM
",,"Results||has||LSTM
",,,,,
results,"Since it can not take advantage of the aspect information , not surprisingly the model has worst performance .","can not take advantage of
aspect information
worst performance",,"aspect information||has||worst performance
",,,,"LSTM||can not take advantage of||aspect information
",,,
results,TD - LSTM : TD - LSTM can improve the performance of sentiment classifier by treating an aspect as a target .,"TD - LSTM
can improve
performance
of
sentiment classifier
by treating
aspect
as
target","TD - LSTM||can improve||performance
performance||of||sentiment classifier
performance||by treating||aspect
aspect||as||target
",,,"Results||has||TD - LSTM
",,,,,
results,"Since there is no attention mechanism in TD - LSTM , it can not "" know "" which words are important for a given aspect .","there is no
attention mechanism
in
TD - LSTM
can not
know
which
words
are
important
for
given aspect","TD - LSTM||there is no||attention mechanism
attention mechanism||in||TD - LSTM
TD - LSTM||can not||know
know||which||words
words||are||important
important||for||given aspect
",,,,,,,,
results,It is worth noting that TC - LSTM performs worse than LSTM and TD - LSTM in .,"TC - LSTM
performs
worse
than
LSTM and TD - LSTM","TC - LSTM||performs||worse
worse||than||LSTM and TD - LSTM
",,,"Results||has||TC - LSTM
",,,,,
results,"ATAE - LSTM not only addresses the shortcoming of the unconformity between word vectors and aspect embeddings , but also can capture the most important information in response to a given aspect .","ATAE - LSTM
addresses
shortcoming
of
unconformity
between
word vectors and aspect embeddings
capture
most important information
in response to
given aspect","ATAE - LSTM||addresses||shortcoming
shortcoming||of||unconformity
unconformity||between||word vectors and aspect embeddings
ATAE - LSTM||capture||most important information
most important information||in response to||given aspect
",,,"Results||has||ATAE - LSTM
",,,,,
research-problem,Aspect Level Sentiment Classification with Deep Memory Network,Aspect Level Sentiment Classification,,,,,"Contribution||has research problem||Aspect Level Sentiment Classification
",,,,
research-problem,Aspect level sentiment classification is a fundamental task in the field of sentiment analysis .,sentiment analysis,,,,,"Contribution||has research problem||sentiment analysis
",,,,
approach,"In pursuit of this goal , we develop deep memory network for aspect level sentiment classification , which is inspired by the recent success of computational models with attention mechanism and explicit memory .","develop
deep memory network
for
aspect level sentiment classification","deep memory network||for||aspect level sentiment classification
",,"Approach||develop||deep memory network
",,,,,,
approach,"Our approach is data - driven , computationally efficient and does not rely on syntactic parser or sentiment lexicon .","is
data - driven
computationally efficient",,,"Approach||is||data - driven
Approach||is||computationally efficient
",,,,,,
approach,The approach consists of multiple computational layers with shared parameters .,"consists of
multiple computational layers
with
shared parameters","multiple computational layers||with||shared parameters
",,"Approach||consists of||multiple computational layers
",,,,,,
approach,"Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information to calculate continuous text representation .","Each layer
is
content - and location - based attention model
first learns
importance / weight
of
each context word
utilizes
information
to calculate
continuous text representation","Each layer||is||content - and location - based attention model
content - and location - based attention model||first learns||importance / weight
importance / weight||of||each context word
content - and location - based attention model||utilizes||information
information||to calculate||continuous text representation
",,,"Approach||has||Each layer
",,,,,
approach,The text representation in the last layer is regarded as the feature for sentiment classification .,"text representation
in
last layer
regarded as
feature
for
sentiment classification","text representation||in||last layer
last layer||regarded as||feature
feature||for||sentiment classification
",,,"Approach||has||text representation
",,,,,
approach,"As every component is differentiable , the entire model could be efficiently trained end - toend with gradient descent , where the loss function is the cross - entropy error of sentiment classification .","every component
is
differentiable
entire model
could be efficiently trained
end - toend
with
gradient descent
where
loss function
is
cross - entropy error
of
sentiment classification","every component||is||differentiable
entire model||could be efficiently trained||end - toend
end - toend||with||gradient descent
gradient descent||where||loss function
loss function||is||cross - entropy error
cross - entropy error||of||sentiment classification
","every component||has||entire model
",,"Approach||has||every component
",,,,,
baselines,"( 1 ) Majority is a basic baseline method , which assigns the majority sentiment label in training set to each instance in the test set .","Majority
is
basic baseline method
assigns
majority sentiment label
in
training set
to each
instance
in
test set","Majority||is||basic baseline method
Majority||assigns||majority sentiment label
majority sentiment label||in||training set
training set||to each||instance
instance||in||test set
",,,"Baselines||has||Majority
",,,,,
baselines,( 2 ) Feature - based SVM performs state - of - the - art on aspect level sentiment classification .,"Feature - based SVM
performs
state - of - the - art
on
aspect level sentiment classification","Feature - based SVM||performs||state - of - the - art
state - of - the - art||on||aspect level sentiment classification
",,,"Baselines||has||Feature - based SVM
",,,,,
baselines,"( 3 ) We compare with three LSTM models ( Tang et al. , 2015 a ) ) .",three LSTM models,,,,"Baselines||has||three LSTM models
",,,,,"three LSTM models||In||TDLSTM
three LSTM models||In||TDLSTM + ATT
"
baselines,"In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .","In
LSTM
LSTM based recurrent model
applied from
start
to
end
of
sentence
last hidden vector
used as
sentence representation","LSTM based recurrent model||applied from||start
start||to||end
end||of||sentence
last hidden vector||used as||sentence representation
","LSTM||has||LSTM based recurrent model
LSTM||has||last hidden vector
",,,,"three LSTM models||In||LSTM
",,,
baselines,"TDLSTM extends LSTM by taking into account of the aspect , and uses two LSTM networks , a forward one and a backward one , towards the aspect .","TDLSTM
extends
LSTM
by taking into account
aspect
uses
two LSTM networks
forward one and a backward one
towards
aspect","TDLSTM||extends||LSTM
TDLSTM||by taking into account||aspect
TDLSTM||uses||two LSTM networks
two LSTM networks||towards||aspect
","two LSTM networks||has||forward one and a backward one
",,,,,,,
baselines,"TDLSTM + ATT extends TDLSTM by incorporating an attention mechanism ( Bahdanau et al. , 2015 ) over the hidden vectors .","TDLSTM + ATT
extends
TDLSTM
by incorporating
attention mechanism
over
hidden vectors","TDLSTM + ATT||by incorporating||attention mechanism
attention mechanism||over||hidden vectors
TDLSTM + ATT||extends||TDLSTM
",,,,,,,,
baselines,We use the same Glove word vectors for fair comparison .,"use
same Glove word vectors
for
fair comparison","same Glove word vectors||for||fair comparison
",,"Baselines||use||same Glove word vectors
",,,,,,
baselines,"( 4 ) We also implement ContextAVG , a simplistic version of our approach .","implement
ContextAVG
simplistic version
of
our approach","simplistic version||of||our approach
","ContextAVG||has||simplistic version
","Baselines||implement||ContextAVG
",,,,,,
results,"We can find that feature - based SVM is an extremely strong performer and substantially outperforms other baseline methods , which demonstrates the importance of a powerful feature representation for aspect level sentiment classification .","find that
feature - based SVM
is
extremely strong performer
substantially outperforms
other baseline methods","feature - based SVM||substantially outperforms||other baseline methods
feature - based SVM||is||extremely strong performer
",,"Results||find that||feature - based SVM
",,,,,,
results,"Among three recurrent models , TDLSTM performs better than LSTM , which indicates that taking into account of the aspect information is helpful .","Among
three recurrent models
TDLSTM
performs
better
than
LSTM","TDLSTM||performs||better
better||than||LSTM
","three recurrent models||has||TDLSTM
","Results||Among||three recurrent models
",,,,,,
results,We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position .,"consider
each hidden vector
of
TDLSTM
encodes
semantics
of
word sequence
until
current position","each hidden vector||of||TDLSTM
each hidden vector||encodes||semantics
semantics||of||word sequence
word sequence||until||current position
",,"Results||consider||each hidden vector
",,,,,,
results,"We can also find that the performance of Contex - tAVG is very poor , which means that assigning the same weight / importance to all the context words is not an effective way .","performance
of
Contex - tAVG
is
very poor","performance||of||Contex - tAVG
Contex - tAVG||is||very poor
",,,"Results||find that||performance
",,,,,
results,"Among all our models from single hop to nine hops , we can observe that using more computational layers could generally lead to better performance , especially when the number of hops is less than six .","observe that
using
more computational layers
lead to
better performance
especially when
number of hops
is
less than six","observe that||using||more computational layers
more computational layers||lead to||better performance
better performance||especially when||number of hops
number of hops||is||less than six
",,,"Results||Among||observe that
",,,,,
results,"The best performances are achieved when the model contains seven and nine hops , respectively .","best performances
achieved when
model
contains
seven and nine hops","best performances||achieved when||model
model||contains||seven and nine hops
",,,"Results||has||best performances
",,,,,
results,"On both datasets , the proposed approach could obtain comparable accuracy compared to the state - of - art feature - based SVM system .","On
both datasets
proposed approach
could obtain
comparable accuracy
compared to
state - of - art feature - based SVM system","proposed approach||could obtain||comparable accuracy
comparable accuracy||compared to||state - of - art feature - based SVM system
","both datasets||has||proposed approach
","Results||On||both datasets
",,,,,,
results,We can find that using multiple computational layers could consistently improve the classification accuracy in all these models .,"multiple computational layers
consistently improve
classification accuracy
in
all these models","multiple computational layers||consistently improve||classification accuracy
classification accuracy||in||all these models
",,,"Results||find that||multiple computational layers
",,,,,
results,All these models perform comparably when the number of hops is larger than five .,"All these models
perform
comparably
when
number of hops
is
larger than five","All these models||perform||comparably
comparably||when||number of hops
number of hops||is||larger than five
",,,"Results||has||All these models
",,,,,
research-problem,Multi - grained Attention Network for Aspect - Level Sentiment Classification,Aspect - Level Sentiment Classification,,,,,"Contribution||has research problem||Aspect - Level Sentiment Classification
",,,,
research-problem,We propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .,aspect level sentiment classification,,,,,"Contribution||has research problem||aspect level sentiment classification
",,,,
model,"In this paper , we propose a multi -grained attention network to address the above two issues in aspect level sentiment classification .","propose
multi -grained attention network",,,"Model||propose||multi -grained attention network
",,,,,,
model,"Specifically , we propose a fine - grained attention mechanism ( i.e. F- Aspect2Context and F - Context2Aspect ) , which is employed to characterize the word - level interactions between aspect and context words , and relieve the information loss occurred in coarse - grained attention mechanism .","fine - grained attention mechanism
to characterize
word - level interactions
between
aspect and context words
relieve
information loss
occurred in
coarse - grained attention mechanism","fine - grained attention mechanism||to characterize||word - level interactions
word - level interactions||between||aspect and context words
fine - grained attention mechanism||relieve||information loss
information loss||occurred in||coarse - grained attention mechanism
",,,"Model||propose||fine - grained attention mechanism
",,,,,
model,"In addition , we utilize the bidirectional coarsegrained attention ( i.e. C- Aspect2Context and C - Context2Aspect ) and combine them with finegrained attention vectors to compose the multigrained attention network for the final sentiment polarity prediction , which can leverage the advantages of them .","utilize
bidirectional coarsegrained attention
combine them with
finegrained attention vectors
to compose
multigrained attention network
for
final sentiment polarity prediction","bidirectional coarsegrained attention||combine them with||finegrained attention vectors
finegrained attention vectors||to compose||multigrained attention network
multigrained attention network||for||final sentiment polarity prediction
",,"Model||utilize||bidirectional coarsegrained attention
",,,,,,
model,"More importantly , in order to make use of the valuable aspect - level interaction information , we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities .","in
to make use of
valuable aspect - level interaction information
design
aspect alignment loss
objective function
to enhance
difference
of
attention weights
towards
aspects
which have
same context
different sentiment polarities","valuable aspect - level interaction information||design||aspect alignment loss
aspect alignment loss||to enhance||difference
difference||of||attention weights
difference||towards||aspects
aspects||which have||same context
aspects||which have||different sentiment polarities
aspect alignment loss||in||objective function
",,"Model||to make use of||valuable aspect - level interaction information
",,,,,,
hyperparameters,"In our experiments , word embeddings for both context and aspect words are initialized by Glove .","word embeddings
for both
context and aspect words
initialized by
Glove","word embeddings||for both||context and aspect words
context and aspect words||initialized by||Glove
",,,"Hyperparameters||has||word embeddings
",,,,,
hyperparameters,The dimension of word embedding d v and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.,"dimension
of
word embedding d v
hidden stated",,,,"Hyperparameters||has||dimension
",,"300||of||word embedding d v
300||of||hidden stated
",,,
hyperparameters,set to 300 .,"set to
300",,,,,,"dimension||set to||300
",,,
hyperparameters,"The weight matrix and bias are initialized by sampling from a uniform distribution U ( 0.01 , 0.01 ) .","weight matrix and bias
are
initialized
by
sampling
from
uniform distribution U ( 0.01 , 0.01 )","weight matrix and bias||are||initialized
initialized||by||sampling
sampling||from||uniform distribution U ( 0.01 , 0.01 )
",,,"Hyperparameters||has||weight matrix and bias
",,,,,
hyperparameters,"The coefficient ? of L 2 regularization item is 10 ? 5 , the parameter ?","coefficient
of
L 2 regularization
is
10 ? 5","coefficient||of||L 2 regularization
L 2 regularization||is||10 ? 5
",,,"Hyperparameters||has||coefficient
",,,,,
hyperparameters,of aspect alignment loss and dropout rate are set to 0.5 .,"dropout rate
set to
0.5","dropout rate||set to||0.5
",,,"Hyperparameters||has||dropout rate
",,,,,
baselines,"Majority is the basic baseline , which chooses the largest sentiment polarity in the training set to each instance in the test set .","Majority
is
basic baseline
chooses
largest sentiment polarity
in
training set
to each
instance
in
test set","Majority||is||basic baseline
Majority||chooses||largest sentiment polarity
largest sentiment polarity||in||training set
training set||to each||instance
instance||in||test set
",,,"Baselines||has||Majority
",,,,,
baselines,"MemNet applys multi-hop attentions on the word embeddings , learns the attention weights on context word vectors with respect to the averaged query vector .","MemNet
applys
multi-hop attentions
on
word embeddings
learns
attention weights
on
context word vectors
with respect to
averaged query vector","MemNet||applys||multi-hop attentions
multi-hop attentions||on||word embeddings
MemNet||learns||attention weights
attention weights||on||context word vectors
context word vectors||with respect to||averaged query vector
",,,"Baselines||has||MemNet
",,,,,
baselines,"IAN interactively learns the coarse - grained attentions between the context and aspect , and concatenate the vectors for prediction .","IAN
interactively learns
coarse - grained attentions
between
context and aspect
concatenate
vectors
for
prediction","IAN||interactively learns||coarse - grained attentions
coarse - grained attentions||concatenate||vectors
vectors||for||prediction
coarse - grained attentions||between||context and aspect
",,,"Baselines||has||IAN
",,,,,
baselines,"BILSTM - ATT -G ( Liu and Zhang , 2017 ) models left and right context with two attention - based LSTMs and utilizes gates to control the importance of left context , right context and the entire sentence for prediction .","BILSTM - ATT -G
models
left and right context
with
two attention - based LSTMs
utilizes
gates
to control
importance
of
left context
right context
entire sentence
for
prediction","BILSTM - ATT -G||models||left and right context
left and right context||with||two attention - based LSTMs
BILSTM - ATT -G||utilizes||gates
gates||to control||importance
importance||of||left context
importance||of||right context
importance||of||entire sentence
entire sentence||for||prediction
",,,"Baselines||has||BILSTM - ATT -G
",,,,,
baselines,"RAM learns multi-hop attentions on the hidden states of bidirectional LSTM networks for context words , and proposes to use GRU network to get the aggregated vector from the attentions .","RAM
learns
multi-hop attentions
on
hidden states
of
bidirectional LSTM networks
for
context words
proposes to use
GRU network
to get
aggregated vector
from
attentions","RAM||proposes to use||GRU network
GRU network||to get||aggregated vector
aggregated vector||from||attentions
RAM||learns||multi-hop attentions
multi-hop attentions||on||hidden states
hidden states||of||bidirectional LSTM networks
bidirectional LSTM networks||for||context words
",,,"Baselines||has||RAM
",,,,,
baselines,"MGAN - C only employs the coarse - grained attentions for prediction , which is similar with IAN .","MGAN - C
employs
coarse - grained attentions
for
prediction","MGAN - C||employs||coarse - grained attentions
coarse - grained attentions||for||prediction
",,,"Baselines||has||MGAN - C
",,,,,
baselines,MGAN - F only utilizes the proposed fine - grained attentions for prediction .,"MGAN - F
utilizes
proposed fine - grained attentions
for
prediction","MGAN - F||utilizes||proposed fine - grained attentions
proposed fine - grained attentions||for||prediction
",,,"Baselines||has||MGAN - F
",,,,,
baselines,"MGAN - CF adopts both the coarse - grained and fine - grained attentions , while without applying the aspect alignment loss .","MGAN - CF
adopts both
coarse - grained and fine - grained attentions","MGAN - CF||adopts both||coarse - grained and fine - grained attentions
",,,"Baselines||has||MGAN - CF
",,,,,
baselines,MGAN is the complete multi-grained attention network model .,"MGAN
is
complete multi-grained attention network model","MGAN||is||complete multi-grained attention network model
",,,"Baselines||has||MGAN
",,,,,
results,( 1 ) Majority performs worst since it only utilizes the data distribution information .,"Majority
performs
worst","Majority||performs||worst
",,,"Results||has||Majority
",,,,,
results,Our method MGAN outperforms Majority and Feature + SVM since MGAN could learn the high quality representation for prediction .,"MGAN
outperforms
Majority and Feature + SVM","MGAN||outperforms||Majority and Feature + SVM
",,,"Results||has||MGAN
",,,,,
results,( 2 ) ATAE - LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation .,"ATAE - LSTM
is
better
than
LSTM","ATAE - LSTM||is||better
better||than||LSTM
",,,"Results||has||ATAE - LSTM
",,,,,
results,"TD - LSTM performs slightly better than ATAE - LSTM , and it employs two LSTM networks to capture the left and right context of the aspect .","TD - LSTM
performs
slightly better
than
ATAE - LSTM","TD - LSTM||performs||slightly better
slightly better||than||ATAE - LSTM
",,,"Results||has||TD - LSTM
",,,,,
results,TD - LSTM performs worse than our method MGAN since it could not properly pay more attentions on the important parts of the context .,"worse
than
our method MGAN","worse||than||our method MGAN
",,,,,,,"TD - LSTM||performs||worse
",
results,"( 3 ) IAN achieves slightly better results with the previous LSTM - based methods , which interactively learns the attended aspect and context vector as final representation .","IAN
achieves
slightly better results
with
previous LSTM - based methods","IAN||achieves||slightly better results
slightly better results||with||previous LSTM - based methods
",,,"Results||has||IAN
",,,,,
results,Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN .,"Our method
consistently performs
better
than
IAN","Our method||consistently performs||better
better||than||IAN
",,,"Results||has||Our method
",,,,,
results,"BILSTM - ATT - G models left context and right context using attention - based LSTMs , which achieves better performance than MemNet .","BILSTM - ATT - G
models
left context and right context
using
attention - based LSTMs
achieves
better performance
than
MemNet","BILSTM - ATT - G||models||left context and right context
left context and right context||using||attention - based LSTMs
BILSTM - ATT - G||achieves||better performance
better performance||than||MemNet
",,,"Results||has||BILSTM - ATT - G
",,,,,
results,RAM performs better than other baselines .,"RAM
performs
better
than
other baselines","RAM||performs||better
better||than||other baselines
",,,"Results||has||RAM
",,,,,
results,"Our proposed MGAN consistently performs better than MemNet , BILSTM - ATT - G and RAM on all three datasets .","Our proposed MGAN
consistently performs
better
than
MemNet
BILSTM - ATT - G
RAM","Our proposed MGAN||consistently performs||better
better||than||MemNet
better||than||BILSTM - ATT - G
better||than||RAM
",,,"Results||has||Our proposed MGAN
",,,,,
research-problem,Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,Sentence Level Discourse Parsing and Sentiment Analysis,,,,,"Contribution||has research problem||Sentence Level Discourse Parsing and Sentiment Analysis
",,,,
model,Our framework consists of three main sub parts .,"framework
consists of
three main sub parts","framework||consists of||three main sub parts
",,,"Model||has||framework
",,,,,
model,"Given a segmented sentence , the first step is to create meaningful vector representations for all the EDUs .","Given
segmented sentence
first step
to create
meaningful vector representations
for
all the EDUs","first step||to create||meaningful vector representations
meaningful vector representations||for||all the EDUs
","segmented sentence||has||first step
","Model||Given||segmented sentence
",,,,,,
model,"Next , we devise three different Recursive Neural Net models , each designed for one of discourse structure prediction , discourse relation prediction and sentiment analysis .","devise
three different Recursive Neural Net models
designed for
one
of
discourse structure prediction
discourse relation prediction
sentiment analysis","three different Recursive Neural Net models||designed for||one
one||of||discourse structure prediction
one||of||discourse relation prediction
one||of||sentiment analysis
",,"Model||devise||three different Recursive Neural Net models
",,,,,,
model,"Finally , we join these Neural Nets in two different ways : Multitasking and Pre-training .","join
Neural Nets
in
two different ways
Multitasking
Pre-training","Neural Nets||in||two different ways
","two different ways||name||Multitasking
two different ways||name||Pre-training
","Model||join||Neural Nets
",,,,,,
hyperparameters,All the neural models presented in this paper were implemented using the Tensor Flow python pack - .,"implemented using
Tensor Flow python pack",,,"Hyperparameters||implemented using||Tensor Flow python pack
",,,,,,
hyperparameters,We minimize the crossentropy error using the Adam optimizer and L2regularization on the set of weights .,"minimize
crossentropy error
using
Adam optimizer
L2regularization
on
set of weights","crossentropy error||using||Adam optimizer
crossentropy error||using||L2regularization
L2regularization||on||set of weights
",,"Hyperparameters||minimize||crossentropy error
",,,,,,
hyperparameters,"For the individual models ( before joining ) , we use 200 training epochs and a batch size of 100 .","For
individual models ( before joining )
use
200 training epochs
batch size
of
100","individual models ( before joining )||use||200 training epochs
individual models ( before joining )||use||batch size
batch size||of||100
",,"Hyperparameters||For||individual models ( before joining )
",,,,,,
results,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .","see
some improvement
on
Discourse Structure prediction
when we are using
joint model
improvement
is
statistically significant
only for
Nuclearity and Relation predictions","some improvement||on||Discourse Structure prediction
Discourse Structure prediction||when we are using||joint model
improvement||is||statistically significant
statistically significant||only for||Nuclearity and Relation predictions
","Discourse Structure prediction||has||improvement
","Results||see||some improvement
",,,,,,
results,"The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , Comparison and Cause relations as .","improvements
on
Relation predictions
mainly on
Contrastive set
specifically
Contrast
Comparison
Cause","improvements||on||Relation predictions
Relation predictions||mainly on||Contrastive set
Contrastive set||specifically||Contrast
Contrastive set||specifically||Comparison
Contrastive set||specifically||Cause
",,,"Results||has||improvements
",,,,,
results,In the fine grained setting we compute the accuracy of exact match across five classes .,"In
fine grained setting
compute
accuracy
of
exact match
across
five classes","fine grained setting||compute||accuracy
accuracy||across||five classes
accuracy||of||exact match
",,"Results||In||fine grained setting
",,,,,,
research-problem,Utilizing BERT for Aspect - Based Sentiment Analysis via Constructing Auxiliary Sentence,Aspect - Based Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - Based Sentiment Analysis
",,,,
research-problem,"Aspect - based sentiment analysis ( ABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect , is a challenging subtask of sentiment analysis ( SA ) .","Aspect - based sentiment analysis ( ABSA )
sentiment analysis ( SA )",,,,,"Contribution||has research problem||Aspect - based sentiment analysis ( ABSA )
Contribution||has research problem||sentiment analysis ( SA )
",,,,
research-problem,"Both SA and ABSA are sentence - level or document - level tasks , but one comment may refer to more than one object , and sentence - level tasks can not handle sentences with multiple targets .","SA
ABSA",,,,,"Contribution||has research problem||SA
Contribution||has research problem||ABSA
",,,,
research-problem,"Therefore , introduce the task of targeted aspect - based sentiment analysis ( TABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect associated with a given target .",targeted aspect - based sentiment analysis ( TABSA ),,,,,"Contribution||has research problem||targeted aspect - based sentiment analysis ( TABSA )
",,,,
approach,"In this paper , we investigate several methods of constructing an auxiliary sentence and transform ( T ) ABSA into a sentence - pair classification task .","investigate
several methods
of
constructing
an
auxiliary sentence
transform
( T ) ABSA
into
sentence - pair classification task","( T ) ABSA||into||sentence - pair classification task
several methods||of||constructing
constructing||an||auxiliary sentence
",,"Approach||transform||( T ) ABSA
Approach||investigate||several methods
",,,,,,
approach,We fine - tune the pre-trained model from BERT and achieve new state - of - the - art results on ( T ) ABSA task .,"fine - tune
pre-trained model
from
BERT","pre-trained model||from||BERT
",,"Approach||fine - tune||pre-trained model
",,,,,,
hyperparameters,We use the pre-trained uncased BERT - base model 5 for fine - tuning .,"use
pre-trained uncased BERT - base model
for
fine - tuning","pre-trained uncased BERT - base model||for||fine - tuning
",,"Hyperparameters||use||pre-trained uncased BERT - base model
",,,,,,
hyperparameters,"The number of Transformer blocks is 12 , the hidden layer size is 768 , the number of self - attention heads is 12 , and the total number of parameters for the pretrained model is 110M .","number of Transformer blocks
is
12
hidden layer size
is
768
number of self - attention heads
is
12
total number of parameters
for
pretrained model
is
110M","number of self - attention heads||is||12
total number of parameters||for||pretrained model
total number of parameters||is||110M
hidden layer size||is||768
number of Transformer blocks||is||12
",,,"Hyperparameters||has||number of self - attention heads
Hyperparameters||has||total number of parameters
Hyperparameters||has||hidden layer size
Hyperparameters||has||number of Transformer blocks
",,,,,
hyperparameters,"the dropout probability at 0.1 , set the number of epochs to 4 .","dropout probability
at
0.1
set
number of epochs
to
4","number of epochs||to||4
dropout probability||at||0.1
",,"Hyperparameters||set||number of epochs
","Hyperparameters||has||dropout probability
",,,,,
hyperparameters,"The initial learning rate is 2 e - 5 , and the batch size is 24 .","initial learning rate
is
2 e - 5
batch size
is
24","initial learning rate||is||2 e - 5
batch size||is||24
",,,"Hyperparameters||has||initial learning rate
Hyperparameters||has||batch size
",,,,,
baselines,LR : a logistic regression classifier with n-gram and pos-tag features .,"LR
logistic regression classifier
with
n-gram and pos-tag features","logistic regression classifier||with||n-gram and pos-tag features
","LR||has||logistic regression classifier
",,"Baselines||has||LR
",,,,,
baselines,LSTM - Final ) : a biLSTM model with the final state as a representation .,"LSTM - Final
biLSTM model
with
final state as a representation","biLSTM model||with||final state as a representation
","LSTM - Final||has||biLSTM model
",,"Baselines||has||LSTM - Final
",,,,,
baselines,LSTM - Loc ) : a biLSTM model with the state associated with the target position as a representation .,"LSTM - Loc
biLSTM model
with
state
associated with
target position as a representation","biLSTM model||with||state
state||associated with||target position as a representation
","LSTM - Loc||has||biLSTM model
",,"Baselines||has||LSTM - Loc
",,,,,
baselines,LSTM + TA + SA ) : a biLSTM model which introduces complex target - level and sentence - level attention mechanisms .,"LSTM + TA + SA
biLSTM model
introduces
complex target - level and sentence - level attention mechanisms","biLSTM model||introduces||complex target - level and sentence - level attention mechanisms
","LSTM + TA + SA||has||biLSTM model
",,"Baselines||has||LSTM + TA + SA
",,,,,
baselines,SenticLSTM : an upgraded version of the LSTM + TA + SA model which introduces external information from Sentic - Net .,"SenticLSTM
upgraded version
of
LSTM + TA + SA model
introduces
external information from Sentic - Net","upgraded version||of||LSTM + TA + SA model
LSTM + TA + SA model||introduces||external information from Sentic - Net
","SenticLSTM||has||upgraded version
",,"Baselines||has||SenticLSTM
",,,,,
baselines,"Dmu - Entnet : a bidirectional EntNet with external "" memory chains "" with a delayed memory update mechanism to track entities .","Dmu - Entnet
bidirectional EntNet
with
external "" memory chains ""
with
delayed memory update mechanism
to track
entities","bidirectional EntNet||with||external "" memory chains ""
external "" memory chains ""||with||delayed memory update mechanism
delayed memory update mechanism||to track||entities
","Dmu - Entnet||has||bidirectional EntNet
",,"Baselines||has||Dmu - Entnet
",,,,,
results,"We find that BERT - single has achieved better results on these two subtasks , and BERT - pair has achieved further improvements over BERT - single .","BERT - single
achieved
better results
BERT - pair
achieved
further improvements
over
BERT - single","BERT - single||achieved||better results
BERT - pair||achieved||further improvements
further improvements||over||BERT - single
",,,"Results||has||BERT - single
Results||has||BERT - pair
",,,,,
results,The BERT - pair - NLI - B model achieves the best performance for aspect category detection .,"BERT - pair - NLI - B model
achieves
best performance
for
aspect category detection","BERT - pair - NLI - B model||achieves||best performance
best performance||for||aspect category detection
",,,"Results||has||BERT - pair - NLI - B model
",,,,,
results,"For aspect category polarity , BERTpair - QA - B performs best on all 4 - way , 3 - way , and binary settings .","For
aspect category polarity
BERTpair - QA - B
performs
best
on
all 4 - way , 3 - way , and binary settings","BERTpair - QA - B||performs||best
best||on||all 4 - way , 3 - way , and binary settings
","aspect category polarity||has||BERTpair - QA - B
","Results||For||aspect category polarity
",,,,,,
research-problem,A Multi-sentiment - resource Enhanced Attention Network for Sentiment Classification,Sentiment Classification,,,,,"Contribution||has research problem||Sentiment Classification
",,,,
model,"In this work , we propose a Multi- sentimentresource Enhanced Attention Network ( MEAN ) for sentence - level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi -path attention mechanism .","propose
Multi- sentimentresource Enhanced Attention Network ( MEAN )
for
sentence - level sentiment classification
to integrate
many kinds of sentiment linguistic knowledge
into
deep neural networks
via
multi -path attention mechanism","Multi- sentimentresource Enhanced Attention Network ( MEAN )||for||sentence - level sentiment classification
Multi- sentimentresource Enhanced Attention Network ( MEAN )||to integrate||many kinds of sentiment linguistic knowledge
many kinds of sentiment linguistic knowledge||into||deep neural networks
many kinds of sentiment linguistic knowledge||via||multi -path attention mechanism
",,"Model||propose||Multi- sentimentresource Enhanced Attention Network ( MEAN )
",,,,,,
model,"Specifically , we first design a coupled word embedding module to model the word representation from character - level and word - level semantics .","design
coupled word embedding module
to model
word representation
from
character - level and word - level semantics","coupled word embedding module||to model||word representation
word representation||from||character - level and word - level semantics
",,"Model||design||coupled word embedding module
",,,,,,
model,"Then , we propose a multisentiment - resource attention module to learn more comprehensive and meaningful sentiment - specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively .","multisentiment - resource attention module
to learn
more comprehensive and meaningful sentiment - specific sentence representation
by using
three types of sentiment resource words
as
attention sources
attending to
context words","multisentiment - resource attention module||to learn||more comprehensive and meaningful sentiment - specific sentence representation
more comprehensive and meaningful sentiment - specific sentence representation||by using||three types of sentiment resource words
three types of sentiment resource words||attending to||context words
three types of sentiment resource words||as||attention sources
",,,"Model||propose||multisentiment - resource attention module
",,,,,
model,"In this way , we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the over all semantics of the sentiment , negation and intensity words for sentiment prediction .","attend to
different sentimentrelevant information
from
different representation subspaces
implied by
different types of sentiment sources
capture
over all semantics
of
sentiment , negation and intensity words
for
sentiment prediction","different sentimentrelevant information||from||different representation subspaces
different representation subspaces||capture||over all semantics
over all semantics||of||sentiment , negation and intensity words
sentiment , negation and intensity words||for||sentiment prediction
different representation subspaces||implied by||different types of sentiment sources
",,,,,"multisentiment - resource attention module||attend to||different sentimentrelevant information
",,,
baselines,RNTN : Recursive Tensor Neural Network ) is used to model correlations between different dimensions of child nodes vectors .,"RNTN
Recursive Tensor Neural Network
to model
correlations
between
different dimensions
of
child nodes vectors","RNTN||to model||correlations
correlations||between||different dimensions
different dimensions||of||child nodes vectors
","RNTN||name||Recursive Tensor Neural Network
",,"Baselines||has||RNTN
",,,,,
baselines,LSTM / Bi-LSTM : Cho et al. ( 2014 ) employs Long Short - Term Memory and the bidirectional variant to capture sequential information .,"LSTM / Bi-LSTM
employs
Long Short - Term Memory
bidirectional variant
to capture
sequential information","LSTM / Bi-LSTM||employs||Long Short - Term Memory
LSTM / Bi-LSTM||employs||bidirectional variant
bidirectional variant||to capture||sequential information
",,,"Baselines||has||LSTM / Bi-LSTM
",,,,,
baselines,"Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .","Tree-LSTM
Memory cells
introduced by
Tree - Structured Long Short - Term Memory
gates
into
tree - structured neural network","gates||into||tree - structured neural network
Memory cells||introduced by||Tree - Structured Long Short - Term Memory
","Tree-LSTM||has||gates
Tree-LSTM||has||Memory cells
",,"Baselines||has||Tree-LSTM
",,,,,
baselines,CNN : Convolutional Neural Networks ) is applied to generate task - specific sentence representation .,"CNN
Convolutional Neural Networks
to generate
task - specific sentence representation","CNN||to generate||task - specific sentence representation
","CNN||name||Convolutional Neural Networks
",,"Baselines||has||CNN
",,,,,
baselines,NCSL : designs a Neural Context - Sensitive Lexicon ( NSCL ) to obtain prior sentiment scores of words in the sentence .,"NCSL
Neural Context - Sensitive Lexicon ( NSCL )
to obtain
prior sentiment scores of words
in
sentence","NCSL||to obtain||prior sentiment scores of words
prior sentiment scores of words||in||sentence
","NCSL||name||Neural Context - Sensitive Lexicon ( NSCL )
",,"Baselines||has||NCSL
",,,,,
baselines,LR - Bi-LSTM : imposes linguistic roles into neural networks by applying linguistic regularization on intermediate outputs with KL divergence .,"LR - Bi-LSTM
imposes
linguistic roles
into
neural networks
by applying
linguistic regularization
on
intermediate outputs
with
KL divergence","LR - Bi-LSTM||imposes||linguistic roles
linguistic roles||into||neural networks
linguistic roles||by applying||linguistic regularization
linguistic regularization||with||KL divergence
linguistic regularization||on||intermediate outputs
",,,"Baselines||has||LR - Bi-LSTM
",,,,,
baselines,Self - attention : proposes a selfattention mechanism to learn structured sentence embedding .,"Self - attention
proposes
selfattention mechanism
to learn
structured sentence embedding","Self - attention||proposes||selfattention mechanism
selfattention mechanism||to learn||structured sentence embedding
",,,"Baselines||has||Self - attention
",,,,,
baselines,ID - LSTM : uses reinforcement learning to learn structured sentence representation for sentiment classification .,"ID - LSTM
uses
reinforcement learning
to learn
structured sentence representation
for
sentiment classification","ID - LSTM||uses||reinforcement learning
reinforcement learning||to learn||structured sentence representation
structured sentence representation||for||sentiment classification
",,,"Baselines||has||ID - LSTM
",,,,,
hyperparameters,"In our experiments , the dimensions of characterlevel embedding and word embedding ( Glo Ve ) are both set to 300 .","dimensions
of
characterlevel embedding and word embedding ( Glo Ve )
set to
300","dimensions||of||characterlevel embedding and word embedding ( Glo Ve )
characterlevel embedding and word embedding ( Glo Ve )||set to||300
",,,"Hyperparameters||has||dimensions
",,,,,
hyperparameters,"Kernel sizes of multi-gram convolution for Char - CNN are set to 2 , 3 , respectively .","Kernel sizes
of
multi-gram convolution
for
Char - CNN
set to
2 , 3","Kernel sizes||of||multi-gram convolution
multi-gram convolution||for||Char - CNN
Kernel sizes||set to||2 , 3
",,,"Hyperparameters||has||Kernel sizes
",,,,,
hyperparameters,"All the weight matrices are initialized as random orthogonal matrices , and we set all the bias vectors as zero vectors .","weight matrices
initialized as
random orthogonal matrices
set
all the bias vectors
as
zero vectors","all the bias vectors||as||zero vectors
weight matrices||initialized as||random orthogonal matrices
",,"Hyperparameters||set||all the bias vectors
","Hyperparameters||has||weight matrices
",,,,,
hyperparameters,"We optimize the proposed model with RMSprop algorithm , using mini-batch training .","optimize
proposed model
with
RMSprop algorithm
using
mini-batch training","proposed model||with||RMSprop algorithm
proposed model||using||mini-batch training
",,"Hyperparameters||optimize||proposed model
",,,,,,
hyperparameters,The size of mini-batch is 60 .,"size
of
mini-batch
is
60","size||of||mini-batch
mini-batch||is||60
",,,"Hyperparameters||has||size
",,,,,
hyperparameters,"The dropout rate is 0.5 , and the coefficient ?","dropout rate
is
0.5
coefficient","dropout rate||is||0.5
",,,"Hyperparameters||has||coefficient
Hyperparameters||has||dropout rate
",,,,,
hyperparameters,of L 2 normalization is set to 10 ?5 . is set to 10 ? 4 . ?,"of
L 2 normalization
set to
10 ?5","L 2 normalization||set to||10 ?5
",,,,,"coefficient||of||L 2 normalization
",,,
results,"First , our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge ( e.g. , RNTN , LSTM , BiLSTM , CNN and ID - LSTM ) on both datasets .","our model
brings
substantial improvement
over
methods
that do not leverage
sentiment linguistic knowledge","our model||brings||substantial improvement
substantial improvement||over||methods
methods||that do not leverage||sentiment linguistic knowledge
",,,"Results||has||our model
",,,,,
results,"Second , our model also consistently outperforms LR - Bi - LSTM which integrates linguistic roles of sentiment , negation and intensity words into neural networks via the linguistic regularization .","consistently outperforms
LR - Bi - LSTM
which integrates
linguistic roles
of
sentiment , negation and intensity words
into
neural networks
via
linguistic regularization","LR - Bi - LSTM||which integrates||linguistic roles
linguistic roles||into||neural networks
neural networks||via||linguistic regularization
linguistic roles||of||sentiment , negation and intensity words
",,,,,"our model||consistently outperforms||LR - Bi - LSTM
",,,
results,"For example , our model achieves 2.4 % improvements over the MR dataset and 0.8 % improvements over the SST dataset compared to LR - Bi - LSTM .","achieves
2.4 % improvements
over
MR dataset
0.8 % improvements
over
SST dataset
compared to LR - Bi - LSTM","2.4 % improvements||over||MR dataset
0.8 % improvements||over||SST dataset
",,,,,"our model||achieves||2.4 % improvements
our model||achieves||0.8 % improvements
our model||achieves||compared to LR - Bi - LSTM
",,,
research-problem,Left - Center - Right Separated Neural Network for Aspect - based Sentiment Analysis with Rotatory Attention,Aspect - based Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - based Sentiment Analysis
",,,,
model,The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,"target2context attention
to capture
most indicative sentiment words
in
left / right contexts","target2context attention||to capture||most indicative sentiment words
most indicative sentiment words||in||left / right contexts
",,,"Model||has||target2context attention
",,,,,
model,"Subsequently , the context2target attention is used to capture the most important word in the target .","context2target attention
to capture
most important word
in
target","context2target attention||to capture||most important word
most important word||in||target
",,,"Model||has||context2target attention
",,,,,
model,This leads to a two - side representation of the target : left - aware target and right - aware target .,"leads to
two - side representation
of
target
left - aware target
right - aware target","two - side representation||of||target
","two - side representation||name||left - aware target
two - side representation||name||right - aware target
","Model||leads to||two - side representation
",,,,,,
research-problem,"Aspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target .",sentiment analysis,,,,,"Contribution||has research problem||sentiment analysis
",,,,
research-problem,"In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task .",sentiment classification,,,,,"Contribution||has research problem||sentiment classification
",,,,
model,"With the attempt to better address the two problems , in this paper we propose a left - center - right separated neural network with rotatory attention mechanism ( LCR - Rot ) .","propose
left - center - right separated neural network
with
rotatory attention mechanism ( LCR - Rot )","left - center - right separated neural network||with||rotatory attention mechanism ( LCR - Rot )
",,"Model||propose||left - center - right separated neural network
",,,,,,
model,"Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and right context ) .","design
left - center - right separated LSTMs
contains
three LSTMs
i.e.
left - , center - and right - LSTM
modeling
three parts
of
review
left context
target phrase
right context","left - center - right separated LSTMs||contains||three LSTMs
three LSTMs||i.e.||left - , center - and right - LSTM
left - center - right separated LSTMs||modeling||three parts
three parts||of||review
","review||name||left context
review||name||target phrase
review||name||right context
","Model||design||left - center - right separated LSTMs
",,,,,,
model,"On this basis , we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts .","rotatory attention mechanism
take into account
interaction
between
targets and contexts
to better represent
targets and contexts","rotatory attention mechanism||take into account||interaction
interaction||to better represent||targets and contexts
interaction||between||targets and contexts
",,,"Model||propose||rotatory attention mechanism
",,,,,
model,The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,,,,,,,,,,
model,"Subsequently , the context2target attention is used to capture the most important word in the target .",,,,,,,,,,
model,This leads to a two - side representation of the target : left - aware target and right - aware target .,,,,,,,,,,
model,"Finally , we concatenate the component representations as the final representation of the sentence and feed it into a softmax layer to predict the sentiment polarity .","concatenate
component representations
as
final representation
of
sentence
feed it into
softmax layer
to predict
sentiment polarity","component representations||as||final representation
final representation||of||sentence
component representations||feed it into||softmax layer
softmax layer||to predict||sentiment polarity
",,"Model||concatenate||component representations
",,,,,,
hyperparameters,"In our work , the dimension of word embedding vectors and hidden state vectors is 300 .","dimension
of
word embedding vectors
hidden state vectors
is
300","dimension||of||word embedding vectors
dimension||of||hidden state vectors
dimension||is||300
",,,"Hyperparameters||has||dimension
",,,,,
hyperparameters,"We use GloVe 2 vectors with 300 dimensions to initialize the word embeddings , the same as .","use
GloVe 2 vectors
with
300 dimensions
to initialize
word embeddings","GloVe 2 vectors||with||300 dimensions
300 dimensions||to initialize||word embeddings
",,"Hyperparameters||use||GloVe 2 vectors
",,,,,,
hyperparameters,"All out - ofvocabulary words and weight matrices are randomly initialized by a uniform distribution U ( - 0.1 , 0.1 ) , and all bias are set to zero .","All out - ofvocabulary words and weight matrices
randomly initialized by
uniform distribution U ( - 0.1 , 0.1 )
all bias
set to
zero","All out - ofvocabulary words and weight matrices||randomly initialized by||uniform distribution U ( - 0.1 , 0.1 )
all bias||set to||zero
",,,"Hyperparameters||has||All out - ofvocabulary words and weight matrices
Hyperparameters||has||all bias
",,,,,
hyperparameters,Tensor Flow is used for implementing our neural network model .,"Tensor Flow
for implementing
neural network model","Tensor Flow||for implementing||neural network model
",,,"Hyperparameters||has||Tensor Flow
",,,,,
hyperparameters,"In model training , the learning rate is set to 0.1 , the weight for L 2 - norm regularization is set to 1 e - 5 , and dropout rate is set to 0.5 .","In
model training
learning rate
set to
0.1
weight
for
L 2 - norm regularization
set to
1 e - 5
dropout rate
set to
0.5","learning rate||set to||0.1
weight||for||L 2 - norm regularization
L 2 - norm regularization||set to||1 e - 5
dropout rate||set to||0.5
","model training||has||learning rate
model training||has||weight
model training||has||dropout rate
","Hyperparameters||In||model training
",,,,,,
hyperparameters,We train the model use stochastic gradient descent optimizer with momentum of 0.9 .,"train
model
use
stochastic gradient descent optimizer
with
momentum
of
0.9","model||use||stochastic gradient descent optimizer
stochastic gradient descent optimizer||with||momentum
momentum||of||0.9
",,"Hyperparameters||train||model
",,,,,,
hyperparameters,The paired t- test is used for the significance testing .,"paired t- test
used for
significance testing","paired t- test||used for||significance testing
",,,"Hyperparameters||has||paired t- test
",,,,,
baselines,"Majority assigns the sentiment polarity that has the largest probability in the training set ; 2 . Simple SVM is a SVM classifier with simple features such as unigrams and bigrams ; 3 . Feature - enhanced SVM is a SVM classifier with a state - of - the - art feature template which contains n-gram features , parse features and lexicon features ; 4 . LSTM represents a standard LSTM for aspect - based sentiment classification task ; 5 . TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; 74.30 66.50 66.50 TD- LSTM 75.60 68.10 70.80 AE - LSTM 76.60 68.90 - ATAE - LSTM 77.20 68.70 - GRNN- G3 79.55 * 71.47 * 70.09 * MemNet 79.98 * 70.33 * 70.52 * IAN 78.60 72.10 - LCR - Rot ( our approach ) 81.34 75.24 72.69 : The performance ( classification accuracy ) of different methods on three datasets .","Majority
assigns
sentiment polarity
largest probability
in
training set
Simple SVM
is
SVM classifier
with
simple features
such as
unigrams and bigrams
Feature - enhanced SVM
is
SVM classifier
with
state - of - the - art feature template
which contains
n-gram features
parse features
lexicon features
TD - LSTM
adopts
two LSTMs
to model
left context
with
target
right context
with
target","Feature - enhanced SVM||is||SVM classifier
SVM classifier||with||state - of - the - art feature template
state - of - the - art feature template||which contains||n-gram features
state - of - the - art feature template||which contains||parse features
state - of - the - art feature template||which contains||lexicon features
Simple SVM||is||SVM classifier
SVM classifier||with||simple features
simple features||such as||unigrams and bigrams
Majority||assigns||sentiment polarity
largest probability||in||training set
TD - LSTM||adopts||two LSTMs
TD - LSTM||to model||right context
right context||with||target
TD - LSTM||to model||left context
left context||with||target
","sentiment polarity||has||largest probability
",,"Baselines||has||Feature - enhanced SVM
Baselines||has||Simple SVM
Baselines||has||Majority
Baselines||has||TD - LSTM
",,,,,
baselines,6 . AE - LSTM is an upgraded version of LSTM .,"AE - LSTM
is
upgraded version
of
LSTM","AE - LSTM||is||upgraded version
upgraded version||of||LSTM
",,,"Baselines||has||AE - LSTM
",,,,,
baselines,7 . ATAE - LSTM is developed based on AE - LSTM .,"ATAE - LSTM
developed based on
AE - LSTM","ATAE - LSTM||developed based on||AE - LSTM
",,,"Baselines||has||ATAE - LSTM
",,,,,
baselines,8 . GRNN - G3 adopts a Gated - RNN to represent sentence and use a three - way structure to leverage contexts .,"GRNN - G3
adopts
Gated - RNN
to represent
sentence
use
three - way structure
to leverage
contexts","GRNN - G3||adopts||Gated - RNN
Gated - RNN||use||three - way structure
three - way structure||to leverage||contexts
Gated - RNN||to represent||sentence
",,,"Baselines||has||GRNN - G3
",,,,,
baselines,MemNet is a deep memory network which considers the content and position of target .,"MemNet
is
deep memory network
considers
content and position
of
target","MemNet||is||deep memory network
deep memory network||considers||content and position
content and position||of||target
",,,"Baselines||has||MemNet
",,,,,
baselines,"IAN interactively learns attentions in the contexts and targets , and generate the representations for targets and contexts separately .","IAN
interactively learns
attentions
in
contexts and targets
generate
representations
for
targets and contexts","IAN||generate||representations
representations||for||targets and contexts
IAN||interactively learns||attentions
attentions||in||contexts and targets
",,,"Baselines||has||IAN
",,,,,
results,"We can find that the Majority method is the worst , which means the majority sentiment polarity occupies 53.50 % , 65.00 % and 50 % of all samples on the Restaurant , Laptop and Twitter testing datasets respectively .","find
Majority method
is
worst
majority sentiment polarity
occupies
53.50 % , 65.00 % and 50 %
on
Restaurant , Laptop and Twitter testing datasets","Majority method||is||worst
majority sentiment polarity||occupies||53.50 % , 65.00 % and 50 %
53.50 % , 65.00 % and 50 %||on||Restaurant , Laptop and Twitter testing datasets
","Majority method||has||majority sentiment polarity
","Results||find||Majority method
",,,,,,
results,The Simple SVM model performs better than Majority .,"Simple SVM model
performs
better
than
Majority","Simple SVM model||performs||better
better||than||Majority
",,,"Results||has||Simple SVM model
",,,,,
results,"With the help of feature engineering , the Feature - enhanced SVM achieves much better results .","With the help of
feature engineering
Feature - enhanced SVM
achieves
much better results","Feature - enhanced SVM||achieves||much better results
","feature engineering||has||Feature - enhanced SVM
","Results||With the help of||feature engineering
",,,,,,
results,Our model achieves significantly better results than feature - enhanced SVM .,"Our model
achieves
significantly better results
than
feature - enhanced SVM","Our model||achieves||significantly better results
significantly better results||than||feature - enhanced SVM
",,,"Results||has||Our model
",,,,,
results,"Among LSTM based neural networks described in this paper , the basic LSTM approach performs the worst .","basic LSTM approach
performs
worst","basic LSTM approach||performs||worst
",,,"Results||has||basic LSTM approach
",,,,,
results,TD - LSTM obtains an improvement of 1 - 2 % over LSTM when target signals are taken into consideration .,"TD - LSTM
obtains
improvement
of
1 - 2 %
over
LSTM
when
target signals
taken into
consideration","TD - LSTM||obtains||improvement
improvement||of||1 - 2 %
1 - 2 %||over||LSTM
improvement||when||target signals
target signals||taken into||consideration
",,,"Results||has||TD - LSTM
",,,,,
results,"MemNet achieves better results than other models on the Restaurant dataset , since it considers not only the contexts of targets but also the position of each context word related to the target .","MemNet
achieves
better results
than
other models
on
Restaurant dataset","MemNet||achieves||better results
better results||than||other models
better results||on||Restaurant dataset
",,,"Results||has||MemNet
",,,,,
results,IAN considers separate representations of targets and obtains better result on the Laptop dataset .,"IAN
considers
separate representations
of
targets
obtains
better result
on
Laptop dataset","IAN||considers||separate representations
separate representations||of||targets
IAN||obtains||better result
better result||on||Laptop dataset
",,,"Results||has||IAN
",,,,,
results,GRNN - G3 achieves competitive results on all datasets because of its three - way structure and special gated - RNN model .,"GRNN - G3
achieves
competitive results
on
all datasets","GRNN - G3||achieves||competitive results
competitive results||on||all datasets
",,,"Results||has||GRNN - G3
",,,,,
results,"In the contrast , our LCR - Rot model achieves the best results on the all datasets among all models .","LCR - Rot model
achieves
best results
on
all datasets
among
all models","LCR - Rot model||achieves||best results
best results||among||all models
best results||on||all datasets
",,,"Results||has||LCR - Rot model
",,,,,
research-problem,Variational Semi-supervised Aspect - term Sentiment Analysis via Transformer,Aspect - term Sentiment Analysis,,,,,"Contribution||has research problem||Aspect - term Sentiment Analysis
",,,,
research-problem,"Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) .","aspect - term sentiment analysis ( ATSA )
aspect - category sentiment analysis ( ACSA )",,,,,"Contribution||has research problem||aspect - term sentiment analysis ( ATSA )
Contribution||has research problem||aspect - category sentiment analysis ( ACSA )
",,,,
research-problem,"ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience .",ACSA,,,,,"Contribution||has research problem||ACSA
",,,,
research-problem,"On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text .",ATSA,,,,,"Contribution||has research problem||ATSA
",,,,
model,"In this paper , we proposed a classifier - agnostic framework which named Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ) .","proposed
classifier - agnostic framework
named
Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET )","classifier - agnostic framework||named||Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET )
",,"Model||proposed||classifier - agnostic framework
",,,,,,
model,The variational autoencoder offers the flexibility to customize the model structure .,"variational autoencoder
offers
flexibility
to customize
model structure","variational autoencoder||offers||flexibility
flexibility||to customize||model structure
",,,"Model||has||variational autoencoder
",,,,,
model,"By regarding the aspect sentiment polarity of the unlabeled data as the discrete latent variable , the model implicitly induces the sentiment polarity via the variational inference .","By regarding
aspect sentiment polarity
of
unlabeled data
as
discrete latent variable
model
implicitly induces
sentiment polarity
via
variational inference","aspect sentiment polarity||as||discrete latent variable
aspect sentiment polarity||of||unlabeled data
model||implicitly induces||sentiment polarity
sentiment polarity||via||variational inference
","aspect sentiment polarity||has||model
","Model||By regarding||aspect sentiment polarity
",,,,,,
model,"Specifically , the representation of the lexical context is extracted by the encoder and the aspect - term sentiment polarity is inferred from the specific ATSA classifier .","representation
of
lexical context
extracted by
encoder
aspect - term sentiment polarity
inferred from
specific ATSA classifier","aspect - term sentiment polarity||inferred from||specific ATSA classifier
representation||of||lexical context
lexical context||extracted by||encoder
",,,"Model||has||aspect - term sentiment polarity
Model||has||representation
",,,,,
model,"In addition , by separating the representation of the input sentence , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .","by separating
representation
of
input sentence
classifier
becomes
independent module
in
our framework","representation||of||input sentence
classifier||becomes||independent module
independent module||in||our framework
","representation||has||classifier
","Model||by separating||representation
",,,,,,
hyperparameters,"The number of units in the encoder and the decoder is 100 and the latent variable is of size 50 and the number of layers of both Transformer blocks is 2 , the number of selfattention heads is 8 .","number of units
in
encoder and the decoder
is
100
latent variable
is
of
size 50
number of layers
of both
Transformer blocks
is
2
number of selfattention heads
8","number of selfattention heads||is||8
number of units||in||encoder and the decoder
number of units||is||100
number of layers||of both||Transformer blocks
Transformer blocks||is||2
latent variable||of||size 50
",,,"Hyperparameters||has||number of selfattention heads
Hyperparameters||has||number of units
Hyperparameters||has||number of layers
Hyperparameters||has||latent variable
",,,,,
hyperparameters,"In this work , the KL weight is set to be 1e - 4 .","KL weight
set to be
1e - 4","KL weight||set to be||1e - 4
",,,"Hyperparameters||has||KL weight
",,,,,
baselines,"TC - LSTM : Two LSTMs are used to model the left and right context of the target separately , then the concatenation of two representations is used to predict the label .","TC - LSTM
Two LSTMs
used to
model
left and right context
of
target
concatenation
of
two representations
used to
predict
label","concatenation||of||two representations
two representations||used to||predict
Two LSTMs||used to||model
left and right context||of||target
","TC - LSTM||has||concatenation
predict||has||label
TC - LSTM||has||Two LSTMs
model||has||left and right context
",,"Baselines||has||TC - LSTM
",,,,,
baselines,"MemNet : It uses the attention mechanism over the word embedding over multiple rounds to aggregate the information in the sentence , the vector of the final round is used for the prediction .","MemNet
uses
attention mechanism
over
word embedding
over
multiple rounds
to aggregate
information
in
sentence","MemNet||uses||attention mechanism
attention mechanism||over||word embedding
word embedding||over||multiple rounds
attention mechanism||to aggregate||information
information||in||sentence
",,,"Baselines||has||MemNet
",,,,,
baselines,IAN : IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer .,"IAN
adopts
two LSTMs
to derive
representations
of
context and the target phrase
concatenation
fed to
softmax layer","IAN||adopts||two LSTMs
two LSTMs||to derive||representations
representations||of||context and the target phrase
concatenation||fed to||softmax layer
","IAN||has||concatenation
",,"Baselines||has||IAN
",,,,,
baselines,BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and makes use of a special gate layer to combine these two representations .,"BILSTM - ATT -G
models
left and right contexts
using
two attention - based LSTMs
makes use of
special gate layer
to combine
two representations","BILSTM - ATT -G||models||left and right contexts
left and right contexts||using||two attention - based LSTMs
BILSTM - ATT -G||makes use of||special gate layer
special gate layer||to combine||two representations
",,,"Baselines||has||BILSTM - ATT -G
",,,,,
baselines,"TNet - AS : Without using an attention module , TNet adopts a convolutional layer to get salient features from the transformed word representations originated from a bidirectional LSTM layer .","TNet - AS
Without using
attention module","TNet - AS||Without using||attention module
",,,"Baselines||has||TNet - AS
",,,,,
results,"From the , the ASVAET is able to improve supervised performance consistently for all classifiers .","ASVAET
able to improve
supervised performance
for
all classifiers","ASVAET||able to improve||supervised performance
supervised performance||for||all classifiers
",,,"Results||has||ASVAET
",,,,,
results,"For the MemNet , the test accuracy can be improved by about 2 % by the TSSVAE , and so as the Macro - averaged F1 .","For
MemNet
test accuracy
improved by
about 2 %
by
TSSVAE","test accuracy||improved by||about 2 %
about 2 %||by||TSSVAE
","MemNet||has||test accuracy
","Results||For||MemNet
",,,,,,
results,The TNet - AS outperforms the other three models .,"TNet - AS
outperforms
other three models","TNet - AS||outperforms||other three models
",,,"Results||has||TNet - AS
",,,,,
results,"Compared with the other two semi-supervised methods , the ASVAET also shows better results .","Compared with
other two semi-supervised methods
ASVAET
shows
better results","ASVAET||shows||better results
","other two semi-supervised methods||has||ASVAET
","Results||Compared with||other two semi-supervised methods
",,,,,,
results,The ASVAET outperforms the compared semisupervised methods evidently .,"outperforms
compared semisupervised methods",,,,,,"ASVAET||outperforms||compared semisupervised methods
",,,
results,The adoption of indomain pre-trained word vectors is beneficial for the performance compared with the Glove vectors .,"adoption of
indomain pre-trained word vectors
beneficial for
performance
compared with
Glove vectors","indomain pre-trained word vectors||beneficial for||performance
performance||compared with||Glove vectors
",,"Results||adoption of||indomain pre-trained word vectors
",,,,,,
research-problem,Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,Multi-modal Sentiment Analysis,,,,,"Contribution||has research problem||Multi-modal Sentiment Analysis
",,,,
research-problem,"Traditionally , sentiment analysis has been applied to a wide variety of texts .",sentiment analysis,,,,,"Contribution||has research problem||sentiment analysis
",,,,
model,"In this paper , we propose a novel method that employs a recurrent neural network based multimodal multi-utterance attention framework for sentiment prediction .","propose
novel method
employs
recurrent neural network based multimodal multi-utterance attention framework
for
sentiment prediction","novel method||employs||recurrent neural network based multimodal multi-utterance attention framework
recurrent neural network based multimodal multi-utterance attention framework||for||sentiment prediction
",,"Model||propose||novel method
",,,,,,
model,To better address these concerns we propose a novel fusion method by focusing on inter-modality relations computed between the target utterance and its context .,"novel fusion method
focusing on
inter-modality relations
computed between
target utterance and its context","novel fusion method||focusing on||inter-modality relations
inter-modality relations||computed between||target utterance and its context
",,,"Model||propose||novel fusion method
",,,,,
model,The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .,"attention mechanism
attend to
important contextual utterances
having
higher relatedness or similarity
with
target utterance","attention mechanism||attend to||important contextual utterances
important contextual utterances||having||higher relatedness or similarity
higher relatedness or similarity||with||target utterance
",,,"Model||has||attention mechanism
",,,,,
model,"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .","attend over
contextual utterances
by computing
correlations
among
modalities
of
target utterance and the context utterances","contextual utterances||by computing||correlations
correlations||among||modalities
modalities||of||target utterance and the context utterances
",,"Model||attend over||contextual utterances
",,,,,,
model,The model facilitates this modality selection by attending over the contextual utterances and thus generates better multimodal feature representation when these modalities from the context are combined with the modalities of the target utterance .,"facilitates
modality selection
attending over
contextual utterances
generates
better multimodal feature representation
when
modalities from the context
combined with
modalities of the target utterance","modality selection||attending over||contextual utterances
modality selection||generates||better multimodal feature representation
better multimodal feature representation||when||modalities from the context
modalities from the context||combined with||modalities of the target utterance
",,"Model||facilitates||modality selection
",,,,,,
hyperparameters,"We use Bi-directional GRUs having 300 neurons , each followed by a dense layer consisting of 100 neurons .","use
Bi-directional GRUs
having
300 neurons
followed by
dense layer
consisting of
100 neurons","Bi-directional GRUs||having||300 neurons
300 neurons||followed by||dense layer
dense layer||consisting of||100 neurons
",,"Hyperparameters||use||Bi-directional GRUs
",,,,,,
hyperparameters,"Utilizing the dense layer , we project the input features of all the three modalities to the same dimensions .","Utilizing
dense layer
project
input features
of
all the three modalities
to
same dimensions","dense layer||project||input features
input features||of||all the three modalities
input features||to||same dimensions
",,"Hyperparameters||Utilizing||dense layer
",,,,,,
hyperparameters,We set dropout = 0.5 ( MOSI ) & 0.3 ( MOSEI ) as a measure of regularization .,"set
dropout
=
0.5 ( MOSI )
0.3 ( MOSEI )
as a measure of
regularization","dropout||=||0.3 ( MOSEI )
regularization||set||dropout
dropout||=||0.5 ( MOSI )
dropout||=||0.3 ( MOSEI )
",,"Hyperparameters||as a measure of||regularization
",,,,,,
hyperparameters,"In addition , we also use dropout = 0.4 ( MOSI ) & 0.3 ( MOSEI ) for the Bi - GRU layers .","use
dropout
=
0.4 ( MOSI )
0.3 ( MOSEI )
for
Bi - GRU layers","Bi - GRU layers||use||dropout
dropout||=||0.4 ( MOSI )
",,"Hyperparameters||for||Bi - GRU layers
",,,,,,
hyperparameters,"We employ ReLu activation function in the dense layers , and softmax activation in the final classification layer .","employ
ReLu activation function
in
dense layers
softmax activation
in
final classification layer","ReLu activation function||in||dense layers
softmax activation||in||final classification layer
",,"Hyperparameters||employ||ReLu activation function
Hyperparameters||employ||softmax activation
",,,,,,
hyperparameters,"For training the network we set the batch size = 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .","For
training the network
set
batch size
=
32
use
Adam optimizer
with
cross - entropy loss function
train for
50 epochs","training the network||set||batch size
batch size||=||32
training the network||use||Adam optimizer
Adam optimizer||with||cross - entropy loss function
training the network||train for||50 epochs
",,"Hyperparameters||For||training the network
",,,,,,
results,"For MOSEI dataset , we obtain better performance with text .","For
MOSEI dataset
obtain
better performance
with
text","MOSEI dataset||obtain||better performance
better performance||with||text
",,"Results||For||MOSEI dataset
",,,,,,
results,"For text - acoustic input pairs , we obtain the highest accuracies with 79. 74 % , 79.60 % and 79.32 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .","text - acoustic input pairs
obtain
highest accuracies
with
79. 74 % , 79.60 % and 79.32 %
for
MMMU - BA , MMUU - SA and MU - SA frameworks","text - acoustic input pairs||obtain||highest accuracies
highest accuracies||with||79. 74 % , 79.60 % and 79.32 %
79. 74 % , 79.60 % and 79.32 %||for||MMMU - BA , MMUU - SA and MU - SA frameworks
",,,"Results||For||text - acoustic input pairs
",,,,,
results,"Finally , we experiment with tri-modal inputs and observe an improved performance of 79. 80 % , 79.76 % and 79.63 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .","experiment with
tri-modal inputs
observe
improved performance
of
79. 80 % , 79.76 % and 79.63 %
for
MMMU - BA , MMUU - SA and MU - SA frameworks","tri-modal inputs||observe||improved performance
improved performance||of||79. 80 % , 79.76 % and 79.63 %
79. 80 % , 79.76 % and 79.63 %||for||MMMU - BA , MMUU - SA and MU - SA frameworks
",,"Results||experiment with||tri-modal inputs
",,,,,,
results,The performance improvement was also found to be statistically significant ( T-test ) than the bimodality and uni-modality inputs .,"performance improvement
found to be
statistically significant ( T-test )
than
bimodality and uni-modality inputs","performance improvement||found to be||statistically significant ( T-test )
statistically significant ( T-test )||than||bimodality and uni-modality inputs
",,,"Results||has||performance improvement
",,,,,
results,"Further , we observe that the MMMU - BA framework reports the best accuracy of 79 . 80 % for the MOSEI dataset , thus supporting our claim that multi-modal attention framework ( i.e. MMMU - BA ) captures more information than the self - attention frameworks ( i.e. MMUU - SA & MU - SA ) .","observe
MMMU - BA framework
reports
best accuracy
of
79 . 80 %
for
MOSEI dataset","MMMU - BA framework||reports||best accuracy
best accuracy||of||79 . 80 %
best accuracy||for||MOSEI dataset
",,"Results||observe||MMMU - BA framework
",,,,,,
research-problem,A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition,EEG Emotion Recognition,,,,,"Contribution||has research problem||EEG Emotion Recognition
",,,,
research-problem,"Inspired by this study , in this paper , we propose a novel bi-hemispheric discrepancy model ( BiHDM ) to learn the asymmetric differences between two hemispheres for electroencephalograph ( EEG ) emotion recognition .",electroencephalograph ( EEG ) emotion recognition,,,,,"Contribution||has research problem||electroencephalograph ( EEG ) emotion recognition
",,,,
research-problem,"As the first step to make machines capture human emotions , emotion recognition has received substantial attention from human - machine - interaction ( HMI ) and pattern recognition research communities in recent years , , .",emotion recognition,,,,,"Contribution||has research problem||emotion recognition
",,,,
model,"Thus , in this paper , we propose a novel neural network model BiHDM to learn the bi-hemispheric discrepancy for EEG emotion recognition .","propose
novel neural network model BiHDM
to learn
bi-hemispheric discrepancy
for
EEG emotion recognition","novel neural network model BiHDM||to learn||bi-hemispheric discrepancy
bi-hemispheric discrepancy||for||EEG emotion recognition
",,"Model||propose||novel neural network model BiHDM
",,,,,,
model,"BiHDM aims to obtain the deep discrepant features between the left and right hemispheres , which is expected to contain more discriminative information to recognize the EEG emotion signals .","BiHDM
aims to obtain
deep discrepant features
between
left and right hemispheres
expected to contain
more discriminative information
to recognize
EEG emotion signals","BiHDM||aims to obtain||deep discrepant features
deep discrepant features||between||left and right hemispheres
left and right hemispheres||expected to contain||more discriminative information
more discriminative information||to recognize||EEG emotion signals
",,,"Model||has||BiHDM
",,,,,
model,"Hence , to avoid losing this intrinsic graph structural information of EEG data , we can simplify the graph structure learning process by using the horizontal and vertical traversing RNNs , which will construct a complete relationship graph and generate discriminative deep features for all the EEG electrodes .","to avoid losing
intrinsic graph structural information
of
EEG data
simplify
graph structure learning process
by using
horizontal and vertical traversing RNNs
which will construct
complete relationship graph
generate
discriminative deep features
for
all the EEG electrodes","graph structure learning process||to avoid losing||intrinsic graph structural information
intrinsic graph structural information||of||EEG data
graph structure learning process||by using||horizontal and vertical traversing RNNs
horizontal and vertical traversing RNNs||which will construct||complete relationship graph
complete relationship graph||generate||discriminative deep features
discriminative deep features||for||all the EEG electrodes
",,"Model||simplify||graph structure learning process
",,,,,,
model,"After obtaining these deep features of each electrodes , we can extract the asymmetric discrepancy information between two hemispheres by performing specific pairwise operations for any paired symmetric electrodes .","After obtaining
deep features
of
each electrodes
extract
asymmetric discrepancy information
between
two hemispheres
by performing
specific pairwise operations
for
any paired symmetric electrodes","deep features||extract||asymmetric discrepancy information
asymmetric discrepancy information||between||two hemispheres
two hemispheres||by performing||specific pairwise operations
specific pairwise operations||for||any paired symmetric electrodes
deep features||of||each electrodes
",,"Model||After obtaining||deep features
",,,,,,
experimental-setup,"We use the released handcrafted features , i.e. , the differential entropy ( DE ) in SEED and SEED - IV , and the Short - Time Fourier Transform ( STFT ) in MPED , as the input to feed our model .","use
released handcrafted features
i.e.
differential entropy ( DE )
in
SEED and SEED - IV
Short - Time Fourier Transform ( STFT )
in
MPED
to feed
model","released handcrafted features||to feed||model
released handcrafted features||i.e.||differential entropy ( DE )
differential entropy ( DE )||in||SEED and SEED - IV
released handcrafted features||i.e.||Short - Time Fourier Transform ( STFT )
Short - Time Fourier Transform ( STFT )||in||MPED
",,"Experimental setup||use||released handcrafted features
",,,,,,
experimental-setup,"Thus the sizes d N of the input sample X t are 5 62 , 5 62 and 1 62 for these three datasets , respectively .","sizes d N
of
input sample X t
are
5 62 , 5 62 and 1 62","sizes d N||of||input sample X t
input sample X t||are||5 62 , 5 62 and 1 62
",,,"Experimental setup||has||sizes d N
",,,,,
experimental-setup,"Moreover , in the experiment , we respectively set the dimension d l of each electrode 's deep representation to 32 ; the parameters d g and K of the global high - level feature to 32 and 6 ; and the dimension do of the output feature to 16 without elaborate traversal .","in
experiment
set
dimension d l
of
each electrode 's deep representation
to
32
parameters d g and K
of
global high - level feature
to
32 and 6
dimension do
of
output feature
to
16
without
elaborate traversal","experiment||set||dimension do
dimension do||of||output feature
output feature||to||16
output feature||without||elaborate traversal
experiment||set||parameters d g and K
parameters d g and K||of||global high - level feature
global high - level feature||to||32 and 6
experiment||set||dimension d l
dimension d l||of||each electrode 's deep representation
each electrode 's deep representation||to||32
",,"Experimental setup||in||experiment
",,,,,,
experimental-setup,"Specifically , we implemented BiHDM using Tensor Flow on one Nvidia 1080 Ti GPU .","implemented
BiHDM
using
Tensor Flow
on
one Nvidia 1080 Ti GPU","BiHDM||using||Tensor Flow
Tensor Flow||on||one Nvidia 1080 Ti GPU
",,"Experimental setup||implemented||BiHDM
",,,,,,
experimental-setup,"The learning rate , momentum and weight decay rate are set as 0.003 , 0.9 and 0.95 respectively .","learning rate , momentum and weight decay rate
set as
0.003 , 0.9 and 0.95","learning rate , momentum and weight decay rate||set as||0.003 , 0.9 and 0.95
",,,"Experimental setup||has||learning rate , momentum and weight decay rate
",,,,,
experimental-setup,The network is trained using SGD with batch size of 200 .,"network
trained using
SGD
with
batch size
of
200","network||trained using||SGD
SGD||with||batch size
batch size||of||200
",,,"Experimental setup||has||network
",,,,,
experimental-setup,"In addition , we adopt the subtraction as the pairwise operation of the BiHDM model in the experiment section , and discuss the other two types of operations in section III - D.","adopt
subtraction
as
pairwise operation
of
BiHDM model","subtraction||as||pairwise operation
pairwise operation||of||BiHDM model
",,"Experimental setup||adopt||subtraction
",,,,,,
experiments,1 ) The subject - dependent experiment :,subject - dependent experiment,,,,,,,,"Tasks||has||subject - dependent experiment
","subject - dependent experiment||has||Baselines
"
experiments,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .","using
twelve methods
including
linear support vector machine ( SVM )
random forest ( RF )
canonical correlation analysis ( CCA )
group sparse canonical correlation analysis ( GSCCA )
deep believe network ( DBN )
graph regularization sparse linear regression ( GRSLR )
graph convolutional neural network ( GCNN )
dynamical graph convolutional neural network ( DGCNN )
domain adversarial neural networks ( DANN )
bi-hemisphere domain adversarial neural network ( BiDANN )
EmotionMeter
attention - long short - term memory ( A - LSTM )","twelve methods||including||linear support vector machine ( SVM )
twelve methods||including||random forest ( RF )
twelve methods||including||canonical correlation analysis ( CCA )
twelve methods||including||group sparse canonical correlation analysis ( GSCCA )
twelve methods||including||deep believe network ( DBN )
twelve methods||including||graph regularization sparse linear regression ( GRSLR )
twelve methods||including||graph convolutional neural network ( GCNN )
twelve methods||including||dynamical graph convolutional neural network ( DGCNN )
twelve methods||including||domain adversarial neural networks ( DANN )
twelve methods||including||bi-hemisphere domain adversarial neural network ( BiDANN )
twelve methods||including||EmotionMeter
twelve methods||including||attention - long short - term memory ( A - LSTM )
",,,,,"Baselines||using||twelve methods
",,,
experiments,"From , we can see that the proposed BiHDM model outperforms all the compared methods on all the three public EEG emotional datasets , which verifies the effectiveness of BiHDM .","see
proposed BiHDM model
outperforms
all the compared methods
on
all the three public EEG emotional datasets
verifies
effectiveness of BiHDM","proposed BiHDM model||outperforms||all the compared methods
all the compared methods||verifies||effectiveness of BiHDM
all the compared methods||on||all the three public EEG emotional datasets
",,,,,"Results||see||proposed BiHDM model
",,,
experiments,"Especially for the result on SEED - IV , the proposed method improves over the state - of - the - art method Emotion - Meter by 4 % .","on
SEED - IV
proposed method
improves over
state - of - the - art method Emotion - Meter
by
4 %","proposed method||improves over||state - of - the - art method Emotion - Meter
state - of - the - art method Emotion - Meter||by||4 %
","SEED - IV||has||proposed method
",,,,"Results||on||SEED - IV
",,,
experiments,"Besides , we can see that the compared method BiDANN , which also considers the bi-hemispheric asymmetry , achieves a comparable performance .","see that
compared method BiDANN
considers
bi-hemispheric asymmetry
achieves
comparable performance","compared method BiDANN||considers||bi-hemispheric asymmetry
compared method BiDANN||achieves||comparable performance
",,,,,"Results||see that||compared method BiDANN
",,,
experiments,"shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .","shows
t- test statistical analysis results
see
BiHDM
is
significantly better
than
baseline method","t- test statistical analysis results||see||BiHDM
BiHDM||is||significantly better
significantly better||than||baseline method
t- test statistical analysis results||see||BiHDM
BiHDM||is||significantly better
significantly better||than||baseline method
",,,,,"Results||shows||t- test statistical analysis results
",,"Results||has||t- test statistical analysis results
",
experiments,2 ) The subject - independent experiment :,subject - independent experiment,,,,,,,,"Tasks||has||subject - independent experiment
","subject - independent experiment||has||Baselines
"
experiments,"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .","use
twelve methods
including
Kullback - Leibler importance estimation procedure ( KLIEP )
unconstrained least - squares importance fitting ( ULSIF )
selective transfer machine ( STM )
linear SVM , transfer component analysis ( TCA )
transfer component analysis ( TCA )
geodesic flow kernel ( GFK )
DANN
DGCNN
deep adaptation network ( DAN )
BiDANN
A - LSTM","twelve methods||including||Kullback - Leibler importance estimation procedure ( KLIEP )
twelve methods||including||unconstrained least - squares importance fitting ( ULSIF )
twelve methods||including||selective transfer machine ( STM )
twelve methods||including||linear SVM , transfer component analysis ( TCA )
twelve methods||including||transfer component analysis ( TCA )
twelve methods||including||geodesic flow kernel ( GFK )
twelve methods||including||DANN
twelve methods||including||DGCNN
twelve methods||including||deep adaptation network ( DAN )
twelve methods||including||BiDANN
twelve methods||including||A - LSTM
",,,,,"Baselines||use||twelve methods
",,,
experiments,"The results are shown in From , it can be clearly seen that the proposed BiHDM method achieves the best performance in the three public datasets , which verifies the effectiveness of BiHDM in dealing with subject - independent EEG emotion recognition .","in
seen that
proposed BiHDM method
achieves
best performance
three public datasets
verifies
effectiveness of BiHDM
in dealing with
subject - independent EEG emotion recognition","proposed BiHDM method||achieves||best performance
best performance||in||three public datasets
best performance||verifies||effectiveness of BiHDM
effectiveness of BiHDM||in dealing with||subject - independent EEG emotion recognition
",,,,,"Results||seen that||proposed BiHDM method
",,,
experiments,"For the three datasets , the improvements on accuracy are 2.2 % , 3.5 % and 2.4 % , respectively , when compared with the existing state - of - the - art methods .","For
three datasets
improvements on
accuracy
are
2.2 % , 3.5 % and 2.4 %
compared with
existing state - of - the - art methods","three datasets||improvements on||accuracy
accuracy||are||2.2 % , 3.5 % and 2.4 %
2.2 % , 3.5 % and 2.4 %||compared with||existing state - of - the - art methods
",,,,,"Results||For||three datasets
",,,
experiments,"shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .","t- test statistical analysis results
see
BiHDM
is
significantly better
than
baseline method",,,,,,,,,
research-problem,Exploiting Document Knowledge for Aspect - level Sentiment Classification,Aspect - level Sentiment Classification,,,,,"Contribution||has research problem||Aspect - level Sentiment Classification
",,,,
model,"Specifically , we explore two transfer methods to incorporate this sort of knowledge - pretraining and multi-task learning .","explore
two transfer methods
to incorporate
knowledge
pretraining
multi-task learning","two transfer methods||to incorporate||knowledge
","knowledge||name||pretraining
knowledge||name||multi-task learning
","Model||explore||two transfer methods
",,,,,,
code,Our source code can be obtained from https://github.com/ruidan/Aspect-level-sentiment.,,,,,,,,,,
hyperparameters,"In all experiments , 300 - dimension Glo Ve vectors are used to initialize E and E when pretraining is not conducted for weight initialization .","300 - dimension Glo Ve vectors
to initialize
E and E
when
pretraining
not conducted for
weight initialization","300 - dimension Glo Ve vectors||to initialize||E and E
E and E||when||pretraining
pretraining||not conducted for||weight initialization
",,,"Hyperparameters||has||300 - dimension Glo Ve vectors
",,,,,
hyperparameters,These vectors are also used for initializing E in the pretraining phase .,"vectors
used for
initializing E
in
pretraining phase","vectors||used for||initializing E
initializing E||in||pretraining phase
",,,"Hyperparameters||has||vectors
",,,,,
hyperparameters,We randomly sample 20 % of the original training data from the aspectlevel dataset as the development set and only use the remaining 80 % for training .,"randomly sample
20 %
of
original training data
from
aspectlevel dataset
as
development set","20 %||as||development set
20 %||of||original training data
original training data||from||aspectlevel dataset
",,"Hyperparameters||randomly sample||20 %
",,,,,,
hyperparameters,"For all experiments , the dimension of LSTM hidden vectors is set to 300 , ?","For
all experiments
dimension
of
LSTM hidden vectors
set to
300","dimension||of||LSTM hidden vectors
LSTM hidden vectors||set to||300
","all experiments||has||dimension
","Hyperparameters||For||all experiments
",,,,,,
hyperparameters,"is set to 0.1 , and we use dropout with probability 0.5 on sentence / document representations before the output layer .","use
dropout
with
probability 0.5
on
sentence / document representations
before
output layer","dropout||with||probability 0.5
probability 0.5||on||sentence / document representations
sentence / document representations||before||output layer
",,"Hyperparameters||use||dropout
",,,,,,
hyperparameters,We use RMSProp as the optimizer with the decay rate set to 0.9 and the base learning rate set to 0.001 .,"RMSProp
as
optimizer
with
decay rate
set to
0.9
base learning rate
set to
0.001","RMSProp||as||optimizer
optimizer||with||decay rate
decay rate||set to||0.9
optimizer||with||base learning rate
base learning rate||set to||0.001
",,,"Hyperparameters||use||RMSProp
",,,,,
hyperparameters,The mini - batch size is set to 32 .,"mini - batch size
set to
32","mini - batch size||set to||32
",,,"Hyperparameters||use||mini - batch size
",,,,,
results,"We observe that PRET is very helpful , and consistently gives a 1 - 3 % increase in accuracy over LSTM + ATT across all datasets .","observe that
PRET
is
very helpful
consistently gives
1 - 3 % increase
in
accuracy
over
LSTM + ATT","PRET||consistently gives||1 - 3 % increase
1 - 3 % increase||in||accuracy
accuracy||over||LSTM + ATT
PRET||is||very helpful
",,"Results||observe that||PRET
",,,,,,
results,"MULT gives similar performance as LSTM + ATT on D1 and D2 , but improvements can be clearly observed for D3 and D4 .","MULT
gives
similar performance
as
LSTM + ATT
on
D1 and D2","MULT||gives||similar performance
similar performance||as||LSTM + ATT
LSTM + ATT||on||D1 and D2
",,,"Results||has||MULT
",,,,,
results,The combination ( PRET + MULT ) over all yields better results .,"combination ( PRET + MULT )
yields
better results","combination ( PRET + MULT )||yields||better results
",,,"Results||has||combination ( PRET + MULT )
",,,,,
results,( 2 ) The numbers of neutral examples in the test sets of D3 and D4 are very small .,"numbers of neutral examples
in
test sets
of
D3 and D4
are
very small","numbers of neutral examples||in||test sets
test sets||are||very small
test sets||of||D3 and D4
",,,"Results||has||numbers of neutral examples
",,,,,
ablation-analysis,"( 2 ) Overall , transfers of the LSTM and embedding layer are more useful than the output layer .","transfers
of
LSTM and embedding layer
are
more useful
than
output layer","transfers||of||LSTM and embedding layer
LSTM and embedding layer||are||more useful
more useful||than||output layer
",,,"Ablation analysis||has||transfers
",,,,,
ablation-analysis,( 3 ) Transfer of the embedding layer is more helpful on D3 and D4 .,"Transfer of the embedding layer
is
more helpful
on
D3 and D4","Transfer of the embedding layer||is||more helpful
more helpful||on||D3 and D4
",,,"Ablation analysis||has||Transfer of the embedding layer
",,,,,
ablation-analysis,Sentiment information is not adequately captured by Glo Ve word embeddings .,"Sentiment information
not adequately captured by
Glo Ve word embeddings","Sentiment information||not adequately captured by||Glo Ve word embeddings
",,,"Ablation analysis||has||Sentiment information
",,,,,
research-problem,Fine - grained Sentiment Classification using BERT,Fine - grained Sentiment Classification,,,,,"Contribution||has research problem||Fine - grained Sentiment Classification
",,,,
research-problem,"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic .",Sentiment classification,,,,,"Contribution||has research problem||Sentiment classification
",,,,
model,"In this paper , we use the pretrained BERT model and finetune it for the fine - grained sentiment classification task on the Stanford Sentiment Treebank ( SST ) dataset .","use
pretrained BERT model
finetune it for
fine - grained sentiment classification task
on
Stanford Sentiment Treebank ( SST ) dataset","pretrained BERT model||finetune it for||fine - grained sentiment classification task
fine - grained sentiment classification task||on||Stanford Sentiment Treebank ( SST ) dataset
",,"Model||use||pretrained BERT model
",,,,,,
baselines,1 ) Word embeddings :,Word embeddings,,,,"Baselines||has||Word embeddings
",,,,,"Word embeddings||has||word vectors
"
baselines,"In this method , the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector , which is then fed to the sentiment classifier to compute the sentiment score .","word vectors
pretrained on
large text corpus
such as
Wikipedia dump
averaged to get
document vector
fed to
sentiment classifier
to compute
sentiment score","word vectors||averaged to get||document vector
document vector||fed to||sentiment classifier
sentiment classifier||to compute||sentiment score
word vectors||pretrained on||large text corpus
large text corpus||such as||Wikipedia dump
",,,,,,,,
baselines,2 ) Recursive networks :,Recursive networks,,,,"Baselines||has||Recursive networks
",,,,,"Recursive networks||has||Various types of recursive neural networks ( RNN )
"
baselines,Various types of recursive neural networks ( RNN ) have been applied on SST .,"Various types of recursive neural networks ( RNN )
applied on
SST","Various types of recursive neural networks ( RNN )||applied on||SST
",,,,,,,,
baselines,3 ) Recurrent networks :,Recurrent networks,,,,"Baselines||has||Recurrent networks
",,,,,"Recurrent networks||has||Sophisticated recurrent networks
"
baselines,Sophisticated recurrent networks such as left - to - right and bidrectional LSTM networks have also been applied on SST .,"Sophisticated recurrent networks
such as
left - to - right and bidrectional LSTM networks
applied on
SST","Sophisticated recurrent networks||such as||left - to - right and bidrectional LSTM networks
left - to - right and bidrectional LSTM networks||applied on||SST
",,,,,,,,
baselines,4 ) Convolutional networks :,Convolutional networks,,,,"Baselines||has||Convolutional networks
",,,,,"Convolutional networks||has||input sequences
"
baselines,"In this approach , the input sequences were passed through a 1 - dimensional convolutional neural network as feature extractors .","input sequences
passed through
1 - dimensional convolutional neural network
as
feature extractors","input sequences||passed through||1 - dimensional convolutional neural network
1 - dimensional convolutional neural network||as||feature extractors
",,,,,,,,
results,"We can see that our model , despite being a simple architecture , performs better in terms of accuracy than many popular and sophisticated NLP models .","see that
our model
performs
better
in terms of
accuracy
than
many popular and sophisticated NLP models","our model||performs||better
better||in terms of||accuracy
accuracy||than||many popular and sophisticated NLP models
",,"Results||see that||our model
",,,,,,
research-problem,Context - Dependent Sentiment Analysis in User- Generated Videos,Context - Dependent Sentiment Analysis,,,,,"Contribution||has research problem||Context - Dependent Sentiment Analysis
",,,,
research-problem,"Multimodal sentiment analysis is a developing area of research , which involves the identification of sentiments in videos .",identification of sentiments in videos,,,,,"Contribution||has research problem||identification of sentiments in videos
",,,,
research-problem,"Sentiment analysis is a ' suitcase ' research problem that requires tackling many NLP sub - tasks , e.g. , aspect extraction , named entity recognition , concept extraction , sarcasm detection , personality recognition , and more .",Sentiment analysis,,,,,"Contribution||has research problem||Sentiment analysis
",,,,
research-problem,"Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust .",Emotion recognition,,,,,"Contribution||has research problem||Emotion recognition
",,,,
research-problem,"Recently , a number of approaches to multimodal sentiment analysis , producing interesting results , have been proposed .",multimodal sentiment analysis,,,,,"Contribution||has research problem||multimodal sentiment analysis
",,,,
model,"In this paper , we discard such an oversimplifying hypothesis and develop a framework based on long shortterm memory ( LSTM ) that takes a sequence of utterances as input and extracts contextual utterancelevel features .","develop
framework
based on
long shortterm memory ( LSTM )
takes
sequence of utterances
as
input
extracts
contextual utterancelevel features","framework||based on||long shortterm memory ( LSTM )
long shortterm memory ( LSTM )||extracts||contextual utterancelevel features
long shortterm memory ( LSTM )||takes||sequence of utterances
sequence of utterances||as||input
",,"Model||develop||framework
",,,,,,
model,"Our model preserves the sequential order of utterances and enables consecutive utterances to share information , thus providing contextual information to the utterance - level sentiment classification process .","preserves
sequential order
of
utterances
enables
consecutive utterances
to share
information
providing
contextual information
to
utterance - level sentiment classification process","sequential order||of||utterances
consecutive utterances||to share||information
information||providing||contextual information
contextual information||to||utterance - level sentiment classification process
",,"Model||preserves||sequential order
Model||enables||consecutive utterances
",,,,,,
results,"As expected , trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework .","trained contextual unimodal features
help
hierarchical fusion framework
to outperform
non-hierarchical framework","trained contextual unimodal features||help||hierarchical fusion framework
hierarchical fusion framework||to outperform||non-hierarchical framework
",,,"Results||has||trained contextual unimodal features
",,,,,
results,"The non-hierarchical model outperforms the baseline uni - SVM , which confirms that it is the contextsensitive learning paradigm that plays the key role in improving performance over the baseline .","non-hierarchical model
outperforms
baseline uni - SVM","non-hierarchical model||outperforms||baseline uni - SVM
",,,"Results||has||non-hierarchical model
",,,,,
results,It is to be noted that both sc - LSTM and bc - LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets .,"noted
both sc - LSTM and bc - LSTM
perform
quite well
on
multimodal emotion recognition and sentiment analysis datasets","both sc - LSTM and bc - LSTM||perform||quite well
quite well||on||multimodal emotion recognition and sentiment analysis datasets
",,"Results||noted||both sc - LSTM and bc - LSTM
",,,,,,
results,"Since bc - LSTM has access to both the preceding and following information of the utterance sequence , it performs consistently better on all the datasets over sc - LSTM .","bc - LSTM
has access to
preceding and following information
of
utterance sequence
performs
consistently better
on
all the datasets over sc - LSTM","bc - LSTM||has access to||preceding and following information
preceding and following information||of||utterance sequence
utterance sequence||performs||consistently better
consistently better||on||all the datasets over sc - LSTM
",,,"Results||has||bc - LSTM
",,,,,
results,The performance improvement is in the range of 0.3 % to 1.5 % on MOSI and MOUD datasets .,"performance improvement
is
in the range
of
0.3 % to 1.5 %
on
MOSI and MOUD datasets","performance improvement||is||in the range
in the range||of||0.3 % to 1.5 %
0.3 % to 1.5 %||on||MOSI and MOUD datasets
",,,"Results||has||performance improvement
",,,,,
results,"On the IEMOCAP dataset , the performance improvement of bc - LSTM and sc - LSTM over h- LSTM is in the range of 1 % to 5 % .","On
IEMOCAP dataset
performance improvement
of
bc - LSTM and sc - LSTM
over
h- LSTM
in the range of
1 % to 5 %","performance improvement||of||bc - LSTM and sc - LSTM
bc - LSTM and sc - LSTM||over||h- LSTM
performance improvement||in the range of||1 % to 5 %
","IEMOCAP dataset||has||performance improvement
","Results||On||IEMOCAP dataset
",,,,,,
results,Every LSTM network variant has outperformed the baseline uni - SVM on all the datasets by the margin of 2 % to 5 % ( see ) .,"Every LSTM network variant
outperformed
baseline uni - SVM
on
all the datasets
by the margin of
2 % to 5 %","Every LSTM network variant||outperformed||baseline uni - SVM
baseline uni - SVM||on||all the datasets
all the datasets||by the margin of||2 % to 5 %
",,,"Results||has||Every LSTM network variant
",,,,,
results,Experimental results in show that the proposed method outperformes by a significant margin .,"show
proposed method
outperformes by
significant margin","proposed method||outperformes by||significant margin
",,"Results||show||proposed method
",,,,,,
research-problem,MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS,MULTI - MODAL EMOTION RECOGNITION,,,,,"Contribution||has research problem||MULTI - MODAL EMOTION RECOGNITION
",,,,
research-problem,Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .,Emotion recognition,,,,,"Contribution||has research problem||Emotion recognition
",,,,
approach,We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes .,"explore
various deep learning based architectures",,,"Approach||explore||various deep learning based architectures
",,,,,,
approach,We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models .,"combine them in
ensemble based architecture
to allow
training
across
different modalities
using
variations of the better individual models","ensemble based architecture||to allow||training
training||across||different modalities
different modalities||using||variations of the better individual models
",,,,,"various deep learning based architectures||combine them in||ensemble based architecture
",,,
approach,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .","Our ensemble
consists of
Long Short Term Memory networks
Convolution Neural Networks
fully connected Multi - Layer Perceptrons
using techniques
such as
Dropout
adaptive optimizers
Adam
pretrained word - embedding models and Attention based RNN decoders","Our ensemble||consists of||Long Short Term Memory networks
Our ensemble||consists of||Convolution Neural Networks
Our ensemble||consists of||fully connected Multi - Layer Perceptrons
Our ensemble||using techniques||Dropout
Our ensemble||using techniques||adaptive optimizers
adaptive optimizers||such as||Adam
Our ensemble||using techniques||pretrained word - embedding models and Attention based RNN decoders
",,,"Approach||has||Our ensemble
",,,,,
approach,This allows us to individually target each modality and only perform feature fusion at the final stage .,"allows us to
individually target
each
modality
perform feature fusion
at
final stage","perform feature fusion||at||final stage
individually target||each||modality
",,"Approach||allows us to||perform feature fusion
Approach||allows us to||individually target
",,,,,,
hyperparameters,"For the text transcript of each of the utterance we use pretrained Glove embeddings of dimension 300 , along with the maximum sequence length of 500 to obtain a ( 500,300 ) vector for each utterance .","text transcript
of
each of the utterance
use
pretrained Glove embeddings
of
dimension 300
along with
maximum sequence length
of
500
to obtain
( 500,300 ) vector
for
each utterance","text transcript||use||pretrained Glove embeddings
pretrained Glove embeddings||of||dimension 300
pretrained Glove embeddings||along with||maximum sequence length
maximum sequence length||of||500
pretrained Glove embeddings||to obtain||( 500,300 ) vector
( 500,300 ) vector||for||each utterance
text transcript||of||each of the utterance
",,,"Hyperparameters||has||text transcript
",,,,,
hyperparameters,"For the Mocap data , for each different mode such as face , hand , head rotation we sample all the feature values between the start and finish time values and split them into 200 partitioned arrays .","For
Mocap data
for
each different mode
such as
face , hand , head rotation
sample
feature values
between
start and finish time values
split them into
200 partitioned arrays","Mocap data||for||each different mode
each different mode||such as||face , hand , head rotation
each different mode||sample||feature values
feature values||split them into||200 partitioned arrays
feature values||between||start and finish time values
",,"Hyperparameters||For||Mocap data
",,,,,,
hyperparameters,"We then average each of the 200 arrays along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each utterance .","average each of
200 arrays
along
columns ( 165 for faces , 18 for hands , and 6 for rotation )
concatenate all of them to obtain
( 200,189 ) dimension vector
for
utterance","200 arrays||concatenate all of them to obtain||( 200,189 ) dimension vector
( 200,189 ) dimension vector||for||utterance
200 arrays||along||columns ( 165 for faces , 18 for hands , and 6 for rotation )
",,,,,"Mocap data||average each of||200 arrays
",,,
results,"Our performance matches the prior state of the art , however the comparison is not fair .","Our performance
matches
prior state of the art","Our performance||matches||prior state of the art
",,,"Results||has||Our performance
",,,,,
research-problem,Multimodal Speech Emotion Recognition and Ambiguity Resolution,Multimodal Speech Emotion Recognition and Ambiguity Resolution,,,,,"Contribution||has research problem||Multimodal Speech Emotion Recognition and Ambiguity Resolution
",,,,
research-problem,Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .,Identifying emotion from speech,,,,,"Contribution||has research problem||Identifying emotion from speech
",,,,
research-problem,"With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .",Speech Emotion Recognition ( SER ),,,,,"Contribution||has research problem||Speech Emotion Recognition ( SER )
",,,,
research-problem,"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .","explore
implication of hand - crafted features
for
SER
compare
performance
of
lighter machine learning models
with
heavily data - reliant deep learning models","performance||of||lighter machine learning models
lighter machine learning models||with||heavily data - reliant deep learning models
implication of hand - crafted features||for||SER
",,"Model||compare||performance
Model||explore||implication of hand - crafted features
",,"Contribution||has research problem||SER
",,,,
model,"Furthermore , we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution .","combine
features
from
textual modality
to understand
correlation between different modalities
aid
ambiguity resolution","features||from||textual modality
textual modality||to understand||correlation between different modalities
textual modality||aid||ambiguity resolution
",,"Model||combine||features
",,,,,,
model,"For both the approaches , we first extract handcrafted features from the time domain of the audio signal and train the respective models .","extract
handcrafted features
from
time domain
of
audio signal
train
respective models","handcrafted features||from||time domain
time domain||of||audio signal
handcrafted features||train||respective models
",,"Model||extract||handcrafted features
",,,,,,
model,"In the first approach , we train traditional machine learning classifiers , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes and Logistic Regression .","In
first approach
train
traditional machine learning classifiers
namely
Random Forests
Gradient Boosting
Support Vector Machines
Naive - Bayes
Logistic Regression","first approach||train||traditional machine learning classifiers
traditional machine learning classifiers||namely||Random Forests
traditional machine learning classifiers||namely||Gradient Boosting
traditional machine learning classifiers||namely||Support Vector Machines
traditional machine learning classifiers||namely||Naive - Bayes
traditional machine learning classifiers||namely||Logistic Regression
",,"Model||In||first approach
",,,,,,
model,"In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a speech signal .","second approach
build
Multi - Layer Perceptron and an LSTM classifier
to recognize
emotion
given
speech signal","second approach||build||Multi - Layer Perceptron and an LSTM classifier
Multi - Layer Perceptron and an LSTM classifier||to recognize||emotion
emotion||given||speech signal
",,,"Model||In||second approach
",,,,,
experimental-setup,"We use librosa , a Python library , to process the audio files and extract features from them .","use
librosa , a Python library
to process
audio files","librosa , a Python library||to process||audio files
",,"Experimental setup||use||librosa , a Python library
",,,,,,
experimental-setup,"We use scikit - learn and xgboost [ 25 ] , the machine learning libraries for Python , to implement all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP .","scikit - learn and xgboost
to implement
all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP","scikit - learn and xgboost||to implement||all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP
",,,"Experimental setup||use||scikit - learn and xgboost
",,,,,
experimental-setup,We use PyTorch to implement the LSTM classifiers described earlier .,"PyTorch
to implement
LSTM classifiers","PyTorch||to implement||LSTM classifiers
",,,"Experimental setup||use||PyTorch
",,,,,
experimental-setup,"In order to regularize the hidden space of the LSTM classifiers , we use a shut - off mechanism , called dropout , where a fraction of neurons are not used for final prediction .","to regularize
hidden space
of
LSTM classifiers
use
shut - off mechanism
called
dropout
where
fraction of neurons are not used
for
final prediction","hidden space||use||shut - off mechanism
shut - off mechanism||called||dropout
shut - off mechanism||where||fraction of neurons are not used
fraction of neurons are not used||for||final prediction
hidden space||of||LSTM classifiers
",,"Experimental setup||to regularize||hidden space
",,,,,,
experimental-setup,We randomly split our dataset into a train ( 80 % ) and test ( 20 % ) set .,"randomly split
our dataset
into
train ( 80 % ) and test ( 20 % ) set","our dataset||into||train ( 80 % ) and test ( 20 % ) set
",,"Experimental setup||randomly split||our dataset
",,,,,,
experimental-setup,The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing .,"LSTM classifiers
trained on
NVIDIA Titan X GPU","LSTM classifiers||trained on||NVIDIA Titan X GPU
",,,"Experimental setup||has||LSTM classifiers
",,,,,
experimental-setup,We stop the training when we do not see any improvement in validation performance for > 10 epochs .,"stop
training
when
do not see any improvement
in
validation performance
for
> 10 epochs","training||when||do not see any improvement
do not see any improvement||in||validation performance
validation performance||for||> 10 epochs
",,"Experimental setup||stop||training
",,,,,,
results,"From , we can see that our simpler and lighter ML models either outperform or are comparable to the much heavier current state - of - the art on this dataset .","our simpler and lighter ML models
either outperform or are comparable to
much heavier current state - of - the art","our simpler and lighter ML models||either outperform or are comparable to||much heavier current state - of - the art
",,,"Results||has||our simpler and lighter ML models
",,,,,
results,Audio - only results :,Audio - only results,,,,"Results||has||Audio - only results
",,,,,"Audio - only results||has||Performance of LSTM and ARE
"
results,Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .,"Performance of LSTM and ARE
reveals
deep models indeed need a lot of information
to learn
features
as
LSTM classifier
trained on
eight - dimensional features
achieves
very low accuracy
as compared to
end - to - end trained ARE","Performance of LSTM and ARE||reveals||deep models indeed need a lot of information
deep models indeed need a lot of information||to learn||features
features||as||LSTM classifier
LSTM classifier||achieves||very low accuracy
very low accuracy||as compared to||end - to - end trained ARE
LSTM classifier||trained on||eight - dimensional features
",,,,,,,,
results,Text - only results :,Text - only results,,,,"Results||has||Text - only results
",,,,,
results,We observe that the performance of all the models for this setting is similar .,"observe
performance of all the models
is
similar","performance of all the models||is||similar
",,,,,"Text - only results||observe||performance of all the models
",,,
results,c) Audio + Text results :,Audio + Text results,,,,"Results||has||Audio + Text results
",,,,,
results,We see that combining audio and text features gives us a boost of ? 14 % for all the metrics .,"combining
audio and text features
gives
boost
of
? 14 % for all the metrics","audio and text features||gives||boost
boost||of||? 14 % for all the metrics
",,,,,"Audio + Text results||combining||audio and text features
",,,
results,"Overall , we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .","conclude that
our simple ML methods
are
very robust
to have achieved
comparable performance","our simple ML methods||are||very robust
very robust||to have achieved||comparable performance
",,"Results||conclude that||our simple ML methods
",,,,,,
research-problem,Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction,Aspect Polarity Classification and Aspect Term Extraction,,,,,"Contribution||has research problem||Aspect Polarity Classification and Aspect Term Extraction
",,,,
research-problem,Aspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .,"Aspect - based sentiment analysis ( ABSA )
aspect term extraction ( ATE )
aspect polarity classification ( APC )",,,,,"Contribution||has research problem||Aspect - based sentiment analysis ( ABSA )
Contribution||has research problem||aspect term extraction ( ATE )
Contribution||has research problem||aspect polarity classification ( APC )
",,,,
research-problem,Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .,"aspect term polarity inferring
aspect term extraction",,,,,"Contribution||has research problem||aspect term polarity inferring
Contribution||has research problem||aspect term extraction
",,,,
research-problem,"By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets .",aspect polarity classification,,,,,"Contribution||has research problem||aspect polarity classification
",,,,
research-problem,"Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects .",Aspect - based sentiment analysis,,,,,"Contribution||has research problem||Aspect - based sentiment analysis
",,,,
research-problem,The APC task is a kind of classification problem .,APC,,,,,"Contribution||has research problem||APC
",,,,
research-problem,"The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer .",ATE,,,,,"Contribution||has research problem||ATE
",,,,
model,"Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously , this paper proposes a multi-task learning model for aspect - based sentiment analysis .","proposes
multi-task learning model
for
aspect - based sentiment analysis","multi-task learning model||for||aspect - based sentiment analysis
",,"Model||proposes||multi-task learning model
",,,,,,
model,The LCF - ATEPC 3 model proposed in this paper is a novel multilingual and multi-task - oriented model .,"LCF - ATEPC 3 model
is
novel multilingual and multi-task - oriented model","LCF - ATEPC 3 model||is||novel multilingual and multi-task - oriented model
",,,"Model||has||LCF - ATEPC 3 model
",,,,,
model,"The proposed model is based on multi-head self - attention ( MHSA ) and integrates the pre-trained and the local context focus mechanism , namely LCF - ATEPC .","based on
multi-head self - attention ( MHSA )
integrates
pre-trained and the local context focus mechanism
namely
LCF - ATEPC","pre-trained and the local context focus mechanism||namely||LCF - ATEPC
",,"Model||based on||multi-head self - attention ( MHSA )
Model||integrates||pre-trained and the local context focus mechanism
",,,,,,
model,"By training on a small amount of annotated data of aspect and their polarity , the model can be adapted to a large - scale dataset , automatically extracting the aspects and predicting the sentiment polarities .","training on
small amount of annotated data of aspect and their polarity
adapted to
large - scale dataset
automatically extracting
aspects
predicting
sentiment polarities","small amount of annotated data of aspect and their polarity||adapted to||large - scale dataset
large - scale dataset||automatically extracting||aspects
large - scale dataset||predicting||sentiment polarities
",,"Model||training on||small amount of annotated data of aspect and their polarity
",,,,,,
code,The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPC,https://github.com/yangheng95/LCF-ATEPC,,,,,"Contribution||Code||https://github.com/yangheng95/LCF-ATEPC
",,,,
baselines,"ATAE - LSTM is a classical LSTM - based network for the APC task , which applies the attention mechanism to focus on the important words in the context .","ATAE - LSTM
is
classical LSTM - based network
for
APC task
applies
attention mechanism
to focus on
important words
in
context","ATAE - LSTM||is||classical LSTM - based network
classical LSTM - based network||applies||attention mechanism
attention mechanism||to focus on||important words
important words||in||context
classical LSTM - based network||for||APC task
",,,"Baselines||has||ATAE - LSTM
",,,,,
baselines,ATSM -S Peng et al.,ATSM -S,,,,"Baselines||has||ATSM -S
",,,,,
baselines,is a baseline model of the ATSM variations for Chinese language - oriented ABSA task .,"is
baseline model
of
ATSM variations
for
Chinese language - oriented ABSA task","baseline model||of||ATSM variations
ATSM variations||for||Chinese language - oriented ABSA task
",,,,,"ATSM -S||is||baseline model
",,,
baselines,GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs .,"GANN
is
novel neural network model
for
APC task
aimed to solve
shortcomings of traditional RNNs and CNNs","GANN||is||novel neural network model
novel neural network model||aimed to solve||shortcomings of traditional RNNs and CNNs
novel neural network model||for||APC task
",,,"Baselines||has||GANN
",,,,,
baselines,"AEN - is an attentional encoder network based on the pretrained BERT model , which aims to solve the aspect polarity classification .","AEN
is
attentional encoder network
based on
pretrained BERT model
aims to solve
aspect polarity classification","AEN||is||attentional encoder network
attentional encoder network||aims to solve||aspect polarity classification
attentional encoder network||based on||pretrained BERT model
",,,"Baselines||has||AEN
",,,,,
baselines,"BERT - is a BERT - adapted model for Review Reading Comprehension ( RRC ) task , a task inspired by machine reading comprehension ( MRC ) , it could be adapted to aspect - level sentiment classification task .","BERT
is
BERT - adapted model
for
Review Reading Comprehension ( RRC ) task","BERT||is||BERT - adapted model
BERT - adapted model||for||Review Reading Comprehension ( RRC ) task
",,,"Baselines||has||BERT
",,,,,
baselines,BERT - BASE is the basic pretrained BERT model .,"BERT - BASE
is
basic pretrained BERT model","BERT - BASE||is||basic pretrained BERT model
",,,"Baselines||has||BERT - BASE
",,,,,
baselines,"We adapt it to ABSA multi-task learning , which equips the same ability to automatically extract aspect terms and classify aspects polarity as LCF - ATEPC model .","adapt it to
ABSA multi-task learning
equips
same ability
to automatically extract
aspect terms
classify
aspects polarity
as
LCF - ATEPC model","ABSA multi-task learning||equips||same ability
same ability||to automatically extract||aspect terms
same ability||classify||aspects polarity
aspects polarity||as||LCF - ATEPC model
",,,,,"BERT - BASE||adapt it to||ABSA multi-task learning
",,,
baselines,BERT - ADA,BERT - ADA,,,,"Baselines||has||BERT - ADA
",,,,,
baselines,Rietzler et al.,,,,,,,,,,
baselines,"is a domain - adapted BERT - based model proposed for the APC task , which finetuned the BERT - BASE model on task - related corpus .","is
domain - adapted BERT - based model
proposed for
APC task
finetuned
BERT - BASE model
on
task - related corpus","domain - adapted BERT - based model||finetuned||BERT - BASE model
BERT - BASE model||on||task - related corpus
domain - adapted BERT - based model||proposed for||APC task
",,,,,"BERT - ADA||is||domain - adapted BERT - based model
",,,
baselines,"LCF - ATEPC 5 is the multi -task learning model for the ATE and APC tasks , which is based on the the BERT - SPC model and local context focus mechanism .","LCF - ATEPC
is
multi -task learning model
for
ATE and APC tasks","LCF - ATEPC||is||multi -task learning model
multi -task learning model||for||ATE and APC tasks
",,,"Baselines||has||LCF - ATEPC
",,,,,
baselines,LCF - ATE are the variations of the LCF - ATEPC model which only optimize for the ATE task .,"LCF - ATE
are
variations of the LCF - ATEPC model
optimize for
ATE task","LCF - ATE||are||variations of the LCF - ATEPC model
variations of the LCF - ATEPC model||optimize for||ATE task
",,,"Baselines||has||LCF - ATE
",,,,,
baselines,LCF - APC are the variations of LCF - ATEPC and it only optimize for the APC task during training process .,"LCF - APC
are
variations of LCF - ATEPC
optimize for
APC task
during
training process","LCF - APC||are||variations of LCF - ATEPC
variations of LCF - ATEPC||optimize for||APC task
APC task||during||training process
",,,"Baselines||has||LCF - APC
",,,,,
results,"The CDM layer works better on twitter dataset because there are a lot of non-standard grammar usage and language abbreviations within it , and the local context focus techniques can promote to infer the polarity of terms .","CDM layer
works better on
twitter dataset","CDM layer||works better on||twitter dataset
",,,"Results||has||CDM layer
",,,,,
results,"After optimizing the model parameters according to the empirical result , the joint model based on BERT - BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets , such as BERT - PT , AEN - BERT , SDGCN - BERT , and soon .","joint model based on BERT - BASE
achieved
hopeful performance
on
all three datasets
even surpassed
other proposed BERT based improved models
on
some datasets","joint model based on BERT - BASE||even surpassed||other proposed BERT based improved models
other proposed BERT based improved models||on||some datasets
joint model based on BERT - BASE||achieved||hopeful performance
hopeful performance||on||all three datasets
",,,"Results||has||joint model based on BERT - BASE
",,,,,
results,"Compared with the BERT - BASE model , BERT - SPC significantly improves the accuracy and F 1 score of aspect polarity classification .","Compared with
BERT - BASE model
BERT - SPC
significantly improves
accuracy and F 1 score
of
aspect polarity classification","BERT - SPC||significantly improves||accuracy and F 1 score
accuracy and F 1 score||of||aspect polarity classification
","BERT - BASE model||has||BERT - SPC
","Results||Compared with||BERT - BASE model
",,,,,,
results,"In addition , for the first time , BERT - SPC has increased the F 1 score of ATE subtask on three datasets up to 99 % .","for
first time
BERT - SPC
has increased
F 1 score
of
ATE subtask
on
three datasets
up to
99 %","BERT - SPC||has increased||F 1 score
F 1 score||up to||99 %
F 1 score||of||ATE subtask
ATE subtask||on||three datasets
","first time||has||BERT - SPC
","Results||for||first time
",,,,,,
results,"ATEPC - Fusion is a supplementary scheme of LCF mechanism , and it adopts a moderate approach to generate local context features .","ATEPC - Fusion
is
supplementary scheme
of
LCF mechanism
adopts
moderate approach
to generate
local context features","ATEPC - Fusion||adopts||moderate approach
moderate approach||to generate||local context features
ATEPC - Fusion||is||supplementary scheme
supplementary scheme||of||LCF mechanism
",,,"Results||has||ATEPC - Fusion
",,,,,
results,The experimental results show that its performance is also better than the existing BERT - based models .,"show
performance
better than
existing BERT - based models","performance||better than||existing BERT - based models
",,,,,"ATEPC - Fusion||show||performance
",,,
research-problem,Deep Learning For Smile Recognition,Smile Recognition,,,,,"Contribution||has research problem||Smile Recognition
",,,,
research-problem,"Inspired by recent successes of deep learning in computer vision , we propose a novel application of deep convolutional neural networks to facial expression recognition , in particular smile recognition .",facial expression recognition,,,,,"Contribution||has research problem||facial expression recognition
",,,,
model,The input images are fed into a convolution comprising a convolutional and a subsampling layer .,"input images
fed into
convolution
comprising
convolutional and a subsampling layer","input images||fed into||convolution
convolution||comprising||convolutional and a subsampling layer
",,,"Model||has||input images
",,,,,
model,That convolution maybe followed by more convolutions to become gradually more invariant to distortions in the input .,"convolution
followed by
more convolutions
to become
more invariant
to
distortions
in
input","convolution||followed by||more convolutions
more convolutions||to become||more invariant
more invariant||to||distortions
distortions||in||input
",,,"Model||has||convolution
",,,,,
model,"In the second stage , a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions .","In
second stage
regular neural network
follows
convolutions
to discriminate
features
learned by
convolutions","regular neural network||follows||convolutions
convolutions||to discriminate||features
features||learned by||convolutions
","second stage||has||regular neural network
","Model||In||second stage
",,,,,,"second stage||has||output layer
"
model,The output layer consists of two units for smile or no smile .,"output layer
consists of
two units
for
smile or no smile","output layer||consists of||two units
two units||for||smile or no smile
",,,,,,,,
model,"The novelty of this approach is that the exact number of convolutions , number of hidden layers and size of hidden layers are not fixed but subject to extensive model selection in Sec. 4.3 .","novelty
is
exact number of convolutions , number of hidden layers and size of hidden layers
are
not fixed
subject to
model selection","novelty||is||exact number of convolutions , number of hidden layers and size of hidden layers
exact number of convolutions , number of hidden layers and size of hidden layers||subject to||model selection
exact number of convolutions , number of hidden layers and size of hidden layers||are||not fixed
",,,"Model||has||novelty
",,,,,
experimental-setup,"Due to training time constraints , some parameters have been fixed to reasonable and empirical values , such as the size of convolutions ( 5 5 pixels , 32 feature maps ) and the size of subsamplings ( 2 2 pixels using max pooling ) .","some parameters
fixed to
reasonable and empirical values
such as
size of convolutions ( 5 5 pixels , 32 feature maps )
size of subsamplings ( 2 2 pixels using max pooling )","some parameters||fixed to||reasonable and empirical values
reasonable and empirical values||such as||size of convolutions ( 5 5 pixels , 32 feature maps )
reasonable and empirical values||such as||size of subsamplings ( 2 2 pixels using max pooling )
",,,"Experimental setup||has||some parameters
",,,,,
experimental-setup,"All layers use ReLU units , except of softmax being used in the output layer .","All layers
use
ReLU units
except of
softmax
used in
output layer","All layers||except of||softmax
softmax||used in||output layer
All layers||use||ReLU units
",,,"Experimental setup||has||All layers
",,,,,
experimental-setup,The learning rate is fixed to ? = 0.01 and not subject to model selection as it would significantly prolong the model selection .,"learning rate
fixed to
? = 0.01
not subject to
model selection","learning rate||not subject to||model selection
learning rate||fixed to||? = 0.01
",,,"Experimental setup||has||learning rate
",,,,,
experimental-setup,"The same considerations apply to the momentum , which is fixed to = 0.9 .","apply to
momentum
fixed to
= 0.9","momentum||fixed to||= 0.9
",,"Experimental setup||apply to||momentum
",,,,,,
experimental-setup,The entire database has been randomly split into a 60% / 20 % / 20 % training / validation / test ratio .,"entire database
randomly split into
60% / 20 % / 20 % training / validation / test ratio","entire database||randomly split into||60% / 20 % / 20 % training / validation / test ratio
",,,"Experimental setup||has||entire database
",,,,,
experimental-setup,The model is implemented using Lasagne 4 and the generated CUDA code is executed on a Tesla K40c 9 as training on a GPU allows to perform a comprehensive model selection in a feasible amount of time .,"implemented using
Lasagne
generated
CUDA code
executed on
Tesla K40c","CUDA code||executed on||Tesla K40c
",,"Experimental setup||generated||CUDA code
Experimental setup||implemented using||Lasagne
",,,,,,
experimental-setup,Stochastic gradient descent with a batch size of 500 is used .,"Stochastic gradient descent
with
batch size
of
500","Stochastic gradient descent||with||batch size
batch size||of||500
",,,"Experimental setup||has||Stochastic gradient descent
",,,,,
experimental-setup,"contains the four parameters to be optimized : the number of convolutions , the number of hidden layers , the number of units per hidden layer and the dropout factor .","contains
four parameters
to be
optimized
number of convolutions
number of hidden layers
number of units per hidden layer
dropout factor","four parameters||to be||optimized
","four parameters||name||number of convolutions
four parameters||name||number of hidden layers
four parameters||name||number of units per hidden layer
four parameters||name||dropout factor
","Experimental setup||contains||four parameters
",,,,,,
experimental-setup,Each model was trained for 50 epochs in the model selection .,"model
trained for
50 epochs
in
model selection","model||trained for||50 epochs
50 epochs||in||model selection
",,,"Experimental setup||has||model
",,,,,
research-problem,"We present CATENA , a sieve - based system to perform temporal and causal relation extraction and classification from English texts , exploiting the interaction between the temporal and the causal model .",temporal and causal relation extraction and classification,,,,,"Contribution||has research problem||temporal and causal relation extraction and classification
",,,,
model,"The CATENA system includes two main classification modules , one for temporal and the other for causal relations between events .","CATENA system
includes
two main classification modules
one for
temporal
other for
causal relations","CATENA system||includes||two main classification modules
two main classification modules||other for||causal relations
two main classification modules||one for||temporal
",,,"Model||name||CATENA system
",,,,,
model,"As shown in , they both take as input a document annotated with the so - called temporal entities according to TimeML guidelines , including the document creation time ( DCT ) , events and time expressions ( timexes ) .","take as input
document
annotated with
temporal entities
according to
TimeML guidelines
including
document creation time ( DCT )
events and time expressions ( timexes )","document||annotated with||temporal entities
temporal entities||including||document creation time ( DCT )
temporal entities||including||events and time expressions ( timexes )
temporal entities||according to||TimeML guidelines
",,"Model||take as input||document
",,,,,,
model,"The output is the same document with temporal links ( TLINKs ) set between pairs of temporal entities , each assigned to one of the TimeML temporal relation types , such as or SIMULTANEOUS , which denotes the temporal ordering .","output
is
same document
with
temporal links ( TLINKs )
set between
pairs
of
temporal entities","output||is||same document
same document||with||temporal links ( TLINKs )
temporal links ( TLINKs )||set between||pairs
pairs||of||temporal entities
",,,"Model||has||output
",,,,,
model,The document is also annotated with causal relations ( CLINKs ) between event pairs .,"document
annotated with
causal relations ( CLINKs )
between
event pairs","document||annotated with||causal relations ( CLINKs )
causal relations ( CLINKs )||between||event pairs
",,,,,,,"output||has||document
",
model,"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .","modules
for
temporal and causal relation classification
rely both on
sieve - based architecture
in which
remaining unlabelled pairs
after running
rule - based component
transitive reasoner
fed into
supervised classifier","modules||for||temporal and causal relation classification
temporal and causal relation classification||rely both on||sieve - based architecture
sieve - based architecture||in which||remaining unlabelled pairs
remaining unlabelled pairs||after running||rule - based component
temporal and causal relation classification||rely both on||transitive reasoner
transitive reasoner||fed into||supervised classifier
",,,,,,,"output||has||modules
",
results,"The evaluation shows that CATENA is the best performing system in both tasks , even if in Task C best precision and best recall are yielded by and , respectively .","CATENA
is
best performing system
in
both tasks","CATENA||is||best performing system
best performing system||in||both tasks
",,,"Results||has||CATENA
",,,,,
results,"If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while CAEVO still achieves the best results on event - DCT and event - event pairs .","performs best on
timex - timex and event - timex relations
while
CAEVO
achieves
best results
on
event - DCT and event - event pairs","timex - timex and event - timex relations||while||CAEVO
CAEVO||achieves||best results
best results||on||event - DCT and event - event pairs
",,,,,"CATENA||performs best on||timex - timex and event - timex relations
",,,
ablation-analysis,"As expected , running a transitive closure module after the temporal rule - based sieve ( RB + TR ) results in improving recall , but the over all performance is still lacking ( less than .30 F1-score ) .","running
transitive closure module
after
temporal rule - based sieve ( RB + TR )
results in
improving recall","transitive closure module||after||temporal rule - based sieve ( RB + TR )
temporal rule - based sieve ( RB + TR )||results in||improving recall
",,"Ablation analysis||running||transitive closure module
",,,,,,
ablation-analysis,Combining rule - based and machine - learned sieves ( RB + ML ) yields a slight improvement compared with enabling only the machine - learned sieve in the system ( ML ) .,"Combining
rule - based and machine - learned sieves ( RB + ML )
yields
slight improvement
compared with
enabling only the machine - learned sieve
in
system ( ML )","rule - based and machine - learned sieves ( RB + ML )||yields||slight improvement
slight improvement||compared with||enabling only the machine - learned sieve
enabling only the machine - learned sieve||in||system ( ML )
",,"Ablation analysis||Combining||rule - based and machine - learned sieves ( RB + ML )
",,,,,,
ablation-analysis,Introducing the temporal reasoner module between the two sieves ( RB + TR + ML ) proves to be even more beneficial .,"Introducing
temporal reasoner module
between
two sieves ( RB + TR + ML )
proves to be
even more beneficial","temporal reasoner module||between||two sieves ( RB + TR + ML )
two sieves ( RB + TR + ML )||proves to be||even more beneficial
",,"Ablation analysis||Introducing||temporal reasoner module
",,,,,,
research-problem,A Structured Learning Approach to Temporal Relation Extraction,Temporal Relation Extraction,,,,,"Contribution||has research problem||Temporal Relation Extraction
",,,,
research-problem,Identifying temporal relations between events is an essential step towards natural language understanding .,Identifying temporal relations between events,,,,,"Contribution||has research problem||Identifying temporal relations between events
",,,,
research-problem,"The fundamental tasks in temporal processing , as identified in the TE workshops , are 1 ) time expression ( the so - called "" timex "" ) extraction and normalization and 2 ) temporal relation ( also known as TLINKs ) extraction .",temporal processing,,,,,"Contribution||has research problem||temporal processing
",,,,
approach,"In this paper , we propose a structured learning approach to temporal relation extraction , where local models are updated based on feedback from global inferences .","propose
structured learning approach
to
temporal relation extraction
where
local models
are
updated
based on
feedback
from
global inferences","structured learning approach||to||temporal relation extraction
temporal relation extraction||where||local models
local models||are||updated
updated||based on||feedback
feedback||from||global inferences
",,"Approach||propose||structured learning approach
",,,,,,
approach,"The structured approach also gives rise to a semisupervised method , making it possible to take advantage of the readily available unlabeled data .","structured approach
gives rise to
semisupervised method
take advantage of
readily available unlabeled data","structured approach||gives rise to||semisupervised method
semisupervised method||take advantage of||readily available unlabeled data
",,,"Approach||has||structured approach
",,,,,
baselines,The first is the regularized averaged perceptron ( AP ) implemented in the LBJava package and is a local method .,"regularized averaged perceptron ( AP )
implemented in
LBJava package","regularized averaged perceptron ( AP )||implemented in||LBJava package
",,,"Baselines||has||regularized averaged perceptron ( AP )
",,,,,
baselines,"On top of the first baseline , we performed global inference in Eq .","On top of
first baseline
performed
global inference","first baseline||performed||global inference
",,"Baselines||On top of||first baseline
",,,,,,
baselines,"Both of them used the same feature set ( i.e. , as designed in ) as in the proposed structured perceptron ( SP ) and CoDL for fair comparisons .","used
same feature set
as in
proposed structured perceptron ( SP )
CoDL","same feature set||as in||proposed structured perceptron ( SP )
same feature set||as in||CoDL
",,"Baselines||used||same feature set
",,,,,,
results,TE3 Task C - Relation Only,TE3 Task C - Relation Only,,,,"Results||has||TE3 Task C - Relation Only
",,,,,
results,"We can see that UT - Time is about 3 % better than AP - 1 in the absolute value of F 1 , which is expected since UTTime included more advanced features derived from syntactic parse trees .","see
UT - Time
is about
3 %
better than
AP - 1
in
absolute value of F 1","UT - Time||is about||3 %
3 %||better than||AP - 1
AP - 1||in||absolute value of F 1
",,,,,"TE3 Task C - Relation Only||see||UT - Time
",,,
results,"On top of AP - 2 , a global inference step enforcing symmetry and transitivity constraints ( "" AP + ILP "" ) can further improve the F 1 score by 9.3 % , which is consistent with previous observations .","On top of
AP - 2
global inference step
enforcing
symmetry and transitivity constraints
further improve
F 1 score
by
9.3 %","global inference step||enforcing||symmetry and transitivity constraints
symmetry and transitivity constraints||further improve||F 1 score
F 1 score||by||9.3 %
","AP - 2||has||global inference step
",,,,"TE3 Task C - Relation Only||On top of||AP - 2
",,,
results,"SP + ILP further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .","SP + ILP
further improved
performance
in
precision , recall , and F 1
reaching
F 1 score
of
67.2 %","SP + ILP||further improved||performance
performance||in||precision , recall , and F 1
SP + ILP||reaching||F 1 score
F 1 score||of||67.2 %
",,,,,,,"TE3 Task C - Relation Only||has||SP + ILP
",
results,TE3 Task C,TE3 Task C,,,,"Results||has||TE3 Task C
",,,,,"TE3 Task C||has||AP + ILP
"
results,"The improvement of SP + ILP ( line 4 ) over AP ( line 2 ) was small and AP + ILP ( line 3 ) was even worse than AP , which necessitates the use of a better approach towards vague TLINKs .","improvement of
SP + ILP
over
AP
was
small
AP + ILP
even worse than
AP","AP + ILP||even worse than||AP
SP + ILP||over||AP
AP||was||small
",,,,,"TE3 Task C||improvement of||SP + ILP
",,,
results,"By applying the postfiltering method proposed in Sec. 4 , we were able to achieve better performances using SP + ILP ( line 5 ) , which shows the effectiveness of this strategy .","applying
postfiltering method
able to achieve
better performances
using
SP + ILP","postfiltering method||able to achieve||better performances
better performances||using||SP + ILP
",,,,,"TE3 Task C||applying||postfiltering method
",,,
results,Comparison with CAEVO,Comparison with CAEVO,,,,"Results||has||Comparison with CAEVO
",,,,,"Comparison with CAEVO||has||SP + ILP
"
results,"SP + ILP outperformed CAEVO and if additional unlabeled dataset TE3 - SV was used , CoDL + ILP achieved the best score with a relative improvement in F 1 score being 6.3 % .","SP + ILP
outperformed
CAEVO","SP + ILP||outperformed||CAEVO
",,,,,,,,
research-problem,Character - level Convolutional Networks for Text Classification,Text Classification,,,,,"Contribution||has research problem||Text Classification
",,,,
research-problem,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",assign predefined categories to free - text documents,,,,,"Contribution||has research problem||assign predefined categories to free - text documents
",,,,
model,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .","explore
treating text
as a kind of
raw signal
at
character level
applying
temporal ( one-dimensional ) ConvNets","treating text||as a kind of||raw signal
raw signal||at||character level
character level||applying||temporal ( one-dimensional ) ConvNets
",,"Model||explore||treating text
",,,,,,
model,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,"used
classification task
to exemplify
ConvNets ' ability
to understand
texts","classification task||to exemplify||ConvNets ' ability
ConvNets ' ability||to understand||texts
",,"Model||used||classification task
",,,,,,
results,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,"most important conclusion
from
our experiments
is
character - level ConvNets
could work for
text classification
without the need for
words","most important conclusion||from||our experiments
our experiments||is||character - level ConvNets
character - level ConvNets||could work for||text classification
text classification||without the need for||words
",,,"Results||has||most important conclusion
",,,,,
results,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,"most obvious trend
coming from
all the plots
is that
larger datasets
tend to perform
better","most obvious trend||coming from||all the plots
all the plots||is that||larger datasets
larger datasets||tend to perform||better
",,,"Results||has||most obvious trend
",,,,,
results,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .","further analysis
needed to validate
hypothesis
that
ConvNets are truly good
at identifying
exotic character combinations
such as
misspellings and emoticons","further analysis||needed to validate||hypothesis
hypothesis||that||ConvNets are truly good
ConvNets are truly good||at identifying||exotic character combinations
exotic character combinations||such as||misspellings and emoticons
",,,"Results||has||further analysis
",,,,,
results,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .","Comparing with
traditional models
suggests
simple use of a distributed word representation
may not give us
an advantage to text classification","traditional models||suggests||simple use of a distributed word representation
simple use of a distributed word representation||may not give us||an advantage to text classification
",,"Results||Comparing with||traditional models
",,,,,,
research-problem,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,Supervised and Semi- Supervised Text Categorization,,,,,"Contribution||has research problem||Supervised and Semi- Supervised Text Categorization
",,,,
research-problem,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",text categorization,,,,,"Contribution||has research problem||text categorization
",,,,
model,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .","consider
more general framework
subsuming
one - hot CNN
jointly trains
feature generator and a linear model
where
feature generator
consists of
region embedding + pooling","more general framework||subsuming||one - hot CNN
more general framework||jointly trains||feature generator and a linear model
feature generator and a linear model||where||feature generator
feature generator||consists of||region embedding + pooling
",,"Model||consider||more general framework
",,,,,,
model,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .","build on
general framework of ' region embedding + pooling '
explore
more sophisticated region embedding
via
Long Short - Term Memory ( LSTM )","general framework of ' region embedding + pooling '||explore||more sophisticated region embedding
more sophisticated region embedding||via||Long Short - Term Memory ( LSTM )
",,"Model||build on||general framework of ' region embedding + pooling '
",,,,,,
model,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .","pursue
best use of LSTM
compare
the resulting model
with
previous best methods
including
one - hot CNN and previous LSTM","best use of LSTM||compare||the resulting model
the resulting model||with||previous best methods
previous best methods||including||one - hot CNN and previous LSTM
",,"Model||pursue||best use of LSTM
",,,,,,
model,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .","strategy
simplify the model as much as possible
including elimination of
word embedding layer
routinely used to produce
input to LSTM","simplify the model as much as possible||including elimination of||word embedding layer
word embedding layer||routinely used to produce||input to LSTM
",,"Model||strategy||simplify the model as much as possible
",,,,,,
results,Experiments ( supervised ),Experiments ( supervised ),,,,"Results||has||Experiments ( supervised )
",,,,,
results,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .","Comparing
two types of LSTM
see that
one - hot bidirectional LSTM with pooling
oh - 2 LSTMp
outperforms
word - vector LSTM
wv - LSTM
on
all the datasets","two types of LSTM||see that||one - hot bidirectional LSTM with pooling
one - hot bidirectional LSTM with pooling||outperforms||word - vector LSTM
word - vector LSTM||on||all the datasets
","one - hot bidirectional LSTM with pooling||name||oh - 2 LSTMp
word - vector LSTM||name||wv - LSTM
",,,,"Experiments ( supervised )||Comparing||two types of LSTM
",,,
results,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .","on
three out of the four datasets
oh - 2 LSTMp
outperforms
SVM and the CNN","oh - 2 LSTMp||outperforms||SVM and the CNN
","three out of the four datasets||has||oh - 2 LSTMp
",,,,"Experiments ( supervised )||on||three out of the four datasets
",,,
results,Semi-supervised experiments,Semi-supervised experiments,,,,"Results||has||Semi-supervised experiments
",,,,,
results,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .","tested
wv - 2 LSTMp
word - vector bidirectional LSTM with pooling
difference from
oh - 2 LSTMp
is
input to the LSTM layers
is
pre-trained word vectors","wv - 2 LSTMp||difference from||oh - 2 LSTMp
oh - 2 LSTMp||is||input to the LSTM layers
input to the LSTM layers||is||pre-trained word vectors
","wv - 2 LSTMp||name||word - vector bidirectional LSTM with pooling
",,,,"Semi-supervised experiments||tested||wv - 2 LSTMp
",,,
results,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .","review
performance
of
one - hot CNN
with
one 200 - dim CNN tv-embedding
comparable with
our LSTM
with
two 100 - dim LSTM tv-embeddings
in terms of
dimensionality of tv-embeddings","performance||of||one - hot CNN
one - hot CNN||with||one 200 - dim CNN tv-embedding
one 200 - dim CNN tv-embedding||comparable with||our LSTM
our LSTM||with||two 100 - dim LSTM tv-embeddings
two 100 - dim LSTM tv-embeddings||in terms of||dimensionality of tv-embeddings
",,,,,"Semi-supervised experiments||review||performance
",,,
research-problem,Bag of Tricks for Efficient Text Classification,Efficient Text Classification,,,,,"Contribution||has research problem||Efficient Text Classification
",,,,
research-problem,This paper explores a simple and efficient baseline for text classification .,text classification,,,,,"Contribution||has research problem||text classification
",,,,
model,shows a simple linear model with rank constraint .,"shows
simple linear model
with
rank constraint","simple linear model||with||rank constraint
",,"Model||shows||simple linear model
",,,,,,
model,The first weight matrix A is a look - up table over the words .,"weight matrix A
is a
look - up table
over
words","weight matrix A||is a||look - up table
look - up table||over||words
",,,"Model||has||weight matrix A
",,,,,
model,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .","word representations
are then averaged into
text representation
in turn fed to
linear classifier","word representations||are then averaged into||text representation
text representation||in turn fed to||linear classifier
",,,"Model||has||word representations
",,,,,
model,We use the softmax function f to compute the probability distribution over the predefined classes .,"use
softmax function f
to compute
probability distribution
over
predefined classes","softmax function f||to compute||probability distribution
probability distribution||over||predefined classes
",,"Model||use||softmax function f
",,,,,,
model,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .","to improve
running time
use
hierarchical softmax
based on
Huffman coding tree","running time||use||hierarchical softmax
hierarchical softmax||based on||Huffman coding tree
",,"Model||to improve||running time
",,,,,,
model,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .","bag of n-grams
as
additional features
to capture
some partial information
about
local word order","bag of n-grams||as||additional features
additional features||to capture||some partial information
some partial information||about||local word order
",,,"Model||use||bag of n-grams
",,,,,
model,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,"very efficient
in practice
achieving
comparable results
to methods that explicitly use
order","comparable results||to methods that explicitly use||order
",,,,,"bag of n-grams||achieving||comparable results
bag of n-grams||in practice||very efficient
",,,
model,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .","maintain
fast and memory efficient mapping
of
n-grams
by using
hashing trick
with
same hashing function as in
10M bins
only used
bigrams","fast and memory efficient mapping||of||n-grams
n-grams||by using||hashing trick
hashing trick||with||same hashing function as in
hashing trick||with||10M bins
10M bins||only used||bigrams
",,"Model||maintain||fast and memory efficient mapping
",,,,,,
tasks,Sentiment analysis,Sentiment analysis,,,,"Tasks||has||Sentiment analysis
",,,,,"Sentiment analysis||has||Hyperparameters
"
tasks,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .","10
hidden units
run
fastText
for
5 epochs
with
learning rate
selected on
validation set
from
{ 0.05 , 0.1 , 0.25 , 0.5 }","fastText||for||5 epochs
5 epochs||with||learning rate
learning rate||selected on||validation set
learning rate||from||{ 0.05 , 0.1 , 0.25 , 0.5 }
",,,,,"Hyperparameters||run||fastText
Hyperparameters||hidden units||10
",,,
tasks,"On this task , adding bigram information improves the performance by 1 - 4 % .","adding
bigram information
improves the performance by
1 - 4 %","bigram information||improves the performance by||1 - 4 %
",,,,,"Results||adding||bigram information
",,,
tasks,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","our accuracy
slightly better than
char - CNN and char - CRNN
bit worse than
VDCNN","our accuracy||bit worse than||VDCNN
our accuracy||slightly better than||char - CNN and char - CRNN
",,,,,,,"Results||has||our accuracy
",
tasks,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .","increase
accuracy slightly
using
more n-grams
for example with
trigrams
performance on
Sogou
goes up to
97.1 %","accuracy slightly||using||more n-grams
more n-grams||for example with||trigrams
trigrams||performance on||Sogou
Sogou||goes up to||97.1 %
",,,,,"Results||increase||accuracy slightly
",,,
tasks,Tag prediction,Tag prediction,,,,"Tasks||has||Tag prediction
",,,,,"Tag prediction||has||Results
"
tasks,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .","At test time
Tagspace
needs to compute
scores
for
all the classes
makes it
relatively slow
our fast inference
gives
significant speed - up
when
number of classes is large ( more than 300 K here )","Tagspace||needs to compute||scores
scores||for||all the classes
all the classes||makes it||relatively slow
our fast inference||gives||significant speed - up
significant speed - up||when||number of classes is large ( more than 300 K here )
",,,,,"Results||At test time||Tagspace
Results||At test time||our fast inference
",,,
tasks,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .","more than an order of magnitude
faster
to obtain
model
with
better quality","faster||to obtain||model
model||with||better quality
",,,,,"Results||more than an order of magnitude||faster
",,,
research-problem,On the Role of Text Preprocessing in Neural Network Architectures : An Evaluation Study on Text Categorization and Sentiment Analysis,Text Preprocessing in Neural Network Architectures,,,,,"Contribution||has research problem||Text Preprocessing in Neural Network Architectures
",,,,
research-problem,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .",Text preprocessing,,,,,"Contribution||has research problem||Text preprocessing
",,,,
research-problem,"In this paper we focus on the role of preprocessing the input text , particularly in how it is split into individual ( meaning - bearing ) tokens and how it affects the performance of standard neural text classification models based on .","focus on the
role of preprocessing the input text
particularly in
how
split into
individual ( meaning - bearing ) tokens
affects the performance of
standard neural text classification models","how||split into||individual ( meaning - bearing ) tokens
how||affects the performance of||standard neural text classification models
",,,,"Contribution||has research problem||role of preprocessing the input text
","preprocessing the input text||particularly in||how
",,,
code,The accompanying materials of this submission can be downloaded at the following repository : github.com/pedrada88/preproc-textclassification .,github.com/pedrada88/preproc-textclassification,,,,,"Contribution||Code||github.com/pedrada88/preproc-textclassification
",,,,
experimental-setup,We tried with two classification models .,"tried
two classification models",,,"Experimental setup||tried||two classification models
",,,,,,
experimental-setup,"The first one is a standard CNN model similar to that of , using ReLU as non-linear activation function .","first one is
standard CNN model
similar to
using ReLU as non-linear activation function","standard CNN model||similar to||using ReLU as non-linear activation function
",,,,,"two classification models||first one is||standard CNN model
",,,
experimental-setup,"In the second model , we add a recurrent layer ( specifically an LSTM ) before passing the pooled features directly to the fully connected softmax layer .","second
model
add
recurrent layer
specifically an
LSTM
before passing
pooled features
directly to
fully connected softmax layer","model||add||recurrent layer
recurrent layer||specifically an||LSTM
recurrent layer||before passing||pooled features
pooled features||directly to||fully connected softmax layer
",,,,,"two classification models||second||model
",,,
experimental-setup,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,"embedding layer
initialized using
300 - dimensional CBOW Word2vec embeddings
trained on
3B - word UMBC WebBase corpus
with
standard hyperparameters","embedding layer||initialized using||300 - dimensional CBOW Word2vec embeddings
300 - dimensional CBOW Word2vec embeddings||trained on||3B - word UMBC WebBase corpus
3B - word UMBC WebBase corpus||with||standard hyperparameters
",,,"Experimental setup||has||embedding layer
",,,,,
results,Experiment 1 : Preprocessing effect,Experiment 1 : Preprocessing effect,,,,"Results||has||Experiment 1 : Preprocessing effect
",,,,,
results,"Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .","use of
more complex preprocessing techniques
such as
lemmatization and multiword grouping
does not
help","more complex preprocessing techniques||such as||lemmatization and multiword grouping
lemmatization and multiword grouping||does not||help
",,,,,"Experiment 1 : Preprocessing effect||use of||more complex preprocessing techniques
",,,
results,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,,,,"Results||has||Experiment 2 : Cross-preprocessing
",,,,,
results,"In this experiment we observe a different trend , with multiwordenhanced vectors exhibiting a better performance both on the single CNN model ( best over all performance in seven of the nine datasets ) and on the CNN + LSTM model ( best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets ) .","observe
different trend , with multiwordenhanced vectors
exhibiting
better performance
both on
single CNN model
best over all performance in
seven of the nine datasets
CNN + LSTM model
best performance in
four datasets
same ballpark as
best results
in four of
remaining five datasets","different trend , with multiwordenhanced vectors||exhibiting||better performance
better performance||both on||single CNN model
single CNN model||best over all performance in||seven of the nine datasets
better performance||both on||CNN + LSTM model
CNN + LSTM model||best performance in||four datasets
CNN + LSTM model||same ballpark as||best results
best results||in four of||remaining five datasets
",,,,,"Experiment 2 : Cross-preprocessing||observe||different trend , with multiwordenhanced vectors
",,,
results,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .","using
multiword - wise embeddings
on
vanilla setting
leads to
consistently better results
than using them on
same multiwordgrouped preprocessed dataset
in
eight of the nine datasets","multiword - wise embeddings||on||vanilla setting
vanilla setting||leads to||consistently better results
consistently better results||than using them on||same multiwordgrouped preprocessed dataset
same multiwordgrouped preprocessed dataset||in||eight of the nine datasets
",,,,,"Experiment 2 : Cross-preprocessing||using||multiword - wise embeddings
",,,
results,"Apart from this somewhat surprising finding , the use of the embeddings trained on a simple tokenized corpus ( i.e. vanilla ) proved again competitive , as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help .","use of
embeddings
trained on
simple tokenized corpus ( i.e. vanilla )","embeddings||trained on||simple tokenized corpus ( i.e. vanilla )
",,,,,"Experiment 2 : Cross-preprocessing||use of||embeddings
",,,
research-problem,Learning Context - Sensitive Convolutional Filters for Text Processing,Text Processing,,,,,"Contribution||has research problem||Text Processing
",,,,
research-problem,"In this paper , we consider an approach of using a small meta network to learn contextsensitive convolutional filters for text processing .",learn contextsensitive convolutional filters for text processing,,,,,"Contribution||has research problem||learn contextsensitive convolutional filters for text processing
",,,,
approach,"In this paper , we propose a generic approach to learn context - sensitive convolutional filters for natural language understanding .","propose
generic approach
to learn
context - sensitive convolutional filters
for
natural language understanding","generic approach||to learn||context - sensitive convolutional filters
context - sensitive convolutional filters||for||natural language understanding
",,"Approach||propose||generic approach
",,,,,,
approach,"In contrast to traditional CNNs , the convolution operation in our framework does not have a fixed set of filters , and thus provides the network with stronger modeling flexibility and capacity .","convolution operation
in
our framework
does not have
fixed set of filters
provides
network
with
stronger modeling flexibility and capacity","convolution operation||in||our framework
our framework||does not have||fixed set of filters
fixed set of filters||provides||network
network||with||stronger modeling flexibility and capacity
",,,"Approach||has||convolution operation
",,,,,
approach,"Specifically , we introduce a meta network to generate a set of contextsensitive filters , conditioned on specific input sentences ; these filters are adaptively applied to either the same ( Section 3.2 ) or different ( Section 3.3 ) text sequences .","introduce
meta network
to generate
set of contextsensitive filters
conditioned on
specific input sentences","meta network||to generate||set of contextsensitive filters
set of contextsensitive filters||conditioned on||specific input sentences
",,"Approach||introduce||meta network
",,,,,,
approach,"In this manner , the learned filters vary from sentence to sentence and allow for more fine - grained feature abstraction .","learned filters
vary from
sentence to sentence
allow for
more fine - grained feature abstraction","learned filters||allow for||more fine - grained feature abstraction
learned filters||vary from||sentence to sentence
",,,"Approach||has||learned filters
",,,,,
approach,"In this regard , we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context - sensitive representations .","novel bidirectional filter generation mechanism
to allow
interactions between sentence pairs
while constructing
context - sensitive representations","novel bidirectional filter generation mechanism||to allow||interactions between sentence pairs
interactions between sentence pairs||while constructing||context - sensitive representations
",,,"Approach||propose||novel bidirectional filter generation mechanism
",,,,,
experimental-setup,"For the document classification experiments , we randomly initialize the word embeddings uniformly within [ ? 0.001 , 0.001 ] and update them during training .","For
document classification experiments
randomly initialize
word embeddings
uniformly within
[ ? 0.001 , 0.001 ]
update them
during training","document classification experiments||randomly initialize||word embeddings
word embeddings||update them||during training
word embeddings||uniformly within||[ ? 0.001 , 0.001 ]
",,"Experimental setup||For||document classification experiments
",,,,,,
experimental-setup,"For the generated filters , we set the window size as h = 5 , with K = 100 feature maps ( the dimension of z is set as 100 ) .","generated filters
set
window size
as
h = 5
with
K = 100 feature maps","generated filters||set||window size
window size||with||K = 100 feature maps
window size||as||h = 5
window size||as||h = 5
",,,"Experimental setup||For||generated filters
",,,,,
experimental-setup,"A one - layer architec - ture is utilized for both the CNN baseline and the ACNN model , since we did not observe significant performance gains with a multilayer architecture .","one - layer architec - ture
utilized
for
both the CNN baseline and the ACNN model","one - layer architec - ture||for||both the CNN baseline and the ACNN model
",,"Experimental setup||utilized||one - layer architec - ture
",,,,,,
experimental-setup,"The minibatch size is set as 128 , and a dropout rate of 0.2 is utilized on the embedding layer .","minibatch size
is set as
128
dropout rate
of
0.2
is utilized on
embedding layer","minibatch size||is set as||128
dropout rate||of||0.2
0.2||is utilized on||embedding layer
",,,"Experimental setup||has||minibatch size
Experimental setup||has||dropout rate
",,,,,
experimental-setup,"For the sentence matching tasks , we initialized the word embeddings with 50 - dimensional Glove word vectors pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .","sentence matching tasks
initialized
word embeddings
with
50 - dimensional Glove word vectors
pretrained from
Wikipedia 2014 and Gigaword 5","sentence matching tasks||initialized||word embeddings
word embeddings||with||50 - dimensional Glove word vectors
50 - dimensional Glove word vectors||pretrained from||Wikipedia 2014 and Gigaword 5
",,,"Experimental setup||For||sentence matching tasks
",,,,,
experimental-setup,"As for the filters , we set the window size as h = 5 , with K = 300 feature maps .","for
filters
set
window size
as
h = 5
with
K = 300 feature maps","filters||set||window size
h = 5||with||K = 300 feature maps
",,"Experimental setup||for||filters
",,,,,,
experimental-setup,"We use Adam to train the models , with a learning rate of 3 10 ?4 .","use
Adam
to train
models
with
learning rate
of
3 10 ?4","Adam||with||learning rate
learning rate||of||3 10 ?4
Adam||to train||models
",,"Experimental setup||use||Adam
",,,,,,
experimental-setup,"Dropout , with a rate of 0.5 , is employed on the word embedding layer .","Dropout
with a rate of
0.5
employed on
word embedding layer","Dropout||with a rate of||0.5
0.5||employed on||word embedding layer
",,,"Experimental setup||has||Dropout
",,,,,
experimental-setup,All models are implemented with TensorFlow and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12 GB memory .,"All models
implemented with
TensorFlow
trained using
one NVIDIA GeForce GTX TITAN X GPU
with
12 GB memory","All models||implemented with||TensorFlow
All models||trained using||one NVIDIA GeForce GTX TITAN X GPU
one NVIDIA GeForce GTX TITAN X GPU||with||12 GB memory
",,,"Experimental setup||has||All models
",,,,,
baselines,"For document classification , we consider several baseline models : ( i ) ngrams , a bag - of - means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams ( up to 5 - grams ) from the training set and use their corresponding counts as features ; ( ii ) small / large word CNN : 6 layer word - based convolutional networks , with 256/1024 features at each layer , denoted as small / large , respectively ; ( iii ) deep CNN : deep convolutional neural networks with 9/17 /29 layers .","For
document classification
consider
several baseline models
ngrams
bag - of - means method
based on
TFIDF representations
built by
choosing the 500,000 most frequent n-grams ( up to 5 - grams )
from
training set
use
corresponding counts
as
features
small / large word CNN
6 layer word - based convolutional networks
with
256/1024 features
at
each layer
deep CNN
deep convolutional neural networks
with
9/17 /29 layers","document classification||consider||several baseline models
bag - of - means method||based on||TFIDF representations
TFIDF representations||use||corresponding counts
corresponding counts||as||features
TFIDF representations||built by||choosing the 500,000 most frequent n-grams ( up to 5 - grams )
choosing the 500,000 most frequent n-grams ( up to 5 - grams )||from||training set
6 layer word - based convolutional networks||with||256/1024 features
256/1024 features||at||each layer
deep convolutional neural networks||with||9/17 /29 layers
","several baseline models||name||ngrams
ngrams||has||bag - of - means method
several baseline models||name||small / large word CNN
small / large word CNN||has||6 layer word - based convolutional networks
several baseline models||name||deep CNN
deep CNN||has||deep convolutional neural networks
","Baselines||For||document classification
",,,,,,
,Document Classification,Document Classification,,,,,,,,,
,"As illustrated in , S - ACNN significantly outperforms S - CNN on both datasets , demonstrating the advantage of the filtergeneration module in our ACNN framework .","in
S - ACNN
significantly outperforms
S - CNN
on
both datasets
demonstrating the advantage of
filtergeneration module
our ACNN framework",,,,,,,,,
,"Although we only use one convolution layer for our ACNN model , it already outperforms other CNN baseline methods with much deeper architectures .","use
one convolution layer
for
ACNN model
already outperforms
other CNN baseline methods
with
much deeper architectures",,,,,,,,,
,"Moreover , our M - ACNN also achieves slightly better performance than self - attentive sentence embeddings proposed in , which requires significant more parameters than our method .","M - ACNN
achieves slightly better performance than
self - attentive sentence embeddings
requires
significant more parameters
than
our method",,,,,,,,,
results,Answer Sentence Selection,Answer Sentence Selection,,,,"Results||has||Answer Sentence Selection
",,,,,"Answer Sentence Selection||has||our model
"
results,"Notably , our model yields significantly better results than an attentive pooling network and ABCNN ( attention - based CNN ) baselines .","our model
yields
significantly better results
than
attentive pooling network and ABCNN ( attention - based CNN ) baselines","our model||yields||significantly better results
significantly better results||than||attentive pooling network and ABCNN ( attention - based CNN ) baselines
",,,,,,,,
research-problem,Universal Language Model Fine - tuning for Text Classification,Text Classification,,,,,"Contribution||has research problem||Text Classification
",,,,
model,"1 ) We propose Universal Language Model Fine - tuning ( ULMFiT ) , a method that can be used to achieve CV - like transfer learning for any task for NLP .","propose
Universal Language Model Fine - tuning
ULMFiT
that can be used to achieve
CV - like transfer learning
for
any task for NLP","Universal Language Model Fine - tuning||that can be used to achieve||CV - like transfer learning
CV - like transfer learning||for||any task for NLP
","Universal Language Model Fine - tuning||name||ULMFiT
","Model||propose||Universal Language Model Fine - tuning
",,,,,,
model,"2 ) We propose discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing , novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine - tuning .","propose
discriminative fine - tuning
slanted triangular learning rates
gradual unfreezing
to retain
previous knowledge
avoid
catastrophic forgetting
during
fine - tuning","previous knowledge||avoid||catastrophic forgetting
catastrophic forgetting||during||fine - tuning
catastrophic forgetting||propose||discriminative fine - tuning
catastrophic forgetting||propose||slanted triangular learning rates
catastrophic forgetting||propose||gradual unfreezing
",,"Model||to retain||previous knowledge
",,,,,,
hyperparameters,"We use the AWD - LSTM language model with an embedding size of 400 , 3 layers , 1150 hidden activations per layer , and a BPTT batch size of 70 .","use
AWD - LSTM language model
with
embedding size
of
400
3 layers
1150
hidden activations
per layer
BPTT batch size
of
70","AWD - LSTM language model||with||embedding size
embedding size||of||400
AWD - LSTM language model||with||3 layers
AWD - LSTM language model||with||hidden activations
hidden activations||per layer||1150
AWD - LSTM language model||with||BPTT batch size
BPTT batch size||of||70
",,"Hyperparameters||use||AWD - LSTM language model
",,,,,,
hyperparameters,"We apply dropout of 0.4 to layers , 0.3 to RNN layers , 0.4 to input embedding layers , 0.05 to embedding layers , and weight dropout of 0.5 to the RNN hidden - to - hidden matrix .","apply
dropout
0.4
to layers
0.3
to RNN layers
to input embedding layers
0.05
to embedding layers
weight dropout
0.5
to the RNN hidden - to - hidden matrix","weight dropout||0.5||to the RNN hidden - to - hidden matrix
dropout||0.3||to RNN layers
dropout||0.05||to embedding layers
dropout||0.4||to layers
dropout||0.4||to input embedding layers
",,"Hyperparameters||apply||weight dropout
Hyperparameters||apply||dropout
",,,,,,
hyperparameters,The classifier has a hidden layer of size 50 .,"classifier
hidden layer
size
50","hidden layer||size||50
","classifier||has||hidden layer
",,"Hyperparameters||has||classifier
",,,,,
hyperparameters,"We use Adam with ? 1 = 0.7 instead of the default ? 1 = 0.9 and ? 2 = 0.99 , similar to .","Adam
with
? 1 = 0.7
? 2 = 0.99","Adam||with||? 1 = 0.7
Adam||with||? 2 = 0.99
",,,"Hyperparameters||use||Adam
",,,,,
hyperparameters,"We use a batch size of 64 , a base learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .","batch size
of
64
base learning rate
0.004 and 0.01
for
finetuning
LM and the classifier
tune
number of epochs
on
validation set","base learning rate||0.004 and 0.01||for
for||finetuning||LM and the classifier
batch size||of||64
number of epochs||on||validation set
",,"Hyperparameters||tune||number of epochs
","Hyperparameters||use||base learning rate
Hyperparameters||use||batch size
",,,,,
results,"Our method outperforms both CoVe , a state - of - the - art transfer learning method based on hypercolumns , as well as the state - of - the - art on both datasets .","method
outperforms
CoVe
state - of - the - art on both datasets","method||outperforms||CoVe
method||outperforms||state - of - the - art on both datasets
",,,"Results||has||method
",,,,,
results,"On IMDb , we reduce the error dramatically by 43.9 % and 22 % with regard to CoVe and the state - of - the - art respectively .","On
IMDb
reduce
error
by
43.9 % and 22 %","IMDb||reduce||error
error||by||43.9 % and 22 %
",,"Results||On||IMDb
",,,,,,
results,"On TREC - 6 , our improvement - similar as the improvements of state - of - the - art approaches - is not statistically significant , due to the small size of the 500 - examples test set .","TREC - 6
of
not statistically significant
due to the small size
500 - examples test set","TREC - 6||not statistically significant||due to the small size
due to the small size||of||500 - examples test set
",,,"Results||On||TREC - 6
",,,,,
results,"On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .","AG
observe
similarly dramatic error reduction
by
23.7 %
compared to
state - of - the - art","AG||observe||similarly dramatic error reduction
similarly dramatic error reduction||by||23.7 %
23.7 %||compared to||state - of - the - art
",,,"Results||On||AG
",,,,,
results,"On DBpedia , Yelp - bi , and Yelp - full , we reduce the error by 4.8 % , 18.2 % , 2.0 % respectively .","DBpedia , Yelp - bi , and Yelp - full
reduce the error
4.8 % , 18.2 % , 2.0 %","DBpedia , Yelp - bi , and Yelp - full||reduce the error||4.8 % , 18.2 % , 2.0 %
",,,"Results||On||DBpedia , Yelp - bi , and Yelp - full
",,,,,
ablation-analysis,Low - shot learning,Low - shot learning,,,,"Ablation analysis||has||Low - shot learning
",,,,,
ablation-analysis,"On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .","On
IMDb and AG
supervised ULMFiT
with only
100 labeled examples
matches the performance of
training from scratch
with
10 and 20 more data","supervised ULMFiT||matches the performance of||training from scratch
training from scratch||with||10 and 20 more data
supervised ULMFiT||with only||100 labeled examples
","IMDb and AG||has||supervised ULMFiT
",,,,"Low - shot learning||On||IMDb and AG
",,,
ablation-analysis,"On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results .","TREC - 6
ULMFiT
significantly improves upon
training from scratch","ULMFiT||significantly improves upon||training from scratch
","TREC - 6||has||ULMFiT
",,,,,,"Low - shot learning||On||TREC - 6
",
ablation-analysis,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .","Pretraining
most useful for
small and medium - sized datasets","Pretraining||most useful for||small and medium - sized datasets
",,,"Ablation analysis||has||Pretraining
",,,,,
ablation-analysis,Impact of LM quality,Impact of LM quality,,,,"Ablation analysis||has||Impact of LM quality
",,,,,
ablation-analysis,"Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .","Using
our fine - tuning techniques
reaches
surprisingly good performance
on
larger datasets","our fine - tuning techniques||reaches||surprisingly good performance
surprisingly good performance||on||larger datasets
",,,,,"Impact of LM quality||Using||our fine - tuning techniques
",,,
ablation-analysis,"On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance .","On
smaller TREC - 6
vanilla LM without dropout
runs the risk of
overfitting","vanilla LM without dropout||runs the risk of||overfitting
","smaller TREC - 6||has||vanilla LM without dropout
",,,,"Impact of LM quality||On||smaller TREC - 6
",,,
,Fine - tuning the LM is most beneficial for larger datasets .,"Fine - tuning
LM
most beneficial for
larger datasets",,,,,,,,,
ablation-analysis,"Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG .","Fine - tuning
classifier
significantly improves over
training from scratch
particularly on
small TREC - 6","classifier||significantly improves over||training from scratch
training from scratch||particularly on||small TREC - 6
","Fine - tuning||has||classifier
",,"Ablation analysis||has||Fine - tuning
",,,,,
ablation-analysis,"At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 .","of
forward and backwards LM - classifier
brings
performance boost
0.5 - 0.7","forward and backwards LM - classifier||brings||performance boost
performance boost||of||0.5 - 0.7
",,,"Ablation analysis||has||forward and backwards LM - classifier
",,,,,
ablation-analysis,On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model .,"On
IMD b
lower
test error
from
5.30
of
single model
to
4.58
for
bidirectional model","IMD b||lower||test error
test error||from||5.30
5.30||of||single model
test error||to||4.58
4.58||for||bidirectional model
",,,,,"forward and backwards LM - classifier||On||IMD b
",,,
research-problem,Universal Sentence Encoder,Universal Sentence Encoder,,,,,"Contribution||has research problem||Universal Sentence Encoder
",,,,
research-problem,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,encoding sentences into embedding vectors that specifically target transfer learning,,,,,"Contribution||has research problem||encoding sentences into embedding vectors that specifically target transfer learning
",,,,
research-problem,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,transfer learning using sentence embeddings,,,,,"Contribution||has research problem||transfer learning using sentence embeddings
",,,,
research-problem,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .",transfer learning via sentence embeddings,,,,,"Contribution||has research problem||transfer learning via sentence embeddings
",,,,
code,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,https://tfhub.dev/google/universal-sentence-encoder/1,,,,,"Contribution||Code||https://tfhub.dev/google/universal-sentence-encoder/1
",,,,
model,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,"based
sentence encoding model
constructs
sentence embeddings
using
encoding sub - graph of the transformer architecture","sentence encoding model||constructs||sentence embeddings
sentence embeddings||using||encoding sub - graph of the transformer architecture
",,,,,"Transformer||based||sentence encoding model
",,,
model,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,"uses
attention
to compute
context aware representations
of
words in a sentence
that take into account
ordering and identity of all the other words","attention||to compute||context aware representations
context aware representations||of||words in a sentence
words in a sentence||that take into account||ordering and identity of all the other words
",,,,,"Transformer||uses||attention
",,,
model,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,"context aware word representations
converted
to a
fixed length sentence encoding vector
by computing
element - wise sum
of
representations at each word position","context aware word representations||to a||fixed length sentence encoding vector
fixed length sentence encoding vector||by computing||element - wise sum
element - wise sum||of||representations at each word position
",,,,,"Transformer||converted||context aware word representations
",,,
model,The encoding model is designed to be as general purpose as possible .,"encoding model
designed to be
general purpose","encoding model||designed to be||general purpose
",,,,,,,"Transformer||has||encoding model
",
model,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,"accomplished by
using multi-task learning
whereby
a single encoding model
is used to feed
multiple downstream tasks","using multi-task learning||whereby||a single encoding model
a single encoding model||is used to feed||multiple downstream tasks
",,,,,"general purpose||accomplished by||using multi-task learning
",,,
model,Deep Averaging Network ( DAN ),Deep Averaging Network ( DAN ),,,,"Model||has||Deep Averaging Network ( DAN )
",,,,,
model,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,"whereby
input embeddings for words and bi-grams
are first
averaged together
then passed through
feedforward deep neural network ( DNN )
to produce
sentence embeddings","input embeddings for words and bi-grams||then passed through||feedforward deep neural network ( DNN )
feedforward deep neural network ( DNN )||to produce||sentence embeddings
input embeddings for words and bi-grams||are first||averaged together
",,,,,"Deep Averaging Network ( DAN )||whereby||input embeddings for words and bi-grams
",,,
model,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .","Transformer
takes as input
lowercased PTB tokenized string
outputs
512 dimensional sentence embedding","lowercased PTB tokenized string||outputs||512 dimensional sentence embedding
",,,,,"Deep Averaging Network ( DAN )||takes as input||lowercased PTB tokenized string
",,,
model,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,"use
mul-titask learning
whereby
single DAN encoder
used to supply
sentence embeddings
for
multiple downstream tasks","mul-titask learning||whereby||single DAN encoder
single DAN encoder||used to supply||sentence embeddings
sentence embeddings||for||multiple downstream tasks
",,,,,"Deep Averaging Network ( DAN )||use||mul-titask learning
",,,
model,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,"primary advantage
of
is
compute time
linear in the length
input sequence","compute time||is||linear in the length
linear in the length||of||input sequence
",,,,,"Deep Averaging Network ( DAN )||primary advantage||compute time
",,,
tasks,MR : Movie review snippet sentiment on a five star scale .,"MR
Movie review snippet sentiment
on
five star scale","Movie review snippet sentiment||on||five star scale
","MR||has||Movie review snippet sentiment
",,"Tasks||has||MR
",,,,,
tasks,CR : Sentiment of sentences mined from customer reviews .,"CR
Sentiment of sentences
mined from
customer reviews","Sentiment of sentences||mined from||customer reviews
","CR||has||Sentiment of sentences
",,"Tasks||has||CR
",,,,,
tasks,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,"SUBJ
Subjectivity of sentences
from
movie reviews and plot summaries","Subjectivity of sentences||from||movie reviews and plot summaries
","SUBJ||has||Subjectivity of sentences
",,"Tasks||has||SUBJ
",,,,,
tasks,MPQA : Phrase level opinion polarity from news data .,"MPQA
Phrase level opinion polarity
from
news data","Phrase level opinion polarity||from||news data
","MPQA||has||Phrase level opinion polarity
",,"Tasks||has||MPQA
",,,,,
tasks,TREC : Fine grained question classification sourced from TREC .,"TREC
Fine grained question classification
sourced from
TREC","Fine grained question classification||sourced from||TREC
","TREC||has||Fine grained question classification
",,"Tasks||has||TREC
",,,,,
tasks,SST : Binary phrase level sentiment classification .,"SST
Binary phrase level sentiment classification",,"SST||has||Binary phrase level sentiment classification
",,"Tasks||has||SST
",,,,,
tasks,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,"STS Benchmark
Semantic textual similarity ( STS )
between
sentence pairs
scored by
Pearson correlation with human judgments","Semantic textual similarity ( STS )||between||sentence pairs
sentence pairs||scored by||Pearson correlation with human judgments
","STS Benchmark||has||Semantic textual similarity ( STS )
",,"Tasks||has||STS Benchmark
",,,,,
tasks,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,"WEAT
Word pairs
from
psychology literature
on
implicit association tests ( IAT )","Word pairs||from||psychology literature
psychology literature||on||implicit association tests ( IAT )
","WEAT||has||Word pairs
",,"Tasks||has||WEAT
",,,,,
baselines,"For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .","For
word level transfer
use
word embeddings from a word2 vec skip - gram model
trained on
corpus
of
news data","word level transfer||use||word embeddings from a word2 vec skip - gram model
word embeddings from a word2 vec skip - gram model||trained on||corpus
corpus||of||news data
",,"Baselines||For||word level transfer
",,,,,,
baselines,The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .,"input to
convolutional neural network models
CNN
DAN",,"convolutional neural network models||name||CNN
",,,,"word embeddings from a word2 vec skip - gram model||input to||convolutional neural network models
word embeddings from a word2 vec skip - gram model||input to||DAN
",,,
baselines,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,"Additional baseline
CNN and DAN models
are trained without using
pretrained word or sentence embeddings","CNN and DAN models||are trained without using||pretrained word or sentence embeddings
","Additional baseline||has||CNN and DAN models
",,"Baselines||has||Additional baseline
",,,,,
results,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,"observe
transfer learning
from
transformer based sentence encoder
performs as good or better than
transfer learning
from
DAN encoder","transfer learning||from||transformer based sentence encoder
transformer based sentence encoder||performs as good or better than||transfer learning
transfer learning||from||DAN encoder
",,"Results||observe||transfer learning
",,,,,,
results,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,"Models
make use of
sentence level transfer learning
tend to perform better than
models that only use word level transfer","Models||make use of||sentence level transfer learning
sentence level transfer learning||tend to perform better than||models that only use word level transfer
",,,"Results||has||Models
",,,,,
results,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .","for
smaller quantities of data
sentence level transfer learning
can achieve
surprisingly good task performance","sentence level transfer learning||for||smaller quantities of data
smaller quantities of data||can achieve||surprisingly good task performance
",,,"Results||observe||sentence level transfer learning
",,,,,
results,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .","As
training set size increases
models that do not make use of transfer learning
approach the performance of
other models","models that do not make use of transfer learning||approach the performance of||other models
","training set size increases||has||models that do not make use of transfer learning
","Results||As||training set size increases
",,,,,,
research-problem,Investigating Capsule Networks with Dynamic Routing for Text Classification,Text Classification,,,,,"Contribution||has research problem||Text Classification
",,,,
code,1 Codes are publicly available at : https://github.com/andyweizhao/capsule_text_classification .,https://github.com/andyweizhao/capsule_text_classification,,,,,"Contribution||Code||https://github.com/andyweizhao/capsule_text_classification
",,,,
research-problem,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,Modeling articles or sentences computationally,,,,,"Contribution||has research problem||Modeling articles or sentences computationally
",,,,
research-problem,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",text categorization,,,,,"Contribution||has research problem||text categorization
",,,,
model,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",,,,,,,,,,
model,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .","consists of
four layers
ngram convolutional layer
primary capsule layer
convolutional capsule layer
fully connected capsule layer",,"four layers||has||ngram convolutional layer
four layers||has||primary capsule layer
four layers||has||convolutional capsule layer
four layers||has||fully connected capsule layer
","Model||consists of||four layers
",,,,,,
model,"In addition , we explore two capsule frameworks to integrate these four components in different ways .","explore
two capsule frameworks
to integrate
four components in different ways","two capsule frameworks||to integrate||four components in different ways
",,"Model||explore||two capsule frameworks
",,,,,,
model,N - gram Convolutional Layer,N - gram Convolutional Layer,,,,"Model||has||N - gram Convolutional Layer
",,,,,
model,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,"is a
standard convolutional layer
extracts
n-gram features
at different positions of
sentence
through
various convolutional filters","standard convolutional layer||extracts||n-gram features
n-gram features||at different positions of||sentence
sentence||through||various convolutional filters
",,,,,"N - gram Convolutional Layer||is a||standard convolutional layer
",,,
model,Primary Capsule Layer,Primary Capsule Layer,,,,"Model||has||Primary Capsule Layer
",,,,,
model,This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .,"is
first capsule layer
in which
capsules
replace
scalar - output feature detectors of CNNs
with
vector- output capsules
to preserve
instantiated parameters
such as
local order of words
semantic representations of words","first capsule layer||in which||capsules
capsules||replace||scalar - output feature detectors of CNNs
scalar - output feature detectors of CNNs||with||vector- output capsules
vector- output capsules||to preserve||instantiated parameters
instantiated parameters||such as||local order of words
instantiated parameters||such as||semantic representations of words
",,,,,"Primary Capsule Layer||is||first capsule layer
",,,
model,Dynamic Routing,Dynamic Routing,,,,"Model||has||Dynamic Routing
",,,,,
model,The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,"to construct
non-linear map
in
iterative manner
ensuring
output of each capsule
gets sent
appropriate parent in the subsequent layer","non-linear map||in||iterative manner
iterative manner||ensuring||output of each capsule
output of each capsule||gets sent||appropriate parent in the subsequent layer
",,,,,"Dynamic Routing||to construct||non-linear map
",,,
model,Convolutional Capsule Layer,Convolutional Capsule Layer,,,,"Model||has||Convolutional Capsule Layer
",,,,,"Convolutional Capsule Layer||has||each capsule
"
model,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .","each capsule
is connected only to
local region K 2 C
spatially in
layer below","each capsule||is connected only to||local region K 2 C
local region K 2 C||spatially in||layer below
",,,,,,,,
model,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,"multiply
transformation matrices
to learn
child - parent relationships
followed by
routing by agreement
to produce
parent capsules in the layer above","transformation matrices||to learn||child - parent relationships
child - parent relationships||followed by||routing by agreement
routing by agreement||to produce||parent capsules in the layer above
",,,,,"Convolutional Capsule Layer||multiply||transformation matrices
",,,
model,Fully Connected Capsule Layer,Fully Connected Capsule Layer,,,,"Model||has||Fully Connected Capsule Layer
",,,,,"Fully Connected Capsule Layer||has||capsules in the layer below
"
model,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,"capsules in the layer below
flattened into
list of capsules
fed into
fully connected capsule layer","capsules in the layer below||flattened into||list of capsules
list of capsules||fed into||fully connected capsule layer
",,,,,,,,
model,The Architectures of Capsule Network,Architectures of Capsule Network,,,,"Model||has||Architectures of Capsule Network
",,,,,
model,We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four,"explore
two capsule architectures
Capsule - A
Capsule - B",,"two capsule architectures||name||Capsule - A
two capsule architectures||name||Capsule - B
",,,,"Architectures of Capsule Network||explore||two capsule architectures
",,,
hyperparameters,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .","use
300 - dimensional word2vec vectors
to initialize
embedding vectors","300 - dimensional word2vec vectors||to initialize||embedding vectors
",,"Hyperparameters||use||300 - dimensional word2vec vectors
",,,,,,
hyperparameters,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,"conduct
mini-batch
with
size 50
for
AG 's news
size 25
for
other datasets","mini-batch||with||size 50
size 50||for||AG 's news
mini-batch||with||size 25
size 25||for||other datasets
",,"Hyperparameters||conduct||mini-batch
",,,,,,
hyperparameters,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,"Adam optimization algorithm
with
1e - 3 learning rate
to train
model","Adam optimization algorithm||with||1e - 3 learning rate
1e - 3 learning rate||to train||model
",,,"Hyperparameters||use||Adam optimization algorithm
",,,,,
hyperparameters,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,"3 iteration of routing
for
all datasets","3 iteration of routing||for||all datasets
",,,"Hyperparameters||use||3 iteration of routing
",,,,,
baselines,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .","including
LSTM / Bi - LSTM
tree - structured LSTM ( Tree - LSTM )
LSTM regularized by linguistic knowledge ( LR - LSTM )
CNNrand / CNN - static / CNN - non-static ( Kim , 2014 )
very deep convolutional network ( VD - CNN )
character - level convolutional network ( CL - CNN )",,,"Baselines||including||tree - structured LSTM ( Tree - LSTM )
Baselines||including||LSTM regularized by linguistic knowledge ( LR - LSTM )
Baselines||including||CNNrand / CNN - static / CNN - non-static ( Kim , 2014 )
Baselines||including||very deep convolutional network ( VD - CNN )
Baselines||including||character - level convolutional network ( CL - CNN )
",,,,,,"Baselines||including|| LSTM / Bi - LSTM
"
results,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .","observe that
capsule networks
achieve best results on
4 out of 6 benchmarks","capsule networks||achieve best results on||4 out of 6 benchmarks
",,"Results||observe that||capsule networks
",,,,,,
ablation-analysis,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .","all three proposed dynamic routing strategies
contribute to
effectiveness of Capsule - B
by alleviating
disturbance of some noise capsules","all three proposed dynamic routing strategies||contribute to||effectiveness of Capsule - B
effectiveness of Capsule - B||by alleviating||disturbance of some noise capsules
",,,"Ablation analysis||has||all three proposed dynamic routing strategies
",,,,,
research-problem,Baseline Needs More Love : On Simple Word - Embedding - Based Models and Associated Pooling Mechanisms,Simple Word - Embedding - Based Models,,,,,"Contribution||has research problem||Simple Word - Embedding - Based Models
",,,,
research-problem,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring a substantial number of parameters and expensive computations .",model the compositionality in text sequences,,,,,"Contribution||has research problem||model the compositionality in text sequences
",,,,
code,The source code and datasets can be obtained from https://github.com/dinghanshen/SWEM .,https://github.com/dinghanshen/SWEM,,,,,"Contribution||Code||https://github.com/dinghanshen/SWEM
",,,,
research-problem,"Leveraging the word - embedding construct , many deep architectures have been proposed to model the compositionality in variable - length text sequences .",model the compositionality in variable - length text sequences,,,,,"Contribution||has research problem||model the compositionality in variable - length text sequences
",,,,
approach,"In this paper , we conduct an extensive experimental investigation to understand when , and why , simple pooling strategies , operated over word embeddings alone , already carry sufficient information for natural language understanding .","conduct
extensive experimental investigation
to understand
when , and why
simple pooling strategies
operated over word embeddings alone
already carry sufficient information for
natural language understanding","extensive experimental investigation||to understand||when , and why
when , and why||simple pooling strategies||operated over word embeddings alone
operated over word embeddings alone||already carry sufficient information for||natural language understanding
",,"Approach||conduct||extensive experimental investigation
",,,,,,
approach,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with existing recurrent and convolutional networks in a pointby - point manner .","To ac- count for
distinct nature of various NLP tasks that may require different semantic features
compare
SWEM - based models
with
existing recurrent and convolutional networks
in a
pointby - point manner","distinct nature of various NLP tasks that may require different semantic features||compare||SWEM - based models
SWEM - based models||with||existing recurrent and convolutional networks
existing recurrent and convolutional networks||in a||pointby - point manner
",,"Approach||To ac- count for||distinct nature of various NLP tasks that may require different semantic features
",,,,,,
results,"Interestingly , for the sentiment analysis tasks , both CNN and LSTM compositional functions perform better than SWEM , suggesting that wordorder information maybe required for analyzing sentiment orientations .","for
sentiment analysis
both
CNN and LSTM compositional functions
perform better than
SWEM","sentiment analysis||both||CNN and LSTM compositional functions
CNN and LSTM compositional functions||perform better than||SWEM
",,"Results||for||sentiment analysis
",,,,,,
hyperparameters,We use Glo Ve word embeddings with K = 300 as initialization for all our models .,"use
Glo Ve word embeddings
with
K = 300
as
initialization
for
all our models","Glo Ve word embeddings||with||K = 300
K = 300||as||initialization
initialization||for||all our models
",,"Hyperparameters||use||Glo Ve word embeddings
",,,,,,
hyperparameters,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .","Out - Of - Vocabulary ( OOV ) words
initialized from
uniform distribution
with range
[ ? 0.01 , 0.01 ]","Out - Of - Vocabulary ( OOV ) words||initialized from||uniform distribution
uniform distribution||with range||[ ? 0.01 , 0.01 ]
",,,"Hyperparameters||has||Out - Of - Vocabulary ( OOV ) words
",,,,,
hyperparameters,"The Glo Ve embeddings are employed in two ways to learn refined word embeddings : ( i ) directly updating each word embedding during training ; and ( ii ) training a 300 dimensional Multilayer Perceptron ( MLP ) layer with ReLU activation , with Glo Ve embeddings as input to the MLP and with output defining the refined word embeddings .","Glo Ve embeddings
employed in
two ways
to learn
refined word embeddings
updating
each word embedding
during
training
training
300 dimensional Multilayer Perceptron ( MLP ) layer
with
ReLU activation
Glo Ve embeddings
as input to
MLP
output
refined word embeddings","Glo Ve embeddings||employed in||two ways
two ways||updating||each word embedding
each word embedding||during||training
two ways||to learn||refined word embeddings
two ways||training||300 dimensional Multilayer Perceptron ( MLP ) layer
300 dimensional Multilayer Perceptron ( MLP ) layer||with||ReLU activation
300 dimensional Multilayer Perceptron ( MLP ) layer||with||Glo Ve embeddings
Glo Ve embeddings||output||refined word embeddings
Glo Ve embeddings||as input to||MLP
",,,"Hyperparameters||has||Glo Ve embeddings
",,,,,
hyperparameters,"Adam ) is used to optimize all models , with learning rate selected from .","Adam
optimize all models",,,"Hyperparameters||Adam||optimize all models
",,,,,,
results,"Surprisingly , on topic prediction tasks , our SWEM model exhibits stronger performances , relative to both LSTM and CNN compositional architectures , this by leveraging both the average and max - pooling features from word embeddings .","on
topic prediction tasks
SWEM model
exhibits
stronger performances
relative to both
LSTM and CNN compositional architectures
by leveraging both
average and max - pooling features
from
word embeddings","SWEM model||exhibits||stronger performances
stronger performances||relative to both||LSTM and CNN compositional architectures
LSTM and CNN compositional architectures||by leveraging both||average and max - pooling features
average and max - pooling features||from||word embeddings
","topic prediction tasks||has||SWEM model
","Results||on||topic prediction tasks
",,,,,,
results,"On the ontology classification problem ( DBpedia dataset ) , we observe the same trend , that SWEM exhibits comparable or even superior results , relative to CNN or LSTM models .","On
ontology classification problem
observe
SWEM
exhibits
comparable or even superior results
relative to
CNN or LSTM models","ontology classification problem||observe||SWEM
SWEM||exhibits||comparable or even superior results
comparable or even superior results||relative to||CNN or LSTM models
",,"Results||On||ontology classification problem
",,,,,,
results,Text Sequence Matching,Text Sequence Matching,,,,"Results||has||Text Sequence Matching
",,,,,
results,"Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .","on
most of the datasets considered ( except WikiQA )
SWEM
demonstrates
best results
compared with
CNN or the LSTM encoder","SWEM||demonstrates||best results
best results||compared with||CNN or the LSTM encoder
","most of the datasets considered ( except WikiQA )||has||SWEM
",,,,"Text Sequence Matching||on||most of the datasets considered ( except WikiQA )
",,,
results,"Notably , on SNLI dataset , we observe that SWEM - max performs the best among all SWEM variants , consistent with the findings in Nie and Bansal ( 2017 ) ; , that max - pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset .","SNLI dataset
observe that
SWEM - max
performs the best
among all SWEM variants","SNLI dataset||observe that||SWEM - max
SWEM - max||performs the best||among all SWEM variants
",,,,,,,"Text Sequence Matching||on||SNLI dataset
",
results,SWEM - hier for sentiment analysis,SWEM - hier for sentiment analysis,,,,"Results||has||SWEM - hier for sentiment analysis
",,,,,"SWEM - hier for sentiment analysis||has||word - order information
"
results,"As demonstrated in Section 4.2.1 , word - order information plays a vital role for sentiment analysis tasks .","word - order information
plays a vital role for
sentiment analysis tasks","word - order information||plays a vital role for||sentiment analysis tasks
",,,,,,,,
results,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :","most important features for
sentiment prediction
maybe
some key n-gram phrase / words","sentiment prediction||maybe||some key n-gram phrase / words
",,,,,"SWEM - hier for sentiment analysis||most important features for||sentiment prediction
",,,
results,"SWEM - hier greatly outperforms the other three SWEM variants , and the corresponding accuracies are comparable to the results of CNN or LSTM ) .","greatly outperforms
other three SWEM variants
corresponding accuracies
comparable to
results of CNN or LSTM","corresponding accuracies||comparable to||results of CNN or LSTM
","other three SWEM variants||has||corresponding accuracies
",,,,"SWEM - hier for sentiment analysis||greatly outperforms||other three SWEM variants
",,,
results,Short Sentence Processing,Short Sentence Processing,,,,"Results||has||Short Sentence Processing
",,,,,"Short Sentence Processing||has||SWEM
"
results,"Compared with CNN / LSTM compositional functions , SWEM yields inferior accuracies on sentiment analysis datasets , consistent with our observation in the case of document categorization .","SWEM
yields
inferior accuracies
on
sentiment analysis datasets","SWEM||yields||inferior accuracies
inferior accuracies||on||sentiment analysis datasets
",,,,,,,,
results,"However , SWEM exhibits comparable performance on the other two tasks , again with much less parameters and faster training .","exhibits
comparable performance
on
other two tasks
with
much less parameters and faster training","comparable performance||on||other two tasks
other two tasks||with||much less parameters and faster training
",,,,,"SWEM||exhibits||comparable performance
",,,
research-problem,Translations as Additional Contexts for Sentence Classification,Sentence Classification,,,,,"Contribution||has research problem||Sentence Classification
",,,,
research-problem,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .","given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative )",,,,,"Contribution||has research problem||given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative )
",,,,
model,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .","present
neural attentionbased multiple context fixing attachment
MCFA",,"neural attentionbased multiple context fixing attachment||name||MCFA
","Model||present||neural attentionbased multiple context fixing attachment
",,,,,,
model,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .","is
series of modules
uses
all the sentence vectors
e.g.
Arabic , English , Korean
as
context
to fix
sentence vector ( e.g. Korean )","series of modules||uses||all the sentence vectors
all the sentence vectors||as||context
all the sentence vectors||e.g.||Arabic , English , Korean
all the sentence vectors||to fix||sentence vector ( e.g. Korean )
",,,,,"neural attentionbased multiple context fixing attachment||is||series of modules
",,,
model,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .","Fixing the vectors
is done
by
selectively moving the vectors
to a location
in the same vector space
that better separates
the class","is done||by||selectively moving the vectors
selectively moving the vectors||to a location||in the same vector space
in the same vector space||that better separates||the class
",,"Model||Fixing the vectors||is done
",,,,,,
model,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,"MCFA
computes
two sentence usability metrics
to control
noise
when
fixing vectors
self usability","MCFA||computes||two sentence usability metrics
two sentence usability metrics||to control||noise
noise||when||fixing vectors
","two sentence usability metrics||name||self usability
",,"Model||has||MCFA
",,,,,"two sentence usability metrics||name||relative usability
"
model,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",relative usability,,,,,,,,,
hyperparameters,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .","use
rectified linear units
three filters
with
different window sizes h = 3 , 4 , 5
with
100 feature maps each","three filters||with||different window sizes h = 3 , 4 , 5
different window sizes h = 3 , 4 , 5||with||100 feature maps each
",,"Hyperparameters||use||rectified linear units
Hyperparameters||use||three filters
",,,,,,
hyperparameters,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .","For
final sentence vector
concatenate
feature maps
to get
300 - dimension vector","final sentence vector||concatenate||feature maps
feature maps||to get||300 - dimension vector
",,"Hyperparameters||For||final sentence vector
",,,,,,
hyperparameters,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,,,,,,,,,,
hyperparameters,"During training , we use mini-batch size of 50 .","During
training
use
mini-batch size
of
50","training||use||mini-batch size
mini-batch size||of||50
",,"Hyperparameters||During||training
",,,,,,
hyperparameters,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,"Training
is done via
stochastic gradient descent
over
shuffled mini-batches
with
Adadelta update rule","Training||is done via||stochastic gradient descent
stochastic gradient descent||over||shuffled mini-batches
shuffled mini-batches||with||Adadelta update rule
",,,"Hyperparameters||has||Training
",,,,,
hyperparameters,We perform early stopping using a random 10 % of the training set as the development set .,"perform
early stopping
using
random 10 %
of
training set
as
development set","early stopping||using||random 10 %
random 10 %||of||training set
training set||as||development set
",,"Hyperparameters||perform||early stopping
",,,,,,
results,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,"show
CNN + MCFA
achieves
state of the art performance
on
three of the four data sets
performs competitively on
one data set","CNN + MCFA||performs competitively on||one data set
CNN + MCFA||achieves||state of the art performance
state of the art performance||on||three of the four data sets
",,"Results||show||CNN + MCFA
",,,,,,
results,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .","When
N = 1
MCFA
increases the performance of
normal CNN
from
85.0
to
87.6","MCFA||increases the performance of||normal CNN
normal CNN||from||85.0
normal CNN||to||87.6
","N = 1||has||MCFA
","Results||When||N = 1
",,,,,,
results,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .","N = 10
MCFA
additionally beats
state of the art
on
TREC data set","MCFA||additionally beats||state of the art
state of the art||on||TREC data set
","N = 10||has||MCFA
",,"Results||When||N = 10
",,,,,
results,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .","ensemble classifier
additionally outperforms
all competing models
on
MR data set","ensemble classifier||additionally outperforms||all competing models
all competing models||on||MR data set
",,,"Results||has||ensemble classifier
",,,,,
research-problem,Token - Level Ensemble Distillation for Grapheme - to - Phoneme Conversion,Grapheme - to - Phoneme Conversion,,,,,"Contribution||has research problem||Grapheme - to - Phoneme Conversion
",,,,
research-problem,Grapheme - to - phoneme ( G2P ) conversion is an important task in automatic speech recognition and text - to - speech systems .,Grapheme - to - phoneme ( G2P ) conversion,,,,,"Contribution||has research problem||Grapheme - to - phoneme ( G2P ) conversion
",,,,
approach,"Inspired by the knowledge distillation in computer vision and natural language processing , in this work , we propose the token - level ensemble distillation for G2P conversion , to address the practical problems mentioned above .","propose
token - level ensemble distillation
for
G2P conversion","token - level ensemble distillation||for||G2P conversion
",,"Approach||propose||token - level ensemble distillation
",,,,,,
approach,"First , we use knowledge distillation to leverage the large amount of unlabeled words .","use
knowledge distillation
to leverage
large amount of unlabeled words","knowledge distillation||to leverage||large amount of unlabeled words
",,"Approach||use||knowledge distillation
",,,,,,
approach,"Specifically , we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence , and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data , and add them into the original training data .","train
teacher model
to generate
phoneme sequence
as well as
probability distribution
given
unlabeled grapheme sequence","teacher model||to generate||phoneme sequence
phoneme sequence||as well as||probability distribution
probability distribution||given||unlabeled grapheme sequence
",,,,,"knowledge distillation||train||teacher model
",,,
approach,"Second , we train a variety of models ( CNN , RNN and Transformer ) for ensemble to get higher accuracy , and transfer the knowledge of the ensemble models to a light - weight model that is suitable for online deployment , again by knowledge distillation .","train
variety of models ( CNN , RNN and Transformer )
for
ensemble
to get
higher accuracy
transfer
knowledge of the ensemble models
to
light - weight model
suitable for
online deployment","knowledge of the ensemble models||to||light - weight model
light - weight model||suitable for||online deployment
variety of models ( CNN , RNN and Transformer )||for||ensemble
ensemble||to get||higher accuracy
",,"Approach||transfer||knowledge of the ensemble models
Approach||train||variety of models ( CNN , RNN and Transformer )
",,,,,,
approach,"Besides , we adopt Transformer instead of RNN or CNN as the basic encoder - decoder model structure , since it demonstrates advantages in a variety of sequence to sequence tasks , such as neural machine translation , text summarization , automatic speech recognition .","adopt
Transformer
instead of
RNN or CNN
as
basic encoder - decoder model structure","Transformer||as||basic encoder - decoder model structure
Transformer||instead of||RNN or CNN
",,"Approach||adopt||Transformer
",,,,,,
experiments,Ensemble Model,Ensemble Model,,,,,,,,"Model||has||Ensemble Model
",
experiments,"We use 4 Transformer models , 3 CNN models and 3 Bi - LSTM models with different hyperparameters for ensemble , which give the best performance on the validation set .","use
4 Transformer models
3 CNN models
3 Bi - LSTM models",,,,,,"Ensemble Model||use||4 Transformer models
Ensemble Model||use||3 CNN models
Ensemble Model||use||3 Bi - LSTM models
",,,
experiments,The 4 Transformer models share the same hidden size ( 256 ) but vary in the number of the encoder - decoder layers .,"4 Transformer models
share
same hidden size ( 256 )
vary in
number of the encoder - decoder layers","4 Transformer models||vary in||number of the encoder - decoder layers
4 Transformer models||share||same hidden size ( 256 )
",,,,,,,"Ensemble Model||has||4 Transformer models
",
experiments,"For the 3 CNN models , they share the same hidden size ( 256 ) but vary in the number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 ) respectively .","3 CNN models
share
same hidden size ( 256 )
vary in
number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 )","3 CNN models||vary in||number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 )
3 CNN models||share||same hidden size ( 256 )
",,,,,,,"Ensemble Model||has||3 CNN models
",
experiments,"For the 3 Bi - LSTM models , they share the same number of encoder - decoder layers ( 1 - 1 ) , but with different hidden sizes ( 256 , 384 and 512 ) .","3 Bi - LSTM models
share
same number of encoder - decoder layers ( 1 - 1 )
with different
hidden sizes ( 256 , 384 and 512 )","3 Bi - LSTM models||share||same number of encoder - decoder layers ( 1 - 1 )
3 Bi - LSTM models||with different||hidden sizes ( 256 , 384 and 512 )
",,,,,,,"Ensemble Model||has||3 Bi - LSTM models
",
experiments,Student Model,Student Model,,,,,,,,"Model||has||Student Model
",
experiments,We choose Transformer as the student model and use the default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ) unless otherwise stated .,"choose
Transformer
use
default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder )",,,,,,"Student Model||use||default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder )
Student Model||choose||Transformer
",,,
experiments,We implement experiments with the fairseq - py 4 library in Py-Torch .,"implement experiments with
fairseq - py 4 library in Py-Torch",,,,,,"Experimental setup||implement experiments with||fairseq - py 4 library in Py-Torch
",,,
experiments,We use Adam optimizer for all models and follow the learning rate schedule in .,"use
Adam optimizer
for
all models
follow
learning rate schedule","Adam optimizer||for||all models
Adam optimizer||follow||learning rate schedule
",,,,,"Experimental setup||use||Adam optimizer
",,,
experiments,"The dropout is 0.3 for Bi - LSTM and CNN models , while the residual dropout , attention dropout and ReLU dropout for Transformer models is 0.2 , 0.4 , 0.4 respectively .","dropout
is
0.3
for
Bi - LSTM and CNN models
residual dropout , attention dropout and ReLU dropout
for
Transformer models
is
0.2 , 0.4 , 0.4","residual dropout , attention dropout and ReLU dropout||for||Transformer models
Transformer models||is||0.2 , 0.4 , 0.4
dropout||is||0.3
0.3||for||Bi - LSTM and CNN models
",,,,,,,"Experimental setup||has||residual dropout , attention dropout and ReLU dropout
Experimental setup||has||dropout
",
experiments,We train each model on 8 NVIDIA M40 GPUs .,"train
each model
on
8 NVIDIA M40 GPUs","each model||on||8 NVIDIA M40 GPUs
",,,,,"Experimental setup||train||each model
",,,
experiments,Each GPU contains roughly 4000 tokens in one mini-batch .,"contains
roughly 4000 tokens
in
one mini-batch","roughly 4000 tokens||in||one mini-batch
",,,,,"8 NVIDIA M40 GPUs||contains||roughly 4000 tokens
",,,
experiments,We use beam search during inference and set beam size to 10 .,"beam search
during
inference
set
beam size to 10","beam search||set||beam size to 10
beam search||during||inference
",,,,,,,"Experimental setup||use||beam search
",
experiments,We use WER ( word error rate ) and PER ( phoneme error rate ) to measure the accuracy of G2P conversion .,"WER ( word error rate ) and PER ( phoneme error rate )
to measure
accuracy of G2P conversion","WER ( word error rate ) and PER ( phoneme error rate )||to measure||accuracy of G2P conversion
",,,,,,,"Experimental setup||use||WER ( word error rate ) and PER ( phoneme error rate )
",
results,"We first compare our method with previous works on CMUDict 0.7 b dataset , as shown in .","on
CMUDict",,,"Results||on||CMUDict
",,,,,,
results,"It can be seen that our method on 6 - layer encoder and 6 - layer decoder Transformer achieves the new state - of - the - art result of 19.88 % WER , outperforming NSGD by 4.22 % WER .","can be seen
our method on 6 - layer encoder and 6 - layer decoder Transformer
achieves
new state - of - the - art result
of
19.88 % WER
outperforming
NSGD
by
4.22 % WER","our method on 6 - layer encoder and 6 - layer decoder Transformer||achieves||new state - of - the - art result
new state - of - the - art result||of||19.88 % WER
new state - of - the - art result||outperforming||NSGD
NSGD||by||4.22 % WER
",,,,,"CMUDict||can be seen||our method on 6 - layer encoder and 6 - layer decoder Transformer
",,,
ablation-analysis,"We first study the effect of distilling from unlabeled source words , as shown in .","study
effect of distilling from unlabeled source words",,,"Ablation analysis||study||effect of distilling from unlabeled source words
",,,,,,
ablation-analysis,"It can be seen that unlabeled source words can boost the accuracy by nearly 1 % WER , demonstrating the effectiveness by introducing abundant unlabeled data into knowledge distillation .","boost
accuracy
by
nearly 1 % WER
demonstrating the effectiveness by
introducing abundant unlabeled data
into
knowledge distillation","accuracy||by||nearly 1 % WER
accuracy||demonstrating the effectiveness by||introducing abundant unlabeled data
introducing abundant unlabeled data||into||knowledge distillation
",,,,,"effect of distilling from unlabeled source words||boost||accuracy
",,,
ablation-analysis,"Furthermore , we study the effect of ensemble teacher model in knowledge distillation .",effect of ensemble teacher model in knowledge distillation,,,,"Ablation analysis||study||effect of ensemble teacher model in knowledge distillation
",,,,,
ablation-analysis,"As shown in , the ensemble teacher model can boost the accuracy by more than 1 % WER , compared with the single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder ) , which demonstrates the strong ensemble teacher model is essential to guarantee the performance of student model in knowledge distillation .","boost
accuracy
by
more than 1 % WER
compared with
single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder )","accuracy||by||more than 1 % WER
accuracy||compared with||single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder )
",,,,,"effect of ensemble teacher model in knowledge distillation||boost||accuracy
",,,
ablation-analysis,"At last , we compare Transformer with RNN and CNN based models , without using knowledge distillation and unlabeled data , as shown in .","compare
Transformer
with
RNN and CNN based models
without using
knowledge distillation and unlabeled data","Transformer||with||RNN and CNN based models
Transformer||without using||knowledge distillation and unlabeled data
","Transformer||outperforms||RNN and CNN based models
","Ablation analysis||compare||Transformer
",,,,,,
ablation-analysis,"We can see that Transformer model outperforms the RNN and CNN based models used in previous works , demonstrating the advantage of Transformer model .","outperforms
RNN and CNN based models",,,,,,,,,
results,"We compare our method with the previous state - of - the - art CNN with NSGD ( which is reproduced by ourself ) on our internal dataset , as shown in .","CNN with NSGD
by
internal dataset",,"internal dataset||outperforms||CNN with NSGD
",,"Results||on||internal dataset
",,,"CNN with NSGD||by||3.52 % WER
",,
results,"Our method outperforms CNN with NSGD by 3.52 % WER , which demonstrates the effectiveness of our method for G2P conversion .","outperforms
3.52 % WER
demonstrates
effectiveness of our method
for
G2P conversion","effectiveness of our method||for||G2P conversion
",,,,,"CNN with NSGD||demonstrates||effectiveness of our method
",,,
research-problem,"FastSpeech : Fast , Robust and Controllable Text to Speech",Text to Speech,,,,,"Contribution||has research problem||Text to Speech
",,,,
research-problem,Neural network based end - to - end text to speech ( TTS ) has significantly improved the quality of synthesized speech .,Neural network based end - to - end text to speech ( TTS ),,,,,"Contribution||has research problem||Neural network based end - to - end text to speech ( TTS )
",,,,
research-problem,Text to speech ( TTS ) has attracted a lot of attention in recent years due to the advance in deep learning .,Text to speech ( TTS ),,,,,"Contribution||has research problem||Text to speech ( TTS )
",,,,
research-problem,Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .,Neural network based TTS,,,,,"Contribution||has research problem||Neural network based TTS
",,,,
model,"Considering the monotonous alignment between text and speech , to speedup mel- spectrogram generation , in this work , we propose a novel model , FastSpeech , which takes a text ( phoneme ) sequence as input and generates mel-spectrograms non-autoregressively .","propose
FastSpeech
takes
text ( phoneme ) sequence as input
generates
mel-spectrograms non-autoregressively","FastSpeech||generates||mel-spectrograms non-autoregressively
FastSpeech||takes||text ( phoneme ) sequence as input
",,"Model||propose||FastSpeech
",,,,,,
model,It adopts a feed - forward network based on the self - attention in Transformer and 1D convolution .,"adopts
feed - forward network
based on
self - attention in Transformer and 1D convolution","feed - forward network||based on||self - attention in Transformer and 1D convolution
",,"Model||adopts||feed - forward network
",,,,,,
model,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .","length regulator
up - samples
phoneme sequence according to the phoneme duration
to match
length of the mel-spectrogram sequence","length regulator||up - samples||phoneme sequence according to the phoneme duration
phoneme sequence according to the phoneme duration||to match||length of the mel-spectrogram sequence
",,,"Model||adopts||length regulator
",,,,,
model,"The regulator is built on a phoneme duration predictor , which predicts the duration of each phoneme .","built on
phoneme duration predictor
predicts
duration of each phoneme","phoneme duration predictor||predicts||duration of each phoneme
",,,,,"length regulator||built on||phoneme duration predictor
",,,
experimental-setup,"We first train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs , with batchsize of 16 sentences on each GPU .","train
autoregressive Transformer TTS model
on
4 NVIDIA V100 GPUs
with
batchsize of 16 sentences","autoregressive Transformer TTS model||on||4 NVIDIA V100 GPUs
4 NVIDIA V100 GPUs||with||batchsize of 16 sentences
",,"Experimental setup||train||autoregressive Transformer TTS model
",,,,,,
experimental-setup,"We use the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 and follow the same learning rate schedule in .","use
Adam optimizer
with
? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9","Adam optimizer||with||? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9
",,"Experimental setup||use||Adam optimizer
",,,,,,
experimental-setup,"In addition , we also leverage sequence - level knowledge distillation that has achieved good performance in non-autoregressive machine translation to transfer the knowledge from the teacher model to the student model .","leverage
sequence - level knowledge distillation",,,"Experimental setup||leverage||sequence - level knowledge distillation
",,,,,,
experimental-setup,"In the inference process , the output mel-spectrograms of our FastSpeech model are transformed into audio samples using the pretrained WaveGlow [ 20 ] 5 .","In the inference process
output mel-spectrograms
transformed into
audio samples using the pretrained WaveGlow","output mel-spectrograms||transformed into||audio samples using the pretrained WaveGlow
",,"Experimental setup||In the inference process||output mel-spectrograms
",,,,,,
results,Audio Quality,Audio Quality,,,,"Results||has||Audio Quality
",,,,,
results,We conduct the MOS ( mean opinion score ) evaluation on the test set to measure the audio quality .,"conduct
MOS ( mean opinion score ) evaluation
on
test set
to measure
audio quality","MOS ( mean opinion score ) evaluation||on||test set
test set||to measure||audio quality
",,,,,"Audio Quality||conduct||MOS ( mean opinion score ) evaluation
",,,
results,Robustness,Robustness,,,,"Results||has||Robustness
",,,,,
results,"It can be seen that Transformer TTS is not robust to these hard cases and gets 34 % error rate , while FastSpeech can effectively eliminate word repeating and skipping to improve intelligibility .","can be seen that
Transformer TTS
not robust to
hard cases
gets
34 % error rate
FastSpeech
effectively eliminate
word repeating and skipping
to improve
intelligibility","FastSpeech||effectively eliminate||word repeating and skipping
word repeating and skipping||to improve||intelligibility
Transformer TTS||not robust to||hard cases
hard cases||gets||34 % error rate
",,,,,"Robustness||can be seen that||FastSpeech
Robustness||can be seen that||Transformer TTS
",,,
results,Voice Speed,Voice Speed,,,,"Results||has||Voice Speed
",,,,,
results,"As demonstrated by the samples , FastSpeech can adjust the voice speed from 0.5x to 1.5 x smoothly , with stable and almost unchanged pitch .","demonstrated by
samples
FastSpeech
can adjust the voice speed
from 0.5x to 1.5 x smoothly
with
stable and almost unchanged pitch","FastSpeech||can adjust the voice speed||from 0.5x to 1.5 x smoothly
from 0.5x to 1.5 x smoothly||with||stable and almost unchanged pitch
","samples||has||FastSpeech
",,,,"Voice Speed||demonstrated by||samples
",,,
ablation-analysis,1D Convolution in FFT Block,1D Convolution in FFT Block,,,,"Ablation analysis||has||1D Convolution in FFT Block
",,,,,
ablation-analysis,"We propose to replace the original fully connected layer ( adopted in Transformer ) with 1D convolution in FFT block , as described in Section 3.1 .","replace
original fully connected layer",,,,,,"1D Convolution in FFT Block||replace||original fully connected layer
",,,
ablation-analysis,"As shown in , replacing 1D convolution with fully connected layer results in - 0.113 CMOS , which demonstrates the effectiveness of 1D convolution .","results in
- 0.113 CMOS",,,,,,"1D Convolution in FFT Block||results in||- 0.113 CMOS
",,,
ablation-analysis,Sequence - Level Knowledge Distillation,Sequence - Level Knowledge Distillation,,,,"Ablation analysis||has||Sequence - Level Knowledge Distillation
",,,,,
ablation-analysis,"We find that removing sequence - level knowledge distillation results in - 0.325 CMOS , which demonstrates the effectiveness of sequence - level knowledge distillation .","removing
sequence - level knowledge distillation
results in
- 0.325 CMOS","sequence - level knowledge distillation||results in||- 0.325 CMOS
",,,,,"Sequence - Level Knowledge Distillation||removing||sequence - level knowledge distillation
",,,
research-problem,Transfer Learning from Speaker Verification to Multispeaker Text - To - Speech Synthesis,Text - To - Speech Synthesis,,,,,"Contribution||has research problem||Text - To - Speech Synthesis
",,,,
research-problem,"We describe a neural network - based system for text - to - speech ( TTS ) synthesis that is able to generate speech audio in the voice of different speakers , including those unseen during training .",text - to - speech ( TTS ) synthesis,,,,,"Contribution||has research problem||text - to - speech ( TTS ) synthesis
",,,,
research-problem,The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner .,build a TTS system,,,,,"Contribution||has research problem||build a TTS system
",,,,
approach,Our approach is to decouple speaker modeling from speech synthesis by independently training a speaker - discriminative embedding network that captures the space of speaker characteristics and training a high quality TTS model on a smaller dataset conditioned on the representation learned by the first network .,"to decouple
speaker modeling
from
speech synthesis
by independently training
speaker - discriminative embedding network
captures
space of speaker characteristics
training
high quality TTS model
on
smaller dataset
conditioned on
representation learned by the first network","speaker modeling||from||speech synthesis
speaker modeling||by independently training||speaker - discriminative embedding network
speaker - discriminative embedding network||captures||space of speaker characteristics
speaker modeling||training||high quality TTS model
high quality TTS model||conditioned on||representation learned by the first network
high quality TTS model||on||smaller dataset
",,"Approach||to decouple||speaker modeling
",,,,,,
approach,We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker .,"train
speaker embedding network
on
speaker verification task
to determine
two different utterances were spoken by the same speaker","speaker embedding network||on||speaker verification task
speaker verification task||to determine||two different utterances were spoken by the same speaker
",,"Approach||train||speaker embedding network
",,,,,,
approach,"In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing reverberation and background noise from a large number of speakers .","trained on
untranscribed speech
containing
reverberation and background noise
from
large number of speakers","untranscribed speech||containing||reverberation and background noise
reverberation and background noise||from||large number of speakers
",,"Approach||trained on||untranscribed speech
",,,,,,
results,Speech naturalness,Speech naturalness,,,,"Results||has||Speech naturalness
",,,,,"Speech naturalness||has||proposed model
"
results,"The proposed model achieved about 4.0 MOS in all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .","proposed model
achieved
about 4.0 MOS
in
all datasets","proposed model||achieved||about 4.0 MOS
about 4.0 MOS||in||all datasets
",,,,,,,,
results,"Most importantly , the audio generated by our model for unseen speakers is deemed to be at least as natural as that generated for seen speakers .","audio generated
for
unseen speakers
is deemed to be at least as natural as
generated for seen speakers","audio generated||for||unseen speakers
unseen speakers||is deemed to be at least as natural as||generated for seen speakers
",,,,,,,"Speech naturalness||has||audio generated
",
results,"Surprisingly , the MOS on unseen speakers is higher than that of seen speakers , by as much as 0.2 points on LibriSpeech .","MOS on unseen speakers
is higher than
seen speakers
by as much as
0.2 points
on
LibriSpeech","MOS on unseen speakers||is higher than||seen speakers
seen speakers||by as much as||0.2 points
0.2 points||on||LibriSpeech
",,,,,,,"Speech naturalness||has||MOS on unseen speakers
",
results,Speaker similarity,Speaker similarity,,,,"Results||has||Speaker similarity
",,,,,
results,"The scores for the VCTK model tend to be higher than those for LibriSpeech , reflecting the cleaner nature of the dataset .","scores for
VCTK model
tend to be higher than those for
LibriSpeech
reflecting
cleaner nature of the dataset","VCTK model||tend to be higher than those for||LibriSpeech
LibriSpeech||reflecting||cleaner nature of the dataset
",,,,,"Speaker similarity||scores for||VCTK model
",,,
results,"For seen speakers on VCTK , the proposed model performs about as well as the baseline which uses an embedding lookup table for speaker conditioning .","For
seen speakers on VCTK
proposed model
performs about as well as
baseline
uses
embedding lookup table for speaker conditioning","proposed model||performs about as well as||baseline
baseline||uses||embedding lookup table for speaker conditioning
","seen speakers on VCTK||has||proposed model
",,,,"Speaker similarity||For||seen speakers on VCTK
",,,
results,Speaker verification,Speaker verification,,,,"Results||has||Speaker verification
",,,,,
results,"As shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on LibriSpeech , the synthesized speech is typically most similar to the ground truth voices .","trained on
LibriSpeech
synthesized speech
most similar to
ground truth voices","synthesized speech||most similar to||ground truth voices
","LibriSpeech||has||synthesized speech
",,,,"Speaker verification||trained on||LibriSpeech
",,,
results,"On this 20 voice discrimination task we obtain an EER of 2.86 % , demonstrating that , while the synthetic speech tends to be close to the target speaker ( cosine similarity > 0.6 , and as in ) , it is nearly always even closer to other synthetic utterances for the same speaker ( similarity > 0.7 ) .","On
20 voice discrimination task
obtain
EER of 2.86 %","20 voice discrimination task||obtain||EER of 2.86 %
",,,,,"Speaker verification||On||20 voice discrimination task
",,,
results,Speaker embedding space,Speaker embedding space,,,,"Results||has||Speaker embedding space
",,,,,"Speaker embedding space||has||PCA visualization
"
results,The PCA visualization ( left ) shows that synthesized utterances tend to lie very close to real speech from the same speaker in the embedding space .,"PCA visualization
shows
synthesized utterances
tend to lie very close to
real speech from the same speaker
in
embedding space","PCA visualization||shows||synthesized utterances
synthesized utterances||tend to lie very close to||real speech from the same speaker
real speech from the same speaker||in||embedding space
",,,,,,,,
results,"However , synthetic utterances are still easily distinguishable from the real human speech as demonstrated by the t - SNE visualization ( right ) where utterances from each synthetic speaker form a distinct cluster adjacent to a cluster of real utterances from the corresponding speaker .","synthetic utterances
easily distinguishable from
real human speech
demonstrated
t - SNE visualization
where
utterances
from
each synthetic speaker
distinct cluster
adjacent to
cluster of real utterances
from
corresponding speaker","t - SNE visualization||where||utterances
utterances||from||each synthetic speaker
each synthetic speaker||from||distinct cluster
distinct cluster||adjacent to||cluster of real utterances
cluster of real utterances||from||corresponding speaker
t - SNE visualization||demonstrated||synthetic utterances
synthetic utterances||easily distinguishable from||real human speech
",,,,,,,"Speaker embedding space||has||t - SNE visualization
",
results,Number of speaker encoder training speakers,Number of speaker encoder training speakers,,,,"Results||has||Number of speaker encoder training speakers
",,,,,
results,"As the number of training speakers increases , both naturalness and similarity improve significantly .","As
number of training speakers
increases
both naturalness and similarity
improve significantly","both naturalness and similarity||As||increases
","increases||has||number of training speakers
",,,,"Number of speaker encoder training speakers||improve significantly||both naturalness and similarity
",,,
results,Fictitious speakers,Fictitious speakers,,,,"Results||has||Fictitious speakers
",,,,,
results,Bypassing the speaker encoder network and conditioning the synthesizer on random points in the speaker embedding space results in speech from fictitious speakers which are not present in the train or test sets of either the synthesizer or the speaker encoder .,"Bypassing
speaker encoder network
conditioning
synthesizer
on
random points
in
speaker embedding space
from","synthesizer||on||random points
random points||in||speaker embedding space
",,,,,"Fictitious speakers||conditioning||synthesizer
Fictitious speakers||Bypassing||speaker encoder network
",,,
research-problem,"As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data .",generating real - valued data,,,,,"Contribution||has research problem||generating real - valued data
",,,,
research-problem,"However , it has limitations when the goal is for generating sequences of discrete tokens .",generating sequences of discrete tokens,,,,,"Contribution||has research problem||generating sequences of discrete tokens
",,,,
research-problem,Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning .,Generating sequential synthetic data,,,,,"Contribution||has research problem||Generating sequential synthetic data
",,,,
model,"In this paper , to address the above two issues , we follow ) and consider the sequence generation procedure as a sequential decision making process .","consider
sequence generation procedure
as
sequential decision making process","sequence generation procedure||as||sequential decision making process
",,"Model||consider||sequence generation procedure
",,,,,,
model,The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .,"generative model
is
treated as
agent of reinforcement learning ( RL )
state
is
generated tokens so far
action
next token to be generated","generative model||treated as||agent of reinforcement learning ( RL )
action||is||next token to be generated
state||is||generated tokens so far
","generative model||has||action
generative model||has||state
",,"Model||has||generative model
",,,,,
model,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .","sequence
employ
discriminator
to evaluate
feedback
evaluation
to guide
learning
of
generative model","discriminator||feedback||evaluation
evaluation||to guide||learning
learning||of||generative model
discriminator||to evaluate||sequence
",,"Model||employ||discriminator
",,,,,,
model,"To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy .","generative model
regard
as a
stochastic parametrized policy","generative model||as a||stochastic parametrized policy
",,"Model||regard||generative model
",,,,,,
model,"In our policy gradient , we employ Monte Carlo ( MC ) search to approximate the state - action value .","In
policy gradient
employ
Monte Carlo ( MC ) search
to approximate
state - action value","policy gradient||employ||Monte Carlo ( MC ) search
Monte Carlo ( MC ) search||to approximate||state - action value
",,"Model||In||policy gradient
",,,,,,
model,"We directly train the policy ( generative model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional GAN .","directly train
policy ( generative model )
via
policy gradient
naturally avoids
differentiation difficulty
for
discrete data
in a
conventional GAN","policy ( generative model )||via||policy gradient
policy gradient||naturally avoids||differentiation difficulty
differentiation difficulty||for||discrete data
discrete data||in a||conventional GAN
",,"Model||directly train||policy ( generative model )
",,,,,,
hyperparameters,"To setup the synthetic data experiments , we first initialize the parameters of an LSTM network following the normal distribution N ( 0 , 1 ) as the oracle describing the real data distribution G oracle ( x t |x 1 , . . . , x t?1 ) .","first initialize
parameters
of an
LSTM network
following
normal distribution N ( 0 , 1 )
as
oracle
describing
real data distribution G oracle ( x t |x 1 , . . . , x t?1 )","parameters||of an||LSTM network
LSTM network||following||normal distribution N ( 0 , 1 )
normal distribution N ( 0 , 1 )||as||oracle
oracle||describing||real data distribution G oracle ( x t |x 1 , . . . , x t?1 )
",,"Hyperparameters||first initialize||parameters
",,,,,,
hyperparameters,"In SeqGAN algorithm , the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from S with the label","In
SeqGAN algorithm
training set
for
discriminator
is comprised by
generated examples
with
label 0
instances
from
S
with
label","training set||for||discriminator
discriminator||is comprised by||generated examples
generated examples||with||label 0
generated examples||with||instances
instances||from||S
S||with||label
","SeqGAN algorithm||has||training set
","Hyperparameters||In||SeqGAN algorithm
",,,,,,
,"For different tasks , one should design specific structure for the convolutional layer and in our synthetic data experiments , the kernel size is from 1 to T and the number of each kernel size is between 100 to 200 3 . ","in
our synthetic data experiments
kernel size
from
1 to T
number
of
each kernel size
between
100 to 200",,,,,,,,,
hyperparameters,Dropout ) and L2 regularization are used to avoid over-fitting .,"Dropout ) and L2 regularization
to avoid
over-fitting","Dropout ) and L2 regularization||to avoid||over-fitting
",,,"Hyperparameters||has||Dropout ) and L2 regularization
",,,,,
baselines,The first model is a random token generation .,random token generation,,,,"Baselines||has||random token generation
",,,,,
baselines,The second one is the MLE trained LSTM G ? .,MLE trained LSTM G,,,,"Baselines||has||MLE trained LSTM G
",,,,,
baselines,The third one is scheduled sampling .,scheduled sampling,,,,"Baselines||has||scheduled sampling
",,,,,
baselines,The fourth one is the Policy Gradient with BLEU ( PG - BLEU ) .,Policy Gradient with BLEU ( PG - BLEU ),,,,"Baselines||has||Policy Gradient with BLEU ( PG - BLEU )
",,,,,
hyperparameters,"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .","scheduled sampling
training process
gradually changes
from
fully guided scheme feeding the true previous tokens
into
LSTM
towards
less guided scheme which mostly feeds the LSTM
with
generated tokens","scheduled sampling||training process||gradually changes
gradually changes||from||fully guided scheme feeding the true previous tokens
fully guided scheme feeding the true previous tokens||into||LSTM
LSTM||towards||less guided scheme which mostly feeds the LSTM
less guided scheme which mostly feeds the LSTM||with||generated tokens
",,,"Hyperparameters||In||scheduled sampling
",,,,,
hyperparameters,A curriculum rate ? is used to control the probability of replacing the true tokens with the generated ones .,"curriculum rate ?
to control
probability
of
replacing the true tokens
with
generated ones","curriculum rate ?||to control||probability
probability||of||replacing the true tokens
replacing the true tokens||with||generated ones
",,,"Hyperparameters||has||curriculum rate ?
",,,,,
results,"Since the evaluation metric is fundamentally instructive , we can see the impact of SeqGAN , which outperforms other baselines significantly .","see
impact of SeqGAN
outperforms
other baselines significantly","impact of SeqGAN||outperforms||other baselines significantly
",,"Results||see||impact of SeqGAN
",,,,,,
results,"A significance T - test on the NLL oracle score distribution of the generated sequences from the compared models is also performed , which demonstrates the significant improvement of SeqGAN over all compared models .","significance T - test
on
NLL oracle score distribution
of
generated sequences
from
compared models
demonstrates
significant improvement
of
SeqGAN
over all
compared models","significance T - test||on||NLL oracle score distribution
NLL oracle score distribution||of||generated sequences
generated sequences||from||compared models
compared models||demonstrates||significant improvement
significant improvement||of||SeqGAN
SeqGAN||over all||compared models
",,,"Results||has||significance T - test
",,,,,
results,"After about 150 training epochs , both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high NLL oracle score , whereas SeqGAN can improve the limit of the generator with the same structure as the baselines significantly .","After about
150 training epochs
both
maximum likelihood estimation and the schedule sampling methods
converge to
relatively high NLL oracle score
SeqGAN
improve the limit of
generator
with
same structure
as
baselines","150 training epochs||both||maximum likelihood estimation and the schedule sampling methods
maximum likelihood estimation and the schedule sampling methods||converge to||relatively high NLL oracle score
SeqGAN||improve the limit of||generator
generator||with||same structure
same structure||as||baselines
","relatively high NLL oracle score||has||SeqGAN
","Results||After about||150 training epochs
",,,,,,
results,This indicates the prospect of applying adversarial training strategies to discrete sequence generative models to breakthrough the limitations of MLE .,"indicates
prospect of applying adversarial training strategies
to
discrete sequence generative models
to breakthrough
limitations of MLE","prospect of applying adversarial training strategies||to||discrete sequence generative models
discrete sequence generative models||to breakthrough||limitations of MLE
",,"Results||indicates||prospect of applying adversarial training strategies
",,,,,,
results,"Additionally , SeqGAN outperforms PG - BLEU , which means the discriminative signal in GAN is more general and effective than a predefined score ( e.g. BLEU ) to guide the generative policy to capture the underlying distribution of the sequence data .","SeqGAN
outperforms
PG - BLEU","SeqGAN||outperforms||PG - BLEU
",,,"Results||has||SeqGAN
",,,,,
research-problem,Adversarial Ranking for Language Generation,Language Generation,,,,,"Contribution||has research problem||Language Generation
",,,,
model,"In this paper , we propose a novel adversarial learning framework , RankGAN , for generating highquality language descriptions .","propose
novel adversarial learning framework
RankGAN",,"novel adversarial learning framework||name||RankGAN
","Model||propose||novel adversarial learning framework
",,,,,,
model,RankGAN learns the model from the relative ranking information between the machine - written and the human - written sentences in an adversarial framework .,"learns
model
from
relative ranking information
between
the machine - written and the human - written sentences
in
adversarial framework","model||from||relative ranking information
relative ranking information||between||the machine - written and the human - written sentences
the machine - written and the human - written sentences||in||adversarial framework
",,"Model||learns||model
",,,,,,
model,"In the proposed RankGAN , we relax the training of the discriminator to a learning - to - rank optimization problem .","relax
training
of
discriminator
to
learning - to - rank optimization problem","training||of||discriminator
training||to||learning - to - rank optimization problem
",,"Model||relax||training
",,,,,,
model,"Specifically , the proposed new adversarial network consists of two neural network models , a generator and a ranker .","proposed
new adversarial network
consists of
two neural network models
a generator and a ranker","new adversarial network||consists of||two neural network models
","two neural network models||name||a generator and a ranker
","Model||proposed||new adversarial network
",,,,,,
model,"As opposed to performing a binary classification task , we propose to train the ranker to rank the machine - written sentences lower than human - written sentences with respect to a reference sentence which is human-written .","train
ranker
to rank
machine - written sentences
lower than
human - written sentences
with respect to
reference sentence
which is
human-written","ranker||to rank||machine - written sentences
machine - written sentences||with respect to||reference sentence
reference sentence||which is||human-written
machine - written sentences||lower than||human - written sentences
","train||has||ranker
",,"Model||propose||train
",,,,,
model,"Accordingly , we train the generator to synthesize sentences which confuse the ranker so that machine - written sentences are ranked higher than human - written sentences in regard to the reference .","train
generator
to synthesize
sentences
which confuse
ranker
so that
machine - written sentences
ranked higher than
human - written sentences","generator||so that||machine - written sentences
machine - written sentences||ranked higher than||human - written sentences
generator||to synthesize||sentences
sentences||which confuse||ranker
",,"Model||train||generator
",,,,,,
model,"During learning , we adopt the policy gradient technique to overcome the non-differentiable problem .","During
learning
adopt
policy gradient technique
to overcome
non-differentiable problem","learning||adopt||policy gradient technique
policy gradient technique||to overcome||non-differentiable problem
",,"Model||During||learning
",,,,,,
model,"Consequently , by viewing a set of data samples collectively and evaluating their quality through relative ranking , the discriminator is able to make better assessment of the quality of the samples , which in turn helps the generator to learn better .","viewing
set of data samples collectively
evaluating
quality
through
relative ranking
discriminator
able to make
better assessment
of
samples
helps
generator
to learn
better","set of data samples collectively||evaluating||quality
quality||through||relative ranking
quality||of||samples
discriminator||able to make||better assessment
better assessment||helps||generator
generator||to learn||better
","samples||has||discriminator
","Model||viewing||set of data samples collectively
",,,,,,
model,Our method is suitable for language learning in comparison to conventional GANs .,"suitable for
language learning
in comparison to
conventional GANs","language learning||in comparison to||conventional GANs
",,"Model||suitable for||language learning
",,,,,,
results,Simulation on synthetic data,"on
synthetic data",,,"Results||on||synthetic data
",,,,,,"synthetic data||has||RankGAN
"
results,It can be seen that the proposed RankGAN performs more favourably against the compared methods .,"RankGAN
performs
more favourably
against
compared methods","RankGAN||performs||more favourably
more favourably||against||compared methods
",,,,,,,,
results,"While MLE , PG - BLEU and SeqGAN tend to converge after 200 training epochs , the proposed RankGAN consistently improves the language generator and achieves relatively lower NLL score .","MLE , PG - BLEU and SeqGAN
tend to
converge
after
200 training epochs
proposed RankGAN
consistently improves
language generator
achieves
relatively lower
NLL score","MLE , PG - BLEU and SeqGAN||tend to||converge
converge||after||200 training epochs
proposed RankGAN||consistently improves||language generator
proposed RankGAN||achieves||relatively lower
","relatively lower||has||NLL score
",,,,,,"synthetic data||has||MLE , PG - BLEU and SeqGAN
synthetic data||has||proposed RankGAN
",
results,It is worth noting that the proposed RankGAN achieves better performance than that of PG - BLEU .,"worth noting
proposed RankGAN
achieves
better performance
than
PG - BLEU","proposed RankGAN||achieves||better performance
better performance||than||PG - BLEU
",,,,,"synthetic data||worth noting||proposed RankGAN
",,,
results,Results on Chinese poems composition,Chinese poems composition,,,,"Results||on||Chinese poems composition
",,,,,
results,"Following the evaluation protocol in , we compute the BLEU - 2 score and estimate the similarity between the human - written poem and the machine - created one .","estimate
similarity
between
human - written poem and the machine - created one","similarity||between||human - written poem and the machine - created one
",,,,,"Chinese poems composition||estimate||similarity
",,,
results,It can be seen that the proposed Rank GAN performs more favourably compared to the state - of - the - art methods in terms of BLEU - 2 score .,"seen that
proposed Rank GAN
performs
more favourably
compared to
state - of - the - art methods
in terms of
BLEU - 2 score","proposed Rank GAN||performs||more favourably
more favourably||compared to||state - of - the - art methods
state - of - the - art methods||in terms of||BLEU - 2 score
",,,,,"Chinese poems composition||seen that||proposed Rank GAN
",,,
results,RankGAN outperforms the compared method in terms of the human evaluation score .,"RankGAN
outperforms
compared method
in terms of
human evaluation score","RankGAN||outperforms||compared method
","human evaluation score||has||RankGAN
",,,,"Chinese poems composition||in terms of||human evaluation score
",,,
results,Results on COCO image captions,COCO image captions,,,,"Results||on||COCO image captions
",,,,,"COCO image captions||has||RankGAN
"
results,RankGAN achieves better performance than the other methods in terms of different BLEU scores .,"RankGAN
achieves
better performance
than
other methods
in terms of
different BLEU scores","RankGAN||achieves||better performance
better performance||than||other methods
other methods||in terms of||different BLEU scores
",,,,,,,,
results,"These examples show that our model is able to generate fluent , novel sentences that are not existing in the training set .","our model
able to generate
fluent , novel sentences
not existing in
training set","our model||able to generate||fluent , novel sentences
fluent , novel sentences||not existing in||training set
",,,,,,,"COCO image captions||has||our model
",
results,"As can be seen , the human - written sentences get the highest score comparing to the language models .","human - written sentences
get
highest score
comparing to
language models","human - written sentences||get||highest score
highest score||comparing to||language models
",,,,,,,"COCO image captions||has||human - written sentences
",
results,"Among the GANs approaches , RankGAN receives better score than SeqGAN , which is consistent to the finding in the Chinese poem composition .","Among
GANs approaches
RankGAN
receives
better score
than
SeqGAN","RankGAN||receives||better score
better score||than||SeqGAN
","GANs approaches||has||RankGAN
",,,,"human - written sentences||Among||GANs approaches
",,,
results,Results on Shakespeare 's plays,Shakespeare 's plays,,,,"Results||on||Shakespeare 's plays
",,,,,"Shakespeare 's plays||has||proposed method
"
results,"As can be seen , the proposed method achieves consistently higher BLEU score than the other methods in terms of the different n-grams criteria .","proposed method
achieves
consistently higher BLEU score
than
other methods
in terms of
different n-grams criteria","proposed method||achieves||consistently higher BLEU score
consistently higher BLEU score||than||other methods
other methods||in terms of||different n-grams criteria
",,,,,,,,
results,"The results indicate the proposed RankGAN is able to capture the transition pattern among the words , even if the training sentences are novel , delicate and complicated .","proposed RankGAN
is able to capture
transition pattern
among
words","proposed RankGAN||is able to capture||transition pattern
transition pattern||among||words
",,,,,,,"Shakespeare 's plays||has||proposed RankGAN
",
research-problem,Long Text Generation via Adversarial Training with Leaked Information,Text Generation,,,,,"Contribution||has research problem||Text Generation
",,,,
research-problem,"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .",Automatically generating coherent and semantically meaningful text,,,,,"Contribution||has research problem||Automatically generating coherent and semantically meaningful text
",,,,
approach,"In this paper , we propose a new algorithmic framework called Leak GAN to address both the non-informativeness and the sparsity issues .","called
Leak GAN
to address
both the non-informativeness and the sparsity issues","Leak GAN||to address||both the non-informativeness and the sparsity issues
",,"Approach||called||Leak GAN
",,,,,,
approach,LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning .,"LeakGAN
providing
richer information
from
discriminator
to
generator
by borrowing
recent advances
in
hierarchical reinforcement learning","LeakGAN||providing||richer information
richer information||from||discriminator
discriminator||to||generator
generator||by borrowing||recent advances
recent advances||in||hierarchical reinforcement learning
",,,"Approach||has||LeakGAN
",,,,,
approach,"As illustrated in , we specifically introduce a hierarchical generator G , which consists of a high - level MANAGER module and a low - level WORKER module .","introduce
hierarchical generator G
consists of
high - level MANAGER module
low - level WORKER module","hierarchical generator G||consists of||high - level MANAGER module
hierarchical generator G||consists of||low - level WORKER module
",,"Approach||introduce||hierarchical generator G
",,,,,,
approach,The MANAGER is along shortterm memory network ( LSTM ) and serves as a mediator .,"MANAGER
along
shortterm memory network ( LSTM )
serves as
mediator","MANAGER||along||shortterm memory network ( LSTM )
MANAGER||serves as||mediator
",,,"Approach||has||MANAGER
",,,,,
approach,"In each step , it receives generator D 's high - level feature representation , e.g. , the feature map of the CNN , and uses it to form the guiding goal for the WORKER module in that timestep .","receives
generator D 's high - level feature representation
form
guiding goal
for
WORKER module","generator D 's high - level feature representation||form||guiding goal
guiding goal||for||WORKER module
",,,,,"MANAGER||receives||generator D 's high - level feature representation
",,,
approach,"Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .","given
goal embedding
produced by
MAN - AGER
WORKER
first encodes
current generated words
with
another LSTM
then combines
output
of
the LSTM and the goal embedding
to take
final action
at
current state","goal embedding||produced by||MAN - AGER
WORKER||then combines||output
output||of||the LSTM and the goal embedding
output||to take||final action
final action||at||current state
WORKER||first encodes||current generated words
current generated words||with||another LSTM
","MAN - AGER||has||WORKER
","Approach||given||goal embedding
",,,,,,
hyperparameters,GAN Setting .,GAN Setting,,,,"Hyperparameters||has||GAN Setting
",,,,,
hyperparameters,"For the discriminator , we choose the CNN architecture as the feature extractor and the binary classifier .","choose
CNN architecture
as
feature extractor and the binary classifier","CNN architecture||as||feature extractor and the binary classifier
",,,,,"GAN Setting||choose||CNN architecture
",,,
hyperparameters,"For the synthetic data experiment , the CNN kernel size ranges from 1 to T .","For
synthetic data experiment
CNN kernel size
ranges
from 1 to T","CNN kernel size||ranges||from 1 to T
","synthetic data experiment||has||CNN kernel size
",,,,"GAN Setting||For||synthetic data experiment
",,,
hyperparameters,The number of each kernel is between 100 and 200 .,"number of
each kernel
between
100 and 200","each kernel||between||100 and 200
",,,,,"GAN Setting||number of||each kernel
",,,
hyperparameters,"In this case , the feature of text is a 1,720 dimensional vector .","feature of text
is
1,720 dimensional vector","feature of text||is||1,720 dimensional vector
",,,,,,,"GAN Setting||has||feature of text
",
hyperparameters,Dropout with the keep rate 0.75 and L2 regularization are performed to avoid overfitting .,"Dropout
with
keep rate 0.75
L2 regularization
performed to avoid
overfitting","Dropout||performed to avoid||overfitting
overfitting||with||keep rate 0.75
overfitting||with||L2 regularization
",,,,,,,"GAN Setting||has||Dropout
",
hyperparameters,"For the generator , we adopt LSTM as the architectures of MANAGER and WORKER to capture the sequence context information .","generator
adopt
LSTM
as
architectures of MANAGER and WORKER
to capture
sequence context information","generator||adopt||LSTM
LSTM||as||architectures of MANAGER and WORKER
architectures of MANAGER and WORKER||to capture||sequence context information
",,,,,,,"GAN Setting||For||generator
",
hyperparameters,The MANAGER produces the 16 - dimensional goal embedding feature vector wt using the feature map extracted by CNN .,"MANAGER
produces
16 - dimensional goal embedding feature vector
using
feature map
extracted by
CNN","MANAGER||produces||16 - dimensional goal embedding feature vector
16 - dimensional goal embedding feature vector||using||feature map
feature map||extracted by||CNN
",,,,,,,"GAN Setting||has||MANAGER
",
hyperparameters,The goal duration time c is a hyperparameter set as 4 after some preliminary experiments .,"goal duration time c
is a
hyperparameter
set as
4
after
some preliminary experiments","goal duration time c||is a||hyperparameter
hyperparameter||set as||4
4||after||some preliminary experiments
",,,,,,,"GAN Setting||has||goal duration time c
",
results,Synthetic Data Experiments,Synthetic Data Experiments,,,,"Results||has||Synthetic Data Experiments
",,,,,
results,"( i ) In the pre-training stage , LeakGAN has already shown observable performance superiority compared to other models , which indicates that the proposed hierarchical architecture itself brings improvement over the previous ones .","In
pre-training stage
LeakGAN
already shown
observable performance superiority
compared to
other models","LeakGAN||already shown||observable performance superiority
observable performance superiority||compared to||other models
","pre-training stage||has||LeakGAN
",,,,"Synthetic Data Experiments||In||pre-training stage
",,,
results,"( ii ) In the adversarial training stage , Leak GAN shows a better speed of convergence , and the local minimum it explores is significantly better than previous results .","adversarial training stage
Leak GAN
shows
better speed of convergence
local minimum
explores
is
significantly better
than
previous results","Leak GAN||shows||better speed of convergence
Leak GAN||explores||local minimum
local minimum||is||significantly better
significantly better||than||previous results
","adversarial training stage||has||Leak GAN
",,,,,,"Synthetic Data Experiments||In||adversarial training stage
",
results,Long Text Generation : EMNLP2017 WMT News,Long Text Generation : EMNLP2017 WMT News,,,,"Results||has||Long Text Generation : EMNLP2017 WMT News
",,,,,
results,"In all measured metrics , LeakGAN shows significant performance gain compared to baseline models .","In
all measured metrics
LeakGAN
shows
significant performance gain
compared to
baseline models","LeakGAN||shows||significant performance gain
significant performance gain||compared to||baseline models
","all measured metrics||has||LeakGAN
",,,,"Long Text Generation : EMNLP2017 WMT News||In||all measured metrics
",,,
results,Middle Text Generation : COCO Image Captions,Middle Text Generation : COCO Image Captions,,,,"Results||has||Middle Text Generation : COCO Image Captions
",,,,,
results,The results of the BLEU scores on the COCO dataset indicate that Leak GAN performs significantly better than baseline models in mid-length text generation task .,"of
BLEU scores
on
COCO dataset
indicate
Leak GAN
performs
significantly better
than
baseline models
in
mid-length text generation task","BLEU scores||on||COCO dataset
COCO dataset||indicate||Leak GAN
Leak GAN||performs||significantly better
significantly better||than||baseline models
baseline models||in||mid-length text generation task
",,,,,"Middle Text Generation : COCO Image Captions||of||BLEU scores
",,,
results,Short Text Generation : Chinese Poems,Short Text Generation : Chinese Poems,,,,"Results||has||Short Text Generation : Chinese Poems
",,,,,
results,The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks .,"indicate
LeakGAN
successfully handles
short text generation tasks","LeakGAN||successfully handles||short text generation tasks
",,,,,"Short Text Generation : Chinese Poems||indicate||LeakGAN
",,,
results,Turing Test and Generated Samples,Turing Test and Generated Samples,,,,"Results||has||Turing Test and Generated Samples
",,,,,"Turing Test and Generated Samples||has||performance
"
results,The performance on two datasets indicates that the generated sentences of Leak GAN are of higher global consistency and better readability than those of SeqGAN .,"performance
on
two datasets
indicates that
generated sentences
of
Leak GAN
of
higher global consistency
better readability
than those of
SeqGAN","performance||on||two datasets
two datasets||indicates that||generated sentences
generated sentences||of||Leak GAN
Leak GAN||than those of||SeqGAN
SeqGAN||of||higher global consistency
SeqGAN||of||better readability
",,,,,,,,
research-problem,An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue Generation,Dialogue Generation,,,,,"Contribution||has research problem||Dialogue Generation
",,,,
research-problem,"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .",Automatic dialogue generation,,,,,"Contribution||has research problem||Automatic dialogue generation
",,,,
research-problem,"However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses .",conversation generation,,,,,"Contribution||has research problem||conversation generation
",,,,
model,"To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .","propose
novel Auto - Encoder Matching model
to learn
utterance - level dependency","novel Auto - Encoder Matching model||to learn||utterance - level dependency
",,"Model||propose||novel Auto - Encoder Matching model
",,,,,,
model,"First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .","use
two auto- encoders
to learn
semantic representations
of
inputs and responses
in
unsupervised style","two auto- encoders||to learn||semantic representations
semantic representations||of||inputs and responses
inputs and responses||in||unsupervised style
",,"Model||use||two auto- encoders
",,,,,,
model,"Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .","given
utterance - level representations
mapping module
taught to learn
utterance - level dependency","mapping module||taught to learn||utterance - level dependency
","utterance - level representations||has||mapping module
","Model||given||utterance - level representations
",,,,,,
hyperparameters,"For dialogue generation , we set the maximum length to 15 words for each generated sentence .","set
maximum length
to
15 words
for
each generated sentence","maximum length||to||15 words
15 words||for||each generated sentence
",,"Hyperparameters||set||maximum length
",,,,,,
hyperparameters,"Based on the performance on the validation set , we set the hidden size to 512 , embedding size to 64 and vocabulary size to 40 K for baseline models and the proposed model .","hidden size
512
embedding size
64
vocabulary size
40 K",,"hidden size||has||512
vocabulary size||has||40 K
embedding size||has||64
",,"Hyperparameters||set||hidden size
Hyperparameters||set||vocabulary size
Hyperparameters||set||embedding size
",,,,,
hyperparameters,"The parameters are updated by the Adam algorithm ( Kingma and Ba , 2014 ) and initialized by sampling from the uniform distribution ( [? 0.1 , 0.1 ] ) .","parameters
updated by
Adam algorithm ( Kingma and Ba , 2014 )
initialized by
sampling
from
uniform distribution ( [? 0.1 , 0.1 ] )","parameters||initialized by||sampling
sampling||from||uniform distribution ( [? 0.1 , 0.1 ] )
parameters||updated by||Adam algorithm ( Kingma and Ba , 2014 )
",,,"Hyperparameters||has||parameters
",,,,,
hyperparameters,The initial learning rate is 0.002 and the model is trained in minibatches with a batch size of 256 . ? 1 and ?,"initial learning rate
0.002
model
trained in
minibatches
with
batch size
of
256","model||trained in||minibatches
minibatches||with||batch size
batch size||of||256
","initial learning rate||has||0.002
",,"Hyperparameters||has||initial learning rate
Hyperparameters||has||model
",,,,,
results,The proposed AEM model significantly outperforms the Seq2Seq model .,"proposed AEM model
significantly outperforms
Seq2Seq model","proposed AEM model||significantly outperforms||Seq2Seq model
",,,"Results||has||proposed AEM model
",,,,,
results,It demonstrates the effectiveness of utterance - level dependency on improving the quality of generated text .,"demonstrates
effectiveness of utterance - level dependency
on improving
quality of generated text","effectiveness of utterance - level dependency||on improving||quality of generated text
",,"Results||demonstrates||effectiveness of utterance - level dependency
",,,,,,
results,The improvement from the AEM model to the AEM + Attention model 2 is 0.68 BLEU - 4 point .,"improvement from
AEM model
to
AEM + Attention model
is
0.68 BLEU - 4 point","AEM model||to||AEM + Attention model
AEM + Attention model||is||0.68 BLEU - 4 point
",,"Results||improvement from||AEM model
",,,,,,
results,We find that the AEM model achieves significant improvement on the diversity of generated text .,"find
AEM model
achieves
significant improvement
on
diversity of generated text","AEM model||achieves||significant improvement
significant improvement||on||diversity of generated text
",,"Results||find||AEM model
",,,,,,
results,"Also , it should be noticed that the attention mechanism performs almost the same compared to the AEM model ( 31.2 K vs. 34.6 K in terms of Dist - 3 ) , which indicates that the utterance - level dependency and the word - level dependency are both indispensable for dialogue generation .","noticed that
attention mechanism
performs
almost the same
compared to
AEM model","attention mechanism||performs||almost the same
almost the same||compared to||AEM model
",,"Results||noticed that||attention mechanism
",,,,,,
results,"Therefore , by combining the two dependencies together , the AEM + Attention model achieves the best results .","combining
two dependencies
AEM + Attention model
achieves
best results","AEM + Attention model||achieves||best results
","two dependencies||has||AEM + Attention model
","Results||combining||two dependencies
",,,,,,
results,shows the results of human evaluation .,"of
human evaluation",,,"Results||of||human evaluation
",,,,,,"human evaluation||has||inter-annotator agreement
"
results,The inter-annotator agreement is satisfactory considering the difficulty of human evaluation .,"inter-annotator agreement
is
satisfactory","inter-annotator agreement||is||satisfactory
",,,,,,,,
results,"The Pearson 's correlation coefficient is 0.69 on coherence and 0.57 on fluency , with p < 0.0001 .","Pearson 's correlation coefficient
is
0.69
on
coherence
0.57
on
fluency","Pearson 's correlation coefficient||is||0.57
0.57||on||fluency
Pearson 's correlation coefficient||is||0.69
0.69||on||coherence
",,,,,,,"human evaluation||has||Pearson 's correlation coefficient
",
results,"First , it is clear that the AEM model outperforms the Seq2Seq model with a large margin , which proves the effectiveness of the AEM model on generating high quality responses .","clear that
AEM model
outperforms
Seq2Seq model
with
large margin","AEM model||outperforms||Seq2Seq model
Seq2Seq model||with||large margin
",,,,,"human evaluation||clear that||AEM model
",,,
results,"Second , it is interesting to note that with the attention mechanism , the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model .","interesting to note that with
attention mechanism
coherence
decreased slightly in
Seq2Seq model
increased significantly in
AEM model","coherence||decreased slightly in||Seq2Seq model
Seq2Seq model||increased significantly in||AEM model
","attention mechanism||has||coherence
",,,,"human evaluation||interesting to note that with||attention mechanism
",,,
results,"Therefore , it is expected that the AEM + Attention model achieves the best G-score .","expected that
AEM + Attention model
achieves
best G-score","AEM + Attention model||achieves||best G-score
",,,,,"human evaluation||expected that||AEM + Attention model
",,,
research-problem,Generating Text through Adversarial Training using Skip - Thought Vectors,Generating Text,,,,,"Contribution||has research problem||Generating Text
",,,,
research-problem,Attempts have been made for utilizing GANs with word embeddings for text generation .,text generation,,,,,"Contribution||has research problem||text generation
",,,,
research-problem,Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .,natural language text generation,,,,,"Contribution||has research problem||natural language text generation
",,,,
code,This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .,https://github.com/enigmaeth/skip-thought-gan,,,,,"Contribution||Code||https://github.com/enigmaeth/skip-thought-gan
",,,,
hyperparameters,The Skip - Thought encoder for the model encodes sentences with length less than 30 words using 2400 GRU units with word vector dimensionality of 620 to produce 4800 - dimensional combineskip vectors . .,"Skip - Thought encoder
encodes
sentences
with
length
less than
30 words
using
2400 GRU units
with
word vector
dimensionality of
620
to produce
4800 - dimensional combineskip vectors","Skip - Thought encoder||encodes||sentences
sentences||with||length
length||less than||30 words
sentences||using||2400 GRU units
2400 GRU units||with||word vector
word vector||dimensionality of||620
620||to produce||4800 - dimensional combineskip vectors
",,,"Hyperparameters||has||Skip - Thought encoder
",,,,,
hyperparameters,"The combine - skip vectors , with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model , are used as they have been found to be the best performing in the experiments","combine - skip vectors
with
first 2400 dimensions
being
uni-skip model
last 2400 bi-skip model
found to be
best performing in the experiments","combine - skip vectors||with||last 2400 bi-skip model
last 2400 bi-skip model||found to be||best performing in the experiments
combine - skip vectors||with||first 2400 dimensions
first 2400 dimensions||being||uni-skip model
",,,"Hyperparameters||has||combine - skip vectors
",,,,,
research-problem,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,Text Modeling,,,,,"Contribution||has research problem||Text Modeling
",,,,
research-problem,"Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .",generative text modeling,,,,,"Contribution||has research problem||generative text modeling
",,,,
model,"We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .","propose
dilated CNN
as
decoder
in
VAE","dilated CNN||as||decoder
decoder||in||VAE
",,"Model||propose||dilated CNN
",,,,,,
model,"In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .","exploit
dilated CNN
for
flexibility
in varying the amount of
conditioning context","dilated CNN||for||flexibility
flexibility||in varying the amount of||conditioning context
",,"Model||exploit||dilated CNN
",,,,,,
hyperparameters,We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders .,"use
LSTM
as
encoder
for
VAE
explore
LSTMs and CNNs
as
decoders","LSTMs and CNNs||as||decoders
LSTM||as||encoder
encoder||for||VAE
",,"Hyperparameters||explore||LSTMs and CNNs
Hyperparameters||use||LSTM
",,,,,,
hyperparameters,"For CNNs , we explore several different configurations .","For
CNNs",,,"Hyperparameters||For||CNNs
",,,,,,
hyperparameters,"We set the convolution filter size to be 3 and gradually increase the depth and dilation from [ 1 , 2 , 4 ] , ] to .","set
convolution filter size
to be
3","convolution filter size||to be||3
",,,,,"CNNs||set||convolution filter size
",,,
hyperparameters,We use Gumbel - softmax to sample y from q ( y|x ) .,"Gumbel - softmax
to sample
y from q ( y|x )","Gumbel - softmax||to sample||y from q ( y|x )
",,,"Hyperparameters||use||Gumbel - softmax
",,,,,
hyperparameters,We use a vocabulary size of 20 k for both data sets and set the word embedding dimension to be 512 .,"vocabulary size
of
20 k
for
both data sets
set
word embedding dimension
to be
512","word embedding dimension||to be||512
vocabulary size||of||20 k
20 k||for||both data sets
",,"Hyperparameters||set||word embedding dimension
","Hyperparameters||use||vocabulary size
",,,,,
hyperparameters,"The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally , as shown in Section 2.3 .","number of channels
for
convolutions
in
CNN decoders
is
512 internally
1024 externally","number of channels||for||convolutions
convolutions||is||512 internally
convolutions||is||1024 externally
","CNN decoders||has||number of channels
","Hyperparameters||in||CNN decoders
",,,,,,
hyperparameters,"We use Adam to optimize all models and the learning rate is selected from [ 2e - 3 , 1 e - 3 , 7.5 e - 4 ] and ?","Adam
to optimize
all models
learning rate
selected from
[ 2e - 3 , 1 e - 3 , 7.5 e - 4 ]","Adam||to optimize||all models
learning rate||selected from||[ 2e - 3 , 1 e - 3 , 7.5 e - 4 ]
",,,"Hyperparameters||use||Adam
Hyperparameters||use||learning rate
",,,,,
hyperparameters,"Empirically , we find learning rate 1e - 3 and ?1 = 0.5 to perform the best .","find
learning rate
1e - 3 and ?1 = 0.5
to perform
best","1e - 3 and ?1 = 0.5||to perform||best
","learning rate||has||1e - 3 and ?1 = 0.5
","Hyperparameters||find||learning rate
",,,,,,
hyperparameters,"We select dropout ratio of LSTMs ( both encoder and decoder ) from [ 0.3 , 0.5 ] .","select
dropout ratio
of
LSTMs ( both encoder and decoder )
from
0.3 , 0.5","dropout ratio||of||LSTMs ( both encoder and decoder )
LSTMs ( both encoder and decoder )||from||0.3 , 0.5
",,"Hyperparameters||select||dropout ratio
",,,,,,
hyperparameters,"Following , we also use drop word for the LSTM decoder , the drop word ratio is selected from [ 0 , 0.3 , 0.5 , 0.7 ] .","drop word
for
LSTM decoder
drop word ratio
selected from
0 , 0.3 , 0.5 , 0.7","drop word||for||LSTM decoder
drop word ratio||selected from||0 , 0.3 , 0.5 , 0.7
",,,"Hyperparameters||use||drop word
Hyperparameters||use||drop word ratio
",,,,,
hyperparameters,"For the CNN decoder , we use a dropout ratio of 0.1 at each layer .","CNN decoder
use
dropout ratio
of
0.1
at
each layer","CNN decoder||use||dropout ratio
dropout ratio||of||0.1
0.1||at||each layer
",,,"Hyperparameters||For||CNN decoder
",,,,,
hyperparameters,We use batch size of 32 and all model are trained for 40 epochs .,"batch size
of
32","batch size||of||32
",,,"Hyperparameters||use||batch size
",,,,,
hyperparameters,"Following , we use KL cost annealing strategy .",KL cost annealing strategy,,,,"Hyperparameters||use||KL cost annealing strategy
",,,,,
hyperparameters,We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .,"set
initial weight
of
KL cost term
to be
0.01
increase
linearly
until
given iteration T","initial weight||of||KL cost term
KL cost term||increase||linearly
linearly||until||given iteration T
KL cost term||to be||0.01
",,,,,"KL cost annealing strategy||set||initial weight
",,,
results,The results for language modeling are shown in .,"for
language modeling",,,"Results||for||language modeling
",,,,,,
results,"For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .","For
SCNN , MCNN and LCNN
VAE results
improve over
LM results
from
345.3
to
337.8
338.3
to
336.2
335.4
to
333.9","VAE results||improve over||LM results
LM results||from||338.3
338.3||to||336.2
LM results||from||335.4
335.4||to||333.9
LM results||from||345.3
345.3||to||337.8
","SCNN , MCNN and LCNN||has||VAE results
",,,,"language modeling||For||SCNN , MCNN and LCNN
",,,
results,"When LCNN is used as the decoder , we obtain an optimal trade off between using contextual information and latent representation .","When
LCNN
used as
decoder
obtain
optimal trade off
between using
contextual information and latent representation","LCNN||used as||decoder
decoder||obtain||optimal trade off
optimal trade off||between using||contextual information and latent representation
",,,,,"language modeling||When||LCNN
",,,
results,"LCNN - VAE achieves a NLL of 333.9 , which improves over LSTM - LM with NLL of 334.9 .","LCNN - VAE
achieves
NLL
of
333.9
improves over
LSTM - LM
with
NLL
of
334.9","LCNN - VAE||achieves||NLL
NLL||of||333.9
LCNN - VAE||improves over||LSTM - LM
LSTM - LM||with||NLL
NLL||of||334.9
",,,,,,,"language modeling||has||LCNN - VAE
",
ablation-analysis,We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .,"see
SCNN - VAE - Semi
best classification accuracy
of
65.5","best classification accuracy||of||65.5
","SCNN - VAE - Semi||has||best classification accuracy
","Ablation analysis||see||SCNN - VAE - Semi
",,,,,,
ablation-analysis,"On the other hand , LCNN - VAE - Semi has the best NLL result .","LCNN - VAE - Semi
best NLL result",,"LCNN - VAE - Semi||has||best NLL result
",,"Ablation analysis||has||LCNN - VAE - Semi
",,,,,
research-problem,Abstractive Text Summarization by Incorporating Reader Comments,Abstractive Text Summarization,,,,,"Contribution||has research problem||Abstractive Text Summarization
",,,,
research-problem,"In neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect .",neural abstractive summarization,,,,,"Contribution||has research problem||neural abstractive summarization
",,,,
research-problem,"To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect .",reader - aware abstractive summary generation,,,,,"Contribution||has research problem||reader - aware abstractive summary generation
",,,,
research-problem,"Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :",abstractive summarization,,,,,"Contribution||has research problem||abstractive summarization
",,,,
model,"In this paper , we propose a summarization framework named reader - aware summary generator ( RASG ) that incorporates reader comments to improve the summarization performance .","propose
summarization framework
named
reader - aware summary generator ( RASG )
incorporates
reader comments
to improve
summarization performance","summarization framework||incorporates||reader comments
reader comments||to improve||summarization performance
summarization framework||named||reader - aware summary generator ( RASG )
",,"Model||propose||summarization framework
",,,,,,
model,"Specifically , a seq2seq architecture with attention mechanism is employed as the basic summary generator .","seq2seq architecture with attention mechanism
employed
as
basic summary generator","seq2seq architecture with attention mechanism||as||basic summary generator
",,"Model||employed||seq2seq architecture with attention mechanism
",,,,,,
model,"We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as reader attention representing the "" reader focused aspect "" .","calculate alignment between
reader comments words and document words
regarded as
reader attention
representing
reader focused aspect","reader comments words and document words||regarded as||reader attention
reader attention||representing||reader focused aspect
",,"Model||calculate alignment between||reader comments words and document words
",,,,,,
model,"Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , "" decoder focused aspect "" .","treat
decoder attention weights
as
focused aspect of the generated summary
a.k.a.
decoder focused aspect","decoder attention weights||as||focused aspect of the generated summary
focused aspect of the generated summary||a.k.a.||decoder focused aspect
",,"Model||treat||decoder attention weights
",,,,,,
model,"After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .","After
each decoding step
supervisor
designed
to measure
distance
between
reader focused aspect and the decoder focused aspect","each decoding step||designed||supervisor
supervisor||to measure||distance
distance||between||reader focused aspect and the decoder focused aspect
",,"Model||After||each decoding step
",,,,,,
model,The training of our framework RASG is conducted in an adversarial way .,"training of
framework RASG
conducted in
adversarial way","framework RASG||conducted in||adversarial way
",,"Model||training of||framework RASG
",,,,,,
baselines,( 1 ) S2S : Sequence - to - sequence framework has been proposed for language generation task .,"S2S
Sequence - to - sequence framework",,"S2S||name||Sequence - to - sequence framework
",,"Baselines||has||S2S
",,,,,
baselines,"( 2 ) S2SR : We simply add the reader attention on attention distribution ? t , in each decoding step .","S2SR
add
reader attention on attention distribution ? t
in
decoding step","S2SR||add||reader attention on attention distribution ? t
reader attention on attention distribution ? t||in||decoding step
",,,"Baselines||has||S2SR
",,,,,
baselines,"( 3 ) CGU : propose to use the convolutional gated unit to refine the source representation , which achieves the state - of - the - art performance on social media text summarization dataset .","CGU
propose to use
convolutional gated unit","CGU||propose to use||convolutional gated unit
",,,"Baselines||has||CGU
",,,,,
baselines,"( 4 ) LEAD1 : LEAD1 is a commonly used baseline , which selects the first sentence of document as the summary .","LEAD1
selects
first sentence of document as the summary","LEAD1||selects||first sentence of document as the summary
",,,"Baselines||has||LEAD1
",,,,,
baselines,"( 5 ) TextRank : propose to build a graph , then add each sentence as a vertex and use link to represent semantic similarity .","TextRank
propose to build
graph
add
each sentence
as
vertex
use
link
to represent
semantic similarity","TextRank||add||each sentence
each sentence||as||vertex
TextRank||use||link
link||to represent||semantic similarity
TextRank||propose to build||graph
",,,"Baselines||has||TextRank
",,,,,
experimental-setup,We implement our experiments in TensorFlow ) on an NVIDIA P40 GPU .,"implement
our experiments
in
TensorFlow
on
NVIDIA P40 GPU","our experiments||in||TensorFlow
TensorFlow||on||NVIDIA P40 GPU
",,"Experimental setup||implement||our experiments
",,,,,,
experimental-setup,The word embedding dimension is set to 256 and the number of hidden units is 512 .,"word embedding dimension
set to
256
number of hidden units
512","word embedding dimension||set to||256
","number of hidden units||has||512
",,"Experimental setup||has||word embedding dimension
Experimental setup||has||number of hidden units
",,,,,
experimental-setup,We use Adagrad optimizer as our optimizing algorithm .,"use
Adagrad optimizer
as
optimizing algorithm","Adagrad optimizer||as||optimizing algorithm
",,"Experimental setup||use||Adagrad optimizer
",,,,,,
experimental-setup,We employ beam search with beam size 5 to generate more fluency summary sentence .,"employ
beam search
with
beam size 5
to generate
more fluency summary sentence","beam search||with||beam size 5
beam size 5||to generate||more fluency summary sentence
",,"Experimental setup||employ||beam search
",,,,,,
results,"We see that RASG achieves a 11.0 % , 9.1 % and 6.6 % increment over the state - of - the - art method CGU in terms of ROUGE - 1 , ROUGE - 2 and ROUGE - L respectively .","see that
RASG
achieves
11.0 % , 9.1 % and 6.6 % increment
over
state - of - the - art method CGU
in terms of
ROUGE - 1 , ROUGE - 2 and ROUGE - L","RASG||achieves||11.0 % , 9.1 % and 6.6 % increment
11.0 % , 9.1 % and 6.6 % increment||over||state - of - the - art method CGU
state - of - the - art method CGU||in terms of||ROUGE - 1 , ROUGE - 2 and ROUGE - L
",,"Results||see that||RASG
",,,,,,
results,It is worth noticing that the baseline model S2SR achieves better performance than S2S which demonstrates the effectiveness of incorporating reader focused aspect in summary generation .,"worth noticing
baseline model S2SR
achieves better performance than
S2S","baseline model S2SR||achieves better performance than||S2S
",,"Results||worth noticing||baseline model S2SR
",,,,,,
ablation-analysis,The discriminator provides the scalar training signal L g c for generator training and the feature vector F ( m t ) for goal tracker .,"discriminator
provides
scalar training signal L g c
for
generator training
feature vector F ( m t )
for
goal tracker","discriminator||provides||scalar training signal L g c
scalar training signal L g c||for||generator training
discriminator||provides||feature vector F ( m t )
feature vector F ( m t )||for||goal tracker
",,,"Ablation analysis||has||discriminator
",,,,,
ablation-analysis,"Consequently , there is an increment of 17.51 % from RASG w / o GTD to RASG w / o GT in terms of ROUGE - L , which demonstrates the effectiveness of discriminator .","increment of
17.51 %
from
RASG w / o GTD to RASG w / o GT
in terms of
ROUGE - L","17.51 %||from||RASG w / o GTD to RASG w / o GT
RASG w / o GTD to RASG w / o GT||in terms of||ROUGE - L
",,,,,"discriminator||increment of||17.51 %
",,,
ablation-analysis,"As for the effectiveness of goal tracker , compared with RASG and RASG w / o GT , RASG w/ o GTD offers a decrease of 45. 23 % and 17.88 % in terms of ROUGE - 1 , respectively .","goal tracker
compared with
RASG and RASG w / o GT
RASG w/ o GTD
offers
decrease of
45. 23 % and 17.88 %
in terms of
ROUGE - 1","goal tracker||compared with||RASG and RASG w / o GT
RASG and RASG w / o GT||offers||RASG w/ o GTD
RASG w/ o GTD||decrease of||45. 23 % and 17.88 %
45. 23 % and 17.88 %||in terms of||ROUGE - 1
",,,"Ablation analysis||has||goal tracker
",,,,,
ablation-analysis,"Finally , RASG w/o DM offers a decrease of 10 . 22 % compared with RASG in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .","RASG w/o DM
offers a decrease of
10 . 22 %
compared with
RASG
in terms of
ROUGE - L","RASG w/o DM||offers a decrease of||10 . 22 %
10 . 22 %||compared with||RASG
RASG||in terms of||ROUGE - L
",,,"Ablation analysis||has||RASG w/o DM
",,,,,
research-problem,Mixture Content Selection for Diverse Sequence Generation,Diverse Sequence Generation,,,,,"Contribution||has research problem||Diverse Sequence Generation
",,,,
research-problem,Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one - to - many relationships between source and the target sequences .,Generating diverse sequences,,,,,"Contribution||has research problem||Generating diverse sequences
",,,,
research-problem,Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences .,Generating target sequences given a source sequence,,,,,"Contribution||has research problem||Generating target sequences given a source sequence
",,,,
research-problem,"Encoder - decoder models are widely used for sequence generation , most notably in machine translation where neural models are now often almost as good as human translators in some language pairs .",sequence generation,,,,,"Contribution||has research problem||sequence generation
",,,,
model,"In this paper , we present a method for diverse generation that separates diversification and generation stages .","separates
diversification and generation stages",,,"Model||separates||diversification and generation stages
",,,,,,
model,"The diversification stage leverages content selection to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .","diversification stage
leverages
content selection
map
source to multiple sequences","diversification stage||leverages||content selection
content selection||map||source to multiple sequences
",,,"Model||has||diversification stage
",,,,,
model,The generation stage uses a standard encoder - decoder model to generate a target sequence given each selected content from the source ( one - to - one mapping ) .,"generation stage
uses
standard encoder - decoder model
generate
target sequence given each selected content from the source","generation stage||uses||standard encoder - decoder model
standard encoder - decoder model||generate||target sequence given each selected content from the source
",,,"Model||has||generation stage
",,,,,
model,We present a generic module called SELECTOR that is specialized for diversification .,"present
generic module
called
SELECTOR
specialized for
diversification","generic module||called||SELECTOR
SELECTOR||specialized for||diversification
",,"Model||present||generic module
",,,,,,
model,This module can be used as a plug - and - play to an arbitrary encoder - decoder model for generation without architecture change .,"used as
plug - and - play
to
arbitrary encoder - decoder model
for
generation without architecture change","plug - and - play||to||arbitrary encoder - decoder model
arbitrary encoder - decoder model||for||generation without architecture change
",,,,,"generic module||used as||plug - and - play
",,,
baselines,Beam Search,Beam Search,,,,"Baselines||has||Beam Search
",,,,,
baselines,This baseline keeps K hypotheses with highest log-probability scores at each decoding step .,"keeps
K hypotheses
with
highest log-probability scores
at
each decoding step","K hypotheses||with||highest log-probability scores
highest log-probability scores||at||each decoding step
",,,,,"Beam Search||keeps||K hypotheses
",,,
baselines,Truncated Sampling,Truncated Sampling,,,,"Baselines||has||Truncated Sampling
",,,,,
baselines,This baseline randomly samples words from top - 10 candidates of the distribution at the decoding step .,"randomly samples
words
from
top - 10 candidates
of
distribution
at
decoding step","words||from||top - 10 candidates
top - 10 candidates||of||distribution
distribution||at||decoding step
",,,,,"Truncated Sampling||randomly samples||words
",,,
baselines,Mixture Decoder,Mixture Decoder,,,,"Baselines||has||Mixture Decoder
",,,,,
baselines,This baseline constructs a hard - MoE of K decoders with uniform mixing coefficient ( referred as hMup in ) and conducts parallel greedy decoding .,"constructs
hard - MoE of K decoders
with
uniform mixing coefficient
conducts
parallel greedy decoding","hard - MoE of K decoders||with||uniform mixing coefficient
",,,,,"Mixture Decoder||conducts||parallel greedy decoding
Mixture Decoder||constructs||hard - MoE of K decoders
",,,
baselines,Mixture Selector ( Ours ),Mixture Selector ( Ours ),,,,"Baselines||has||Mixture Selector ( Ours )
",,,,,
baselines,We construct a hard - MoE of K SELECTORs with uniform mixing coefficient that infers K different focus from source sequence .,"construct
hard - MoE of K SELECTORs
with
uniform mixing coefficient
infers
K different focus from source sequence","hard - MoE of K SELECTORs||with||uniform mixing coefficient
uniform mixing coefficient||infers||K different focus from source sequence
",,,,,"Mixture Selector ( Ours )||construct||hard - MoE of K SELECTORs
",,,
experimental-setup,"For all experiments , we tie the weights of the encoder embedding , the decoder embedding , and the decoder output layers .","tie
weights
of
encoder embedding
decoder embedding
decoder output layers","weights||of||encoder embedding
weights||of||decoder embedding
weights||of||decoder output layers
",,"Experimental setup||tie||weights
",,,,,,
experimental-setup,We train up to 20 epochs and select the checkpoint with the best oracle metric .,"train
up to 20 epochs
select
checkpoint
with
best oracle metric","up to 20 epochs||select||checkpoint
checkpoint||with||best oracle metric
",,"Experimental setup||train||up to 20 epochs
",,,,,,
,"We use Adam ( Kingma and Ba , 2015 ) optimizer with learning rate 0.001 and momentum parmeters ? 1 = 0.9 and ? 2 = 0.999 . ","use
Adam ( Kingma and Ba , 2015 ) optimizer
with
learning rate 0.001
momentum parmeters ? 1 = 0.9 and ? 2 = 0.999",,,,,,,,,
experimental-setup,Minibatch size is 64 and 32 for question generation and abstractive summarization .,"Minibatch size
is
64 and 32
for
question generation and abstractive summarization","Minibatch size||is||64 and 32
64 and 32||for||question generation and abstractive summarization
",,,"Experimental setup||has||Minibatch size
",,,,,
experimental-setup,"All models are implemented in PyTorch and trained on single Tesla P40 GPU , based on NAVER Smart Machine Learning ( NSML ) platform .","implemented in
PyTorch
trained on
single Tesla P40 GPU
based on
NAVER Smart Machine Learning ( NSML ) platform","single Tesla P40 GPU||based on||NAVER Smart Machine Learning ( NSML ) platform
",,"Experimental setup||implemented in||PyTorch
Experimental setup||trained on||single Tesla P40 GPU
",,,,,,
results,Diversity vs. Accuracy Trade - off compare our method with different diversitypromoting techniques in question generation and abstractive summarization .,"Diversity vs. Accuracy Trade - off
in",,,,"Results||has||Diversity vs. Accuracy Trade - off
",,,,,
results,The tables show that our mixture SELECTOR method outperforms all baselines in Top - 1 and oracle metrics and achieves the best trade - off between diversity and accuracy .,"show
mixture SELECTOR method
outperforms
all baselines
Top - 1 and oracle metrics
achieves
best trade - off
between
diversity and accuracy","mixture SELECTOR method||achieves||best trade - off
best trade - off||between||diversity and accuracy
mixture SELECTOR method||outperforms||all baselines
","all baselines||in||Top - 1 and oracle metrics
",,,,"Diversity vs. Accuracy Trade - off||show||mixture SELECTOR method
",,,
results,"Notably , our method scores state - of - the - art BLEU - 4 in question generation on SQuAD and ROUGE comparable to state - of - the - art methods in abstractive summarization in CNN - DM ( See also for state - of - the - art results in CNN - DM ) .","our method
scores
state - of - the - art BLEU
in
question generation
on
SQuAD and ROUGE
in
abstractive summarization in CNN - DM","our method||scores||state - of - the - art BLEU
state - of - the - art BLEU||in||question generation
question generation||on||SQuAD and ROUGE
SQuAD and ROUGE||in||abstractive summarization in CNN - DM
",,,,,,,"Diversity vs. Accuracy Trade - off||has||our method
",
results,Diversity vs. Number of Mixtures,Diversity vs. Number of Mixtures,,,,"Results||has||Diversity vs. Number of Mixtures
",,,,,
results,Here we compare the effect of number of mixtures in our SELECTOR and Mixture Decoder .,"compare
effect of number of mixtures
in
SELECTOR and Mixture Decoder","effect of number of mixtures||in||SELECTOR and Mixture Decoder
",,,,,"Diversity vs. Number of Mixtures||compare||effect of number of mixtures
",,,
results,show that pairwise similarity increases ( diversity ?) when the number of mixtures increases for Mixture Decoder .,"show
pairwise similarity increases ( diversity ?)
when
number of mixtures increases
for
Mixture Decoder","Mixture Decoder||show||pairwise similarity increases ( diversity ?)
pairwise similarity increases ( diversity ?)||when||number of mixtures increases
",,,,,"Diversity vs. Number of Mixtures||for||Mixture Decoder
",,,
research-problem,Soft Layer - Specific Multi - Task Summarization with Entailment and Question Generation,Multi - Task Summarization,,,,,"Contribution||has research problem||Multi - Task Summarization
",,,,
research-problem,"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document .",abstractive summarization,,,,,"Contribution||has research problem||abstractive summarization
",,,,
research-problem,"In this work , we improve abstractive text summarization via soft , high - level ( semantic ) layerspecific multi-task learning with two relevant auxiliary tasks .",abstractive text summarization,,,,,"Contribution||has research problem||abstractive text summarization
",,,,
model,"Further , we also present novel multi-task learning architectures based on multi-layered encoder and decoder models , where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks , while keeping the lower - level ( lexico- syntactic ) layers unshared .","present
novel multi-task learning architectures
based on
multi-layered encoder and decoder models
empirically show
substantially better
to share
higherlevel semantic layers
between
three aforementioned tasks
keeping
lower - level ( lexico- syntactic ) layers unshared","novel multi-task learning architectures||empirically show||substantially better
substantially better||to share||higherlevel semantic layers
higherlevel semantic layers||keeping||lower - level ( lexico- syntactic ) layers unshared
higherlevel semantic layers||between||three aforementioned tasks
novel multi-task learning architectures||based on||multi-layered encoder and decoder models
",,"Model||present||novel multi-task learning architectures
",,,,,,
results,Pointer + Coverage Baseline,Pointer + Coverage Baseline,,,,"Results||has||Pointer + Coverage Baseline
",,,,,
results,"4 On Gigaword dataset , our baseline model ( with pointer only , since coverage not needed for this single - sentence summarization task ) performs better than all previous works , as shown in .","On
Gigaword dataset
baseline model
performs better than
all previous works","baseline model||performs better than||all previous works
","Gigaword dataset||has||baseline model
",,,,"Pointer + Coverage Baseline||On||Gigaword dataset
",,,
results,Multi - Task with Entailment Generation,Multi - Task with Entailment Generation,,,,"Results||has||Multi - Task with Entailment Generation
",,,,,
results,"4 . shows that this multi-task setting is better than our strong baseline models and the improvements are statistically significant on all metrics 5 on both CNN / DailyMail ( p < 0.01 in ROUGE - 1 / ROUGE - L / METEOR and p < 0.05 in ROUGE - 2 ) and Gigaword ( p < 0.01 on all metrics ) datasets , showing that entailment generation task is inducing useful inference skills to the summarization task ( also see analysis examples in Sec. 7 ) .","shows
multi-task setting
better than
our strong baseline models","multi-task setting||better than||our strong baseline models
",,,,,"Multi - Task with Entailment Generation||shows||multi-task setting
",,,
results,"For multi-task learning with question generation , the improvements are statistically significant in ROUGE - 1 ( p < 0.01 ) , ROUGE - L ( p < 0.05 ) , and METEOR ( p < 0.01 ) for CNN / DailyMail and in all metrics ( p < 0.01 ) for Gigaword , compared to the respective baseline models .","For
multi-task learning with question generation
improvements
statistically significant
in
ROUGE - 1 ( p < 0.01 )
ROUGE - L ( p < 0.05 )
METEOR ( p < 0.01 )
for
CNN / DailyMail
in
all metrics ( p < 0.01 )
Gigaword","multi-task learning with question generation||improvements||statistically significant
statistically significant||for||CNN / DailyMail
CNN / DailyMail||in||ROUGE - 1 ( p < 0.01 )
CNN / DailyMail||in||ROUGE - L ( p < 0.05 )
CNN / DailyMail||in||METEOR ( p < 0.01 )
statistically significant||for||Gigaword
Gigaword||in||all metrics ( p < 0.01 )
",,"Results||For||multi-task learning with question generation
",,,,,,
ablation-analysis,Soft - sharing vs. Hard - sharing,Soft - sharing vs. Hard - sharing,,,,"Ablation analysis||has||Soft - sharing vs. Hard - sharing
",,,,,
ablation-analysis,"As described in Sec. 4.2 , we choose soft - sharing over hard - sharing because of the more expressive parameter sharing it provides to the model .","choose
soft - sharing
over
hard - sharing
because of
more expressive parameter sharing","soft - sharing||over||hard - sharing
soft - sharing||because of||more expressive parameter sharing
",,,,,"Soft - sharing vs. Hard - sharing||choose||soft - sharing
",,,
ablation-analysis,Empirical results in 8 prove that soft - sharing method is statistically significantly better than hard - sharing with p < 0.001 in all metrics .,"in
prove
soft - sharing method
is statistically significantly better than
hard - sharing
with
p < 0.001
all metrics","soft - sharing method||is statistically significantly better than||hard - sharing
hard - sharing||with||p < 0.001
p < 0.001||in||all metrics
",,,,,"Soft - sharing vs. Hard - sharing||prove||soft - sharing method
",,,
ablation-analysis,Quantitative Improvements in Entailment,Quantitative Improvements in Entailment,,,,"Ablation analysis||has||Quantitative Improvements in Entailment
",,,,,
ablation-analysis,We found that our 2 - way MTL model with entailment generation reduces this extraneous count by 17.2 % w.r.t. the baseline .,"found
our 2 - way MTL model
with
entailment generation
reduces
extraneous count
by
17.2 % w.r.t. the baseline","our 2 - way MTL model||with||entailment generation
entailment generation||reduces||extraneous count
extraneous count||by||17.2 % w.r.t. the baseline
",,,,,"Quantitative Improvements in Entailment||found||our 2 - way MTL model
",,,
ablation-analysis,Quantitative Improvements in Saliency Detection,Quantitative Improvements in Saliency Detection,,,,"Ablation analysis||has||Quantitative Improvements in Saliency Detection
",,,,,
ablation-analysis,"The results are shown in Table 10 , where the 2 - way - QG MTL model ( with question generation ) versus baseline improvement is stat. significant ( p < 0.01 ) .","results are
2 - way - QG MTL model ( with question generation )
versus
baseline improvement
is
stat. significant ( p < 0.01 )","2 - way - QG MTL model ( with question generation )||versus||baseline improvement
baseline improvement||is||stat. significant ( p < 0.01 )
",,,,,"Quantitative Improvements in Saliency Detection||results are||2 - way - QG MTL model ( with question generation )
",,,
ablation-analysis,"Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .","Qualitative Examples on Entailment and Saliency Improvements
summaries",,,,"Ablation analysis||has||Qualitative Examples on Entailment and Saliency Improvements
",,,,,"Qualitative Examples on Entailment and Saliency Improvements||has||3 - way multi-task model
"
ablation-analysis,"Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .","3 - way multi-task model
generates
are both better at
logical entailment
contain more salient information",,,,,,"summaries||are both better at||logical entailment
summaries||are both better at||contain more salient information
","3 - way multi-task model||generates||summaries
",,
research-problem,Global Encoding for Abstractive Summarization,Abstractive Summarization,,,,,"Contribution||has research problem||Abstractive Summarization
",,,,
research-problem,"Therefore , sequence - to - sequence learning can be applied to neural abstractive summarization , whose model consists of an encoder and a decoder .",neural abstractive summarization,,,,,"Contribution||has research problem||neural abstractive summarization
",,,,
model,"To tackle this problem , we propose a model of global encoding for abstractive summarization .","of
global encoding
for
abstractive summarization","global encoding||for||abstractive summarization
",,"Model||of||global encoding
",,,,,,
model,We set a convolutional gated unit to perform global encoding on the source context .,"set
convolutional gated unit
to perform
global encoding
on
source context","convolutional gated unit||to perform||global encoding
global encoding||on||source context
",,"Model||set||convolutional gated unit
",,,,,,
model,"The gate based on convolutional neural network ( CNN ) filters each encoder output based on the global context due to the parameter sharing , so that the representations at each time step are refined with consideration of the global context .","gate
based on
convolutional neural network ( CNN )
filters
each encoder output
based on
global context
due to
parameter sharing","gate||based on||convolutional neural network ( CNN )
gate||filters||each encoder output
each encoder output||based on||global context
global context||due to||parameter sharing
",,,"Model||has||gate
",,,,,
experimental-setup,We implement our experiments in PyTorch on an NVIDIA 1080 Ti GPU .,"implement
our experiments
in
PyTorch on an NVIDIA 1080 Ti GPU","our experiments||in||PyTorch on an NVIDIA 1080 Ti GPU
",,"Experimental setup||implement||our experiments
",,,,,,
experimental-setup,The word embedding dimension and the number of hidden units are both 512 .,"word embedding dimension and the number of hidden units
512",,,"Experimental setup||word embedding dimension and the number of hidden units||512
",,,,,,
experimental-setup,"In both experiments , the batch size is set to 64 .","batch size
set to
64","batch size||set to||64
",,,"Experimental setup||has||batch size
",,,,,
experimental-setup,"We use Adam optimizer ( Kingma and Ba , 2014 ) with the default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 .","use
Adam optimizer ( Kingma and Ba , 2014 )
with
default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8","Adam optimizer ( Kingma and Ba , 2014 )||with||default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8
",,"Experimental setup||use||Adam optimizer ( Kingma and Ba , 2014 )
",,,,,,
experimental-setup,The learning rate is halved every epoch .,"learning rate
halved
every epoch","learning rate||halved||every epoch
",,,"Experimental setup||has||learning rate
",,,,,
experimental-setup,"Gradient clipping is applied with range [ - 10 , 10 ] .","Gradient clipping
applied with
range [ - 10 , 10 ]","Gradient clipping||applied with||range [ - 10 , 10 ]
",,,"Experimental setup||has||Gradient clipping
",,,,,
baselines,Baselines for LCSTS are introduced in the following .,"for
LCSTS
are",,,"Baselines||for||LCSTS
",,,,,,"LCSTS||has||RNN and RNN - context
"
baselines,"RNN and RNN - context are the RNNbased seq2seq models , without and with attention mechanism respectively .","RNN and RNN - context
RNNbased seq2seq models
without and with
attention mechanism","RNNbased seq2seq models||without and with||attention mechanism
","RNN and RNN - context||are||RNNbased seq2seq models
",,,,,,,
baselines,Copy - Net is the attention - based seq2seq model with the copy mechanism .,"Copy - Net
is
attention - based seq2seq model
with
copy mechanism","Copy - Net||is||attention - based seq2seq model
attention - based seq2seq model||with||copy mechanism
",,,,,,,"LCSTS||has||Copy - Net
",
baselines,SRB is a model that improves semantic relevance between source text and summary .,"SRB
improves
semantic relevance
between
source text and summary","SRB||improves||semantic relevance
semantic relevance||between||source text and summary
",,,,,,,"LCSTS||has||SRB
",
baselines,DRGD is the conventional seq2seq with a deep recurrent generative decoder .,"DRGD
is
conventional seq2seq
with
deep recurrent generative decoder","DRGD||is||conventional seq2seq
conventional seq2seq||with||deep recurrent generative decoder
",,,,,,,"LCSTS||has||DRGD
",
baselines,"As to the baselines for Gigaword , ABS and ABS + are the models with local attention and handcrafted features .","Gigaword
ABS and ABS +
are
models
with
local attention and handcrafted features","ABS and ABS +||are||models
models||with||local attention and handcrafted features
","Gigaword||has||ABS and ABS +
",,"Baselines||for||Gigaword
",,,,,
baselines,Feats is a fully RNN seq2seq model with some specific methods to control the vocabulary size .,"Feats
is
fully RNN seq2seq model
with
some specific methods
to control
vocabulary size","Feats||is||fully RNN seq2seq model
fully RNN seq2seq model||with||some specific methods
some specific methods||to control||vocabulary size
",,,,,,,"Gigaword||has||Feats
",
baselines,RAS - LSTM and RAS - Elman are seq2seq models with a convolutional encoder and an LSTM decoder and an Elman RNN decoder respectively .,"RAS - LSTM and RAS - Elman
are
seq2seq models
with
convolutional encoder
LSTM decoder
Elman RNN decoder","RAS - LSTM and RAS - Elman||are||seq2seq models
seq2seq models||with||convolutional encoder
seq2seq models||with||LSTM decoder
seq2seq models||with||Elman RNN decoder
",,,,,,,"Gigaword||has||RAS - LSTM and RAS - Elman
",
baselines,SEASS is a seq2seq model with a selective gate mechanism .,"SEASS
is a
seq2seq model
with
selective gate mechanism","SEASS||is a||seq2seq model
seq2seq model||with||selective gate mechanism
",,,,,,,"Gigaword||has||SEASS
",
baselines,DRGD is also a baseline for Gigaword .,"DRGD
baseline for
Gigaword","DRGD||baseline for||Gigaword
","Gigaword||has||DRGD
",,,,,,,
results,"In the experiments on the two datasets , our model achieves advantages of ROUGE score over the baselines , and the advantages of ROUGE score on the LCSTS are significant .","on
two datasets
our model
achieves
advantages
of
ROUGE score
over
baselines
on
LCSTS
are
significant","our model||achieves||advantages
advantages||of||ROUGE score
ROUGE score||over||baselines
ROUGE score||on||LCSTS
LCSTS||are||significant
","two datasets||has||our model
","Results||on||two datasets
",,,,,,
results,"Compared with the conventional seq2seq model , our model owns an advantage of ROUGE - 2 score 3.7 and 1.5 on the LCSTS and Gigaword respectively .","Compared with
conventional seq2seq model
our model
owns an advantage
of
ROUGE - 2 score 3.7 and 1.5
on
LCSTS and Gigaword","conventional seq2seq model||owns an advantage||our model
our model||of||ROUGE - 2 score 3.7 and 1.5
ROUGE - 2 score 3.7 and 1.5||on||LCSTS and Gigaword
",,"Results||Compared with||conventional seq2seq model
",,,,,,
research-problem,Selective Encoding for Abstractive Sentence Summarization,Abstractive Sentence Summarization,,,,,"Contribution||has research problem||Abstractive Sentence Summarization
",,,,
research-problem,"The second level representation is tailored for sentence summarization task , which leads to better performance .",sentence summarization,,,,,"Contribution||has research problem||sentence summarization
",,,,
model,In this paper we propose Selective Encoding for Abstractive Sentence Summarization ( SEASS ) .,"propose
Selective Encoding for Abstractive Sentence Summarization ( SEASS )",,,"Model||propose||Selective Encoding for Abstractive Sentence Summarization ( SEASS )
",,,,,,
model,"We treat the sentence summarization as a threephase task : encoding , selection , and decoding .","treat
sentence summarization
as a
threephase task
encoding
selection
decoding","sentence summarization||as a||threephase task
","threephase task||name||encoding
threephase task||name||selection
threephase task||name||decoding
","Model||treat||sentence summarization
",,,,,,
model,"It consists of a sentence encoder , a selective gate network , and a summary decoder .","consists of
sentence encoder
selective gate network
summary decoder",,,"Model||consists of||sentence encoder
Model||consists of||selective gate network
Model||consists of||summary decoder
",,,,,,
model,"First , the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation .","First
sentence encoder
reads
input words
through
RNN unit
to construct
first level sentence representation","sentence encoder||reads||input words
input words||through||RNN unit
RNN unit||to construct||first level sentence representation
",,"Model||First||sentence encoder
",,,,,,
model,Then the selective gate network selects the encoded information to construct the second level sentence representation .,"selective gate network
selects
encoded information
to construct
second level sentence representation","selective gate network||selects||encoded information
encoded information||to construct||second level sentence representation
",,,"Model||has||selective gate network
",,,,,
model,"The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information , which helps improve encoding effectiveness and release the burden of the decoder .","selective mechanism
controls
information flow
from
encoder to decoder
by applying
gate network
according to
sentence information","selective mechanism||controls||information flow
information flow||from||encoder to decoder
encoder to decoder||by applying||gate network
gate network||according to||sentence information
",,,"Model||has||selective mechanism
",,,,,
model,"Finally , the attention - equipped decoder generates the summary using the second level sentence representation .","attention - equipped decoder
generates
summary
using
second level sentence representation","attention - equipped decoder||generates||summary
summary||using||second level sentence representation
",,,"Model||has||attention - equipped decoder
",,,,,
results,DUC 2004,DUC 2004,,,,"Results||has||DUC 2004
",,,,,"DUC 2004||has||SEASS
"
hyperparameters,We initialize model parameters randomly using a Gaussian distribution with Xavier scheme .,"initialize
model parameters randomly
using
Gaussian distribution
with
Xavier scheme","model parameters randomly||using||Gaussian distribution
Gaussian distribution||with||Xavier scheme
",,"Hyperparameters||initialize||model parameters randomly
",,,,,,
hyperparameters,We use Adam as our optimizing algorithm .,"use
Adam
as
optimizing algorithm","Adam||as||optimizing algorithm
",,"Hyperparameters||use||Adam
",,,,,,
hyperparameters,"For the hyperparameters of Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .","learning rate
0.001
two momentum parameters
? 1 = 0.9 and ? 2 = 0.999",,,,,,"Adam||learning rate||0.001
Adam||two momentum parameters||? 1 = 0.9 and ? 2 = 0.999
",,,
hyperparameters,"During training , we test the model performance ( ROUGE - 2 F1 ) on development set for every 2,000 batches .","During
training
test
model performance ( ROUGE - 2 F1 )
on
development set
for
every 2,000 batches","training||test||model performance ( ROUGE - 2 F1 )
model performance ( ROUGE - 2 F1 )||on||development set
development set||for||every 2,000 batches
",,"Hyperparameters||During||training
",,,,,,
hyperparameters,"We also apply gradient clipping with range [ ? 5 , 5 ] during training .","apply
gradient clipping
with
range [ ? 5 , 5 ]
during
training","gradient clipping||with||range [ ? 5 , 5 ]
range [ ? 5 , 5 ]||during||training
",,"Hyperparameters||apply||gradient clipping
",,,,,,
hyperparameters,"To both speedup the training and converge quickly , we use mini-batch size 64 by grid search .","To both speedup
training and converge quickly
mini-batch size 64
by
grid search","mini-batch size 64||by||grid search
grid search||To both speedup||training and converge quickly
",,,"Hyperparameters||use||mini-batch size 64
",,,,,
baselines,"ABS + Based on ABS model , further with two - layer LSTMs for the encoder - decoder with 500 hidden units in each layer implemented in .","ABS +
Based on
ABS model
further with
two - layer LSTMs
for
encoder - decoder
with
500 hidden units
in
each layer","ABS +||Based on||ABS model
ABS model||further with||two - layer LSTMs
two - layer LSTMs||for||encoder - decoder
encoder - decoder||with||500 hidden units
500 hidden units||in||each layer
",,,"Baselines||has||ABS +
",,,,,
baselines,s 2 s+ att,s 2 s+ att,,,,,,,,"ABS +||Based on||s 2 s+ att
",
baselines,"We also implement a sequence - to sequence model with attention as our baseline and denote it as "" s2 s + att "" .","implement
sequence - to sequence model
with
attention","sequence - to sequence model||with||attention
",,,,,"s 2 s+ att||implement||sequence - to sequence model
",,,
results,Our SEASS model with beam search outperforms all baseline models by a large margin .,"SEASS model
with
beam search
outperforms
all baseline models
by
large margin","SEASS model||with||beam search
beam search||outperforms||all baseline models
all baseline models||by||large margin
",,,,,,,"English Gigaword||has||SEASS model
",
results,"Even for greedy search , our model still performs better than other methods which used beam search .","Even for
greedy search
still performs better than
other methods","greedy search||still performs better than||other methods
",,,,,"SEASS model||Even for||greedy search
",,,
results,"For the popular ROUGE - 2 metric , our SEASS model achieves 17.54 F1 score and performs better than the previous works .","For the popular
ROUGE - 2 metric
achieves
17.54 F1 score
performs better than
previous works","ROUGE - 2 metric||performs better than||previous works
ROUGE - 2 metric||achieves||17.54 F1 score
",,,,,"SEASS model||For the popular||ROUGE - 2 metric
",,,
results,"Compared to the ABS model , our model has a 6.22 ROUGE - 2 F1 relative gain .","Compared to
ABS model
6.22 ROUGE - 2 F1 relative gain",,"ABS model||has||6.22 ROUGE - 2 F1 relative gain
",,,,"SEASS model||Compared to||ABS model
",,,
results,"Compared to the highest CAs 2s baseline , our model achieves 1.57 ROUGE - 2 F1 improvement and passes the significant test according to the official ROUGE script .","highest CAs 2s baseline
achieves
1.57 ROUGE - 2 F1 improvement
passes
significant test
according to
official ROUGE script","highest CAs 2s baseline||passes||significant test
significant test||according to||official ROUGE script
highest CAs 2s baseline||achieves||1.57 ROUGE - 2 F1 improvement
",,,,,,,"SEASS model||Compared to||highest CAs 2s baseline
",
results,DUC 2004,,,,,,,,,,
results,"As summarized in , our SEASS outperforms all the baseline methods and achieves 29.21 , 9.56 and 25.51 for ROUGE 1 , 2 and L recall .","SEASS
outperforms
all the baseline methods
achieves
29.21 , 9.56 and 25.51
for
ROUGE 1 , 2 and L recall","SEASS||achieves||29.21 , 9.56 and 25.51
29.21 , 9.56 and 25.51||for||ROUGE 1 , 2 and L recall
SEASS||outperforms||all the baseline methods
",,,,,,,,
results,"Compared to the ABS + model which is tuned using DUC 2003 data , our model performs significantly better by 1.07 ROUGE - 2 recall score and is trained only with English Gigaword sentence - summary data without being tuned using DUC data .","Compared to
ABS + model
tuned using
DUC 2003 data
our model
performs
significantly better
by
1.07 ROUGE - 2 recall score
English Gigaword","ABS + model||tuned using||DUC 2003 data
our model||performs||significantly better
significantly better||by||1.07 ROUGE - 2 recall score
","ABS + model||has||our model
",,,,"DUC 2004||Compared to||ABS + model
",,,
research-problem,Coarse-to-Fine Attention Models for Document Summarization,Document Summarization,,,,,"Contribution||has research problem||Document Summarization
",,,,
approach,"Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .","to scale
attention models
prune down
length of the source sequence",,,"Approach||prune down||length of the source sequence
Approach||to scale||attention models
",,,,,,
approach,"Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention .","Instead of
naively attending to all the words
of
source
to use
two - layer hierarchical attention","two - layer hierarchical attention||Instead of||naively attending to all the words
naively attending to all the words||of||source
",,"Approach||to use||two - layer hierarchical attention
",,,,,,
approach,"For document summarization , this means dividing the document into chunks of text , sparsely attending to one or a few chunks at a time using hard attention , then applying the usual full attention over those chunks - we call this method coarse - to - fine attention .","For
document summarization
means
dividing the document
into
chunks of text
sparsely attending to
one or a few chunks at a time
using
hard attention
applying
usual full attention
over
those chunks
call
coarse - to - fine attention","document summarization||call||coarse - to - fine attention
document summarization||means||dividing the document
dividing the document||into||chunks of text
document summarization||sparsely attending to||one or a few chunks at a time
one or a few chunks at a time||applying||usual full attention
usual full attention||over||those chunks
one or a few chunks at a time||using||hard attention
",,"Approach||For||document summarization
",,,,,,
experimental-setup,"We train with minibatch stochastic gradient descent ( SGD ) with batch size 20 for 20 epochs , renormalizing gradients below norm 5 .","train with
minibatch stochastic gradient descent ( SGD )
with
batch size 20
for
20 epochs
renormalizing gradients
below norm 5","minibatch stochastic gradient descent ( SGD )||with||batch size 20
batch size 20||for||20 epochs
minibatch stochastic gradient descent ( SGD )||renormalizing gradients||below norm 5
",,"Experimental setup||train with||minibatch stochastic gradient descent ( SGD )
",,,,,,
experimental-setup,"We initialize the learning rate to 0.1 for the top - level encoder and 1 for the rest of the model , and begin decaying it by a factor of 0.5 each epoch after the validation perplexity stops decreasing .","initialize
learning rate
to
0.1
for
top - level encoder
1
for
rest of the model
begin decaying it by
a factor of 0.5
after
validation perplexity
stops
decreasing","learning rate||to||1
1||for||rest of the model
learning rate||to||0.1
0.1||for||top - level encoder
learning rate||begin decaying it by||a factor of 0.5
a factor of 0.5||after||validation perplexity
validation perplexity||stops||decreasing
",,"Experimental setup||initialize||learning rate
",,,,,,
experimental-setup,"We use 2 layer LSTMs with 500 hidden units , and we initialize word embeddings with 300 dimensional word2vec embeddings .","use
2 layer LSTMs
with
500 hidden units
word embeddings
with
300 dimensional word2vec embeddings","2 layer LSTMs||with||500 hidden units
word embeddings||with||300 dimensional word2vec embeddings
",,"Experimental setup||use||2 layer LSTMs
","Experimental setup||initialize||word embeddings
",,,,,
experimental-setup,"We initialize all other parameters as uniform in the interval [ ? 0.1 , 0.1 ] .","all other parameters
as uniform in
interval [ ? 0.1 , 0.1 ]","all other parameters||as uniform in||interval [ ? 0.1 , 0.1 ]
",,,"Experimental setup||initialize||all other parameters
",,,,,
experimental-setup,"For convolutional layers , we use a kernel width of 6 and 600 filters .","For
convolutional layers
use
kernel width
of
6 and 600 filters","convolutional layers||use||kernel width
kernel width||of||6 and 600 filters
",,"Experimental setup||For||convolutional layers
",,,,,,
experimental-setup,Positional embeddings have dimension 25 .,"Positional embeddings
have
dimension 25","Positional embeddings||have||dimension 25
",,,"Experimental setup||has||Positional embeddings
",,,,,
experimental-setup,We use dropout between stacked LSTM hidden states and before the final word generator layer to regularize ( with dropout probability 0.3 ) .,"dropout
between
stacked LSTM hidden states and before the final word generator layer
to regularize
dropout probability 0.3","dropout||between||stacked LSTM hidden states and before the final word generator layer
stacked LSTM hidden states and before the final word generator layer||to regularize||dropout probability 0.3
",,,"Experimental setup||use||dropout
",,,,,
experimental-setup,"At test time , we run beam search to produce the summary with a beam size of 5 .","At
test time
run
beam search
to produce
summary
with
beam size of 5","test time||run||beam search
beam search||to produce||summary
summary||with||beam size of 5
",,"Experimental setup||At||test time
",,,,,,
experimental-setup,Our models are implemented using Torch based on a past version of the Open NMT system,"implemented using
Torch
based on
past version of the Open NMT system","Torch||based on||past version of the Open NMT system
",,"Experimental setup||implemented using||Torch
",,,,,,
experimental-setup,4 . We ran our experiments on a 12GB Geforce GTX Titan X GPU .,"ran
our experiments
on
12GB Geforce GTX Titan X GPU","our experiments||on||12GB Geforce GTX Titan X GPU
",,"Experimental setup||ran||our experiments
",,,,,,
results,The ILP model ROUGE scores are surprisingly low .,"ILP model
ROUGE scores
surprisingly low","ILP model||ROUGE scores||surprisingly low
",,,"Results||has||ILP model
",,,,,
results,C2 F results are significantly worse than soft attention results .,"C2 F
significantly worse than
soft attention results","C2 F||significantly worse than||soft attention results
",,,"Results||has||C2 F
",,,,,
ablation-analysis,Sharpness of Attention,Sharpness of Attention,,,,"Ablation analysis||has||Sharpness of Attention
",,,,,
ablation-analysis,We compute the entropy numbers by averaging over all generated words in the validation set .,"compute
entropy numbers
by averaging
over all generated words
in
validation set","entropy numbers||by averaging||over all generated words
over all generated words||in||validation set
",,,,,"Sharpness of Attention||compute||entropy numbers
",,,
ablation-analysis,We note that the entropy of C2F is very low ( before taking the argmax at test time ) .,"note
entropy of C2F
is
very low","entropy of C2F||is||very low
",,,,,"Sharpness of Attention||note||entropy of C2F
",,,
ablation-analysis,This is exactly what we had hoped for - we will see that the model in fact learns to focus on only a few top - level chunks of the document over the course of generation .,"model
learns to focus on
few top - level chunks
of
document
over
course of generation","model||learns to focus on||few top - level chunks
few top - level chunks||of||document
document||over||course of generation
",,,,,,,"Sharpness of Attention||has||model
",
ablation-analysis,Attention Heatmaps,Attention Heatmaps,,,,"Ablation analysis||has||Attention Heatmaps
",,,,,
ablation-analysis,"In HIER , we observe that the attention becomes washed out ( in accord with its high entropy ) and is essentially averaging all of the encoder hidden states .","In
HIER
observe
attention becomes washed out
averaging
all of the encoder hidden states","HIER||observe||attention becomes washed out
attention becomes washed out||averaging||all of the encoder hidden states
",,,,,"Attention Heatmaps||In||HIER
",,,
ablation-analysis,"In C2 F , we see that we get very sharp attention on some rows as we had hoped .","C2 F
get
very sharp attention
on
some rows","C2 F||get||very sharp attention
very sharp attention||on||some rows
",,,,,,,"Attention Heatmaps||In||C2 F
",
research-problem,Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence Summarization,Abstractive Sentence Summarization,,,,,"Contribution||has research problem||Abstractive Sentence Summarization
",,,,
research-problem,"In this paper , we investigate the sentence summarization task that produces a summary from a source sentence .",sentence summarization,,,,,"Contribution||has research problem||sentence summarization
",,,,
model,"To incorporate entailment knowledge into abstractive summarization models , we propose in this work an entailment - aware encoder and an entailment - aware decoder .","propose
entailment - aware encoder
entailment - aware decoder",,,"Model||propose||entailment - aware encoder
Model||propose||entailment - aware decoder
",,,,,,
model,"We share the encoder of the summarization generation system with the entailment recognition system , so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships .","share
encoder
of
summarization generation system
with
entailment recognition system","encoder||of||summarization generation system
summarization generation system||with||entailment recognition system
",,"Model||share||encoder
",,,,,,
model,"Furthermore , we propose an entailment Reward Augmented Maximum Likelihood ( RAML ) training that encourages the decoder of the summarization system to produce summary entailed by the source .","entailment Reward Augmented Maximum Likelihood ( RAML ) training
encourages
decoder of the summarization system
to produce
summary
entailed by
source","entailment Reward Augmented Maximum Likelihood ( RAML ) training||encourages||decoder of the summarization system
decoder of the summarization system||to produce||summary
summary||entailed by||source
",,,"Model||propose||entailment Reward Augmented Maximum Likelihood ( RAML ) training
",,,,,
baselines,employ a selective encoding model to control the information flow from encoder to decoder .,"employ
selective encoding model
to control
information flow
from
encoder to decoder","selective encoding model||to control||information flow
information flow||from||encoder to decoder
",,,,,"Seq2seq + selective||employ||selective encoding model
",,,
baselines,ABS . first apply the seq2seq model to abstractive sentence summarization .,"ABS
apply
seq2seq model
to
abstractive sentence summarization","ABS||apply||seq2seq model
seq2seq model||to||abstractive sentence summarization
",,,"Baselines||has||ABS
",,,,,
baselines,ABS +. propose a neural machine translation model with two - layer LSTMs for the encoder - decoder .,"propose
neural machine translation model
with
two - layer LSTMs
for
encoder - decoder","neural machine translation model||with||two - layer LSTMs
two - layer LSTMs||for||encoder - decoder
",,,,,"ABS +||propose||neural machine translation model
",,,
baselines,Seq2seq .,Seq2seq,,,,"Baselines||has||Seq2seq
",,,,,
baselines,This is a standard seq2seq model with attention mechanism .,"is a
standard seq2seq model
with
attention mechanism","standard seq2seq model||with||attention mechanism
",,,,,"Seq2seq||is a||standard seq2seq model
",,,
baselines,Seq2seq + MTL .,Seq2seq + MTL,,,,"Baselines||has||Seq2seq + MTL
",,,,,
baselines,"This is our proposed model with entailment - aware encoder , which applies a multi-task learning ( MTL ) framework to seq2seq model .","with
entailment - aware encoder
applies
multi-task learning ( MTL ) framework
to
seq2seq model","entailment - aware encoder||applies||multi-task learning ( MTL ) framework
multi-task learning ( MTL ) framework||to||seq2seq model
",,,,,"Seq2seq + MTL||with||entailment - aware encoder
",,,
baselines,Seq2seq + MTL ( Share decoder ) .,Seq2seq + MTL ( Share decoder ),,,,"Baselines||has||Seq2seq + MTL ( Share decoder )
",,,,,
baselines,propose a multi - task learning ( MTL ) framework in which the decoder is shared for summarization generation and entailment generation task .,"propose
multi - task learning ( MTL ) framework
in which
decoder
is shared for
summarization generation and entailment generation task","multi - task learning ( MTL ) framework||in which||decoder
decoder||is shared for||summarization generation and entailment generation task
",,,,,"Seq2seq + MTL ( Share decoder )||propose||multi - task learning ( MTL ) framework
",,,
baselines,Seq2seq + ERAML .,Seq2seq + ERAML,,,,"Baselines||has||Seq2seq + ERAML
",,,,,
baselines,"This is our proposed model with entailment - aware decoder , which conducts an Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework .","with
entailment - aware decoder
conducts
Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework","entailment - aware decoder||conducts||Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework
",,,,,"Seq2seq + ERAML||with||entailment - aware decoder
",,,
baselines,Seq2seq + ROUGE -2 RAML .,Seq2seq + ROUGE -2 RAML,,,,"Baselines||has||Seq2seq + ROUGE -2 RAML
",,,,,
baselines,We apply ROUGE - 2 RAML training for seq2seq model .,"apply
ROUGE - 2 RAML training
for
seq2seq model","ROUGE - 2 RAML training||for||seq2seq model
",,,,,"Seq2seq + ROUGE -2 RAML||apply||ROUGE - 2 RAML training
",,,
baselines,Seq2seq + RL .,Seq2seq + RL,,,,"Baselines||has||Seq2seq + RL
",,,,,
baselines,We implement Reinforcement Learning ( RL ) models ( policy gradient ) with reward metrics of Entailment and ROUGE - 2 .,"implement
Reinforcement Learning ( RL ) models
with
reward metrics
of
Entailment and ROUGE - 2","Reinforcement Learning ( RL ) models||with||reward metrics
reward metrics||of||Entailment and ROUGE - 2
",,,,,"Seq2seq + RL||implement||Reinforcement Learning ( RL ) models
",,,
baselines,Seq2seq + selective .,Seq2seq + selective,,,,"Baselines||has||Seq2seq + selective
",,,,,
baselines,employ a selective encoding model to control the information flow from encoder to decoder .,,,,,,,,,,
results,Experimental Results : Gigaword Corpus,Gigaword Corpus,,,,"Results||on||Gigaword Corpus
",,,,,"Gigaword Corpus||has||Our model
"
results,Our model performs better than the previous works .,"Our model
performs better than
previous works","Our model||performs better than||previous works
",,,,,,,,
results,Experimental Results : DUC 2004,DUC 2004,,,,"Results||on||DUC 2004
",,,,,
results,"In , experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models , surpassing Feats2s by 0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L without fine - tuning on DUC data .","show
Seq2seq + selective + MTL + ERAML model
achieves
significant improvements
over
baseline models
surpassing
Feats2s
by
0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L
without
fine - tuning
on","Seq2seq + selective + MTL + ERAML model||achieves||significant improvements
significant improvements||over||baseline models
significant improvements||surpassing||Feats2s
Feats2s||by||0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L
0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L||without||fine - tuning
",,,,,"DUC 2004||show||Seq2seq + selective + MTL + ERAML model
",,,
ablation-analysis,Does our summarization model learn entailment knowledge ?,Does our summarization model learn entailment knowledge ?,,,,"Ablation analysis||has||Does our summarization model learn entailment knowledge ?
",,,,,
ablation-analysis,"For the test set of , the average entailment score for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .","For
test set
average entailment score
for
reference
is
0.72
basic seq2seq model
entailment score is
0.46","average entailment score||for||reference
reference||is||0.72
average entailment score||for||basic seq2seq model
basic seq2seq model||entailment score is||0.46
","test set||has||average entailment score
",,,,"Does our summarization model learn entailment knowledge ?||For||test set
",,,
ablation-analysis,"When we adopt entailmentbased strategies , the entailment score rises to 0.63 for seq2seq model .","adopt
entailmentbased strategies
entailment score
rises to
0.63
for
seq2seq model","entailment score||rises to||0.63
0.63||for||seq2seq model
","entailmentbased strategies||has||entailment score
",,,,"Does our summarization model learn entailment knowledge ?||adopt||entailmentbased strategies
",,,
ablation-analysis,"Note that the entailment score is 0.57 for seq2seq model with selective encoding , and we believe that the selective mechanism can filter out secondary information in the input , which will reduce the possibility to generate irrelevant information .","Note
entailment score
is
0.57
for
seq2seq model
with
selective encoding
selective mechanism
filter out
secondary information
in
input","entailment score||is||0.57
0.57||for||seq2seq model
seq2seq model||with||selective encoding
selective mechanism||filter out||secondary information
secondary information||in||input
",,,,,"Does our summarization model learn entailment knowledge ?||Note||entailment score
",,"Does our summarization model learn entailment knowledge ?||has||selective mechanism
",
ablation-analysis,Entailment - aware selective model achieves a high entailment reward of 0.71 .,"Entailment - aware selective model
achieves
high entailment reward
of
0.71","Entailment - aware selective model||achieves||high entailment reward
high entailment reward||of||0.71
",,,,,,,"Does our summarization model learn entailment knowledge ?||has||Entailment - aware selective model
",
ablation-analysis,"In part at least , we can conclude that our model has successfully learned entailment knowledge .","conclude
our model
successfully learned
entailment knowledge","our model||successfully learned||entailment knowledge
",,,,,"Does our summarization model learn entailment knowledge ?||conclude||our model
",,,
ablation-analysis,Is it less abstractive for our model ?,Is it less abstractive for our model ?,,,,"Ablation analysis||has||Is it less abstractive for our model ?
",,,,,
ablation-analysis,"shows that the seq2seq model produces more novel words ( i.e. , words that do not appear in the article ) than our model , indicating a lower degree of abstraction for our model .","shows that
seq2seq model
produces
more novel words
than
our model
indicating
lower degree of abstraction","seq2seq model||produces||more novel words
more novel words||than||our model
our model||indicating||lower degree of abstraction
",,,,,"Is it less abstractive for our model ?||shows that||seq2seq model
",,,
ablation-analysis,"However , when we exclude all the words not in the reference ( these words may lead to wrong information ) , our model generates more novel words , suggesting that our model provides a compromise solution for informativeness and correctness .","exclude
all the words not in the reference
model
generates
more novel words
suggesting that
our model
provides
compromise solution
for
informativeness and correctness","model||generates||more novel words
more novel words||suggesting that||our model
our model||provides||compromise solution
compromise solution||for||informativeness and correctness
","all the words not in the reference||has||model
",,,,"Is it less abstractive for our model ?||exclude||all the words not in the reference
",,,
ablation-analysis,6.6.3 Could the entailment recognition also be improved ?,Could the entailment recognition also be improved ?,,,,"Ablation analysis||has||Could the entailment recognition also be improved ?
",,,,,
ablation-analysis,shows that our summarization model with MTL outperforms basic seq2seq model .,"shows
our summarization model
with
MTL
outperforms
basic seq2seq model","our summarization model||with||MTL
MTL||outperforms||basic seq2seq model
",,,,,"Could the entailment recognition also be improved ?||shows||our summarization model
",,,
ablation-analysis,"As ? increases , the accuracy of entailment recognition improves and finally exceeds that of the model without MTL , which reveals the advantage of MTL framework .","accuracy
of
entailment recognition","accuracy||of||entailment recognition
",,,,,,,"Could the entailment recognition also be improved ?||has||accuracy
",
research-problem,Structure - Infused Copy Mechanisms for Abstractive Summarization,Abstractive Summarization,,,,,"Contribution||has research problem||Abstractive Summarization
",,,,
research-problem,Seq2seq learning has produced promising results on summarization .,summarization,,,,,"Contribution||has research problem||summarization
",,,,
model,In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary - worthy content and compose summaries that preserve the important meaning of the source texts .,"incorporating
source syntactic structure
in
neural sentence summarization
to help
system
identify
summary - worthy content
compose
summaries
that preserve
important meaning of the source texts","source syntactic structure||in||neural sentence summarization
neural sentence summarization||to help||system
system||identify||summary - worthy content
system||compose||summaries
summaries||that preserve||important meaning of the source texts
",,"Model||incorporating||source syntactic structure
",,,,,,
model,We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences .,"present
structure - infused copy mechanisms
to facilitate copying
source words and relations
to
summary
based on
semantic and structural importance
in
source sentences","structure - infused copy mechanisms||to facilitate copying||source words and relations
source words and relations||to||summary
summary||based on||semantic and structural importance
semantic and structural importance||in||source sentences
",,"Model||present||structure - infused copy mechanisms
",,,,,,
results,We first report results on the Gigaword valid - 2000 dataset in .,"on
Gigaword valid - 2000 dataset",,,"Results||on||Gigaword valid - 2000 dataset
",,,,,,
results,"We present R - 1 , R - 2 , and R - L scores ) that respectively measures the overlapped unigrams , bigrams , and longest common subsequences between the system and reference summaries 3 .","present
R - 1 , R - 2 , and R - L scores
measures
overlapped unigrams , bigrams , and longest common subsequences
between
system and reference summaries","R - 1 , R - 2 , and R - L scores||measures||overlapped unigrams , bigrams , and longest common subsequences
overlapped unigrams , bigrams , and longest common subsequences||between||system and reference summaries
",,,,,"Gigaword valid - 2000 dataset||present||R - 1 , R - 2 , and R - L scores
",,,
results,"Overall , we observe that models equipped with the structure - infused copy mechanisms are superior to the baseline , suggesting that combining source syntactic structure with the copy mechanism is effective .","observe
models
with
structure - infused copy mechanisms
superior to
baseline","models||with||structure - infused copy mechanisms
structure - infused copy mechanisms||superior to||baseline
",,,,,"Gigaword valid - 2000 dataset||observe||models
",,,
results,"We found that the "" Struct + Hidden "" architecture , which directly concatenates structural embeddings with the encoder hidden states , outperforms "" Struct + Input "" despite that the latter requires more parameters .","found
"" Struct + Hidden "" architecture
directly concatenates
structural embeddings
with
encoder hidden states
outperforms
"" Struct + Input """,""" Struct + Hidden "" architecture||directly concatenates||structural embeddings
structural embeddings||with||encoder hidden states
"" Struct + Hidden "" architecture||outperforms||"" Struct + Input ""
",,,,,"Gigaword valid - 2000 dataset||found||"" Struct + Hidden "" architecture
",,,
results,""" Struct + 2 Way + Word "" also demonstrates strong performance , achieving 43.21 % , 21. 84 % , and 40.86 % F 1 scores , for R - 1 , R - 2 , and R - L respectively .","Struct + 2 Way + Word
demonstrates
strong performance
achieving
43.21 % , 21. 84 % , and 40.86 % F 1 scores
for
R - 1 , R - 2 , and R - L","Struct + 2 Way + Word||demonstrates||strong performance
Struct + 2 Way + Word||achieving||43.21 % , 21. 84 % , and 40.86 % F 1 scores
43.21 % , 21. 84 % , and 40.86 % F 1 scores||for||R - 1 , R - 2 , and R - L
",,,,,,,"Gigaword valid - 2000 dataset||has||Struct + 2 Way + Word
",
research-problem,Concept Pointer Network for Abstractive Summarization,Abstractive Summarization,,,,,"Contribution||has research problem||Abstractive Summarization
",,,,
research-problem,Abstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .,Abstractive summarization ( ABS ),,,,,"Contribution||has research problem||Abstractive summarization ( ABS )
",,,,
model,"Hence , in this paper , we propose a novel model based on a concept pointer generator that encourages the generation of conceptual and abstract words .","propose
novel model
based on
concept pointer generator
encourages the generation of
conceptual and abstract words","novel model||based on||concept pointer generator
concept pointer generator||encourages the generation of||conceptual and abstract words
",,"Model||propose||novel model
",,,,,,
model,"As a hidden benefit , the model also alleviates the OOV problems .","alleviates
OOV problems",,,"Model||alleviates||OOV problems
",,,,,,
model,"Our model uses pointer network to capture the salient information from a source text , and then employs another pointer to generalize the detailed words according to their upper level of expressions .","uses
pointer network
to capture
salient information
from
source text
employs
another pointer
to generalize
detailed words
according to
upper level of expressions","another pointer||to generalize||detailed words
detailed words||according to||upper level of expressions
pointer network||to capture||salient information
salient information||from||source text
",,"Model||employs||another pointer
Model||uses||pointer network
",,,,,,
model,The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .,"optimization function
adaptive so as to cater for
different datasets
with
distantly - supervised training","optimization function||adaptive so as to cater for||different datasets
different datasets||with||distantly - supervised training
",,,"Model||has||optimization function
",,,,,
model,"The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .","network
optimized end - to - end
using
reinforcement learning
with
distant - supervision strategy","network||using||reinforcement learning
reinforcement learning||with||distant - supervision strategy
",,"Model||optimized end - to - end||network
",,,,,,
experimental-setup,We initialize word embeddings with 128 - d vectors and fine - tune them during training .,"initialize
word embeddings
128 - d vectors
fine - tune
during training","word embeddings||fine - tune||during training
word embeddings||initialize||128 - d vectors
",,,"Experimental setup||has||word embeddings
",,,,,
experimental-setup,The vocabulary size was set to 150 k for both the source and target text .,"vocabulary size
set to
150 k
for
both the source and target text","vocabulary size||set to||150 k
150 k||for||both the source and target text
",,,"Experimental setup||has||vocabulary size
",,,,,
experimental-setup,The hidden state size was set to 256 .,"hidden state size
set to
256","hidden state size||set to||256
",,,"Experimental setup||has||hidden state size
",,,,,
code,"Our code is available on https :// github.com/wprojectsn/codes , and the vocabularies and candidate concepts are also included .",https :// github.com/wprojectsn/codes,,,,,"Contribution||Code||https :// github.com/wprojectsn/codes
",,,,
experimental-setup,We trained our models on a single GTX TI - TAN GPU machine .,"models
on
single GTX TI - TAN GPU machine","models||on||single GTX TI - TAN GPU machine
",,,"Experimental setup||has||models
",,,,,
experimental-setup,We used the Adagrad optimizer with a batch size of 64 to minimize the loss .,"used
Adagrad optimizer
with
batch size of 64
to minimize
loss","Adagrad optimizer||with||batch size of 64
batch size of 64||to minimize||loss
",,"Experimental setup||used||Adagrad optimizer
",,,,,,
experimental-setup,"The initial learning rate and the accumulator value were set to 0.15 and 0.1 , respectively .","initial learning rate
accumulator value
set to
0.15
0.1",,"0.1||has||accumulator value
0.15||has||initial learning rate
","Experimental setup||set to||0.1
Experimental setup||set to||0.15
",,,,,,
experimental-setup,We used gradient clipping with a maximum gradient norm of 2 .,"gradient clipping
with
maximum gradient norm
of
2","gradient clipping||with||maximum gradient norm
maximum gradient norm||of||2
",,,"Experimental setup||used||gradient clipping
",,,,,
experimental-setup,"We trained our concept pointer generator for 450 k iterations yielded the best performance , then took the optimization using RL rewards for RG - L at 95 K iterations on DUC - 2004 and at 50 K iterations on Gigaword .","trained
concept pointer generator
for
450 k iterations
yielded
best performance
optimization using
RL rewards
for
RG - L
at
95 K iterations
on
DUC - 2004
50 K iterations
on
Gigaword","RL rewards||for||RG - L
RG - L||at||50 K iterations
50 K iterations||on||Gigaword
RG - L||at||95 K iterations
95 K iterations||on||DUC - 2004
concept pointer generator||for||450 k iterations
450 k iterations||yielded||best performance
",,"Experimental setup||optimization using||RL rewards
Experimental setup||trained||concept pointer generator
",,,,,,
experimental-setup,We took the distancesupervised training at 5 K iterations on DUC - 2004 and at 6.5 K iterations on Gigaword .,"took
distancesupervised training
at
5 K iterations
on
DUC - 2004
6.5 K iterations
on
Gigaword","distancesupervised training||at||5 K iterations
5 K iterations||on||DUC - 2004
distancesupervised training||at||6.5 K iterations
6.5 K iterations||on||Gigaword
",,"Experimental setup||took||distancesupervised training
",,,,,,
baselines,ABS + is a tuned ABS model with additional features .,"ABS +
is a
tuned ABS model","ABS +||is a||tuned ABS model
",,,"Baselines||has||ABS +
",,,,,
baselines,RAS - Elman ) is a convolution encoder and an Elman RNN decoder with attention .,"RAS - Elman
is a
convolution encoder
Elman RNN decoder
with
attention","RAS - Elman||is a||convolution encoder
RAS - Elman||is a||Elman RNN decoder
Elman RNN decoder||with||attention
",,,"Baselines||has||RAS - Elman
",,,,,
baselines,Seq2seq + att is two - layer BiLSTM encoder and one - layer LSTM decoder equipped with attention .,"Seq2seq + att
is
two - layer BiLSTM encoder
one - layer LSTM decoder
equipped with
attention","Seq2seq + att||is||two - layer BiLSTM encoder
Seq2seq + att||is||one - layer LSTM decoder
one - layer LSTM decoder||equipped with||attention
",,,"Baselines||has||Seq2seq + att
",,,,,
baselines,lvt5 k - lsent uses temporal attention to keep track of the past attentive weights of the decoder and restrains the repetition in later sequences .,"lvt5 k - lsent
uses
temporal attention
to keep track of
past attentive weights
of
decoder
restrains
repetition
in
later sequences","lvt5 k - lsent||uses||temporal attention
temporal attention||to keep track of||past attentive weights
past attentive weights||of||decoder
temporal attention||restrains||repetition
repetition||in||later sequences
",,,"Baselines||has||lvt5 k - lsent
",,,,,
baselines,SEASS includes an additional selective gate to control information flow from the encoder to the decoder .,"SEASS
includes
additional selective gate
to control
information flow
from
encoder
to
decoder","SEASS||includes||additional selective gate
additional selective gate||to control||information flow
information flow||from||encoder
encoder||to||decoder
",,,"Baselines||has||SEASS
",,,,,
baselines,Pointer - generator is an integrated pointer network and seq2seq model .,"Pointer - generator
is
integrated pointer network
seq2seq model","Pointer - generator||is||integrated pointer network
Pointer - generator||is||seq2seq model
",,,"Baselines||has||Pointer - generator
",,,,,
baselines,CGU ) sets a convolutional gated unit and self - attention for global encoding .,"CGU
sets
convolutional gated unit
self - attention
for
global encoding","CGU||for||global encoding
global encoding||sets||convolutional gated unit
global encoding||sets||self - attention
",,,"Baselines||has||CGU
",,,,,
results,We observe that our model outperformed all the strong state of - the - art models on both datasets in all metrics except for RG - 2 on Gigaword .,"observe
model",,,"Results||observe||model
",,,,,,"model||outperformed||all the strong state of - the - art models
model||outperformed||in all metrics
"
results,"In terms of the pointer generator performance , the improvements made by our concept pointer are statistically significant ( p < 0.01 ) across all metrics .","In terms of
pointer generator performance
improvements made
concept pointer
statistically significant ( p < 0.01 )
across
all metrics","concept pointer||improvements made||statistically significant ( p < 0.01 )
statistically significant ( p < 0.01 )||across||all metrics
","pointer generator performance||has||concept pointer
","Results||In terms of||pointer generator performance
",,,,,,
research-problem,Entity Commonsense Representation for Neural Abstractive Summarization,Abstractive Summarization,,,,,"Contribution||has research problem||Abstractive Summarization
",,,,
research-problem,Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text .,Text summarization,,,,,"Contribution||has research problem||Text summarization
",,,,
model,"To this end , we present a method to effectively apply linked entities in sequence - tosequence models , called Entity2Topic ( E2T ) .","method
effectively apply linked entities
in
sequence - tosequence models
called
Entity2Topic ( E2T )","effectively apply linked entities||in||sequence - tosequence models
sequence - tosequence models||called||Entity2Topic ( E2T )
",,"Model||method||effectively apply linked entities
",,,,,,
model,E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .,"E2T
module",,,"Model||module||E2T
",,,,,,
model,"The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .","encodes
entities extracted from the original text
by
entity linking system ( ELS )
constructs
vector
representing
topic
of
summary to be generated
informs
decoder
about
constructed topic vector","entities extracted from the original text||by||entity linking system ( ELS )
vector||representing||topic
topic||of||summary to be generated
decoder||about||constructed topic vector
",,,,,"E2T||encodes||entities extracted from the original text
E2T||constructs||vector
E2T||informs||decoder
",,,
model,We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention .,"using
entity encoders
with
selective disambiguation
constructing
topic vectors
using
firm attention","entity encoders||with||selective disambiguation
entity encoders||constructing||topic vectors
topic vectors||using||firm attention
",,"Model||using||entity encoders
",,,,,,
experimental-setup,"For both datasets , we further reduce the size of the input , output , and entity vocabularies to at most 50 K as suggested in and replace less frequent words to "" < unk > "" .","reduce
size of the input , output , and entity vocabularies
to
at most 50 K
replace
less frequent words
to
< unk >","size of the input , output , and entity vocabularies||to||at most 50 K
less frequent words||to||< unk >
",,"Experimental setup||reduce||size of the input , output , and entity vocabularies
Experimental setup||replace||less frequent words
",,,,,,
experimental-setup,We use 300D Glove 6 and 1000D wiki2vec 7 pre-trained vectors to initialize our word and entity vectors .,"use
300D Glove
1000D wiki2vec
pre-trained vectors
initialize
word and entity vectors","word and entity vectors||use||pre-trained vectors
","pre-trained vectors||name||300D Glove
pre-trained vectors||name||1000D wiki2vec
","Experimental setup||initialize||word and entity vectors
",,,,,,
experimental-setup,"For GRUs , we set the state size to 500 .","For
GRUs
set
state size
to
500","GRUs||set||state size
state size||to||500
",,"Experimental setup||For||GRUs
",,,,,,
experimental-setup,"For CNN , we set h = 3 , 4 , 5 with 400 , 300 , 300 feature maps , respectively .","CNN
set
h = 3 , 4 , 5
with
400 , 300 , 300 feature maps","CNN||set||h = 3 , 4 , 5
h = 3 , 4 , 5||with||400 , 300 , 300 feature maps
",,,"Experimental setup||For||CNN
",,,,,
experimental-setup,"For firm attention , k is tuned by calculating the perplexity of the model starting with smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... ) and stopping when the perplexity of the model becomes worse than the previous model .","firm attention
k
tuned
by calculating
perplexity
of
model
starting with
smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... )
stopping when
perplexity of the model becomes worse
than
previous model","firm attention||tuned||k
k||by calculating||perplexity
perplexity||of||model
model||starting with||smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... )
model||stopping when||perplexity of the model becomes worse
perplexity of the model becomes worse||than||previous model
",,,"Experimental setup||For||firm attention
",,,,,
experimental-setup,We use dropout on all non-linear connections with a dropout rate of 0.5 .,"use
dropout
on
all non-linear connections
with
dropout rate of 0.5","dropout||on||all non-linear connections
all non-linear connections||with||dropout rate of 0.5
",,"Experimental setup||use||dropout
",,,,,,
experimental-setup,"We set the batch sizes of Gigaword and CNN datasets to 80 and 10 , respectively .","set
batch sizes
of
Gigaword and CNN datasets
to
80 and 10","batch sizes||of||Gigaword and CNN datasets
Gigaword and CNN datasets||to||80 and 10
",,"Experimental setup||set||batch sizes
",,,,,,
experimental-setup,"Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule , with l 2 constraint ( Hinton et al. , 2012 ) of 3 .","Training is done via
stochastic gradient descent
over
shuffled mini-batches
with
Adadelta update rule
l 2 constraint ( Hinton et al. , 2012 ) of 3","stochastic gradient descent||over||shuffled mini-batches
shuffled mini-batches||with||Adadelta update rule
shuffled mini-batches||with||l 2 constraint ( Hinton et al. , 2012 ) of 3
",,"Experimental setup||Training is done via||stochastic gradient descent
",,,,,,
experimental-setup,We perform early stopping using a subset of the given development dataset .,"perform
early stopping
using
subset of the given development dataset","early stopping||using||subset of the given development dataset
",,"Experimental setup||perform||early stopping
",,,,,,
experimental-setup,We use beam search of size 10 to generate the summary .,"beam search
of size
10
to generate
summary","beam search||of size||10
10||to generate||summary
",,,"Experimental setup||use||beam search
",,,,,
baselines,"For the Gigaword dataset , we compare our models with the following abstractive baselines :","For
Gigaword dataset
compare our models with",,,"Baselines||For||Gigaword dataset
",,,,"Gigaword dataset||compare our models with||Feat2s
Gigaword dataset||compare our models with||Luong - NMT
Gigaword dataset||compare our models with||RAS - Elman
Gigaword dataset||compare our models with||ABS +
Gigaword dataset||compare our models with||SEASS
",,
baselines,"ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .","ABS +
is a
fine tuned version of ABS
uses
attentive CNN encoder
NNLM decoder
Feat2s
is an
RNN sequence - to - sequence model
with
lexical and statistical features
in
encoder
Luong - NMT
is a
two - layer LSTM encoder - decoder model
RAS - Elman
uses
attentive CNN encoder
Elman RNN decoder
SEASS
uses
BiGRU encoders
GRU decoders
with
selective encoding","Feat2s||is an||RNN sequence - to - sequence model
RNN sequence - to - sequence model||with||lexical and statistical features
lexical and statistical features||in||encoder
Luong - NMT||is a||two - layer LSTM encoder - decoder model
RAS - Elman||uses||attentive CNN encoder
RAS - Elman||uses||Elman RNN decoder
ABS +||is a||fine tuned version of ABS
fine tuned version of ABS||uses||attentive CNN encoder
fine tuned version of ABS||uses||NNLM decoder
SEASS||uses||BiGRU encoders
SEASS||uses||GRU decoders
GRU decoders||with||selective encoding
",,,,,,,,
baselines,"For the CNN dataset , we compare our models with the following extractive and abstractive baselines :","CNN dataset
compare our models with",,,,"Baselines||For||CNN dataset
",,,"CNN dataset||compare our models with||Distraction - M3
CNN dataset||compare our models with||GBA
CNN dataset||compare our models with||Lead - 3
CNN dataset||compare our models with||Bi - GRU
CNN dataset||compare our models with||LexRank
",,
baselines,"Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .","Lead - 3
is a
extracts
first three sentences of the document
as
summary
LexRank
extracts
texts
using
LexRank
Bi - GRU
is a
non-hierarchical one - layer sequence - to - sequence abstractive baseline
Distraction - M3
uses
sequence - to - sequence abstractive model
with
distraction - based networks
GBA
graph - based attentional neural abstractive model","Distraction - M3||uses||sequence - to - sequence abstractive model
sequence - to - sequence abstractive model||with||distraction - based networks
GBA||is a||graph - based attentional neural abstractive model
Lead - 3||extracts||first three sentences of the document
first three sentences of the document||as||summary
Bi - GRU||is a||non-hierarchical one - layer sequence - to - sequence abstractive baseline
LexRank||extracts||texts
texts||using||LexRank
",,,,,,,,
results,"In Gigaword dataset where the texts are short , our best model achieves a comparable performance with the current state - of - the - art .","In
Gigaword dataset
where
texts are short
our best model
achieves
comparable performance
with
current state - of - the - art","Gigaword dataset||where||texts are short
our best model||achieves||comparable performance
comparable performance||with||current state - of - the - art
","Gigaword dataset||has||our best model
","Results||In||Gigaword dataset
",,,,,,
results,"In CNN dataset where the texts are longer , our best model outperforms all the previous models .","CNN dataset
where
texts are longer
our best model
outperforms
all the previous models","CNN dataset||where||texts are longer
our best model||outperforms||all the previous models
","CNN dataset||has||our best model
",,"Results||In||CNN dataset
",,,,,
results,"Overall , E2T achieves a significant improvement over the baseline model BASE , with at least 2 ROUGE - 1 points increase in the Gigaword dataset and 6 ROUGE - 1 points increase in the CNN dataset .","E2T
achieves
significant improvement
over
baseline model BASE
with
at least 2 ROUGE
1 points increase
in
Gigaword dataset
6 ROUGE
1 points increase
in
CNN dataset","E2T||achieves||significant improvement
significant improvement||over||baseline model BASE
baseline model BASE||with||at least 2 ROUGE
1 points increase||in||Gigaword dataset
baseline model BASE||with||6 ROUGE
1 points increase||in||CNN dataset
","at least 2 ROUGE||has||1 points increase
6 ROUGE||has||1 points increase
",,"Results||has||E2T
",,,,,
results,"Among the model variants , the CNN - based encoder with selective disambiguation and firm attention performs the best .","Among
model variants
CNN - based encoder
with
selective disambiguation
firm attention
performs
best","CNN - based encoder||with||selective disambiguation
CNN - based encoder||with||firm attention
CNN - based encoder||performs||best
","model variants||has||CNN - based encoder
","Results||Among||model variants
",,,,,,
research-problem,"Retrieve , Rerank and Rewrite : Soft Template Based Neural Summarization",Neural Summarization,,,,,"Contribution||has research problem||Neural Summarization
",,,,
research-problem,"In this paper , we focus on an increasingly intriguing task , i.e. , abstractive sentence summarization , which generates a shorter version of a given sentence while attempting to preserve its original meaning .",abstractive sentence summarization,,,,,"Contribution||has research problem||abstractive sentence summarization
",,,,
approach,"Due to the strong rewriting ability of the seq2seq framework , in this paper , we propose to combine the seq2seq and template based summarization approaches .","combine
seq2seq and template based summarization approaches",,,"Approach||combine||seq2seq and template based summarization approaches
",,,,,,
approach,"We call our summarization system Re 3 Sum , which consists of three modules : Retrieve , Rerank and Rewrite .","call
summarization system
Re 3 Sum
consists of
three modules
Retrieve
Rerank
Rewrite","Re 3 Sum||consists of||three modules
","summarization system||name||Re 3 Sum
three modules||name||Retrieve
three modules||name||Rerank
three modules||name||Rewrite
","Approach||call||summarization system
",,,,,,
approach,We utilize a widely - used Information Retrieval ( IR ) platform to find out candidate soft templates from the training corpus .,"utilize
widely - used Information Retrieval ( IR ) platform
to find out
candidate soft templates
from
training corpus","widely - used Information Retrieval ( IR ) platform||to find out||candidate soft templates
candidate soft templates||from||training corpus
",,"Approach||utilize||widely - used Information Retrieval ( IR ) platform
",,,,,,
approach,"Then , we extend the seq2seq model to jointly learn template saliency measurement ( Rerank ) and final summary generation ( Rewrite ) .","extend
seq2seq model
to jointly learn
template saliency measurement ( Rerank )
final summary generation ( Rewrite )","seq2seq model||to jointly learn||template saliency measurement ( Rerank )
seq2seq model||to jointly learn||final summary generation ( Rewrite )
",,"Approach||extend||seq2seq model
",,,,,,
approach,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the input sentence and each candidate template into hidden states .","Recurrent Neural Network ( RNN ) encoder
applied to
convert
input sentence
each candidate template
into
hidden states","Recurrent Neural Network ( RNN ) encoder||applied to||convert
convert||into||hidden states
","hidden states||name||input sentence
hidden states||name||each candidate template
",,"Approach||has||Recurrent Neural Network ( RNN ) encoder
",,,,,
approach,"In Rerank , we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence .","In
Rerank
measure
informativeness
of
candidate template
according to
hidden state relevance
to
input sentence","Rerank||measure||informativeness
informativeness||of||candidate template
candidate template||according to||hidden state relevance
hidden state relevance||to||input sentence
",,"Approach||In||Rerank
",,,,,,
approach,The candidate template with the highest predicted informativeness is regarded as the actual soft template .,"with
highest predicted informativeness
is regarded as
actual soft template","highest predicted informativeness||is regarded as||actual soft template
",,,,,"candidate template||with||highest predicted informativeness
",,,
approach,"In Rewrite , the summary is generated according to the hidden states of both the sentence and template .","Rewrite
summary
generated
according to
hidden states
of both
sentence and template","Rewrite||generated||summary
summary||according to||hidden states
hidden states||of both||sentence and template
",,,"Approach||In||Rewrite
",,,,,
code,Code and results can be found at http://www4.comp.polyu.edu.hk/cszqcao/,http://www4.comp.polyu.edu.hk/cszqcao/,,,,,"Contribution||Code||http://www4.comp.polyu.edu.hk/cszqcao/
",,,,
experimental-setup,We use the popular seq2seq framework Open - NMT 5 as the starting point .,"use
popular seq2seq framework
Open - NMT",,"popular seq2seq framework||name||Open - NMT
","Experimental setup||use||popular seq2seq framework
",,,,,,
experimental-setup,"To make our model more general , we retain the default settings of Open NMT to build the network architecture .","retain
default settings
of
Open NMT
to build
network architecture","default settings||of||Open NMT
Open NMT||to build||network architecture
",,"Experimental setup||retain||default settings
",,,,,,
experimental-setup,"Specifically , the dimensions of word embeddings and RNN are both 500 , and the encoder and decoder structures are two - layer bidirectional Long Short Term Memory Networks ( LSTMs ) .","dimensions
word embeddings and RNN
are both
500
encoder and decoder structures
are
two - layer bidirectional Long Short Term Memory Networks ( LSTMs )","encoder and decoder structures||are||two - layer bidirectional Long Short Term Memory Networks ( LSTMs )
word embeddings and RNN||are both||500
",,"Experimental setup||dimensions||word embeddings and RNN
","Experimental setup||has||encoder and decoder structures
",,,,,
experimental-setup,"On our computer ( GPU : GTX 1080 , Memory : 16G , CPU : i7-7700 K ) , the training spends about 2 days .","On
our computer
GPU
GTX 1080
Memory
16G
CPU
i7-7700 K
training spends
about 2 days","our computer||Memory||16G
our computer||CPU||i7-7700 K
our computer||GPU||GTX 1080
our computer||training spends||about 2 days
",,"Experimental setup||On||our computer
",,,,,,
experimental-setup,"During test , we use beam search of size 5 to generate summaries .","During test
beam search
of size
5
to generate
summaries","beam search||of size||5
5||to generate||summaries
",,"Experimental setup||During test||beam search
",,,,,,
experimental-setup,"We add the argument "" replace unk "" to replace the generated unknown words with the source word that holds the highest attention weight .","add
argument
replace unk
to replace
generated unknown words
with
source word
that holds
highest attention weight","replace unk||to replace||generated unknown words
generated unknown words||with||source word
source word||that holds||highest attention weight
","argument||name||replace unk
","Experimental setup||add||argument
",,,,,,
experimental-setup,"Since the generated summaries are often shorter than the actual ones , we introduce an additional length penalty argument "" alpha 1 "" to encourage longer generation , like .","introduce
additional length penalty argument
alpha 1
to encourage
longer generation","alpha 1||to encourage||longer generation
","additional length penalty argument||name||alpha 1
","Experimental setup||introduce||additional length penalty argument
",,,,,,
baselines,OpenNMT,OpenNMT,,,,"Baselines||has||OpenNMT
",,,,,
baselines,We also implement the standard attentional seq2seq model with OpenNMT .,"implement
standard attentional seq2seq model",,,,,,"OpenNMT||implement||standard attentional seq2seq model
",,,
baselines,All the settings are the same as our system .,,,,,,,,,,
baselines,FTSum encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries .,"FTSum
encoded
facts
extracted from
source sentence
to improve
both
faithfulness
informativeness
generated summaries","FTSum||encoded||facts
facts||extracted from||source sentence
source sentence||to improve||generated summaries
generated summaries||both||faithfulness
generated summaries||both||informativeness
",,,"Baselines||has||FTSum
",,,,,
baselines,"In addition , to evaluate the effectiveness of our joint learning framework , we develop a baseline named "" PIPELINE "" .",PIPELINE,,,,"Baselines||has||PIPELINE
",,,,,
baselines,"However , it trains the Rerank module and Rewrite module in pipeline .","trains
Rerank module
Rewrite module",,,,,,"PIPELINE||trains||Rerank module
PIPELINE||trains||Rewrite module
",,,
results,We also examine the performance of directly regarding soft templates as output summaries .,"examine
performance
of directly regarding
soft templates
as
output summaries","performance||of directly regarding||soft templates
soft templates||as||output summaries
",,"Results||examine||performance
",,,,,,
results,We introduce five types of different soft templates :,"introduce
five types of different soft templates",,,,,,"soft templates||introduce||five types of different soft templates
",,,
results,"As shown in , the performance of Random is terrible , indicating it is impossible to use one summary template to fit various actual summaries .","performance of
Random
is
terrible","Random||is||terrible
",,,,,"five types of different soft templates||performance of||Random
",,,
results,"Rerank largely outperforms First , which verifies the effectiveness of the Rerank module .","Rerank
largely outperforms
First","Rerank||largely outperforms||First
",,,,,,,"five types of different soft templates||has||Rerank
",
results,"Likewise , comparing Max and First , we observe that the improving capacity of the Retrieve module is high .","comparing
Max and First
observe that
improving capacity
of
Retrieve module
is
high","Max and First||observe that||improving capacity
improving capacity||of||Retrieve module
Retrieve module||is||high
",,,,,"five types of different soft templates||comparing||Max and First
",,,
results,Notice that Optimal greatly exceeds all the state - of - the - art approaches .,"Optimal
greatly exceeds
all the state - of - the - art approaches","Optimal||greatly exceeds||all the state - of - the - art approaches
",,,,,,,"five types of different soft templates||has||Optimal
",
results,"We also measure the linguistic quality of generated summaries from various aspects , and the results are present in .","measure
linguistic quality
of
generated summaries","linguistic quality||of||generated summaries
",,"Results||measure||linguistic quality
",,,,,,
results,"As can be seen from the rows "" LEN DIF "" and "" LESS 3 "" , the performance of Re 3 Sum is almost the same as that of soft templates .","performance of
Re 3 Sum
almost the same as
soft templates","Re 3 Sum||almost the same as||soft templates
",,,,,"linguistic quality||performance of||Re 3 Sum
",,,
results,"In this section , we investigate how soft templates affect our model .","investigate
soft templates
affect
our model","soft templates||affect||our model
",,"Results||investigate||soft templates
",,,,,,
results,"As illustrated in , the more high - quality templates are provided , the higher ROUGE scores are achieved .","more high - quality templates
provided
higher ROUGE scores
achieved","more high - quality templates||achieved||higher ROUGE scores
",,,,,"soft templates||provided||more high - quality templates
",,,
results,"Next , we manually inspect the summaries generated by different methods .","manually inspect
summaries
generated by
different methods","summaries||generated by||different methods
",,,,,"soft templates||manually inspect||summaries
",,,
results,We find the outputs of Re 3 Sum are usually longer and more flu - ent than the outputs of OpenNMT .,"find
outputs of Re 3 Sum
longer and more flu - ent than
outputs of OpenNMT","outputs of Re 3 Sum||longer and more flu - ent than||outputs of OpenNMT
",,,,,"summaries||find||outputs of Re 3 Sum
",,,
results,"As can be seen , with different templates given , our model is likely to generate dissimilar summaries .","with
different templates given
our model
likely to generate
dissimilar summaries","our model||likely to generate||dissimilar summaries
","different templates given||has||our model
",,,,"soft templates||with||different templates given
",,,
research-problem,Deep Recurrent Generative Decoder for Abstractive Text Summarization,Abstractive Text Summarization,,,,,"Contribution||has research problem||Abstractive Text Summarization
",,,,
research-problem,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,Automatic summarization,,,,,"Contribution||has research problem||Automatic summarization
",,,,
model,"To tackle the above mentioned problems , we design a new framework based on sequence to - sequence oriented encoder - decoder model equipped with a latent structure modeling component .","design
new framework
based on
sequence to - sequence oriented encoder - decoder model
equipped with
latent structure modeling component","new framework||based on||sequence to - sequence oriented encoder - decoder model
sequence to - sequence oriented encoder - decoder model||equipped with||latent structure modeling component
",,"Model||design||new framework
",,,,,,
model,We employ Variational Auto - Encoders ( VAEs ) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling .,"employ
Variational Auto - Encoders ( VAEs )
as
base model
for
our generative framework
can handle
inference problem
associated with
complex generative modeling","Variational Auto - Encoders ( VAEs )||as||base model
base model||for||our generative framework
our generative framework||can handle||inference problem
inference problem||associated with||complex generative modeling
",,"Model||employ||Variational Auto - Encoders ( VAEs )
",,,,,,
model,"Inspired by , we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .","add
historical dependencies
on
latent variables
of
VAEs
propose
deep recurrent generative decoder ( DRGD )
for
latent structure modeling","historical dependencies||on||latent variables
latent variables||of||VAEs
deep recurrent generative decoder ( DRGD )||for||latent structure modeling
",,"Model||add||historical dependencies
Model||propose||deep recurrent generative decoder ( DRGD )
",,,,,,
model,Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework .,"standard discriminative deterministic decoder and the recurrent generative decoder
integrated
into
unified decoding framework","standard discriminative deterministic decoder and the recurrent generative decoder||into||unified decoding framework
",,"Model||integrated||standard discriminative deterministic decoder and the recurrent generative decoder
",,,,,,
model,The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information .,"target summaries
decoded
based on both
discriminative deterministic variables
generative latent structural information","target summaries||based on both||discriminative deterministic variables
target summaries||based on both||generative latent structural information
",,"Model||decoded||target summaries
",,,,,,
research-problem,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,,,,,,,,,,
baselines,TOPIARY is the best on DUC2004 Task - 1 for compressive text summarization .,"TOPIARY
for
compressive text summarization","TOPIARY||for||compressive text summarization
",,,"Baselines||has||TOPIARY
",,,,,
baselines,It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization .,"combines
system using linguistic based transformations
an unsupervised topic detection algorithm",,,,,,"compressive text summarization||combines||system using linguistic based transformations
compressive text summarization||combines||an unsupervised topic detection algorithm
",,,
baselines,MOSES + uses a phrasebased statistical machine translation system trained on Gigaword to produce summaries .,"MOSES +
uses
phrasebased statistical machine translation system
trained on
Gigaword
to produce
summaries","MOSES +||uses||phrasebased statistical machine translation system
phrasebased statistical machine translation system||trained on||Gigaword
Gigaword||to produce||summaries
",,,"Baselines||has||MOSES +
",,,,,
baselines,ABS and ABS + are both the neural network based models with local attention modeling for abstractive sentence summarization .,"ABS and ABS +
with
local attention modeling
for
abstractive sentence summarization","ABS and ABS +||with||local attention modeling
local attention modeling||for||abstractive sentence summarization
",,,"Baselines||has||ABS and ABS +
",,,,,
baselines,"ABS + is trained on the Gigaword corpus , but combined with an additional log - linear extractive summarization model with handcrafted features .","ABS +
trained on
Gigaword corpus
combined with
additional log - linear extractive summarization model
with
handcrafted features","ABS +||combined with||additional log - linear extractive summarization model
additional log - linear extractive summarization model||with||handcrafted features
ABS +||trained on||Gigaword corpus
",,,"Baselines||has||ABS +
",,,,,
baselines,RNN and RNN - context are two seq2seq architectures .,"RNN and RNN - context
are
two seq2seq architectures","RNN and RNN - context||are||two seq2seq architectures
",,,"Baselines||has||RNN and RNN - context
",,,,,
baselines,Copy Net integrates a copying mechanism into the sequence - to sequence framework .,"Copy Net
integrates
copying mechanism
into
sequence - to sequence framework","Copy Net||integrates||copying mechanism
copying mechanism||into||sequence - to sequence framework
",,,"Baselines||has||Copy Net
",,,,,
baselines,RNN - distract uses a new attention mechanism by distracting the historical attention in the decoding steps .,"RNN - distract
uses
new attention mechanism
by distracting
historical attention
in
decoding steps","RNN - distract||uses||new attention mechanism
new attention mechanism||by distracting||historical attention
historical attention||in||decoding steps
",,,"Baselines||has||RNN - distract
",,,,,
baselines,RAS - LSTM and RAS - Elman both consider words and word positions as input and use convolutional encoders to handle the source information .,"RAS - LSTM and RAS - Elman
consider
words and word positions
as
input
use
convolutional encoders
to handle
source information","RAS - LSTM and RAS - Elman||use||convolutional encoders
convolutional encoders||to handle||source information
RAS - LSTM and RAS - Elman||consider||words and word positions
words and word positions||as||input
",,,"Baselines||has||RAS - LSTM and RAS - Elman
",,,,,
baselines,LenEmb uses a mechanism to control the summary length by considering the length embedding vector as the input .,"LenEmb
control
summary length
by considering
length embedding vector
as
input","LenEmb||control||summary length
summary length||by considering||length embedding vector
length embedding vector||as||input
",,,"Baselines||has||LenEmb
",,,,,
baselines,ASC+ FSC 1 ) uses a generative model with attention mechanism to conduct the sentence compression problem .,"ASC+ FSC
uses
generative model
with
attention mechanism
to conduct
sentence compression problem","ASC+ FSC||uses||generative model
generative model||with||attention mechanism
attention mechanism||to conduct||sentence compression problem
",,,"Baselines||has||ASC+ FSC
",,,,,
baselines,lvt2k - 1sent and lvt5k - 1sent utilize a trick to control the vocabulary size to improve the training efficiency .,"lvt2k - 1sent and lvt5k - 1sent
utilize
trick
to control
vocabulary size
to improve
training efficiency","lvt2k - 1sent and lvt5k - 1sent||utilize||trick
trick||to control||vocabulary size
vocabulary size||to improve||training efficiency
",,,"Baselines||has||lvt2k - 1sent and lvt5k - 1sent
",,,,,
experimental-setup,"For the experiments on the English dataset Gigawords , we set the dimension of word embeddings to 300 , and the dimension of hidden states and latent variables to 500 .","on
English dataset Gigawords
set
dimension
of
word embeddings
to
300
of
hidden states and latent variables
to
500","hidden states and latent variables||to||500
English dataset Gigawords||set||dimension
dimension||of||hidden states and latent variables
hidden states and latent variables||to||500
dimension||of||word embeddings
word embeddings||to||300
",,"Experimental setup||on||English dataset Gigawords
",,,,,,"English dataset Gigawords||has||batch size
"
experimental-setup,The maximum length of documents and summaries is 100 and 50 respectively .,"maximum length of
documents and summaries
is
100 and 50","documents and summaries||is||100 and 50
",,,,,"English dataset Gigawords||maximum length of||documents and summaries
",,,
experimental-setup,The batch size of mini-batch training is 256 .,"batch size
mini-batch training
is
256","mini-batch training||is||256
","batch size||of||mini-batch training
",,,,,,,
experimental-setup,"For DUC - 2004 , the maximum length of summaries is 75 bytes .","For
DUC - 2004
maximum length of
summaries
is
75 bytes","DUC - 2004||maximum length of||summaries
summaries||is||75 bytes
",,"Experimental setup||For||DUC - 2004
",,,,,,
experimental-setup,"For the dataset of LCSTS , the dimension of word embeddings is 350 .","dataset of LCSTS
dimension of
word embeddings
is
350","dataset of LCSTS||dimension of||word embeddings
word embeddings||is||350
",,,"Experimental setup||For||dataset of LCSTS
",,,,,
experimental-setup,We also set the dimension of hidden states and latent variables to 500 .,"hidden states and latent variables
to
500",,,,,,,,"dataset of LCSTS||dimension of||hidden states and latent variables
",
experimental-setup,"The maximum length of documents and summaries is 120 and 25 respectively , and the batch size is also 256 .","maximum length of
documents and summaries
is
120 and 25","documents and summaries||is||120 and 25
",,,,,"dataset of LCSTS||maximum length of||documents and summaries
",,,
experimental-setup,The beam size of the decoder was set to be 10 .,"beam size of
decoder
set to
10","decoder||set to||10
",,"Experimental setup||beam size of||decoder
",,,,,,
experimental-setup,Adadelta with hyperparameter ? = 0.95 and = 1 e ? 6 is used for gradient based optimization .,"Adadelta
with
hyperparameter ? = 0.95
used for
gradient based optimization","Adadelta||with||hyperparameter ? = 0.95
hyperparameter ? = 0.95||used for||gradient based optimization
",,,"Experimental setup||has||Adadelta
",,,,,
experimental-setup,"Our neural network based framework is implemented using Theano ( Theano Development Team , 2016 ) .","neural network based framework
implemented using
Theano","neural network based framework||implemented using||Theano
",,,"Experimental setup||has||neural network based framework
",,,,,
results,ROUGE Evaluation,ROUGE Evaluation,,,,"Results||has||ROUGE Evaluation
",,,,,
results,The results on the Chinese dataset LCSTS are shown in .,"on
Chinese dataset LCSTS",,,,,,"ROUGE Evaluation||on||Chinese dataset LCSTS
",,,"Chinese dataset LCSTS||has||Our model DRGD
"
results,Our model DRGD also achieves the best performance .,"Our model DRGD
achieves
best performance","Our model DRGD||achieves||best performance
",,,,,,,,
research-problem,Cutting - off Redundant Repeating Generations for Neural Abstractive Summarization,Neural Abstractive Summarization,,,,,"Contribution||has research problem||Neural Abstractive Summarization
",,,,
research-problem,"The RNN - based encoder - decoder ( EncDec ) approach has recently been providing significant progress in various natural language generation ( NLG ) tasks , i.e. , machine translation ( MT ) and abstractive summarization ( ABS ) .",abstractive summarization ( ABS ),,,,,"Contribution||has research problem||abstractive summarization ( ABS )
",,,,
model,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,"of
jointly estimate
upper-bound frequency
each target vocabulary
that can occur in
summary
during
encoding process
control
output words
in
each decoding step","upper-bound frequency||of||each target vocabulary
each target vocabulary||that can occur in||summary
summary||during||encoding process
upper-bound frequency||control||output words
output words||in||each decoding step
",,"Model||jointly estimate||upper-bound frequency
",,,,,,
model,We refer to our additional component as a wordfrequency estimation ( WFE ) sub-model .,"additional component
wordfrequency estimation ( WFE ) sub-model",,,"Model||additional component||wordfrequency estimation ( WFE ) sub-model
",,,,,,
model,The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process .,"WFE sub-model
explicitly manages
how many times each word has been generated so far
might be generated in the future","WFE sub-model||explicitly manages||how many times each word has been generated so far
WFE sub-model||explicitly manages||might be generated in the future
",,,"Model||has||WFE sub-model
",,,,,
research-problem,Bottom - Up Abstractive Summarization,Abstractive Summarization,,,,,"Contribution||has research problem||Abstractive Summarization
",,,,
research-problem,Text summarization systems aim to generate natural language summaries that compress the information in a longer text .,Text summarization,,,,,"Contribution||has research problem||Text summarization
",,,,
research-problem,Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .,neural abstractive summarization,,,,,"Contribution||has research problem||neural abstractive summarization
",,,,
approach,"Motivated by this approach , we consider bottom - up attention for neural abstractive summarization .","consider
bottom - up attention",,,"Approach||consider||bottom - up attention
",,,,,,
approach,Our approach first selects a selection mask for the source document and then constrains a standard neural model by this mask .,"first selects
selection mask
for
source document
constrains a standard neural model
by this mask","selection mask||by this mask||constrains a standard neural model
selection mask||for||source document
",,"Approach||first selects||selection mask
",,,,,,
approach,Our full model incorporates a separate content selection system to decide on relevant aspects of the source document .,"incorporates
separate content selection system
to decide
relevant aspects
of
source document","separate content selection system||to decide||relevant aspects
relevant aspects||of||source document
",,"Approach||incorporates||separate content selection system
",,,,,,
approach,"We frame this selection task as a sequence - tagging problem , with the objective of identifying tokens from a document that are part of its summary .","frame
selection task
as a
sequence - tagging problem
with the objective of
identifying tokens
from
document
part of
summary","selection task||as a||sequence - tagging problem
sequence - tagging problem||with the objective of||identifying tokens
identifying tokens||from||document
identifying tokens||part of||summary
",,"Approach||frame||selection task
",,,,,,
approach,"To incorporate bottom - up attention into abstractive summarization models , we employ masking to constrain copying words to the selected parts of the text , which produces grammatical outputs .","employ
masking
to constrain
copying words
to
selected parts
of
text
produces
grammatical outputs","masking||to constrain||copying words
copying words||to||selected parts
selected parts||of||text
masking||produces||grammatical outputs
",,"Approach||employ||masking
",,,,,,
experimental-setup,All inference parameters are tuned on a 200 example subset of the validation set .,"inference parameters
tuned on
200 example subset","inference parameters||tuned on||200 example subset
",,,"Experimental setup||has||inference parameters
",,,,,
experimental-setup,"Length penalty parameter ? and copy mask differ across models , with ? ranging from 0.6 to 1.4 , and ranging from 0.1 to 0.2 .","Length penalty parameter
copy mask
ranging from
0.6 to 1.4
ranging from
0.1 to 0.2","Length penalty parameter||ranging from||0.6 to 1.4
copy mask||ranging from||0.1 to 0.2
",,,"Experimental setup||has||Length penalty parameter
Experimental setup||has||copy mask
",,,,,
experimental-setup,The minimum length of the generated summary is set to 35 for CNN - DM and 6 for NYT .,"minimum length of
generated summary
is set to
35
for
CNN - DM
6
for
NYT","generated summary||is set to||35
35||for||CNN - DM
generated summary||is set to||6
6||for||NYT
",,"Experimental setup||minimum length of||generated summary
",,,,,,
experimental-setup,"The coverage penalty parameter ? is set to 10 , and the copy attention normalization parameter ? to 2 for both approaches .","coverage penalty parameter
set to
10
copy attention normalization parameter
to
2","copy attention normalization parameter||to||2
coverage penalty parameter||set to||10
",,,"Experimental setup||has||copy attention normalization parameter
Experimental setup||has||coverage penalty parameter
",,,,,
experimental-setup,"We use AllenNLP for the content selector , and Open NMT - py for the abstractive models .","use
AllenNLP
for
content selector
Open NMT - py
for
abstractive models","AllenNLP||for||content selector
Open NMT - py||for||abstractive models
",,"Experimental setup||use||AllenNLP
Experimental setup||use||Open NMT - py
",,,,,,
results,"3 . shows our main results on the CNN - DM corpus , with abstractive models shown in the top , and bottom - up attention methods at the bottom .","on
CNN - DM corpus",,,"Results||on||CNN - DM corpus
",,,,,,
results,"We first observe that using a coverage inference penalty scores the same as a full coverage mechanism , without requiring any additional model parameters or model fine - tuning .","using
coverage inference penalty
scores
the same
as a
full coverage mechanism
without requiring
additional model parameters
model fine - tuning","coverage inference penalty||scores||the same
the same||as a||full coverage mechanism
full coverage mechanism||without requiring||additional model parameters
full coverage mechanism||without requiring||model fine - tuning
",,,,,"CNN - DM corpus||using||coverage inference penalty
",,,
results,"The results with the CopyTransformer and coverage penalty indicate a slight improvement across all three scores , but we observe no significant difference between Pointer - Generator and CopyTransformer with bottom - up attention .","with
CopyTransformer and coverage penalty
indicate
slight improvement
across
all three scores
observe
no significant difference
between
Pointer - Generator and CopyTransformer with bottom - up attention","CopyTransformer and coverage penalty||indicate||slight improvement
slight improvement||across||all three scores
no significant difference||between||Pointer - Generator and CopyTransformer with bottom - up attention
",,"Results||with||CopyTransformer and coverage penalty
Results||observe||no significant difference
",,,,,,
research-problem,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,Abstractive Sentence Summarization,,,,,"Contribution||has research problem||Abstractive Sentence Summarization
",,,,
research-problem,Generating a condensed version of a passage while preserving its meaning is known as text summarization .,text summarization,,,,,"Contribution||has research problem||text summarization
",,,,
model,"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .","consists of
conditional recurrent neural network
acts as
decoder
to generate
summary
of
input sentence","conditional recurrent neural network||acts as||decoder
decoder||to generate||summary
summary||of||input sentence
",,"Model||consists of||conditional recurrent neural network
",,,,,,
model,"In addition , at every time step the decoder also takes a conditioning input which is the output of an encoder module .","at
every time step
decoder
takes
conditioning input
output of
encoder module","decoder||takes||conditioning input
conditioning input||at||every time step
conditioning input||output of||encoder module
",,,"Model||has||decoder
",,,,,
model,"Depending on the current state of the RNN , the encoder computes scores over the words in the input sentence .","encoder
computes scores over
words
in
input sentence","encoder||computes scores over||words
words||in||input sentence
",,,"Model||has||encoder
",,,,,
model,Both the decoder and encoder are jointly trained on a data set consisting of sentence - summary pairs .,"decoder and encoder
jointly trained on
data set
consisting of
sentence - summary pairs","decoder and encoder||jointly trained on||data set
data set||consisting of||sentence - summary pairs
",,,"Model||has||decoder and encoder
",,,,,
model,"Lastly , our encoder uses a convolutional network to encode input words .","uses
convolutional network
to encode
input words","convolutional network||to encode||input words
",,,,,"encoder||uses||convolutional network
",,,
experimental-setup,We implemented our models in the Torch library ( http://torch.ch/),"implemented
our models
in
Torch library","our models||in||Torch library
",,"Experimental setup||implemented||our models
",,,,,,
experimental-setup,2 . To optimize our loss ( Equation 5 ) we used stochastic gradient descent with mini-batches of size 32 .,"optimize
our loss
used
stochastic gradient descent
with
mini-batches
of size
32","our loss||used||stochastic gradient descent
stochastic gradient descent||with||mini-batches
mini-batches||of size||32
",,"Experimental setup||optimize||our loss
",,,,,,
experimental-setup,"During training we measure the perplexity of the summaries in the validation set and adjust our hyper - parameters , such as the learning rate , based on this number .","During
training
measure
perplexity
of
summaries
in
validation set
adjust
hyper - parameters
such as
learning rate","training||measure||perplexity
perplexity||of||summaries
summaries||in||validation set
training||adjust||hyper - parameters
hyper - parameters||such as||learning rate
",,"Experimental setup||During||training
",,,,,,
experimental-setup,For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,"For
decoder
experimented with
Elman RNN
Long - Short Term Memory ( LSTM ) architecture","decoder||experimented with||Elman RNN
decoder||experimented with||Long - Short Term Memory ( LSTM ) architecture
",,"Experimental setup||For||decoder
",,,,,,
experimental-setup,We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .,"chose
hyper - parameters
based on
grid search
picked the one which gave
best perplexity
on
validation set","hyper - parameters||based on||grid search
hyper - parameters||picked the one which gave||best perplexity
best perplexity||on||validation set
",,"Experimental setup||chose||hyper - parameters
",,,,,,
,"Our final Elman architecture ( RAS - Elman ) uses a single layer with H = 512 , ? = 0.5 , ? = 2 , and ? = 10 . ","final Elman architecture ( RAS - Elman )
uses
single layer
with
H = 512 , ? = 0.5 , ? = 2 , and ? = 10",,,,,,,,,
experimental-setup,"The LSTM model ( RAS - LSTM ) also has a single layer with H = 512 , ? = 0.1 , ? = 2 , and ? = 10 .","LSTM model ( RAS - LSTM )
single layer
with
H = 512 , ? = 0.1 , ? = 2 , and ? = 10","single layer||with||H = 512 , ? = 0.1 , ? = 2 , and ? = 10
","LSTM model ( RAS - LSTM )||has||single layer
",,"Experimental setup||has||LSTM model ( RAS - LSTM )
",,,,,
results,shows that both our RAS - Elman and RAS - LSTM models achieve lower perplexity than ABS as well as other models reported in .,"shows
RAS - Elman and RAS - LSTM models
achieve
lower perplexity
than
ABS","RAS - Elman and RAS - LSTM models||achieve||lower perplexity
lower perplexity||than||ABS
",,"Results||shows||RAS - Elman and RAS - LSTM models
",,,,,,
results,"The RAS - LSTM performs slightly worse than RAS - Elman , most likely due to over-fitting .","RAS - LSTM
performs slightly worse than
RAS - Elman","RAS - LSTM||performs slightly worse than||RAS - Elman
",,,"Results||has||RAS - LSTM
",,,,,
results,The ROUGE results show that our models comfortably outperform both ABS and ABS + by a wide margin on all metrics .,"ROUGE results
show
our models
comfortably outperform
ABS and ABS +
by
wide margin
on
all metrics","ROUGE results||show||our models
our models||comfortably outperform||ABS and ABS +
ABS and ABS +||by||wide margin
wide margin||on||all metrics
",,,"Results||has||ROUGE results
",,,,,
results,On DUC - 2004 we report recall ROUGE as is customary on this dataset .,"On
DUC - 2004",,,"Results||On||DUC - 2004
",,,,,,
results,The results ( Table 3 ) show that our models are better than ABS + .,"show
our models
better than
ABS +","our models||better than||ABS +
",,,,,"DUC - 2004||show||our models
",,,
research-problem,Learning document embeddings along with their uncertainties,Learning document embeddings,,,,,"Contribution||has research problem||Learning document embeddings
",,,,
research-problem,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,topic identification,,,,,"Contribution||has research problem||topic identification
",,,,
research-problem,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,,,,,,,,,,
research-problem,"L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -.",L EARNING word and document embeddings,,,,,"Contribution||has research problem||L EARNING word and document embeddings
",,,,
model,"In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a generative model for bag - ofwords representation of documents .","present
Bayesian subspace multinomial model ( Bayesian SMM )
as a
generative model
for
bag - ofwords representation of documents","Bayesian subspace multinomial model ( Bayesian SMM )||as a||generative model
generative model||for||bag - ofwords representation of documents
",,"Model||present||Bayesian subspace multinomial model ( Bayesian SMM )
",,,,,,
model,"We show that our model can learn to represent each document in the form of a Gaussian distribution , thereby encoding the uncertainty in its covariance .","learn to represent
each document
in the form of
Gaussian distribution
encoding
uncertainty in its covariance","each document||in the form of||Gaussian distribution
Gaussian distribution||encoding||uncertainty in its covariance
",,"Model||learn to represent||each document
",,,,,,
model,"Further , we propose a generative Gaussian classifier that exploits this uncertainty for topic identification ( ID ) .","propose
generative Gaussian classifier
exploits
uncertainty
for
topic identification ( ID )","generative Gaussian classifier||exploits||uncertainty
uncertainty||for||topic identification ( ID )
",,"Model||propose||generative Gaussian classifier
",,,,,,
model,"The proposed VB framework can be extended in a straightforward way for subspace n-gram model , that can model n-gram distribution of words in sentences .","proposed VB framework
extended
for
subspace n-gram model
can model
n-gram distribution of words in sentences","proposed VB framework||for||subspace n-gram model
subspace n-gram model||can model||n-gram distribution of words in sentences
",,"Model||extended||proposed VB framework
",,,,,,
hyperparameters,"The embedding dimension was chosen from K = { 100 , . . . , 800 } , and regularization weight from ? = { 0.0001 , . . . , 10.0 }.","embedding dimension
chosen from
K = { 100 , . . . , 800 }
regularization weight
from","embedding dimension||chosen from||K = { 100 , . . . , 800 }
",,,"Hyperparameters||has||regularization weight
Hyperparameters||has||embedding dimension
",,,"regularization weight||from||? = { 0.0001 , . . . , 10.0 }
",,
baselines,"1 ) NVDM : Since NVDM and our proposed Bayesian SMM share similarities , we chose to extract the embeddings from NVDM and use them for training linear classifiers .","NVDM
extract
embeddings from NVDM
use them for
training linear classifiers","NVDM||extract||embeddings from NVDM
embeddings from NVDM||use them for||training linear classifiers
",,,"Baselines||has||NVDM
",,,,,
baselines,"2 ) SMM : Our second baseline system is non-Bayesian SMM with 1 regularization over the rows in T matrix , i.e. , 1 SMM .","SMM
is
non-Bayesian SMM with 1 regularization over the rows in T matrix","SMM||is||non-Bayesian SMM with 1 regularization over the rows in T matrix
",,,"Baselines||has||SMM
",,,,,
baselines,3 ) ULMFiT : The third baseline system is the universal language model fine - tuned for classification ( ULMFiT ) .,"ULMFiT
is
universal language model
fine - tuned for
classification","ULMFiT||is||universal language model
universal language model||fine - tuned for||classification
",,,"Baselines||has||ULMFiT
",,,,,
baselines,4 ) TF - IDF :,TF - IDF,,,,"Baselines||has||TF - IDF
",,,,,
baselines,"The fourth baseline system is a standard term frequency - inverse document frequency ( TF - IDF ) based document representation , followed by multi-class logistic regression ( LR ) .","is a
standard term frequency - inverse document frequency ( TF - IDF ) based document representation
followed by
multi-class logistic regression ( LR )","standard term frequency - inverse document frequency ( TF - IDF ) based document representation||followed by||multi-class logistic regression ( LR )
",,,,,"TF - IDF||is a||standard term frequency - inverse document frequency ( TF - IDF ) based document representation
",,,
results,"presents the classification results on Fisher speech corpora with manual and automatic transcriptions , where the first two rows are the results from earlier published works .","Fisher speech corpora
with
manual and automatic transcriptions","Fisher speech corpora||with||manual and automatic transcriptions
",,,"Results||has||Fisher speech corpora
",,,,,
results,"We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .","see that
our proposed systems
achieve
consistently better accuracies
GLCU
exploits
uncertainty in document embeddings
much lower cross - entropy
than
GLC","much lower cross - entropy||than||GLC
GLCU||exploits||uncertainty in document embeddings
our proposed systems||achieve||consistently better accuracies
","GLCU||has||much lower cross - entropy
",,,,"Fisher speech corpora||see that||GLCU
Fisher speech corpora||see that||our proposed systems
",,,
results,presents classification results on 20 Newsgroups dataset .,20 Newsgroups dataset,,,,"Results||has||20 Newsgroups dataset
",,,,,"20 Newsgroups dataset||see that||topic ID systems based on Bayesian SMM
"
results,"We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models , except for the purely discriminative CNN model .","see that
topic ID systems based on Bayesian SMM and logistic regression
better than
all the other models
except for
purely discriminative CNN model","topic ID systems based on Bayesian SMM and logistic regression||better than||all the other models
all the other models||except for||purely discriminative CNN model
",,,,,"20 Newsgroups dataset||see that||topic ID systems based on Bayesian SMM and logistic regression
",,,
results,"We can also see that all the topic ID systems based on Bayesian SMM are consistently better than variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM .","topic ID systems based on Bayesian SMM
consistently better than
variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM","topic ID systems based on Bayesian SMM||consistently better than||variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM
",,,,,,,,
