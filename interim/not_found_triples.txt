# If none of the phrases in a triple can be found in any sentence in the corresponding info unit,
# and the triple does not take the form of "Contribution||has||(unit name)"
# it will be listed here.
['Experiments', 'has', 'Tasks']
['Character - Level Machine Translation', 'has', 'Results']
['Character Prediction', 'has', 'Results']
['Contribution', 'has research problem', 'Neural Machine Translation in Linear Time']
['Results', 'has', 'Tasks']
['Baselines', 'has', 'Word - by - word translation ( WBW )']
['MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )', 'has', 'Results']
['100 BILLION WORD GOOGLE NEWS CORPUS', 'has', 'Results']
['MACHINE TRANSLATION', 'has', 'Results']
['SENTIMENT ANALYSIS', 'has', 'Results']
['Experiments', 'has', 'Results']
['Experimental setup', 'varied', 'dropout']
['Experimental setup', 'varied', 'hidden units']
['Experimental setup', 'varied', 'capitalization']
['Experimental setup', 'varied', 'char ) embedding dimensions']
['Experimental setup', 'varied', 'learning rate']
['learning rate', 'has', '[ 0.001 , 0.015 ]']
['[ 0.001 , 0.015 ]', 'by', 'step 0.002']
['SQuAD v 1.1', 'has', 'Hyperparameters']
['SQuAD v 1.1', 'has', 'Results']
['SQuAD v 2.0', 'has', 'Results']
['GLUE', 'has', 'Hyperparameters']
['GLUE', 'has', 'Results']
['SWAG', 'has', 'Hyperparameters']
['SWAG', 'has', 'Results']
['Experiments', 'has', 'Tasks']
['Natural Language Inference', 'has', 'Results']
['Sentiment Analysis', 'has', 'Results']
['Model', 'has', 'retrieved supplementary texts']
['retrieved supplementary texts', 'read together with', 'task inputs']
['retrieved supplementary texts', 'by', 'initial reading module']
['initial reading module', 'whose', 'outputs']
['Experiments', 'on', 'Tasks']
['Tasks', 'on', 'paraphrase identification task']
['paraphrase identification task', 'has', 'Baselines']
['halved', 'whenever meeting', 'bad iteration']
['Answer Sentence Selection', 'has', 'Results']
['Experimental setup', 'has', 'dropout probability']
['dropout probability', 'set to', '0.1']
['Contribution', 'Code', 'https://github.com/Helsinki-NLP/HBMP']
['SQUAD', 'has', 'Results']
['Experiments', 'has', 'Tasks']
['QUESTION ANSWERING', 'has', 'Results']
['ASSOCIATIVE RECALL TASK', 'has', 'Results']
['Experimental Setup', 'use', 'Adam optimizer ( Kingma and Ba , 2015 )']
['Experimental Setup', 'use', 'exponentially decaying learning rate']
['exponentially decaying learning rate', 'with', 'linear warmup']
['Contribution', 'Code', 'https://github.com/momohuang/FlowQA']
['Experiments', 'has', 'Tasks']
['Machine Reading Comprehension', 'has', 'Results']
['Experiments', 'has', 'Experimental setup']
['Experiments', 'has', 'Tasks']
['Experiments', 'has', 'Baselines']
['Contribution', 'Code', 'https://worksheets. codalab.org/worksheets/ 0x91d77db37e0a4bbbaeb37b8972f4784f/']
['Experiments', 'has', 'Results']
['Experiments', 'has', 'Baselines']
['Experiments', 'has', 'Tasks']
['Trivia QA and SQuAD - Open', 'has', 'Results']
['Experiments', 'has', 'Tasks']
['Experiments', 'has', 'Tasks']
['Predictive Performance', 'has', 'Results']
['Experiments', 'has', 'Tasks']
['Experiments', 'has', 'Tasks']
['HYPERNYM PREDICTION', 'has', 'Results']
['Model', 'exploit', 'partial order structure']
['partial order structure', 'by learning', 'mapping']
['mapping', 'not', 'distance - preserving']
['distance - preserving', 'but', 'order - preserving']
['mapping', 'between', 'visualsemantic hierarchy']
['mapping', 'between', 'partial order']
['partial order', 'over', 'embedding space']
['partial order structure', 'of', 'visual - semantic hierarchy']
['Experiments', 'has', 'Tasks']
['Graph Reachability', 'has', 'Results']
['CNN and Daily Mail Datasets', 'has', 'Results']
['SQuAD Dataset', 'has', 'Results']
['Results', 'has', 'QUASAR - T']
['Model', 'refer to', 'match - LSTM']
['Model', 'refer to', 'm LSTM']
['m LSTM', 'for', 'short']
['Model', 'adopt', 'match - LSTM model']
['Model', 'adopt', 'Pointer Net ( Ptr - Net ) model']
['Pointer Net ( Ptr - Net ) model', 'enables', 'predictions']
['predictions', 'of', 'tokens']
['tokens', 'from', 'input sequence only']
['Experiments', 'has', 'Tasks']
['Experiments', 'has', 'Tasks']
['Tasks', 'has', 'Natural Language Inference']
['Natural Language Inference', 'has', 'Results']
['Experimental setup', 'For', 'DMP task']
['Model', 'has', 'several new novel components']
['Experiments', 'has', 'Tasks']
['MULTI - EVIDENCE QUESTION ANSWERING', 'has', 'Results']
['Experiments', 'has', 'Tasks']
['Experiments', 'has', 'Tasks']
['Natural Language Inference', 'has', 'Results']
['Natural Language Inference', 'has', 'Baselines']
['Language Modeling', 'has', 'Results']
['Language Modeling', 'has', 'Baselines']
['Sentiment Analysis', 'has', 'Hyperparameters']
['Sentiment Analysis', 'has', 'Results']
['Model', 'Given', 'two sentences']
['Experiments', 'has', 'Tasks']
['Multi-mention Reading Comprehension', 'has', 'Results']
['TRANSFER LEARNING PERFORMANCE', 'has', 'Results']
['COMPARISON WITH STATE - OF - THE - ART RESULTS', 'has', 'Results']
['Experiments', 'has', 'TREC - CAR']
['Transfer learning on Reverb', 'best results', '67 % accuracy']
['Transfer learning on Reverb', 'best results', '68 %']
['68 %', 'for', 'ensemble of 5 models']
['Tasks', 'Answer Sentence Selection', 'Results']
['Tasks', 'Natural Language Inference', 'Results']
['Tasks', 'Sentence Classification', 'Results']
['Contribution', 'Code', 'https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering']
['MovieQA dataset', 'has', 'Results']
['Memex QA', 'has', 'Experimental setup']
['Memex QA', 'has', 'Results']
['Memex QA', 'has', 'Ablation analysis']
['Contribution', 'Code', 'https://github.com/shuohangwang/mprc']
['Approach', 'has', 'End - to - end Model']
['Contribution', 'Code', 'https://github.com/allenai/scibert/']
['Tasks', 'has', 'Dependency Parsing']
['Dependency Parsing', 'name', 'DEP']
['Results', 'has', 'Baselines']
['Baselines', 'perform', 'reasonably well']
['Baselines', 'perform', 'much better']
['much better', 'than', 'random baseline']
['Contribution', 'Code', 'https :// github.com/donglixp/coarse2fine']
['Results', 'has', 'best results']
['three filters', 'with', 'different window sizes h = 3 , 4 , 5']
['different window sizes h = 3 , 4 , 5', 'with', '100 feature maps each']
['in - domain setting', 'has', 'our method']
['Hyperparameters', 'has', 'weights']
['weights', 'of', 'hidden units']
['hidden units', 'initialized using', 'orthogonal']
['Contribution', 'Code', 'https :// github.com/stanfordnlp/treelstm']
['Model', 'call', 'Suffix BiLSTM']
['Model', 'call', 'SuBiLSTM']
['SuBiLSTM', 'in', 'short']
['Contribution', 'Code', 'https://github.com/tanthongtan/dv-cosine']
['Contribution', 'Code', 'https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis']
['Results', 'has', 'ACSA']
['Baselines', 'has', 'c LSTM']
['Baselines', 'has', 'LSTM']
['Experiments', 'has', 'Tasks']
['subject - dependent experiment', 'has', 'Results']
['subject - independent experiment', 'has', 'Results']
['Contribution', 'Code', 'https://github.com/ruidan/Aspect-level-sentiment']
['Sentiment analysis', 'has', 'Results']
['Approach', 'focus on the role of', 'preprocessing the input text']
['Model', 'has', 'Transformer']
['Model', 'has', 'capsule network']
['Hyperparameters', 'use', 'dropout']
['dropout', 'on', 'all nonlinear connections']
['all nonlinear connections', 'with dropout rate', '0.5']
['Experiments', 'has', 'Experimental setup']
['Experiments', 'has', 'Model']
['Hyperparameters', 'in', 'our synthetic data experiments']
['our synthetic data experiments', 'has', 'kernel size']
['kernel size', 'from', '1 to T']
['our synthetic data experiments', 'has', 'number']
['number', 'of', 'each kernel size']
['each kernel size', 'between', '100 to 200']
['Experimental setup', 'use', 'Adam ( Kingma and Ba , 2015 ) optimizer']
['Adam ( Kingma and Ba , 2015 ) optimizer', 'with', 'learning rate 0.001']
['Adam ( Kingma and Ba , 2015 ) optimizer', 'with', 'momentum parmeters ? 1 = 0.9 and ? 2 = 0.999']
['Results', 'has', 'English Gigaword']
['Baselines', 'has', 'ABS +']
['in all metrics', 'except for', 'RG - 2']
['RG - 2', 'on', 'Gigaword']
['Experimental setup', 'has', 'final Elman architecture ( RAS - Elman )']
['final Elman architecture ( RAS - Elman )', 'uses', 'single layer']
['single layer', 'with', 'H = 512 , ? = 0.5 , ? = 2 , and ? = 10']
