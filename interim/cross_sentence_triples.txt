['two recurrent neural networks ( RNN )', 'trained', 'jointly']
[15, 2, 'The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .']
[17, 4, 'The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['encoder', 'maps', 'a variable - length source sequence to a fixed - length vector']
[15, 2, 'The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .']
[16, 3, 'The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['decoder', 'maps', 'the vector representation back to a variable - length target sequence']
[15, 2, 'The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .']
[16, 3, 'The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Hardware and Schedule', 'trained', 'models']
[158, 29, 'Hardware and Schedule']
[159, 30, 'We trained our models on one machine with 8 NVIDIA P100 GPUs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Optimizer', 'used', 'Adam optimizer']
[164, 33, 'Optimizer']
[165, 34, 'We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Regularization', 'has', 'Residual Dropout']
[169, 36, 'Regularization']
[171, 37, 'Residual Dropout']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Residual Dropout', 'apply', 'dropout']
[171, 37, 'Residual Dropout']
[172, 38, 'We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['dropout', 'to', 'sums of the embeddings and the positional encodings']
[172, 38, 'We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .']
[173, 39, 'In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Label Smoothing', 'employed', 'value ls = 0.1']
[175, 40, 'Label Smoothing']
[176, 41, 'During training , we employed label smoothing of value ls = 0.1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Encoder and Decoder Stacks', 'has', 'Encoder']
[47, 3, 'Encoder and Decoder Stacks']
[48, 4, 'Encoder :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Encoder', 'composed of', 'stack of N = 6 identical layers']
[48, 4, 'Encoder :']
[49, 5, 'The encoder is composed of a stack of N = 6 identical layers .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['stack of N = 6 identical layers', 'has', 'two sub-layers']
[49, 5, 'The encoder is composed of a stack of N = 6 identical layers .']
[50, 6, 'Each layer has two sub-layers .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two sub-layers', 'first', 'multi-head self - attention mechanism']
[50, 6, 'Each layer has two sub-layers .']
[51, 7, 'The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two sub-layers', 'second', 'simple , positionwise fully connected feed - forward network']
[50, 6, 'Each layer has two sub-layers .']
[51, 7, 'The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Decoder', 'composed of', 'stack of N = 6 identical layers']
[55, 9, 'Decoder :']
[56, 10, 'The decoder is also composed of a stack of N = 6 identical layers .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Attention', 'described as', 'mapping a query and a set of key - value pairs']
[61, 14, 'Attention']
[62, 15, 'An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['output', 'computed as', 'weighted sum of the values']
[62, 15, 'An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .']
[63, 16, 'The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['weighted sum of the values', 'where', 'weight assigned to each value']
[62, 15, 'An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .']
[63, 16, 'The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Position - wise Feed - Forward Networks', 'contains', 'fully connected feed - forward network']
[100, 19, 'Position - wise Feed - Forward Networks']
[101, 20, 'In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['each position separately and identically', 'consists', 'two linear transformations']
[101, 20, 'In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .']
[102, 21, 'This consists of two linear transformations with a ReLU activation in between .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Embeddings and Softmax', 'use', 'learned embeddings']
[107, 22, 'Embeddings and Softmax']
[108, 23, 'Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Positional Encoding', 'inject', 'some information']
[112, 25, 'Positional Encoding']
[113, 26, 'Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['relative or absolute position', 'of', 'tokens in the sequence']
[113, 26, 'Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .']
[116, 27, 'tokens in the sequence .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Machine Translation', 'On', 'WMT 2014 English - to - German translation task']
[179, 42, 'Machine Translation']
[180, 43, 'On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing a new state - of - the - art BLEU score of 28.4 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['English Constituency Parsing', 'performs', 'surprisingly well']
[203, 45, 'English Constituency Parsing']
[213, 46, 'Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['a new type of linear connections', 'called', 'fast - forward connections']
[30, 1, 'In this work , we introduce a new type of linear connections for multi - layer recurrent networks .']
[31, 2, 'These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Single models', 'has', 'English - to - French']
[236, 10, 'Single models']
[237, 11, 'English - to - French : First we list our single model results on the English - to - French task in Tab .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['English - to - French', 'From', 'Deep - ED']
[237, 11, 'English - to - French : First we list our single model results on the English - to - French task in Tab .']
[240, 12, 'From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Number of weight - sharing layers', 'achieved', 'best translation performance']
[191, 16, 'Number of weight - sharing layers']
[198, 17, 'And the best translation performance is achieved when only one layer is shared in our system .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Constrained English - Estonian and Estonian - English NMT systems', 'trained using', 'two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data']
[31, 10, 'Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .']
[32, 11, 'The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Unconstrained English - Estonian and Estonian - English NMT systems', 'trained using', 'back - translated data']
[28, 7, 'Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .']
[29, 8, 'These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Text Classification', 'implement', 'Recurrent CNN - based model']
[189, 19, 'Text Classification is a conventional machine learning task and is evaluated by accuracy .']
[190, 20, "Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Text Classification', 'test it on', "AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG )"]
[189, 19, 'Text Classification is a conventional machine learning task and is evaluated by accuracy .']
[190, 20, "Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Machine Translation', 'choose', 'two datasets : WMT14 English - German and IWSLT14 German - English datasets']
[185, 16, 'Machine Translation is a popular task in both deep learning and natural language processing .']
[186, 17, 'We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Language Modeling', 'goal', 'predict the next word']
[181, 12, 'Language Modeling is a basic task in natural language processing .']
[182, 13, 'The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Language Modeling', 'evaluated by', 'perplexity']
[181, 12, 'Language Modeling is a basic task in natural language processing .']
[182, 13, 'The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Word Similarity', 'use', 'skip - gram model']
[176, 9, "Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation ."]
[177, 10, 'We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Text Classification', 'outperforms', 'baseline']
[226, 29, 'Text Classification']
[228, 30, 'Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Machine Translation', 'outperform', 'baselines']
[220, 26, 'Machine Translation']
[222, 27, 'We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Language Modeling', 'outperforms', 'two baselines']
[213, 21, 'Language Modeling']
[216, 22, 'In all these settings , our method outperforms the two baselines .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two types of models', 'first one', 'RNN Encoder - Decoder']
[117, 6, 'We train two types of models .']
[118, 7, 'The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two types of models', 'other', 'RNNsearch']
[117, 6, 'We train two types of models .']
[118, 7, 'The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Results', 'Speed Comparisons', 'Speed vs. increasing N']
[209, 8, 'Speed Comparisons']
[214, 9, 'Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Speed vs. increasing N', 'curve', 'surprisingly lower and highly stable']
[214, 9, 'Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .']
[220, 10, 'Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Speed vs. increasing N', 'over growing N', 'almost no rise of selection time']
[214, 9, 'Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .']
[220, 10, 'Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Speed with fixed N', 'is', 'fastest algorithm']
[225, 11, 'Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .']
[227, 12, 'First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Prediction accuracies vs. increasing K', 'K increases', 'prediction accuracies generally increase']
[251, 16, 'Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .']
[255, 17, 'As we shall see , the prediction accuracies generally increase as K increases .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Results', 'Prediction Accuracy', 'Accuracy comparison']
[240, 13, 'Prediction Accuracy']
[241, 14, 'Accuracy comparison']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Accuracy comparison', 'achieve', 'appreciable advantage']
[241, 14, 'Accuracy comparison']
[248, 15, 'Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LSTM - CRF model', 'dropout rate', '0.5']
[159, 5, 'Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .']
[161, 6, 'We set the dropout rate to 0.5 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['German , Dutch and Spanish', 'has', 'LSTM - CRF model']
[182, 11, 'and 4 present our results on NER for German , Dutch and Spanish respectively in comparison to other models .']
[183, 12, 'On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['all previous methods', 'exception', 'Dutch']
[183, 12, 'On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .']
[184, 13, 'The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['token - wise representations', 'prevents', 'overfitting']
[32, 6, 'Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .']
[33, 7, 'This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['token - wise representations', 'provides opportunities to', 'inject supervision on intermediate activations of the network']
[32, 6, 'Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .']
[33, 7, 'This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['OntoNotes 5.0 English NER', 'has', 'greedy model']
[193, 17, 'OntoNotes 5.0 English NER']
[199, 18, 'Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Document - level prediction', 'on', 'CoNLL - 2003']
[185, 15, 'Document - level prediction']
[186, 16, 'In we show that adding document - level context improves every model on CoNLL - 2003 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Sentence - level prediction', 'has', 'Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN']
[168, 11, '6.3 CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .']
[170, 12, 'The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CoNLL 2003 NER', 'Hyperparameters', 'two bidirectional GRUs']
[74, 4, 'CoNLL 2003 NER .']
[79, 5, 'We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CoNLL 2000 chunking', 'Hyperparameters', 'baseline sequence tagger']
[82, 8, 'CoNLL 2000 chunking .']
[86, 9, 'The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Coreference resolution', 'with', 'OntoNotes coreference annotations']
[116, 8, 'As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .']
[119, 9, 'In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Textual entailment', 'has', 'Stanford Natural Language Inference ( SNLI )']
[107, 5, 'Textual entailment is the task of determining whether a " hypothesis " is true , given a " premise " .']
[108, 6, 'The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Stanford Natural Language Inference ( SNLI )', 'improves', 'accuracy']
[108, 6, 'The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .']
[110, 7, 'Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Sequence Labelling', 'For', 'NER']
[177, 14, 'Final Results for Sequence Labelling']
[186, 15, 'For NER , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Classification', 'on', 'movie review dataset']
[164, 10, 'Final Results for Classification']
[169, 11, 'As shown in , the final results on the movie review dataset are consistent with the development results , where S - LSTM outperforms BiL - STM significantly , with a faster speed .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ONTONOTES', 'significantly outperforms', 'Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 )']
[166, 12, 'In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .']
[171, 15, 'Results on ONTONOTES']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CONLL', 'significantly outperforms', 'models']
[154, 9, 'First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .']
[170, 14, 'Results on CONLL']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['pre-trained embeddings , the characterlevel LSTM and dropout layers', 'contribute significantly to', 'effectiveness of our model']
[147, 16, 'To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .']
[149, 17, 'From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Hyperparameters', 'Pre-trained embeddings', 'Glo Ve']
[122, 10, 'Pre-trained embeddings']
[123, 11, 'Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['characterlevel LSTM', 'help deal with', 'out - of - vocabulary problem']
[24, 8, 'Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .']
[25, 9, 'Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Effect of Model Size', 'is', 'first work']
[249, 27, 'Effect of Model Size']
[259, 28, 'However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Feature - based Approach with BERT', 'demonstrates', 'BERT is effective']
[262, 29, 'Feature - based Approach with BERT']
[276, 31, 'This demonstrates that BERT is effective for both finetuning and feature - based approaches .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Feature - based Approach with BERT', 'has', 'BERT LARGE']
[262, 29, 'Feature - based Approach with BERT']
[274, 30, 'BERT LARGE performs competitively with state - of - the - art methods .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SQuAD v 1.1', 'name', 'Stanford Question Answering Dataset ( SQuAD v1.1 )']
[180, 16, 'SQuAD v 1.1']
[181, 17, 'The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['GLUE', 'name', 'General Language Understanding Evaluation ( GLUE )']
[154, 8, 'GLUE']
[155, 9, 'The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SWAG', 'name', 'Situations With Adversarial Generations']
[215, 23, 'SWAG']
[216, 24, 'The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RE', 'has', 'BioBERT v1.0 ( PubMed )']
[140, 18, 'The RE results of each model are shown in .']
[142, 19, 'On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NER', 'has', 'BioBERT']
[135, 15, 'The results of NER are shown in .']
[137, 16, 'On the other hand , BioBERT achieves higher scores than BERT on all the datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['margin', 'set to', '0.1']
[228, 9, 'The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?']
[229, 10, 'was set to 0.1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Importance of data sources', 'has', 'paraphrases']
[263, 17, 'Importance of data sources']
[265, 18, 'While paraphrases do not seem to help much on WebQuestions and SimpleQuestions , except when training only with synthetic questions , they have a dramatic impact on the performance on Reverb .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Transfer learning on Reverb', 'has', 'best results']
[255, 13, 'Transfer learning on Reverb']
[258, 14, 'Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Deep neural network with value - shared weights', 'introduce', 'novel value - shared weighting scheme']
[50, 4, 'Deep neural network with value - shared weights :']
[51, 5, 'We introduce a novel value - shared weighting scheme in deep neural networks as a counterpart of the position - shared weighting scheme in CNNs , based on the idea that semantic matching between a question and answer is mainly about the ( semantic similarity ) value regularities rather than spatial regularities .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Incorporate attention scheme over question terms', 'incorporate', 'attention scheme']
[52, 6, 'Incorporate attention scheme over question terms :']
[53, 7, 'We incorporate the attention scheme over the question terms using a gating function , so that we can explicitly discriminate the question term importance .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RTE experiments', 'introduction of', 'our refinement strategy']
[146, 11, 'shows the results of our RTE experiments .']
[147, 12, 'In general , the introduction of our refinement strategy almost always helps , both with and without external knowledge .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SNLI', 'compare to', 'recent biLSTM - Max Encoder']
[78, 21, "We also compare to the recent biLSTM - Max Encoder of , which served as our model 's 1 - layer starting point ."]
[79, 22, "The results indicate that ' Our Shortcut - Stacked Encoder ' sur-passes all the previous state - of - the - art encoders , and achieves the new best encoding - based result on SNLI , suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['recent biLSTM - Max Encoder', 'has', 'results']
[78, 21, "We also compare to the recent biLSTM - Max Encoder of , which served as our model 's 1 - layer starting point ."]
[79, 22, "The results indicate that ' Our Shortcut - Stacked Encoder ' sur-passes all the previous state - of - the - art encoders , and achieves the new best encoding - based result on SNLI , suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NarrativeQA', 'has', 'baselines']
[160, 10, 'NarrativeQA']
[162, 11, 'We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['neural bag - of - words ( summed ) representation', 're-expanded to', 'original sequence length']
[25, 5, 'Specifically , we compress the input document an arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 ) into a neural bag - of - words ( summed ) representation .']
[26, 6, 'The compact sequence is then passed through affine transformation layers and then re-expanded to the original sequence length .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['neural bag - of - words ( summed ) representation', 'passed through', 'affine transformation layers']
[25, 5, 'Specifically , we compress the input document an arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 ) into a neural bag - of - words ( summed ) representation .']
[26, 6, 'The compact sequence is then passed through affine transformation layers and then re-expanded to the original sequence length .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Narrative QA benchmark', 'observe that', '300d MRU']
[197, 23, 'are baselines reported by . reports our results on the Narrative QA benchmark .']
[198, 24, 'First , we observe that 300d MRU can achieve comparable performance with BiDAF .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['final hyperparameter grid', 'has', 'Number of epochs']
[115, 9, 'The final hyperparameter grid is as follows :']
[116, 10, 'Batch size : 16 , 32 Learning rate : 1 e - 5 , 2 e - 5 , 3 e - 5 Number of epochs : 3 , 4 , 6 In addition , we observed that in rare cases BERT fails to train ; that is , after several training epochs it has accuracy approximately equal to that of random guessing .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['final hyperparameter grid', 'has', 'Learning rate']
[115, 9, 'The final hyperparameter grid is as follows :']
[116, 10, 'Batch size : 16 , 32 Learning rate : 1 e - 5 , 2 e - 5 , 3 e - 5 Number of epochs : 3 , 4 , 6 In addition , we observed that in rare cases BERT fails to train ; that is , after several training epochs it has accuracy approximately equal to that of random guessing .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['final hyperparameter grid', 'has', 'Batch size']
[115, 9, 'The final hyperparameter grid is as follows :']
[116, 10, 'Batch size : 16 , 32 Learning rate : 1 e - 5 , 2 e - 5 , 3 e - 5 Number of epochs : 3 , 4 , 6 In addition , we observed that in rare cases BERT fails to train ; that is , after several training epochs it has accuracy approximately equal to that of random guessing .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MultiNLI dataset', 'has', 'Our plain DRCN']
[136, 23, 'shows the results on MATCHED and MISMATCHED problems of MultiNLI dataset .']
[137, 24, 'Our plain DRCN has a competitive performance without any contextualized knowledge .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Quora question pair dataset', 'obtained', 'accuracies']
[144, 27, 'Pair shows our results on the Quora question pair dataset .']
[146, 28, 'We obtained accuracies of 90.15 % and 91.30 % in single and ensemble methods , respectively , surpassing the previous state - of - the - art model of DIIN .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TrecQA and SelQA datasets', 'has', 'proposed DRCN']
[147, 29, 'TrecQA and SelQA shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question .']
[149, 30, 'However , the proposed DRCN using collective attentions over multiple layers , achieves the new state - of the - art performance , exceeding the current state - of - the - art performance significantly on both datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['answer sentence selection tasks', 'experiment on', 'two datasets']
[187, 29, 'In this Sub-section , we study the effectiveness of our model for answer sentence selection tasks .']
[189, 30, 'We experiment on two datasets : TREC - QA and WikiQA .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['dynamic - critical reinforcement learning', 'dynamically decides', 'reward and the baseline']
[29, 4, 'As for the second problem , we extend the traditional training method with a novel approach called dynamic - critical reinforcement learning .']
[30, 5, 'Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach dynamically decides the reward and the baseline according to two sampling strategies , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SQuAD', 'has', 'R.M - Reader']
[197, 12, 'We submitted our model on the hidden test set of SQuAD for evaluation .']
[199, 13, 'As shown in , R.M - Reader achieves an EM score of 79.5 % and F1 score of 86.6 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Glove 840B 300D 1 ( d e = 300 )', 'train', 'more universally usable sentence encoder']
[164, 5, 'We used the Glove 840B 300D 1 ( d e = 300 ) for the pre-trained word embedding without any finetuning .']
[165, 6, 'This is to train the more universally usable sentence encoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MultiNLI dataset', 'Compared with', 'result of RepEVAL 2017']
[185, 16, 'The results of applying SNLI best model to MultiNLI dataset without additional parameter tuning are presented in .']
[187, 17, 'Compared with the result of RepEVAL 2017 , we can see that the Distance - based Self - Attention Network performs well .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SNLI data', 'Compared with', 'existing state - of - the - art model']
[173, 12, 'Experimental results of SNLI data compared with the existing models on the SNLI leader - board 2 are shown in .']
[174, 13, 'Compared with the existing state - of - the - art model , the number of parameters and the training time increased , but our results show the new state - of - theart record .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TriviaQA dataset', 'shows', 'AMANDA']
[185, 16, 'shows the results on the TriviaQA dataset .']
[190, 17, 'shows that AMANDA achieves state - of the - art results in both Wikipedia and Web domain on distantly supervised and verified data .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Search QA dataset', 'has', 'AMANDA']
[192, 18, 'Results on the Search QA dataset are shown in .']
[199, 19, 'AMANDA outperforms both systems , especially for multi-word - answer questions by a huge margin .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['cross-entropy loss', 'has', 'minimize']
[179, 7, 'We use cross-entropy loss plus L2 regularization penalty as optimization objective .']
[180, 8, 'We minimize it by Adadelta ) ( an optimizer of mini - batch SGD ) with batch size of 64 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Natural Language Inference', 'Compared to', 'results']
[194, 18, 'Natural Language Inference']
[214, 19, 'Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Sentiment Analysis', 'has', 'DiSAN']
[225, 26, 'Sentiment Analysis']
[234, 27, 'To the best of our knowledge , DiSAN improves the last best accuracy ( given by CNN - Tensor ) by 0.52 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three components', 'has', 'sequence encoder']
[27, 4, 'Our model consists of three components :']
[28, 5, '1 ) an out - ofshelf semantic role labeler to annotate the input sentences with a variety of semantic role labels ; 2 ) an sequence encoder where a pre-trained language model is used to build representation for input raw texts and the semantic role labels are mapped to embedding in parallel ; 3 ) a semantic integration component to integrate the text representation with the contextual explicit semantic embedding to obtain the joint representation for downstream tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three components', 'has', 'an out - ofshelf semantic role labeler']
[27, 4, 'Our model consists of three components :']
[28, 5, '1 ) an out - ofshelf semantic role labeler to annotate the input sentences with a variety of semantic role labels ; 2 ) an sequence encoder where a pre-trained language model is used to build representation for input raw texts and the semantic role labels are mapped to embedding in parallel ; 3 ) a semantic integration component to integrate the text representation with the contextual explicit semantic embedding to obtain the joint representation for downstream tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three components', 'has', 'semantic integration component']
[27, 4, 'Our model consists of three components :']
[28, 5, '1 ) an out - ofshelf semantic role labeler to annotate the input sentences with a variety of semantic role labels ; 2 ) an sequence encoder where a pre-trained language model is used to build representation for input raw texts and the semantic role labels are mapped to embedding in parallel ; 3 ) a semantic integration component to integrate the text representation with the contextual explicit semantic embedding to obtain the joint representation for downstream tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['COPYING MEMORY TASK', 'has', 'RUM']
[145, 4, 'COPYING MEMORY TASK']
[150, 5, '1 . RUM utilizes a different representation of memory that outperforms those of LSTM and GRU ; 2 . RUM solves the task completely , despite its update gate , which does not allow all of the information encoded in the hidden stay to pass through .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CHARACTER LEVEL LANGUAGE MODELING', 'has', 'PENN TREEBANK CORPUS DATA SET']
[192, 15, 'CHARACTER LEVEL LANGUAGE MODELING']
[195, 16, 'PENN TREEBANK CORPUS DATA SET']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Wasserstein distance based adversarial learning', 'can help', 'our model']
[324, 25, 'Accordingly , we conclude that the performance of PAAG benefits from using Wasserstein distance based adversarial learning with gradient penalty .']
[325, 26, 'This approach can help our model to achieve a better performance than the model using the vanilla GAN architecture .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['model', 'using', 'vanilla GAN architecture']
[324, 25, 'Accordingly , we conclude that the performance of PAAG benefits from using Wasserstein distance based adversarial learning with gradient penalty .']
[325, 26, 'This approach can help our model to achieve a better performance than the model using the vanilla GAN architecture .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['beam size', 'has', '4']
[285, 15, 'To produce better answers , we use beam search with beam size']
[286, 16, '4 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Neural bag - of - words ( NBOW )', 'has', 'Each sequence']
[154, 12, 'Neural bag - of - words ( NBOW ) :']
[155, 13, 'Each sequence as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Experiment - I : Recognizing Textual Entailment', 'has', 'proposed two C - LSTMs models with four stacked blocks']
[159, 17, 'Experiment - I : Recognizing Textual Entailment']
[168, 18, 'Our proposed two C - LSTMs models with four stacked blocks outperform all the competitor models , which indicates that our thinner and deeper network does work effectively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WikiQA dataset', 'obtain', 'result']
[159, 26, 'Results on WikiQA dataset are listed in .']
[162, 27, 'We obtain a result on par with the state - of - the - art reported on this dataset .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['development set', 'see that', 'each of our model components']
[229, 26, 'The ablation results of QA performances in the development set of Hotpot QA are shown in .']
[230, 27, 'From the table we can see that each of our model components can provide from 1 % to 2 % relative gain over the QA performance .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['dynamic entity graph', 'has', 'process']
[50, 5, 'For the first challenge , DFGN constructs a dynamic entity graph based on entity mentions in the query and documents .']
[51, 6, 'This process iterates in multiple rounds to achieve multihop reasoning .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Hotpot QA', 'see that', 'our model']
[221, 22, 'We first present a comparison between baseline models and our DFGN 2 . shows the performance of different models in the private test set of Hotpot QA .']
[222, 23, 'From the table we can see that our model achieves the second best result on the leaderboard now 3 ( on March 1st ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SQuAD', 'has', 'shared - norm model']
[195, 20, 'SQuAD']
[209, 21, 'While all our approaches had some benefit , the shared - norm model is the strongest , and is the only one to not lose performance as large numbers of paragraphs are used .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SQuAD', 'has', 'all our approaches']
[195, 20, 'SQuAD']
[209, 21, 'While all our approaches had some benefit , the shared - norm model is the strongest , and is the only one to not lose performance as large numbers of paragraphs are used .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Trivia QA Unfiltered', 'has', 'base model']
[186, 18, 'Trivia QA Unfiltered']
[191, 19, 'Note the base model starts to lose performance as more paragraphs are used , showing that errors are being caused by the model being overly confident in incorrect extractions . :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Trivia QA Web', 'find', 'effective']
[173, 14, 'Trivia QA Web']
[179, 15, 'We find both TF - IDF ranking and the sum objective to be effective ; even without changing the model we achieve state - of - the - art results .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['single model', 'has', 'contribute']
[155, 17, 'We also run the ablations of our single model on SQ u AD dev set to evaluate the individual contribution .']
[156, 18, "As shows , both syntactic embeddings and semantic embeddings contribute towards the model 's performance and the POS tags seem to be more important ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three parts', 'has', 'high - efficiency multilayer memory network']
[27, 4, 'Our model consists of three parts :']
[28, 5, '1 ) the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three parts', 'has', 'pointer - network based answer boundary prediction layer']
[27, 4, 'Our model consists of three parts :']
[28, 5, '1 ) the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three parts', 'has', 'encoding']
[27, 4, 'Our model consists of three parts :']
[28, 5, '1 ) the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Stanford Question Answering Dataset ( SQuAD ) v 1.1', 'has', 'our model']
[131, 15, 'We also use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .']
[137, 16, 'The results of this dataset are all exhibited on a leaderboard , and top methods are almost all ensemble models , our model achieves an exact match score of 75.37 % and an F1 score of 82 . 66 % , which is competitive to state - of - the - art method .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['competitive', 'to', 'state - of - the - art method']
[131, 15, 'We also use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .']
[137, 16, 'The results of this dataset are all exhibited on a leaderboard , and top methods are almost all ensemble models , our model achieves an exact match score of 75.37 % and an F1 score of 82 . 66 % , which is competitive to state - of - the - art method .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Wiki QA', 'has', 'paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features']
[175, 10, 'Wiki QA Results .']
[177, 11, 'The neural network models in the table , paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features , are mostly based on sentence modeling .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features', 'has', 'Our model']
[177, 11, 'The neural network models in the table , paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features , are mostly based on sentence modeling .']
[178, 12, 'Our model outperforms them all .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['coreference relations', 'has', 'Tokens']
[43, 5, 'We use MAGE - RNN to model coreference relations for text comprehension tasks , where answers to a query have to be extracted from a context document .']
[44, 6, 'Tokens in a document are connected by a coreference relation if they refer to the same underlying entity .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN dataset', 'Augmenting', 'bi - GRU model']
[244, 18, 'Cloze - style QA : Lastly , we test our models on the CNN dataset from , which consists of pairs of news articles and a cloze - style question over the contents .']
[256, 19, 'Augmenting the bi - GRU model with MAGE leads to an improvement of 2.5 % on the test set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['improvement', 'of', '2.5 %']
[244, 18, 'Cloze - style QA : Lastly , we test our models on the CNN dataset from , which consists of pairs of news articles and a cloze - style question over the contents .']
[256, 19, 'Augmenting the bi - GRU model with MAGE leads to an improvement of 2.5 % on the test set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Story Based', 'has', 'Our model']
[170, 7, 'Story Based']
[181, 8, 'Our model achieves new state - of - the - art results , outperforming strong baselines such as QRNs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Broad Context Language Modeling', 'pick', 'LAMBADA dataset']
[219, 13, 'Broad Context Language Modeling :']
[220, 14, 'For our second benchmark we pick the LAMBADA dataset from , where the task is to predict the last word in a given passage .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LAMBADA dataset', 'has', 'Our implementation of GA']
[220, 14, 'For our second benchmark we pick the LAMBADA dataset from , where the task is to predict the last word in a given passage .']
[231, 15, 'Our implementation of GA gave higher performance than that reported by , without the use of linguistic features .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['original WMT16 Romanian - English', 'has', 'Preliminary results']
[193, 21, 'For each row we experiment on the original WMT16 Romanian - English augmented with back - translation data .']
[195, 22, '1 . Preliminary results suggested that our approach was less effective without back - translation data , and prone to overfitting - future work should explore additional regularization techniques .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['recently proposed ELI5 dataset', 'find', 'BART']
[185, 19, "We use the recently proposed ELI5 dataset to test the model 's ability to generate long freeform answers ."]
[186, 20, 'We find BART outperforms the best previous work by 1.2 ROUGE - L , but the dataset remains a challenging , because answers are only weakly specified by the question .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['several text generation tasks', 'present', 'results']
[168, 13, 'We also experiment with several text generation tasks .']
[174, 14, 'To provide a comparison with the state - of - the - art in summarization , we present results on two summarization datasets , CNN / DailyMail and XSum , which have distinct properties .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two summarization datasets', 'has', 'BART']
[174, 14, 'To provide a comparison with the state - of - the - art in summarization , we present results on two summarization datasets , CNN / DailyMail and XSum , which have distinct properties .']
[177, 15, 'Nevertheless , BART outperforms all existing work .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['dialogue response generation', 'has', 'BART']
[182, 17, 'We evaluate dialogue response generation on CONVAI2 , in which agents must generate responses conditioned on both the previous context and a textually - specified persona .']
[183, 18, 'BART outperforms previous work on two automated metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['FABIR and BiDAF', 'shows', 'shorter answers']
[254, 22, 'In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD .']
[255, 23, 'shows that shorter answers are easier for both models : while they reach more than 75 % F1 for answers that are shorter than four words , for answers longer than ten words these scores drop to 60.4 % and 67.3 % for FABIR and BiDAF , respectively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Baselines', 'compare with', 'published Rank1 - 3 models']
[170, 14, 'We compare with the results from the sentences selected by TF - IDF method and our selector ( Dyn ) .']
[171, 15, 'We also compare with published Rank1 - 3 models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['full Narra - tive QA task', 'observe', 'decline']
[207, 10, 'summarizes the results on the full Narra - tive QA task , where the context documents are full stories .']
[208, 11, 'As expected ( and desired ) , we observe a decline in performance of the span- selection oracle IR model , compared with the results on summaries .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['four additional judgments', 'Of these', '58 %']
[21, 6, 'In a separate validation phase , we collected four additional judgments for each label for 56,941 of the examples .']
[22, 7, 'Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['four additional judgments', 'Of these', '98 %']
[21, 6, 'In a separate validation phase , we collected four additional judgments for each label for 56,941 of the examples .']
[22, 7, 'Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['iterative inference process', 'involves', 'novel alternating attention mechanism']
[21, 5, 'This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .']
[22, 6, 'The result of this alternating search is fed back into the iterative inference process to seed the next search step .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['novel alternating attention mechanism', 'has', 'result']
[21, 5, 'This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .']
[22, 6, 'The result of this alternating search is fed back into the iterative inference process to seed the next search step .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['encoding layer completely', 'demonstrate', 'feature extraction layer']
[199, 30, "If we remove encoding layer completely , then we 'll obtain a 73.5 for matched score and 73.2 for mismatched score ."]
[200, 31, 'The result demonstrate the feature extraction layer have powerful capability to capture the semantic feature .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['both self - attention and fuse gate', 'has', 'result']
[201, 32, 'In experiment 5 , we remove both self - attention and fuse gate , thus retaining only highway network .']
[202, 33, 'The result improves to 77.7 and 77.3 respectively on matched and mismatched development set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MULTINLI', 'has', 'Our approach']
[158, 20, 'EXPERIMENT ON MULTINLI']
[163, 21, 'Our approach , without using any recurrent structure , achieves the new state - of - the - art performance of 80.0 % , exceeding current state - of - the - art performance by more than 5 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SNLI', 'show', 'our model , DIIN']
[166, 23, 'EXPERIMENT ON SNLI']
[174, 24, 'We show our model , DIIN , achieves state - of - the - art performance on the competitive leaderboard .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['QUORA QUESTION PAIR DATASET', 'has', 'BIMPM']
[178, 25, 'EXPERIMENT ON QUORA QUESTION PAIR DATASET']
[181, 26, 'BIMPM models different perspective of matching between sentence pair on both direction , then aggregates matching vector with LSTM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WikiQA and TREC - QA case', 'apply', '8 latent clusters']
[116, 12, 'We select k ( for the kmax - pool in equation 5 ) as 6 and 4 for the WikiQA and TREC - QA case , respectively .']
[117, 13, 'In both datasets , we apply 8 latent clusters .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['YahooCQA', 'has', 'key competitors']
[171, 4, 'YahooCQA']
[187, 6, '- The key competitors of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and HD - LSTM from Tay et al.']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WikiQA', 'has', 'key competitors']
[189, 8, 'Additionally , we also report our own implementations of QA - BiLSTM , QA - CNN , AP - BiLSTM and AP - CNN on this dataset based on our experimental setup . WikiQA']
[190, 9, '- The key competitors of this dataset are the Paragraph Vector ( PV ) and PV + Cnt models of Le and Mikolv , CNN + Cnt model from Yu et al. and LCLR ( Yih et al . ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['PV + Cnt models', 'of', 'Le and Mikolv']
[189, 8, 'Additionally , we also report our own implementations of QA - BiLSTM , QA - CNN , AP - BiLSTM and AP - CNN on this dataset based on our experimental setup . WikiQA']
[190, 9, '- The key competitors of this dataset are the Paragraph Vector ( PV ) and PV + Cnt models of Le and Mikolv , CNN + Cnt model from Yu et al. and LCLR ( Yih et al . ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TrecQA ( raw )', 'has', 'Hyper QA']
[238, 22, 'reports the results on TrecQA ( raw ) .']
[239, 23, 'Hyper QA achieves very competitive performance on both MAP and MRR metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['competitive performance', 'on', 'MAP and MRR metrics']
[238, 22, 'reports the results on TrecQA ( raw ) .']
[239, 23, 'Hyper QA achieves very competitive performance on both MAP and MRR metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TrecQA ( clean )', 'has', 'Hyper QA']
[246, 25, 'Similarly , reports the results on TrecQA ( clean ) .']
[247, 26, 'Similarly , Hyper QA also outperforms MP - CNN , AP - CNN and QA - CNN .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SemEvalCQA', 'has', 'proposed approach']
[226, 18, 'reports the experimental results on SemEvalCQA .']
[227, 19, 'Our proposed approach achieves highly competitive performance on this dataset .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DA', 'fall into', 'local minima']
[162, 9, 'The results demonstrate that DA exhibits fast yet shallow convergence .']
[163, 10, 'It tends to fall into local minima , which finally fails to reach zero loss .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DA', 'has', 'fails']
[162, 9, 'The results demonstrate that DA exhibits fast yet shallow convergence .']
[163, 10, 'It tends to fall into local minima , which finally fails to reach zero loss .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Key- value attention', 'has', 'performance']
[164, 11, 'Key- value attention helps NUTM converge completely with fewer iterations .']
[165, 12, 'The performance is further improved with the proposed regularization loss .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['IR Baseline', 'rank', 'answer candidates']
[147, 5, 'IR Baseline :']
[148, 6, 'For this baseline , we rank answer candidates by the maximum tf .idf document retrieval score using an unboosted query of question and answer terms ( see Section 4.1 for retrieval details ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['IR ++', 'with', 'only the IR ++ feature group']
[149, 7, 'IR ++ :']
[150, 8, 'This baseline uses the same architecture as the full model , as described in Section 4.3 , but with only the IR ++ feature group .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['IR ++', 'uses', 'same architecture']
[149, 7, 'IR ++ :']
[150, 8, 'This baseline uses the same architecture as the full model , as described in Section 4.3 , but with only the IR ++ feature group .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['AI2 Kaggle question set', 'By way of', 'loose comparison']
[182, 11, 'also tackle the AI2 Kaggle question set with an approach that learns alignments between questions and structured and semistructured KB data .']
[184, 12, 'By way of a loose comparison ( since we are evaluating on different data partitions ) , our model has approximately 5 % higher performance despite our simpler set of features and unstructured KB .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['QA Performance', 'has', 'Our full model']
[170, 9, 'QA Performance']
[176, 10, 'Our full model that combines IR ++ , lexical overlap , discourse , and embeddings - based features , has a P@1 of 53.3 % ( line 7 ) , an absolute gain of 6.3 % over the strong IR baseline despite using the same background knowledge .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Justification Performance', 'has', '61 %']
[196, 14, 'Justification Performance']
[227, 15, 'Note that 61 % of the top - ranked justifications from our system were rated as Good as compared to 52 % from the IR baseline ( a gain of 9 % ) , despite the systems using identical corpora .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Comparison with SkipThought', 'With', 'much less data ( 570 k compared to 64M sentences )']
[169, 15, 'Comparison with SkipThought']
[171, 16, 'With much less data ( 570 k compared to 64M sentences ) but with high - quality supervision from the SNLI dataset , we are able to consistently outperform the results obtained by SkipThought vectors .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Image - caption retrieval results', 'trained with', 'ResNet features and 30 k more training data']
[187, 26, 'Image - caption retrieval results']
[190, 27, 'When trained with ResNet features and 30 k more training data , the SkipThought vectors perform significantly better than the original setting , going from 33.8 to 37.9 for caption retrieval R@1 , and from 25.9 to 30.6 on image retrieval R@1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MultiGenre NLI', 'observe', 'significant boost']
[194, 29, 'MultiGenre NLI']
[198, 30, 'We observe a significant boost in performance over all compared to the model trained only on SLNI .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Embedding size', 'has', 'increased embedding sizes']
[151, 13, 'Embedding size']
[152, 14, 'Since it is easier to linearly separate in high dimension , especially with logistic regression , it is not surprising that increased embedding sizes lead to increased performance for almost all models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Domain adaptation on SICK tasks', 'has', 'Our transfer learning approach']
[182, 22, 'Domain adaptation on SICK tasks']
[183, 23, 'Our transfer learning approach obtains better results than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Architecture impact', 'has', 'BiLSTM - 4096']
[140, 9, 'Architecture impact']
[143, 10, 'The BiLSTM - 4096 with the max - pooling operation performs best on both SNLI and transfer tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BiLSTM - 4096', 'Looking at', 'micro and macro averages']
[143, 10, 'The BiLSTM - 4096 with the max - pooling operation performs best on both SNLI and transfer tasks .']
[144, 11, 'Looking at the micro and macro averages , we see that it performs significantly better than the other models LSTM , GRU , BiGRU - last , BiLSTM - Mean , inner-attention and the hierarchical - ConvNet. also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM - Mean for instance .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NLI as a supervised training set', 'indicate', 'our model']
[177, 20, 'NLI as a supervised training set']
[178, 21, 'Our findings indicate that our model trained on SNLI obtains much better over all results than models trained on other supervised tasks such as COCO , dictionary definitions , NMT , PPDB and SST .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two components', 'name', 'perceptual front - end']
[27, 5, 'An important insight from the work of is to decompose a function for relational reasoning into two components or " modules " :']
[28, 6, 'a perceptual front - end , which is tasked to recognize objects in the raw input and represent them as vectors , and a relational reasoning module , which uses the representation to reason about the objects and their interactions .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two components', 'name', 'relational reasoning module']
[27, 5, 'An important insight from the work of is to decompose a function for relational reasoning into two components or " modules " :']
[28, 6, 'a perceptual front - end , which is tasked to recognize objects in the raw input and represent them as vectors , and a relational reasoning module , which uses the representation to reason about the objects and their interactions .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Embedding Layer', 'use', '100 - dimensional embedding vector']
[282, 28, 'Embedding Layer']
[283, 29, 'We use a 100 - dimensional embedding vector for each symbol in the query and graph description .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Embedding Layer', 'apply', 'dropout']
[149, 7, 'Embedding Layer :']
[151, 9, 'We also apply dropout with probability 0.2 to the embedding layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Embedding Layer', 'use', '300 - dimensional pretrained Glove word embeddings']
[149, 7, 'Embedding Layer :']
[150, 8, 'We choose 300 - dimensional word embeddings , and use the 300 - dimensional pretrained Glove word embeddings for initialization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Embedding Layer', 'choose', '300 - dimensional word embeddings']
[149, 7, 'Embedding Layer :']
[150, 8, 'We choose 300 - dimensional word embeddings , and use the 300 - dimensional pretrained Glove word embeddings for initialization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Iterative Attention Reader , EpiReader and GA Reader', 'has', 'outperforms']
[197, 15, 'Iterative Attention Reader , EpiReader and GA Reader are the three multi-turn reasoning models with xed reasoning steps .']
[198, 16, 'ReasoNet also outperforms all of them by integrating termination gate in the model which allows di erent reasoning steps for di erent test cases .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Vocab Size', 'use', 'python NLTK tokenizer']
[216, 19, 'Vocab Size :']
[217, 20, 'We use the python NLTK tokenizer 6 to preprocess passages and questions , and obtain about 100K words in the vocabulary .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Multitask learning', 'has', 'NLI objective']
[170, 21, 'Multitask learning has been shown to be helpful to learn English sentence embeddings .']
[172, 22, 'As shown in , the NLI objective leads to a better performance on the English NLI test set , but this comes at the cost of a worse cross - lingual transfer performance in XNLI and Tatoeba .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BUCC : bitext mining', 'has', 'our system']
[132, 12, 'BUCC : bitext mining']
[144, 13, 'As shown in , our system establishes a new state - of - the - art for all language pairs with the exception of English - Chinese test .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MLDoc : cross - lingual classification', 'has', 'our system']
[124, 10, 'MLDoc : cross - lingual classification']
[130, 11, 'As shown in , our system obtains the best published results for 5 of the 7 transfer languages .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['XNLI : cross - lingual NLI', 'has', 'Our proposed method']
[94, 5, 'XNLI : cross - lingual NLI']
[105, 6, '9 Our proposed method obtains the best results in zero - shot cross - lingual transfer for all languages but Spanish .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Tatoeba : similarity search', 'are', '55']
[147, 15, 'Tatoeba : similarity search']
[155, 16, 'Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .']
[156, 17, '11 This is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Tatoeba : similarity search', 'are', '48 languages']
[147, 15, 'Tatoeba : similarity search']
[155, 16, 'Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .']
[156, 17, '11 This is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Tatoeba : similarity search', 'are', '15 languages']
[147, 15, 'Tatoeba : similarity search']
[155, 16, 'Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .']
[157, 18, 'There are only 15 languages with error rates above 50 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Tatoeba : similarity search', 'has', 'similarity error rates']
[147, 15, 'Tatoeba : similarity search']
[155, 16, 'Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['below 5 %', 'for', '37 languages']
[155, 16, 'Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance .']
[156, 17, '11 This is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ELMo', 'helped', 'our model ( MAMCN + ELMo )']
[160, 18, 'First , we add ELMo which is the weighted sum of hidden layers of language model with regularization as an additional feature to our word embeddings .']
[161, 19, 'This helped our model ( MAMCN + ELMo ) to improve F1 to 85.13 and EM to 77.44 and is the best among the models only with the additional feature augmentation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['our model ( MAMCN + ELMo )', 'is', 'best']
[160, 18, 'First , we add ELMo which is the weighted sum of hidden layers of language model with regularization as an additional feature to our word embeddings .']
[161, 19, 'This helped our model ( MAMCN + ELMo ) to improve F1 to 85.13 and EM to 77.44 and is the best among the models only with the additional feature augmentation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['our model ( MAMCN + ELMo + DC )', 'achieve', 'state of the art performance']
[167, 20, 'We replace all the BiGRU units with this embedding block except the controller layer in our model ( MAMCN + ELMo + DC ) .']
[168, 21, 'We achieve the state of the art performance , 86.73 F1 and 79.69 EM , with the help of this em-bedding block .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TriviaQA', 'has', 'Our model']
[151, 16, 'TriviaQA : We compare proposed model with all the previously suggested approaches as shown in .']
[155, 17, 'Our model achieves the state of the art performance over the existing approaches as shown in 77.58 84.16 O - QANet 76.24 84.60 O O SAN 76.83 84.40 O O Fusion Net 75.97 83.90 O O RaSoR + TR 75.79 83.26 O - Conducter- net 74.41 82.74 O O Reinforced Mnemonic Reader 73.20 81.80 O O BiDAF + Self-attention 72.14 81.05 - O MEMEN 70.98 80.36 O - MAMCN 70.99 79.94 -r- net 71.30 79.70 - O Document Reader 70.73 79.35 O - FastQAExt 70 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['more performance improvement', 'achieve', '68.13 EM and 70.32 F1']
[149, 14, 'As described in , the baseline ( BiDAF + DNC ) results in a reasonable gain , however , our proposed memory controller gives more performance improvement .']
[150, 15, 'We achieve 68.13 EM and 70.32 F1 for short documents and 63.44 and 65.19 for long documents which are the current best results .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['more performance improvement', 'achieve', '63.44 and 65.19']
[149, 14, 'As described in , the baseline ( BiDAF + DNC ) results in a reasonable gain , however , our proposed memory controller gives more performance improvement .']
[150, 15, 'We achieve 68.13 EM and 70.32 F1 for short documents and 63.44 and 65.19 for long documents which are the current best results .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TREC - QA dataset', 'has', 'AP - CNN']
[200, 14, 'In , we present the experimental results of the four NNs for the TREC - QA dataset .']
[202, 15, 'We use the official trec eval that AP - CNN outperforms QA - CNN by a large margin in both metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Insurance QA dataset', 'see that', 'AP - CNN']
[180, 10, 'In , we present the experimental results of the four NNs for the Insurance QA dataset .']
[182, 11, 'On the bottom part of this table , we can see that AP - CNN outperforms QA - CNN by a large margin in both test sets , as well as in the dev set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WikiQA dataset', 'has', 'AP - CNN']
[210, 18, 'shows the experimental results of the four NNs for the WikiQA dataset .']
[211, 19, 'Like in the other two datasets , AP - CNN outperforms QA - CNN , and AP - biLSTM outperforms the QA - biLSTM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WikiQA dataset', 'has', 'AP - biLSTM']
[210, 18, 'shows the experimental results of the four NNs for the WikiQA dataset .']
[211, 19, 'Like in the other two datasets , AP - CNN outperforms QA - CNN , and AP - biLSTM outperforms the QA - biLSTM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Adam method', 'for', 'optimization']
[141, 8, 'We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ?']
[143, 10, '2 set to 0.999 for optimization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['optimization', 'with', 'hyperparameters']
[141, 8, 'We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ?']
[143, 10, '2 set to 0.999 for optimization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['hyperparameters', 'set to', '0.9']
[141, 8, 'We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ?']
[142, 9, '1 set to 0.9 and ?']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['hyperparameters', 'set to', '0.999']
[141, 8, 'We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ?']
[142, 9, '1 set to 0.9 and ?']
[143, 10, '2 set to 0.999 for optimization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SQuAD dataset', 'are', 'better']
[149, 16, 'Results shows our main results on the SQuAD dataset .']
[150, 17, 'Compared to the scores reported in , our exact match ( EM ) and F1 on the development set and EM score on the test set are better , and F1 on the test set is comparable .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SQuAD dataset', 'are', 'comparable']
[149, 16, 'Results shows our main results on the SQuAD dataset .']
[150, 17, 'Compared to the scores reported in , our exact match ( EM ) and F1 on the development set and EM score on the test set are better , and F1 on the test set is comparable .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TF - IDF', 'has', 'Retrieval - based models']
[220, 12, 'TF - IDF']
[221, 13, 'Retrieval - based models are known to be strong QA baselines if candidate answers are provided .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Majority - candidate - per-query - type', 'Predicts', 'candidate']
[216, 9, 'Majority - candidate - per-query - type']
[217, 10, 'Predicts the candidate c ?']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['candidate', 'that was', 'most frequently observed']
[217, 10, 'Predicts the candidate c ?']
[218, 11, 'C q that was most frequently observed as the true answer in the training set , given the query type of q .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Max- mention', 'Predicts', 'most frequently mentioned candidate']
[213, 7, 'Max- mention']
[214, 8, 'Predicts the most frequently mentioned candidate in the support documents']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['interactions between the endpoints', 'has', 'RASOR']
[161, 16, 'Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .']
[162, 17, 'RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['300 dimensional GloVe embeddings', 'cover', '200 k words']
[105, 4, 'We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .']
[106, 5, 'These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['300 dimensional GloVe embeddings', 'has', 'all out of vocabulary ( OOV ) words']
[105, 4, 'We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .']
[106, 5, 'These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['one', 'of', '1 m randomly initialized 300d embeddings']
[105, 4, 'We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .']
[106, 5, 'These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['GA reader', 'performs', 'much better']
[222, 13, 'The results for SA reader are far below the per-formance of GA reader .']
[223, 14, 'We also see that it performs much better on anonymized entities than on non-anonymized ones .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['reinforcement learning part', 'has', 'result']
[183, 22, 'Finally , we ablate the reinforcement learning part , in other words , we only use the original loss function to optimize the model ( set ? = 1 ) .']
[184, 23, 'The result drops about 0.5 % , which proves that it is helpful to utilize all the information from the annotators .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SNLI development dataset', 'has', 'result']
[173, 16, 'As shown in , we conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model .']
[175, 17, 'The result is obviously not satisfactory , which indicates that only using sentence embedding from discourse markers to predict the answer is not ideal in large - scale datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['not ideal', 'in', 'large - scale datasets']
[173, 16, 'As shown in , we conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model .']
[175, 17, 'The result is obviously not satisfactory , which indicates that only using sentence embedding from discourse markers to predict the answer is not ideal in large - scale datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['sentence encoder model', 'observe that', 'performance']
[176, 18, "We then remove the sentence encoder model , which means we do n't use the knowledge transferred from the DMP task and thus the representations r p and r hare set to be zero vectors in the equation ( 6 ) and the equation ."]
[177, 19, 'We observe that the performance drops significantly to 87 . 24 % , which is nearly 1.5 % to our DMAN model , which indicates that the discourse markers have deep connections with the logical relations between two sentences they links .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['drops significantly', 'to', '87 . 24 %']
[176, 18, "We then remove the sentence encoder model , which means we do n't use the knowledge transferred from the DMP task and thus the representations r p and r hare set to be zero vectors in the equation ( 6 ) and the equation ."]
[177, 19, 'We observe that the performance drops significantly to 87 . 24 % , which is nearly 1.5 % to our DMAN model , which indicates that the discourse markers have deep connections with the logical relations between two sentences they links .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WikiMovies', 'has', 'Key - Value Memory Networks']
[160, 7, 'WikiMovies']
[169, 8, 'However , Key - Value Memory Networks outperform all other methods on all three data source types .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WikiQA', 'has', 'Key - Value Memory Networks']
[186, 10, 'WikiQA']
[201, 11, 'Key - Value Memory Networks outperform a large set of other methods , although the results of the L.D.C. method of are very similar .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BoW Model', 'trained on', 'spans']
[140, 6, 'BoW Model']
[141, 7, 'The BoW model is trained on spans up to length 10 to keep the computation tractable .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['FastQA', 'tokenize', 'input']
[70, 5, 'FastQA']
[150, 14, 'We tokenize the input on whitespaces ( exclusive ) and non-alphanumeric characters ( inclusive ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CAFE', 'obtains', '88.5 % accuracy']
[206, 21, 'CAFE obtains']
[207, 22, '88.5 % accuracy on the SNLI test set , an extremely competitive score on the extremely popular benchmark .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SNLI benchmark', 'On', 'cross sentence ( single model setting )']
[203, 19, 'Table 1 reports our results on the SNLI benchmark .']
[204, 20, 'On the cross sentence ( single model setting ) , the performance of our proposed CAFE model is extremely competitive .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['performance gain', 'are', '10 % ? 13 %']
[231, 32, 'The performance gain over strong baselines such as DecompAtt and ESIM are ?']
[232, 33, '10 % ? 13 % in terms of accuracy .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
[', .!?', 'treated as', 'normal word']
[173, 15, 'Special characters such as , .!?']
[174, 16, 'are treated as a normal word .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['PV - DBOW', 'has', 'learned vector representations']
[169, 11, 'The vector presented to the classifier is a concatenation of two vectors , one from PV - DBOW and one from PV - DM .']
[170, 12, 'In PV - DBOW , the learned vector representations have 400 dimensions .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['PV - DM', 'has', 'learned vector representations']
[169, 11, 'The vector presented to the classifier is a concatenation of two vectors , one from PV - DBOW and one from PV - DM .']
[170, 12, 'In PV - DBOW , the learned vector representations have 400 dimensions .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Msap', 'trains', 'logistic regression']
[174, 10, 'Msap :']
[176, 11, 'It trains a logistic regression based on stylistic and languagemodel based features .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Majority Vote', 'has', 'ensemble method']
[179, 13, 'Majority Vote :']
[180, 14, 'This ensemble method uses the features extracted for each of the K = 3 aspects , to train K separate logistic regression models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Soft Voting', 'learns', 'K different aspect - specific classifiers']
[182, 15, 'Soft Voting :']
[183, 16, 'This baseline also learns K different aspect - specific classifiers .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Aspect - aware Ensemble', 'trains', 'K different aspectspecific classifiers']
[188, 17, 'Aspect - aware Ensemble :']
[189, 18, 'Like the voting methods , this baseline also trains K different aspectspecific classifiers .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['combined representation', 'used for', 'out - of - vocabulary words']
[24, 4, 'Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation .']
[25, 5, 'These representations can then be used for out - of - vocabulary words , or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an external data source .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['combined representation', 'combined with', 'withinvocabulary word embeddings']
[24, 4, 'Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation .']
[25, 5, 'These representations can then be used for out - of - vocabulary words , or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an external data source .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['tree - LSTM block', 'performance', 'drops']
[203, 17, 'Each tree node is implemented with a tree - LSTM block same as in model .']
[204, 18, 'shows that with this replacement , the performance drops to 88.2 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['weight', 'in', 'dual context aggregation']
[132, 10, 'The weight ?']
[133, 11, 'in the dual context aggregation is 0.5 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['weight', 'is', '0.5']
[132, 10, 'The weight ?']
[133, 11, 'in the dual context aggregation is 0.5 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TreeRNN architecture', 'At', 'test time']
[22, 3, 'This design improves upon the TreeRNN architecture in three ways :']
[23, 4, 'At test time , it can simultaneously parse and interpret unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['four parts', 'has', 'recurrent network encoder']
[22, 3, 'Our model consists of four parts :']
[23, 4, '1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['four parts', 'has', 'gated matching layer']
[22, 3, 'Our model consists of four parts :']
[23, 4, '1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['four parts', 'has', 'self - matching layer']
[22, 3, 'Our model consists of four parts :']
[23, 4, '1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['four parts', 'has', 'pointernetwork based answer boundary prediction layer']
[22, 3, 'Our model consists of four parts :']
[23, 4, '1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three modules', 'follow', 'boundary - based MRC models']
[50, 3, 'The over all framework of our model is demonstrated in , which consists of three modules .']
[51, 4, 'First , we follow the boundary - based MRC models to find an answer candidate for each passage by identifying the start and end position of the answer ( .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['start and end position', 'of', 'answer']
[50, 3, 'The over all framework of our model is demonstrated in , which consists of three modules .']
[51, 4, 'First , we follow the boundary - based MRC models to find an answer candidate for each passage by identifying the start and end position of the answer ( .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DuReader', 'see that', 'paragraph ranking']
[160, 12, 'Results on DuReader']
[164, 13, 'We can see that this paragraph ranking can boost the BiDAF baseline significantly .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['possible negative sentence endings ( or counterfactuals )', 'filter', 'aggressively and adversarially']
[37, 3, 'We then use a state - of - theart language model fine - tuned on this data to massively oversample a diverse set of possible negative sentence endings ( or counterfactuals ) .']
[38, 4, 'Next , we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LSTM sequence model', 'has', 'greatly improves']
[235, 6, 'The best model that only uses the ending is the LSTM sequence model with ELMo embeddings , which obtains 43.6 % .']
[236, 7, 'This model , as with most models studied , greatly improves with more context : by 3.1 % when given the initial noun phrase , and by an ad-ditional 4 % when also given the first sentence .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Top - layer Classifier', 'has', 'Our inference model']
[66, 12, 'Top - layer Classifier']
[67, 13, 'Our inference model feeds the resulting vectors obtained above to the final classifier to determine the over all inference relationship .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Sequence Encoder', 'has', 'sentence pairs']
[39, 6, 'Sequence Encoder']
[40, 7, 'To represent words and their context in a premise and hypothesis , sentence pairs are fed into sentence encoders to obtain hidden vectors ( h p and h h ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Composition Layer', 'compose', 'hidden vectors']
[52, 9, 'Composition Layer']
[53, 10, 'To transform sentences into fixed - length vector representations and reason using those representations , we need to compose the hidden vectors obtained by the sequence encoder layer ( h p and h h ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Word Embedding', 'concatenate', 'embeddings']
[30, 4, 'Word Embedding']
[32, 5, 'We concatenate embeddings learned at two different levels to represent each word in the sentence : the character composition and holistic word - level embedding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Trivia QA', 'can see', 'our Smarnet model']
[210, 15, 'We also compare our models on the recently proposed dataset Trivia QA. shows the performance comparison on the test set of Trivia QA .']
[211, 16, 'We can see our Smarnet model outperforms the other baselines on both wikipedia domain and web domain .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['scope resolution', 'On', 'Sherlock dataset']
[207, 16, 'For scope resolution :']
[208, 17, 'On the Sherlock dataset , we achieve an F1 of 92.36 , outperforming the previous State of the Art by a significant margin ( almost 3.0 F1 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['decoder', 'has', '2']
[131, 11, 'The number of layers in the encoder is 1 and in decoder']
[132, 12, '2 . Models are trained with stochastic gradient descent with learning rate fixed at a value of 5 10 ? 5 with dropout rate of 30 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MSCOCO dataset', 'have', 'significant improvement']
[186, 17, 'Those numbers are reported in the Measure column with row best - BLEU / best - METEOR . , we report the results for MSCOCO dataset .']
[188, 18, 'As we can see , we have a significant improvement w.r.t. the baselines .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Quora dataset', 'has', 'both variations']
[197, 22, 'In , we report results for the Quora dataset .']
[198, 23, 'As we can see , both variations of our model perform significantly better than unsupervised VAE and VAE - S , which is not surprising .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['PTB - WSJ dataset', 'has', 'baseline ( BiLSTM - CRF ) model']
[138, 7, 'PTB - WSJ dataset .']
[140, 8, 'As expected , our baseline ( BiLSTM - CRF ) model ( accuracy 97.54 % ) performs on par with other state - of - the - art systems .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Character - to - word Models', 'has', 'F1 score']
[199, 19, '5 Results and Analysis 5.1 Character - to - word Models presents the performance of different character - to - word models on six benchmark datasets .']
[203, 20, 'The result shows that for most of the datasets , the F1 score does not improve much when we directly add more layers .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['state - of - the - art results', 'show that', 'our char - IntNet']
[213, 27, 'Table 2 presents our proposed model in comparison with state - of - the - art results .']
[219, 28, 'These experiments show that our char - IntNet generally improves results across different models and datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['OntoNotes 5.0', 'has', 'BiLSTM - LAN']
[189, 10, 'OntoNotes 5.0 . In NER , BiLSTM - CRF is widely used , because local dependencies between neighboring labels relatively more important that POS tagging and CCG supertagging .']
[190, 11, 'BiLSTM - LAN also significantly outperforms BiLSTM - CRF by 1.17 F1-score ( p < 0.01 ) . CCGBank .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['significantly outperforms', 'has', 'BiLSTM - CRF']
[189, 10, 'OntoNotes 5.0 . In NER , BiLSTM - CRF is widely used , because local dependencies between neighboring labels relatively more important that POS tagging and CCG supertagging .']
[190, 11, 'BiLSTM - LAN also significantly outperforms BiLSTM - CRF by 1.17 F1-score ( p < 0.01 ) . CCGBank .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Universal Dependencies ( UD ) v 2.2', 'has', 'Our model']
[185, 7, 'Universal Dependencies ( UD ) v 2.2 . We design a multilingual experiment to compare BiLSTMsoftmax , BiLSTM - CRF ( strictly following 1 , which is the state - of - theart on multi-lingual POS tagging ) and BiLSTM - LAN .']
[187, 8, 'Our model outperforms all the baselines on all the languages .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['all the baselines', 'on', 'all the languages']
[185, 7, 'Universal Dependencies ( UD ) v 2.2 . We design a multilingual experiment to compare BiLSTMsoftmax , BiLSTM - CRF ( strictly following 1 , which is the state - of - theart on multi-lingual POS tagging ) and BiLSTM - LAN .']
[187, 8, 'Our model outperforms all the baselines on all the languages .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WSJ', 'has', 'BiLSTM - LAN']
[178, 5, 'WSJ . shows the final POS tagging results on WSJ .']
[180, 6, 'BiLSTM - LAN gives significant accuracy improvements over both BiLSTM - CRF and BiLSTM- softmax ( p < 0.01 ) , which is consistent with observations on development experiments .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Morphological Tagging Results', 'has', 'Our models']
[149, 8, 'Morphological Tagging Results']
[154, 9, 'Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task ( i.e. , 1.8 % absolute improvement on average , corresponding to a RRIE of 21.20 % ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Part - of - Speech Tagging Results', 'has', 'Our model']
[134, 5, 'Part - of - Speech Tagging Results']
[140, 6, 'Our model outperforms in 32 of the 54 treebanks with 13 ties .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BM25 + Doc2query', 'expand', 'documents']
[69, 4, 'BM25 + Doc2query :']
[70, 5, 'We first expand the documents using the proposed Doc2query method .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BM25 + Doc2query', 'index and rank', 'expanded documents']
[69, 4, 'BM25 + Doc2query :']
[71, 6, 'We then index and rank the expanded documents exactly as in the BM25 method above .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RM3', 'compare', 'document expansion']
[75, 7, 'RM3 :']
[76, 8, 'To compare document expansion with query expansion , we applied the RM3 query expansion technique .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['model', 'produces', 'words']
[98, 14, 'We notice that the model tends to copy some words from the input document ( e.g. , Washington DC , River , chromosome ) , meaning that it can effectively perform term re-weighting ( i.e. , increasing the importance of key terms ) .']
[99, 15, 'Nevertheless , the model also produces words not present in the input document ( e.g. , weather , relationship ) , which can be characterized as expansion by synonyms and other related terms .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Flickr30 k', 'observe that', 'our method']
[187, 12, 'For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .']
[188, 13, 'We observe that our method obtains a higher performance on almost all categories even when VGG16 is used as the visual backbone .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['model', 'based on', 'PNASNet']
[187, 12, 'For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .']
[189, 14, 'The model based on PNASNet consistently outperforms the state - of - the - art on all categories on both metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['consistently outperforms', 'has', 'state - of - the - art']
[187, 12, 'For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .']
[189, 14, 'The model based on PNASNet consistently outperforms the state - of - the - art on all categories on both metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['state - of - the - art', 'on', 'all categories']
[187, 12, 'For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .']
[189, 14, 'The model based on PNASNet consistently outperforms the state - of - the - art on all categories on both metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['state - of - the - art', 'on', 'both metrics']
[187, 12, 'For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on Flickr30 k .']
[189, 14, 'The model based on PNASNet consistently outperforms the state - of - the - art on all categories on both metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['models', 'name', 'BERT - base uncased']
[83, 5, 'We performed experiments with the following models :']
[84, 6, 'BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['models', 'name', '3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM )']
[83, 5, 'We performed experiments with the following models :']
[84, 6, 'BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['models', 'name', 'Minitagger ( SVM ) ) + GloVe']
[83, 5, 'We performed experiments with the following models :']
[84, 6, 'BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['models', 'name', 'MarMoT ( CRF )']
[83, 5, 'We performed experiments with the following models :']
[84, 6, 'BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['models', 'name', 'Majority class per word']
[83, 5, 'We performed experiments with the following models :']
[84, 6, 'BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['manually annotated test set from The Boston University radio news corpus', 'has', 'good results']
[126, 25, 'As the proposed dataset has been automatically generated as described in Section 3 , we also tested the best two models , BERT and BiLSTM , with a manually annotated test set from The Boston University radio news corpus . :']
[130, 26, 'The good results 6 from this experiment provide further support for the quality of the new dataset .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Reranking', 'see that', 'multitasking with paraphrase data']
[187, 8, 'Reranking']
[200, 9, 'First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['URAE+ MLP', 'use', 'Unfolding Recursive Autoencoder']
[142, 13, 'The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :']
[143, 14, 'We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SENNA + MLP / SIM', 'use', 'SENNA - type sentence model']
[143, 14, 'We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :']
[144, 15, 'We use the SENNA - type sentence model for sentence representation ;']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WORDEMBED', 'calculated', 'matching score']
[142, 13, 'The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :']
[143, 14, 'We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Hyperparameters', 'margin', '0.1']
[228, 6, 'The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?']
[229, 7, 'was set to 0.1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Transfer learning on Reverb', 'test', 'our ability']
[255, 9, 'Transfer learning on Reverb']
[256, 10, 'In this set of experiments , all Reverb facts are added to the memory , without any retraining , and we test our ability to rerank answers on the companion QA set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['influence of various filter types', 'constructed', '5 groups of']
[190, 11, 'Third , we tested the influence of various filter types .']
[191, 12, 'We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['QASent dataset', 'got', 'comparable MRR']
[201, 13, 'QASent dataset .']
[213, 14, 'We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['QASent dataset', 'got', 'best MAP']
[201, 13, 'QASent dataset .']
[213, 14, 'We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MSRP dataset', 'obtained', 'comparable performance']
[224, 17, 'MSRP dataset .']
[236, 18, 'Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Wiki QA dataset', 'more effective than', 'other models']
[214, 15, 'Wiki QA dataset .']
[223, 16, 'The last row of shows that our model is more effective than the other models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['sliding window', 'operates on', 'two different views of text sentences']
[31, 8, 'We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .']
[32, 9, "Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN', 'improves', 'state - of - the - art accuracy']
[148, 16, 'CNN']
[150, 17, 'The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Results', 'on', 'CBT']
[132, 14, 'CBT']
[134, 15, 'Our model ( line 7 ) sets a new stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CBT', 'sets', 'a new stateof - the - art']
[132, 14, 'CBT']
[134, 15, 'Our model ( line 7 ) sets a new stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Story - based QA', 'In', '1 k data']
[217, 11, 'Story - based QA .']
[219, 12, "In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Dialog', 'outperforms', 'previous work']
[221, 14, 'Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in in Appendix ) .']
[224, 15, 'QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['previous work', 'by', 'large margin ( 2.0 + % )']
[221, 14, 'Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in in Appendix ) .']
[224, 15, 'QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['word embeddings', 'has', 'linear mapping layer']
[174, 20, 'The word embeddings are pre-trained 300 - D Glove 840B vectors .']
[175, 21, 'For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MMA - NSE attention model', 'obtained', '85.4 % accuracy score']
[157, 15, 'Our MMA - NSE attention model is similar to the LSTM attention model .']
[159, 16, 'This model obtained 85.4 % accuracy score .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['boundary model', 'with', 'search mechanism']
[40, 5, 'We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .']
[41, 6, 'We also further extend the boundary model with a search mechanism .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Model', 'adopt', 'Pointer Net ( Ptr - Net ) model']
[38, 3, 'Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .']
[39, 4, 'We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['multiple tokens', 'from', 'original text']
[38, 3, 'Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .']
[39, 4, 'We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Graph Neural Network ( GNN )', 'include', 'model variant']
[173, 9, 'Graph Neural Network ( GNN ) -']
[174, 10, 'To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the hidden state as a combination of the activations ( Eq 1 ) and the previous state .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['WebQSP - WD data set', 'has', 'graph models']
[218, 12, 'We compare the results on the WebQSP - WD data set in .']
[219, 13, 'As can be seen , the graph models outperform all other models across precision , recall and F-score , with GGNN showing the best over all result .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['memory - less attention mechanism', 'allows', 'attention']
[22, 7, 'Second , we use a memory - less attention mechanism .']
[25, 8, 'It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .']
[26, 9, 'It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['attention', 'at', 'each time step']
[25, 8, 'It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .']
[26, 9, 'It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['memory - less attention mechanism', 'forces', 'attention layer']
[22, 7, 'Second , we use a memory - less attention mechanism .']
[25, 8, 'It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['memory - less attention mechanism', 'enables', 'modeling layer']
[22, 7, 'Second , we use a memory - less attention mechanism .']
[25, 8, 'It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['FVTA network', 'with', 'modality number']
[256, 38, 'In the MovieQA dataset , each QA is given a set of N movie clips of the same movie , and each clip comes with subtitles .']
[257, 39, 'We implement FVTA network for Movie QA task with modality number of 2 ( video & text ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['modality number', 'of', '2 ( video & text )']
[256, 38, 'In the MovieQA dataset , each QA is given a set of N movie clips of the same movie , and each clip comes with subtitles .']
[257, 39, 'We implement FVTA network for Movie QA task with modality number of 2 ( video & text ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LSTM', 'for', 'all media documents']
[217, 22, 'Given a hidden state size of d , which is set to 50 , we concatenate the output of both directions of the LSTM and get a question matrix Q ?']
[219, 24, 'R 2dV KN 6 for all media documents .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['all media documents', 'get', 'context tensor H']
[217, 22, 'Given a hidden state size of d , which is set to 50 , we concatenate the output of both directions of the LSTM and get a question matrix Q ?']
[218, 23, 'R 2 d M and context tensor H ?']
[219, 24, 'R 2dV KN 6 for all media documents .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['context tensor H', 'has', 'R 2dV KN 6']
[218, 23, 'R 2 d M and context tensor H ?']
[219, 24, 'R 2dV KN 6 for all media documents .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['all media documents', 'get', 'question matrix Q']
[217, 22, 'Given a hidden state size of d , which is set to 50 , we concatenate the output of both directions of the LSTM and get a question matrix Q ?']
[219, 24, 'R 2dV KN 6 for all media documents .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['question matrix Q', 'has', 'R 2 d M']
[217, 22, 'Given a hidden state size of d , which is set to 50 , we concatenate the output of both directions of the LSTM and get a question matrix Q ?']
[218, 23, 'R 2 d M and context tensor H ?']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['context - aware question attention', 'shows', 'question attention']
[246, 34, 'We compare the effectiveness of context - aware question attention by removing the question attention and use the last timestep of the LSTM output from the question as the question representation .']
[247, 35, 'It shows the question attention provides slight improvement .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['FVTA without photos', 'has', 'result']
[248, 36, 'Finally , we train FVTA without photos to see the contribution of visual information .']
[249, 37, 'The result is quite good but it is perhaps not surprising due to the language bias in the questions and answers of the dataset , which is not uncommon in VQA dataset and in Visual7W .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Logistic Regression', 'predicts', 'answer']
[182, 9, 'Memex QA provides 4 answer choices and only one correct answer for each question .']
[190, 10, 'We implement the following methods as baselines : Logistic Regression predicts the answer with concatenated image , question and metadata features as reported in .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['answer', 'with', 'concatenated image , question and metadata features']
[182, 9, 'Memex QA provides 4 answer choices and only one correct answer for each question .']
[190, 10, 'We implement the following methods as baselines : Logistic Regression predicts the answer with concatenated image , question and metadata features as reported in .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three sources', 'name', 'answer - signaling words']
[37, 5, 'FVTA proposes a novel kernel to compute the attention tensor that jointly models the latent information in three sources :']
[38, 6, '1 ) answer - signaling words in the question , 2 ) temporal correlation within a sequence , and 3 ) cross-modal interaction between the text and image .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three sources', 'name', 'temporal correlation']
[37, 5, 'FVTA proposes a novel kernel to compute the attention tensor that jointly models the latent information in three sources :']
[38, 6, '1 ) answer - signaling words in the question , 2 ) temporal correlation within a sequence , and 3 ) cross-modal interaction between the text and image .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three sources', 'name', 'cross-modal interaction']
[37, 5, 'FVTA proposes a novel kernel to compute the attention tensor that jointly models the latent information in three sources :']
[38, 6, '1 ) answer - signaling words in the question , 2 ) temporal correlation within a sequence , and 3 ) cross-modal interaction between the text and image .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RACE', 'has', 'key competitors']
[177, 7, 'RACE']
[178, 8, 'The key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SearchQA', 'has', 'main competitor baseline']
[184, 9, 'SearchQA']
[185, 10, 'The main competitor baseline is the AMANDA model proposed by .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NarrativeQA', 'has', 'baselines']
[189, 11, 'NarrativeQA']
[191, 12, 'We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , ASR and BiDAF .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NarrativeQA benchmark', 'observe that', '300d DCU']
[248, 29, 'Contrary to MCQ - based datasets , we found that reports our results on the NarrativeQA benchmark .']
[249, 30, 'First , we observe that 300d DCU can achieve comparable performance with BiDAF .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RACE benchmark dataset', 'has', 'Our proposed DCU model']
[220, 21, 'reports our results on the RACE benchmark dataset .']
[221, 22, 'Our proposed DCU model achieves the best result for both single models and ensemble models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Search QA dataset', 'achieve', 'same accuracy']
[242, 26, 'Table 2 reports our results on the Search QA dataset .']
[244, 27, 'We achieve the same accuracy as AMANDA without using any LSTM or GRU encoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NewsQA', 'has', 'key competitors']
[151, 8, 'NewsQA']
[155, 9, 'On this dataset , the key competitors are BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Quasar -T', 'has', 'key competitors']
[156, 10, 'Quasar -T']
[158, 11, 'The key competitors on this dataset are BiDAF and the Reinforced Ranker - Reader ( R 3 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SearchQA', 'has', 'competitor baselines']
[160, 12, 'SearchQA']
[164, 13, 'The competitor baselines on this dataset are Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Narrative QA', 'compare with', 'baselines']
[165, 14, 'Narrative QA ] is a recent QA dataset that involves comprehension over stories .']
[167, 15, 'We compare with the baselines in the original paper , namely Seq2Seq , Attention Sum Reader and BiDAF .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Quasar - T', 'has', 'Our model']
[196, 33, 'reports the results on Quasar - T .']
[197, 34, 'Our model achieves state - of - the - art performance on this dataset , outperforming the state - of - the - art R 3 ( Reinforced Ranker Reader ) by a considerable margin of + 4.4 % EM / + 6 % F1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['our model', 'can outperform', 'base R - NET']
[208, 37, 'SQuAD reports dev scores 8 of our model against several representative models on the popular SQuAD benchmark .']
[209, 38, 'While our model does not achieve state - of - the - art performance , our model can outperform the base R - NET ( both our implementation as well as the published score ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['our model', 'does not achieve', 'state - of - the - art performance']
[208, 37, 'SQuAD reports dev scores 8 of our model against several representative models on the popular SQuAD benchmark .']
[209, 38, 'While our model does not achieve state - of - the - art performance , our model can outperform the base R - NET ( both our implementation as well as the published score ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['re-rankers', 'are', 'strength - based re-ranker']
[38, 6, 'The re-rankers are :']
[39, 7, 'A strength - based re-ranker , which ranks the answer candidates according to how often their evidence occurs in different passages .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['PCFG - Trans', 'has', 'rule - based system']
[70, 10, 'PCFG - Trans']
[71, 11, 'The rule - based system 1 modified on the code released by .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NQG', 'extend', 's 2s+ att']
[75, 14, 'NQG']
[76, 15, 'We extend the s 2s+ att with our feature - rich encoder to build the NQG system .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NQG ++', 'Based on', 'NQG +']
[80, 19, 'NQG ++']
[81, 20, 'Based on NQG + , we use both pre-train word embedding and STshare methods , to further improve the performance .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['s 2 s+ att', 'implement', 'seq2seq with attention']
[73, 12, 's 2 s+ att']
[74, 13, 'We implement a seq2seq with attention as the baseline method .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['differentiable embedding', 'used by', 'question decoder']
[25, 5, 'Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding .']
[26, 6, 'This embedding is used by a question decoder to decode the appropriate question .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Effect of Data Augmentation', 'show', 'each augmentation step']
[100, 10, 'Effect of Data Augmentation']
[105, 11, "The tables show that each augmentation step affects the model 's efficiency negatively ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two enhancements', 'allow', 'entity information to further help downstream relation classification']
[29, 8, 'Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .']
[30, 9, 'These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two enhancements', 'alleviate', 'low - performance entity detection in early stages of training']
[29, 8, 'Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .']
[30, 9, 'These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['End - to - end Model', 'contains', 'LSTM - based decoding layer']
[96, 6, 'The End - to - end Model']
[99, 8, 'It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['biased loss', 'enhance', 'relevance of entity tags']
[99, 8, 'It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .']
[100, 9, 'The biased loss can enhance the relevance of entity tags .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['End - to - end Model', 'contains', 'bi-directional Long Short Term Memory ( Bi - LSTM ) layer']
[96, 6, 'The End - to - end Model']
[99, 8, 'It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LSTM- CRF', 'proposed for', 'entity recognition']
[175, 23, 'In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .']
[176, 24, 'LSTM - CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LSTM - LSTM', 'uses', 'LSTM layer']
[175, 23, 'In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .']
[177, 25, 'Different from LSTM - CRF , LSTM - LSTM uses a LSTM layer to decode the tag sequence instead of CRF .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['pipelined methods', 'has', 'FCM']
[169, 17, "For the pipelined methods , we follow ) 's settings :"]
[173, 21, '( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['pipelined methods', 'has', 'DS-logistic']
[169, 17, "For the pipelined methods , we follow ) 's settings :"]
[172, 20, '( 1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ;']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['pipelined methods', 'has', 'LINE']
[169, 17, "For the pipelined methods , we follow ) 's settings :"]
[172, 20, '( 1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ;']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Hyperparameters', 'bias parameter ?', '10']
[157, 14, 'The bias parameter ?']
[158, 15, 'corresponding to the results in is 10 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['softmax', 'leads to', '? 2 % decrease']
[265, 21, 'Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .']
[266, 22, 'Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['softmax', 'leads to', 'a slight decrease']
[265, 21, 'Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .']
[266, 22, 'Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DREC dataset', 'use', 'boundaries and the strict settings']
[248, 15, 'We also report results for the DREC dataset , with two different evaluation settings .']
[249, 16, 'Specifically , we use the boundaries and the strict settings .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DREC dataset', 'has', 'boundaries evaluation']
[248, 15, 'We also report results for the DREC dataset , with two different evaluation settings .']
[252, 17, 'In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Adversarial training ( AT )', 'exploit', 'idea of AT']
[66, 12, 'Adversarial training ( AT )']
[67, 13, 'We exploit the idea of AT as a regularization method to make our model robust to input perturbations .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['baseline model', 'aims to detect', 'type and the boundaries of the entities']
[35, 3, 'The baseline model , described in detail in , is illustrated in .']
[36, 4, 'It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['baseline model', 'aims to detect', 'relations between them']
[35, 3, 'The baseline model , described in detail in , is illustrated in .']
[36, 4, 'It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CoNLL04 dataset', 'has', 'baseline model']
[110, 20, 'For the CoNLL04 dataset , we use two evaluation settings .']
[112, 21, 'The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DREC dataset', 'In', 'boundaries evaluation']
[116, 22, 'For the DREC dataset , we use two evaluation methods .']
[117, 23, 'In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['adversarial training on top of the baseline model', 'for', 'ACE04']
[118, 24, 'and show the effectiveness of the adversarial training on top of the baseline model .']
[121, 26, 'Specifically , for ACE04 , there is an improvement in both tasks as well as in the over all F 1 performance ( 0.4 % ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TACRED dev set', 'find', 'entity representations and feedforward layers']
[178, 26, 'To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .']
[179, 27, 'We find that : The entity representations and feedforward layers contribute 1.0 F 1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Neural sequence model', 'presented', 'competitive sequence model']
[131, 9, 'Neural sequence model .']
[132, 10, 'Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Dependency - based models', 'has', 'logistic regression ( LR ) classifier']
[123, 5, 'Dependency - based models .']
[125, 6, '( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Effect of Path - centric Pruning', 'compare', 'two GCN models']
[167, 21, 'Effect of Path - centric Pruning']
[168, 22, 'To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Effect of Path - centric Pruning', 'compare', 'Tree - LSTM']
[167, 21, 'Effect of Path - centric Pruning']
[168, 22, 'To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TACRED Dataset', 'observe', 'our GCN model']
[148, 11, 'Results on the TACRED Dataset']
[150, 12, 'We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SemEval Dataset', 'under', 'conventional with- entity evaluation']
[162, 17, 'Results on the SemEval Dataset']
[164, 18, 'We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Results', 'of', 'QA']
[144, 21, 'The QA results are shown in .']
[147, 22, 'All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['QA', 'has', 'BioBERT v1.1 ( PubMed )']
[144, 21, 'The QA results are shown in .']
[147, 22, 'All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['QA', 'has', 'BioBERT']
[144, 21, 'The QA results are shown in .']
[147, 22, 'All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RE', 'has', 'BioBERT v1.0 ( PubMed )']
[140, 18, 'The RE results of each model are shown in .']
[142, 19, 'On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['NER', 'has', 'BioBERT']
[135, 15, 'The results of NER are shown in .']
[137, 16, 'On the other hand , BioBERT achieves higher scores than BERT on all the datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT SP with position embedding on the final attention layer', 'encode', 'paragraph']
[78, 10, 'BERT SP with position embedding on the final attention layer .']
[80, 11, 'In this method , the BERT model encode the paragraph to the last attention - layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT SP with position embedding on the final attention layer', 'for', 'each entity pair']
[78, 10, 'BERT SP with position embedding on the final attention layer .']
[81, 12, 'Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SemEval 2018 Task 7', 'has', 'Our Entity - Aware BERT SP']
[115, 16, 'The results on SemEval 2018 Task 7 are shown in .']
[116, 17, 'Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ACE 2005', 'first observation', 'our model architecture']
[83, 13, 'Results on ACE 2005']
[85, 14, 'The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Corpus', 'train', 'SCIBERT']
[33, 7, 'Corpus']
[34, 8, 'We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Biomedical Domain', 'observe', 'SCIBERT']
[105, 16, 'Biomedical Domain']
[106, 17, 'We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Computer Science Domain', 'observe', 'SCIBERT']
[117, 19, 'Computer Science Domain']
[118, 20, 'We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Multiple Domains', 'observe', 'SCIBERT']
[121, 22, 'Multiple Domains']
[122, 23, 'We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['feature - based structured perceptron model', 'employ', 'segment - based decoder']
[169, 10, 'The model proposed by is a feature - based structured perceptron model with efficient beam - search .']
[170, 11, 'They employ a segment - based decoder instead of token - based decoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Multiple Relations', 'find that', 'modifying our objective']
[135, 9, 'Multiple Relations']
[196, 20, 'We find that modifying our objective to include multiple relations improves the recall of our system on relations , leading to slight improvement on the over all performance on relations .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MIMLRE', 'has', 'graphical model']
[198, 9, 'MultiR : Probabilistic graphical model for multi instance learning by MIMLRE :']
[199, 10, 'A graphical model which jointly models multiple instances and multiple labels .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['cross - sentence n- ary relation extraction task', 'has', 'feature - based classifier']
[181, 10, 'For cross - sentence n- ary relation extraction task , we consider three kinds of models as baselines :']
[182, 11, '1 ) a feature - based classifier based on shortest dependency paths between all entity pairs , 2 ) Graph - structured LSTM methods , including Graph LSTM , bidirectional DAG LSTM ( Bidir DAG LSTM ) and Graph State LSTM ( GS GLSTM ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['cross - sentence n- ary relation extraction task', 'has', 'Graph - structured LSTM methods']
[181, 10, 'For cross - sentence n- ary relation extraction task , we consider three kinds of models as baselines :']
[182, 11, '1 ) a feature - based classifier based on shortest dependency paths between all entity pairs , 2 ) Graph - structured LSTM methods , including Graph LSTM , bidirectional DAG LSTM ( Bidir DAG LSTM ) and Graph State LSTM ( GS GLSTM ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SemEval dataset', 'has', 'Our C - AGGCN model ( 85.7 )']
[229, 23, 'We also evaluate our model on the SemEval dataset under the same settings as .']
[232, 24, 'Our C - AGGCN model ( 85.7 ) consistently outperforms the C - GCN model ( 84.8 ) , showing the good generalizability .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT SP with position embedding on the final attention layer', 'has', 'more straightforward way']
[78, 12, 'BERT SP with position embedding on the final attention layer .']
[79, 13, 'This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT SP', 'observed', '2 % gap']
[98, 21, 'For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .']
[99, 22, 'A 2 % gap is observed as expected .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['hop - 0 level', 'has', 'F 1 score']
[170, 28, 'We find that : ( 1 ) At hop - 0 level , precision increases as we provide more negative examples , while recall stays almost unchanged .']
[171, 29, 'F 1 score keeps increasing .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['position - aware term', 'contributes', 'about 1 % F 1 score']
[166, 24, 'The entire attention mechanism contributes about 1.5 % F 1 , where the position - aware term in Eq.']
[167, 25, '( 3 ) alone contributes about 1 % F 1 score .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Neural sequence model', 'has', 'competitive sequence model']
[131, 8, 'Neural sequence model .']
[132, 9, 'Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Dependency - based models', 'has', 'A logistic regression ( LR ) classifier']
[123, 4, 'Dependency - based models .']
[125, 5, '( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two GCN models and the Tree - LSTM', 'performance', 'peaks']
[169, 22, 'To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .']
[171, 23, 'As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['peaks', 'when', 'K = 1']
[169, 22, 'To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .']
[171, 23, 'As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TACRED Dataset', 'observe', 'GCN model']
[148, 10, 'Results on the TACRED Dataset']
[150, 11, 'We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SemEval Dataset', 'find that under', 'conventional with- entity evaluation']
[162, 18, 'Results on the SemEval Dataset']
[164, 19, 'We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Experimental setup', 'Entity - Relation Task weight', '0.3']
[196, 10, 'head , ? tail ) 0.5,0.5 Entity - Relation Task weight ?']
[197, 11, '0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ?']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Experimental setup', 'Entity - Task weights', '0.5']
[195, 9, 'Word embedding dimension k 50 POS embedding dimension l 5 Batch size n 50 Entity - Task weights ( ?']
[197, 11, '0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ?']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Experimental setup', 'l 2 penalty', '0.0001']
[197, 11, '0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ?']
[198, 12, '0.0001']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['several simplified models', 'has', 'first simplification']
[158, 11, 'To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .']
[159, 12, 'The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['first simplification', 'to use', 'model']
[158, 11, 'To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .']
[159, 12, 'The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['model', 'with', 'pooling attention layer']
[158, 11, 'To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .']
[159, 12, 'The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['model', 'without', 'input attention mechanism']
[158, 11, 'To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .']
[159, 12, 'The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['several simplified models', 'has', 'third']
[158, 11, 'To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .']
[161, 14, 'The third removes both forms of attention and additionally uses a regular objective function based on the inner product s = r w for a sentence representation r and relation class embedding w.']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['several simplified models', 'has', 'second']
[158, 11, 'To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .']
[160, 13, 'The second removes both attention mechanisms .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['contextual features', 'has', 'effect']
[287, 30, 'Next , we include contextual features to this network .']
[288, 31, 'Here , the effect of discourse features is primarily seen in the Pol dataset getting an increase of 3 % in F1 ( row 2 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['content - based CNN only', 'provides', 'worst relative performance']
[285, 28, 'First , we test performance for the content - based CNN only ( row 1 ) .']
[286, 29, 'This setting provides the worst relative performance with almost 10 % lesser accuracy than optimal .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['simple concatenation', 'causes', 'significant drop']
[292, 34, 'We challenge the use of CCA for the generation of user embeddings and thus replace it with simple concatenation .']
[293, 35, 'This however causes a significant drop in performance ( row 3 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN - SVM', 'consists of', 'CNN']
[263, 18, 'CNN - SVM :']
[264, 19, 'This model proposed by consists of a CNN for content modeling and other pre-trained CNNs for extracting sentiment , emotion and personality features from the given comment .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN - SVM', 'consists of', 'other pre-trained CNNs']
[263, 18, 'CNN - SVM :']
[264, 19, 'This model proposed by consists of a CNN for content modeling and other pre-trained CNNs for extracting sentiment , emotion and personality features from the given comment .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Bag - of - Words', 'uses', "comment 's"]
[257, 15, 'Bag - of - Words :']
[258, 16, "This model uses a comment 's word - counts as features in a vector ."]
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CUE - CNN', 'models', 'user embeddings']
[266, 20, 'CUE - CNN :']
[267, 21, 'This method proposed by also models user embeddings with a method akin to ParagraphVector .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Effectiveness', 'test', 'TRANX']
[23, 5, 'Effectiveness']
[24, 6, 'We test TRANX on four semantic parsing ( ATIS , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['following principles', 'has', 'Generalization ability']
[18, 2, 'TRANX is designed with the following principles in mind :']
[19, 3, 'Generalization ability TRANX employs ASTs as a general - purpose intermediate meaning representation , and the task - dependent grammar is provided to the system as external knowledge to guide the parsing process , therefore decoupling the semantic parsing procedure with specificities of grammars .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Semantic Parsing', 'has', 'Our system']
[125, 7, 'Semantic Parsing Tab.']
[128, 8, 'Our system outperforms existing neural network - based approaches .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Code Generation', 'has', 'TRANX']
[131, 10, 'Code Generation Tab.']
[133, 12, 'TRANX achieves state - of - the - art results on DJANGO .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['state - of - the - art results', 'on', 'DJANGO']
[132, 11, '2 lists the results on DJANGO .']
[133, 12, 'TRANX achieves state - of - the - art results on DJANGO .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['deep bidirectional LSTMs', 'differ by', 'using']
[13, 4, 'Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .']
[14, 5, 'However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['deep bidirectional LSTMs', 'differ by', 'ensembling']
[13, 4, 'Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .']
[14, 5, 'However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['deep bidirectional LSTMs', 'differ by', 'introducing']
[13, 4, 'Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .']
[14, 5, 'However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['deep bidirectional LSTMs', 'differ by', 'decoding']
[13, 4, 'Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .']
[14, 5, 'However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['deep bidirectional LSTMs', 'differ by', 'simplifying']
[13, 4, 'Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .']
[14, 5, 'However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BiLSTM Attention ( with and without ELMo )', 'optimizes', 'network']
[172, 21, 'BiLSTM Attention ( with and without ELMo ) .']
[173, 22, 'This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BiLSTM Attention ( with and without ELMo )', 'uses', 'similar architecture']
[172, 21, 'BiLSTM Attention ( with and without ELMo ) .']
[173, 22, 'This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Results', 'conclude that', 'translations']
[187, 24, 'We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .']
[188, 25, 'Overall , we conclude that translations are better additional contexts than topics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['translations', 'are', 'better additional contexts']
[187, 24, 'We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .']
[188, 25, 'Overall , we conclude that translations are better additional contexts than topics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['better additional contexts', 'than', 'topics']
[187, 24, 'We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .']
[188, 25, 'Overall , we conclude that translations are better additional contexts than topics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Abstractive seq2seq', 'is', 'abstractive sequence - to - sequence model']
[182, 17, 'Abstractive seq2seq :']
[183, 18, 'This is an abstractive sequence - to - sequence model trained on 3.8 million Gigaword title - article pairs as described in Section 1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Traditional ILP', 'is', 'ILP - based method']
[178, 15, 'Traditional ILP :']
[179, 16, 'This is the ILP - based method proposed by .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['multi - task learning', 'predicting both', 'CCG supertags and sentence compression']
[86, 8, 'Our baseline ( BASELINE - LSTM ) is a multi - task learning 1 http://groups.inf.ed.ac.uk/ccg/']
[87, 9, 'bi -LSTM predicting both CCG supertags and sentence compression ( word deletion ) at the outer layer .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MDRE', 'achieves', 'state - of - the - art performance']
[146, 13, 'Second , the newly proposed model , MDRE , shows a substantial performance gain .']
[147, 14, 'It thus achieves the state - of - the - art performance with a WAP value of 0.718 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['c- LSTM+ Att', 'has', 'variant attention']
[150, 12, 'c- LSTM+ Att :']
[151, 13, 'In this variant attention is applied applied to the c - LSTM output at each timestamp by following Eqs. and .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TFN', 'specific to', 'multimodal scenario']
[153, 14, 'TFN :']
[154, 15, 'This is specific to multimodal scenario .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TFN', 'has', 'Tensor outer product']
[153, 14, 'TFN :']
[155, 16, 'Tensor outer product is used to capture intermodality and intra-modality interactions .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Memnet', 'has', 'output']
[160, 20, 'Memnet : As described in , the current utterance is fed to a memory network , where the memories correspond to preceding utterances .']
[161, 21, 'The output from the memory network is used as the final utterance representation for emotion classification .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BiDialogueRNN', 'outperforms', 'Dialogue RNN']
[192, 29, 'BiDialogueRNN : Since BiDialogueRNN captures context from the future utterances , we expect improved performance from it over DialogueRNN .']
[193, 30, 'This is confirmed in , where BiDialogueRNN outperforms Dialogue RNN on average on both datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DialogueRNN vs. DialogueRNN Variants', 'has', 'DialogueRNN l']
[184, 26, 'DialogueRNN vs. DialogueRNN Variants']
[186, 27, 'DialogueRNN l :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DialogueRNN l', 'using', 'explicit listener state update']
[186, 27, 'DialogueRNN l :']
[187, 28, 'Following , using explicit listener state update yields slightly worse performance than regular DialogueRNN .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CMN Self', 'use', 'only self history']
[270, 22, '[ 1 , K ] } for ? ? {a , b}. CMN Self :']
[271, 23, 'In this baseline , we use only self history for classifying emotion of utterance u i .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['bc - LSTM', 'has', 'bi-directional LSTM']
[259, 19, 'bc - LSTM :']
[260, 20, 'A bi-directional LSTM equipped with hierarchical fusion , proposed by .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SVM - ensemble', 'has', 'strong context - free benchmark model']
[256, 17, 'SVM - ensemble :']
[257, 18, 'A strong context - free benchmark model which uses similar multimodal approach on an ensemble of trees .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Multi - task training', 'tune', 'our parameters']
[93, 8, 'Multi - task training']
[94, 9, 'We tune our parameters of learning rate , L2 regularization , whether to pre-train our model and batch size with the average accuracy of the development set of all datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Pre-training Emo2Vec', 'has', 'Emo2 Vec embedding matrix and the CNN model']
[86, 4, 'Pre-training Emo2Vec']
[87, 5, 'Emo2 Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['AE - LSTM', 'compare with', 'state - of - the - art attention - based LSTM']
[231, 6, 'AE - LSTM : RNN / LSTM is another popular attention based neural model .']
[232, 7, 'Here we compare with a state - of - the - art attention - based LSTM for ASC , AE - LSTM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ATAE - LSTM', 'has', 'attention - based LSTM']
[233, 8, 'ATAE - LSTM :']
[234, 9, 'Another attention - based LSTM for ASC reported in .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Target - sensitive Memory Networks ( TMNs )', 'has', 'six proposed techniques']
[235, 10, 'Target - sensitive Memory Networks ( TMNs ) :']
[236, 11, 'The six proposed techniques , NP , CNP , IT , CI , JCI , and JPI give six target - sensitive memory networks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['first approach', 'model', 'each target']
[27, 3, 'The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .']
[39, 4, 'To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['mixture', 'of', 'K aspect embeddings']
[27, 3, 'The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .']
[39, 4, 'To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Feature - based SVM', 'compare with', 'reported results']
[164, 9, '( 1 ) Feature - based SVM :']
[165, 10, 'We compare with the reported results of a top system in SemEval 2014 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LSTM', 'regards', 'average value']
[133, 12, 'LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .']
[134, 13, 'Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['hidden states', 'as', 'representation of sentence']
[133, 12, 'LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .']
[134, 13, 'Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['datasets Restaurant and Laptop', 'observe', 'our proposed PBAN model']
[144, 18, 'shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .']
[145, 19, 'We can observe that our proposed PBAN model achieves the best performance among all methods .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MSA Model', 'size of', 'embedding layer']
[78, 7, 'MSA Model ( message - level )']
[155, 9, 'The size of the embedding layer is 300 , and the LSTM layers 150 ( 300 for BiLSTM ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MSA Model', 'has', 'LSTM layers 150']
[78, 7, 'MSA Model ( message - level )']
[155, 9, 'The size of the embedding layer is 300 , and the LSTM layers 150 ( 300 for BiLSTM ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TSA Model', 'size of', 'LSTM layers']
[102, 8, 'TSA Model ( topic - based )']
[159, 12, 'The size of the embedding layer is 300 , and the LSTM layers 64 ( 128 for BiLSTM ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TSA Model', 'size of', 'embedding layer']
[102, 8, 'TSA Model ( topic - based )']
[159, 12, 'The size of the embedding layer is 300 , and the LSTM layers 64 ( 128 for BiLSTM ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['d-TBCNN model', 'achieves', '87.9 % accuracy']
[207, 11, 'Nonetheless , our d-TBCNN model achieves']
[208, 12, '87.9 % accuracy , ranking third in the list .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ATSA', 'has', 'IAN']
[191, 28, 'ATSA']
[196, 29, 'IAN has better performance than TD - LSTM and ATAE - LSTM , because two attention layers guides the representation learning of the context and the entity interactively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Embeddings', 'has', 'standard pre-trained word vectors']
[124, 6, 'Embeddings .']
[125, 7, 'The standard pre-trained word vectors used for English are the GloVe ones trained on 840 billion tokens of Common Crawl data 1 , while for other languages , we rely on the Facebook fastText Wikipedia embeddings as input representations .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['standard pre-trained word vectors', 'are', '300 - dimensional']
[125, 7, 'The standard pre-trained word vectors used for English are the GloVe ones trained on 840 billion tokens of Common Crawl data 1 , while for other languages , we rely on the Facebook fastText Wikipedia embeddings as input representations .']
[126, 8, 'All of these are 300 - dimensional .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['VADER', 'contain', 'separate domain - specific scores']
[134, 14, 'We consider a recent sentiment lexicon called VADER .']
[136, 15, 'These contain separate domain - specific scores for 250 different Reddit communities , and hence result in 250 - dimensional embeddings .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Non - RNN based baselines', 'has', 'Feature - based SVM']
[136, 15, 'Non - RNN based baselines :']
[137, 16, 'Feature - based SVM is a traditional support vector machine based model with extensive feature engineering .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Basic BERT - based model', 'has', 'BERT - SPC']
[152, 30, 'Basic BERT - based model :']
[153, 31, 'BERT - SPC feeds sequence " [ CLS ] + context + [ SEP ] + target + [ SEP ] " into the basic BERT model for sentence pair classification task .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['AEN - Glo Ve ablations', 'has', 'AEN - GloVe w/ o PCT']
[147, 25, 'AEN - Glo Ve ablations :']
[148, 26, 'AEN - GloVe w/ o PCT ablates PCT module .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ATAE - LSTM', 'use', 'LSTM']
[143, 21, 'ATAE - LSTM']
[144, 22, '( Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ATAE - LSTM', 'strengthens', 'effect of target embeddings']
[143, 21, 'ATAE - LSTM']
[144, 22, '( Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RNN based baselines', 'has', 'TD - LSTM']
[140, 19, 'RNN based baselines :']
[141, 20, 'TD - LSTM extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT BASE', 'treat', 'each utterance']
[205, 15, 'BERT BASE :']
[207, 16, 'We treat each utterance with its context as a single document .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT BASE', 'limit', 'document length']
[205, 15, 'BERT BASE :']
[208, 17, 'We limit the document length to the last 100 tokens to allow larger batch size .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['KET StdAttn', 'replace', 'dynamic contextaware affective graph attention']
[218, 23, 'KET StdAttn :']
[219, 24, 'We replace the dynamic contextaware affective graph attention by the standard graph attention .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DialogueRNN', 'models both', 'context and speakers information']
[210, 18, 'DialogueRNN : The stateof - the - art model for emotion detection in textual conversations .']
[211, 19, 'It models both context and speakers information .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN+cLSTM', 'has', 'c LSTM']
[203, 13, 'CNN+cLSTM : An CNN is used to extract utterance features .']
[204, 14, 'An c LSTM is then applied to learn context representations .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['KET SingleSelfAttn', 'replace', 'hierarchical self - attention']
[215, 20, 'KET SingleSelfAttn :']
[216, 21, 'We replace the hierarchical self - attention by a single self - attention layer to learn context representations .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Ent Net vs. our model', 'see', 'consistent performance gains']
[115, 11, 'Ent Net vs. our model .']
[116, 12, 'We see consistent performance gains for our model in both aspect detection and sentiment classification , compared to EntNet , esp. for aspect detection , underlining the benefit of delayed update gate activation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two novel neural units', 'other is', 'parameterized gate']
[21, 4, 'Specifically , we design two novel neural units that take target aspects into account .']
[22, 5, 'One is parameterized filter , the other is parameterized gate .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two novel neural units', 'One is', 'parameterized filter']
[21, 4, 'Specifically , we design two novel neural units that take target aspects into account .']
[22, 5, 'One is parameterized filter , the other is parameterized gate .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Non-Transfer', 'has', 'Target Network ( TN )']
[197, 17, 'Non-Transfer']
[198, 18, 'To demonstrate the benefits from coarse - tofine task transfer , we compare with the following state - of the - art AT - level methods without transfer : Target Network ( TN ) :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Target Network ( TN )', 'is', 'proposed base model ( BiLSTM + C2A + Pas )']
[198, 18, 'To demonstrate the benefits from coarse - tofine task transfer , we compare with the following state - of the - art AT - level methods without transfer : Target Network ( TN ) :']
[199, 19, 'It is our proposed base model ( BiLSTM + C2A + Pas ) trained on D t for the target task .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Transfer', 'has', 'Source- only ( SO )']
[201, 20, 'Transfer']
[203, 21, 'Source- only ( SO ) :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Source- only ( SO )', 'uses', 'source network']
[203, 21, 'Source- only ( SO ) :']
[204, 22, 'It uses a source network trained on D s to initialize a target network and then tests it on D t .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Comparison with Transfer', 'has', 'SO']
[223, 29, 'Comparison with Transfer']
[226, 30, 'SO performs poorly due to no adaptation applied .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Comparison with Non - Transfer', 'has', 'MGAN']
[213, 26, 'Comparison with Non - Transfer']
[219, 27, '( 2 ) MGAN consistently outperforms the MGAN w / o C2 F , where C2F module of the source network is removed and the source position information is missed ( we set all p s i to 1 ) , by 1.41 % , 1.03 % , 1.09 % for accuracy and 1.79 % , 3.62 % and 1.16 % for Macro - F1 on average .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Effect of C2F Attention Module', 'has', 'MGAN']
[232, 35, 'Effect of C2F Attention Module']
[236, 36, 'Then , compared with MGAN w / o C2F , MGAN further uses C2F to capture more specific aspect terms from the context towards the aspect category , such as " shells " to food seafood sea , which helps the source task capture more fine - grained semantics of aspect category and detailed position information like the target task , such that the sentiment attention can be positionaware and identify more relevant sentiment features towards the aspect .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['MGAN', 'compared with', 'MGAN w / o C2F']
[236, 36, 'Then , compared with MGAN w / o C2F , MGAN further uses C2F to capture more specific aspect terms from the context towards the aspect category , such as " shells " to food seafood sea , which helps the source task capture more fine - grained semantics of aspect category and detailed position information like the target task , such that the sentiment attention can be positionaware and identify more relevant sentiment features towards the aspect .']
[239, 37, 'While MGAN w / o C2F locates wrong sentiment contexts and fails in ( c ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN - ASP', 'is', 'CNN - based model']
[153, 11, 'CNN - ASP :']
[154, 12, 'It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ;']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TD - LSTM', 'performs', 'predictions']
[155, 13, 'TD - LSTM :']
[156, 14, 'It employs two LSTMs to model the left and right contexts of the target separately , then performs predictions based on concatenated context representations ;']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TD - LSTM', 'employs', 'two LSTMs']
[155, 13, 'TD - LSTM :']
[156, 14, 'It employs two LSTMs to model the left and right contexts of the target separately , then performs predictions based on concatenated context representations ;']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TD- LSTM', 'has', 'sequence']
[165, 9, 'TD- LSTM']
[166, 10, 'Following , sequence of words preceding ( left context ) and succeeding ( right context ) target aspect term are fed to two different LSTMs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['more accurate representation', 'name', 'refined representation']
[33, 5, 'Then , we employ memory networks to repeatedly match the target aspect representation with the other aspects to generate more accurate representation of the target aspect .']
[34, 6, 'This refined representation is fed to a softmax classifier for final classification .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Review Sentiment Analysis', 'has', 'The representation learned by our model']
[98, 4, 'Review Sentiment Analysis']
[104, 5, 'The representation learned by our model achieves 91.8 % significantly outperforming the state of the art of 90.2 % by a 30 model ensemble .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['L1 regularization', 'Fitting', 'threshold']
[111, 8, 'L1 regularization is known to reduce sample complexity when there are many irrelevant features .']
[116, 9, 'Fitting a threshold to this single unit achieves a test accuracy of 92.30 % which outperforms a strong supervised results on the dataset , the 91.87 % of NB - SVM trigram , but is still below the semi-supervised state of the art of 94.09 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Capacity Ceiling', 'try', 'approach']
[121, 11, 'Capacity Ceiling']
[123, 12, 'We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Logistic Regression', 'based on', 'linguistic features']
[160, 11, 'Logistic Regression']
[161, 12, 'Many existing works in the aspect - based sentiment analysis task , 3 use a classifier , such as logistic regression or SVM , based on linguistic features such as n-grams , POS information or more hand - engineered features .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['sparse representations of locations', 'name', 'Mask target entity n-grams']
[163, 13, 'More concretely , we define the following sparse representations of locations :']
[164, 14, 'Mask target entity n-grams :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['sparse representations of locations', 'name', 'Left - right n- grams']
[163, 13, 'More concretely , we define the following sparse representations of locations :']
[167, 15, 'Left - right n- grams : we create an n-gram representation for both the right and the left context around each location mention .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['sparse representations of locations', 'name', 'Left right pooling']
[163, 13, 'More concretely , we define the following sparse representations of locations :']
[169, 16, 'Left right pooling :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Long Short - Term Memory ( LSTM )', 'use', 'bidirectional LSTM']
[176, 17, 'Long Short - Term Memory ( LSTM )']
[177, 18, 'Inspired by the recent success of applying deep neural networks on language tasks , we use a bidirectional LSTM to learn a classifier for each of the aspects .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['location ( e l )', 'using', 'Final output state ( LSTM - Final )']
[178, 19, 'Representations for a location ( e l ) are obtained using one of the following two approaches :']
[179, 20, 'Final output state ( LSTM - Final ) : e l is the output embedding of the bidirectional LSTM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['location ( e l )', 'using', 'Location output state ( LSTM - Location )']
[178, 19, 'Representations for a location ( e l ) are obtained using one of the following two approaches :']
[180, 21, 'Location output state ( LSTM - Location ) :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Multi - hop vs No - hop', 'removal of', 'multi-hop']
[277, 36, 'Multi - hop vs No - hop : Variants 2 and 3 represent cases where multi-hop is omitted , i.e. , R = 1 . Performance for them are poorer than variants having multi-hop mechanism ( variants 4 - 7 ) .']
[278, 37, 'Also , removal of multi-hop leads to worse performance than the removal of DGIM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['multi-hop', 'leads to', 'worse performance']
[277, 36, 'Multi - hop vs No - hop : Variants 2 and 3 represent cases where multi-hop is omitted , i.e. , R = 1 . Performance for them are poorer than variants having multi-hop mechanism ( variants 4 - 7 ) .']
[278, 37, 'Also , removal of multi-hop leads to worse performance than the removal of DGIM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['worse performance', 'than', 'removal of DGIM']
[277, 36, 'Multi - hop vs No - hop : Variants 2 and 3 represent cases where multi-hop is omitted , i.e. , R = 1 . Performance for them are poorer than variants having multi-hop mechanism ( variants 4 - 7 ) .']
[278, 37, 'Also , removal of multi-hop leads to worse performance than the removal of DGIM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Self vs Dual History', 'Compared to', 'dual - history variants ( variants 3 , 5 , and 7 )']
[271, 33, 'Self vs Dual History :']
[273, 34, 'Compared to the dual - history variants ( variants 3 , 5 , and 7 ) , these models provide lesser performance .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ICON', 'has', 'performance']
[246, 25, 'For each emotion , ICON outperforms all the compared models except for happiness emotion .']
[247, 26, 'However , its performance is still at par with c LSTM without a significant gap .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Average Context', 'has', 'first one']
[148, 7, 'Average Context :']
[150, 9, 'The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Average Context', 'has', 'second one']
[148, 7, 'Average Context :']
[151, 10, 'The second one , named AC , averages the word vectors of the full context .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['second one', 'averages', 'word vectors']
[150, 9, 'The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .']
[151, 10, 'The second one , named AC , averages the word vectors of the full context .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['word vectors', 'of', 'full context']
[149, 8, 'There are two versions of this method .']
[150, 9, 'The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .']
[151, 10, 'The second one , named AC , averages the word vectors of the full context .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three LSTM models', 'In', 'LSTM']
[160, 10, '( 3 ) We compare with three LSTM models ( Tang et al. , 2015 a ) ) .']
[161, 11, 'In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three LSTM models', 'In', 'TDLSTM']
[160, 10, '( 3 ) We compare with three LSTM models ( Tang et al. , 2015 a ) ) .']
[161, 11, 'In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .']
[162, 12, 'TDLSTM extends LSTM by taking into account of the aspect , and uses two LSTM networks , a forward one and a backward one , towards the aspect .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['three LSTM models', 'In', 'TDLSTM + ATT']
[160, 10, '( 3 ) We compare with three LSTM models ( Tang et al. , 2015 a ) ) .']
[161, 11, 'In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .']
[163, 13, 'TDLSTM + ATT extends TDLSTM by incorporating an attention mechanism ( Bahdanau et al. , 2015 ) over the hidden vectors .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['dimension', 'set to', '300']
[171, 7, 'The dimension of word embedding d v and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.']
[172, 8, 'set to 300 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['300', 'of', 'word embedding d v']
[171, 7, 'The dimension of word embedding d v and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.']
[172, 8, 'set to 300 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['300', 'of', 'hidden stated']
[171, 7, 'The dimension of word embedding d v and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.']
[172, 8, 'set to 300 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['coefficient', 'of', 'L 2 normalization']
[103, 18, 'The dropout rate is 0.5 , and the coefficient ?']
[104, 19, 'of L 2 normalization is set to 10 ?5 . is set to 10 ? 4 . ?']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Recursive networks', 'has', 'Various types of recursive neural networks ( RNN )']
[128, 5, '2 ) Recursive networks :']
[129, 6, 'Various types of recursive neural networks ( RNN ) have been applied on SST .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Recurrent networks', 'has', 'Sophisticated recurrent networks']
[132, 7, '3 ) Recurrent networks :']
[133, 8, 'Sophisticated recurrent networks such as left - to - right and bidrectional LSTM networks have also been applied on SST .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Convolutional networks', 'has', 'input sequences']
[134, 9, '4 ) Convolutional networks :']
[135, 10, 'In this approach , the input sequences were passed through a 1 - dimensional convolutional neural network as feature extractors .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Word embeddings', 'has', 'word vectors']
[126, 3, '1 ) Word embeddings :']
[127, 4, 'In this method , the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector , which is then fed to the sentiment classifier to compute the sentiment score .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['various deep learning based architectures', 'combine them in', 'ensemble based architecture']
[14, 2, 'We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes .']
[15, 3, 'We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Text - only results', 'observe', 'performance of all the models']
[206, 18, 'Text - only results :']
[207, 19, 'We observe that the performance of all the models for this setting is similar .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Audio + Text results', 'combining', 'audio and text features']
[211, 20, 'c) Audio + Text results :']
[212, 21, 'We see that combining audio and text features gives us a boost of ? 14 % for all the metrics .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Audio - only results', 'has', 'Performance of LSTM and ARE']
[200, 16, 'Audio - only results :']
[202, 17, 'Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT - ADA', 'is', 'domain - adapted BERT - based model']
[206, 20, 'BERT - ADA']
[208, 22, 'is a domain - adapted BERT - based model proposed for the APC task , which finetuned the BERT - BASE model on task - related corpus .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ATSM -S', 'is', 'baseline model']
[194, 13, 'ATSM -S Peng et al.']
[195, 14, 'is a baseline model of the ATSM variations for Chinese language - oriented ABSA task .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['BERT - BASE', 'adapt it to', 'ABSA multi-task learning']
[202, 18, 'BERT - BASE is the basic pretrained BERT model .']
[203, 19, 'We adapt it to ABSA multi-task learning , which equips the same ability to automatically extract aspect terms and classify aspects polarity as LCF - ATEPC model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ATEPC - Fusion', 'show', 'performance']
[247, 30, 'ATEPC - Fusion is a supplementary scheme of LCF mechanism , and it adopts a moderate approach to generate local context features .']
[248, 31, 'The experimental results show that its performance is also better than the existing BERT - based models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['second stage', 'has', 'output layer']
[57, 4, 'In the second stage , a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions .']
[58, 5, 'The output layer consists of two units for smile or no smile .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Comparison with CAEVO', 'has', 'SP + ILP']
[238, 15, 'Comparison with CAEVO']
[248, 16, 'SP + ILP outperformed CAEVO and if additional unlabeled dataset TE3 - SV was used , CoDL + ILP achieved the best score with a relative improvement in F 1 score being 6.3 % .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TE3 Task C', 'has', 'AP + ILP']
[222, 12, 'TE3 Task C']
[231, 13, 'The improvement of SP + ILP ( line 4 ) over AP ( line 2 ) was small and AP + ILP ( line 3 ) was even worse than AP , which necessitates the use of a better approach towards vague TLINKs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TE3 Task C', 'improvement of', 'SP + ILP']
[222, 12, 'TE3 Task C']
[231, 13, 'The improvement of SP + ILP ( line 4 ) over AP ( line 2 ) was small and AP + ILP ( line 3 ) was even worse than AP , which necessitates the use of a better approach towards vague TLINKs .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TE3 Task C - Relation Only', 'see', 'UT - Time']
[204, 8, 'TE3 Task C - Relation Only']
[213, 9, 'We can see that UT - Time is about 3 % better than AP - 1 in the absolute value of F 1 , which is expected since UTTime included more advanced features derived from syntactic parse trees .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Semi-supervised experiments', 'tested', 'wv - 2 LSTMp']
[194, 9, 'Semi-supervised experiments']
[211, 10, 'Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Experiments ( supervised )', 'Comparing', 'two types of LSTM']
[123, 6, 'Experiments ( supervised )']
[138, 7, 'Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['bag of n-grams', 'achieving', 'comparable results']
[44, 7, 'Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .']
[45, 8, 'This is very efficient in practice while achieving comparable results to methods that explicitly use the order .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['bag of n-grams', 'in practice', 'very efficient']
[44, 7, 'Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .']
[45, 8, 'This is very efficient in practice while achieving comparable results to methods that explicitly use the order .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two classification models', 'first one is', 'standard CNN model']
[62, 4, 'We tried with two classification models .']
[63, 5, 'The first one is a standard CNN model similar to that of , using ReLU as non-linear activation function .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Experiment 2 : Cross-preprocessing', 'observe', 'different trend , with multiwordenhanced vectors']
[96, 10, 'Experiment 2 : Cross-preprocessing']
[99, 11, 'In this experiment we observe a different trend , with multiwordenhanced vectors exhibiting a better performance both on the single CNN model ( best over all performance in seven of the nine datasets ) and on the CNN + LSTM model ( best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Experiment 1 : Preprocessing effect', 'use of', 'more complex preprocessing techniques']
[93, 8, 'Experiment 1 : Preprocessing effect']
[94, 9, 'Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Answer Sentence Selection', 'has', 'our model']
[187, 21, 'Answer Sentence Selection']
[199, 22, 'Notably , our model yields significantly better results than an attentive pooling network and ABCNN ( attention - based CNN ) baselines .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Low - shot learning', 'On', 'IMDb and AG']
[192, 13, 'Low - shot learning']
[201, 14, 'On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Impact of LM quality', 'Using', 'our fine - tuning techniques']
[208, 17, 'Impact of LM quality']
[210, 18, 'Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['word embeddings from a word2 vec skip - gram model', 'input to', 'convolutional neural network models']
[83, 23, 'For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .']
[84, 24, 'The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['word embeddings from a word2 vec skip - gram model', 'input to', 'DAN']
[83, 23, 'For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .']
[84, 24, 'The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['general purpose', 'accomplished by', 'using multi-task learning']
[47, 8, 'The encoding model is designed to be as general purpose as possible .']
[48, 9, 'This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Deep Averaging Network ( DAN )', 'whereby', 'input embeddings for words and bi-grams']
[53, 10, 'Deep Averaging Network ( DAN )']
[54, 11, 'The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Dynamic Routing', 'to construct', 'non-linear map']
[83, 11, 'Dynamic Routing']
[84, 12, 'The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Fully Connected Capsule Layer', 'has', 'capsules in the layer below']
[116, 16, 'Fully Connected Capsule Layer']
[117, 17, 'The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Primary Capsule Layer', 'is', 'first capsule layer']
[57, 9, 'Primary Capsule Layer']
[58, 10, 'This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['N - gram Convolutional Layer', 'is a', 'standard convolutional layer']
[43, 7, 'N - gram Convolutional Layer']
[44, 8, 'This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Convolutional Capsule Layer', 'has', 'each capsule']
[106, 13, 'Convolutional Capsule Layer']
[107, 14, 'In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Architectures of Capsule Network', 'explore', 'two capsule architectures']
[123, 18, 'The Architectures of Capsule Network']
[124, 19, 'We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Text Sequence Matching', 'on', 'most of the datasets considered ( except WikiQA )']
[148, 13, 'Text Sequence Matching']
[151, 14, 'Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['SWEM - hier for sentiment analysis', 'has', 'word - order information']
[175, 16, 'SWEM - hier for sentiment analysis']
[176, 17, 'As demonstrated in Section 4.2.1 , word - order information plays a vital role for sentiment analysis tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Short Sentence Processing', 'has', 'SWEM']
[190, 20, 'Short Sentence Processing']
[194, 21, 'Compared with CNN / LSTM compositional functions , SWEM yields inferior accuracies on sentiment analysis datasets , consistent with our observation in the case of document categorization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['two sentence usability metrics', 'name', 'relative usability']
[45, 5, 'MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .']
[46, 6, '( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['neural attentionbased multiple context fixing attachment', 'is', 'series of modules']
[41, 2, 'Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .']
[42, 3, 'MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['effect of ensemble teacher model in knowledge distillation', 'boost', 'accuracy']
[151, 25, 'Furthermore , we study the effect of ensemble teacher model in knowledge distillation .']
[152, 26, 'As shown in , the ensemble teacher model can boost the accuracy by more than 1 % WER , compared with the single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder ) , which demonstrates the strong ensemble teacher model is essential to guarantee the performance of student model in knowledge distillation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['effect of distilling from unlabeled source words', 'boost', 'accuracy']
[144, 23, 'We first study the effect of distilling from unlabeled source words , as shown in .']
[145, 24, 'It can be seen that unlabeled source words can boost the accuracy by nearly 1 % WER , demonstrating the effectiveness by introducing abundant unlabeled data into knowledge distillation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Transformer', 'outperforms', 'RNN and CNN based models']
[153, 27, 'At last , we compare Transformer with RNN and CNN based models , without using knowledge distillation and unlabeled data , as shown in .']
[154, 28, 'We can see that Transformer model outperforms the RNN and CNN based models used in previous works , demonstrating the advantage of Transformer model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['knowledge distillation', 'train', 'teacher model']
[22, 3, 'First , we use knowledge distillation to leverage the large amount of unlabeled words .']
[23, 4, 'Specifically , we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence , and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data , and add them into the original training data .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['8 NVIDIA M40 GPUs', 'contains', 'roughly 4000 tokens']
[123, 17, 'We train each model on 8 NVIDIA M40 GPUs .']
[124, 18, 'Each GPU contains roughly 4000 tokens in one mini-batch .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Ensemble Model', 'use', '4 Transformer models']
[109, 7, 'Ensemble Model']
[111, 8, 'We use 4 Transformer models , 3 CNN models and 3 Bi - LSTM models with different hyperparameters for ensemble , which give the best performance on the validation set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Ensemble Model', 'use', '3 CNN models']
[109, 7, 'Ensemble Model']
[111, 8, 'We use 4 Transformer models , 3 CNN models and 3 Bi - LSTM models with different hyperparameters for ensemble , which give the best performance on the validation set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Ensemble Model', 'use', '3 Bi - LSTM models']
[109, 7, 'Ensemble Model']
[111, 8, 'We use 4 Transformer models , 3 CNN models and 3 Bi - LSTM models with different hyperparameters for ensemble , which give the best performance on the validation set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Student Model', 'use', 'default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder )']
[115, 12, 'Student Model']
[116, 13, 'We choose Transformer as the student model and use the default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ) unless otherwise stated .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Student Model', 'choose', 'Transformer']
[115, 12, 'Student Model']
[116, 13, 'We choose Transformer as the student model and use the default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ) unless otherwise stated .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CMUDict', 'can be seen', 'our method on 6 - layer encoder and 6 - layer decoder Transformer']
[131, 21, 'We first compare our method with previous works on CMUDict 0.7 b dataset , as shown in .']
[135, 22, 'It can be seen that our method on 6 - layer encoder and 6 - layer decoder Transformer achieves the new state - of - the - art result of 19.88 % WER , outperforming NSGD by 4.22 % WER .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['internal dataset', 'outperforms', 'CNN with NSGD']
[156, 29, 'We compare our method with the previous state - of - the - art CNN with NSGD ( which is reproduced by ourself ) on our internal dataset , as shown in .']
[157, 30, 'Our method outperforms CNN with NSGD by 3.52 % WER , which demonstrates the effectiveness of our method for G2P conversion .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN with NSGD', 'demonstrates', 'effectiveness of our method']
[156, 29, 'We compare our method with the previous state - of - the - art CNN with NSGD ( which is reproduced by ourself ) on our internal dataset , as shown in .']
[157, 30, 'Our method outperforms CNN with NSGD by 3.52 % WER , which demonstrates the effectiveness of our method for G2P conversion .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN with NSGD', 'by', '3.52 % WER']
[156, 29, 'We compare our method with the previous state - of - the - art CNN with NSGD ( which is reproduced by ourself ) on our internal dataset , as shown in .']
[157, 30, 'Our method outperforms CNN with NSGD by 3.52 % WER , which demonstrates the effectiveness of our method for G2P conversion .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['1D Convolution in FFT Block', 'replace', 'original fully connected layer']
[188, 18, '1D Convolution in FFT Block']
[189, 19, 'We propose to replace the original fully connected layer ( adopted in Transformer ) with 1D convolution in FFT block , as described in Section 3.1 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Sequence - Level Knowledge Distillation', 'removing', 'sequence - level knowledge distillation']
[192, 21, 'Sequence - Level Knowledge Distillation']
[195, 22, 'We find that removing sequence - level knowledge distillation results in - 0.325 CMOS , which demonstrates the effectiveness of sequence - level knowledge distillation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['length regulator', 'built on', 'phoneme duration predictor']
[29, 6, 'Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .']
[30, 7, 'The regulator is built on a phoneme duration predictor , which predicts the duration of each phoneme .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Audio Quality', 'conduct', 'MOS ( mean opinion score ) evaluation']
[141, 12, 'Audio Quality']
[142, 13, 'We conduct the MOS ( mean opinion score ) evaluation on the test set to measure the audio quality .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Voice Speed', 'demonstrated by', 'samples']
[165, 16, 'Voice Speed']
[168, 17, 'As demonstrated by the samples , FastSpeech can adjust the voice speed from 0.5x to 1.5 x smoothly , with stable and almost unchanged pitch .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Robustness', 'can be seen that', 'FastSpeech']
[154, 14, 'Robustness']
[158, 15, 'It can be seen that Transformer TTS is not robust to these hard cases and gets 34 % error rate , while FastSpeech can effectively eliminate word repeating and skipping to improve intelligibility .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Robustness', 'can be seen that', 'Transformer TTS']
[154, 14, 'Robustness']
[158, 15, 'It can be seen that Transformer TTS is not robust to these hard cases and gets 34 % error rate , while FastSpeech can effectively eliminate word repeating and skipping to improve intelligibility .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Number of speaker encoder training speakers', 'improve significantly', 'both naturalness and similarity']
[172, 19, 'Number of speaker encoder training speakers']
[184, 20, 'As the number of training speakers increases , both naturalness and similarity improve significantly .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Speech naturalness', 'has', 'proposed model']
[114, 6, 'Speech naturalness']
[122, 7, 'The proposed model achieved about 4.0 MOS in all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Speaker embedding space', 'has', 'PCA visualization']
[165, 16, 'Speaker embedding space']
[168, 17, 'The PCA visualization ( left ) shows that synthesized utterances tend to lie very close to real speech from the same speaker in the embedding space .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Fictitious speakers', 'conditioning', 'synthesizer']
[189, 21, 'Fictitious speakers']
[190, 22, 'Bypassing the speaker encoder network and conditioning the synthesizer on random points in the speaker embedding space results in speech from fictitious speakers which are not present in the train or test sets of either the synthesizer or the speaker encoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Fictitious speakers', 'Bypassing', 'speaker encoder network']
[189, 21, 'Fictitious speakers']
[190, 22, 'Bypassing the speaker encoder network and conditioning the synthesizer on random points in the speaker embedding space results in speech from fictitious speakers which are not present in the train or test sets of either the synthesizer or the speaker encoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Speaker similarity', 'scores for', 'VCTK model']
[130, 10, 'Speaker similarity']
[134, 11, 'The scores for the VCTK model tend to be higher than those for LibriSpeech , reflecting the cleaner nature of the dataset .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Speaker verification', 'trained on', 'LibriSpeech']
[151, 13, 'Speaker verification']
[159, 14, 'As shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on LibriSpeech , the synthesized speech is typically most similar to the ground truth voices .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['synthetic data', 'has', 'RankGAN']
[167, 10, 'Simulation on synthetic data']
[188, 11, 'It can be seen that the proposed RankGAN performs more favourably against the compared methods .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Chinese poems composition', 'estimate', 'similarity']
[202, 14, 'Results on Chinese poems composition']
[208, 15, 'Following the evaluation protocol in , we compute the BLEU - 2 score and estimate the similarity between the human - written poem and the machine - created one .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
["Shakespeare 's plays", 'has', 'proposed method']
[259, 23, "Results on Shakespeare 's plays"]
[265, 24, 'As can be seen , the proposed method achieves consistently higher BLEU score than the other methods in terms of the different n-grams criteria .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['COCO image captions', 'has', 'RankGAN']
[239, 18, 'Results on COCO image captions']
[248, 19, 'RankGAN achieves better performance than the other methods in terms of different BLEU scores .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['GAN Setting', 'choose', 'CNN architecture']
[162, 8, 'GAN Setting .']
[163, 9, 'For the discriminator , we choose the CNN architecture as the feature extractor and the binary classifier .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Short Text Generation : Chinese Poems', 'indicate', 'LeakGAN']
[208, 24, 'Short Text Generation : Chinese Poems']
[213, 25, 'The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Turing Test and Generated Samples', 'has', 'performance']
[222, 26, 'Turing Test and Generated Samples']
[230, 27, 'The performance on two datasets indicates that the generated sentences of Leak GAN are of higher global consistency and better readability than those of SeqGAN .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Long Text Generation : EMNLP2017 WMT News', 'In', 'all measured metrics']
[184, 20, 'Long Text Generation : EMNLP2017 WMT News']
[195, 21, 'In all measured metrics , LeakGAN shows significant performance gain compared to baseline models .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Synthetic Data Experiments', 'In', 'pre-training stage']
[177, 17, 'Synthetic Data Experiments']
[181, 18, '( i ) In the pre-training stage , LeakGAN has already shown observable performance superiority compared to other models , which indicates that the proposed hierarchical architecture itself brings improvement over the previous ones .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Middle Text Generation : COCO Image Captions', 'of', 'BLEU scores']
[197, 22, 'Middle Text Generation : COCO Image Captions']
[207, 23, 'The results of the BLEU scores on the COCO dataset indicate that Leak GAN performs significantly better than baseline models in mid-length text generation task .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['human evaluation', 'has', 'inter-annotator agreement']
[119, 16, 'shows the results of human evaluation .']
[120, 17, 'The inter-annotator agreement is satisfactory considering the difficulty of human evaluation .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['KL cost annealing strategy', 'set', 'initial weight']
[181, 16, 'Following , we use KL cost annealing strategy .']
[182, 17, 'We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNNs', 'set', 'convolution filter size']
[154, 5, 'For CNNs , we explore several different configurations .']
[155, 6, 'We set the convolution filter size to be 3 and gradually increase the depth and dilation from [ 1 , 2 , 4 ] , ] to .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['language modeling', 'For', 'SCNN , MCNN and LCNN']
[185, 18, 'The results for language modeling are shown in .']
[190, 19, 'For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['discriminator', 'increment of', '17.51 %']
[246, 21, 'The discriminator provides the scalar training signal L g c for generator training and the feature vector F ( m t ) for goal tracker .']
[247, 22, 'Consequently , there is an increment of 17.51 % from RASG w / o GTD to RASG w / o GT in terms of ROUGE - L , which demonstrates the effectiveness of discriminator .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Truncated Sampling', 'randomly samples', 'words']
[162, 11, 'Truncated Sampling']
[163, 12, 'This baseline randomly samples words from top - 10 candidates of the distribution at the decoding step .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Mixture Decoder', 'conducts', 'parallel greedy decoding']
[164, 13, 'Mixture Decoder']
[165, 14, 'This baseline constructs a hard - MoE of K decoders with uniform mixing coefficient ( referred as hMup in ) and conducts parallel greedy decoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Mixture Decoder', 'constructs', 'hard - MoE of K decoders']
[164, 13, 'Mixture Decoder']
[165, 14, 'This baseline constructs a hard - MoE of K decoders with uniform mixing coefficient ( referred as hMup in ) and conducts parallel greedy decoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Beam Search', 'keeps', 'K hypotheses']
[160, 9, 'Beam Search']
[161, 10, 'This baseline keeps K hypotheses with highest log-probability scores at each decoding step .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Mixture Selector ( Ours )', 'construct', 'hard - MoE of K SELECTORs']
[167, 15, 'Mixture Selector ( Ours )']
[168, 16, 'We construct a hard - MoE of K SELECTORs with uniform mixing coefficient that infers K different focus from source sequence .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['generic module', 'used as', 'plug - and - play']
[36, 7, 'We present a generic module called SELECTOR that is specialized for diversification .']
[37, 8, 'This module can be used as a plug - and - play to an arbitrary encoder - decoder model for generation without architecture change .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Diversity vs. Accuracy Trade - off', 'show', 'mixture SELECTOR method']
[207, 22, 'Diversity vs. Accuracy Trade - off compare our method with different diversitypromoting techniques in question generation and abstractive summarization .']
[208, 23, 'The tables show that our mixture SELECTOR method outperforms all baselines in Top - 1 and oracle metrics and achieves the best trade - off between diversity and accuracy .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['all baselines', 'in', 'Top - 1 and oracle metrics']
[207, 22, 'Diversity vs. Accuracy Trade - off compare our method with different diversitypromoting techniques in question generation and abstractive summarization .']
[208, 23, 'The tables show that our mixture SELECTOR method outperforms all baselines in Top - 1 and oracle metrics and achieves the best trade - off between diversity and accuracy .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Diversity vs. Number of Mixtures', 'compare', 'effect of number of mixtures']
[213, 25, 'Diversity vs. Number of Mixtures']
[214, 26, 'Here we compare the effect of number of mixtures in our SELECTOR and Mixture Decoder .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Soft - sharing vs. Hard - sharing', 'choose', 'soft - sharing']
[191, 9, 'Soft - sharing vs. Hard - sharing']
[192, 10, 'As described in Sec. 4.2 , we choose soft - sharing over hard - sharing because of the more expressive parameter sharing it provides to the model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Quantitative Improvements in Saliency Detection', 'results are', '2 - way - QG MTL model ( with question generation )']
[209, 14, 'Quantitative Improvements in Saliency Detection']
[212, 15, 'The results are shown in Table 10 , where the 2 - way - QG MTL model ( with question generation ) versus baseline improvement is stat. significant ( p < 0.01 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Qualitative Examples on Entailment and Saliency Improvements', 'has', '3 - way multi-task model']
[214, 16, 'Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .']
[217, 17, 'Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['3 - way multi-task model', 'generates', 'summaries']
[214, 16, 'Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .']
[217, 17, 'Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['summaries', 'are both better at', 'logical entailment']
[214, 16, 'Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .']
[217, 17, 'Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['summaries', 'are both better at', 'contain more salient information']
[214, 16, 'Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .']
[217, 17, 'Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Quantitative Improvements in Entailment', 'found', 'our 2 - way MTL model']
[202, 12, 'Quantitative Improvements in Entailment']
[207, 13, 'We found that our 2 - way MTL model with entailment generation reduces this extraneous count by 17.2 % w.r.t. the baseline .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Multi - Task with Entailment Generation', 'shows', 'multi-task setting']
[149, 6, 'Multi - Task with Entailment Generation']
[151, 7, '4 . shows that this multi-task setting is better than our strong baseline models and the improvements are statistically significant on all metrics 5 on both CNN / DailyMail ( p < 0.01 in ROUGE - 1 / ROUGE - L / METEOR and p < 0.05 in ROUGE - 2 ) and Gigaword ( p < 0.01 on all metrics ) datasets , showing that entailment generation task is inducing useful inference skills to the summarization task ( also see analysis examples in Sec. 7 ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Pointer + Coverage Baseline', 'On', 'Gigaword dataset']
[143, 4, 'Pointer + Coverage Baseline']
[148, 5, '4 On Gigaword dataset , our baseline model ( with pointer only , since coverage not needed for this single - sentence summarization task ) performs better than all previous works , as shown in .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['LCSTS', 'has', 'RNN and RNN - context']
[99, 11, 'Baselines for LCSTS are introduced in the following .']
[100, 12, 'RNN and RNN - context are the RNNbased seq2seq models , without and with attention mechanism respectively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['RNN and RNN - context', 'are', 'RNNbased seq2seq models']
[99, 11, 'Baselines for LCSTS are introduced in the following .']
[100, 12, 'RNN and RNN - context are the RNNbased seq2seq models , without and with attention mechanism respectively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['s 2 s+ att', 'implement', 'sequence - to sequence model']
[178, 17, 's 2 s+ att']
[179, 18, 'We also implement a sequence - to sequence model with attention as our baseline and denote it as " s2 s + att " .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Adam', 'learning rate', '0.001']
[164, 11, 'We use Adam as our optimizing algorithm .']
[165, 12, 'For the hyperparameters of Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Adam', 'two momentum parameters', '? 1 = 0.9 and ? 2 = 0.999']
[164, 11, 'We use Adam as our optimizing algorithm .']
[165, 12, 'For the hyperparameters of Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DUC 2004', 'has', 'SEASS']
[138, 9, 'DUC 2004']
[198, 25, 'As summarized in , our SEASS outperforms all the baseline methods and achieves 29.21 , 9.56 and 25.51 for ROUGE 1 , 2 and L recall .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Attention Heatmaps', 'In', 'HIER']
[244, 20, 'Attention Heatmaps']
[249, 21, 'In HIER , we observe that the attention becomes washed out ( in accord with its high entropy ) and is essentially averaging all of the encoder hidden states .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Sharpness of Attention', 'compute', 'entropy numbers']
[233, 16, 'Sharpness of Attention']
[236, 17, 'We compute the entropy numbers by averaging over all generated words in the validation set .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Could the entailment recognition also be improved ?', 'shows', 'our summarization model']
[204, 35, '6.6.3 Could the entailment recognition also be improved ?']
[207, 36, 'shows that our summarization model with MTL outperforms basic seq2seq model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Does our summarization model learn entailment knowledge ?', 'For', 'test set']
[188, 26, 'Does our summarization model learn entailment knowledge ?']
[191, 27, 'For the test set of , the average entailment score for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Is it less abstractive for our model ?', 'shows that', 'seq2seq model']
[197, 32, 'Is it less abstractive for our model ?']
[201, 33, 'shows that the seq2seq model produces more novel words ( i.e. , words that do not appear in the article ) than our model , indicating a lower degree of abstraction for our model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Seq2seq + RL', 'implement', 'Reinforcement Learning ( RL ) models']
[163, 18, 'Seq2seq + RL .']
[164, 19, 'We implement Reinforcement Learning ( RL ) models ( policy gradient ) with reward metrics of Entailment and ROUGE - 2 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Seq2seq', 'is a', 'standard seq2seq model']
[153, 8, 'Seq2seq .']
[154, 9, 'This is a standard seq2seq model with attention mechanism .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Seq2seq + selective', 'employ', 'selective encoding model']
[124, 5, 'employ a selective encoding model to control the information flow from encoder to decoder .']
[165, 20, 'Seq2seq + selective .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Seq2seq + MTL', 'with', 'entailment - aware encoder']
[155, 10, 'Seq2seq + MTL .']
[156, 11, 'This is our proposed model with entailment - aware encoder , which applies a multi-task learning ( MTL ) framework to seq2seq model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Seq2seq + MTL ( Share decoder )', 'propose', 'multi - task learning ( MTL ) framework']
[157, 12, 'Seq2seq + MTL ( Share decoder ) .']
[158, 13, 'propose a multi - task learning ( MTL ) framework in which the decoder is shared for summarization generation and entailment generation task .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Seq2seq + ROUGE -2 RAML', 'apply', 'ROUGE - 2 RAML training']
[161, 16, 'Seq2seq + ROUGE -2 RAML .']
[162, 17, 'We apply ROUGE - 2 RAML training for seq2seq model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Seq2seq + ERAML', 'with', 'entailment - aware decoder']
[159, 14, 'Seq2seq + ERAML .']
[160, 15, 'This is our proposed model with entailment - aware decoder , which conducts an Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Results', 'on', 'DUC 2004']
[176, 24, 'Experimental Results : DUC 2004']
[180, 25, 'In , experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models , surpassing Feats2s by 0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L without fine - tuning on DUC data .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DUC 2004', 'show', 'Seq2seq + selective + MTL + ERAML model']
[176, 24, 'Experimental Results : DUC 2004']
[180, 25, 'In , experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models , surpassing Feats2s by 0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L without fine - tuning on DUC data .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Gigaword Corpus', 'has', 'Our model']
[171, 22, 'Experimental Results : Gigaword Corpus']
[175, 23, 'Our model performs better than the previous works .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Gigaword valid - 2000 dataset', 'present', 'R - 1 , R - 2 , and R - L scores']
[214, 4, 'We first report results on the Gigaword valid - 2000 dataset in .']
[215, 5, 'We present R - 1 , R - 2 , and R - L scores ) that respectively measures the overlapped unigrams , bigrams , and longest common subsequences between the system and reference summaries 3 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Gigaword dataset', 'compare our models with', 'Feat2s']
[188, 16, 'For the Gigaword dataset , we compare our models with the following abstractive baselines :']
[189, 17, 'ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Gigaword dataset', 'compare our models with', 'Luong - NMT']
[188, 16, 'For the Gigaword dataset , we compare our models with the following abstractive baselines :']
[189, 17, 'ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Gigaword dataset', 'compare our models with', 'RAS - Elman']
[188, 16, 'For the Gigaword dataset , we compare our models with the following abstractive baselines :']
[189, 17, 'ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Gigaword dataset', 'compare our models with', 'ABS +']
[188, 16, 'For the Gigaword dataset , we compare our models with the following abstractive baselines :']
[189, 17, 'ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Gigaword dataset', 'compare our models with', 'SEASS']
[188, 16, 'For the Gigaword dataset , we compare our models with the following abstractive baselines :']
[189, 17, 'ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN dataset', 'compare our models with', 'Distraction - M3']
[190, 18, 'For the CNN dataset , we compare our models with the following extractive and abstractive baselines :']
[191, 19, 'Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN dataset', 'compare our models with', 'GBA']
[190, 18, 'For the CNN dataset , we compare our models with the following extractive and abstractive baselines :']
[191, 19, 'Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN dataset', 'compare our models with', 'Lead - 3']
[190, 18, 'For the CNN dataset , we compare our models with the following extractive and abstractive baselines :']
[191, 19, 'Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN dataset', 'compare our models with', 'Bi - GRU']
[190, 18, 'For the CNN dataset , we compare our models with the following extractive and abstractive baselines :']
[191, 19, 'Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN dataset', 'compare our models with', 'LexRank']
[190, 18, 'For the CNN dataset , we compare our models with the following extractive and abstractive baselines :']
[191, 19, 'Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['E2T', 'encodes', 'entities extracted from the original text']
[31, 3, 'E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .']
[32, 4, 'The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['E2T', 'constructs', 'vector']
[31, 3, 'E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .']
[32, 4, 'The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['E2T', 'informs', 'decoder']
[31, 3, 'E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .']
[32, 4, 'The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['candidate template', 'with', 'highest predicted informativeness']
[34, 7, 'In Rerank , we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence .']
[35, 8, 'The candidate template with the highest predicted informativeness is regarded as the actual soft template .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['OpenNMT', 'implement', 'standard attentional seq2seq model']
[150, 18, 'OpenNMT']
[151, 19, 'We also implement the standard attentional seq2seq model with OpenNMT .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['PIPELINE', 'trains', 'Rerank module']
[156, 22, 'In addition , to evaluate the effectiveness of our joint learning framework , we develop a baseline named " PIPELINE " .']
[158, 23, 'However , it trains the Rerank module and Rewrite module in pipeline .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['PIPELINE', 'trains', 'Rewrite module']
[156, 22, 'In addition , to evaluate the effectiveness of our joint learning framework , we develop a baseline named " PIPELINE " .']
[158, 23, 'However , it trains the Rerank module and Rewrite module in pipeline .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['linguistic quality', 'performance of', 'Re 3 Sum']
[182, 30, 'We also measure the linguistic quality of generated summaries from various aspects , and the results are present in .']
[183, 31, 'As can be seen from the rows " LEN DIF " and " LESS 3 " , the performance of Re 3 Sum is almost the same as that of soft templates .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['five types of different soft templates', 'performance of', 'Random']
[163, 25, 'We introduce five types of different soft templates :']
[175, 26, 'As shown in , the performance of Random is terrible , indicating it is impossible to use one summary template to fit various actual summaries .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['summaries', 'find', 'outputs of Re 3 Sum']
[209, 34, 'Next , we manually inspect the summaries generated by different methods .']
[210, 35, 'We find the outputs of Re 3 Sum are usually longer and more flu - ent than the outputs of OpenNMT .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['compressive text summarization', 'combines', 'system using linguistic based transformations']
[191, 8, 'TOPIARY is the best on DUC2004 Task - 1 for compressive text summarization .']
[192, 9, 'It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['compressive text summarization', 'combines', 'an unsupervised topic detection algorithm']
[191, 8, 'TOPIARY is the best on DUC2004 Task - 1 for compressive text summarization .']
[192, 9, 'It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['English dataset Gigawords', 'maximum length of', 'documents and summaries']
[208, 20, 'For the experiments on the English dataset Gigawords , we set the dimension of word embeddings to 300 , and the dimension of hidden states and latent variables to 500 .']
[209, 21, 'The maximum length of documents and summaries is 100 and 50 respectively .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['English dataset Gigawords', 'has', 'batch size']
[208, 20, 'For the experiments on the English dataset Gigawords , we set the dimension of word embeddings to 300 , and the dimension of hidden states and latent variables to 500 .']
[210, 22, 'The batch size of mini-batch training is 256 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['batch size', 'of', 'mini-batch training']
[208, 20, 'For the experiments on the English dataset Gigawords , we set the dimension of word embeddings to 300 , and the dimension of hidden states and latent variables to 500 .']
[210, 22, 'The batch size of mini-batch training is 256 .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['ROUGE Evaluation', 'on', 'Chinese dataset LCSTS']
[228, 30, 'ROUGE Evaluation']
[229, 31, 'The results on the Chinese dataset LCSTS are shown in .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Chinese dataset LCSTS', 'has', 'Our model DRGD']
[229, 31, 'The results on the Chinese dataset LCSTS are shown in .']
[230, 32, 'Our model DRGD also achieves the best performance .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['CNN - DM corpus', 'using', 'coverage inference penalty']
[191, 13, '3 . shows our main results on the CNN - DM corpus , with abstractive models shown in the top , and bottom - up attention methods at the bottom .']
[192, 14, 'We first observe that using a coverage inference penalty scores the same as a full coverage mechanism , without requiring any additional model parameters or model fine - tuning .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['DUC - 2004', 'show', 'our models']
[117, 17, 'On DUC - 2004 we report recall ROUGE as is customary on this dataset .']
[118, 18, 'The results ( Table 3 ) show that our models are better than ABS + .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['TF - IDF', 'is a', 'standard term frequency - inverse document frequency ( TF - IDF ) based document representation']
[305, 12, '4 ) TF - IDF :']
[306, 13, 'The fourth baseline system is a standard term frequency - inverse document frequency ( TF - IDF ) based document representation , followed by multi-class logistic regression ( LR ) .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Fisher speech corpora', 'see that', 'GLCU']
[361, 14, 'presents the classification results on Fisher speech corpora with manual and automatic transcriptions , where the first two rows are the results from earlier published works .']
[366, 15, 'We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['Fisher speech corpora', 'see that', 'our proposed systems']
[361, 14, 'presents the classification results on Fisher speech corpora with manual and automatic transcriptions , where the first two rows are the results from earlier published works .']
[366, 15, 'We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['20 Newsgroups dataset', 'see that', 'topic ID systems based on Bayesian SMM and logistic regression']
[369, 16, 'presents classification results on 20 Newsgroups dataset .']
[375, 17, 'We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models , except for the purely discriminative CNN model .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
['20 Newsgroups dataset', 'see that', 'topic ID systems based on Bayesian SMM']
[369, 16, 'presents classification results on 20 Newsgroups dataset .']
[375, 17, 'We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models , except for the purely discriminative CNN model .']
[376, 18, 'We can also see that all the topic ID systems based on Bayesian SMM are consistently better than variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM .']
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
