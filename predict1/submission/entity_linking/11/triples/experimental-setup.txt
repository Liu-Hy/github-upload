(BERT||used||model named)
(" bert - largecased "||of||PyTorch implementation)
(vectors||of||dimension 1024)
(PyTorch implementation||consists of||vectors)
(vectors||of||dimension 1024)
(vectors||trained on||Book s Corpus)
(vectors||trained on||English Wikipedia)
(BERT||has||model named)
(model named||has||" bert - largecased ")
(Experimental setup||For||BERT)
(Transformer encoder layers||used||same parameters)
(same parameters||as||" base " model)
(6 layers||with||8 attention heads)
(6 layers||with||dropout)
(Transformer encoder layers||has||same parameters)
(same parameters||has||" base " model)
(Experimental setup||For||Transformer encoder layers)
(Experimental setup||trained on||one Nvidia 's Titan X GPU)
(Contribution||has||Experimental setup)
