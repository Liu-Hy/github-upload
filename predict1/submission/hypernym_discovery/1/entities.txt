2	35	60	Neural Hypernym Discovery
4	23	41	hypernym discovery
13	4	22	hypernym discovery
17	23	47	representation for terms
17	50	59	including
17	60	77	words and phrases
17	90	106	phrase embedding
17	107	135	could not be obtained byword
23	18	27	introduce
23	30	57	neural network architecture
23	58	61	for
23	66	80	concerned task
23	85	102	empirically study
23	103	126	various neural networks
23	127	135	to model
23	140	167	distributed representations
23	168	171	for
23	172	189	words and phrases
83	0	9	Our model
83	14	31	implemented using
83	36	42	Theano
84	4	20	diagonal variant
84	21	23	of
84	24	34	Ada - Grad
84	38	46	used for
84	47	70	neural network training
87	4	20	hidden dimension
87	21	23	of
87	24	41	all neural models
87	42	45	are
87	46	49	200
88	4	14	batch size
88	18	24	set to
88	25	27	20
88	36	76	word embedding and sense embedding sizes
88	81	87	set to
88	88	91	300
89	22	32	trained on
89	35	67	single GPU ( NVIDIA GTX 980 Ti )
89	70	74	with
89	75	87	roughly 1.5h
89	88	91	for
89	92	117	general - purpose subtask
89	118	121	for
89	122	129	English
89	180	183	for
89	184	201	medical and music
97	8	15	observe
97	16	47	CNN - based network performance
97	51	62	better than
97	63	74	RNN - based
100	0	21	All the neural models
100	22	32	outperform
100	33	57	term embedding averaging
100	58	69	in terms of
100	70	85	all the metrics
100	115	123	performs
100	124	130	better
100	131	135	than
100	136	152	RNN - based ones
100	153	155	in
100	156	175	most of the metrics
100	176	181	using
100	182	196	word embedding
101	0	13	Compared with
101	14	28	word embedding
101	35	50	sense embedding
101	51	56	shows
101	59	77	much poorer result
