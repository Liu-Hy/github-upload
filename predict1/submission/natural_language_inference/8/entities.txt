2	9	35	Natural Language Inference
4	19	27	proposed
4	30	61	sentence encoding - based model
4	62	77	for recognizing
4	78	93	text entailment
11	0	4	With
11	5	30	less number of parameters
11	33	42	our model
11	43	55	outperformed
11	60	108	existing best sentence encoding - based approach
11	109	111	by
11	114	126	large margin
13	40	75	recognizing text entailment ( RTE )
30	19	27	proposed
30	30	61	unified deep learning framework
30	62	77	for recognizing
30	78	96	textual entailment
31	19	27	based on
31	28	53	building biL - STM models
77	4	22	training objective
77	36	38	is
77	39	59	cross - entropy loss
77	69	72	use
77	73	86	minibatch SGD
77	87	91	with
77	96	103	Rmsprop
77	135	138	for
77	139	151	optimization
78	4	14	batch size
78	15	17	is
78	18	21	128
79	2	15	dropout layer
79	20	30	applied in
79	35	41	output
79	42	44	of
79	49	56	network
79	57	61	with
79	66	78	dropout rate
79	79	85	set to
79	86	90	0.25
80	18	22	used
80	23	57	pretrained 300D Glove 840B vectors
80	58	71	to initialize
80	76	90	word embedding
81	0	27	Out - of - vocabulary words
81	28	30	in
81	35	47	training set
81	52	72	randomly initialized
81	73	84	by sampling
81	85	91	values
81	92	101	uniformly
81	102	106	from
81	107	122	( 0.05 , 0.05 )
85	4	8	Keep
85	15	29	representation
85	30	44	stays close to
85	45	65	unseen similar words
85	66	68	in
85	69	83	inference time
85	92	100	improved
85	105	110	model
