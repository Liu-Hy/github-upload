(each document||into||sentences)
(sentences||using||sentence tokenizer)
(sentence tokenizer||of||NLTK toolkit)
(Experimental setup||split||each document)
(each sentence||using||word tokenizer)
(word tokenizer||of||NLTK)
(tokenize||has||each sentence)
(Experimental setup||further||tokenize)
(Experimental setup||implemented using||Python and Tensorflow)
(weights||of||model)
(weights||initialized by||Glorot Initialization)
(model||initialized by||Glorot Initialization)
(biases||initialized with||zeros)
(Experimental setup||has||weights)
(300 dimensional word vectors||from||GloVe)
(GloVe||with||840 billion pre-trained vectors)
(300 dimensional word vectors||to initialize||word embeddings)
(Experimental setup||use||300 dimensional word vectors)
(sampling||from||uniform random distribution)
(Experimental setup||do not appear in||Glove)
(dropout||between||layers)
(dropout||with||keep probability)
(keep probability||of||0.8)
(Experimental setup||apply||dropout)
(number of hidden units||set to||100)
(number of hidden units||has||100)
(Experimental setup||has||number of hidden units)
(' sample size ' ( number of relevant sentences )||based on||model performance)
(model performance||on||devset)
(hyperparameter||has||' sample size ' ( number of relevant sentences ))
(Experimental setup||has||hyperparameter)
(Contribution||has||Experimental setup)
