2	49	70	Reading Comprehension
8	43	50	propose
8	54	82	N - best re-ranking strategy
8	83	98	to double check
8	103	111	validity
8	112	114	of
8	119	129	candidates
8	154	165	performance
20	19	26	present
20	29	62	novel neural network architecture
20	65	71	called
20	72	106	attention - over - attention model
26	8	15	propose
26	19	47	N - best re-ranking strategy
26	48	59	to re-score
26	64	74	candidates
26	75	77	in
26	78	93	various aspects
26	118	129	performance
57	0	21	Children 's Book Test
141	0	15	Embedding Layer
142	4	21	embedding weights
142	22	25	are
142	26	46	randomly initialized
142	56	78	uniformed distribution
142	79	81	in
142	86	113	interval [ ? 0.05 , 0.05 ].
145	0	7	CB Test
148	0	12	Hidden Layer
148	15	31	Internal weights
148	32	34	of
148	35	39	GRUs
148	44	60	initialized with
148	61	87	random orthogonal matrices
150	3	10	adopted
150	11	25	ADAM optimizer
150	26	29	for
150	30	45	weight updating
150	48	52	with
150	56	77	initial learning rate
150	78	80	of
150	81	86	0.001
151	7	16	GRU units
151	23	34	suffer from
151	39	64	gradient exploding issues
151	70	73	set
151	78	105	gradient clipping threshold
151	106	108	to
151	109	110	5
152	3	7	used
152	8	33	batched training strategy
152	34	36	of
152	37	47	32 samples
154	0	2	In
154	3	18	re-ranking step
154	24	32	generate
154	33	46	5 - best list
154	47	51	from
154	56	85	baseline neural network model
155	4	27	language model features
155	32	42	trained on
155	47	66	training proportion
155	67	69	of
155	70	82	each dataset
155	85	89	with
155	90	116	8 - gram wordbased setting
155	121	143	Kneser - Ney smoothing
155	144	154	trained by
155	155	168	SRILM toolkit
157	4	18	ensemble model
157	22	32	made up of
157	33	49	four best models
157	62	75	trained using
157	76	97	different random seed
158	0	14	Implementation
158	18	27	done with
158	28	79	Theano ( Theano Development Team , 2016 ) and Keras
158	101	111	trained on
163	21	35	our AoA Reader
163	36	47	outperforms
163	48	78	state - of - the - art systems
163	79	81	by
163	84	96	large margin
163	99	104	where
163	105	142	2.3 % and 2.0 % absolute improvements
163	143	147	over
163	148	157	EpiReader
163	158	160	in
163	161	187	CBTest NE and CN test sets
163	196	207	demonstrate
164	8	14	adding
164	15	34	additional features
164	35	37	in
164	42	57	re-ranking step
164	60	68	there is
164	69	109	another significant boost 2.0 % to 3.7 %
164	110	114	over
164	115	126	Ao A Reader
164	127	129	in
164	130	154	CBTest NE / CN test sets
165	13	18	found
165	24	40	our single model
165	47	51	stay
165	52	58	on par
165	59	63	with
165	68	97	previous best ensemble system
165	120	140	absolute improvement
165	141	143	of
165	144	149	0.9 %
165	150	156	beyond
165	161	204	best ensemble model ( Iterative Attention )
165	205	207	in
165	212	236	CBTest NE validation set
166	17	31	ensemble model
166	34	48	our AoA Reader
166	54	59	shows
166	60	84	significant improvements
166	85	89	over
166	90	119	previous best ensemble models
166	120	122	by
166	125	137	large margin
166	142	147	setup
166	150	183	new state - of - the - art system
168	17	47	pre-defined merging heuristics
168	54	61	letting
168	66	71	model
168	72	88	explicitly learn
168	93	100	weights
168	101	108	between
168	109	130	individual attentions
168	131	141	results in
168	144	161	significant boost
168	162	164	in
168	169	180	performance
168	233	236	CNN
168	261	268	against
168	269	279	CAS Reader
