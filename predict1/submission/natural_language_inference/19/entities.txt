2	0	26	Natural Language Inference
4	19	26	propose
4	31	46	TBCNNpair model
4	47	59	to recognize
4	60	88	entailment and contradiction
4	89	96	between
4	97	110	two sentences
8	113	147	natural language inference ( NLI )
33	19	26	propose
33	31	56	TBCNN - pair neural model
33	57	69	to recognize
33	70	98	entailment and contradiction
33	99	106	between
33	107	120	two sentences
128	24	33	including
128	34	44	embeddings
128	52	58	set to
128	59	73	300 dimensions
129	4	9	model
129	10	12	is
129	13	26	mostly robust
129	27	31	when
129	36	45	dimension
129	46	48	is
129	49	79	large , e.g. , several hundred
130	0	15	Word embeddings
130	21	44	pretrained ourselves by
130	45	53	word2vec
130	54	56	on
130	61	85	English Wikipedia corpus
130	90	108	fined tuned during
130	109	117	training
130	118	120	as
130	121	126	apart
130	130	146	model parameters
131	3	10	applied
131	11	20	2 penalty
131	21	23	of
131	24	31	310 ? 4
131	34	41	dropout
131	46	55	chosen by
131	56	66	validation
131	67	71	with
131	86	88	of
131	89	92	0.1
134	0	21	Initial learning rate
134	26	32	set to
134	33	34	1
134	43	54	power decay
134	59	66	applied
135	3	7	used
135	8	35	stochastic gradient descent
135	36	40	with
135	43	53	batch size
135	54	56	of
135	57	59	50
138	14	39	TBCNN sentence pair model
138	42	53	followed by
138	54	80	simple concatenation alone
138	83	94	outperforms
138	95	140	existing sentence encoding - based approaches
138	143	150	without
138	151	162	pretraining
138	167	176	including
138	179	200	feature - rich method
138	201	206	using
138	207	215	6 groups
138	216	218	of
138	219	243	humanengineered features
138	246	268	long short term memory
140	0	13	Model Variant
144	17	31	each heuristic
144	45	50	using
144	51	79	element - wise product alone
144	80	82	is
144	83	102	significantly worse
144	103	107	than
144	108	150	concatenation or element - wise difference
145	0	9	Combining
145	10	39	different matching heuristics
145	40	48	improves
145	53	59	result
145	66	84	TBCNN - pair model
145	85	89	with
145	90	103	concatenation
145	106	128	element - wise product
145	144	150	yields
145	155	174	highest performance
145	175	177	of
145	178	184	82.1 %
148	8	16	applying
148	17	39	element - wise product
148	40	48	improves
148	53	61	accuracy
148	62	64	by
148	65	78	another 0.5 %
149	4	27	full TBCNN - pair model
149	28	39	outperforms
149	40	89	all existing sentence encoding - based approaches
149	92	104	in - cluding
149	107	153	1024d gated recurrent unit ( GRU ) - based RNN
149	154	158	with
149	159	189	" skip - thought " pretraining
