2	74	92	Sentence Embedding
10	4	10	vector
10	19	27	used for
10	28	52	various downstream tasks
10	55	59	e.g.
10	62	80	sentiment analysis
10	83	109	natural language inference
15	0	16	Self - attention
15	17	25	computes
15	26	43	attention weights
15	44	46	by
15	51	64	inner product
15	65	72	between
15	73	110	words and the learnable weight vector
16	4	17	weight vector
16	42	49	detects
16	50	67	informative words
16	77	79	is
16	80	86	static
16	87	93	during
16	94	103	inference
23	36	43	propose
23	46	76	new self - attention mechanism
23	77	80	for
23	81	99	sentence embedding
23	102	108	namely
23	109	141	Dynamic Self - Attention ( DSA )
24	17	23	modify
24	24	39	dynamic routing
24	40	49	such that
24	53	65	functions as
24	66	82	self - attention
24	83	87	with
24	92	113	dynamic weight vector
28	3	23	design and implement
28	24	56	Dynamic Self - Attention ( DSA )
28	92	95	for
28	96	114	sentence embedding
29	3	9	devise
29	14	35	dynamic weight vector
29	36	40	with
29	47	50	DSA
29	51	59	computes
29	60	77	attention weights
96	3	12	implement
96	13	23	single DSA
96	26	38	multiple DSA
96	43	59	self - attention
97	5	29	DSA and self - attention
97	34	44	stacked on
97	45	70	CNN with Dense Connection
97	71	74	for
98	29	39	initialize
98	40	55	word embeddings
98	56	58	by
98	59	94	300D Glo Ve 840B pretrained vectors
98	101	116	fix them during
98	117	125	training
99	3	6	use
99	7	25	cross-entropy loss
99	26	28	as
99	32	50	objective function
99	55	65	both tasks
100	3	6	set
100	7	15	do = 600
100	24	27	for
100	28	38	single DSA
102	0	34	Natural Language Inference Results
115	0	4	With
115	5	14	tradeoffs
115	15	26	in terms of
115	27	65	parameters and learning time per epoch
115	68	80	multiple DSA
115	81	92	outperforms
115	93	105	other models
115	106	108	by
115	111	135	large margin ( + 1.1 % )
116	0	16	In comparison to
116	32	42	single DSA
116	43	48	shows
116	49	67	better performance
116	68	72	than
116	73	89	self - attention
118	10	28	our implementation
118	29	31	of
118	36	44	baseline
118	47	60	selfattention
118	61	71	stacked on
118	72	75	CNN
118	76	80	with
118	81	97	Dense Connection
118	100	105	shows
118	106	136	better performance ( + 0.4 % )
118	137	141	than
118	150	160	stacked on
118	161	167	BiLSTM
119	0	26	Sentiment Analysis Results
127	0	10	Single DSA
127	11	22	outperforms
127	23	46	all the baseline models
127	47	49	in
127	50	65	SST - 2 dataset
127	72	80	achieves
127	81	100	comparative results
127	101	103	in
127	104	111	SST - 5
128	41	43	in
128	44	56	SNLI dataset
128	71	73	in
128	74	85	SST dataset
128	114	116	in
128	121	132	performance
128	133	140	between
128	141	185	DSA and the previous self - attentive models
