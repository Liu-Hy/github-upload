17	60	82	short and long answers
17	83	85	in
17	88	100	single model
17	121	138	pipeline approach
17	154	167	each document
17	168	172	into
17	173	200	multiple training instances
17	201	209	by using
17	210	229	overlapping windows
17	230	232	of
17	233	239	tokens
17	302	325	aggressively downsample
17	326	377	null instances ( i.e. instances without an answer )
17	378	380	at
17	381	394	training time
17	395	404	to create
17	407	428	balanced training set
17	464	466	at
17	467	480	training time
17	481	491	to predict
17	492	496	null
18	22	24	as
18	25	35	BERT joint
18	102	104	in
18	107	119	single model
18	120	131	rather than
54	3	14	initialized
54	15	24	our model
54	25	29	from
54	32	42	BERT model
54	51	63	finetuned on
54	64	75	SQ u AD 1.1
56	3	10	trained
56	15	20	model
56	21	23	by
56	24	34	minimizing
56	35	41	loss L
56	57	61	with
56	66	80	Adam optimizer
56	106	110	with
56	113	123	batch size
56	124	126	of
56	127	128	8
60	0	14	Our BERT model
60	15	18	for
60	19	21	NQ
60	22	30	performs
60	31	50	dramatically better
60	51	55	than
60	84	101	original NQ paper
