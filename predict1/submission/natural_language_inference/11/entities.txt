2	78	96	QUESTION ANSWERING
4	52	70	question answering
12	0	25	Question answering ( QA )
22	3	12	introduce
22	17	52	Dynamic Coattention Network ( DCN )
22	75	104	end - to - end neural network
22	105	108	for
22	109	127	question answering
23	10	21	consists of
23	24	43	coattentive encoder
23	49	57	captures
23	62	74	interactions
23	75	82	between
23	87	112	question and the document
23	115	117	as
23	118	125	well as
23	128	152	dynamic pointing decoder
23	158	168	alternates
23	169	176	between
23	177	187	estimating
23	192	208	start and end of
23	213	224	answer span
127	3	13	preprocess
127	18	24	corpus
127	30	33	use
127	38	47	tokenizer
127	48	52	from
127	53	69	Stanford CoreNLP
128	3	6	use
128	10	29	Glo Ve word vectors
128	30	43	pretrained on
128	48	72	840B Common Crawl corpus
129	3	8	limit
129	13	23	vocabulary
129	24	26	to
129	27	32	words
129	42	52	present in
129	57	76	Common Crawl corpus
129	81	84	set
129	85	95	embeddings
129	96	99	for
129	100	127	out - of - vocabulary words
129	128	130	to
129	131	135	zero
131	3	6	use
131	9	28	max sequence length
131	29	31	of
131	32	35	600
131	36	42	during
131	43	51	training
131	58	75	hidden state size
131	79	82	200
131	83	86	for
131	87	106	all recurrent units
131	109	122	maxout layers
131	129	142	linear layers
132	10	14	have
132	15	46	randomly initialized parameters
132	54	75	initial state of zero
133	0	16	Sentinel vectors
133	17	20	are
133	21	55	randomly initialized and optimized
133	56	62	during
133	63	71	training
134	0	3	For
134	8	23	dynamic decoder
134	29	32	set
134	37	65	maximum number of iterations
134	66	68	to
134	69	70	4
134	75	78	use
134	81	97	maxout pool size
134	98	100	of
134	101	103	16
135	3	6	use
135	7	14	dropout
135	15	28	to regularize
135	29	40	our network
135	41	47	during
135	48	56	training
135	63	71	optimize
135	76	81	model
135	82	87	using
135	88	92	ADAM
136	15	43	implemented and trained with
136	44	51	Chainer
145	4	15	performance
145	16	18	of
145	23	50	Dynamic Coattention Network
145	74	85	compared to
145	148	151	DCN
145	160	170	capability
145	171	182	to estimate
145	187	207	start and end points
145	208	210	of
145	215	226	answer span
145	227	241	multiple times
145	254	268	conditioned on
145	273	291	previous estimates
169	0	2	On
169	7	19	decoder side
169	25	40	experiment with
169	41	59	various pool sizes
169	60	63	for
169	68	85	HMN maxout layers
169	88	93	using
169	96	109	2 - layer MLP
169	110	120	instead of
169	123	126	HMN
169	133	140	forcing
169	145	156	HMN decoder
169	157	159	to
169	162	178	single iteration
170	17	24	achieve
170	29	45	best performance
170	46	48	on
170	53	68	development set
170	69	73	with
170	77	90	iterative HMN
170	91	95	with
170	96	108	pool size 16
170	115	119	find
170	129	134	model
170	135	161	consistently benefits from
170	164	198	deeper , iterative decoder network
171	4	15	performance
171	16	24	improves
171	25	27	as
171	32	68	number of maximum allowed iterations
171	69	78	increases
171	86	104	little improvement
171	105	110	after
172	0	2	On
172	7	19	encoder side
172	22	31	replacing
172	36	57	coattention mechanism
172	58	62	with
172	66	85	attention mechanism
172	124	131	setting
172	132	134	CD
172	135	137	to
172	138	142	QA D
172	157	167	results in
172	170	187	1.9 point F1 drop
175	0	25	Performance across length
178	27	32	there
178	39	70	notable performance degradation
178	71	74	for
178	75	105	longer documents and questions
182	12	19	becomes
182	20	44	increasingly challenging
182	45	55	to compute
182	60	77	correct word span
182	78	80	as
182	85	100	number of words
182	101	110	increases
185	8	12	note
185	22	29	mean F1
185	30	32	of
185	33	36	DCN
185	37	44	exceeds
185	54	70	previous systems
185	71	77	across
185	78	96	all question types
