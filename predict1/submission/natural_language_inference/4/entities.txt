2	13	48	Large - Scale Reading Comprehension
6	19	26	propose
6	29	67	novel deep neural network architecture
6	68	77	to handle
6	80	103	long - range dependency
6	104	106	in
6	107	115	RC tasks
13	0	28	Reading comprehension ( RC )
21	34	41	develop
21	44	72	customized memory controller
21	73	83	along with
21	87	115	external memory augmentation
21	116	119	for
21	120	140	complicated RC tasks
24	3	9	extend
24	14	31	memory controller
24	32	36	with
24	39	58	residual connection
24	59	71	to alleviate
24	76	98	information distortion
25	8	14	expand
25	19	47	gated recurrent unit ( GRU )
25	48	52	with
25	55	71	dense connection
25	77	84	conveys
25	85	102	enriched features
25	103	105	to
25	110	120	next layer
25	121	131	containing
25	136	144	original
25	145	155	as well as
25	160	183	transformed information
31	9	16	propose
31	20	53	extended memory controller module
31	54	57	for
31	58	66	RC tasks
121	0	4	NLTK
121	8	16	used for
121	17	33	tokenizing words
122	0	2	In
122	7	24	memory controller
122	30	33	use
122	34	49	four read heads
122	54	68	one write head
122	79	90	memory size
122	94	100	set to
122	101	107	100 36
122	119	133	initialized as
122	134	135	0
123	4	29	hidden vector dimension l
123	33	39	set to
123	40	43	200
124	3	6	use
124	7	33	AdaDelta ( Zeiler , 2012 )
124	34	36	as
124	40	49	optimizer
124	50	54	with
124	57	70	learning rate
124	71	73	of
124	74	77	0.5
125	4	14	batch size
125	18	24	set to
125	25	27	20
125	28	31	for
125	32	40	TriviaQA
125	45	47	30
125	52	57	SQuAD
125	62	68	QUASAR
126	3	6	use
126	10	36	exponential moving average
126	37	39	of
126	40	47	weights
126	48	52	with
126	55	70	decaying factor
126	71	73	of
126	74	79	0.001
129	38	41	use
129	42	67	BiDAF with self attention
129	70	72	as
129	92	101	maintains
129	106	118	best results
129	119	131	published on
129	137	164	TriviaQA and SQuAD datasets
130	3	34	TriviaQA and QUASAR - T dataset
130	40	47	compare
130	63	68	BiDAF
130	92	98	called
130	101	112	BiDAF + DNC
130	126	140	augmented with
130	144	181	existing external memory architecture
130	182	193	just before
130	198	221	answer prediction layer
130	222	224	in
130	229	234	BiDAF
131	10	12	in
131	13	37	lengthy - document cases
131	38	45	such as
131	46	55	Trivi aQA
131	73	82	our model
131	83	94	outperforms
131	95	120	all the published results
131	164	185	short - document case
131	212	219	achieve
131	224	236	best results
133	0	8	TriviaQA
134	14	23	our model
134	46	57	outperforms
134	62	100	existing state - of - the - art method
134	101	108	such as
134	111	128	BiDAF + SA + SN '
134	129	131	by
134	134	146	large margin
135	0	14	Our model with
135	15	19	DEBS
135	28	36	replaces
135	37	57	BiGRU encoder blocks
135	60	68	performs
135	69	80	even better
135	81	85	than
135	105	118	all the cases
135	119	129	except for
135	134	148	combination of
135	153	184	' full ' and ' Wikipedia ' case
137	3	7	note
137	13	23	our method
137	24	32	achieves
137	39	58	outstanding results
137	116	127	BiDAF + DNC
137	138	146	involves
137	150	178	existing memory architecture
137	181	189	improves
137	190	201	performance
137	202	206	over
137	207	212	BiDAF
138	11	20	our model
138	21	25	with
138	30	56	proposed memory controller
138	57	65	achieves
138	66	94	significantly better results
138	95	106	compared to
138	107	119	other models
141	11	16	adopt
141	17	21	ELMo
141	22	24	to
141	25	51	our model ( without DEBS )
141	60	64	uses
141	65	79	word embedding
141	80	82	as
141	87	95	weighted
141	141	145	with
141	146	160	regularization
141	189	208	our word embeddings
142	5	13	improves
142	18	27	F 1 score
142	28	30	of
142	31	40	our model
142	41	46	up to
142	47	52	85.13
142	57	59	EM
142	63	69	77. 44
143	91	100	our model
143	101	108	without
143	109	113	DEBS
143	114	122	performs
143	123	128	worse
143	129	133	than
143	138	146	baseline
143	149	179	BiDAF + Self Attention + ELMo.
166	20	25	using
166	26	30	DEBS
166	31	33	in
166	34	48	all the places
166	49	57	improves
166	62	73	performance
166	74	78	most
166	103	120	memory controller
166	121	125	with
166	126	130	DEBS
166	131	136	gives
166	141	167	largest performance margin
