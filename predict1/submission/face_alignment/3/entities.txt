2	23	44	Robust Face Alignment
13	40	88	https://github.com/protossw512/AdaptiveWingLoss.
15	0	14	Face alignment
15	22	27	known
15	31	59	facial landmark localization
16	0	14	Face alignment
26	30	36	models
26	37	49	trained with
26	54	62	MSE loss
26	63	78	tend to predict
26	81	107	blurry and dilated heatmap
26	108	112	with
26	113	126	low intensity
26	127	129	on
26	130	147	foreground pixels
26	148	159	compared to
26	164	176	ground truth
30	8	15	propose
30	18	35	new loss function
30	40	47	name it
30	48	66	Adaptive Wing loss
30	84	91	able to
30	92	113	significantly improve
30	118	125	quality
30	126	128	of
30	129	155	heatmap regression results
31	0	6	Due to
31	11	33	translation invariance
31	34	36	of
31	41	62	convolution operation
31	63	65	in
31	66	107	bottom - up and top - down CNN structures
31	108	115	such as
31	116	140	stacked Hourglass ( HG )
31	147	154	network
31	177	199	coordinate information
31	229	232	for
31	233	261	facial landmark localization
32	0	11	Inspired by
32	16	34	Coord - Conv layer
32	63	74	encode into
32	75	84	our model
32	89	116	full coordinate information
32	125	136	information
32	137	141	only
32	142	144	on
32	145	155	boundaries
32	156	170	predicted from
32	175	193	previous HG module
32	194	198	into
32	199	208	our model
33	4	34	encoded coordinate information
33	35	51	further improves
33	56	67	performance
33	68	70	of
33	71	83	our approach
34	0	9	To encode
34	10	30	boundary coordinates
34	41	44	add
34	47	55	sub-task
34	56	58	of
34	59	78	boundary prediction
34	79	95	by concatenating
34	99	126	additional boundary channel
34	127	131	into
34	136	156	ground truth heatmap
34	166	186	jointly trained with
34	187	201	other channels
38	0	4	With
38	5	31	proposed Weighted Loss Map
38	51	56	focus
38	60	77	foreground pixels
38	82	109	difficult background pixels
38	110	116	during
38	117	125	training
39	0	6	Encode
39	7	29	coordinate information
39	32	41	including
39	42	53	coordinates
39	54	56	on
39	57	65	boundary
39	68	72	into
39	77	101	face alignment algorithm
39	102	107	using
39	108	117	CoordConv
128	4	21	reduced influence
128	22	24	of
128	25	44	correct estimations
128	45	50	helps
128	55	62	network
128	63	70	to stay
128	71	80	converged
128	83	93	instead of
128	94	105	oscillating
128	106	110	like
128	115	135	L1 and the Wing loss
190	0	27	Difficult background pixels
190	43	50	focused
190	111	121	accurately
190	165	172	area of
190	173	190	foreground pixels
190	191	201	to improve
190	202	223	localization accuracy
227	0	3	For
227	8	20	WFLW dataset
227	51	54	are
227	55	72	not very accurate
227	75	84	to ensure
227	85	98	all landmarks
227	103	117	preserved from
227	118	126	cropping
227	132	139	enlarge
227	144	158	bounding boxes
227	159	161	by
227	162	166	10 %
227	167	169	on
227	170	185	both dimensions
229	4	12	input of
229	17	24	network
229	25	27	is
229	28	35	256 256
229	42	51	output of
229	52	67	each stacked HG
229	68	70	is
229	71	76	64 64
231	0	6	During
231	7	15	training
231	21	24	use
231	25	35	RM - SProp
231	36	40	with
231	44	65	initial learning rate
231	66	68	of
231	69	76	1 10 ?4
232	3	6	set
232	11	19	momentum
232	20	25	to be
232	26	27	0
232	53	65	weight decay
232	66	71	to be
232	72	79	1 10 ?5
233	3	12	train for
233	13	24	240 epoches
233	35	48	learning rate
233	52	62	reduced to
233	63	83	1 10 ?5 and 1 10 ? 6
233	84	89	after
233	90	108	80 and 160 epoches
234	0	17	Data augmentation
234	21	35	performed with
234	36	58	random rotation ( 50 )
234	61	82	translation ( 25 px )
234	85	102	flipping ( 50 % )
234	109	127	rescaling ( 15 % )
235	0	20	Random Gaussian blur
235	23	42	noise and occlusion
240	5	17	Our approach
240	18	29	outperforms
240	30	61	previous state - of - the - art
240	62	64	by
240	67	85	significant margin
240	88	101	especially on
240	106	118	failure rate
241	15	21	reduce
241	26	38	failure rate
241	39	50	measured at
241	51	59	10 % NME
241	60	64	from
241	65	71	3.73 %
241	72	74	to
241	75	81	0.99 %
243	16	18	on
243	23	27	COFW
243	28	33	shows
243	38	48	robustness
243	49	51	of
243	52	64	our approach
243	65	72	against
243	73	78	faces
243	79	83	with
243	84	114	large pose and heavy occlusion
245	0	3	Our
245	14	29	able to achieve
245	34	68	state - of - the - art performance
245	69	71	on
245	76	96	300W testing dataset
246	0	3	For
246	8	41	challenge subset ( iBug dataset )
246	51	58	able to
246	59	69	outperform
251	0	10	Our method
251	17	25	achieves
251	30	42	best results
251	43	45	on
251	50	62	WFLW dataset
251	77	105	significantly more difficult
251	106	110	than
251	111	124	COFW and 300W
252	0	2	On
252	3	15	every subset
252	19	29	outperform
252	34	65	previous state - of - the - art
255	20	26	reduce
255	31	43	failure rate
255	48	56	increase
255	61	64	AUC
255	65	77	dramatically
255	88	97	improving
255	102	130	overall localization quality
255	131	144	significantly
256	13	25	our approach
256	26	31	fails
256	32	34	on
256	35	46	only 2.84 %
256	47	49	of
256	50	60	all images
273	0	4	Note
273	9	39	baseline model ( model trained
273	45	50	MSE )
273	51	64	underperforms
273	69	88	state - of - theart
274	3	15	compare with
274	18	35	naive weight mask
274	36	52	without focus on
274	53	73	hard negative pixels
274	79	89	introduced
274	92	129	baseline weight map W M base =? W + 1
274	132	137	where
274	181	199	Adaptive Wing loss
274	208	216	improves
274	221	230	benchmark
274	231	233	by
274	234	240	0.74 %
275	0	17	All other modules
275	18	29	contributed
275	30	43	incrementally
275	51	75	localization performance
275	78	99	our Weighted Loss Map
275	100	108	improves
275	109	115	0.25 %
275	118	162	boundary prediction and coordinates encoding
275	167	185	able to contribute
275	186	193	another
275	194	200	0.09 %
290	0	31	Our proposed Adaptive Wing loss
290	32	52	significantly boosts
290	53	64	performance
290	65	78	compared with
290	79	82	MSE
331	30	42	our approach
331	49	60	outperforms
331	61	93	previous state - of - the - arts
331	94	96	in
331	97	109	all datasets
331	110	116	except
331	121	134	common subset
331	143	158	full dataset of
331	159	163	300W
338	0	7	Runtime
338	11	23	evaluated on
338	24	56	Nvidia GTX 1080 Ti graphics card
338	57	61	with
338	62	77	batch size of 1
