2	43	62	Text Categorization
4	72	91	text categorization
11	0	19	Text categorization
23	19	30	incorporate
23	31	49	positioninvariance
23	50	54	into
23	55	58	RNN
23	63	70	propose
23	85	90	named
23	91	137	Disconnected Recurrent Neural Network ( DRNN )
27	0	11	To maintain
27	16	37	position - invariance
27	43	50	utilize
27	51	62	max pooling
27	63	73	to extract
27	78	99	important information
31	29	40	trade - off
31	41	48	between
31	49	99	position - invariance and long - term dependencies
31	100	102	in
31	107	111	DRNN
39	7	14	propose
39	17	28	novel model
39	29	43	to incorporate
39	44	63	position - variance
39	64	68	into
39	69	72	RNN
43	19	26	propose
43	30	46	empirical method
43	47	54	to find
43	59	78	optimal window size
152	3	11	tokenize
152	12	26	all the corpus
152	27	31	with
152	32	49	NLTK 's tokenizer
156	3	10	utilize
156	15	39	300D Glo Ve 840B vectors
156	69	71	as
156	76	103	pre-trained word embeddings
159	3	6	use
159	7	33	Adadelta ( Zeiler , 2012 )
159	34	45	to optimize
159	46	74	all the trainable parameters
160	4	18	hyperparameter
160	19	21	of
160	22	30	Adadelta
160	34	40	set as
160	41	56	Zeiler ( 2012 )
160	57	64	suggest
160	73	80	1 e ? 6
160	87	89	is
160	90	94	0.95
161	0	8	To avoid
161	13	39	gradient explosion problem
161	45	50	apply
161	51	73	gradient norm clipping
162	4	14	batch size
162	18	24	set to
162	25	28	128
162	33	51	all the dimensions
162	52	54	of
162	55	79	input vectors and hidden
162	80	85	shows
162	91	109	our proposed model
162	110	135	significantly outperforms
162	136	156	all the other models
162	157	159	in
162	160	170	7 datasets
167	0	40	Fast - Text and region embedding methods
167	41	48	achieve
167	49	71	comparable performance
167	72	76	with
167	77	107	other CNN and RNN based models
170	4	12	D - LSTM
170	13	15	is
170	18	43	discriminative LSTM model
171	0	38	Hierarchical attention network ( HAN )
171	39	41	is
171	44	66	hierarchical GRU model
171	67	71	with
171	72	89	attentive pooling
172	7	10	see
172	16	39	very deep CNN ( VDCNN )
172	40	48	performs
172	49	53	well
172	54	56	in
172	57	71	large datasets
174	37	44	achieve
174	52	65	compared with
174	66	88	CNN better performance
174	107	109	by
174	127	144	large window size
175	0	9	Char-CRNN
175	47	55	combines
175	56	74	positioninvariance
175	75	77	of
175	78	81	CNN
175	86	110	long - term dependencies
175	111	113	of
175	114	117	RNN
180	0	5	shows
180	11	20	our model
180	21	29	achieves
180	30	64	10 - 50 % relative error reduction
180	65	78	compared with
180	79	90	char - CRNN
181	0	27	Comparison with RNN and CNN
186	0	5	shows
186	11	15	DRNN
186	16	24	performs
186	25	35	far better
186	36	40	than
186	41	44	CNN
193	0	14	Our model DRNN
193	15	23	achieves
193	24	47	much better performance
193	48	52	than
193	53	65	GRU and LSTM
222	3	12	find that
222	17	39	disconnected naive RNN
222	40	48	performs
222	69	73	than
222	74	131	disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU )
222	132	136	when
222	141	152	window size
222	153	155	is
222	156	168	lower than 5
226	0	4	DGRU
226	5	13	achieves
226	18	34	best performance
226	35	39	when
226	44	55	window size
226	56	58	is
226	59	61	15
226	74	90	best window size
226	91	94	for
226	95	100	DLSTM
226	101	103	is
226	104	105	5
233	33	35	on
233	36	46	AG dataset
235	20	28	see that
235	33	43	DRNN model
235	44	48	with
235	49	60	max pooling
235	61	69	performs
235	70	76	better
235	77	81	than
235	86	92	others
237	3	7	find
237	8	25	attentive pooling
237	29	32	not
237	59	71	window sizes
239	0	20	Window size analysis
