(All approaches||better than||traditional bag - of - words method)
(Results||has||All approaches)
(state - of - the - art methods||on||two largest datasets)
(outperforms||has||state - of - the - art methods)
(two largest datasets||name||Yahoo and DBPedia)
(Results||has||outperforms)
(other methods||with||different proportion of labeled data)
(300 - dimensional Glo Ve word embeddings||as||initialization)
(initialization||for||word embeddings and label embeddings)
(word embeddings and label embeddings||in||our model)
(Results||use||300 - dimensional Glo Ve word embeddings)
(our model 's parameters||with||Adam Optimizer)
(our model 's parameters||with||initial learning rate)
(our model 's parameters||with||minibatch size)
(initial learning rate||has||0.001)
(minibatch size||has||100)
(Results||train||our model 's parameters)
(LEAM||provides||best AUC score)
(better F1 and P@5 values||than||all methods)
(all methods||except||CNN)
(LEAM||has||best AUC score)
(Results||has||LEAM)
(logistic regression baseline||performs||worse)
(worse||than||all deep learning architectures)
(Contribution||has||Results)
