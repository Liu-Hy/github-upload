(TensorFlow||on||GPUs)
(Experimental setup||used||TensorFlow)
(gradient clipping||with||norm)
(norm||set to||1.0)
(1.0||on||all the parameters)
(all the parameters||except||word embeddings)
(Experimental setup||applied||gradient clipping)
(runtime||on||GPU)
(runtime||used||truncated backpropagation)
(GPU||used||truncated backpropagation)
(truncated backpropagation||up to||400 words)
(400 words||from||each end of the)
(each end of the||has||sequence)
(Experimental setup||To reduce||runtime)
(regularization||of||recurrent language model)
(regularization||applied||dropout)
(recurrent language model||applied||dropout)
(dropout||on||word embedding layer)
(word embedding layer||with||0.5 dropout rate)
(Experimental setup||For||regularization)
(512 hidden units LSTM||For||standard order and reversed order sequences)
(bidirectional LSTM model||used||512 hidden units LSTM)
(512 hidden units LSTM||for||standard order and reversed order sequences)
(bidirectional LSTM model||used||256 dimensional word embeddings)
(256 dimensional word embeddings||shared with||both of the LSTMs)
(Experimental setup||For||bidirectional LSTM model)
(optimization||used||Adam optimizer)
(Adam optimizer||with||0.0005 initial learning rate)
(Batch sizes||are||64)
(64||on||IMDB)
(64||on||DBpedia)
(128||on||DBpedia)
(128||on||DBpedia)
(Batch sizes||has||64)
(Experimental setup||has||Batch sizes)
(gradient clipping||with||norm)
(norm||as||1.0)
(1.0||on||all the parameters)
(all the parameters||except||word embedding)
(gradient clipping||has||norm)
(Experimental setup||applied||gradient clipping)
(Contribution||has||Experimental setup)
