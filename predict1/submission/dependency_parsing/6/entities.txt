2	28	53	NEURAL DEPENDENCY PARSING
13	3	9	modify
13	14	40	neural graphbased approach
13	64	67	few
13	73	83	to achieve
13	84	107	competitive performance
13	113	118	build
13	121	128	network
13	137	143	larger
13	148	152	uses
13	153	172	more regularization
13	178	185	replace
13	190	233	traditional MLP - based attention mechanism
13	238	244	affine
13	245	261	label classifier
13	262	266	with
13	267	280	biaffine ones
13	402	416	MLP operations
13	417	428	that reduce
13	435	449	dimensionality
15	4	20	resulting parser
15	21	30	maintains
15	31	53	most of the simplicity
15	54	56	of
15	57	88	neural graph - based approaches
15	111	122	performance
15	123	125	of
15	130	157	SOTA transition - based one
56	3	7	call
56	15	48	deep bilinear attention mechanism
56	51	53	as
56	54	64	opposed to
56	65	91	shallow bilinear attention
56	100	104	uses
56	109	125	recurrent states
60	3	6	use
60	7	45	100 - dimensional uncased word vectors
60	52	67	POS tag vectors
60	70	89	three BiLSTM layers
60	133	176	500 - and 100 - dimensional ReLU MLP layers
61	8	13	apply
61	14	21	dropout
61	22	24	at
61	25	36	every stage
61	37	39	of
61	98	102	drop
61	103	108	nodes
61	109	111	in
61	116	129	LSTM layers (
61	166	174	applying
61	179	196	same dropout mask
61	197	199	at
61	271	276	nodes
61	277	279	in
61	284	310	MLP layers and classifiers
62	3	11	optimize
62	16	23	network
62	24	28	with
62	29	42	annealed Adam
62	43	46	for
62	47	65	about 50,000 steps
62	68	81	rounded up to
62	86	99	nearest epoch
73	24	43	deep bilinear model
73	44	55	outperforms
73	60	66	others
73	67	82	with respect to
73	83	106	both speed and accuracy
74	4	9	model
74	10	14	with
74	15	57	shallow bilinear arc and label classifiers
74	58	62	gets
74	67	93	same unlabeled performance
74	94	96	as
74	101	111	deep model
74	112	116	with
74	121	134	same settings
74	236	240	runs
74	241	252	much slower
74	257	265	overfits
83	3	7	find
83	13	18	using
83	19	39	three or four layers
83	40	44	gets
83	45	77	significantly better performance
83	78	82	than
83	83	93	two layers
83	100	110	increasing
83	115	125	LSTM sizes
83	126	130	from
83	131	137	200 to
83	138	159	300 or 400 dimensions
83	169	190	signficantly improves
83	191	202	performance
87	8	19	implemented
87	24	77	coupled input - forget gate LSTM cells ( Cif - LSTM )
87	118	127	resulting
87	167	190	more popular LSTM cells
87	227	239	much smaller
91	48	50	in
91	55	79	recurrent and MLP layers
91	103	113	regularize
91	118	129	input layer
94	16	25	not using
94	26	41	any tags at all
94	51	61	results in
94	62	80	better performance
94	81	85	than
94	86	91	using
94	92	96	tags
94	97	104	without
102	0	9	Our model
102	10	14	gets
102	15	46	nearly the same UAS performance
102	47	49	on
102	50	64	PTB - SD 3.3.0
102	65	67	as
102	72	90	current SOTA model
102	153	157	gets
102	158	178	SOTA UAS performance
102	179	181	on
102	182	189	CTB 5.1
102	192	199	as well
102	203	219	SOTA performance
102	220	222	on
102	223	245	all CoNLL 09 languages
