2	13	63	Single - stage Dense Face Localisation in the Wild
4	44	71	uncontrolled face detection
4	74	114	accurate and efficient face localisation
5	11	19	presents
5	60	65	named
5	66	76	RetinaFace
5	85	93	performs
5	94	124	pixel - wise face localisation
5	125	127	on
5	128	151	various scales of faces
5	152	175	by taking advantages of
5	176	240	joint extra-supervised and self - supervised multi-task learning
9	17	20	add
9	23	57	selfsupervised mesh decoder branch
9	58	72	for predicting
9	75	113	pixel - wise 3D shape face information
9	114	130	in parallel with
9	135	163	existing supervised branches
15	0	27	Automatic face localisation
15	56	77	facial image analysis
18	13	65	single - stage pixel - wise face localisation method
18	66	73	employs
18	74	132	extra-supervised and self - supervised multi-task learning
18	133	149	in parallel with
18	154	205	existing box classification and regression branches
19	0	20	Each positive anchor
19	21	28	outputs
19	37	47	face score
19	58	66	face box
19	75	96	five facial landmarks
19	109	132	dense 3 D face vertices
19	133	145	projected on
19	150	161	image plane
25	26	33	improve
25	38	77	single - stage face detection framework
25	82	89	propose
25	92	145	state - of - the - art dense face localisation method
25	146	159	by exploiting
25	160	177	multi-task losses
25	178	189	coming from
25	190	239	strongly supervised and self - supervised signals
29	0	8	Inspired
29	14	27	MTCNN and STN
29	28	51	simultaneously detected
29	52	57	faces
29	62	83	five facial landmarks
41	19	25	employ
41	28	47	mesh decoder branch
41	48	55	through
41	56	83	self - supervision learning
41	84	98	for predicting
41	101	128	pixel - wise 3 D face shape
41	129	145	in parallel with
41	150	178	existing supervised branches
43	0	8	Based on
43	11	32	single - stage design
43	38	45	propose
43	48	91	novel pixel - wise face localisation method
43	92	97	named
43	98	110	Reti- naFace
43	119	126	employs
43	129	157	multi-task learning strategy
43	158	183	to simultaneously predict
43	184	194	face score
43	197	205	face box
43	208	229	five facial landmarks
43	236	251	3D position and
43	252	266	correspondence
43	267	269	of
43	270	287	each facial pixel
132	0	3	For
132	4	20	negative anchors
132	28	47	classification loss
132	51	58	applied
134	3	9	employ
134	12	41	shared loss head ( 1 1 conv )
134	42	48	across
134	49	71	different feature maps
140	3	6	set
140	11	21	scale step
140	22	24	at
140	25	32	2 1 / 3
140	41	53	aspect ratio
140	91	101	matched to
140	104	122	ground - truth box
140	123	127	when
140	128	131	IoU
140	135	146	larger than
140	147	150	0.5
142	6	36	most of the anchors ( > 99 % )
142	37	40	are
142	41	49	negative
142	50	55	after
142	60	73	matching step
142	79	85	employ
142	86	99	standard OHEM
142	100	112	to alleviate
142	113	134	significant imbalance
142	135	142	between
142	147	186	positive and negative training examples
143	23	27	sort
143	28	44	negative anchors
143	45	47	by
143	52	63	loss values
143	68	74	select
143	79	87	top ones
143	88	95	so that
143	100	105	ratio
143	106	113	between
143	118	147	negative and positive samples
143	148	150	is
143	151	163	at least 3:1
150	3	8	train
150	13	23	RetinaFace
150	24	29	using
150	30	43	SGD optimiser
150	46	54	momentum
150	58	61	0.9
150	109	111	on
150	112	147	four NVIDIA Tesla P40 ( 24GB ) GPUs
151	4	17	learning rate
151	18	29	starts from
151	30	36	10 ? 3
151	39	48	rising to
151	49	55	10 ? 2
151	56	61	after
151	62	70	5 epochs
151	78	88	divided by
151	89	91	10
151	92	94	at
152	4	20	training process
152	21	34	terminates at
152	35	44	80 epochs
155	0	10	Box voting
155	21	31	applied on
155	36	69	union set of predicted face boxes
155	70	75	using
155	79	92	IoU threshold
155	93	95	at
155	96	99	0.4
160	3	11	applying
160	16	25	practices
160	29	62	state - of - the - art techniques
160	65	69	i.e.
160	76	90	context module
160	97	119	deformable convolution
160	127	132	setup
160	135	163	strong baseline ( 91.286 % )
160	175	195	slightly better than
160	196	211	ISRN ( 90.9 % )
161	0	6	Adding
161	11	52	branch of five facial landmark regression
161	53	75	significantly improves
161	80	103	face box AP ( 0.408 % )
161	108	123	mAP ( 0.775 % )
161	124	126	on
161	131	142	Hard subset
162	14	20	adding
162	25	48	dense regression branch
162	49	58	increases
162	63	74	face box AP
162	75	77	on
162	78	101	Easy and Medium subsets
162	106	127	slightly deteriorates
162	132	139	results
162	140	142	on
162	147	158	Hard subset
163	15	23	learning
163	24	53	landmark and dense regression
163	54	61	jointly
163	62	69	enables
163	72	91	further improvement
163	92	103	compared to
163	104	135	adding landmark regression only
164	5	17	demonstrates
164	23	42	landmark regression
164	43	52	does help
164	53	69	dense regression
164	86	92	boosts
164	93	119	face detection performance
171	13	19	outper
171	34	64	state - of - the - art methods
171	65	76	in terms of
171	77	79	AP
172	20	30	RetinaFace
172	31	39	produces
172	44	51	best AP
172	52	54	in
172	55	66	all subsets
172	67	69	of
172	75	99	validation and test sets
172	102	106	i.e.
172	109	124	96.9 % ( Easy )
172	127	144	96.1 % ( Medium )
172	149	164	91.8 % ( Hard )
172	165	168	for
172	169	183	validation set
172	190	205	96.3 % ( Easy )
172	208	225	95.6 % ( Medium )
172	230	245	91.4 % ( Hard )
172	246	249	for
172	250	258	test set
173	0	11	Compared to
173	16	44	recent best performed method
173	47	61	Reti - na Face
173	62	69	sets up
173	72	116	new impressive record ( 91.4 % v.s. 90.3 % )
173	117	119	on
173	124	135	Hard subset
173	136	150	which contains
173	153	179	large number of tiny faces
176	0	7	Besides
176	8	31	accurate bounding boxes
176	38	59	five facial landmarks
176	60	72	predicted by
176	73	84	Retina Face
176	85	88	are
176	94	105	very robust
176	106	111	under
176	116	161	variations of pose , occlusion and resolution
183	0	10	RetinaFace
183	11	34	significantly decreases
183	39	69	normalised mean errors ( NME )
183	70	74	from
183	75	81	2.72 %
183	82	84	to
183	85	91	2.21 %
183	97	108	compared to
183	109	114	MTCNN
185	0	11	Compared to
185	12	17	MTCNN
185	20	30	RetinaFace
185	31	44	significantly
185	59	71	failure rate
185	72	76	from
185	77	84	26.31 %
185	85	87	to
185	88	94	9.37 %
199	19	30	demonstrate
199	35	60	our face detection method
199	65	70	boost
199	75	86	performance
199	87	89	of
199	92	157	state - of - the - art publicly available face recognition method
199	160	164	i.e.
199	165	172	ArcFace
204	12	14	on
204	15	23	CFP - FP
204	26	37	demonstrate
204	43	57	Reti - na Face
204	62	67	boost
204	68	100	ArcFace 's verification accuracy
204	101	105	from
204	106	113	98.37 %
204	114	116	to
204	117	124	99.49 %
209	3	9	employ
209	10	20	two tricks
209	28	37	flip test
209	42	62	face detection score
209	63	65	to
209	66	79	weigh samples
209	80	86	within
209	87	96	templates
209	99	123	to progressively improve
209	128	154	face verification accuracy
210	0	5	Under
210	6	21	fair comparison
210	24	29	TAR (
210	30	32	at
210	33	48	FAR = 1 e ? 6 )
210	49	71	significantly improves
210	72	76	from
210	77	86	88 . 29 %
210	87	89	to
210	90	97	89.59 %
210	105	117	by replacing
210	118	123	MTCNN
210	124	128	with
210	129	139	RetinaFace
215	0	20	Inference Efficiency
