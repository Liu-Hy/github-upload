2	10	32	Coreference Resolution
4	31	53	coreference resolution
10	0	22	Coreference resolution
16	26	31	train
16	34	53	deep neural network
16	54	62	to build
16	63	90	distributed representations
16	91	93	of
16	94	99	pairs
16	100	102	of
16	103	123	coreference clusters
17	5	13	captures
17	14	40	entity - level information
17	41	45	with
17	48	93	large number of learned , continuous features
17	94	104	instead of
17	107	154	small number of hand - crafted categorical ones
19	0	2	At
19	3	12	test time
19	16	25	builds up
19	26	46	coreference clusters
19	47	60	incrementally
19	63	76	starting with
19	77	89	each mention
19	90	92	in
19	118	125	merging
19	128	144	pair of clusters
20	15	24	decisions
20	25	29	with
20	32	78	novel easy - first cluster - ranking procedure
20	84	92	combines
20	97	106	strengths
20	107	109	of
20	110	180	cluster - ranking ( Rahman and and easy - first coreference algorithms
21	0	40	Training incremental coreference systems
22	19	24	using
22	27	59	learning - to - search algorithm
22	60	71	inspired by
22	72	77	SEARN
22	78	86	to train
22	87	105	our neural network
23	40	82	which action ( a cluster merge ) available
23	92	105	current state
23	110	152	partially completed coreference clustering
23	171	178	lead to
23	181	217	high - scoring coreference partition
78	3	14	initialized
78	15	34	our word embeddings
78	35	39	with
78	40	59	50 dimensional ones
78	60	71	produced by
78	72	80	word2vec
78	81	83	on
78	88	103	Gigaword corpus
78	104	107	for
78	108	115	English
78	120	139	64 dimensional ones
78	140	151	provided by
78	193	200	Chinese
79	0	24	Averaged word embeddings
79	30	34	held
79	35	40	fixed
79	41	47	during
79	48	56	training
79	67	77	embeddings
79	78	86	used for
79	87	99	single words
79	105	112	updated
80	3	6	set
80	11	29	hidden layer sizes
80	30	32	to
80	33	43	M 1 = 1000
80	46	59	M 2 = d = 500
80	64	73	minimized
80	78	96	training objective
80	97	102	using
80	103	113	RMS - Prop
81	0	13	To regularize
81	18	25	network
81	31	38	applied
81	39	56	L2 regularization
81	57	59	to
81	64	77	model weights
81	82	89	dropout
81	90	94	with
81	97	101	rate
81	102	104	of
81	105	108	0.5
81	109	111	on
81	116	131	word embeddings
81	140	146	output
81	147	149	of
81	150	167	each hidden layer
83	11	16	found
83	22	33	pretraining
83	37	48	crucial for
83	53	84	mentionranking model 's success
188	0	35	Mention - Ranking Model Experiments
192	3	7	find
192	12	50	small number of non-embedding features
192	51	73	substantially improves
192	74	91	model performance
192	94	104	especially
192	109	146	distance and string matching features
200	0	35	Cluster - Ranking Model Experiments
205	0	5	Using
205	6	24	pretrained weights
205	25	41	greatly improves
205	42	53	performance
211	3	7	find
211	12	33	easy - first approach
211	34	54	slightly outperforms
211	55	60	using
211	63	89	left - to - right ordering
211	90	92	of
211	93	101	mentions
229	4	27	mention - ranking model
229	28	37	surpasses
229	38	58	all previous systems
231	4	27	cluster - ranking model
231	28	36	improves
231	37	44	results
231	45	59	further across
231	60	101	both languages and all evaluation metrics
236	50	95	much more complicated cluster - ranking model
236	96	102	brings
236	108	127	fairly modest gains
236	128	130	in
236	131	142	performance
