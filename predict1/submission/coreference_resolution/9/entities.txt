2	9	31	Coreference Resolution
4	3	8	apply
4	9	13	BERT
4	14	16	to
4	17	39	coreference resolution
4	42	51	achieving
4	52	71	strong improvements
4	72	74	on
4	79	105	OntoNotes ( + 3.9 F1 ) and
4	106	134	GAP ( + 11.5 F1 ) benchmarks
12	3	10	present
12	11	19	two ways
12	20	22	of
12	23	32	extending
12	37	55	c 2f - coref model
13	4	23	independent variant
13	24	28	uses
13	29	53	non-overlapping segments
13	68	75	acts as
13	79	99	independent instance
13	100	103	for
13	104	108	BERT
14	4	19	overlap variant
14	20	26	splits
14	31	39	document
14	40	44	into
14	45	65	overlapping segments
14	66	82	so as to provide
14	87	92	model
14	93	97	with
14	98	105	context
14	106	112	beyond
14	113	123	512 tokens
16	2	40	https://github.com/mandarjoshi90/coref
21	8	12	find
21	18	30	BERT - large
21	31	50	benefits from using
21	51	93	longer context windows ( 384 word pieces )
21	100	111	BERT - base
21	112	120	performs
21	121	127	better
21	128	132	with
21	133	169	shorter contexts ( 128 word pieces )
63	3	9	extend
63	14	49	original Tensorflow implementations
63	50	52	of
63	53	76	c 2f - coref 3 and BERT
64	3	12	fine tune
64	13	23	all models
64	24	26	on
64	31	53	OntoNotes English data
64	54	57	for
64	58	67	20 epochs
64	68	73	using
64	76	83	dropout
64	84	86	of
64	87	90	0.3
64	97	111	learning rates
64	112	114	of
64	115	135	1 10 ?5 and 2 10 ? 4
64	136	140	with
64	141	153	linear decay
64	154	157	for
64	162	177	BERT parameters
64	186	201	task parameters
66	3	10	trained
66	11	26	separate models
66	27	31	with
66	32	47	max segment len
66	48	50	of
66	51	76	128 , 256 , 384 , and 512
66	90	100	trained on
66	101	124	128 and 384 word pieces
66	125	134	performed
66	139	143	best
66	144	147	for
66	148	159	BERT - base
69	95	102	refines
69	103	123	span representations
69	124	129	using
69	130	139	attention
69	140	143	for
69	144	168	higher - order reasoning
71	0	3	GAP
71	6	8	is
71	11	33	human - labeled corpus
71	34	36	of
71	37	67	ambiguous pronoun - name pairs
71	68	80	derived from
71	81	99	Wikipedia snippets
72	9	11	in
72	16	27	GAP dataset
72	28	38	fit within
72	41	60	single BERT segment
72	68	79	eliminating
72	93	118	cross - segment inference
73	15	22	trained
73	27	58	BERT - based c 2f - coref model
73	59	61	on
73	62	71	OntoNotes
76	8	13	shows
76	19	23	BERT
76	24	32	improves
76	33	46	c 2 f - coref
76	47	49	by
76	50	64	9 % and 11.5 %
76	65	68	for
76	73	94	base and large models
78	0	26	Document Level : OntoNotes
83	0	5	shows
83	11	22	BERT - base
83	23	29	offers
83	33	44	improvement
83	45	47	of
83	48	53	0.9 %
83	54	58	over
83	63	91	ELMo - based c2 fcoref model
87	0	12	BERT - large
87	25	33	improves
87	34	47	c 2 f - coref
87	48	50	by
87	55	73	much larger margin
87	74	76	of
87	77	82	3.9 %
88	8	15	observe
88	25	40	overlap variant
88	41	47	offers
88	48	62	no improvement
88	63	67	over
88	68	79	independent
91	18	27	Span BERT
91	65	74	pretrains
91	75	95	span representations
91	96	105	achieving
91	106	147	state of the art results ( Avg. F1 79.6 )
91	148	152	with
91	157	176	independent variant
98	0	12	BERT - large
98	13	26	improves over
98	27	38	BERT - base
98	60	69	including
98	70	88	pronoun resolution
98	93	109	lexical matching
104	0	16	Longer documents
104	17	19	in
104	20	29	OntoNotes
104	40	47	contain
104	48	85	larger and more spread - out clusters
109	86	136	more effectively encoding document - level context
109	137	142	using
109	143	165	sparse representations
