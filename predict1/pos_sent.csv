,idx,text,main_heading,heading,topic,paper_idx,BIO,BIO_1,offset1,pro1,offset2,pro2,offset3,pro3,mask,labels,title,paper
0,2,Recurrent Neural Network Grammars,title,title,constituency_parsing,0,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0044843049327354,1,0.0,1,research-problem,title,constituency_parsing0
1,12,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .",Introduction,Introduction,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.0434782608695652,11,0.0493273542600896,4,0.2,1,model,Introduction,constituency_parsing0
2,15,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .",Introduction,Introduction,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",7,0.0760869565217391,14,0.0627802690582959,7,0.35,1,model,Introduction,constituency_parsing0
3,20,"Similar to previously published discriminative bottomup transition - based parsers , greedy prediction with our model yields a linear -",Introduction,Introduction,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n']",12,0.1304347826086956,19,0.085201793721973,12,0.6,1,model,Introduction,constituency_parsing0
4,25,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,Introduction,Refer to for an example .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",17,0.1847826086956521,24,0.1076233183856502,17,0.85,1,model,Introduction: Refer to for an example .,constituency_parsing0
5,85,Transition Sequences from Trees,Introduction,,constituency_parsing,0,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",77,0.8369565217391305,84,0.3766816143497757,0,0.0,1,research-problem,Introduction,constituency_parsing0
6,164,For training we used stochastic gradient descent with a learning rate of 0.1 .,Experiments,Model and training parameters .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",7,0.875,163,0.7309417040358744,7,0.4375,1,hyperparameters,Experiments: Model and training parameters .,constituency_parsing0
7,222,"Despite slightly higher language modeling perplexity on PTB 23 , the fixed RNNG still outperforms a highly optimized sequential LSTM baseline . :",Results after Correction,Results after Correction,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",5,0.8333333333333334,221,0.9910313901345292,5,0.8333333333333334,1,results,Results after Correction,constituency_parsing0
8,2,Cloze - driven Pretraining of Self - attention Networks,title,,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0049019607843137,1,0.0,1,research-problem,title,constituency_parsing1
9,4,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .,abstract,abstract,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.0147058823529411,1,0.25,1,research-problem,abstract,constituency_parsing1
10,11,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .",Introduction,Introduction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.1764705882352941,10,0.0490196078431372,3,0.1764705882352941,1,approach,Introduction,constituency_parsing1
11,13,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,Introduction,Introduction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.2941176470588235,12,0.0588235294117647,5,0.2941176470588235,1,approach,Introduction,constituency_parsing1
12,117,We subsample up to 18B tokens .,Datasets for pretraining,,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.5454545454545454,116,0.5686274509803921,6,0.2608695652173913,1,experimental-setup,Datasets for pretraining,constituency_parsing1
13,128,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .",Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",5,0.4545454545454545,127,0.6225490196078431,17,0.7391304347826086,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
14,129,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.5454545454545454,128,0.6274509803921569,18,0.782608695652174,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
15,130,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",7,0.6363636363636364,129,0.6323529411764706,19,0.8260869565217391,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
16,131,We also use the NCCL2 library and the torch .,Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O']",8,0.7272727272727273,130,0.6372549019607843,20,0.8695652173913043,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
17,133,"We train models with 16 bit floating point precision , following .",Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",10,0.9090909090909092,132,0.6470588235294118,22,0.9565217391304348,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
18,151,"All our models outperform the uni-directional transformer ( OpenAI GPT ) of , however , our model is about 50 % larger than their model .",Results,More information can be found in .,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O']",16,0.4210526315789473,150,0.7352941176470589,15,0.6818181818181818,1,results,Results: More information can be found in .,constituency_parsing1
19,153,"Our CNN base model performs as well as STILTs in aggregate , however , on some tasks involving sentence - pairs , STILTs performs much better ( MRPC , RTE ) ; there is a similar trend for BERT .",Results,More information can be found in .,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-n', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4736842105263157,152,0.7450980392156863,17,0.7727272727272727,1,results,Results: More information can be found in .,constituency_parsing1
20,159,Structured Prediction,Results,,constituency_parsing,1,"['O', 'O']","['B-n', 'I-n']",24,0.631578947368421,158,0.7745098039215687,0,0.0,1,results,Results,constituency_parsing1
21,2,An Empirical Study of Building a Strong Baseline for Constituency Parsing,title,title,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0074074074074074,1,0.0,1,research-problem,title,constituency_parsing2
22,4,This paper investigates the construction of a strong baseline based on general purpose sequence - to - sequence models for constituency parsing .,abstract,abstract,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.5,3,0.0222222222222222,1,0.5,1,research-problem,abstract,constituency_parsing2
23,5,"We incorporate several techniques that were mainly developed in natural language generation tasks , e.g. , machine translation and summarization , and demonstrate that the sequenceto - sequence model achieves the current top - notch parsers ' performance without requiring explicit task - specific knowledge or architecture of constituent parsing .",abstract,abstract,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,1.0,4,0.0296296296296296,2,1.0,1,research-problem,abstract,constituency_parsing2
24,7,"Sequence - to - sequence ( Seq2seq ) models have successfully improved many well - studied NLP tasks , especially for natural language generation ( NLG ) tasks , such as machine translation ( MT ) and abstractive summarization .",Introduction,Introduction,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0131578947368421,6,0.0444444444444444,1,0.0833333333333333,1,research-problem,Introduction,constituency_parsing2
25,13,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,Introduction,Introduction,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",7,0.0921052631578947,12,0.0888888888888888,7,0.5833333333333334,1,model,Introduction,constituency_parsing2
26,19,Constituency Parsing by Seq2seq,Introduction,,constituency_parsing,2,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O']",13,0.1710526315789473,18,0.1333333333333333,0,0.0,1,research-problem,Introduction,constituency_parsing2
27,83,Model ensemble,,,constituency_parsing,2,"['O', 'O']","['B-n', 'I-n']",0,0.0,82,0.6074074074074074,0,0.0,1,experiments,,constituency_parsing2
28,84,Ensembling several independently trained models together significantly improves many NLP tasks .,Model ensemble,Model ensemble,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.125,83,0.6148148148148148,1,0.125,1,experiments,Model ensemble,constituency_parsing2
29,87,Language model ( LM ) reranking,Model ensemble,Model ensemble,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",4,0.5,86,0.6370370370370371,4,0.5,1,experiments,Model ensemble,constituency_parsing2
30,95,"For data pre-processing , all the parse trees were transformed into linearized forms , which include standard UNK replacement for OOV words and POS - tag normalization by XX - tags .",Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",3,0.0769230769230769,94,0.6962962962962963,3,0.09375,1,hyperparameters,Experiments,constituency_parsing2
31,103,Ensembling and Reranking : shows the results of our models with model ensembling and LM- reranking .,Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O']",11,0.282051282051282,102,0.7555555555555555,11,0.34375,1,results,Experiments,constituency_parsing2
32,111,( 1 ) Smaller mini-batch size M and gradient clipping G provided the better performance .,Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.4871794871794871,110,0.8148148148148148,19,0.59375,1,results,Experiments,constituency_parsing2
33,113,"( 2 ) Larger layer size , hidden state dimension , and beam size have little impact on the performance ; our setting , L = 2 , H = 200 , and B = 5 looks adequate in terms of speed / performance trade - off .",Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.5384615384615384,112,0.8296296296296296,21,0.65625,1,ablation-analysis,Experiments,constituency_parsing2
34,116,( e ) shows the results of utilizing subword splits .,Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",24,0.6153846153846154,115,0.8518518518518519,24,0.75,1,results,Experiments,constituency_parsing2
35,119,"Thus , using subword information as features is one promising approach for leveraging subword information into constituency parsing .",Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",27,0.6923076923076923,118,0.8740740740740741,27,0.84375,1,results,Experiments,constituency_parsing2
36,126,Our Seq2seq approach successfully achieved the competitive level as the current top - notch methods : RNNG and its variants .,Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']",34,0.8717948717948718,125,0.925925925925926,1,0.1666666666666666,1,results,Experiments,constituency_parsing2
37,2,Grammar as a Foreign Language,title,,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0047619047619047,1,0.0,1,research-problem,title,constituency_parsing3
38,4,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .,abstract,abstract,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.0142857142857142,1,0.1666666666666666,1,research-problem,abstract,constituency_parsing3
39,118,But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,Evaluation,Evaluation,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O']",9,0.2647058823529412,117,0.5571428571428572,9,0.1578947368421052,1,results,Evaluation,constituency_parsing3
40,119,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .",Evaluation,Evaluation,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",10,0.2941176470588235,118,0.5619047619047619,10,0.175438596491228,1,results,Evaluation,constituency_parsing3
41,120,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,Evaluation,Evaluation,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",11,0.3235294117647059,119,0.5666666666666667,11,0.1929824561403508,1,results,Evaluation,constituency_parsing3
42,130,"The difference between the F 1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser , 1.7 for the baseline LSTM , and 0.7 for LSTM + A .",Evaluation,"The results , presented in , are surprising .",constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",21,0.6176470588235294,129,0.6142857142857143,21,0.3684210526315789,1,results,"Evaluation: The results , presented in , are surprising .",constituency_parsing3
43,132,"Surprisingly , LSTM +A shows less degradation with length than BerkeleyParser - a full O ( n 3 ) chart parser that uses a lot more memory .",Evaluation,"The results , presented in , are surprising .",constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6764705882352942,131,0.6238095238095238,23,0.4035087719298245,1,results,"Evaluation: The results , presented in , are surprising .",constituency_parsing3
44,145,"As described in the previous section , we initialized the word - vector embedding with pre-trained word vectors obtained from word2 vec .",Pre-training influence .,Pre-training influence .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",1,0.0294117647058823,144,0.6857142857142857,36,0.631578947368421,1,experiments,Pre-training influence .,constituency_parsing3
45,157,LSTM + A trained on the high - confidence corpus ( which only includes text from news ) achieved an F 1 score of 95.7 on QTB and 84.6 on WEB .,Pre-training influence .,Performance on other datasets .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O']",13,0.3823529411764705,156,0.7428571428571429,48,0.8421052631578947,1,results,Pre-training influence .: Performance on other datasets .,constituency_parsing3
46,158,Our score on WEB is higher both than the best score reported in ( 83.5 ) and the best score we achieved with an in - house reimplementation of Berkeley Parser trained on human - annotated data ( 84.4 ) .,Pre-training influence .,Performance on other datasets .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.4117647058823529,157,0.7476190476190476,49,0.8596491228070176,1,results,Pre-training influence .: Performance on other datasets .,constituency_parsing3
47,159,We managed to achieve a slightly higher score ( 84.8 ) with the in - house Berkeley Parser trained on a large corpus .,Pre-training influence .,Performance on other datasets .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",15,0.4411764705882353,158,0.7523809523809524,50,0.8771929824561403,1,results,Pre-training influence .: Performance on other datasets .,constituency_parsing3
48,160,"On QTB , the 95.7 score of LSTM + A is also lower than the best score of our in - house BerkeleyParser ( 96.2 ) .",Pre-training influence .,Performance on other datasets .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.4705882352941176,159,0.7571428571428571,51,0.8947368421052632,1,results,Pre-training influence .: Performance on other datasets .,constituency_parsing3
49,2,Improving Neural Parsing by Disentangling Model Combination and Reranking Effects,title,title,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0069444444444444,1,0.0,1,research-problem,title,constituency_parsing4
50,80,"For the LSTM generative model ( LM ) , we use the pre-trained model released by Choe and .",Experiments,Experiments,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",3,0.073170731707317,79,0.5486111111111112,3,0.25,1,hyperparameters,Experiments,constituency_parsing4
51,84,We use actionsynchronous beam search ( Section 2.1 ) with beam size K = 100 for RD and word - synchronous beam ( Section 2.2 ) with K w = 100 and K a = 1000 for the generative models RG and LM .,Experiments,Experiments,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O']",7,0.1707317073170731,83,0.5763888888888888,7,0.5833333333333334,1,hyperparameters,Experiments,constituency_parsing4
52,88,"In comparison , we found higher performance for the LM model when using a candidate list from the RD parser : 93.66 F1 versus 92.79 F1 on the development data .",Experiments,Experiments,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",11,0.2682926829268293,87,0.6041666666666666,11,0.9166666666666666,1,results,Experiments,constituency_parsing4
53,111,"We find that combining the scores of both models improves on using the score of either model alone , regardless of the source of candidates .",Experiments,Score combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8292682926829268,110,0.7638888888888888,4,0.3636363636363636,1,results,Experiments: Score combination,constituency_parsing4
54,113,Score combination also more than compensates for the decrease in performance we saw previously when adding in candidates from the generative model : RD ?,Experiments,These improvements are statistically significant in all cases .,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",36,0.8780487804878049,112,0.7777777777777778,6,0.5454545454545454,1,results,Experiments: These improvements are statistically significant in all cases .,constituency_parsing4
55,122,"The same trends we observed on the development data , on which the interpolation parameters were tuned , hold here : score combination improves results for all models ( row 3 vs. row 2 ; row 6 vs. row 5 ) , with candidate augmentation from the generative models giving a further increase ( rows 4 and 7 ) .",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.12,121,0.8402777777777778,3,0.1764705882352941,1,experiments,Strengthening model combination,constituency_parsing4
56,130,"As in the PTB training data setting , using all models for candidates and score combinations is best , achieving 94.66 F1 ( row 9 ) .",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",11,0.44,129,0.8958333333333334,11,0.6470588235294118,1,experiments,Strengthening model combination,constituency_parsing4
57,134,"Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",15,0.6,133,0.9236111111111112,15,0.8823529411764706,1,experiments,Strengthening model combination,constituency_parsing4
58,135,"In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']",16,0.64,134,0.9305555555555556,16,0.9411764705882352,1,experiments,Strengthening model combination,constituency_parsing4
59,2,In- Order Transition - based Constituent Parsing,title,title,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0046948356807511,1,0.0,1,research-problem,title,constituency_parsing5
60,9,"Furthermore , the system achieves 93.6 F 1 with supervised reranking and 94.2 F 1 with semi-supervised reranking , which are the best results on the WSJ benchmark .",abstract,abstract,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,8,0.0375586854460093,6,1.0,1,research-problem,abstract,constituency_parsing5
61,11,Transition - based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences .,Introduction,Introduction,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0222222222222222,10,0.0469483568075117,1,0.0129870129870129,1,research-problem,Introduction,constituency_parsing5
62,32,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .",Introduction,Introduction,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.4888888888888889,31,0.1455399061032863,22,0.2857142857142857,1,model,Introduction,constituency_parsing5
63,49,We release our code at https://github.com/LeonCrashCode/InOrderParser .,Introduction,,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O']",39,0.8666666666666667,48,0.2253521126760563,39,0.5064935064935064,1,code,Introduction,constituency_parsing5
64,116,is a regularization hyperparameter (? = 10 ?6 ) . We use stochastic gradient descent with a 0.1 initialized learning rate with a 0.05 learning rate decay .,Training,Training,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,1.0,115,0.539906103286385,11,1.0,1,hyperparameters,Training,constituency_parsing5
65,133,The bottom - up system performs slightly better than the top - down system .,Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.3333333333333333,132,0.6197183098591549,2,0.2,1,results,Reranking experiments,constituency_parsing5
66,134,The inorder system outperforms both the bottom - up and the top - down system .,Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.5,133,0.6244131455399061,3,0.3,1,results,Reranking experiments,constituency_parsing5
67,135,shows the parsing results on the English test dataset .,Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",4,0.6666666666666666,134,0.6291079812206573,4,0.4,1,results,Reranking experiments,constituency_parsing5
68,136,"We find that the bottom - up parser and the top - down parser have similar results under the greedy setting , and the in - order parser outperforms both of them .",Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O']",5,0.8333333333333334,135,0.6338028169014085,5,0.5,1,results,Reranking experiments,constituency_parsing5
69,137,"Also , with supervised reranking , the in - order parser achieves the best results .",Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",6,1.0,136,0.6384976525821596,6,0.6,1,results,Reranking experiments,constituency_parsing5
70,138,English constituent results,,,constituency_parsing,5,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,137,0.6431924882629108,7,0.7,1,results,,constituency_parsing5
71,140,"With the fully - supervise setting 5 , the inorder parser outperforms the state - of - the - art discrete parser , the state - of - the - art neural parsers",English constituent results,English constituent results,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",2,0.6666666666666666,139,0.6525821596244131,9,0.9,1,results,English constituent results,constituency_parsing5
72,145,Chinese dependency results,,,constituency_parsing,5,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,144,0.676056338028169,0,0.0,1,results,,constituency_parsing5
73,147,UAS LAS,Chinese dependency results,Chinese dependency results,constituency_parsing,5,"['O', 'O']","['B-n', 'I-n']",2,0.0392156862745098,146,0.6854460093896714,0,0.0,1,results,Chinese dependency results,constituency_parsing5
74,170,"The in - order parser performs the best on all constituent types , demonstrating that the in - order parser can benefit from both bottom - up and top - down information .",Chinese dependency results,Influence of constituent type,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.4901960784313725,169,0.7934272300469484,3,1.0,1,results,Chinese dependency results: Influence of constituent type,constituency_parsing5
75,2,Parsing as Language Modeling,,,constituency_parsing,6,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",0,0.0,1,0.0111111111111111,0,0.0,1,experiments,,constituency_parsing6
76,5,"When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .",abstract,abstract,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,1.0,4,0.0444444444444444,2,1.0,1,research-problem,abstract,constituency_parsing6
77,8,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .",Introduction,Introduction,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",2,0.2857142857142857,7,0.0777777777777777,2,0.2857142857142857,1,model,Introduction,constituency_parsing6
78,14,Language Modeling,,,constituency_parsing,6,"['O', 'O']","['B-n', 'I-n']",0,0.0,13,0.1444444444444444,0,0.0,1,experiments,,constituency_parsing6
79,18,Parsing as Language Modeling,,,constituency_parsing,6,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",0,0.0,17,0.1888888888888888,0,0.0,1,experiments,,constituency_parsing6
80,54,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-p', 'B-n', 'O']",9,0.8181818181818182,53,0.5888888888888889,4,0.6666666666666666,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
81,55,The learning rate is 0.25 0.85 max where is an epoch number .,Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.9090909090909092,54,0.6,5,0.8333333333333334,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
82,2,What Do Recurrent Neural Network Grammars Learn About Syntax ?,title,title,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",1,0.0,1,0.0043478260869565,1,0.0,1,research-problem,title,constituency_parsing7
83,7,We find that explicit modeling of composition is crucial for achieving the best performance .,abstract,abstract,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",4,0.6666666666666666,6,0.0260869565217391,4,0.6666666666666666,1,research-problem,abstract,constituency_parsing7
84,8,"Through the attention mechanism , we find that headedness plays a central role in phrasal representation ( with the model 's latent attention largely agreeing with predictions made by hand - crafted head rules , albeit with some important differences ) .",abstract,abstract,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.8333333333333334,7,0.0304347826086956,5,0.8333333333333334,1,research-problem,abstract,constituency_parsing7
85,9,"By training grammars without nonterminal labels , we find that phrasal representations depend minimally on nonterminals , providing support for the endocentricity hypothesis .",abstract,abstract,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",6,1.0,8,0.0347826086956521,6,1.0,1,research-problem,abstract,constituency_parsing7
86,11,"In this paper , we focus on a recently proposed class of probability distributions , recurrent neural network grammars ( RNNGs ; ) , designed to model syntactic derivations of sentences .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",1,0.0188679245283018,10,0.0434782608695652,1,0.0526315789473684,1,model,Introduction,constituency_parsing7
87,12,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0377358490566037,11,0.0478260869565217,2,0.1052631578947368,1,model,Introduction,constituency_parsing7
88,21,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",11,0.2075471698113207,20,0.0869565217391304,11,0.5789473684210527,1,model,Introduction,constituency_parsing7
89,23,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O']",13,0.2452830188679245,22,0.0956521739130434,13,0.6842105263157895,1,model,Introduction,constituency_parsing7
90,36,"Unlike previous works that rely on hand - crafted rules to compose more fine - grained phrase representations , the RNNG implicitly parameterizes the information passed through compositions of phrases ( in ? and the neural network architecture ) , hence weakening the strong independence assumptions in classical probabilistic context - free grammars .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.490566037735849,35,0.1521739130434782,6,0.2222222222222222,1,model,Introduction,constituency_parsing7
91,38,"To generate a sentence x and its phrase - structure tree y , the RNNG samples a sequence of actions to construct y top - down .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",28,0.5283018867924528,37,0.1608695652173913,8,0.2962962962962963,1,model,Introduction,constituency_parsing7
92,40,The RNNG uses three different actions :,Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",30,0.5660377358490566,39,0.1695652173913043,10,0.3703703703703703,1,model,Introduction,constituency_parsing7
93,44,"The RNNG consists of a stack , buffer of generated words , and list of past actions that lead to the current configuration .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",34,0.6415094339622641,43,0.1869565217391304,14,0.5185185185185185,1,model,Introduction,constituency_parsing7
94,48,"At each timestep , the model encodes the stack , buffer , and past actions , with a separate LSTM for each component as features to define a distribution over the next action to take ( conditioned on the full algorithmic state ) .",Introduction,This figure is due to .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",38,0.7169811320754716,47,0.2043478260869565,18,0.6666666666666666,1,model,Introduction: This figure is due to .,constituency_parsing7
95,56,Both inference problems can be solved using an importance sampling procedure .,Introduction,This figure is due to .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",46,0.8679245283018868,55,0.2391304347826087,26,0.9629629629629628,1,model,Introduction: This figure is due to .,constituency_parsing7
96,80,The generative model did not use any pretrained word embeddings or POS tags ; a discriminative variant of the standard RNNG was used to obtain tree samples for the generative model .,Experimental settings .,Experimental settings .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,79,0.3434782608695652,15,0.5,1,hyperparameters,Experimental settings .,constituency_parsing7
97,105,Do the phrasal representations learned by RN - NGs depend on individual lexical heads or multiple heads ?,Experimental results .,We consider two theories about phrasal representation .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n']",23,0.4693877551020408,104,0.4521739130434782,5,0.5555555555555556,1,results,Experimental results .: We consider two theories about phrasal representation .,constituency_parsing7
98,149,NPs .,Experimental results .,,constituency_parsing,7,"['O', 'O']","['B-n', 'O']",17,0.4358974358974359,148,0.6434782608695652,14,0.4666666666666667,1,results,Experimental results .,constituency_parsing7
99,176,"The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .",Results .,Results .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",4,0.2352941176470588,175,0.7608695652173914,10,0.625,1,experiments,Results .,constituency_parsing7
100,192,"On test data ( with the usual split ) , the GA - RNNG achieves 94.2 % , while the U - GA - RNNG achieves 93.5 % .",Experiments .,Experiments .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.0952380952380952,191,0.8304347826086956,9,0.3214285714285714,1,results,Experiments .,constituency_parsing7
101,2,Constituency Parsing with a Self - Attentive Encoder,title,,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0049261083743842,1,0.0,1,research-problem,title,constituency_parsing8
102,4,We demonstrate that replacing an LSTM encoder with a self - attentive architecture can lead to improvements to a state - of the - art discriminative constituency parser .,abstract,abstract,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1666666666666666,3,0.0147783251231527,1,0.1666666666666666,1,research-problem,abstract,constituency_parsing8
103,16,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .",Introduction,Introduction,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O']",6,0.2857142857142857,15,0.0738916256157635,6,0.2857142857142857,1,model,Introduction,constituency_parsing8
104,24,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .",Introduction,Introduction,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6666666666666666,23,0.1133004926108374,14,0.6666666666666666,1,model,Introduction,constituency_parsing8
105,89,The model presented above achieves a score of 92.67 F1 on the Penn Treebank WSJ development set .,Results,Results,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0666666666666666,88,0.4334975369458128,1,0.1428571428571428,1,results,Results,constituency_parsing8
106,91,"For comparison , a model that uses the same decode procedure with an LSTM - based encoder achieves a development set score of 92.24 .",Results,Results,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",3,0.2,90,0.4433497536945813,3,0.4285714285714285,1,results,Results,constituency_parsing8
107,96,Content vs. Position Attention,Results,,constituency_parsing,8,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",8,0.5333333333333333,95,0.4679802955665024,0,0.0,1,results,Results,constituency_parsing8
108,99,"We can see that our model learns to use a combination of the two attention types , with positionbased attention being the most important .",Results,Content vs. Position Attention,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",11,0.7333333333333333,98,0.4827586206896552,3,0.75,1,ablation-analysis,Results: Content vs. Position Attention,constituency_parsing8
109,100,"We also see that content - based attention is more useful at later layers in the network , which is consistent with the idea that the initial layers of our model act similarly to a dilated convolutional network while the upper layers have a greater balance between the two attention types .",Results,Content vs. Position Attention,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8,99,0.4876847290640394,4,1.0,1,results,Results: Content vs. Position Attention,constituency_parsing8
110,2,Improving Coreference Resolution by Learning Entity - Level Distributed Representations,title,title,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.00390625,1,0.0,1,research-problem,title,coreference_resolution0
111,4,A long - standing challenge in coreference resolution has been the incorporation of entity - level information - features defined over clusters of mentions instead of mention pairs .,abstract,abstract,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.01171875,1,0.2,1,research-problem,abstract,coreference_resolution0
112,10,"Coreference resolution , the task of identifying which mentions in a text refer to the same realworld entity , is fundamentally a clustering problem .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.05,9,0.03515625,1,0.05,1,research-problem,Introduction,coreference_resolution0
113,16,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.35,15,0.05859375,7,0.35,1,model,Introduction,coreference_resolution0
114,17,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.4,16,0.0625,8,0.4,1,model,Introduction,coreference_resolution0
115,19,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",10,0.5,18,0.0703125,10,0.5,1,model,Introduction,coreference_resolution0
116,20,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.55,19,0.07421875,11,0.55,1,model,Introduction,coreference_resolution0
117,21,Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made .,Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,20,0.078125,12,0.6,1,research-problem,Introduction,coreference_resolution0
118,22,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",13,0.65,21,0.08203125,13,0.65,1,model,Introduction,coreference_resolution0
119,23,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.7,22,0.0859375,14,0.7,1,model,Introduction,coreference_resolution0
120,78,"We initialized our word embeddings with 50 dimensional ones produced by word2vec on the Gigaword corpus for English and 64 dimensional ones provided by Polyglot ( Al - Rfou et al. , 2013 ) for Chinese .",Training Details .,Training Details .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",1,0.25,77,0.30078125,48,0.7384615384615385,1,hyperparameters,Training Details .,coreference_resolution0
121,79,Averaged word embeddings were held fixed during training while the embeddings used for single words were updated .,Training Details .,Training Details .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O']",2,0.5,78,0.3046875,49,0.7538461538461538,1,hyperparameters,Training Details .,coreference_resolution0
122,80,"We set our hidden layer sizes to M 1 = 1000 , M 2 = d = 500 and minimized the training objective using RMS - Prop .",Training Details .,Training Details .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",3,0.75,79,0.30859375,50,0.7692307692307693,1,hyperparameters,Training Details .,coreference_resolution0
123,81,"To regularize the network , we applied L2 regularization to the model weights and dropout with a rate of 0.5 on the word embeddings and the output of each hidden layer .",Training Details .,Training Details .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,1.0,80,0.3125,51,0.7846153846153846,1,hyperparameters,Training Details .,coreference_resolution0
124,83,"As in , we found that pretraining is crucial for the mentionranking model 's success .",Pretraining .,Pretraining .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0094339622641509,82,0.3203125,53,0.8153846153846154,1,hyperparameters,Pretraining .,coreference_resolution0
125,188,Mention - Ranking Model Experiments,Pretraining .,,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",106,1.0,187,0.73046875,41,0.5942028985507246,1,experiments,Pretraining .,coreference_resolution0
126,192,"We find the small number of non-embedding features substantially improves model performance , especially the distance and string matching features .",Feature Ablations .,Feature Ablations .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.081081081081081,191,0.74609375,45,0.6521739130434783,1,ablation-analysis,Feature Ablations .,coreference_resolution0
127,200,Cluster - Ranking Model Experiments,Feature Ablations .,,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",11,0.2972972972972973,199,0.77734375,53,0.7681159420289855,1,ablation-analysis,Feature Ablations .,coreference_resolution0
128,205,Using pretrained weights greatly improves performance .,Feature Ablations .,Pretrained Weights .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-n', 'O']",16,0.4324324324324324,204,0.796875,58,0.8405797101449275,1,ablation-analysis,Feature Ablations .: Pretrained Weights .,coreference_resolution0
129,211,We find the easy - first approach slightly outperforms using a left - to - right ordering of mentions .,Feature Ablations .,Easy - First Cluster Ranking .,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",22,0.5945945945945946,210,0.8203125,64,0.927536231884058,1,ablation-analysis,Feature Ablations .: Easy - First Cluster Ranking .,coreference_resolution0
130,229,Our mention - ranking model surpasses all previous systems .,Final System Performance,Final System Performance,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.2222222222222222,228,0.890625,2,0.2222222222222222,1,results,Final System Performance,coreference_resolution0
131,231,"The cluster - ranking model improves results further across both languages and all evaluation metrics , demonstrating the utility of incorporating entity - level information .",Final System Performance,Final System Performance,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4444444444444444,230,0.8984375,4,0.4444444444444444,1,results,Final System Performance,coreference_resolution0
132,236,"However , it is worth noting that in practice the much more complicated cluster - ranking model brings only fairly modest gains in performance .",Final System Performance,Final System Performance,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",9,1.0,235,0.91796875,9,1.0,1,results,Final System Performance,coreference_resolution0
133,2,End - to - end Deep Reinforcement Learning Based Coreference Resolution,title,title,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0068027210884353,1,0.0,1,research-problem,title,coreference_resolution1
134,4,Recent neural network models have significantly advanced the task of coreference resolution .,abstract,abstract,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.1428571428571428,3,0.0204081632653061,1,0.1428571428571428,1,research-problem,abstract,coreference_resolution1
135,12,"Coreference resolution is one of the most fundamental tasks in natural language processing ( NLP ) , which has a significant impact on many downstream applications including information extraction ) , question answering , and entity linking .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0555555555555555,11,0.0748299319727891,1,0.0555555555555555,1,research-problem,Introduction,coreference_resolution1
136,23,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O']",12,0.6666666666666666,22,0.1496598639455782,12,0.6666666666666666,1,model,Introduction,coreference_resolution1
137,24,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'O']",13,0.7222222222222222,23,0.1564625850340136,13,0.7222222222222222,1,model,Introduction,coreference_resolution1
138,26,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",15,0.8333333333333334,25,0.1700680272108843,15,0.8333333333333334,1,model,Introduction,coreference_resolution1
139,27,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",16,0.8888888888888888,26,0.1768707482993197,16,0.8888888888888888,1,model,Introduction,coreference_resolution1
140,104,"First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .",Experiments,Experiments,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",3,0.4285714285714285,103,0.7006802721088435,3,0.4285714285714285,1,hyperparameters,Experiments,coreference_resolution1
141,105,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .",Experiments,Experiments,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",4,0.5714285714285714,104,0.7074829931972789,4,0.5714285714285714,1,hyperparameters,Experiments,coreference_resolution1
142,110,"In , we compare our model with the coreference systems that have produced significant improvement over the last 3 years on the OntoNotes benchmark .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",1,0.0909090909090909,109,0.7414965986394558,1,0.0909090909090909,1,results,Results,coreference_resolution1
143,115,"Built on top of the model in but excluding ELMo , our base reinforced model improves the average F 1 score around 2 points ( statistical significant t- test with p < 0.05 ) compared with .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5454545454545454,114,0.7755102040816326,6,0.5454545454545454,1,results,Results,coreference_resolution1
144,117,"Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",8,0.7272727272727273,116,0.7891156462585034,8,0.7272727272727273,1,results,Results,coreference_resolution1
145,118,"Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.8181818181818182,117,0.7959183673469388,9,0.8181818181818182,1,results,Results,coreference_resolution1
146,119,"We also notice that our full model 's improvement is mainly from higher precision scores and reasonably good recall scores , which indicates that our reinforced model combined with more active exploration produces better coreference scores to reduce false positive coreference links .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.9090909090909092,118,0.8027210884353742,10,0.9090909090909092,1,results,Results,coreference_resolution1
147,120,"Overall , our full model achieves the state - of the - art performance of 73.8 % F1 - score when using ELMo and entropy regularization ( compared to models marked with * in , and our approach simultaneously obtains the best F1 -score of 70.5 % when using fixed word embedding only .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']",11,1.0,119,0.8095238095238095,11,1.0,1,results,Results,coreference_resolution1
148,2,Deep Reinforcement Learning for Mention - Ranking Coreference Models,title,,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0,1,0.0066666666666666,1,0.0,1,research-problem,title,coreference_resolution2
149,15,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .",Introduction,Introduction,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",6,0.0652173913043478,14,0.0933333333333333,6,0.1578947368421052,1,model,Introduction,coreference_resolution2
150,16,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .",Introduction,Introduction,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.0760869565217391,15,0.1,7,0.1842105263157894,1,model,Introduction,coreference_resolution2
151,18,Our model is a neural mention - ranking model .,Introduction,Introduction,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.0978260869565217,17,0.1133333333333333,9,0.2368421052631578,1,model,Introduction,coreference_resolution2
152,70,Reinforcement Learning,Introduction,,coreference_resolution,2,"['O', 'O']","['B-n', 'I-n']",61,0.6630434782608695,69,0.46,0,0.0,1,experiments,Introduction,coreference_resolution2
153,110,"We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .",Results,Results,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0606060606060606,109,0.7266666666666667,2,0.2857142857142857,1,results,Results,coreference_resolution2
154,112,"During training it optimizes the model 's performance in expectation , but at test - time it takes the most probable sequence of actions .",Results,Results,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",4,0.1212121212121212,111,0.74,4,0.5714285714285714,1,ablation-analysis,Results,coreference_resolution2
155,115,"The reward - rescaled max - margin loss combines the best of both worlds , resulting in superior performance .",Results,Results,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",7,0.2121212121212121,114,0.76,7,1.0,1,results,Results,coreference_resolution2
156,2,Higher - order Coreference Resolution with Coarse - to - fine Inference,title,title,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0074626865671641,1,0.0,1,research-problem,title,coreference_resolution3
157,15,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,14,0.1044776119402985,6,0.375,1,model,Introduction,coreference_resolution3
158,19,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .",Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.625,18,0.1343283582089552,10,0.625,1,model,Introduction,coreference_resolution3
159,103,1 . increasing the maximum span width from 10 to 30 words .,Experimental Setup,Experimental Setup,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.4666666666666667,102,0.7611940298507462,7,0.4666666666666667,1,experiments,Experimental Setup,coreference_resolution3
160,104,1 https://github.com/kentonl/e2e-coref,Experimental Setup,Experimental Setup,coreference_resolution,3,"['O', 'O']","['O', 'B-n']",8,0.5333333333333333,103,0.7686567164179104,8,0.5333333333333333,1,code,Experimental Setup,coreference_resolution3
161,105,2 . using 3 highway LSTMs instead of 1 .,Experimental Setup,Experimental Setup,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",9,0.6,104,0.7761194029850746,9,0.6,1,experiments,Experimental Setup,coreference_resolution3
162,106,3 . using Glo Ve word embeddings with a window size of 2 for the headword embeddings and a window size of 10 for the LSTM inputs .,Experimental Setup,Experimental Setup,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",10,0.6666666666666666,105,0.7835820895522388,10,0.6666666666666666,1,hyperparameters,Experimental Setup,coreference_resolution3
163,110,"On the development set , the second - order model ( N = 2 ) outperforms the first - order model by 0.8 F1 , but the third order model only provides an additional 0.1 F1 improvement .",Experimental Setup,Experimental Setup,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.9333333333333332,109,0.8134328358208955,14,0.9333333333333332,1,results,Experimental Setup,coreference_resolution3
164,118,"The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .",Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",6,0.4615384615384615,117,0.8731343283582089,6,0.4615384615384615,1,results,Results,coreference_resolution3
165,119,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .",Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.5384615384615384,118,0.8805970149253731,7,0.5384615384615384,1,results,Results,coreference_resolution3
166,123,We also observe further improvement by including the second - order inference ( Section 3 ) .,Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",11,0.8461538461538461,122,0.9104477611940298,11,0.8461538461538461,1,results,Results,coreference_resolution3
167,124,"The improvement is largely driven by the over all increase in precision , which is expected since the higher - order inference mainly serves to rule out inconsistent clusters .",Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.9230769230769232,123,0.917910447761194,12,0.9230769230769232,1,results,Results,coreference_resolution3
168,2,A Mention - Ranking Model for Abstract Anaphora Resolution,title,title,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0041666666666666,1,0.0,1,research-problem,title,coreference_resolution4
169,4,"Resolving abstract anaphora is an important , but difficult task for text understanding .",abstract,abstract,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.0125,1,0.1,1,research-problem,abstract,coreference_resolution4
170,28,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .",Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']",14,0.5185185185185185,27,0.1125,14,0.5185185185185185,1,model,Introduction,coreference_resolution4
171,29,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .",Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",15,0.5555555555555556,28,0.1166666666666666,15,0.5555555555555556,1,model,Introduction,coreference_resolution4
172,30,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O']",16,0.5925925925925926,29,0.1208333333333333,16,0.5925925925925926,1,model,Introduction,coreference_resolution4
173,41,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora.,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",27,1.0,40,0.1666666666666666,27,1.0,1,code,Introduction,coreference_resolution4
174,160,"PS BL always performs worse than the KZH13 model on the ASN , so we report it only for ARRAU - AA .",Baselines and evaluation metrics,Baselines and evaluation metrics,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,159,0.6625,4,1.0,1,results,Baselines and evaluation metrics,coreference_resolution4
175,174,"Embeddings for tags are initialized with values drawn from the uniform distribution U ? 1 ? d+t , 1 ? d+t , where t is the number of tags 16 and d ? { 50 , qlog - U ( 30 , 100 ) } the size of the tag embeddings .",Hyperparameters tuning .,Input representation .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",12,0.2307692307692307,173,0.7208333333333333,13,0.3513513513513513,1,hyperparameters,Hyperparameters tuning .: Input representation .,coreference_resolution4
176,177,"The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.2884615384615384,176,0.7333333333333333,16,0.4324324324324324,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
177,178,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3076923076923077,177,0.7375,17,0.4594594594594595,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
178,179,The first feed - forward layer size is set to a value in Optimization .,Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",17,0.3269230769230769,178,0.7416666666666667,18,0.4864864864864865,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
179,180,"We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O']",18,0.3461538461538461,179,0.7458333333333333,19,0.5135135135135135,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
180,181,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.3653846153846153,180,0.75,20,0.5405405405405406,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
181,182,We train for 10 epochs and choose the model that performs best on the devset .,Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'O']",20,0.3846153846153846,181,0.7541666666666667,21,0.5675675675675675,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
182,184,"We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) }.",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4230769230769231,183,0.7625,23,0.6216216216216216,1,hyperparameters,Hyperparameters tuning .: Regularization .,coreference_resolution4
183,185,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",23,0.4423076923076923,184,0.7666666666666667,24,0.6486486486486487,1,hyperparameters,Hyperparameters tuning .: Regularization .,coreference_resolution4
184,186,6 Results and analysis 6.1 Results on shell noun resolution dataset provides the results of the mentionranking model ( MR - LSTM ) on the ASN corpus using default HPs .,Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",24,0.4615384615384615,185,0.7708333333333334,25,0.6756756756756757,1,results,Hyperparameters tuning .: Regularization .,coreference_resolution4
185,188,"In terms of s@1 score , MR - LSTM outperforms both KZH13 's results and TAG BL without even necessitating HP tuning .",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",26,0.5,187,0.7791666666666667,27,0.7297297297297297,1,results,Hyperparameters tuning .: Regularization .,coreference_resolution4
186,190,"From we observe : ( 1 ) with HPs tuned on ARRAU - AA , we obtain results well beyond KZH13 , ( 2 ) all ablated model variants perform worse than the full model , ( 3 ) a large performance drop when omitting syntactic information ( tag , cut ) suggests that the model makes good use of it .",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5384615384615384,189,0.7875,29,0.7837837837837838,1,results,Hyperparameters tuning .: Regularization .,coreference_resolution4
187,199,Results on the ARRAU corpus,Hyperparameters tuning .,,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n']",37,0.7115384615384616,198,0.825,0,0.0,1,results,Hyperparameters tuning .,coreference_resolution4
188,200,"The MR - LSTM is more successful in resolving nominal than pronominal anaphors , although the training data provides only pronominal ones .",Hyperparameters tuning .,Results on the ARRAU corpus,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.7307692307692307,199,0.8291666666666667,1,0.0666666666666666,1,results,Hyperparameters tuning .: Results on the ARRAU corpus,coreference_resolution4
189,206,"Contrary to shell noun resolution , omitting syntactic information boosts performance in ARRAU - AA .",Hyperparameters tuning .,Results on the ARRAU corpus,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",44,0.8461538461538461,205,0.8541666666666666,7,0.4666666666666667,1,results,Hyperparameters tuning .: Results on the ARRAU corpus,coreference_resolution4
190,209,"This is what we can observe from row 2 vs. row 6 in Table 5 : the MR - LSTM without context embedding ( ctx ) achieves a comparable s@ 2 score with the variant that omits syntactic information , but better s@3 - 4 scores .",Hyperparameters tuning .,Results on the ARRAU corpus,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",47,0.903846153846154,208,0.8666666666666667,10,0.6666666666666666,1,results,Hyperparameters tuning .: Results on the ARRAU corpus,coreference_resolution4
191,2,Learning Global Features for Coreference Resolution,title,,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0042553191489361,1,0.0,1,research-problem,title,coreference_resolution5
192,13,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,12,0.051063829787234,3,0.2727272727272727,1,model,Introduction,coreference_resolution5
193,14,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",4,0.3636363636363636,13,0.0553191489361702,4,0.3636363636363636,1,model,Introduction,coreference_resolution5
194,15,We incorporate these representations into a mention - ranking style coreference system .,Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.4545454545454545,14,0.0595744680851063,5,0.4545454545454545,1,model,Introduction,coreference_resolution5
195,17,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,16,0.0680851063829787,7,0.6363636363636364,1,model,Introduction,coreference_resolution5
196,192,"In we present our main results on the CoNLL English test set , and compare with other recent stateof - the - art systems .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0294117647058823,191,0.8127659574468085,1,0.0625,1,results,Results,coreference_resolution5
197,193,"We see a statistically significant improvement of over 0.8 Co NLL points over the previous state of the art , and the highest F 1 scores to date on all three CoNLL metrics .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O']",2,0.0588235294117647,192,0.8170212765957446,2,0.125,1,results,Results,coreference_resolution5
198,194,We now consider in more detail the impact of global features and RNNs on performance .,Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",3,0.088235294117647,193,0.8212765957446808,3,0.1875,1,results,Results,coreference_resolution5
199,203,"In we see that the RNN improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3529411764705882,202,0.8595744680851064,12,0.75,1,results,Results,coreference_resolution5
200,204,"While WL errors also decrease for both these mention - categories under the RNN model , FN errors increase .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O']",13,0.3823529411764705,203,0.8638297872340426,13,0.8125,1,results,Results,coreference_resolution5
201,205,"Importantly , the RNN performance is significantly better than that of the Avg baseline , which barely improves over mention - ranking , even with oracle history .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",14,0.4117647058823529,204,0.8680851063829788,14,0.875,1,results,Results,coreference_resolution5
202,207,"We also note that while RNN performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting , we are still able to recover a significant portion of the possible performance improvement .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",16,0.4705882352941176,206,0.8765957446808511,16,1.0,1,results,Results,coreference_resolution5
203,2,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution,title,title,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0078740157480314,1,0.0,1,research-problem,title,coreference_resolution6
204,4,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .",abstract,abstract,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.2,3,0.0236220472440944,1,0.2,1,research-problem,abstract,coreference_resolution6
205,24,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .",Introduction,Introduction,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.2112676056338028,23,0.1811023622047244,15,0.5172413793103449,1,model,Introduction,coreference_resolution6
206,25,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .",Introduction,Introduction,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",16,0.2253521126760563,24,0.1889763779527559,16,0.5517241379310345,1,model,Introduction,coreference_resolution6
207,26,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .",Introduction,Introduction,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'I-p', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",17,0.2394366197183098,25,0.1968503937007874,17,0.5862068965517241,1,model,Introduction,coreference_resolution6
208,39,Language Representation Learning,Introduction,,coreference_resolution,6,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",30,0.4225352112676056,38,0.2992125984251969,0,0.0,1,experiments,Introduction,coreference_resolution6
209,68,LSTMs with Cross - Sentence Attention,Introduction,,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",59,0.8309859154929577,67,0.5275590551181102,9,0.4285714285714285,1,research-problem,Introduction,coreference_resolution6
210,87,"In practice , the LSTM modules applied in our model have 200 output units .",Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.1666666666666666,86,0.6771653543307087,1,0.1666666666666666,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
211,88,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .",Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.3333333333333333,87,0.6850393700787402,2,0.3333333333333333,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
212,89,The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.5,88,0.6929133858267716,3,0.5,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
213,90,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .",Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.6666666666666666,89,0.7007874015748031,4,0.6666666666666666,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
214,92,"In co-reference prediction , we select 250 candidate antecedents as our baseline model .",Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",6,1.0,91,0.7165354330708661,6,1.0,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
215,97,"Comparing with the baseline model that achieved 67.2 % F1 score , the ASL model improved the performance by 0.6 % and achieved 67.8 % average F1 .",Experiment Results and Discussion,Experiment Results and Discussion,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.1333333333333333,96,0.7559055118110236,4,0.1333333333333333,1,results,Experiment Results and Discussion,coreference_resolution6
216,100,- Uh- huh .,Experiment Results and Discussion,Experiment Results and Discussion,coreference_resolution,6,"['O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O']",7,0.2333333333333333,99,0.7795275590551181,7,0.2333333333333333,1,results,Experiment Results and Discussion,coreference_resolution6
217,108,"show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .",Experiment Results and Discussion,Experiment Results and Discussion,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']",15,0.5,107,0.84251968503937,15,0.5,1,results,Experiment Results and Discussion,coreference_resolution6
218,122,"With the proposed context gate , ASL takes knowledge from context sentences if local inputs are not informative enough .",Experiment Results and Discussion,Experiment Results and Discussion,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",29,0.9666666666666668,121,0.952755905511811,29,0.9666666666666668,1,results,Experiment Results and Discussion,coreference_resolution6
219,2,Coreference Resolution with Entity Equalization,title,,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0077519379844961,1,0.0,1,research-problem,title,coreference_resolution7
220,16,"The problem of entity - level representation ( also referred to as high - order coreference models ) has attracted considerable interest recently , with methods ranging from imitation learning to iterative refinement .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2857142857142857,15,0.1162790697674418,6,0.2857142857142857,1,research-problem,Introduction,coreference_resolution7
221,18,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.3809523809523809,17,0.1317829457364341,8,0.3809523809523809,1,model,Introduction,coreference_resolution7
222,23,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']",13,0.6190476190476191,22,0.1705426356589147,13,0.6190476190476191,1,model,Introduction,coreference_resolution7
223,25,We show that this can be done by using BERT in a fully convolutional manner .,Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",15,0.7142857142857143,24,0.1860465116279069,15,0.7142857142857143,1,model,Introduction,coreference_resolution7
224,26,"Our work is the first to use BERT for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.7619047619047619,25,0.1937984496124031,16,0.7619047619047619,1,model,Introduction,coreference_resolution7
225,81,Using BERT,Baseline Model,,coreference_resolution,7,"['O', 'O']","['B-p', 'B-n']",43,0.7413793103448276,80,0.6201550387596899,13,0.4642857142857143,1,baselines,Baseline Model,coreference_resolution7
226,119,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg.",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n']",4,0.5,118,0.9147286821705426,4,0.5,1,results,Results,coreference_resolution7
227,120,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.625,119,0.9224806201550388,5,0.625,1,results,Results,coreference_resolution7
228,121,"Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.75,120,0.9302325581395348,6,0.75,1,results,Results,coreference_resolution7
229,122,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.875,121,0.937984496124031,7,0.875,1,results,Results,coreference_resolution7
230,123,"Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,1.0,122,0.9457364341085271,8,1.0,1,results,Results,coreference_resolution7
231,2,End - to - end Neural Coreference Resolution,title,title,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0040322580645161,1,0.0,1,research-problem,title,coreference_resolution8
232,4,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .,abstract,abstract,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.2,3,0.0120967741935483,1,0.2,1,research-problem,abstract,coreference_resolution8
233,10,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .,Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0625,9,0.0362903225806451,1,0.0625,1,model,Introduction,coreference_resolution8
234,12,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .",Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O']",3,0.1875,11,0.0443548387096774,3,0.1875,1,model,Introduction,coreference_resolution8
235,13,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.25,12,0.0483870967741935,4,0.25,1,model,Introduction,coreference_resolution8
236,14,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .",Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",5,0.3125,13,0.0524193548387096,5,0.3125,1,model,Introduction,coreference_resolution8
237,15,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .",Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-p', 'I-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",6,0.375,14,0.0564516129032258,6,0.375,1,model,Introduction,coreference_resolution8
238,24,The head - finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions .,Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",15,0.9375,23,0.0927419354838709,15,0.9375,1,model,Introduction,coreference_resolution8
239,118,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .",Hyperparameters,Word representations,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",2,0.0666666666666666,117,0.4717741935483871,2,0.25,1,hyperparameters,Hyperparameters: Word representations,coreference_resolution8
240,123,The hidden states in the LSTMs have 200 dimensions .,Hyperparameters,Hidden dimensions,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.2333333333333333,122,0.4919354838709677,7,0.875,1,hyperparameters,Hyperparameters: Hidden dimensions,coreference_resolution8
241,126,We encode speaker information as a binary feature indicating whether a pair of spans are from the same speaker .,Hyperparameters,Feature encoding,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-n', 'I-n', 'I-n', 'O']",10,0.3333333333333333,125,0.5040322580645161,1,0.1666666666666666,1,hyperparameters,Hyperparameters: Feature encoding,coreference_resolution8
242,128,"All features ( speaker , genre , span distance , mention width ) are represented as learned 20 - dimensional embeddings .",Hyperparameters,Feature encoding,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.4,127,0.5120967741935484,3,0.5,1,hyperparameters,Hyperparameters: Feature encoding,coreference_resolution8
243,130,"We prune the spans such that the maximum span width L = 10 , the number of spans per word ? = 0.4 , and the maximum number of antecedents K = 250 .",Hyperparameters,Pruning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",14,0.4666666666666667,129,0.5201612903225806,5,0.8333333333333334,1,hyperparameters,Hyperparameters: Pruning,coreference_resolution8
244,133,We use ADAM for learning with a minibatch size of 1 .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",17,0.5666666666666667,132,0.532258064516129,1,0.125,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
245,136,We apply 0.2 dropout to all hidden layers and feature embeddings .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.6666666666666666,135,0.5443548387096774,4,0.5,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
246,138,The learning rate is decayed by 0.1 % every 100 steps .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",22,0.7333333333333333,137,0.5524193548387096,6,0.75,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
247,144,Ensembling is performed for both the span pruning and antecedent decisions .,Hyperparameters,Ensembling,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",28,0.9333333333333332,143,0.5766129032258065,2,0.5,1,hyperparameters,Hyperparameters: Ensembling,coreference_resolution8
248,154,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .",Coreference Results,Coreference Results,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'B-n', 'O']",3,0.4285714285714285,153,0.6169354838709677,3,0.4285714285714285,1,results,Coreference Results,coreference_resolution8
249,155,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .",Coreference Results,Coreference Results,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O']",4,0.5714285714285714,154,0.6209677419354839,4,0.5714285714285714,1,results,Coreference Results,coreference_resolution8
250,162,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .",Ablations,Features,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.036144578313253,161,0.6491935483870968,3,0.1666666666666666,1,ablation-analysis,Ablations: Features,coreference_resolution8
251,183,"With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .",Ablations,Comparing Span Pruning Strategies,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2891566265060241,182,0.7338709677419355,5,1.0,1,ablation-analysis,Ablations: Comparing Span Pruning Strategies,coreference_resolution8
252,194,Mention Precision,Ablations,,coreference_resolution,8,"['O', 'O']","['B-n', 'I-n']",35,0.4216867469879518,193,0.7782258064516129,0,0.0,1,ablation-analysis,Ablations,coreference_resolution8
253,198,"For spans with 2 - 5 words , 75 - 90 % of the predictions are constituents , indicating that the vast majority of the mentions are syntactically plausible .",Ablations,Mention Precision,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.4698795180722891,197,0.7943548387096774,4,0.8,1,ablation-analysis,Ablations: Mention Precision,coreference_resolution8
254,2,BERT for Coreference Resolution : Baselines and Analysis,title,,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0084745762711864,1,0.0,1,research-problem,title,coreference_resolution9
255,4,"We apply BERT to coreference resolution , achieving strong improvements on the OntoNotes ( + 3.9 F1 ) and GAP ( + 11.5 F1 ) benchmarks .",abstract,abstract,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.25,3,0.0254237288135593,1,0.25,1,research-problem,abstract,coreference_resolution9
256,12,We present two ways of extending the c 2f - coref model in .,Introduction,Introduction,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",4,0.2222222222222222,11,0.0932203389830508,4,0.2222222222222222,1,model,Introduction,coreference_resolution9
257,13,The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT .,Introduction,Introduction,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",5,0.2777777777777778,12,0.1016949152542373,5,0.2777777777777778,1,model,Introduction,coreference_resolution9
258,14,The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens .,Introduction,Introduction,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",6,0.3333333333333333,13,0.1101694915254237,6,0.3333333333333333,1,model,Introduction,coreference_resolution9
259,16,1 https://github.com/mandarjoshi90/coref,Introduction,Introduction,coreference_resolution,9,"['O', 'O']","['O', 'B-n']",8,0.4444444444444444,15,0.1271186440677966,8,0.4444444444444444,1,code,Introduction,coreference_resolution9
260,21,We also find that BERT - large benefits from using longer context windows ( 384 word pieces ) while BERT - base performs better with shorter contexts ( 128 word pieces ) .,Introduction,Introduction,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.7222222222222222,20,0.1694915254237288,13,0.7222222222222222,1,experiments,Introduction,coreference_resolution9
261,63,We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.02,62,0.5254237288135594,1,0.0344827586206896,1,baselines,Implementation and Hyperparameters,coreference_resolution9
262,64,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O']",2,0.04,63,0.5338983050847458,2,0.0689655172413793,1,hyperparameters,Implementation and Hyperparameters,coreference_resolution9
263,66,"We trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for BERT - base and BERT - large respectively .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O']",4,0.08,65,0.5508474576271186,4,0.1379310344827586,1,results,Implementation and Hyperparameters,coreference_resolution9
264,69,"In addition to being more computationally efficient than e2e -coref , c2 f - coref iteratively refines span representations using attention for higher - order reasoning .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.14,68,0.576271186440678,7,0.2413793103448276,1,ablation-analysis,Implementation and Hyperparameters,coreference_resolution9
265,71,GAP ) is a human - labeled corpus of ambiguous pronoun - name pairs derived from Wikipedia snippets .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",9,0.18,70,0.5932203389830508,9,0.3103448275862069,1,hyperparameters,Implementation and Hyperparameters,coreference_resolution9
266,72,"Examples in the GAP dataset fit within a single BERT segment , thus eliminating the need for cross - segment inference .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.2,71,0.6016949152542372,10,0.3448275862068966,1,results,Implementation and Hyperparameters,coreference_resolution9
267,73,"Following , we trained our BERT - based c 2f - coref model on OntoNotes .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",11,0.22,72,0.6101694915254238,11,0.3793103448275862,1,experiments,Implementation and Hyperparameters,coreference_resolution9
268,76,Table 2 shows that BERT improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",14,0.28,75,0.635593220338983,14,0.4827586206896552,1,results,Implementation and Hyperparameters,coreference_resolution9
269,78,Document Level : OntoNotes,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",16,0.32,77,0.652542372881356,16,0.5517241379310345,1,results,Implementation and Hyperparameters,coreference_resolution9
270,83,shows that BERT - base offers an improvement of 0.9 % over the ELMo - based c2 fcoref model .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.42,82,0.6949152542372882,21,0.7241379310344828,1,results,Implementation and Hyperparameters,coreference_resolution9
271,87,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",25,0.5,86,0.7288135593220338,25,0.8620689655172413,1,experiments,Implementation and Hyperparameters,coreference_resolution9
272,88,We also observe that the overlap variant offers no improvement over independent .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",26,0.52,87,0.7372881355932204,26,0.896551724137931,1,results,Implementation and Hyperparameters,coreference_resolution9
273,91,"Also concurrent , Span BERT , another self - supervised method , pretrains span representations achieving state of the art results ( Avg. F1 79.6 ) with the independent variant .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",29,0.58,90,0.7627118644067796,29,1.0,1,results,Implementation and Hyperparameters,coreference_resolution9
274,98,"BERT - large improves over BERT - base in a variety of ways including pronoun resolution and lexical matching ( e.g. , racetrack and track ) .",Implementation and Hyperparameters,Strengths,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.72,97,0.8220338983050848,2,0.125,1,results,Implementation and Hyperparameters: Strengths,coreference_resolution9
275,104,Longer documents in OntoNotes generally contain larger and more spread - out clusters .,Implementation and Hyperparameters,Weaknesses,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",42,0.84,103,0.8728813559322034,8,0.5,1,experiments,Implementation and Hyperparameters: Weaknesses,coreference_resolution9
276,109,These observations suggest that future research in pretraining methods should look at more effectively encoding document - level context using sparse representations .,Implementation and Hyperparameters,Weaknesses,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",47,0.94,108,0.9152542372881356,13,0.8125,1,experiments,Implementation and Hyperparameters: Weaknesses,coreference_resolution9
277,2,A Hierarchical Model for Data - to - Text Generation,title,title,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0036496350364963,1,0.0,1,research-problem,title,data-to-text_generation0
278,4,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .",abstract,abstract,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.1666666666666666,3,0.010948905109489,1,0.1666666666666666,1,research-problem,abstract,data-to-text_generation0
279,19,Recent datato - text models mostly rely on an encoder - decoder architecture in which the data - structure is first encoded sequentially into a fixed - size vectorial representation by an encoder .,Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2571428571428571,18,0.0656934306569343,9,0.2571428571428571,1,research-problem,Introduction,data-to-text_generation0
280,20,"Then , a decoder generates words conditioned on this representation .",Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O']",10,0.2857142857142857,19,0.0693430656934306,10,0.2857142857142857,1,model,Introduction,data-to-text_generation0
281,36,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .",Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",26,0.7428571428571429,35,0.1277372262773722,26,0.7428571428571429,1,model,Introduction,data-to-text_generation0
282,37,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .",Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",27,0.7714285714285715,36,0.1313868613138686,27,0.7714285714285715,1,model,Introduction,data-to-text_generation0
283,39,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .",Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",29,0.8285714285714286,38,0.1386861313868613,29,0.8285714285714286,1,model,Introduction,data-to-text_generation0
284,192,"Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .",Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",4,0.5,191,0.6970802919708029,4,0.5,1,baselines,Baselines,data-to-text_generation0
285,195,"It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .",Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",7,0.875,194,0.708029197080292,7,0.875,1,baselines,Baselines,data-to-text_generation0
286,196,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .",Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'B-p', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",8,1.0,195,0.7116788321167883,8,1.0,1,baselines,Baselines,data-to-text_generation0
287,206,"For the encoder module , both the low - level and high - level encoders use a two - layers multi-head self - attention with two heads .",Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.1818181818181818,205,0.7481751824817519,2,0.1818181818181818,1,hyperparameters,Implementation details,data-to-text_generation0
288,207,"To fit with the small number of record keys in our dataset , their embedding size is fixed to 20 .",Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",3,0.2727272727272727,206,0.7518248175182481,3,0.2727272727272727,1,hyperparameters,Implementation details,data-to-text_generation0
289,208,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",4,0.3636363636363636,207,0.7554744525547445,4,0.3636363636363636,1,hyperparameters,Implementation details,data-to-text_generation0
290,209,We use dropout at rate 0.5 .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.4545454545454545,208,0.7591240875912408,5,0.4545454545454545,1,hyperparameters,Implementation details,data-to-text_generation0
291,210,The models are trained with a batch size of 64 .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.5454545454545454,209,0.7627737226277372,6,0.5454545454545454,1,hyperparameters,Implementation details,data-to-text_generation0
292,212,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .",Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.7272727272727273,211,0.7700729927007299,8,0.7272727272727273,1,hyperparameters,Implementation details,data-to-text_generation0
293,213,We used beam search with beam size of 5 during inference .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",9,0.8181818181818182,212,0.7737226277372263,9,0.8181818181818182,1,hyperparameters,Implementation details,data-to-text_generation0
294,214,All the models are implemented in Open NMT - py .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.9090909090909092,213,0.7773722627737226,10,0.9090909090909092,1,hyperparameters,Implementation details,data-to-text_generation0
295,215,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n']",11,1.0,214,0.781021897810219,11,1.0,1,code,Implementation details,data-to-text_generation0
296,229,"Second , the comparison between scenario Hierarchical - kv and Hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted CS - R% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1,228,0.8321167883211679,13,0.2708333333333333,1,ablation-analysis,Ablation studies,data-to-text_generation0
297,234,"Scores of Hierarchical -k are sharp , with all of the weight on the correct record ( PTS QTR1 , 26 ) whereas scores of Hierarchical - kv are more distributed over all PTS QTR records , ultimately failing to retrieve the correct one .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",10,0.2,233,0.8503649635036497,18,0.375,1,ablation-analysis,Ablation studies,data-to-text_generation0
298,235,over all models ; our best model Hierarchical -k reaching 17.5 vs. 16.5 against the best baseline .,Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",11,0.22,234,0.8540145985401459,19,0.3958333333333333,1,ablation-analysis,Ablation studies,data-to-text_generation0
299,244,"Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman , reinforcing the crucial role of structure in data semantics and saliency .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4,243,0.8868613138686131,28,0.5833333333333334,1,ablation-analysis,Ablation studies,data-to-text_generation0
300,247,"However , Wiseman achieves only 75 . 62 % of precision , effectively mentioning on average a total of 22.25 records ( wrong or accurate ) , where our model Hierarchical -k scores a precision of 89 . 46 % , leading to 23.66 total mentions , just slightly above Wiseman .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.46,246,0.8978102189781022,31,0.6458333333333334,1,ablation-analysis,Ablation studies,data-to-text_generation0
301,252,This suggests that introducing the Transformer architecture is promising way to implicitly account for data structure .,Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",28,0.56,251,0.916058394160584,36,0.75,1,ablation-analysis,Ablation studies,data-to-text_generation0
302,253,"Our hierarchical models outperform the two - step decoders of Li and Puduppully - plan on both BLEU and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.58,252,0.9197080291970804,37,0.7708333333333334,1,ablation-analysis,Ablation studies,data-to-text_generation0
303,254,"While our models sensibly outperform in precision at factual mentions , the baseline Puduppully - plan reaches 34.28 mentions on average , showing that incorporating modules dedicated to entity extraction leads to over- focusing on entities ; contrasting with our models that learn to generate more balanced descriptions .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6,253,0.9233576642335768,38,0.7916666666666666,1,ablation-analysis,Ablation studies,data-to-text_generation0
304,255,The comparison with Puduppully - updt shows that dynamically updating the encoding across the generation process can lead to better Content Ordering ( CO ) and RG - P% .,Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",31,0.62,254,0.927007299270073,39,0.8125,1,ablation-analysis,Ablation studies,data-to-text_generation0
305,259,"In contrast , our model encodes saliency among records / entities more effectively ( CS metric ) .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'O']",35,0.7,258,0.9416058394160584,43,0.8958333333333334,1,ablation-analysis,Ablation studies,data-to-text_generation0
306,266,"In this work we have proposed a hierarchical encoder for structured data , which 1 ) leverages the structure to form efficient representation of its input ; 2 ) has strong synergy with the hierarchical attention of its associated decoder .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",42,0.84,265,0.9671532846715328,1,0.1428571428571428,1,ablation-analysis,Ablation studies,data-to-text_generation0
307,2,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation,title,title,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0044247787610619,1,0.0,1,research-problem,title,data-to-text_generation1
308,12,Our work focuses on language generators whose inputs are structured meaning representations ( MRs ) .,Introduction,Introduction,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.1666666666666666,11,0.0486725663716814,4,0.1666666666666666,1,approach,Introduction,data-to-text_generation1
309,28,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .",Introduction,Located near,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.8333333333333334,27,0.1194690265486725,20,0.8333333333333334,1,approach,Introduction: Located near,data-to-text_generation1
310,158,We built our ensemble model using the seq2seq framework for TensorFlow .,Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",1,0.0833333333333333,157,0.6946902654867256,1,0.1428571428571428,1,experimental-setup,Experimental Setup,data-to-text_generation1
311,159,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .",Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",2,0.1666666666666666,158,0.6991150442477876,2,0.2857142857142857,1,experimental-setup,Experimental Setup,data-to-text_generation1
312,160,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O']",3,0.25,159,0.7035398230088495,3,0.4285714285714285,1,experimental-setup,Experimental Setup,data-to-text_generation1
313,162,"After experimenting with different beam search parameters , we settled on the beam width of 10 .",Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.4166666666666667,161,0.7123893805309734,5,0.7142857142857143,1,experimental-setup,Experimental Setup,data-to-text_generation1
314,164,"The length penalty providing the best results on the E2E dataset was 0.6 , whereas for the TV and Laptop datasets it was 0.9 and 1.0 , respectively .",Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",7,0.5833333333333334,163,0.7212389380530974,7,1.0,1,experiments,Experimental Setup,data-to-text_generation1
315,165,Experiments on the E2E Dataset,Experimental Setup,,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n']",8,0.6666666666666666,164,0.7256637168141593,0,0.0,1,experiments,Experimental Setup,data-to-text_generation1
316,172,The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo - samples in the training set .,Automatic Metric Evaluation,Automatic Metric Evaluation,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",2,0.2,171,0.7566371681415929,2,0.2,1,results,Automatic Metric Evaluation,data-to-text_generation1
317,175,Testing our ensembling approach reveals that reranking predictions pooled from different models produces an ensemble model that is over all more robust than the individual submodels .,Automatic Metric Evaluation,Automatic Metric Evaluation,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",5,0.5,174,0.7699115044247787,5,0.5,1,results,Automatic Metric Evaluation,data-to-text_generation1
318,179,"Analyzing the outputs , we also observed that the CNN model surpassed the two LSTM models in the ability to realize the "" fast food "" and "" pub "" values reliably , both of which were hardly present in the validation set but very frequent in the test set .",Automatic Metric Evaluation,Automatic Metric Evaluation,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,178,0.7876106194690266,9,0.9,1,results,Automatic Metric Evaluation,data-to-text_generation1
319,206,"We observe , however , that a hybrid ensemble model manages to perform the best in terms of the error rate , as well as the naturalness .",Experiments with Data Selection,Experiments with Data Selection,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']",10,0.5,205,0.9070796460176992,10,0.7692307692307693,1,results,Experiments with Data Selection,data-to-text_generation1
320,212,"As shows , our ensemble model performs competitively with the baseline on the TV dataset , and it outperforms it on the Laptop dataset by a wide margin .",Experiments with Data Selection,Experiments on TV and Laptop Datasets,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",16,0.8,211,0.9336283185840708,2,0.3333333333333333,1,experiments,Experiments with Data Selection: Experiments on TV and Laptop Datasets,data-to-text_generation1
321,2,Deep Graph Convolutional Encoders for Structured Data to Text Generation,title,title,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0052356020942408,1,0.0,1,research-problem,title,data-to-text_generation2
322,11,Recent neural generation approaches build on encoder - decoder architectures proposed for machine translation .,Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",2,0.0229885057471264,10,0.0523560209424083,2,0.064516129032258,1,research-problem,Introduction,data-to-text_generation2
323,14,In this work we focus on two generation scenarios where the source data is graph structured .,Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.057471264367816,13,0.0680628272251308,5,0.1612903225806451,1,model,Introduction,data-to-text_generation2
324,15,"One is the generation of multi-sentence descriptions of Knowledge Base ( KB ) entities from RDF graphs ) , namely the WebNLG task .",Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",6,0.0689655172413793,14,0.0732984293193717,6,0.1935483870967742,1,research-problem,Introduction,data-to-text_generation2
325,23,They rely on recurrent data encoders with memory and gating mechanisms ( LSTM ; ) .,Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",14,0.160919540229885,22,0.1151832460732984,14,0.4516129032258064,1,model,Introduction,data-to-text_generation2
326,25,"In this work , we compare with a model that explicitly encodes structure and is trained end - to - end .",Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.1839080459770115,24,0.1256544502617801,16,0.5161290322580645,1,model,Introduction,data-to-text_generation2
327,26,"Concretely , we use a Graph Convolutional Network ( GCN ; ) as our encoder .",Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",17,0.1954022988505747,25,0.1308900523560209,17,0.5483870967741935,1,model,Introduction,data-to-text_generation2
328,37,"Formally , we address the task of text generation from graph - structured data considering as input a directed labeled graph X = ( V , E ) where V is a set of nodes and E is a set of edges between nodes in V .",Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O']",28,0.3218390804597701,36,0.1884816753926701,28,0.9032258064516128,1,research-problem,Introduction,data-to-text_generation2
329,72,The WebNLG task aims at the generation of entity descriptions from a set of RDF triples related to an entity of a given category .,Introduction,Task,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.7241379310344828,71,0.3717277486910995,4,0.5714285714285714,1,research-problem,Introduction: Task,data-to-text_generation2
330,103,Both take as input a linearised version of the source graph .,Experiments,Experiments,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",6,0.2,102,0.5340314136125655,6,0.4285714285714285,1,baselines,Experiments,data-to-text_generation2
331,104,"For the WebNLG baseline , we use the linearis ation scripts provided by .",Experiments,Experiments,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",7,0.2333333333333333,103,0.5392670157068062,7,0.5,1,hyperparameters,Experiments,data-to-text_generation2
332,115,We obtained the best results with an encoder with four GCN layers with residual connections .,Experiments,GCN Encoders,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.6,114,0.5968586387434555,3,0.1666666666666666,1,results,Experiments: GCN Encoders,data-to-text_generation2
333,126,Encoder ( decoder ) embeddings and hidden dimensions were set to 300 .,Experiments,GCN Encoders,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",29,0.9666666666666668,125,0.6544502617801047,14,0.7777777777777778,1,tasks,Experiments: GCN Encoders,data-to-text_generation2
334,135,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,Results,Results,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.1379310344827586,134,0.7015706806282722,4,0.1818181818181818,1,results,Results,data-to-text_generation2
335,137,The GCN EC model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points ; and MELBOURNE by .014 BLEU points .,Results,Results,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",6,0.2068965517241379,136,0.7120418848167539,6,0.2727272727272727,1,results,Results,data-to-text_generation2
336,138,GCN EC is behind ADAPT which relies on sub-word encoding .,Results,Results,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",7,0.2413793103448276,137,0.7172774869109948,7,0.3181818181818182,1,baselines,Results,data-to-text_generation2
337,139,SR11 Deep task,Results,Results,data-to-text_generation,2,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",8,0.2758620689655172,138,0.7225130890052356,8,0.3636363636363636,1,results,Results,data-to-text_generation2
338,162,In ( BLEU ) we report an ablation study on the impact of the number of layers and the type of skip connections on the WebNLG dataset .,Ablation Study,Ablation Study,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",1,0.2,161,0.8429319371727748,1,0.2,1,ablation-analysis,Ablation Study,data-to-text_generation2
339,163,The first thing we notice is the importance of skip connections between GCN layers .,Ablation Study,Ablation Study,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.4,162,0.8481675392670157,2,0.4,1,ablation-analysis,Ablation Study,data-to-text_generation2
340,164,Residual and dense connections lead to similar results .,Ablation Study,Ablation Study,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",3,0.6,163,0.8534031413612565,3,0.6,1,ablation-analysis,Ablation Study,data-to-text_generation2
341,165,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .",Ablation Study,Ablation Study,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",4,0.8,164,0.8586387434554974,4,0.8,1,ablation-analysis,Ablation Study,data-to-text_generation2
342,2,Pragmatically Informative Text Generation,title,,data-to-text_generation,3,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0079365079365079,1,0.0,1,research-problem,title,data-to-text_generation3
343,4,We improve the informativeness of models for conditional text generation using techniques from computational pragmatics .,abstract,abstract,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.0238095238095238,1,0.2,1,research-problem,abstract,data-to-text_generation3
344,12,"In this paper , we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations ) and summarization .",Introduction,Introduction,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0714285714285714,11,0.0873015873015873,3,0.12,1,model,Introduction,data-to-text_generation3
345,20,Reconstructor - based pragmatic system ( S R 1 ),Introduction,Introduction,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",11,0.2619047619047619,19,0.1507936507936507,11,0.44,1,model,Introduction,data-to-text_generation3
346,93,"We also report two extractive baselines : Lead - 3 , which uses the first three sentences of the document as the summary , and Inputs , the concatenation of the extracted sentences used as inputs to our models ( i.e. , i ( 1 ) , . . . , i ( P ) ) .",Abstractive Summarization,Abstractive Summarization,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2,92,0.7301587301587301,4,0.5,1,experiments,Abstractive Summarization,data-to-text_generation3
347,94,"The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .",Abstractive Summarization,Abstractive Summarization,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.25,93,0.7380952380952381,5,0.625,1,experiments,Abstractive Summarization,data-to-text_generation3
348,2,Data - to - Text Generation with Content Selection and Planning,title,title,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0033222591362126,1,0.0,1,research-problem,title,data-to-text_generation4
349,19,"In this paper , we address these shortcomings by explicitly modeling content selection and planning within a neural data - to - text architecture .",Introduction,Introduction,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.4,18,0.0598006644518272,10,0.4,1,model,Introduction,data-to-text_generation4
350,134,"For each stage , we utilize beam search to approximately obtain the best results .",Training and Inference,Training and Inference,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",2,1.0,133,0.4418604651162791,2,1.0,1,hyperparameters,Training and Inference,data-to-text_generation4
351,159,Input feeding was employed for the text decoder .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",4,0.4444444444444444,158,0.5249169435215947,17,0.7727272727272727,1,hyperparameters,Training Configuration,data-to-text_generation4
352,160,We applied dropout ) at a rate of 0.3 .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",5,0.5555555555555556,159,0.5282392026578073,18,0.8181818181818182,1,hyperparameters,Training Configuration,data-to-text_generation4
353,161,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .",Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",6,0.6666666666666666,160,0.53156146179402,19,0.8636363636363636,1,hyperparameters,Training Configuration,data-to-text_generation4
354,162,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .",Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",7,0.7777777777777778,161,0.5348837209302325,20,0.9090909090909092,1,hyperparameters,Training Configuration,data-to-text_generation4
355,163,We set the beam size to 5 during inference .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",8,0.8888888888888888,162,0.5382059800664452,21,0.9545454545454546,1,hyperparameters,Training Configuration,data-to-text_generation4
356,164,All models are implemented in Open NMT - py .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,1.0,163,0.5415282392026578,22,1.0,1,hyperparameters,Training Configuration,data-to-text_generation4
357,179,"As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.40625,178,0.5913621262458472,14,0.1794871794871795,1,results,Automatic Evaluation,data-to-text_generation4
358,180,"In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4375,179,0.5946843853820598,15,0.1923076923076923,1,results,Automatic Evaluation,data-to-text_generation4
359,181,"Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",15,0.46875,180,0.5980066445182725,16,0.2051282051282051,1,results,Automatic Evaluation,data-to-text_generation4
360,182,"Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']",16,0.5,181,0.6013289036544851,17,0.2179487179487179,1,results,Automatic Evaluation,data-to-text_generation4
361,183,The results of the oracle system ( NCP + OR ) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output .,Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",17,0.53125,182,0.6046511627906976,18,0.2307692307692307,1,results,Automatic Evaluation,data-to-text_generation4
362,184,"As far as the template - based system is concerned , we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.5625,183,0.6079734219269103,19,0.2435897435897435,1,results,Automatic Evaluation,data-to-text_generation4
363,187,84.5 % of the records in NCP + CC are non-duplicates compared to who obtain 72.9 % showing that our model is less repetitive .,Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.65625,186,0.6179401993355482,22,0.282051282051282,1,results,Automatic Evaluation,data-to-text_generation4
364,189,"We see in that content selection and planning individually contribute to performance improvements over the baseline ( ED + CC ) , and accuracy further increases when both components are taken into account .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O']",23,0.71875,188,0.6245847176079734,24,0.3076923076923077,1,results,Automatic Evaluation,data-to-text_generation4
365,191,"Compared to the full system ( NCP + CC ) , content selection precision and recall are higher ( by 4.5 % and 2 % , respectively ) as well as content ordering ( by 1.8 % ) .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O']",25,0.78125,190,0.6312292358803987,26,0.3333333333333333,1,results,Automatic Evaluation,data-to-text_generation4
366,194,"CS precision is higher than 85 % , CS recall is higher than 93 % , and CO higher than 84 % .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",28,0.875,193,0.6411960132890365,29,0.3717948717948718,1,results,Automatic Evaluation,data-to-text_generation4
367,197,"NCP achieves higher accuracy in all metrics including relation generation , content selection , content ordering , and BLEU compared to .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O']",31,0.96875,196,0.6511627906976745,32,0.4102564102564102,1,results,Automatic Evaluation,data-to-text_generation4
368,243,"We find that NCP + CC over all performs best , however there is a significant gap between automatically generated summaries and human - authored ones .",Human - Based Evaluation,Klay,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,1.0,242,0.8039867109634552,78,1.0,1,results,Human - Based Evaluation: Klay,data-to-text_generation4
369,2,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation,title,title,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.5,1,0.0031347962382445,1,0.5,1,research-problem,title,data-to-text_generation5
370,18,The system is given a set of RDF triplets describing facts ( entities and relations between them ) and has to produce a fluent text that is faithful to the facts .,Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']",2,0.048780487804878,17,0.0532915360501567,2,0.048780487804878,1,model,Introduction,data-to-text_generation5
371,49,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .",Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",33,0.8048780487804879,48,0.1504702194357366,33,0.8048780487804879,1,model,Introduction,data-to-text_generation5
372,50,The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",34,0.8292682926829268,49,0.1536050156739812,34,0.8292682926829268,1,model,Introduction,data-to-text_generation5
373,51,This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts .,Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",35,0.8536585365853658,50,0.1567398119122257,35,0.8536585365853658,1,model,Introduction,data-to-text_generation5
374,57,We release our code and the corpus extended with matching plans in https://github.com/AmitMY/ chimera .,Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",41,1.0,56,0.1755485893416928,41,1.0,1,code,Introduction,data-to-text_generation5
375,189,We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase .,Training details,Training details,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'O']",4,0.4,188,0.5893416927899686,16,0.8421052631578947,1,hyperparameters,Training details,data-to-text_generation5
376,190,"Concretely , we use the Open NMT toolkit with the copy attn flag .",Training details,Training details,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.5,189,0.5924764890282131,17,0.8947368421052632,1,hyperparameters,Training details,data-to-text_generation5
377,192,"The pretrained embeddings are used to initialize the relation tokens in the plans , as well as the tokens in the reference texts .",Training details,Training details,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",7,0.7,191,0.5987460815047022,19,1.0,1,hyperparameters,Training details,data-to-text_generation5
378,208,"We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .",Compared Systems,Compared Systems,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,207,0.6489028213166145,2,0.2857142857142857,1,baselines,Compared Systems,data-to-text_generation5
379,210,"It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .",Compared Systems,Compared Systems,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",3,0.3,209,0.6551724137931034,4,0.5714285714285714,1,baselines,Compared Systems,data-to-text_generation5
380,213,6 Experiments and Results,Compared Systems,Compared Systems,data-to-text_generation,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",6,0.6,212,0.664576802507837,7,1.0,1,baselines,Compared Systems,data-to-text_generation5
381,228,"BestPlan reduces all error types compared to StrongNeural , by 85 % , 56 % and 90 % respectively .",Manual Evaluation,Faithfulness,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",10,0.2272727272727272,227,0.7115987460815048,10,0.4761904761904761,1,results,Manual Evaluation: Faithfulness,data-to-text_generation5
382,244,"BestPlan performed on - par with StrongNeural , and surpassed the previous state - of - the - art UPF - FORGe .",Manual Evaluation,Faithfulness,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",26,0.5909090909090909,243,0.7617554858934169,4,0.6666666666666666,1,results,Manual Evaluation: Faithfulness,data-to-text_generation5
383,2,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation,title,title,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0055865921787709,1,0.0,1,research-problem,title,data-to-text_generation6
384,4,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .",abstract,abstract,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",1,0.125,3,0.0167597765363128,1,0.125,1,research-problem,abstract,data-to-text_generation6
385,16,"Sequence - to - sequence frameworks have proved to be very effective in natural language generation ( NLG ) tasks , as well as in machine translation and in language modeling .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2222222222222222,15,0.0837988826815642,4,0.2222222222222222,1,research-problem,Introduction,data-to-text_generation6
386,23,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.6111111111111112,22,0.1229050279329608,11,0.6111111111111112,1,model,Introduction,data-to-text_generation6
387,24,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6666666666666666,23,0.1284916201117318,12,0.6666666666666666,1,model,Introduction,data-to-text_generation6
388,27,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.8333333333333334,26,0.1452513966480447,15,0.8333333333333334,1,model,Introduction,data-to-text_generation6
389,105,"We developed our system using the PyTorch framework 2 , release 0.4.1 3 .",Implementation Details,Implementation Details,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0769230769230769,104,0.5810055865921788,1,0.0769230769230769,1,experimental-setup,Implementation Details,data-to-text_generation6
390,106,"The training has been carried out as described in subsection 2.3 : this training procedure needs the two GRUs to have the same dimensions , in terms of input size , hidden size , number of layers and presence of a bias term .",Implementation Details,Implementation Details,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",2,0.1538461538461538,105,0.5865921787709497,2,0.1538461538461538,1,experimental-setup,Implementation Details,data-to-text_generation6
391,107,"Moreover , they both have to be bidirectional , even if the decoder ignores the backward part of its current GRU .",Implementation Details,Implementation Details,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2307692307692307,106,0.5921787709497207,3,0.2307692307692307,1,ablation-analysis,Implementation Details,data-to-text_generation6
392,108,"We minimize the negative log - likelihood loss using teacher forcing and Adam , the latter being an optimizer that computes individual adaptive learning rates .",Implementation Details,Implementation Details,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.3076923076923077,107,0.5977653631284916,4,0.3076923076923077,1,experimental-setup,Implementation Details,data-to-text_generation6
393,110,We also propose a new formulation of P ( c ) that helps the model to learn when it is necessary to start a copying phase :,Implementation Details,Implementation Details,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'I-p', 'O', 'O', 'O', 'B-n', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O']",6,0.4615384615384615,109,0.6089385474860335,6,0.4615384615384615,1,baselines,Implementation Details,data-to-text_generation6
394,121,"The second one is TGen , a word - based model , still derived from , but integrating a beam search mechanism and a reranker over the top k outputs , in order to dis advantage utterances that do not verbalize all the information contained in the MR .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",3,0.06,120,0.6703910614525139,3,0.06,1,baselines,Results and Discussion,data-to-text_generation6
395,123,"We used the official code provided in the E2E NLG Challenge website for TGen , and we developed our models and EDA in PyTorch , training them on NVIDIA GPUs .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.1,122,0.6815642458100558,5,0.1,1,ablation-analysis,Results and Discussion,data-to-text_generation6
396,137,"A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.38,136,0.7597765363128491,19,0.38,1,results,Results and Discussion,data-to-text_generation6
397,138,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.4,137,0.7653631284916201,20,0.4,1,results,Results and Discussion,data-to-text_generation6
398,140,"A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.44,139,0.776536312849162,22,0.44,1,results,Results and Discussion,data-to-text_generation6
399,141,"Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",23,0.46,140,0.7821229050279329,23,0.46,1,results,Results and Discussion,data-to-text_generation6
400,142,"Finally , the baseline model , EDA , is largely outperformed by all other examined methods .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",24,0.48,141,0.7877094972067039,24,0.48,1,results,Results and Discussion,data-to-text_generation6
401,144,"We highlight that EDA_CS 's model 's good results are achieved even if it consists in a fully end - to - end model which does not benefit from the delexicalizationrelexicalization procedure , differently from TGen .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.52,143,0.7988826815642458,26,0.52,1,results,Results and Discussion,data-to-text_generation6
402,2,An improved neural network model for joint POS tagging and dependency parsing,title,title,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0058823529411764,1,0.0,1,research-problem,title,dependency_parsing0
403,9,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,abstract,abstract,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",6,1.0,8,0.0470588235294117,6,1.0,1,code,abstract,dependency_parsing0
404,18,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .",Introduction,Introduction,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.6153846153846154,17,0.1,8,0.6153846153846154,1,model,Introduction,dependency_parsing0
405,71,"As mentioned in Section 4 , our model generally outperforms j PTDP v1.0 with 2.5 + % LAS improvements on universal dependencies ( UD ) treebanks .",Joint model training,Joint model training,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,1.0,70,0.4117647058823529,7,1.0,1,results,Joint model training,dependency_parsing0
406,73,"Our model is released as jPTDP v2.0 , available at https://github.com/datquocnguyen/",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'B-n']",1,0.0111111111111111,72,0.4235294117647059,1,0.0588235294117647,1,code,Implementation details,dependency_parsing0
407,75,Our jPTDP v 2.0 is implemented using DYNET v2.0 with a fixed random seed .,Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",3,0.0333333333333333,74,0.4352941176470588,3,0.1764705882352941,1,hyperparameters,Implementation details,dependency_parsing0
408,77,"Word embeddings are initialized either randomly or by pre-trained word vectors , while character and POS tag embeddings are randomly initialized .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.0555555555555555,76,0.4470588235294118,5,0.2941176470588235,1,hyperparameters,Implementation details,dependency_parsing0
409,78,"For learning character - level word embeddings , we use one - layer BiLSTM seq , and set the size of LSTM hidden states to be equal to the vector size of character embeddings .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O']",6,0.0666666666666666,77,0.4529411764705882,6,0.3529411764705882,1,hyperparameters,Implementation details,dependency_parsing0
410,79,We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.0777777777777777,78,0.4588235294117647,7,0.4117647058823529,1,hyperparameters,Implementation details,dependency_parsing0
411,80,"Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0888888888888888,79,0.4647058823529412,8,0.4705882352941176,1,hyperparameters,Implementation details,dependency_parsing0
412,82,"We optimize the objective loss using Adam ( Kingma and Ba , 2014 ) with an initial learning rate at 0.001 and no mini-batches .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']",10,0.1111111111111111,81,0.4764705882352941,10,0.5882352941176471,1,hyperparameters,Implementation details,dependency_parsing0
413,83,"For training , we run for 30 epochs , and restart the Adam optimizer and anneal its initial learning rate at a proportion of 0.5 every 10 epochs .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.1222222222222222,82,0.4823529411764706,11,0.6470588235294118,1,experiments,Implementation details,dependency_parsing0
414,86,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.1555555555555555,85,0.5,14,0.8235294117647058,1,hyperparameters,Implementation details,dependency_parsing0
415,87,We also fix the number of hidden nodes in MLPs at 100 .,Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",15,0.1666666666666666,86,0.5058823529411764,15,0.8823529411764706,1,hyperparameters,Implementation details,dependency_parsing0
416,88,"Due to limited computational resource , for experiments presented in Section 3 , we perform a minimal grid search of hyper - parameters to select the number of BiLSTM pos and BiLSTM dep layers from { 1 , 2 } and the size of LSTM hidden states in each layer from { 128 , 256 } .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.1777777777777777,87,0.5117647058823529,16,0.9411764705882352,1,hyperparameters,Implementation details,dependency_parsing0
417,89,"For experiments presented in sections 4 and 5 , we fix the number of BiLSTM layers at 2 and the size of hidden states at 128 .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",17,0.1888888888888888,88,0.5176470588235295,17,1.0,1,hyperparameters,Implementation details,dependency_parsing0
418,94,Word embeddings are initialized by 100 dimensional Glo Ve word vectors pre-trained on Wikipedia and Gigaword .,Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",22,0.2444444444444444,93,0.5470588235294118,4,0.08,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
419,96,"As mentioned in Section 2.5 , we perform a minimal grid search of hyper - parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256 - dimensional LSTM hidden states ( in , we present scores obtained on the development set when using 2 BiLSTM layers ) .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2666666666666666,95,0.5588235294117647,6,0.12,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
420,99,"Clearly , our model produces very competitive parsing results .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",27,0.3,98,0.5764705882352941,9,0.18,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
421,100,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.3111111111111111,99,0.5823529411764706,10,0.2,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
422,101,"Our model also does better than the previous transition - based joint models in , and , while obtaining similar UAS and LAS scores to the joint model JMT proposed by .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",29,0.3222222222222222,100,0.5882352941176471,11,0.22,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
423,102,We achieve 0.9 % lower parsing scores than the state - of - the - art dependency parser of .,Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",30,0.3333333333333333,101,0.5941176470588235,12,0.24,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
424,103,"While also a BiLSTM - and graph - based model , it uses a more sophisticated attention mechanism "" biaffine "" for better decoding dependency arcs and relation types .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",31,0.3444444444444444,102,0.6,13,0.26,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
425,104,"In future work , we will extend our model with the biaffine attention mechanism to investigate the benefit for our model .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",32,0.3555555555555555,103,0.6058823529411764,14,0.28,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
426,106,"We also obtain a state - of - the - art POS tagging accuracy at 97.97 % on the test Section 23 , which is about 0.4 + % higher than those by , and .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.3777777777777777,105,0.6176470588235294,16,0.32,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
427,108,4 4 UniMelb in the CoNLL 2018 shared task on UD parsing,Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n']",36,0.4,107,0.6294117647058823,18,0.36,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
428,114,"For each big or small treebank , we train a joint model for universal POS tagging and dependency parsing , using a fixed random seed and a fixed set .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",42,0.4666666666666667,113,0.6647058823529411,24,0.48,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
429,121,"Here , we utilize the tokenization , word and sentence segmentation predicted by UD - Pipe 1.2 .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']",49,0.5444444444444444,120,0.7058823529411765,31,0.62,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
430,125,The final test runs are carried out on the TIRA platform .,Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",53,0.5888888888888889,124,0.7294117647058823,35,0.7,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
431,126,presents our results in the CoNLL 2018 shared task on multilingual parsing from raw texts to universal dependencies .,Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.6,125,0.7352941176470589,36,0.72,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
432,127,"Over all 82 test sets , we outperform the baseline UDPipe 1.2 with 0.6 % absolute higher average UPOS F1 score and 2.5 + % higher average UAS and LAS F1 scores .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",55,0.6111111111111112,126,0.7411764705882353,37,0.74,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
433,128,"In particular , for the "" big "" category consisting of 61 treebank test sets , we obtain 0.8 % higher UPOS and 3.1 % higher UAS and 3.6 % higher LAS than UDPipe 1.2 .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",56,0.6222222222222222,127,0.7470588235294118,38,0.76,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
434,129,Our ( UniMelb ) official LAS - based rank is at 14 th place while the baseline UDPipe 1.2 is at 18 th place over total 26 participating systems .,Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",57,0.6333333333333333,128,0.7529411764705882,39,0.78,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
435,136,"In , we also present our average UPOS , UAS and LAS accuracies with respect to ( w.r.t. ) gold - standard tokenization , word and sentence segmentation .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",64,0.7111111111111111,135,0.7941176470588235,46,0.92,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
436,151,"In particular , we achieved the highest F 1 scores for both biomedical event extraction and opinion analysis .",Implementation details,UniMelb in the EPE 2018 campaign,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",79,0.8777777777777778,150,0.8823529411764706,10,0.4761904761904761,1,experiments,Implementation details: UniMelb in the EPE 2018 campaign,dependency_parsing0
437,2,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,title,title,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0031948881789137,1,0.0,1,research-problem,title,dependency_parsing1
438,4,We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) .,abstract,abstract,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.0095846645367412,1,0.2,1,research-problem,abstract,dependency_parsing1
439,10,"The focus of this paper is on feature representation for dependency parsing , using recent techniques from the neural - networks ( "" deep learning "" ) literature .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0384615384615384,9,0.0287539936102236,1,0.0384615384615384,1,model,Introduction,dependency_parsing1
440,24,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5769230769230769,23,0.0734824281150159,15,0.5769230769230769,1,model,Introduction,dependency_parsing1
441,25,"The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an "" infinite "" window around it .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",16,0.6153846153846154,24,0.0766773162939297,16,0.6153846153846154,1,model,Introduction,dependency_parsing1
442,26,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.6538461538461539,25,0.0798722044728434,17,0.6538461538461539,1,model,Introduction,dependency_parsing1
443,30,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.8076923076923077,29,0.0926517571884984,21,0.8076923076923077,1,model,Introduction,dependency_parsing1
444,278,"For Chinese , we use the Penn Chinese Treebank 5.1 ( CTB5 ) , using the train / test / dev splits of with gold partof - speech tags , also following .",Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",7,0.1891891891891892,277,0.8849840255591054,7,0.1891891891891892,1,results,Experiments and Results,dependency_parsing1
445,281,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .",Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",10,0.2702702702702703,280,0.8945686900958466,10,0.2702702702702703,1,hyperparameters,Experiments and Results,dependency_parsing1
446,282,The code is available at the github repository https://github.com/elikip / bist -parser .,Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.2972972972972973,281,0.8977635782747604,11,0.2972972972972973,1,code,Experiments and Results,dependency_parsing1
447,283,"We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .",Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",12,0.3243243243243243,282,0.9009584664536742,12,0.3243243243243243,1,hyperparameters,Experiments and Results,dependency_parsing1
448,286,11 https://github.com/clab/cnn/tree/,Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O']","['O', 'B-n']",15,0.4054054054054054,285,0.9105431309904152,15,0.4054054054054054,1,code,Experiments and Results,dependency_parsing1
449,288,The word and POS embeddings e ( w i ) and e ( p i ) are initialized to random values and trained together with the rest of the parsers ' networks .,Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.4594594594594595,287,0.9169329073482428,17,0.4594594594594595,1,hyperparameters,Experiments and Results,dependency_parsing1
450,295,"We train the parsers for up to 30 iterations , and choose the best model according to the UAS accuracy on the development set .",Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",24,0.6486486486486487,294,0.939297124600639,24,0.6486486486486487,1,results,Experiments and Results,dependency_parsing1
451,300,"When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .",Experiments and Results,Hyperparameter Tuning,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",29,0.7837837837837838,299,0.9552715654952076,29,0.7837837837837838,1,results,Experiments and Results: Hyperparameter Tuning,dependency_parsing1
452,302,Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,Experiments and Results,Hyperparameter Tuning,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'O']",31,0.8378378378378378,301,0.9616613418530352,31,0.8378378378378378,1,results,Experiments and Results: Hyperparameter Tuning,dependency_parsing1
453,311,Dynamic oracle training yields nice gains for both English and Chinese .,Additional Results,Additional Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'I-n', 'O']",2,1.0,310,0.9904153354632588,2,1.0,1,results,Additional Results,dependency_parsing1
454,2,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,title,title,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0046296296296296,1,0.0,1,research-problem,title,dependency_parsing2
455,13,"In 3 , we apply this idea to build a firstorder graph - based ( FOG ) ensemble parser ) that seeks consensus among 20 randomly - initialized stack LSTM parsers , achieving nearly the best - reported performance on the standard Penn Treebank Stanford dependencies task ( 94.51 UAS , 92.70 LAS ) .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0566037735849056,12,0.0555555555555555,3,0.2142857142857142,1,model,Introduction,dependency_parsing2
456,18,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1509433962264151,17,0.0787037037037037,8,0.5714285714285714,1,model,Introduction,dependency_parsing2
457,19,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",9,0.1698113207547169,18,0.0833333333333333,9,0.6428571428571429,1,model,Introduction,dependency_parsing2
458,22,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2264150943396226,21,0.0972222222222222,12,0.8571428571428571,1,model,Introduction,dependency_parsing2
459,69,"Our ensembles of greedy , locally normalized parsers perform comparably to the best previously reported , due to , which uses a beam ( width 32 ) for training and decoding .",Experiment .,Penn Treebank task .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.057471264367816,68,0.3148148148148148,15,0.3125,1,experiments,Experiment .: Penn Treebank task .,dependency_parsing2
460,114,"3 . When the ensemble is confident , cost for its choice ( s ) is lower than it would be under Hamming cost - even when the ensemble is wrong .",Experiment .,Hard targets alone do not capture this information .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-p', 'O', 'O', 'I-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.5747126436781609,113,0.5231481481481481,11,0.3548387096774194,1,experiments,Experiment .: Hard targets alone do not capture this information .,dependency_parsing2
461,150,"Second , we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer .",Experiment .,Distilled Parser,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",86,0.9885057471264368,149,0.6898148148148148,15,0.9375,1,experiments,Experiment .: Distilled Parser,dependency_parsing2
462,151,"While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes , we find that this additional per-epoch decay consistently improves performance across all settings and languages .",Experiment .,Distilled Parser,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",87,1.0,150,0.6944444444444444,16,1.0,1,experiments,Experiment .: Distilled Parser,dependency_parsing2
463,155,We used the standard splits for all languages .,Experimental settings .,,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.25,154,0.7129629629629629,3,0.0714285714285714,1,hyperparameters,Experimental settings .,dependency_parsing2
464,157,For German we use the predicted tags provided by the CoNLL 2009 shared task organizers .,Experimental settings .,We used the standard splits for all languages .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.75,156,0.7222222222222222,5,0.119047619047619,1,hyperparameters,Experimental settings .: We used the standard splits for all languages .,dependency_parsing2
465,158,"All models were augmented with pretrained structured - skipgram embeddings ; for English we used the Gigaword corpus and 100 dimensions , for Chinese Gigaword and 80 , and for German WMT 2010 monolingual data and 64 .",Experimental settings .,We used the standard splits for all languages .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",4,1.0,157,0.7268518518518519,6,0.1428571428571428,1,hyperparameters,Experimental settings .: We used the standard splits for all languages .,dependency_parsing2
466,161,For the Adam optimizer we use the default settings in the CNN neural network library .,Hyperparameters .,The hyperparameters for neural FOG are summarized in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0571428571428571,160,0.7407407407407407,9,0.2142857142857142,1,hyperparameters,Hyperparameters .: The hyperparameters for neural FOG are summarized in .,dependency_parsing2
467,178,"Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .",Hyperparameters .,All scores are shown in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",19,0.5428571428571428,177,0.8194444444444444,26,0.6190476190476191,1,results,Hyperparameters .: All scores are shown in .,dependency_parsing2
468,183,"The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS , compared to 93.6 UAS and 91.1 LAS for the model with distillation cost .",Hyperparameters .,All scores are shown in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']",24,0.6857142857142857,182,0.8425925925925926,31,0.7380952380952381,1,results,Hyperparameters .: All scores are shown in .,dependency_parsing2
469,2,From POS tagging to dependency parsing for biomedical event extraction,title,title,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0038461538461538,1,0.0,1,research-problem,title,dependency_parsing3
470,12,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.,abstract,,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",9,1.0,11,0.0423076923076923,9,0.375,1,code,abstract,dependency_parsing3
471,97,"For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",6,0.24,96,0.3692307692307692,21,0.525,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
472,101,"We perform a grid search of hyperparameters to select the number of BiLSTM layers from { 1 , 2 } and the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.4,100,0.3846153846153846,25,0.625,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
473,102,Early stopping is applied when no performance improvement on the development set is obtained after 10 contiguous epochs .,Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",11,0.44,101,0.3884615384615384,26,0.65,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
474,103,"For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.48,102,0.3923076923076923,27,0.675,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
475,104,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",13,0.52,103,0.3961538461538461,28,0.7,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
476,105,"We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.56,104,0.4,29,0.725,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
477,109,https://github.com/tdozat/Parser-v2,Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,['O'],['B-n'],18,0.72,108,0.4153846153846154,33,0.825,1,code,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
478,113,Corpus - level accuracy differences of at least 0.17 % in GENIA and 0.26 % in CRAFT between two POS tagging models are significant at p ? 0.05 .,Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.88,112,0.4307692307692308,37,0.925,1,experiments,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
479,118,POS tagging results,,,dependency_parsing,3,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,117,0.45,0,0.0,1,results,,dependency_parsing3
480,119,"In general , we find that the six retrained models produce competitive results .",POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.0476190476190476,118,0.4538461538461538,1,0.0476190476190476,1,results,POS tagging results,dependency_parsing3
481,124,BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .,POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.2857142857142857,123,0.473076923076923,6,0.2857142857142857,1,results,POS tagging results,dependency_parsing3
482,125,"Using character - level word embeddings helps to produce about 0.5 % and Trained on the PTB sections 0 - 18 , the accuracies for the GENIA tagger , Stanford tagger , MarMoT , NLP4J - POS , BiLSTM- CRF and BiLSTM - CRF + CNN - char on the benchmark test set of PTB sections 22 - 24 were reported at 97.05 % , 97.23 % , 97.28 % , 97.64 % , 97.45 % and 97.55 % , respectively .",POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'I-n']",7,0.3333333333333333,124,0.4769230769230769,7,0.3333333333333333,1,results,POS tagging results,dependency_parsing3
483,130,"Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .",POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",12,0.5714285714285714,129,0.4961538461538461,12,0.5714285714285714,1,results,POS tagging results,dependency_parsing3
484,148,"On GENIA , among pre-trained models , BLLIP obtains highest results .",Overall dependency parsing results,Overall dependency parsing results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",8,0.6153846153846154,147,0.5653846153846154,8,0.6153846153846154,1,results,Overall dependency parsing results,dependency_parsing3
485,152,Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .,Overall dependency parsing results,Overall dependency parsing results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.9230769230769232,151,0.5807692307692308,12,0.9230769230769232,1,results,Overall dependency parsing results,dependency_parsing3
486,153,"Regarding the retrained parsing models , on both GENIA and CRAFT , Stanford - Biaffine achieves the",Overall dependency parsing results,Overall dependency parsing results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O']",13,1.0,152,0.5846153846153846,13,1.0,1,results,Overall dependency parsing results,dependency_parsing3
487,159,"As expected , all parsers produce better results for shorter sentences on both corpora ; longer sentences are likely to have longer dependencies which are typically harder to predict precisely .",Parsing result analysis,Parsing result analysis,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0862068965517241,158,0.6076923076923076,5,0.1785714285714285,1,results,Parsing result analysis,dependency_parsing3
488,237,Impact of parsing on event extraction,Evaluation setup,,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n']",24,0.5853658536585366,236,0.9076923076923076,0,0.0,1,results,Evaluation setup,dependency_parsing3
489,238,"The results for parsers trained with the GENIA treebank ( Rows 1 - 6 , ) are generally higher than http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi",Evaluation setup,Impact of parsing on event extraction,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",25,0.6097560975609756,237,0.9115384615384616,1,0.0588235294117647,1,results,Evaluation setup: Impact of parsing on event extraction,dependency_parsing3
490,244,"Among the four dependency parsers trained on GENIA , Stanford - Biaffine , jPTDP and NLP4J - dep produce similar event extraction scores on the development set , while on the the test set jPTDP and NLP4 Jdep obtain the lowest and highest scores , respectively .",Evaluation setup,Impact of parsing on event extraction,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",31,0.7560975609756098,243,0.9346153846153846,7,0.4117647058823529,1,results,Evaluation setup: Impact of parsing on event extraction,dependency_parsing3
491,2,Stack - Pointer Networks for Dependency Parsing,title,,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0042553191489361,1,0.0,1,research-problem,title,dependency_parsing4
492,4,We introduce a novel architecture for dependency parsing : stack - pointer networks ( STACKPTR ) .,abstract,abstract,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1666666666666666,3,0.0127659574468085,1,0.1666666666666666,1,research-problem,abstract,dependency_parsing4
493,11,"Dependency parsing , which predicts the existence and type of linguistic dependency relations between words , is a first step towards deep language understanding .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0434782608695652,10,0.0425531914893617,1,0.0434782608695652,1,research-problem,Introduction,dependency_parsing4
494,25,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.6521739130434783,24,0.102127659574468,15,0.6521739130434783,1,model,Introduction,dependency_parsing4
495,27,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",17,0.7391304347826086,26,0.1106382978723404,17,0.7391304347826086,1,model,Introduction,dependency_parsing4
496,28,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",18,0.782608695652174,27,0.1148936170212766,18,0.782608695652174,1,model,Introduction,dependency_parsing4
497,29,"This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees , while maintaining a number of parsing steps linear in the sentence length .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.8260869565217391,28,0.1191489361702127,19,0.8260869565217391,1,model,Introduction,dependency_parsing4
498,128,"For all the parsing models in different languages , we initialize word vectors with pretrained word embeddings .",Implementation Details,Implementation Details,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.2,127,0.5404255319148936,2,0.1111111111111111,1,hyperparameters,Implementation Details,dependency_parsing4
499,129,"For Chinese , Dutch , English , German and Spanish , we use the structured - skipgram embeddings .",Implementation Details,Implementation Details,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.3,128,0.5446808510638298,3,0.1666666666666666,1,hyperparameters,Implementation Details,dependency_parsing4
500,130,For other languages we use Polyglot embeddings .,Implementation Details,Implementation Details,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",4,0.4,129,0.548936170212766,4,0.2222222222222222,1,hyperparameters,Implementation Details,dependency_parsing4
501,132,Parameter optimization is performed with the Adam optimizer with ? 1 = ? 2 = 0.9 . We choose an initial learning rate of ? 0 = 0.001 .,Implementation Details,Optimization .,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.6,131,0.5574468085106383,6,0.3333333333333333,1,hyperparameters,Implementation Details: Optimization .,dependency_parsing4
502,133,The learning rate ?,Implementation Details,Optimization .,dependency_parsing,4,"['O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O']",7,0.7,132,0.5617021276595745,7,0.3888888888888889,1,hyperparameters,Implementation Details: Optimization .,dependency_parsing4
503,136,"To reduce the effects of "" gradient exploding "" , we use gradient clipping of 5.0 .",Implementation Details,Optimization .,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",10,1.0,135,0.574468085106383,10,0.5555555555555556,1,hyperparameters,Implementation Details: Optimization .,dependency_parsing4
504,138,"To mitigate overfitting , we apply dropout .",Dropout Training .,Dropout Training .,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O']",1,0.1428571428571428,137,0.5829787234042553,12,0.6666666666666666,1,hyperparameters,Dropout Training .,dependency_parsing4
505,139,"For BLSTM , we use recurrent dropout with a drop rate of 0.33 between hidden states and 0.33 between layers .",Dropout Training .,Dropout Training .,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']",2,0.2857142857142857,138,0.5872340425531914,13,0.7222222222222222,1,hyperparameters,Dropout Training .,dependency_parsing4
506,140,"Following , we also use embedding dropout with a rate of 0.33 on all word , character , and POS embeddings .",Dropout Training .,Dropout Training .,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.4285714285714285,139,0.5914893617021276,14,0.7777777777777778,1,hyperparameters,Dropout Training .,dependency_parsing4
507,162,"We compare the performance of four variations of our model with different decoder inputs - Org , + gpar , + sib and Full - where the Org model utilizes only the encoder hidden states of head words , while the + gpar and + sib models augments the original one with grandparent and sibling information , respectively .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",2,0.0392156862745098,161,0.6851063829787234,2,0.1176470588235294,1,results,Main Results,dependency_parsing4
508,166,"An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.1176470588235294,165,0.7021276595744681,6,0.3529411764705882,1,results,Main Results,dependency_parsing4
509,168,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.1568627450980392,167,0.7106382978723405,8,0.4705882352941176,1,results,Main Results,dependency_parsing4
510,169,The results of our parser on RA are slightly worse than BIAF .,Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",9,0.1764705882352941,168,0.7148936170212766,9,0.5294117647058824,1,results,Main Results,dependency_parsing4
511,172,"Our Full model significantly outperforms all the transition - based parsers on all three languages , and achieves better results than most graph - based parsers .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.2352941176470588,171,0.7276595744680852,12,0.7058823529411765,1,results,Main Results,dependency_parsing4
512,175,"re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2941176470588235,174,0.7404255319148936,15,0.8823529411764706,1,results,Main Results,dependency_parsing4
513,176,"Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",16,0.3137254901960784,175,0.7446808510638298,16,0.9411764705882352,1,results,Main Results,dependency_parsing4
514,186,"Consistent with the analysis in , STACKPTR tends to perform better on shorter sentences , which make fewer parsing decisions , significantly reducing the chance of error propagation .",Main Results,Sentence Length .,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",26,0.5098039215686274,185,0.7872340425531915,4,0.1379310344827586,1,results,Main Results: Sentence Length .,dependency_parsing4
515,213,CoNLL,Experiments on Other Treebanks,,dependency_parsing,4,['O'],['B-n'],1,0.0909090909090909,212,0.902127659574468,1,0.5,1,results,Experiments on Other Treebanks,dependency_parsing4
516,220,"First , both BIAF and STACKPTR parsers achieve relatively high parsing accuracies on all the 12 languages - all with UAS are higher than 90 % .",Experiments on Other Treebanks,For UD,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",8,0.7272727272727273,219,0.9319148936170212,5,0.625,1,results,Experiments on Other Treebanks: For UD,dependency_parsing4
517,221,"On nine languages - Catalan , Czech , Dutch , English , French , German , Norwegian , Russian and Spanish - STACKPTR outperforms BIAF for both UAS and LAS .",Experiments on Other Treebanks,For UD,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",9,0.8181818181818182,220,0.9361702127659576,6,0.75,1,results,Experiments on Other Treebanks: For UD,dependency_parsing4
518,222,"On Bulgarian , STACKPTR achieves slightly better UAS while LAS is slightly worse than BIAF .",Experiments on Other Treebanks,For UD,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",10,0.9090909090909092,221,0.9404255319148936,7,0.875,1,results,Experiments on Other Treebanks: For UD,dependency_parsing4
519,223,"On Italian and Romanian , BIAF obtains marginally better parsing performance than STACKPTR .",Experiments on Other Treebanks,For UD,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",11,1.0,222,0.9446808510638298,8,1.0,1,results,Experiments on Other Treebanks: For UD,dependency_parsing4
520,2,Structured Training for Neural Network Transition - Based Parsing,title,,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0035087719298245,1,0.0,1,research-problem,title,dependency_parsing5
521,10,Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention .,Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0217391304347826,9,0.031578947368421,1,0.0217391304347826,1,research-problem,Introduction,dependency_parsing5
522,13,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O']",4,0.0869565217391304,12,0.0421052631578947,4,0.0869565217391304,1,model,Introduction,dependency_parsing5
523,22,"Furthermore , because the neural network uses a distributed representation , it is able to model lexical , part - of - speech ( POS ) tag , and arc label similarities in a continuous space .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",13,0.2826086956521739,21,0.0736842105263157,13,0.2826086956521739,1,model,Introduction,dependency_parsing5
524,24,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",15,0.3260869565217391,23,0.0807017543859649,15,0.3260869565217391,1,model,Introduction,dependency_parsing5
525,26,"In addition , by incorporating unlabeled data into training , we further improve the accuracy of our model to 94.26 % UAS / 92.41 % LAS ( 93.46 % UAS / 91.49 % LAS for our greedy model ) .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3695652173913043,25,0.087719298245614,17,0.3695652173913043,1,research-problem,Introduction,dependency_parsing5
526,27,"In our approach we start with the basic structure of , but with a deeper architecture and improvements to the optimization procedure .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",18,0.391304347826087,26,0.0912280701754386,18,0.391304347826087,1,model,Introduction,dependency_parsing5
527,31,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",22,0.4782608695652174,30,0.1052631578947368,22,0.4782608695652174,1,model,Introduction,dependency_parsing5
528,35,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",26,0.5652173913043478,34,0.119298245614035,26,0.5652173913043478,1,model,Introduction,dependency_parsing5
529,47,To this end we generate large quantities of high - confidence parse trees by parsing an unlabeled corpus and selecting only the sentences on which two different parsers produced the same parse trees .,Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",38,0.8260869565217391,46,0.1614035087719298,38,0.8260869565217391,1,model,Introduction,dependency_parsing5
530,48,"This idea comes from tri-training and while applicable to other parsers as well , we show that it benefits neural network parsers more than models with discrete features .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'I-n', 'O']",39,0.8478260869565217,47,0.1649122807017544,39,0.8478260869565217,1,model,Introduction,dependency_parsing5
531,173,"We use a CRF - based POS tagger to generate 5 fold jack - knifed POS tags on the training set and predicted tags on the dev , test and tune sets ; our tagger gets comparable accuracy to the Stanford POS tagger with 97 . 44 % on the test set .",Experimental Setup,Experimental Setup,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'I-n', 'O']",5,0.3125,172,0.6035087719298246,5,0.3125,1,results,Experimental Setup,dependency_parsing5
532,177,We train on the union of each corpora 's training set and test on each domain separately .,Experimental Setup,Experimental Setup,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O']",9,0.5625,176,0.6175438596491228,9,0.5625,1,baselines,Experimental Setup,dependency_parsing5
533,180,"We process it with the Berkeley - Parser , a latent variable constituency parser , and a reimplementation of ZPar , a transition - based parser with beam search .",Experimental Setup,Experimental Setup,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",12,0.75,179,0.6280701754385964,12,0.75,1,baselines,Experimental Setup,dependency_parsing5
534,196,randomly using a Gaussian distribution with variance 10 ?4 .,Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.0833333333333333,195,0.6842105263157895,2,0.0833333333333333,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
535,197,"We used fixed initialization with bi = 0.2 , to ensure that most Relu units are activated during the initial rounds of training .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.125,196,0.6877192982456141,3,0.125,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
536,199,"For the word embedding matrix E word , we initialized the parameters using pretrained word embeddings .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.2083333333333333,198,0.6947368421052632,5,0.2083333333333333,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
537,201,"For words not appearing in the unsupervised data and the special "" NULL "" etc. tokens , we used random initialization .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",7,0.2916666666666667,200,0.7017543859649122,7,0.2916666666666667,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
538,205,All hyperparameters ( including structure ) were tuned using Section 24 of the WSJ only .,Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O']",11,0.4583333333333333,204,0.7157894736842105,11,0.4583333333333333,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
539,206,"When not tri-training , we used hyperparameters of ? = 0.2 , ? 0 = 0.05 , = 0.9 , early stopping after roughly 16 hours of training time .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",12,0.5,205,0.7192982456140351,12,0.5,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
540,209,"For the Treebank Union setup , we set M 1 = M 2 = 1024 for the standard training set and for the tri-training setup .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",15,0.625,208,0.7298245614035088,15,0.625,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
541,211,We compare to the best dependency parsers in the literature .,Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",17,0.7083333333333334,210,0.7368421052631579,17,0.7083333333333334,1,baselines,Model Initialization & Hyperparameters,dependency_parsing5
542,213,"On the WSJ and Web tasks , our parser outperforms all dependency parsers in our comparison by a substantial margin .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.7916666666666666,212,0.743859649122807,19,0.7916666666666666,1,results,Model Initialization & Hyperparameters,dependency_parsing5
543,214,"The Question ( QTB ) dataset is more sensitive to the smaller beam size we use in order to train the models in a reasonable time ; if we increase to B = 32 at inference time only , our perceptron performance goes up to 92.29 % LAS .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",20,0.8333333333333334,213,0.7473684210526316,20,0.8333333333333334,1,results,Model Initialization & Hyperparameters,dependency_parsing5
544,216,"Although tritraining did help the baseline on the dev set , test set performance did not improve significantly .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.9166666666666666,215,0.7543859649122807,22,0.9166666666666666,1,results,Model Initialization & Hyperparameters,dependency_parsing5
545,218,"As expected , tri-training helps most dramatically to increase accuracy on the Treebank Union setup with diverse domains , yielding 0.4 - 1.0 % absolute LAS improvement gains for our most accurate model .",Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",24,1.0,217,0.7614035087719299,24,1.0,1,results,Model Initialization & Hyperparameters,dependency_parsing5
546,242,"While adding a second hidden layer results in a large gain on the tune set , there is no gain on the dev set if pre-trained embeddings are not used .",Results,Impact of Network Structure,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",23,0.46,241,0.8456140350877193,13,0.5416666666666666,1,results,Results: Impact of Network Structure,dependency_parsing5
547,273,"For our neural network model , training on the output of the BerkeleyParser yields only modest gains , while training on the data where the two parsers agree produces significantly better results .",Impact of Tri-Training,Impact of Tri-Training,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",3,0.3,272,0.9543859649122808,3,0.5,1,results,Impact of Tri-Training,dependency_parsing5
548,274,"This was especially pronounced for the greedy models : after tri-training , the greedy neural network model surpasses the BerkeleyParser in accuracy .",Impact of Tri-Training,Impact of Tri-Training,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",4,0.4,273,0.9578947368421052,4,0.6666666666666666,1,results,Impact of Tri-Training,dependency_parsing5
549,275,It is also interesting to note that up - training improved results far more than tri-training for the baseline .,Impact of Tri-Training,Impact of Tri-Training,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O']",5,0.5,274,0.9614035087719298,5,0.8333333333333334,1,results,Impact of Tri-Training,dependency_parsing5
550,278,"Regardless of tri-training , using the structured perceptron improved error rates on some of the common and difficult labels : ROOT , ccomp , cc , conj , and nsubj all improved by > 1 % .",Impact of Tri-Training,Error Analysis,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.8,277,0.9719298245614036,1,0.3333333333333333,1,ablation-analysis,Impact of Tri-Training: Error Analysis,dependency_parsing5
551,2,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING,title,title,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0087719298245614,1,0.0,1,research-problem,title,dependency_parsing6
552,13,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .",INTRODUCTION,INTRODUCTION,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",4,0.0439560439560439,12,0.1052631578947368,4,0.6666666666666666,1,model,INTRODUCTION,dependency_parsing6
553,15,The resulting parser maintains most of the simplicity of neural graph - based approaches while approaching the performance of the SOTA transition - based one .,INTRODUCTION,INTRODUCTION,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.0659340659340659,14,0.1228070175438596,6,1.0,1,model,INTRODUCTION,dependency_parsing6
554,56,"We call this a deep bilinear attention mechanism , as opposed to shallow bilinear attention , which uses the recurrent states directly .",INTRODUCTION,INTRODUCTION,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']",47,0.5164835164835165,55,0.4824561403508772,16,0.7272727272727273,1,model,INTRODUCTION,dependency_parsing6
555,60,We use 100 - dimensional uncased word vectors 2 and POS tag vectors ; three BiLSTM layers ( 400 dimensions in each direction ) ; and 500 - and 100 - dimensional ReLU MLP layers .,INTRODUCTION,INTRODUCTION,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",51,0.5604395604395604,59,0.5175438596491229,20,0.9090909090909092,1,model,INTRODUCTION,dependency_parsing6
556,61,"We also apply dropout at every stage of the model : we drop words and tags ( independently ) ; we drop nodes in the LSTM layers ( input and recurrent connections ) , applying the same dropout mask at every recurrent timestep ( cf. the Bayesian dropout of ) ; and we drop nodes in the MLP layers and classifiers , likewise applying the same dropout mask at every timestep .",INTRODUCTION,INTRODUCTION,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5714285714285714,60,0.5263157894736842,21,0.9545454545454546,1,model,INTRODUCTION,dependency_parsing6
557,62,"We optimize the network with annealed Adam for about 50,000 steps , rounded up to the nearest epoch .",INTRODUCTION,INTRODUCTION,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",53,0.5824175824175825,61,0.5350877192982456,22,1.0,1,model,INTRODUCTION,dependency_parsing6
558,73,What we see is that the deep bilinear model outperforms the others with respect to both speed and accuracy .,INTRODUCTION,DATASETS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",64,0.7032967032967034,72,0.631578947368421,2,0.25,1,experiments,INTRODUCTION: DATASETS,dependency_parsing6
559,74,"The model with shallow bilinear arc and label classifiers gets the same unlabeled performance as the deep model with the same settings , but because the label classifier is much larger ( ( 801 c 801 ) as opposed to ( 101 c 101 ) ) , it runs much slower and overfits .",INTRODUCTION,DATASETS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O']",65,0.7142857142857143,73,0.6403508771929824,3,0.375,1,experiments,INTRODUCTION: DATASETS,dependency_parsing6
560,83,"We find that using three or four layers gets significantly better performance than two layers , and increasing the LSTM sizes from 200 to 300 or 400 dimensions likewise signficantly improves performance .",INTRODUCTION,DATASETS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",74,0.8131868131868132,82,0.7192982456140351,3,0.75,1,experiments,INTRODUCTION: DATASETS,dependency_parsing6
561,87,"We also implemented the coupled input - forget gate LSTM cells ( Cif - LSTM ) suggested by , 6 finding that while the resulting model still slightly underperforms the more popular LSTM cells , the difference between the two is much smaller .",INTRODUCTION,DATASETS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",78,0.8571428571428571,86,0.7543859649122807,2,0.2222222222222222,1,experiments,INTRODUCTION: DATASETS,dependency_parsing6
562,91,"In addition to using relatively extreme dropout in the recurrent and MLP layers mentioned in , we also regularize the input layer .",INTRODUCTION,DATASETS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",82,0.9010989010989012,90,0.7894736842105263,6,0.6666666666666666,1,model,INTRODUCTION: DATASETS,dependency_parsing6
563,94,"Interestingly , not using any tags at all actually results in better performance than using tags without dropout .",INTRODUCTION,DATASETS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'B-p', 'I-n', 'O']",85,0.934065934065934,93,0.8157894736842105,9,1.0,1,experiments,INTRODUCTION: DATASETS,dependency_parsing6
564,102,"Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .",RESULTS,RESULTS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.125,101,0.8859649122807017,1,0.125,1,results,RESULTS,dependency_parsing6
565,2,Training with Exploration Improves a Greedy Stack LSTM Parser,title,,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0097087378640776,1,0.0,1,research-problem,title,dependency_parsing7
566,12,"Coupled with a recursive tree composition function , the feature representation is able to capture information from the entirety of the state , without resorting to locality assumptions that were common in most other transition - based parsers .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.217391304347826,11,0.1067961165048543,5,0.3846153846153846,1,model,Introduction,dependency_parsing7
567,13,"The use of a novel stack LSTM data structure allows the parser to maintain a constant time per-state update , and retain an over all linear parsing time .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.2608695652173913,12,0.116504854368932,6,0.4615384615384615,1,model,Introduction,dependency_parsing7
568,15,"At test time , the parser makes greedy decisions according to the learned model .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",8,0.3478260869565217,14,0.1359223300970873,8,0.6153846153846154,1,model,Introduction,dependency_parsing7
569,17,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O']",10,0.4347826086956521,16,0.1553398058252427,10,0.7692307692307693,1,model,Introduction,dependency_parsing7
570,19,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",12,0.5217391304347826,18,0.174757281553398,12,0.9230769230769232,1,model,Introduction,dependency_parsing7
571,78,The score achieved by the dynamic oracle for English is 93.56 UAS .,Experiments,Experiments,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.1333333333333333,77,0.7475728155339806,2,0.1333333333333333,1,results,Experiments,dependency_parsing7
572,84,"The error - exploring dynamic - oracle training always improves over static oracle training controlling for the transition system , but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle .",Experiments,Experiments,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",8,0.5333333333333333,83,0.8058252427184466,8,0.5333333333333333,1,results,Experiments,dependency_parsing7
573,85,Flattening the sampling distribution ( ? = 0.75 ) is especially beneficial when training with pretrained word embeddings .,Experiments,Experiments,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",9,0.6,84,0.8155339805825242,9,0.6,1,hyperparameters,Experiments,dependency_parsing7
574,11,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'I-n', 'O']",4,0.2,10,0.0421940928270042,4,0.2,1,model,Introduction,dependency_parsing8
575,13,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",6,0.3,12,0.050632911392405,6,0.3,1,model,Introduction,dependency_parsing8
576,15,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",8,0.4,14,0.0590717299578059,8,0.4,1,model,Introduction,dependency_parsing8
577,21,"As discussed in more detail in Section 5 , we also outperform previous structured training approaches used for neural network transitionbased parsing .",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.7,20,0.0843881856540084,14,0.7,1,model,Introduction,dependency_parsing8
578,27,"We also provide a pre-trained , state - of - the art English dependency parser called "" Parsey McParseface , "" which we tuned for a balance of speed , simplicity , and accuracy .",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1.0,26,0.1097046413502109,20,1.0,1,model,Introduction,dependency_parsing8
579,72,We use stochastic gradient descent on the negative log - likelihood of the data under the model .,Training,Training,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']",2,0.0256410256410256,71,0.2995780590717299,2,0.1428571428571428,1,experiments,Training,dependency_parsing8
580,151,"We apply our approach to POS tagging , syntactic dependency parsing , and sentence compression .",Experiments,Experiments,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",2,0.1333333333333333,150,0.6329113924050633,2,0.25,1,baselines,Experiments,dependency_parsing8
581,152,"While directly optimizing the global model defined by Eq. ( 5 ) works well , we found that training the model in two steps achieves the same precision much faster : we first pretrain the network using the local objective given in Eq. ( 4 ) , and then perform additional training steps using the global objective given in Eq. ( 6 ) .",Experiments,Experiments,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2,151,0.6371308016877637,3,0.375,1,ablation-analysis,Experiments,dependency_parsing8
582,156,"Specifically , we use averaged stochastic gradient descent with momentum , and we tune the learning rate , learning rate schedule , momentum , and early stopping time using a separate held - out corpus for each task .",Experiments,Experiments,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.4666666666666667,155,0.6540084388185654,7,0.875,1,hyperparameters,Experiments,dependency_parsing8
583,159,"Part of speech ( POS ) tagging is a classic NLP task , where modeling the structure of the output is important for achieving state - of - the - art performance .",Experiments,Part of Speech Tagging,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.6666666666666666,158,0.6666666666666666,1,0.0333333333333333,1,experiments,Experiments: Part of Speech Tagging,dependency_parsing8
584,167,"We extract features from words , POS tags , and dependency labels from a window of tokens centered on the in - put , as well as features from the history of predictions .",Model Configuration .,Model Configuration .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",2,0.6666666666666666,166,0.70042194092827,9,0.3,1,ablation-analysis,Model Configuration .,dependency_parsing8
585,168,We use a single hidden layer of size 400 .,Model Configuration .,Model Configuration .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",3,1.0,167,0.7046413502109705,10,0.3333333333333333,1,hyperparameters,Model Configuration .,dependency_parsing8
586,171,Our globally normalized model again significantly outperforms the local model .,Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",2,0.3333333333333333,170,0.7172995780590717,13,0.4333333333333333,1,experiments,Results .,dependency_parsing8
587,173,"We also compare to the sentence compression system from , a 3 - layer stacked LSTM which uses dependency label information .",Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.6666666666666666,172,0.7257383966244726,15,0.5,1,baselines,Results .,dependency_parsing8
588,174,"The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings , but our model is roughly 100 faster .",Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.8333333333333334,173,0.729957805907173,16,0.5333333333333333,1,experiments,Results .,dependency_parsing8
589,175,All compressions kept approximately 42 % of the tokens on average and all the models are significantly better than the automatic extractions ( p < 0.05 ) .,Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,174,0.7341772151898734,17,0.5666666666666667,1,experiments,Results .,dependency_parsing8
590,177,"Inspired by the integrated POS tagging and parsing transition system of , we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack .",Model Configuration .,Model Configuration .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']",1,0.3333333333333333,176,0.7426160337552743,19,0.6333333333333333,1,baselines,Model Configuration .,dependency_parsing8
591,178,"We extract the following features on a window 3 tokens centered at the current focus token : word , cluster , character n- gram up to length 3 .",Model Configuration .,Model Configuration .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",2,0.6666666666666666,177,0.7468354430379747,20,0.6666666666666666,1,hyperparameters,Model Configuration .,dependency_parsing8
592,185,Our local model already compares favorably against these methods on average .,The Results .,The Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,184,0.7763713080168776,27,0.9,1,results,The Results .,dependency_parsing8
593,186,"Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .",The Results .,The Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,185,0.7805907172995781,28,0.9333333333333332,1,results,The Results .,dependency_parsing8
594,187,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .",The Results .,The Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.7,186,0.7848101265822784,29,0.9666666666666668,1,experiments,The Results .,dependency_parsing8
595,192,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .",Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O']",1,0.1111111111111111,191,0.8059071729957806,3,0.6,1,experiments,Results .,dependency_parsing8
596,194,Our results also significantly outperform the LSTM - based approaches of .,Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",3,0.3333333333333333,193,0.8143459915611815,5,1.0,1,experiments,Results .,dependency_parsing8
597,2,Bag of Tricks for Efficient Text Classification,title,title,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.010752688172043,1,0.0,1,research-problem,title,document_classification0
598,4,This paper explores a simple and efficient baseline for text classification .,abstract,abstract,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.3333333333333333,3,0.032258064516129,1,0.3333333333333333,1,research-problem,abstract,document_classification0
599,8,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",Introduction,Introduction,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,7,0.075268817204301,1,0.1111111111111111,1,research-problem,Introduction,document_classification0
600,53,Sentiment analysis,Experiments,,document_classification,0,"['O', 'O']","['B-n', 'I-n']",5,1.0,52,0.5591397849462365,0,0.0,1,experiments,Experiments,document_classification0
601,61,"On this task , adding bigram information improves the performance by 1 - 4 % .",Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.2727272727272727,60,0.6451612903225806,8,0.5333333333333333,1,tasks,Results .: We present the results in .,document_classification0
602,62,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",4,0.3636363636363636,61,0.6559139784946236,9,0.6,1,tasks,Results .: We present the results in .,document_classification0
603,63,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.4545454545454545,62,0.6666666666666666,10,0.6666666666666666,1,tasks,Results .: We present the results in .,document_classification0
604,72,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,Dataset and baselines .,Dataset and baselines .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1176470588235294,71,0.7634408602150538,3,0.1666666666666666,1,experiments,Dataset and baselines .,document_classification0
605,76,"The vocabulary size is 297,141 and there are 312,116 unique tags .",Dataset and baselines .,Dataset and baselines .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.3529411764705882,75,0.8064516129032258,7,0.3888888888888889,1,experiments,Dataset and baselines .,document_classification0
606,79,We consider a frequency - based baseline which predicts the most frequent tag .,Dataset and baselines .,We report precision at 1 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",9,0.5294117647058824,78,0.8387096774193549,10,0.5555555555555556,1,baselines,Dataset and baselines .: We report precision at 1 .,document_classification0
607,80,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",Dataset and baselines .,We report precision at 1 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']",10,0.5882352941176471,79,0.8494623655913979,11,0.6111111111111112,1,baselines,Dataset and baselines .: We report precision at 1 .,document_classification0
608,81,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",Dataset and baselines .,We report precision at 1 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",11,0.6470588235294118,80,0.8602150537634409,12,0.6666666666666666,1,results,Dataset and baselines .: We report precision at 1 .,document_classification0
609,83,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",Dataset and baselines .,Results and training time . and 200 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",13,0.7647058823529411,82,0.8817204301075269,14,0.7777777777777778,1,tasks,Dataset and baselines .: Results and training time . and 200 .,document_classification0
610,84,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",Dataset and baselines .,Results and training time . and 200 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.8235294117647058,83,0.8924731182795699,15,0.8333333333333334,1,tasks,Dataset and baselines .: Results and training time . and 200 .,document_classification0
611,2,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION,,,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']",2,0.1666666666666666,1,0.0035211267605633,1,0.0,1,research-problem,,document_classification1
612,5,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",,,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4166666666666667,4,0.0140845070422535,2,0.2222222222222222,1,research-problem,,document_classification1
613,35,"In this paper , we propose to jointly tackle both language and domain transfer .",INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.6470588235294118,34,0.1197183098591549,22,0.6470588235294118,1,approach,INTRODUCTION,document_classification1
614,37,"Using this unlabeled data , we combine the aforementioned cross - lingual methods with recently proposed unsupervised domain adaptation and weak supervision techniques on the task of cross - lingual document classification .",INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",24,0.7058823529411765,36,0.1267605633802817,24,0.7058823529411765,1,approach,INTRODUCTION,document_classification1
615,38,"In particular , we focus on two approaches for domain adaptation .",INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",25,0.7352941176470589,37,0.130281690140845,25,0.7352941176470589,1,approach,INTRODUCTION,document_classification1
616,39,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",26,0.7647058823529411,38,0.1338028169014084,26,0.7647058823529411,1,approach,INTRODUCTION,document_classification1
617,46,We propose to alleviate this issue by using self - training technique to do the domain adaptation from the source language into the target language .,INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",33,0.9705882352941176,45,0.1584507042253521,33,0.9705882352941176,1,approach,INTRODUCTION,document_classification1
618,79,"In this paper , we adopt the second approach as the basic model , and utilize the XLM model ) as our base model , which has been pre-trained by large - scale parallel and monolingual data from various languages .",BASELINE APPROACHES,BASELINE APPROACHES,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'O']",2,0.0465116279069767,78,0.2746478873239437,2,0.0465116279069767,1,baselines,BASELINE APPROACHES,document_classification1
619,82,SEMI - SUPERVISED XLU,BASELINE APPROACHES,,document_classification,1,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",5,0.1162790697674418,81,0.2852112676056338,5,0.1162790697674418,1,baselines,BASELINE APPROACHES,document_classification1
620,86,Masked Language,BASELINE APPROACHES,SEMI - SUPERVISED XLU,document_classification,1,"['O', 'O']","['B-n', 'I-n']",9,0.2093023255813953,85,0.2992957746478873,9,0.2093023255813953,1,experiments,BASELINE APPROACHES: SEMI - SUPERVISED XLU,document_classification1
621,106,"Alleviating the Train - Test Discrepancy of the UDA Method With the UDA algorithm , the classifier is able to learn some prior information on the target domain , however it still suffers from the train - test discrepancy .",BASELINE APPROACHES,SEMI - SUPERVISED XLU,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",29,0.6744186046511628,105,0.3697183098591549,29,0.6744186046511628,1,experiments,BASELINE APPROACHES: SEMI - SUPERVISED XLU,document_classification1
622,118,"Follwing this process , we obtain a new classifier trained only based on the target domain , which does not suffer from the train - test mismatch problem .",BASELINE APPROACHES,SEMI - SUPERVISED XLU,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",41,0.9534883720930232,117,0.4119718309859155,41,0.9534883720930232,1,baselines,BASELINE APPROACHES: SEMI - SUPERVISED XLU,document_classification1
623,147,"In the sentiment classification task , because the size of the unlabeled corpus in each target domain is large enough , we fine - tune an XLM with MLM loss for each target domain respectively .",EXPERIMENTS,MASKED LANGUAGE MODEL PRE - TRAINING STRATEGY,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']",26,0.9285714285714286,146,0.5140845070422535,23,0.92,1,experiments,EXPERIMENTS: MASKED LANGUAGE MODEL PRE - TRAINING STRATEGY,document_classification1
624,152,Fine-tune ( Ft ) : Fine - tuning the pre-trained model with the source - domain training set .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0307692307692307,151,0.5316901408450704,2,0.054054054054054,1,baselines,MAIN RESULTS,document_classification1
625,171,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.3230769230769231,170,0.5985915492957746,21,0.5675675675675675,1,results,MAIN RESULTS,document_classification1
626,173,"In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",23,0.3538461538461538,172,0.6056338028169014,23,0.6216216216216216,1,results,MAIN RESULTS,document_classification1
627,176,The combination of both methods - as in the UDA ( XLM ft ) model - consistently outperforms either method alone .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",26,0.4,175,0.6161971830985915,26,0.7027027027027027,1,results,MAIN RESULTS,document_classification1
628,178,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",28,0.4307692307692308,177,0.6232394366197183,28,0.7567567567567568,1,results,MAIN RESULTS,document_classification1
629,179,It offers best results in both XLM and XLM ft based classifiers .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",29,0.4461538461538462,178,0.6267605633802817,29,0.7837837837837838,1,results,MAIN RESULTS,document_classification1
630,181,"In the MLdoc dataset , self - training also achieves the best results over all , however the gains are less clear .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4769230769230769,180,0.6338028169014085,31,0.8378378378378378,1,results,MAIN RESULTS,document_classification1
631,183,"Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",33,0.5076923076923077,182,0.6408450704225352,33,0.8918918918918919,1,results,MAIN RESULTS,document_classification1
632,184,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']",34,0.5230769230769231,183,0.6443661971830986,34,0.918918918918919,1,results,MAIN RESULTS,document_classification1
633,186,The experment results show that it lags behind the ones using unlabeled data from the target domain .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",36,0.5538461538461539,185,0.6514084507042254,36,0.972972972972973,1,results,MAIN RESULTS,document_classification1
634,194,"In contrast , the performance of the model improves consistently with more labeled data in the monolingual setting .",MAIN RESULTS,LABELED DATA IN THE SOURCE LANGUAGE HAS LIMITED VALUE,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",44,0.676923076923077,193,0.6795774647887324,6,0.2222222222222222,1,results,MAIN RESULTS: LABELED DATA IN THE SOURCE LANGUAGE HAS LIMITED VALUE,document_classification1
635,213,"From the results , we conclude that t2t is the best performing approach , as it 's the best matched to the target domain .",MAIN RESULTS,ABLATION STUDY : AUGMENTATION STRATEGIES,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",63,0.9692307692307692,212,0.7464788732394366,25,0.925925925925926,1,ablation-analysis,MAIN RESULTS: ABLATION STUDY : AUGMENTATION STRATEGIES,document_classification1
636,2,Neural Attentive Bag - of - Entities Model for Text Classification,title,title,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0060975609756097,1,0.0,1,research-problem,title,document_classification10
637,8,"As a result , our model achieved state - of - the - art results on all datasets .",abstract,abstract,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.8333333333333334,7,0.0426829268292682,5,0.8333333333333334,1,research-problem,abstract,document_classification10
638,21,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .",Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'O']",11,0.6111111111111112,20,0.1219512195121951,11,0.6111111111111112,1,model,Introduction,document_classification10
639,23,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-n', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'O']",13,0.7222222222222222,22,0.1341463414634146,13,0.7222222222222222,1,model,Introduction,document_classification10
640,25,"Furthermore , the attention mechanism improves the interpretability of the model because it enables us to inspect the small number of entities that strongly affect the classification decisions .",Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.8333333333333334,24,0.1463414634146341,15,0.8333333333333334,1,model,Introduction,document_classification10
641,28,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",18,1.0,27,0.1646341463414634,18,1.0,1,code,Introduction,document_classification10
642,64,We initialized the embeddings of words ( v w ) and entities ( v e ) using pretrained embeddings trained on KB .,Experimental Setup,Pretrained Embeddings,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",3,0.125,63,0.3841463414634146,1,0.2,1,hyperparameters,Experimental Setup: Pretrained Embeddings,document_classification10
643,90,FTS- BRNN,Baselines,BoW,document_classification,10,"['O', 'O']","['B-n', 'I-n']",4,0.3076923076923077,89,0.5426829268292683,4,0.3076923076923077,1,baselines,Baselines: BoW,document_classification10
644,92,It uses the logistic regression classifier with the features derived by the RNN .,Baselines,BoW,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",6,0.4615384615384615,91,0.5548780487804879,6,0.4615384615384615,1,baselines,Baselines: BoW,document_classification10
645,94,"Similar to our previous experiment , we also add SWEM - concat , and the variants of our NABoEentity and NABoE - full models based on Wikifier and TAGME ( see Section 4.2 ) .",Baselines,BoW,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.6153846153846154,93,0.5670731707317073,8,0.6153846153846154,1,baselines,Baselines: BoW,document_classification10
646,97,"Overall , our models achieved enhanced performance on this task .",Baselines,BoW,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",11,0.8461538461538461,96,0.5853658536585366,11,0.8461538461538461,1,results,Baselines: BoW,document_classification10
647,98,"In particular , the NABoE - full model successfully outperformed all the baseline models , and the NABoE-entity model achieved competitive performance and outperformed all the baseline models in the literature category .",Baselines,BoW,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.9230769230769232,97,0.5914634146341463,12,0.9230769230769232,1,results,Baselines: BoW,document_classification10
648,101,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .",Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.027027027027027,100,0.6097560975609756,1,0.1111111111111111,1,results,Results,document_classification10
649,102,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",2,0.054054054054054,101,0.6158536585365854,2,0.2222222222222222,1,results,Results,document_classification10
650,103,"Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .",Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",3,0.081081081081081,102,0.6219512195121951,3,0.3333333333333333,1,results,Results,document_classification10
651,104,"Moreover , our attention mechanism consistently improved the performance .",Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'O', 'B-n', 'O']",4,0.1081081081081081,103,0.6280487804878049,4,0.4444444444444444,1,results,Results,document_classification10
652,107,"Further , the models based on the dictionarybased entity detection ( see Section 2.1 ) generally outperformed the models based on the entity linking systems ( i.e. , Wikifier and TAGME ) .",Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O', 'O']",7,0.1891891891891892,106,0.6463414634146342,7,0.7777777777777778,1,results,Results,document_classification10
653,109,"Moreover , our attention mechanism consistently improved the performance for Wikifierand TAGME - based models because the attention mechanism enabled the model to focus on entities that were relevant to the document .",Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2432432432432432,108,0.6585365853658537,9,1.0,1,results,Results,document_classification10
654,131,Factoid Question Answering,Results,,document_classification,10,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",31,0.8378378378378378,130,0.7926829268292683,0,0.0,1,experiments,Results,document_classification10
655,139,"Furthermore , similar to the previous text classification experiment , the attention mechanism and the pretrained embeddings consistently improved the performance .",Results and Analysis,Results and Analysis,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'O']",1,0.1666666666666666,138,0.8414634146341463,1,0.1666666666666666,1,results,Results and Analysis,document_classification10
656,140,"Moreover , the models based on dictionary - based entity detection outperformed the models based on the entity linking systems .",Results and Analysis,Results and Analysis,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",2,0.3333333333333333,139,0.8475609756097561,2,0.3333333333333333,1,results,Results and Analysis,document_classification10
657,2,Task - oriented Word Embedding for Text Classification,title,,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0047619047619047,1,0.0,1,research-problem,title,document_classification11
658,4,Distributed word representation plays a pivotal role in various natural language processing tasks .,abstract,abstract,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0769230769230769,3,0.0142857142857142,1,0.0769230769230769,1,research-problem,abstract,document_classification11
659,15,AI : a combination of active learning and self learning for named entity recognition on twitter using conditional random fields learning :,abstract,Text Classification Expected Distribution Word Embeddings Actual Distribution,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.9230769230769232,14,0.0666666666666666,12,0.9230769230769232,1,research-problem,abstract: Text Classification Expected Distribution Word Embeddings Actual Distribution,document_classification11
660,18,Learning word representation is a fundamental step in various natural language processing tasks .,Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0303030303030303,17,0.0809523809523809,1,0.0303030303030303,1,research-problem,Introduction,document_classification11
661,30,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .",Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O']",13,0.3939393939393939,29,0.1380952380952381,13,0.3939393939393939,1,model,Introduction,document_classification11
662,37,"Specifically , we focus on text classification .",Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",20,0.6060606060606061,36,0.1714285714285714,20,0.6060606060606061,1,model,Introduction,document_classification11
663,39,"In the joint learning framework , the contextual information is captured following the context prediction task introduced by .",Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",22,0.6666666666666666,38,0.1809523809523809,22,0.6666666666666666,1,model,Introduction,document_classification11
664,40,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .",Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']",23,0.696969696969697,39,0.1857142857142857,23,0.696969696969697,1,model,Introduction,document_classification11
665,46,We propose a task - oriented word embedding method that is specially designed for text classification .,Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",29,0.8787878787878788,45,0.2142857142857142,29,0.8787878787878788,1,model,Introduction,document_classification11
666,47,It introduces the function - aware component and highlights word 's functional attributes in the embedding space by regularizing the distribution of words to have a clear classification boundary .,Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",30,0.9090909090909092,46,0.219047619047619,30,0.9090909090909092,1,model,Introduction,document_classification11
667,147,It represents each document as a bag of words and the weighting scheme is TFIDF .,Baseline Methods,Baseline Methods,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.3333333333333333,146,0.6952380952380952,2,0.3333333333333333,1,baselines,Baseline Methods,document_classification11
668,150,"It comprises two models , i.e. , CBOW which predicts the target word using context information , and the Skip - gram ( denoted as SG ) which predicts each context word using the target word ; ( 3 ) the Glo Ve method is a state - of - the - art matrix factorization method .",Baseline Methods,Baseline Methods,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.8333333333333334,149,0.7095238095238096,5,0.8333333333333334,1,baselines,Baseline Methods,document_classification11
669,153,"In this paper , we use the text classification task to evaluate the performance of word embeddings .",Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.0192307692307692,152,0.7238095238095238,1,0.0384615384615384,1,ablation-analysis,Experimental Settings,document_classification11
670,155,"We regard document embedding as a document feature and trained a linear classifier using Liblinear 7 , since the feature size is large , and Liblinear can quickly train a linear classifier with high dimension features .",Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0576923076923076,154,0.7333333333333333,3,0.1153846153846153,1,experimental-setup,Experimental Settings,document_classification11
671,160,"We tokenized the corpus with the Stanford Tokenizer 8 and converted it to lowercase , then removed the stop words .",Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",8,0.1538461538461538,159,0.7571428571428571,8,0.3076923076923077,1,experimental-setup,Experimental Settings,document_classification11
672,161,"For a fair comparison , all word embeddings adhere to the following settings : the dimensionality of vectors is 300 , the size of the context window is 5 , the number of negative samples is 25 .",Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",9,0.173076923076923,160,0.7619047619047619,9,0.3461538461538461,1,experimental-setup,Experimental Settings,document_classification11
673,164,Group dataset .,Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O']","['B-n', 'I-n', 'O']",12,0.2307692307692307,163,0.7761904761904762,12,0.4615384615384615,1,experiments,Experimental Settings,document_classification11
674,168,Group dataset .,Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O']","['B-n', 'I-n', 'O']",16,0.3076923076923077,167,0.7952380952380952,16,0.6153846153846154,1,experiments,Experimental Settings,document_classification11
675,170,The recommended N is 150 with the constraint that the total size of S is under 1200 based on practical experience .,Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3461538461538461,169,0.8047619047619048,18,0.6923076923076923,1,experimental-setup,Experimental Settings,document_classification11
676,175,"were tuned from 0 to 1 , with a step size of 0.1 .",Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",23,0.4423076923076923,174,0.8285714285714286,23,0.8846153846153846,1,experimental-setup,Experimental Settings,document_classification11
677,176,"The proposed method based on Skip - gram and CBOW reaches optimal performance when ? = 0.4 and ? = 0.3 , respectively .",Experimental Settings,Experimental Settings,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",24,0.4615384615384615,175,0.8333333333333334,24,0.9230769230769232,1,results,Experimental Settings,document_classification11
678,183,"( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",31,0.5961538461538461,182,0.8666666666666667,4,0.2857142857142857,1,results,Experimental Settings: Overall Performance,document_classification11
679,184,"In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'O']",32,0.6153846153846154,183,0.8714285714285714,5,0.3571428571428571,1,results,Experimental Settings: Overall Performance,document_classification11
680,186,"( 2 ) The word embedding methods outperform the basic bag - of - words methods in most cases , indicating the superiority of distributed word representation over the one - hot representation .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.6538461538461539,185,0.8809523809523809,7,0.5,1,results,Experimental Settings: Overall Performance,document_classification11
681,189,( 3 ) The Retrofit method is the knowledge - base enhanced word embedding method .,Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",37,0.7115384615384616,188,0.8952380952380953,10,0.7142857142857143,1,baselines,Experimental Settings: Overall Performance,document_classification11
682,190,"Our method achieves better performance over Retrofit method , indicating that the task - specific features could be more effective compared with general semantic relations constructed by humans in the knowledge bases .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.7307692307692307,189,0.9,11,0.7857142857142857,1,results,Experimental Settings: Overall Performance,document_classification11
683,191,"( 4 ) In sentence classification , such as the MR and SST datasets , it is obvious that TWE achieves a relatively lower performance .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",39,0.75,190,0.9047619047619048,12,0.8571428571428571,1,results,Experimental Settings: Overall Performance,document_classification11
684,193,"Our method outperforms the TWE method on both the document - level and sentence - level tasks , which shows the stability and reliability of modeling taskspecific features in real - world applications .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.7884615384615384,192,0.9142857142857144,14,1.0,1,results,Experimental Settings: Overall Performance,document_classification11
685,2,Graph Convolutional Networks for Text Classification,title,,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0045045045045045,1,0.0,1,research-problem,title,document_classification12
686,4,Text classification is an important and classical problem in natural language processing .,abstract,abstract,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.0135135135135135,1,0.1111111111111111,1,research-problem,abstract,document_classification12
687,14,Text classification is a fundamental problem in natural language processing ( NLP ) .,Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0105263157894736,13,0.0585585585585585,1,0.05,1,research-problem,Introduction,document_classification12
688,22,"In this work , we propose a new graph neural networkbased method for text classification .",Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",9,0.0947368421052631,21,0.0945945945945946,9,0.45,1,model,Introduction,document_classification12
689,24,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .",Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.1157894736842105,23,0.1036036036036036,11,0.55,1,model,Introduction,document_classification12
690,26,We then turn text classification problem into anode classification problem .,Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",13,0.1368421052631579,25,0.1126126126126126,13,0.65,1,model,Introduction,document_classification12
691,28,Our source code is available at https://github. com/yao8839836/text_gcn .,Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",15,0.1578947368421052,27,0.1216216216216216,15,0.75,1,code,Introduction,document_classification12
692,30,We propose a novel graph neural network method for text classification .,Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",17,0.1789473684210526,29,0.1306306306306306,17,0.85,1,model,Introduction,document_classification12
693,40,Deep Learning for Text Classification,Introduction,,document_classification,12,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n']",27,0.2842105263157894,39,0.1756756756756756,0,0.0,1,research-problem,Introduction,document_classification12
694,113,Can our model learn predictive word and document embeddings ?,Experiment,Experiment,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",4,1.0,112,0.5045045045045045,4,0.0373831775700934,1,experiments,Experiment,document_classification12
695,116,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0196078431372549,115,0.5180180180180181,7,0.0654205607476635,1,baselines,Baselines .,document_classification12
696,117,Logistic Regression is used as the classifier .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']",3,0.0294117647058823,116,0.5225225225225225,8,0.0747663551401869,1,baselines,Baselines .,document_classification12
697,118,CNN : Convolutional Neural Network ( Kim 2014 ) .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",4,0.0392156862745098,117,0.527027027027027,9,0.0841121495327102,1,baselines,Baselines .,document_classification12
698,120,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",6,0.0588235294117647,119,0.536036036036036,11,0.102803738317757,1,baselines,Baselines .,document_classification12
699,122,"Bi- LSTM : a bi-directional LSTM , commonly used in text classification .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",8,0.0784313725490196,121,0.545045045045045,13,0.1214953271028037,1,baselines,Baselines .,document_classification12
700,123,We input pre-trained word embeddings to Bi - LSTM .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",9,0.088235294117647,122,0.5495495495495496,14,0.1308411214953271,1,baselines,Baselines .,document_classification12
701,124,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",10,0.0980392156862745,123,0.5540540540540541,15,0.1401869158878504,1,baselines,Baselines .,document_classification12
702,125,We used Logistic Regression as the classifier .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",11,0.1078431372549019,124,0.5585585585585585,16,0.1495327102803738,1,baselines,Baselines .,document_classification12
703,126,"PV - DM : a paragraph vector model proposed by , which considers the word order .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.1176470588235294,125,0.5630630630630631,17,0.1588785046728972,1,baselines,Baselines .,document_classification12
704,127,We used Logistic Regression as the classifier .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",13,0.1274509803921568,126,0.5675675675675675,18,0.1682242990654205,1,baselines,Baselines .,document_classification12
705,128,"PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",14,0.1372549019607843,127,0.5720720720720721,19,0.1775700934579439,1,baselines,Baselines .,document_classification12
706,129,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",15,0.1470588235294117,128,0.5765765765765766,20,0.1869158878504672,1,baselines,Baselines .,document_classification12
707,131,"SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",17,0.1666666666666666,130,0.5855855855855856,22,0.205607476635514,1,baselines,Baselines .,document_classification12
708,132,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.1764705882352941,131,0.5900900900900901,23,0.2149532710280373,1,baselines,Baselines .,document_classification12
709,134,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",20,0.196078431372549,133,0.5990990990990991,25,0.2336448598130841,1,baselines,Baselines .,document_classification12
710,135,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) . ,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2058823529411764,134,0.6036036036036037,26,0.2429906542056074,1,baselines,Baselines .,document_classification12
711,136,Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",22,0.2156862745098039,135,0.6081081081081081,27,0.2523364485981308,1,baselines,Baselines .,document_classification12
712,155,"For Text GCN , we set the embedding size of the first convolution layer as 200 and set the window size as 20 .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",41,0.4019607843137255,154,0.6936936936936937,46,0.4299065420560747,1,hyperparameters,Baselines .: Settings .,document_classification12
713,158,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.4313725490196078,157,0.7072072072072072,49,0.4579439252336448,1,hyperparameters,Baselines .: Settings .,document_classification12
714,161,"Text GCN performs the best and significantly outperforms all baseline models ( p < 0.05 based on student t- test ) on four datasets , which showcases the effectiveness of the proposed method on long text datasets .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.4607843137254901,160,0.7207207207207207,52,0.4859813084112149,1,results,Baselines .: Settings .,document_classification12
715,162,"For more in - depth performance analysis , we note that TF - IDF + LR performs well on long text datasets like 20 NG and can outperform CNN with randomly initialized word embeddings .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",48,0.4705882352941176,161,0.7252252252252253,53,0.4953271028037383,1,results,Baselines .: Settings .,document_classification12
716,163,"When pre-trained Glo Ve word embeddings are provided , CNN performs much better , especially on Ohsumed and 20 NG .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",49,0.4803921568627451,162,0.7297297297297297,54,0.5046728971962616,1,results,Baselines .: Settings .,document_classification12
717,164,"CNN also achieves the best results on short text dataset MR with pre-trained word embeddings , which shows it can 7 http://nlp.stanford.edu/data/glove.6B.zip model consecutive and short - distance semantics well .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.4901960784313725,163,0.7342342342342343,55,0.514018691588785,1,results,Baselines .: Settings .,document_classification12
718,166,"PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5098039215686274,165,0.7432432432432432,57,0.5327102803738317,1,results,Baselines .: Settings .,document_classification12
719,168,"PV - DM performs worse than PV - DBOW , the only comparable results are on MR , where word orders are more essential .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.5294117647058824,167,0.7522522522522522,59,0.5514018691588785,1,results,Baselines .: Settings .,document_classification12
720,169,The results of PV - DBOW and PV - DM indicate that unsupervised document embeddings are not very discriminative in text classification .,Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",55,0.5392156862745098,168,0.7567567567567568,60,0.5607476635514018,1,results,Baselines .: Settings .,document_classification12
721,170,PTE and fast Text clearly outperform PV - DBOW and PV - DM because they learn document embeddings in a supervised manner so that label information can be utilized to learn more discriminative embeddings .,Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.5490196078431373,169,0.7612612612612613,61,0.5700934579439252,1,results,Baselines .: Settings .,document_classification12
722,171,"The two recent methods SWEM and LEAM perform quite well , which demonstrates the effectiveness of simple pooling methods and label descriptions / embeddings .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.5588235294117647,170,0.7657657657657657,62,0.5794392523364486,1,results,Baselines .: Settings .,document_classification12
723,172,Graph - CNN models also show competitive performances .,Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",58,0.5686274509803921,171,0.7702702702702703,63,0.5887850467289719,1,results,Baselines .: Settings .,document_classification12
724,175,"1 ) the text graph can capture both document - word relations and global word - word relations ; 2 ) the GCN model , as a special form of Laplacian smoothing , computes the new features of anode as the weighted average of itself and its second order neighbors .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",61,0.5980392156862745,174,0.7837837837837838,66,0.616822429906542,1,baselines,Baselines .: Settings .,document_classification12
725,176,"The label information of document nodes can be passed to their neighboring word nodes ( words within the documents ) , then relayed to other word nodes and document nodes thatare neighbor to the first step neighboring word nodes .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",62,0.6078431372549019,175,0.7882882882882883,67,0.6261682242990654,1,baselines,Baselines .: Settings .,document_classification12
726,178,"However , we also observed that Text GCN did not outperform CNN and LSTM - based models on MR .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",64,0.6274509803921569,177,0.7972972972972973,69,0.6448598130841121,1,results,Baselines .: Settings .,document_classification12
727,194,We note that Text GCN can achieve higher test accuracy with limited labeled documents .,Baselines .,Effects of the Size of Labeled Data .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",80,0.7843137254901961,193,0.8693693693693694,85,0.794392523364486,1,results,Baselines .: Effects of the Size of Labeled Data .,document_classification12
728,2,Deep Pyramid Convolutional Neural Networks for Text Categorization,title,title,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0044247787610619,1,0.0,1,research-problem,title,document_classification13
729,4,This paper proposes a low - complexity word - level deep convolutional neural network ( CNN ) architecture for text categorization that can efficiently represent longrange associations in text .,abstract,abstract,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1428571428571428,3,0.0132743362831858,1,0.1428571428571428,1,research-problem,abstract,document_classification13
730,29,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .",Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",18,0.5142857142857142,28,0.1238938053097345,18,0.5142857142857142,1,model,Introduction,document_classification13
731,30,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .",Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-n', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",19,0.5428571428571428,29,0.1283185840707964,19,0.5428571428571428,1,model,Introduction,document_classification13
732,37,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .",Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",26,0.7428571428571429,36,0.1592920353982301,26,0.7428571428571429,1,model,Introduction,document_classification13
733,40,We use max pooling for all pooling layers .,Introduction,,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",29,0.8285714285714286,39,0.1725663716814159,29,0.8285714285714286,1,model,Introduction,document_classification13
734,125,"To minimize a log loss with softmax , minibatch SGD with momentum 0.9 was conducted for n epochs ( n was fixed to 50 for AG , 30 for Yelp.f / p and Dbpedia , and 15 for the rest ) while the learning rate was set to ?",Training protocol,Training protocol,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O']",2,0.1111111111111111,124,0.5486725663716814,16,0.6666666666666666,1,hyperparameters,Training protocol,document_classification13
735,129,The minibatch size was fixed to 100 .,Training protocol,,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",6,0.3333333333333333,128,0.5663716814159292,20,0.8333333333333334,1,hyperparameters,Training protocol,document_classification13
736,130,Regularization was done by weight decay with the parameter 0.0001 and by optional dropout with 0.5 applied to the input to the top layer .,Training protocol,The minibatch size was fixed to 100 .,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'I-n', 'I-n', 'O']",7,0.3888888888888889,129,0.5707964601769911,21,0.875,1,hyperparameters,Training protocol: The minibatch size was fixed to 100 .,document_classification13
737,131,"In some cases overfitting was observed , and so we performed early stopping , based on the validation performance , after reducing the learning rate to 0.1 ?.",Training protocol,The minibatch size was fixed to 100 .,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",8,0.4444444444444444,130,0.5752212389380531,22,0.9166666666666666,1,hyperparameters,Training protocol: The minibatch size was fixed to 100 .,document_classification13
738,132,Weights were initialized by the Gaussian distribution with zero mean and standard deviation 0.01 .,Training protocol,The minibatch size was fixed to 100 .,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",9,0.5,131,0.5796460176991151,23,0.9583333333333334,1,hyperparameters,Training protocol: The minibatch size was fixed to 100 .,document_classification13
739,133,"The discrete input to the region embedding layer was fixed to the bow input , and the region size was chosen from { 1 , 3 , 5 } , while fixing output dimensionality to 250 ( same as convolution layers ) .",Training protocol,The minibatch size was fixed to 100 .,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5555555555555556,132,0.584070796460177,24,1.0,1,hyperparameters,Training protocol: The minibatch size was fixed to 100 .,document_classification13
740,148,The dimensionality of unsupervised embeddings was set to 300 unless otherwise specified .,Models,Weighted square loss,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O']",6,0.8571428571428571,147,0.6504424778761062,6,0.8571428571428571,1,experiments,Models: Weighted square loss,document_classification13
741,151,"In the results below , the depth of DPCNN was fixed to 15 unless otherwise specified .",Results,Results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O']",1,0.3333333333333333,150,0.6637168141592921,1,0.3333333333333333,1,ablation-analysis,Results,document_classification13
742,152,Making it deeper did not substantially improve or degrade accuracy .,Results,Results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'I-n', 'B-n', 'O']",2,0.6666666666666666,151,0.668141592920354,2,0.6666666666666666,1,results,Results,document_classification13
743,155,Large data results,,,document_classification,13,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,154,0.6814159292035398,1,0.0232558139534883,1,experiments,,document_classification13
744,159,"On all the five datasets , DPCNN outperforms all of the previous results , which validates the effectiveness of our approach .",Large data results,Large data results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'I-n', 'O']",4,0.1142857142857142,158,0.6991150442477876,5,0.1162790697674418,1,results,Large data results,document_classification13
745,191,Small data results,,,document_classification,13,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,190,0.8407079646017699,37,0.8604651162790697,1,experiments,,document_classification13
746,195,One difference from the large dataset results is that the strength of shallow models stands out .,Small data results,Small data results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'I-n', 'O']",4,0.1290322580645161,194,0.8584070796460177,41,0.9534883720930232,1,results,Small data results,document_classification13
747,196,"ShallowCNN ( row 2 ) rivals DPCNN ( row 1 ) , and Zhang et al. 's best linear model ( row 3 ) moved up from the worst performer to the third best performer .",Small data results,Small data results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.1612903225806451,195,0.8628318584070797,42,0.9767441860465116,1,results,Small data results,document_classification13
748,205,The error rate improves as the depth increases .,Small data results,,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",14,0.4516129032258064,204,0.9026548672566372,7,0.2916666666666667,1,results,Small data results,document_classification13
749,2,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,title,title,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.00390625,1,0.0,1,research-problem,title,document_classification14
750,16,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .",Introduction,Introduction,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-n', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O']",4,0.036036036036036,15,0.05859375,4,0.1176470588235294,1,approach,Introduction,document_classification14
751,17,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .",Introduction,Introduction,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O']",5,0.045045045045045,16,0.0625,5,0.1470588235294117,1,approach,Introduction,document_classification14
752,19,"A document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",Introduction,Introduction,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",7,0.063063063063063,18,0.0703125,7,0.2058823529411764,1,approach,Introduction,document_classification14
753,31,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",Introduction,JZ15 proposed variations to alleviate these issues .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.1711711711711711,30,0.1171875,19,0.5588235294117647,1,approach,Introduction: JZ15 proposed variations to alleviate these issues .,document_classification14
754,32,LSTM ) is a recurrent neural network .,Introduction,,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",20,0.1801801801801801,31,0.12109375,20,0.5882352941176471,1,approach,Introduction,document_classification14
755,42,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .",Introduction,Our findings are threefold .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",30,0.2702702702702703,41,0.16015625,30,0.8823529411764706,1,experiments,Introduction: Our findings are threefold .,document_classification14
756,43,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .",Introduction,Our findings are threefold .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2792792792792792,42,0.1640625,31,0.9117647058823528,1,experiments,Introduction: Our findings are threefold .,document_classification14
757,46,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,Introduction,Our findings are threefold .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",34,0.3063063063063063,45,0.17578125,34,1.0,1,code,Introduction: Our findings are threefold .,document_classification14
758,76,Supervised LSTM for text categorization,Introduction,,document_classification,14,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n']",64,0.5765765765765766,75,0.29296875,0,0.0,1,research-problem,Introduction,document_classification14
759,93,Pooling : simplifying sub - problems,Introduction,,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",81,0.7297297297297297,92,0.359375,2,0.028169014084507,1,research-problem,Introduction,document_classification14
760,102,Chopping for speeding up training,Introduction,,document_classification,14,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n']",90,0.8108108108108109,101,0.39453125,11,0.1549295774647887,1,research-problem,Introduction,document_classification14
761,130,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .",Experiments ( supervised ),The datasets are summarized in .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'O']",6,0.0857142857142857,129,0.50390625,39,0.5492957746478874,1,hyperparameters,Experiments ( supervised ): The datasets are summarized in .,document_classification14
762,133,"RCV1 ( second - level topics only ) and 20 NG are for topic categorization of Reuters news articles and newsgroup messages , respectively .",Experiments ( supervised ),The datasets are summarized in .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",9,0.1285714285714285,132,0.515625,42,0.5915492957746479,1,experiments,Experiments ( supervised ): The datasets are summarized in .,document_classification14
763,135,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,Experiments ( supervised ),The datasets are summarized in .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",11,0.1571428571428571,134,0.5234375,44,0.6197183098591549,1,experiments,Experiments ( supervised ): The datasets are summarized in .,document_classification14
764,139,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",Experiments ( supervised ),The datasets are summarized in .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2142857142857142,138,0.5390625,48,0.676056338028169,1,experiments,Experiments ( supervised ): The datasets are summarized in .,document_classification14
765,142,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .",Experiments ( supervised ),Now we review the non -LSTM baseline methods .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",18,0.2571428571428571,141,0.55078125,51,0.7183098591549296,1,tasks,Experiments ( supervised ): Now we review the non -LSTM baseline methods .,document_classification14
766,143,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",Experiments ( supervised ),Now we review the non -LSTM baseline methods .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.2714285714285714,142,0.5546875,52,0.7323943661971831,1,results,Experiments ( supervised ): Now we review the non -LSTM baseline methods .,document_classification14
767,144,"However , on RCV1 , it underperforms both .",Experiments ( supervised ),,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'O']",20,0.2857142857142857,143,0.55859375,53,0.7464788732394366,1,results,Experiments ( supervised ),document_classification14
768,147,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'O']",23,0.3285714285714285,146,0.5703125,56,0.7887323943661971,1,results,"Experiments ( supervised ): However , on RCV1 , it underperforms both .",document_classification14
769,148,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",24,0.3428571428571428,147,0.57421875,57,0.8028169014084507,1,results,"Experiments ( supervised ): However , on RCV1 , it underperforms both .",document_classification14
770,150,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",26,0.3714285714285714,149,0.58203125,59,0.8309859154929577,1,results,"Experiments ( supervised ): However , on RCV1 , it underperforms both .",document_classification14
771,156,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']",32,0.4571428571428571,155,0.60546875,65,0.9154929577464788,1,results,"Experiments ( supervised ): However , on RCV1 , it underperforms both .",document_classification14
772,158,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.4857142857142857,157,0.61328125,67,0.943661971830986,1,results,"Experiments ( supervised ): However , on RCV1 , it underperforms both .",document_classification14
773,160,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",36,0.5142857142857142,159,0.62109375,69,0.971830985915493,1,results,Experiments ( supervised ): Comparison with the previous best results on 20 NG,document_classification14
774,161,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",37,0.5285714285714286,160,0.625,70,0.9859154929577464,1,results,Experiments ( supervised ): Comparison with the previous best results on 20 NG,document_classification14
775,175,"The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one - hot CNN , resulting in higher accuracy .",Experiments ( supervised ),Consider two views of the input .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",51,0.7285714285714285,174,0.6796875,12,0.75,1,tasks,Experiments ( supervised ): Consider two views of the input .,document_classification14
776,205,"Compared with the supervised oh - 2 LSTMp , clear performance improvements were obtained on all the datasets , thus , confirming the effectiveness of our approach .",Semi-supervised experiments,Other details followed the supervised experiments .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.196078431372549,204,0.796875,10,0.3571428571428571,1,results,Semi-supervised experiments: Other details followed the supervised experiments .,document_classification14
777,210,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .",Semi-supervised experiments,Other details followed the supervised experiments .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.2941176470588235,209,0.81640625,15,0.5357142857142857,1,results,Semi-supervised experiments: Other details followed the supervised experiments .,document_classification14
778,216,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .",Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",21,0.4117647058823529,215,0.83984375,21,0.75,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
779,217,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .",Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']",22,0.4313725490196078,216,0.84375,22,0.7857142857142857,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
780,218,"Still , it underperformed the models with region tv - embeddings ( row # 4 , 5 ) , which used the same domain unlabeled data .",Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",23,0.4509803921568627,217,0.84765625,23,0.8214285714285714,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
781,221,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'O']",26,0.5098039215686274,220,0.859375,26,0.9285714285714286,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
782,222,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5294117647058824,221,0.86328125,27,0.9642857142857144,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
783,238,"For example , adding the CNN tv-embeddings to the LSTM of row# 1 , the error rate on IMDB improved from 6.66 to 5.94 , and adding the LSTM tv-embeddings to the CNN of row # 2 , the error rate on RCV1 improved from 7.71 to 7.15 .",Semi-supervised experiments,Error rates ( % ) .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O']",43,0.8431372549019608,237,0.92578125,14,0.9333333333333332,1,results,Semi-supervised experiments: Error rates ( % ) .,document_classification14
784,239,"The results indicate that , as expected , LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined .",Semi-supervised experiments,Error rates ( % ) .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'O']",44,0.8627450980392157,238,0.9296875,15,1.0,1,results,Semi-supervised experiments: Error rates ( % ) .,document_classification14
785,243,"The best supervised results on IMDB / Elec of JZ15a are in the first row , obtained by integrating a document embedding layer into one - hot CNN .",Semi-supervised experiments,Comparison with the previous best results,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",48,0.9411764705882352,242,0.9453125,3,0.5,1,results,Semi-supervised experiments: Comparison with the previous best results,document_classification14
786,246,"As shown in the last row of , our new model further improved it to 5.94 ; also on Elec and RCV1 , our best models exceeded the previous best results .",Semi-supervised experiments,Comparison with the previous best results,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",51,1.0,245,0.95703125,6,1.0,1,results,Semi-supervised experiments: Comparison with the previous best results,document_classification14
787,2,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,,,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']",2,0.25,1,0.0049261083743842,1,0.0,1,research-problem,,document_classification15
788,6,We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself .,,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.75,5,0.0246305418719211,3,0.6,1,model,,document_classification15
789,13,"It improves not only robustness to adversarial examples , but also generalization performance for original examples .",INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.1739130434782608,12,0.0591133004926108,4,0.1739130434782608,1,model,INTRODUCTION,document_classification15
790,16,"This is done by regularizing the model so that given an example , the model will produce the same output distribution as it produces on an adversarial perturbation of that example .",INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",7,0.3043478260869565,15,0.0738916256157635,7,0.3043478260869565,1,model,INTRODUCTION,document_classification15
791,22,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .",INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",13,0.5652173913043478,21,0.1034482758620689,13,0.5652173913043478,1,model,INTRODUCTION,document_classification15
792,100,All experiments used TensorFlow on GPUs .,EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",1,0.05,99,0.4876847290640394,1,0.05,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
793,101,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text.,EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n']",2,0.1,100,0.4926108374384236,2,0.1,1,code,EXPERIMENTAL SETTINGS,document_classification15
794,113,We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",14,0.7,112,0.5517241379310345,14,0.7,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
795,114,"To reduce runtime on GPU , we used truncated backpropagation up to 400 words from each end of the sequence .",EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O']",15,0.75,113,0.5566502463054187,15,0.75,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
796,115,"For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .",EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",16,0.8,114,0.5615763546798029,16,0.8,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
797,116,"For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .",EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.85,115,0.5665024630541872,17,0.85,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
798,119,Pretraining with a recurrent language model was very effective on classification performance on all the datasets we tested on and so our results in Section 5 are with this pretraining .,EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1.0,118,0.5812807881773399,20,1.0,1,results,EXPERIMENTAL SETTINGS,document_classification15
799,124,"For optimization , we again used the Adam optimizer , with 0.0005 initial learning rate 0.9998 exponential decay .",TRAINING CLASSIFICATION MODELS,TRAINING CLASSIFICATION MODELS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'O']",4,0.1428571428571428,123,0.6059113300492611,4,0.1428571428571428,1,experimental-setup,TRAINING CLASSIFICATION MODELS,document_classification15
800,125,"Batch sizes are 64 on IMDB , Elec , RCV1 , and 128 on DBpedia .",TRAINING CLASSIFICATION MODELS,TRAINING CLASSIFICATION MODELS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']",5,0.1785714285714285,124,0.6108374384236454,5,0.1785714285714285,1,experimental-setup,TRAINING CLASSIFICATION MODELS,document_classification15
801,129,We again applied gradient clipping with the norm as 1.0 on all the parameters except the word embedding .,TRAINING CLASSIFICATION MODELS,TRAINING CLASSIFICATION MODELS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",9,0.3214285714285714,128,0.6305418719211823,9,0.3214285714285714,1,experimental-setup,TRAINING CLASSIFICATION MODELS,document_classification15
802,148,Every adversarial training method outperformed every random perturbation method .,TRAINING CLASSIFICATION MODELS,TRAINING CLASSIFICATION MODELS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",28,1.0,147,0.7241379310344828,28,1.0,1,results,TRAINING CLASSIFICATION MODELS,document_classification15
803,159,"For the baseline and random perturbation method , the cosine distances were 0.361 and 0.377 , respectively .",RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",10,0.3125,158,0.7783251231527094,9,0.3,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
804,166,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .",RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.53125,165,0.812807881773399,16,0.5333333333333333,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
805,167,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']",18,0.5625,166,0.8177339901477833,17,0.5666666666666667,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
806,170,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .",RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.65625,169,0.8325123152709359,20,0.6666666666666666,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
807,180,"We can see that the baseline method has already achieved nearly the current state of the art performance , and our proposed method improves from the baseline method .",RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",31,0.96875,179,0.8817733990147784,30,1.0,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
808,2,A C - LSTM Neural Network for Text Classification,title,,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0045871559633027,1,0.0,1,research-problem,title,document_classification16
809,4,Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling .,abstract,abstract,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1428571428571428,3,0.0137614678899082,1,0.1428571428571428,1,research-problem,abstract,document_classification16
810,8,C - LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics .,abstract,abstract,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.7142857142857143,7,0.0321100917431192,5,0.7142857142857143,1,research-problem,abstract,document_classification16
811,29,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.6923076923076923,28,0.128440366972477,18,0.6923076923076923,1,model,Introduction,document_classification16
812,30,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",19,0.7307692307692307,29,0.1330275229357798,19,0.7307692307692307,1,model,Introduction,document_classification16
813,31,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",20,0.7692307692307693,30,0.1376146788990825,20,0.7692307692307693,1,model,Introduction,document_classification16
814,32,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']",21,0.8076923076923077,31,0.1422018348623853,21,0.8076923076923077,1,model,Introduction,document_classification16
815,33,"In this way , instead of constructing LSTM directly from the input sentence , we first transform each sentence into successive window ( n- gram ) features to help disentangle factors of variations within sentences .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",22,0.8461538461538461,32,0.146788990825688,22,0.8461538461538461,1,model,Introduction,document_classification16
816,37,"We also show that the combination of CNN and LSTM outperforms individual multi - layer CNN models and RNN models , which indicates that LSTM can learn longterm dependencies from sequences of higher - level representations better than the other models .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,1.0,36,0.1651376146788991,26,1.0,1,experiments,Introduction,document_classification16
817,142,"We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",1,0.0357142857142857,141,0.6467889908256881,1,0.0357142857142857,1,baselines,Experimental Settings,document_classification16
818,143,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O']",2,0.0714285714285714,142,0.6513761467889908,2,0.0714285714285714,1,experimental-setup,Experimental Settings,document_classification16
819,144,"For text preprocessing , we only convert all characters in the dataset to lowercase .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",3,0.1071428571428571,143,0.6559633027522935,3,0.1071428571428571,1,experimental-setup,Experimental Settings,document_classification16
820,145,"For SST , we conduct hyperparameter ( number of filters , filter length in CNN ; memory dimension in LSTM ; dropout rate and which layer to apply , etc. ) tuning on the validation data in the standard split .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",4,0.1428571428571428,144,0.6605504587155964,4,0.1428571428571428,1,experimental-setup,Experimental Settings,document_classification16
821,146,"For TREC , we holdout 1000 samples from the training dataset for hyperparameter search and train the model using the remaining data .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",5,0.1785714285714285,145,0.6651376146788991,5,0.1785714285714285,1,experimental-setup,Experimental Settings,document_classification16
822,147,"In our final settings , we only use one convolutional layer and one LSTM layer for both tasks .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",6,0.2142857142857142,146,0.6697247706422018,6,0.2142857142857142,1,experimental-setup,Experimental Settings,document_classification16
823,148,"For the filter size , we investigated filter lengths of 2 , 3 and 4 in two cases : a ) single convolutional layer with the same filter length , and b ) multiple convolutional layers with different lengths of filters in parallel .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.25,147,0.6743119266055045,7,0.25,1,experimental-setup,Experimental Settings,document_classification16
824,153,Binary is a 2 - classification task .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.4285714285714285,152,0.6972477064220184,12,0.4285714285714285,1,baselines,Experimental Settings,document_classification16
825,155,The third block are methods related to convolutional neural networks .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",14,0.5,154,0.7064220183486238,14,0.5,1,baselines,Experimental Settings,document_classification16
826,158,The last block is our model .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",17,0.6071428571428571,157,0.7201834862385321,17,0.6071428571428571,1,experimental-setup,Experimental Settings,document_classification16
827,159,features after convolution and the sequence of window representations is fed into LSTM .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",18,0.6428571428571429,158,0.7247706422018348,18,0.6428571428571429,1,baselines,Experimental Settings,document_classification16
828,162,We also exploit different combinations of different filter lengths .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",21,0.75,161,0.7385321100917431,21,0.75,1,experimental-setup,Experimental Settings,document_classification16
829,164,"According to the experiments , we choose a single convolutional layer with filter length",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n']",23,0.8214285714285714,163,0.7477064220183486,23,0.8214285714285714,1,experimental-setup,Experimental Settings,document_classification16
830,165,"3 . For SST , the number of filters of length 3 is set to be 150 and the memory dimension of LSTM is set to be 150 , too .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O']",24,0.8571428571428571,164,0.7522935779816514,24,0.8571428571428571,1,experimental-setup,Experimental Settings,document_classification16
831,166,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']",25,0.8928571428571429,165,0.7568807339449541,25,0.8928571428571429,1,experimental-setup,Experimental Settings,document_classification16
832,167,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']",26,0.9285714285714286,166,0.7614678899082569,26,0.9285714285714286,1,experimental-setup,Experimental Settings,document_classification16
833,168,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']",27,0.9642857142857144,167,0.7660550458715596,27,0.9642857142857144,1,experimental-setup,Experimental Settings,document_classification16
834,169,We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",28,1.0,168,0.7706422018348624,28,1.0,1,experimental-setup,Experimental Settings,document_classification16
835,185,"For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .",Results and Model Analysis,Sentiment Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.4838709677419355,184,0.8440366972477065,12,0.7058823529411765,1,results,Results and Model Analysis: Sentiment Classification,document_classification16
836,189,( 2 ) Comparing our results against single CNN and LSTM models shows that LSTM does learn long - term dependencies across sequences of higher - level representations better .,Results and Model Analysis,Sentiment Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.6129032258064516,188,0.8623853211009175,16,0.9411764705882352,1,results,Results and Model Analysis: Sentiment Classification,document_classification16
837,198,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .",Results and Model Analysis,Question Type Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.9032258064516128,197,0.9036697247706422,7,0.7,1,results,Results and Model Analysis: Question Type Classification,document_classification16
838,199,( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,Results and Model Analysis,Question Type Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",29,0.935483870967742,198,0.908256880733945,8,0.8,1,results,Results and Model Analysis: Question Type Classification,document_classification16
839,203,Here we investigate the impact of different filter configurations in the convolutional layer on the model performance .,Model Analysis,Model Analysis,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",1,0.1,202,0.926605504587156,1,0.1,1,ablation-analysis,Model Analysis,document_classification16
840,211,"For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .",Model Analysis,Model Analysis,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,210,0.963302752293578,9,0.9,1,ablation-analysis,Model Analysis,document_classification16
841,2,Very Deep Convolutional Networks for Text Classification,title,title,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0043290043290043,1,0.0,1,research-problem,title,document_classification17
842,6,We present a new architecture ( VD - CNN ) for text processing which operates directly at the character level and uses only small convolutions and pooling operations .,abstract,abstract,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'I-n', 'O', 'B-n', 'I-n', 'O']",3,0.6,5,0.0216450216450216,3,0.6,1,research-problem,abstract,document_classification17
843,17,The fundamental idea of is to consider feature extraction and classification as one jointly trained task .,Introduction,Introduction,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O']",8,0.2285714285714285,16,0.0692640692640692,8,0.2285714285714285,1,model,Introduction,document_classification17
844,35,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .",Introduction,We propose the following analogy .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",26,0.7428571428571429,34,0.1471861471861472,26,0.7428571428571429,1,model,Introduction: We propose the following analogy .,document_classification17
845,43,The proposed deep convolutional network shows significantly better results than previous ConvNets approach .,Introduction,This paper is structured as follows .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",34,0.9714285714285714,42,0.1818181818181818,34,0.9714285714285714,1,experiments,Introduction: This paper is structured as follows .,document_classification17
846,138,Going from depth 9 to 17 and 29 for Amazon Full reduces the error rate by 1 % absolute .,Architecture,Depth improves performance .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",48,0.5783132530120482,137,0.5930735930735931,3,0.0789473684210526,1,results,Architecture: Depth improves performance .,document_classification17
847,141,"Overall , compared to previous state - of - the - art , our best architecture with depth 29 and max - pooling has a test error of 37.0 compared to 40.43 % .",Architecture,Depth improves performance .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",51,0.6144578313253012,140,0.6060606060606061,6,0.1578947368421052,1,results,Architecture: Depth improves performance .,document_classification17
848,144,Max - pooling performs better than other pooling types .,Architecture,This represents again of 3.43 % absolute accuracy .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",54,0.6506024096385542,143,0.6190476190476191,9,0.2368421052631578,1,experiments,Architecture: This represents again of 3.43 % absolute accuracy .,document_classification17
849,148,Our models outperform state - of - the - art Con -vNets .,Architecture,This represents again of 3.43 % absolute accuracy .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",58,0.6987951807228916,147,0.6363636363636364,13,0.3421052631578947,1,experiments,Architecture: This represents again of 3.43 % absolute accuracy .,document_classification17
850,167,"When using shortcut connections , we observe improved results when the network has 49 layers : both the training and test errors go down and the network is less prone to underfitting than it was without shortcut connections .",Architecture,Shortcut connections help reduce the degradation .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.927710843373494,166,0.7186147186147186,32,0.8421052631578947,1,experiments,Architecture: Shortcut connections help reduce the degradation .,document_classification17
851,195,"Following , all processing is done at the character level which is the atomic representation of a sentence , same as pixels for images .",Common model settings,Common model settings,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2142857142857142,194,0.8398268398268398,3,0.2142857142857142,1,baselines,Common model settings,document_classification17
852,198,The character embedding is of size 16 .,Common model settings,,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",6,0.4285714285714285,197,0.8528138528138528,6,0.4285714285714285,1,experimental-setup,Common model settings,document_classification17
853,199,"Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .",Common model settings,The character embedding is of size 16 .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']",7,0.5,198,0.8571428571428571,7,0.5,1,experimental-setup,Common model settings: The character embedding is of size 16 .,document_classification17
854,201,We initialize our convolutional layers following .,Common model settings,,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']",9,0.6428571428571429,200,0.8658008658008658,9,0.6428571428571429,1,experimental-setup,Common model settings,document_classification17
855,204,The implementation is done using Torch 7 .,Common model settings,,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O']",12,0.8571428571428571,203,0.8787878787878788,12,0.8571428571428571,1,experimental-setup,Common model settings,document_classification17
856,205,All experiments are performed on a single NVidia K40 GPU .,Common model settings,The implementation is done using Torch 7 .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.9285714285714286,204,0.8831168831168831,13,0.9285714285714286,1,experimental-setup,Common model settings: The implementation is done using Torch 7 .,document_classification17
857,206,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .",Common model settings,The implementation is done using Torch 7 .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",14,1.0,205,0.8874458874458875,14,1.0,1,experimental-setup,Common model settings: The implementation is done using Torch 7 .,document_classification17
858,210,"Our deep architecture works well on big data sets in particular , even for small depths .",Experimental results,Experimental results,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",3,0.3,209,0.9047619047619048,3,0.3,1,results,Experimental results,document_classification17
859,212,"For the smallest depth we use ( 9 convolutional layers ) , we see that our model already performs better than Zhang 's convolutional baselines ( which includes 6 convolutional layers and has a different architecture ) on the biggest data sets :",Experimental results,Experimental results,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.5,211,0.9134199134199136,5,0.5,1,results,Experimental results,document_classification17
860,213,"Yelp Full , Yahoo Answers and Amazon Full and Polarity .",Experimental results,Experimental results,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.6,212,0.9177489177489178,6,0.6,1,results,Experimental results,document_classification17
861,214,The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples . :,Experimental results,Experimental results,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",7,0.7,213,0.922077922077922,7,0.7,1,results,Experimental results,document_classification17
862,217,"We also observe that for a small depth , temporal max - pooling works best on all data sets .",Experimental results,Best published results from previous work .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",10,1.0,216,0.935064935064935,10,1.0,1,results,Experimental results: Best published results from previous work .,document_classification17
863,2,Character - level Convolutional Networks for Text Classification,title,,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.5,1,0.0044247787610619,1,0.5,1,research-problem,title,document_classification18
864,13,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",Introduction,Introduction,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0204081632653061,12,0.0530973451327433,1,0.0555555555555555,1,research-problem,Introduction,document_classification18
865,18,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",Introduction,Introduction,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",6,0.1224489795918367,17,0.0752212389380531,6,0.3333333333333333,1,model,Introduction,document_classification18
866,30,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .",Introduction,Introduction,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",18,0.3673469387755102,29,0.1283185840707964,18,1.0,1,model,Introduction,document_classification18
867,118,The dimension of the embedding is 300 .,Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",15,0.8823529411764706,117,0.5176991150442478,15,0.8823529411764706,1,experiments,Traditional Methods,document_classification18
868,120,The number of means is 5000 .,Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",17,1.0,119,0.5265486725663717,17,1.0,1,experiments,Traditional Methods,document_classification18
869,2,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling,title,title,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0041322314049586,1,0.0,1,research-problem,title,document_classification19
870,31,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .",Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",16,0.4848484848484848,30,0.1239669421487603,16,0.4848484848484848,1,model,Introduction,document_classification19
871,32,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",17,0.5151515151515151,31,0.128099173553719,17,0.5151515151515151,1,model,Introduction,document_classification19
872,33,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.5454545454545454,32,0.1322314049586777,18,0.5454545454545454,1,model,Introduction,document_classification19
873,34,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",19,0.5757575757575758,33,0.1363636363636363,19,0.5757575757575758,1,model,Introduction,document_classification19
874,36,"This paper proposes a combined framework , which utilizes BLSTM to capture long - term sentence dependencies , and extracts features by 2D convolution and 2D max pooling operation for sequence modeling tasks .",Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",21,0.6363636363636364,35,0.1446280991735537,21,0.6363636363636364,1,model,Introduction,document_classification19
875,38,"This work introduces two combined models BLSTM - 2DPooling and BLSTM - 2DCNN , and verifies them on six text classification tasks , including sentiment analysis , question classification , subjectivity classification , and newsgroups classification .",Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",23,0.696969696969697,37,0.152892561983471,23,0.696969696969697,1,model,Introduction,document_classification19
876,170,"The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .",Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",26,0.8387096774193549,169,0.6983471074380165,4,0.4444444444444444,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
877,171,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .",Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",27,0.8709677419354839,170,0.7024793388429752,5,0.5555555555555556,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
878,172,We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",28,0.9032258064516128,171,0.7066115702479339,6,0.6666666666666666,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
879,173,"For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .",Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",29,0.935483870967742,172,0.7107438016528925,7,0.7777777777777778,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
880,174,These values are chosen via a grid search on the SST - 1 development set .,Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",30,0.967741935483871,173,0.7148760330578512,8,0.8888888888888888,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
881,175,"We only tune these hyperparameters , and more finer tuning , such as using different numbers of hidden units of LSTM layer , or using wide convolution , may further improve the performance .",Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",31,1.0,174,0.71900826446281,9,1.0,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
882,178,"This work implements four models , BLSTM , BLSTM - Att , BLSTM - 2DPooling , and BLSTM - 2DCNN . presents the performance of the four models and other state - of - the - art models on six classification tasks .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0344827586206896,177,0.731404958677686,1,0.0232558139534883,1,results,Results: Overall Performance,document_classification19
883,179,The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.0517241379310344,178,0.7355371900826446,2,0.0465116279069767,1,results,Results: Overall Performance,document_classification19
884,180,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",4,0.0689655172413793,179,0.7396694214876033,3,0.0697674418604651,1,results,Results: Overall Performance,document_classification19
885,181,BLSTM - 2DPooling performs worse than the state - of - the - art models .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.0862068965517241,180,0.743801652892562,4,0.0930232558139534,1,results,Results: Overall Performance,document_classification19
886,183,"BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",7,0.1206896551724138,182,0.7520661157024794,6,0.1395348837209302,1,results,Results: Overall Performance,document_classification19
887,184,"As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",8,0.1379310344827586,183,0.756198347107438,7,0.1627906976744186,1,results,Results: Overall Performance,document_classification19
888,188,"Compared with RCNN , BLSTM - 2DCNN achieves a comparable result .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.2068965517241379,187,0.7727272727272727,11,0.2558139534883721,1,results,Results: Overall Performance,document_classification19
889,192,"BLSTM-2DCNN is an extension of BLSTM - 2DPooling , and the results show that BLSTM - 2DCNN can capture more dependencies in text .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",16,0.2758620689655172,191,0.7892561983471075,15,0.3488372093023256,1,results,Results: Overall Performance,document_classification19
890,193,"Ada Sent utilizes a more complicated model to form a hierarchy of representations , and it outperforms BLSTM - 2DCNN on Subj and MR datasets .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-n', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.293103448275862,192,0.7933884297520661,16,0.3720930232558139,1,results,Results: Overall Performance,document_classification19
891,198,DRNN : Deep recursive neural networks for compositionality in language .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",22,0.3793103448275862,197,0.8140495867768595,21,0.4883720930232558,1,results,Results: Overall Performance,document_classification19
892,200,CNN -nonstatic / MC : Convolutional neural networks for sentence classification .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",24,0.4137931034482758,199,0.8223140495867769,23,0.5348837209302325,1,baselines,Results: Overall Performance,document_classification19
893,202,"Molding - CNN : Molding CNNs for text : non-linear , non-consecutive convolutions .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'O', 'O', 'I-n', 'O']",26,0.4482758620689655,201,0.8305785123966942,25,0.5813953488372093,1,baselines,Results: Overall Performance,document_classification19
894,204,"MVCNN : Multichannel variable - size convolution for sentence classification ( Yin and Schtze , 2016 ) .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.4827586206896552,203,0.8388429752066116,27,0.627906976744186,1,baselines,Results: Overall Performance,document_classification19
895,205,RCNN : Recurrent Convolutional Neural Networks for Text Classification .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",29,0.5,204,0.8429752066115702,28,0.6511627906976745,1,baselines,Results: Overall Performance,document_classification19
896,206,S- LSTM : Long short - term memory over recursive structures .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",30,0.5172413793103449,205,0.8471074380165289,29,0.6744186046511628,1,baselines,Results: Overall Performance,document_classification19
897,207,LSTM / BLSTM / Tree-LSTM :,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",31,0.5344827586206896,206,0.8512396694214877,30,0.6976744186046512,1,baselines,Results: Overall Performance,document_classification19
898,209,LSTMN : Long short - term memory - networks for machine reading .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",33,0.5689655172413793,208,0.859504132231405,32,0.7441860465116279,1,baselines,Results: Overall Performance,document_classification19
899,217,C - LSTM : A C - LSTM Neural Network for Text Classification .,Results,Ada,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",41,0.7068965517241379,216,0.8925619834710744,40,0.9302325581395348,1,baselines,Results: Ada,document_classification19
900,221,Effect of Sentence Length,Results,,document_classification,19,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",45,0.7758620689655172,220,0.9090909090909092,0,0.0,1,results,Results,document_classification19
901,222,It is found that both BLSTM - 2DPooling and BLSTM - 2DCNN outperform the other two models .,Results,Effect of Sentence Length,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",46,0.7931034482758621,221,0.9132231404958676,1,0.25,1,results,Results: Effect of Sentence Length,document_classification19
902,232,"The best accuracy is 52.6 with 2D filter size ( 5 , 5 ) and 2D max pooling size ( 5 , 5 ) , this shows that finer tuning can further improve the performance reported here .",Results,Effect of Sentence Length,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.9655172413793104,231,0.9545454545454546,6,0.75,1,results,Results: Effect of Sentence Length,document_classification19
903,2,Rethinking Complex Neural Network Architectures for Document Classification,title,,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0078125,1,0.0,1,research-problem,title,document_classification2
904,7,"Surprisingly , our simple model is able to achieve these results without attention mechanisms .",abstract,abstract,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.6666666666666666,6,0.046875,4,0.6666666666666666,1,research-problem,abstract,document_classification2
905,25,Our work provides an opensource platform and the foundation for future work in document classification .,Introduction,Introduction,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",15,1.0,24,0.1875,15,1.0,1,model,Introduction,document_classification2
906,67,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .",Experimental Setup,Experimental Setup,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",8,0.4,66,0.515625,8,0.8888888888888888,1,experimental-setup,Experimental Setup,document_classification2
907,68,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,Experimental Setup,Experimental Setup,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",9,0.45,67,0.5234375,9,1.0,1,experimental-setup,Experimental Setup,document_classification2
908,82,"For HAN , we use a batch size of 32 across all the datasets , with a learning rate of 0.01 for Reuters and 0.001 for the rest .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']",2,0.1428571428571428,81,0.6328125,2,0.1,1,experimental-setup,Training and Hyperparameters,document_classification2
909,83,"To train XML - CNN , we select a dynamic pooling window length of eight , a learning rate of 0.001 , and 128 output channels , with batch sizes of 32 and 64 for single - label and multilabel datasets , respectively .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",3,0.2142857142857142,82,0.640625,3,0.15,1,experimental-setup,Training and Hyperparameters,document_classification2
910,84,"For KimCNN , we use a batch size of 64 with a learning rate of 0.01 .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",4,0.2857142857142857,83,0.6484375,4,0.2,1,experimental-setup,Training and Hyperparameters,document_classification2
911,87,"For LSTM reg and LSTM base , we use the Adam optimizer with a learning rate of 0.01 on Reuters and 0.001 on the rest of the datasets , using batch sizes of 32 and 64 for multi-label and single - label tasks , respectively .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",7,0.5,86,0.671875,7,0.35,1,experimental-setup,Training and Hyperparameters,document_classification2
912,88,"For LSTM reg , we also apply temporal averaging ( TA ) : as shown in , TA reduces both generalization error and stochastic noise in recent parameter estimates from stochastic approximation .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5714285714285714,87,0.6796875,8,0.4,1,experimental-setup,Training and Hyperparameters,document_classification2
913,89,We set the default TA exponential smoothing coefficient of ? EMA to 0.99 .,Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",9,0.6428571428571429,88,0.6875,9,0.45,1,experimental-setup,Training and Hyperparameters,document_classification2
914,90,"We choose 512 hidden units for the Bi - LSTM models , whose max - pooled output is regularized using a dropout rate of 0.5 .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",10,0.7142857142857143,89,0.6953125,10,0.5,1,experimental-setup,Training and Hyperparameters,document_classification2
915,91,"We also regularize the input-hidden and hidden - hidden Bi - LSTM connections using embedding dropout and weight dropping , respectively , with dropout rates of 0.1 and 0.2 .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",11,0.7857142857142857,90,0.703125,11,0.55,1,experimental-setup,Training and Hyperparameters,document_classification2
916,92,"For our optimization objective , we use crossentropy and binary cross - entropy loss for singlelabel and multi-label tasks , respectively .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",12,0.8571428571428571,91,0.7109375,12,0.6,1,experimental-setup,Training and Hyperparameters,document_classification2
917,93,"On all datasets and models , we use 300 - dimensional word vectors pre-trained on Google News .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",13,0.9285714285714286,92,0.71875,13,0.65,1,experimental-setup,Training and Hyperparameters,document_classification2
918,94,"We train all neural models for 30 epochs with five random seeds , reporting the mean validation set scores and their corresponding test set results .",Training and Hyperparameters,Training and Hyperparameters,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,1.0,93,0.7265625,14,0.7,1,experimental-setup,Training and Hyperparameters,document_classification2
919,111,"We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0833333333333333,110,0.859375,10,0.4761904761904761,1,results,Baseline Comparison .,document_classification2
920,113,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'O', 'O', 'O']",3,0.25,112,0.875,12,0.5714285714285714,1,results,Baseline Comparison .,document_classification2
921,116,"We also find the accuracy of LSTM reg and our reimplemented version of HAN on Yelp 2014 to be almost two points lower than the copied result of HAN ( rows 6 , 7 , and 10 ) from .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,115,0.8984375,15,0.7142857142857143,1,results,Baseline Comparison .,document_classification2
922,117,"On the other hand , both of the models surpass the original result by nearly two points for the IMDB dataset .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",7,0.5833333333333334,116,0.90625,16,0.7619047619047619,1,results,Baseline Comparison .,document_classification2
923,119,"Interestingly , the non-neural LR and SVM baselines perform remarkably well .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",9,0.75,118,0.921875,18,0.8571428571428571,1,results,Baseline Comparison .,document_classification2
924,120,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.8333333333333334,119,0.9296875,19,0.9047619047619048,1,results,Baseline Comparison .,document_classification2
925,121,"On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.9166666666666666,120,0.9375,20,0.9523809523809524,1,results,Baseline Comparison .,document_classification2
926,2,Practical Text Classification With Large Pre-Trained Language Models,title,,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0046728971962616,1,0.0,1,research-problem,title,document_classification20
927,4,Multi-emotion sentiment classification is a natural language processing ( NLP ) problem with valuable use cases on realworld data .,abstract,abstract,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.014018691588785,1,0.125,1,research-problem,abstract,document_classification20
928,6,"By training an attention - based Transformer network ( Vaswani et al. 2017 ) on 40 GB of text ( Amazon reviews ) ( McAuley et al. 2015 ) and fine - tuning on the training set , our model achieves a 0.69 F1 score on the SemEval Task 1:E - c multidimensional emotion classification problem ( Mohammad et al. 2018 ) , based on the Plutchik wheel of emotions ( Plutchik 1979 ) .",abstract,abstract,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.375,5,0.0233644859813084,3,0.375,1,research-problem,abstract,document_classification20
929,17,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .",Introduction,Introduction,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.3333333333333333,16,0.0747663551401869,5,0.3333333333333333,1,model,Introduction,document_classification20
930,25,"By training a language model across a large text dataset , we expose our model to many contexts .",Introduction,Introduction,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8666666666666667,24,0.1121495327102803,13,0.8666666666666667,1,model,Introduction,document_classification20
931,97,"We also compare our language models to ELMo ) , a contextualized word representation based on a deep bidirectional language model , trained on large text corpus .",ELMo Baseline,ELMo Baseline,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.0833333333333333,96,0.4485981308411215,1,0.0238095238095238,1,baselines,ELMo Baseline,document_classification20
932,107,"We find that the inclusion of easier , more balanced label categories improves performance on harder ones in .",ELMo Baseline,Single Head Finetuning Decoders,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O']",11,0.9166666666666666,106,0.4953271028037383,11,0.2619047619047619,1,results,ELMo Baseline: Single Head Finetuning Decoders,document_classification20
933,110,"For both the multihead MLP and the single linear layer instantiating off d , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as t * = 0.5 .",Thresholding Supervised Results,Thresholding Supervised Results,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0344827586206896,109,0.5093457943925234,14,0.3333333333333333,1,results,Thresholding Supervised Results,document_classification20
934,148,We find that our models outperform Watson on every emotion category .,Results,Multi - Label Emotion Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",9,0.1323529411764706,147,0.6869158878504673,8,0.4,1,results,Results: Multi - Label Emotion Tweets,document_classification20
935,150,"We submitted our finetuned Transformer model to the SemEval Task1:E - C challenge , as seen in Table 6 .",Results,Sem Eval Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1617647058823529,149,0.6962616822429907,10,0.5,1,results,Results: Sem Eval Tweets,document_classification20
936,152,"Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .",Results,Sem Eval Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",13,0.1911764705882352,151,0.705607476635514,12,0.6,1,results,Results: Sem Eval Tweets,document_classification20
937,154,We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .,Results,Sem Eval Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.2205882352941176,153,0.7149532710280374,14,0.7,1,results,Results: Sem Eval Tweets,document_classification20
938,163,"As with the SemEval challenge tweets , the Transformer outperformed the mLSTM .",Results,Plutchik on Company Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'O', 'B-n', 'O']",24,0.3529411764705882,162,0.7570093457943925,2,0.3333333333333333,1,results,Results: Plutchik on Company Tweets,document_classification20
939,165,Both models performed significantly better than the Watson API on all categories for which Watson supplies predictions .,Results,Plutchik on Company Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",26,0.3823529411764705,164,0.7663551401869159,4,0.6666666666666666,1,results,Results: Plutchik on Company Tweets,document_classification20
940,188,"Applying the SemEval - trained Transformer directly to our company tweets dataset gets reasonably good results ( 0.338 macro average ) , also validating that our labeling technique is similar to that of SemEval .",Results,Classification Performance by Dataset Size,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O']",49,0.7205882352941176,187,0.8738317757009346,20,0.5128205128205128,1,results,Results: Classification Performance by Dataset Size,document_classification20
941,189,"Looking at rater agreement by dataset , we see that Plutchik category labels contain large rater dis agreement , even among vetted raters who passed the golden set test .",Results,Classification Performance by Dataset Size,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",50,0.7352941176470589,188,0.8785046728971962,21,0.5384615384615384,1,results,Results: Classification Performance by Dataset Size,document_classification20
942,2,Squeezed Very Deep Convolutional Neural Networks for Text Classification,title,,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0052356020942408,1,0.0,1,research-problem,title,document_classification3
943,19,VDCNN accuracy increases with depth .,I. INTRODUCTION,I. INTRODUCTION,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'O']",10,0.0787401574803149,18,0.094240837696335,16,0.0851063829787234,1,research-problem,I. INTRODUCTION,document_classification3
944,32,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .",I. INTRODUCTION,I. INTRODUCTION,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",23,0.1811023622047244,31,0.1623036649214659,29,0.1542553191489361,1,model,I. INTRODUCTION,document_classification3
945,94,a) Temporal Depthwise Separable Convolutions ( TD - SCs ) :,I. INTRODUCTION,"Later ,",document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",85,0.6692913385826772,93,0.4869109947643979,91,0.4840425531914893,1,model,"I. INTRODUCTION: Later ,",document_classification3
946,124,b) Global Average Pooling ( GAP ) :,I. INTRODUCTION,"Later ,",document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",115,0.905511811023622,123,0.643979057591623,121,0.6436170212765957,1,model,"I. INTRODUCTION: Later ,",document_classification3
947,147,"The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .",EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.5263157894736842,146,0.7643979057591623,144,0.7659574468085106,1,experimental-setup,EXPERIMENTS,document_classification3
948,148,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .",EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",11,0.5789473684210527,147,0.7696335078534031,145,0.7712765957446809,1,experiments,EXPERIMENTS,document_classification3
949,149,All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.631578947368421,148,0.774869109947644,146,0.776595744680851,1,experimental-setup,EXPERIMENTS,document_classification3
950,153,The use of TDSCs promoted a significant reduction in convolutional parameters compared to VDCNN .,EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",16,0.8421052631578947,152,0.7958115183246073,150,0.7978723404255319,1,results,EXPERIMENTS,document_classification3
951,158,The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks .,RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037037037,157,0.8219895287958116,155,0.824468085106383,1,results,RESULTS,document_classification3
952,167,"Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .",RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3703703703703703,166,0.8691099476439791,164,0.8723404255319149,1,results,RESULTS,document_classification3
953,170,The base property of VDCNN model is preserved on its squeezed model : the performance still increasing up with the depth and b),RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4814814814814814,169,0.8848167539267016,167,0.8882978723404256,1,results,RESULTS,document_classification3
954,171,"The performance evaluated for the most extensive dataset , i.e. , Yelp Review ( 62.30 % ) , still overcomes the accuracy of the Char - CNN model ( 62.05 % ) .",RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",14,0.5185185185185185,170,0.8900523560209425,168,0.8936170212765957,1,results,RESULTS,document_classification3
955,2,Joint Embedding of Words and Labels for Text Classification,title,,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0037735849056603,1,0.0,1,research-problem,title,document_classification4
956,15,Text classification is a fundamental problem in natural language processing ( NLP ) .,Introduction,Introduction,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0344827586206896,14,0.0528301886792452,1,0.0526315789473684,1,research-problem,Introduction,document_classification4
957,107,All approaches are better than traditional bag - of - words method .,Model,Testing accuracy,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.1403508771929824,106,0.4,8,0.2962962962962963,1,results,Model: Testing accuracy,document_classification4
958,108,"Our proposed LEAM outperforms the state - of - the - art methods on two largest datasets , i.e. , Yahoo and DBPedia .",Model,Testing accuracy,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",9,0.1578947368421052,107,0.4037735849056604,9,0.3333333333333333,1,results,Model: Testing accuracy,document_classification4
959,119,LEAM consistently outperforms other methods with different proportion of labeled data .,Model,Testing accuracy,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.3508771929824561,118,0.4452830188679245,20,0.7407407407407407,1,results,Model: Testing accuracy,document_classification4
960,199,Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model .,Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.025,198,0.7471698113207547,1,0.125,1,results,Experimental Results,document_classification4
961,200,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.05,199,0.7509433962264151,2,0.25,1,ablation-analysis,Experimental Results,document_classification4
962,202,"We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .",Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",4,0.1,201,0.7584905660377359,4,0.5,1,results,Experimental Results,document_classification4
963,203,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .",Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.125,202,0.7622641509433963,5,0.625,1,ablation-analysis,Experimental Results,document_classification4
964,204,The model is implemented using Tensorflow and is trained on GPU Titan X.,Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n']",6,0.15,203,0.7660377358490567,6,0.75,1,ablation-analysis,Experimental Results,document_classification4
965,205,The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",7,0.175,204,0.769811320754717,7,0.875,1,code,Experimental Results,document_classification4
966,240,"To demonstrate the practical value of label embeddings , we apply LEAM for a real healthcare scenario : medical code prediction on the Electronic Health Records dataset .",Applications to Clinical Text,Applications to Clinical Text,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.125,239,0.9018867924528302,1,0.125,1,experiments,Applications to Clinical Text,document_classification4
967,250,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .",Results,Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.1818181818181818,249,0.939622641509434,2,0.1818181818181818,1,baselines,Results,document_classification4
968,256,"LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .",Results,Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",8,0.7272727272727273,255,0.9622641509433962,8,0.7272727272727273,1,results,Results,document_classification4
969,257,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .",Results,Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.8181818181818182,256,0.9660377358490566,9,0.8181818181818182,1,results,Results,document_classification4
970,2,HDLTex : Hierarchical Deep Learning for Text Classification,title,,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0037878787878787,1,0.0,1,research-problem,title,document_classification5
971,4,"Increasingly large document collections require improved information processing methods for searching , retrieving , and organizing text .",abstract,abstract,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1428571428571428,3,0.0113636363636363,1,0.0057142857142857,1,research-problem,abstract,document_classification5
972,6,Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased .,abstract,abstract,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.4285714285714285,5,0.0189393939393939,3,0.0171428571428571,1,research-problem,abstract,document_classification5
973,15,"Much of the recent work on automatic document classification has involved supervised learning techniques such as classification trees , nave Bayes , support vector machines ( SVM ) , neural nets , and ensemble methods .",I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0225988700564971,14,0.053030303030303,12,0.0685714285714285,1,research-problem,I. INTRODUCTION,document_classification5
974,24,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.0734463276836158,23,0.0871212121212121,21,0.12,1,model,I. INTRODUCTION,document_classification5
975,25,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.0790960451977401,24,0.0909090909090909,22,0.1257142857142857,1,model,I. INTRODUCTION,document_classification5
976,30,Researchers have studied and developed a variety of methods for document classification .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",19,0.1073446327683615,29,0.1098484848484848,27,0.1542857142857142,1,research-problem,I. INTRODUCTION,document_classification5
977,41,This paper uses newer methods of machine learning for document classification taken from deep learning .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",30,0.1694915254237288,40,0.1515151515151515,38,0.2171428571428571,1,model,I. INTRODUCTION,document_classification5
978,60,This paper describes the use of deep learning approaches to create a hierarchical document classification approach .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",49,0.2768361581920904,59,0.2234848484848484,57,0.3257142857142857,1,model,I. INTRODUCTION,document_classification5
979,66,This paper compares fifteen methods for performing document classification .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'O']",55,0.3107344632768362,65,0.2462121212121212,63,0.36,1,model,I. INTRODUCTION,document_classification5
980,74,Multi - Class SVM : Text classification using string kernels within SVMs has been successful in many research projects .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.3559322033898305,73,0.2765151515151515,71,0.4057142857142857,1,research-problem,I. INTRODUCTION,document_classification5
981,84,Stacking Support Vector Machines ( SVM ) :,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",73,0.4124293785310734,83,0.3143939393939394,81,0.4628571428571428,1,model,I. INTRODUCTION,document_classification5
982,232,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.1111111111111111,231,0.875,39,0.7090909090909091,1,experiments,B. Hardware and Implementation,document_classification5
983,238,CNN performs secondbest for three data sets .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.4444444444444444,237,0.8977272727272727,45,0.8181818181818182,1,results,B. Hardware and Implementation,document_classification5
984,239,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5,238,0.9015151515151516,46,0.8363636363636363,1,results,B. Hardware and Implementation,document_classification5
985,244,"For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",14,0.7777777777777778,243,0.9204545454545454,51,0.9272727272727272,1,results,B. Hardware and Implementation,document_classification5
986,246,This is significantly better than all of the others except for the combination of CNN and DNN .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.8888888888888888,245,0.928030303030303,53,0.9636363636363636,1,results,B. Hardware and Implementation,document_classification5
987,247,For data set W OS ? 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2 .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']",17,0.9444444444444444,246,0.9318181818181818,54,0.9818181818181818,1,results,B. Hardware and Implementation,document_classification5
988,254,"Document classification is an important problem to address , given the growing size of scientific literature and other document sets .",C. Empirical Results,C. Empirical Results,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3333333333333333,253,0.9583333333333334,5,0.3333333333333333,1,results,C. Empirical Results,document_classification5
989,256,"This paper introduces a new approach to hierarchical document classification , HDLTex , that combines multiple deep learning approaches to produce hierarchical classifications .",C. Empirical Results,C. Empirical Results,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4666666666666667,255,0.9659090909090908,7,0.4666666666666667,1,baselines,C. Empirical Results,document_classification5
990,257,Testing on a data set of documents obtained from the Web of Science shows that combinations of RNN at the higher level and DNN or CNN at the lower level produced accuracies consistently higher than those obtainable by conventional approaches using nave Bayes or SVM .,C. Empirical Results,C. Empirical Results,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.5333333333333333,256,0.9696969696969696,8,0.5333333333333333,1,results,C. Empirical Results,document_classification5
991,2,Explicit Interaction Model towards Text Classification,title,,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0039682539682539,1,0.0,1,research-problem,title,document_classification6
992,4,Text classification is one of the fundamental tasks in natural language processing .,abstract,abstract,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.0119047619047619,1,0.125,1,research-problem,abstract,document_classification6
993,25,"Thereafter , a fullyconnected ( FC ) layer at the topmost of the network is appended to make the final decision .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",13,0.2166666666666666,24,0.0952380952380952,13,0.3939393939393939,1,model,Introduction,document_classification6
994,29,"Mathematically , it interprets the parameter matrix of the FC layer as a set of class representations ( each column is associated with a class ) .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2833333333333333,28,0.1111111111111111,17,0.5151515151515151,1,model,Introduction,document_classification6
995,31,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",19,0.3166666666666666,30,0.119047619047619,19,0.5757575757575758,1,model,Introduction,document_classification6
996,33,"From the word - level representation , it computes an interaction matrix , in which each entry is the matching score between a word and a class ( dot -product between their representations ) , illustrating the word - level matching signals .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.35,32,0.1269841269841269,21,0.6363636363636364,1,model,Introduction,document_classification6
997,35,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'O', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'O']",23,0.3833333333333333,34,0.1349206349206349,23,0.696969696969697,1,model,Introduction,document_classification6
998,36,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",24,0.4,35,0.1388888888888889,24,0.7272727272727273,1,model,Introduction,document_classification6
999,37,The word - level encoder projects the textual contents into the word - level representations .,Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",25,0.4166666666666667,36,0.1428571428571428,25,0.7575757575757576,1,model,Introduction,document_classification6
1000,38,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O']",26,0.4333333333333333,37,0.1468253968253968,26,0.7878787878787878,1,model,Introduction,document_classification6
1001,42,"In summary , the contributions of this work are threefold : We present a novel framework , EXAM , which leverages the interaction mechanism to explicitly compute the wordlevel interaction signals for the text classification .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",30,0.5,41,0.1626984126984127,30,0.9090909090909092,1,model,Introduction,document_classification6
1002,141,"For the multi -class task , we chose region embedding as the Encoder in EXAM .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",1,0.2,140,0.5555555555555556,12,0.1263157894736842,1,experimental-setup,Hyperparameters,document_classification6
1003,142,The region size is 7 and embedding size is 128 .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.4,141,0.5595238095238095,13,0.1368421052631579,1,experimental-setup,Hyperparameters,document_classification6
1004,143,We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",3,0.6,142,0.5634920634920635,14,0.1473684210526315,1,experimental-setup,Hyperparameters,document_classification6
1005,144,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.8,143,0.5674603174603174,15,0.1578947368421052,1,experimental-setup,Hyperparameters,document_classification6
1006,145,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,1.0,144,0.5714285714285714,16,0.1684210526315789,1,experimental-setup,Hyperparameters,document_classification6
1007,149,1 ) models based on feature engineering ;,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",3,0.0681818181818181,148,0.5873015873015873,20,0.2105263157894736,1,baselines,Baselines,document_classification6
1008,150,"2 ) Char - based deep models , and 3 ) Word - based deep models .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.0909090909090909,149,0.5912698412698413,21,0.2210526315789473,1,baselines,Baselines,document_classification6
1009,154,Models based on feature engineering get the worst results on all the five datasets compared to the other methods .,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",8,0.1818181818181818,153,0.6071428571428571,25,0.2631578947368421,1,results,Baselines,document_classification6
1010,156,Char - based models get the highest over all scores on the two Amazon datasets .,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",10,0.2272727272727272,155,0.6150793650793651,27,0.2842105263157894,1,results,Baselines,document_classification6
1011,159,"For the three char - based baselines , VDCNN gets the best performance on almost all the datasets because it has 29 convolutional layers allowing the model to learn more combinations of characters .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2954545454545454,158,0.626984126984127,30,0.3157894736842105,1,results,Baselines,document_classification6
1012,160,Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",14,0.3181818181818182,159,0.6309523809523809,31,0.3263157894736842,1,results,Baselines,document_classification6
1013,162,"For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3636363636363636,161,0.6388888888888888,33,0.3473684210526316,1,results,Baselines,document_classification6
1014,163,"It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",17,0.3863636363636363,162,0.6428571428571429,34,0.3578947368421052,1,results,Baselines,document_classification6
1015,164,"For the Yah.A. , EXAM improves the best performance by 1.1 % .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.4090909090909091,163,0.6468253968253969,35,0.3684210526315789,1,results,Baselines,document_classification6
1016,165,"Additionally , as a word - based model , EXAM beats all the word - based baselines on the other two Amazon datasets with a performance gain of 1.0 % on the Amazon Full , because our EXAM considers more fine - grained interaction features between classes and words , which is quite helpful in this task .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4318181818181818,164,0.6507936507936508,36,0.3789473684210526,1,results,Baselines,document_classification6
1017,168,We built a model called EXAM Encoder to preserve only the Encoder component with a max pooling layer and FC layer to derive the final probabilities .,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",22,0.5,167,0.6626984126984127,39,0.4105263157894737,1,baselines,Baselines,document_classification6
1018,195,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.064516129032258,194,0.7698412698412699,66,0.6947368421052632,1,experimental-setup,Hyperparameters,document_classification6
1019,196,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'I-n', 'O']",3,0.0967741935483871,195,0.7738095238095238,67,0.7052631578947368,1,experimental-setup,Hyperparameters,document_classification6
1020,197,The accumulated MLP has 60 hidden units .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",4,0.1290322580645161,196,0.7777777777777778,68,0.7157894736842105,1,experimental-setup,Hyperparameters,document_classification6
1021,198,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",5,0.1612903225806451,197,0.7817460317460317,69,0.7263157894736842,1,experimental-setup,Hyperparameters,document_classification6
1022,199,The validation set is applied for early - stopping to avoid overfitting .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",6,0.1935483870967742,198,0.7857142857142857,70,0.7368421052631579,1,experimental-setup,Hyperparameters,document_classification6
1023,200,All hyperparameters are chosen empirically .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'B-n', 'O']",7,0.2258064516129032,199,0.7896825396825397,71,0.7473684210526316,1,experimental-setup,Hyperparameters,document_classification6
1024,210,Word - based models are better than char - based models in Kanshan - Cup dataset .,Hyperparameters,Metrics,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.5483870967741935,209,0.8293650793650794,81,0.8526315789473684,1,results,Hyperparameters: Metrics,document_classification6
1025,212,"For word - based baseline models , all the baselines have similar performance which corroborates the conclusion in FastText ) that simple network is on par with deep learning classifiers in text classification .",Hyperparameters,Metrics,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",19,0.6129032258064516,211,0.8373015873015873,83,0.8736842105263158,1,results,Hyperparameters: Metrics,document_classification6
1026,213,Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .,Hyperparameters,Metrics,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.6451612903225806,212,0.8412698412698413,84,0.8842105263157894,1,results,Hyperparameters: Metrics,document_classification6
1027,2,A Corpus for Multilingual Document Classification in Eight Languages,title,,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0071942446043165,1,0.0,1,research-problem,title,document_classification7
1028,4,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .,abstract,abstract,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.0215827338129496,1,0.1,1,research-problem,abstract,document_classification7
1029,17,There is a large body of research on approaches for document classification .,Introduction,Introduction,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",3,0.0535714285714285,16,0.1151079136690647,3,0.1428571428571428,1,research-problem,Introduction,document_classification7
1030,31,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .",Introduction,Introduction,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']",17,0.3035714285714285,30,0.2158273381294964,17,0.8095238095238095,1,model,Introduction,document_classification7
1031,73,Since the initial work by many alternative approaches to cross -lingual document classification have been developed .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",2,0.0317460317460317,72,0.5179856115107914,2,0.0869565217391304,1,results,Baseline results,document_classification7
1032,76,"In this paper , we propose initial strong baselines which represent two complementary directions of research : one based on the aggregation of multilingual word embeddings , and another one , which directly learns multilingual sentence representations .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.0793650793650793,75,0.539568345323741,5,0.217391304347826,1,baselines,Baseline results,document_classification7
1033,82,"We will name this case "" zero - shot cross - lingual document classification "" .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",11,0.1746031746031746,81,0.5827338129496403,11,0.4782608695652174,1,results,Baseline results,document_classification7
1034,86,This type of cross - lingual document classification needs a very strong multilingual representation since no knowledge on the target language was used during the development of the classifier .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.238095238095238,85,0.6115107913669064,15,0.6521739130434783,1,results,Baseline results,document_classification7
1035,100,"We train a simple one - layer convolutional neural network ( CNN ) on top of the word embeddings , which has shown to perform well on text classification tasks regardless of training data size .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4603174603174603,99,0.7122302158273381,5,0.625,1,baselines,Baseline results,document_classification7
1036,101,"Specifically , convolutional filters are applied to windows of word embeddings , with a max - over - time pooling on top of them .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O']",30,0.4761904761904761,100,0.7194244604316546,6,0.75,1,ablation-analysis,Baseline results,document_classification7
1037,103,"Hyper- parameters such as convolutional output dimension , window sizes are done by grid search over the Dev set of the same language as the train set .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",32,0.5079365079365079,102,0.7338129496402878,8,1.0,1,hyperparameters,Baseline results,document_classification7
1038,105,A second direction of research is to directly learn multilingual sentence representations .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",34,0.5396825396825397,104,0.7482014388489209,1,0.04,1,results,Baseline results,document_classification7
1039,106,"In this paper , we evaluate a recently proposed technique to learn joint multilingual sentence representations .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",35,0.5555555555555556,105,0.7553956834532374,2,0.08,1,baselines,Baseline results,document_classification7
1040,111,"We have developed two versions of the system : one trained on the Europarl corpus to cover the languages English , German , French , Spanish and Italian , and another one trained on the United Nations corpus which allows to learn a joint sentence embedding for English , French , Spanish , Russian and Chinese .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-n', 'O', 'I-n', 'O', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",40,0.6349206349206349,110,0.7913669064748201,7,0.28,1,results,Baseline results,document_classification7
1041,112,We use a one hidden - layer MLP as classifier .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",41,0.6507936507936508,111,0.7985611510791367,8,0.32,1,ablation-analysis,Baseline results,document_classification7
1042,113,"For comparison , we have evaluated its performance on the original subset of RCV2 as used in previous publications on cross - lingual document classification : we are able to outperform the current state - of - the - art in three out of six transfer directions .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",42,0.6666666666666666,112,0.8057553956834532,9,0.36,1,results,Baseline results,document_classification7
1043,114,Zero - shot cross - lingual document classification,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",43,0.6825396825396826,113,0.8129496402877698,10,0.4,1,results,Baseline results,document_classification7
1044,116,The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",45,0.7142857142857143,115,0.8273381294964028,12,0.48,1,results,Baseline results,document_classification7
1045,117,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",46,0.7301587301587301,116,0.8345323741007195,13,0.52,1,results,Baseline results,document_classification7
1046,120,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.7777777777777778,119,0.8561151079136691,16,0.64,1,results,Baseline results,document_classification7
1047,123,Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.8253968253968254,122,0.8776978417266187,19,0.76,1,results,Baseline results,document_classification7
1048,124,Targeted cross - lingual document classification,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",53,0.8412698412698413,123,0.8848920863309353,20,0.8,1,results,Baseline results,document_classification7
1049,2,Disconnected Recurrent Neural Networks for Text Categorization,title,title,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.003875968992248,1,0.0,1,research-problem,title,document_classification8
1050,4,Recurrent neural network ( RNN ) has achieved remarkable performance in text categorization .,abstract,abstract,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.1666666666666666,3,0.0116279069767441,1,0.1666666666666666,1,research-problem,abstract,document_classification8
1051,11,"Text categorization is a fundamental and traditional task in natural language processing ( NLP ) , which can be applied in various applications such as sentiment analysis , question classification and topic classification .",Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0285714285714285,10,0.0387596899224806,1,0.0285714285714285,1,research-problem,Introduction,document_classification8
1052,23,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .",Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.3714285714285714,22,0.0852713178294573,13,0.3714285714285714,1,model,Introduction,document_classification8
1053,27,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .",Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.4857142857142857,26,0.1007751937984496,17,0.4857142857142857,1,model,Introduction,document_classification8
1054,31,We also find that there is a trade - off between position - invariance and long - term dependencies in the DRNN .,Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",21,0.6,30,0.1162790697674418,21,0.6,1,model,Introduction,document_classification8
1055,39,1 . We propose a novel model to incorporate position - variance into RNN .,Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",29,0.8285714285714286,38,0.1472868217054263,29,0.8285714285714286,1,model,Introduction,document_classification8
1056,43,"Based on this , we propose an empirical method to find the optimal window size .",Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",33,0.9428571428571428,42,0.1627906976744186,33,0.9428571428571428,1,model,Introduction,document_classification8
1057,152,We tokenize all the corpus with NLTK 's tokenizer .,Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.0625,151,0.5852713178294574,1,0.0625,1,experimental-setup,Implementation Details,document_classification8
1058,156,"We utilize the 300D Glo Ve 840B vectors ( Pennington et al. , 2014 ) as our pre-trained word embeddings .",Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.3125,155,0.6007751937984496,5,0.3125,1,experimental-setup,Implementation Details,document_classification8
1059,159,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .",Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.5,158,0.6124031007751938,8,0.5,1,experimental-setup,Implementation Details,document_classification8
1060,160,The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']",9,0.5625,159,0.6162790697674418,9,0.5625,1,experimental-setup,Implementation Details,document_classification8
1061,161,"To avoid the gradient explosion problem , we apply gradient norm clipping .",Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",10,0.625,160,0.6201550387596899,10,0.625,1,experimental-setup,Implementation Details,document_classification8
1062,162,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.6875,161,0.624031007751938,11,0.6875,1,experimental-setup,Implementation Details,document_classification8
1063,167,Fast - Text and region embedding methods achieve comparable performance with other CNN and RNN based models .,Implementation Details,DRNN does not have too many hyperparameters .,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,1.0,166,0.6434108527131783,16,1.0,1,results,Implementation Details: DRNN does not have too many hyperparameters .,document_classification8
1064,170,The D - LSTM is a discriminative LSTM model .,Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",2,0.0235294117647058,169,0.6550387596899225,2,0.1666666666666666,1,baselines,Experimental Results,document_classification8
1065,171,Hierarchical attention network ( HAN ) is a hierarchical GRU model with attentive pooling .,Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.0352941176470588,170,0.6589147286821705,3,0.25,1,baselines,Experimental Results,document_classification8
1066,172,We can see that very deep CNN ( VDCNN ) performs well in large datasets .,Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.0470588235294117,171,0.6627906976744186,4,0.3333333333333333,1,results,Experimental Results,document_classification8
1067,174,"By contrast , our proposed model can achieve : DGRU compared with CNN better performance in these datasets by simply setting a large window size .",Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.0705882352941176,173,0.6705426356589147,6,0.5,1,results,Experimental Results,document_classification8
1068,175,Char-CRNN in the fourth block is a model which combines positioninvariance of CNN and long - term dependencies of RNN .,Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",7,0.0823529411764705,174,0.6744186046511628,7,0.5833333333333334,1,baselines,Experimental Results,document_classification8
1069,180,shows that our model achieves 10 - 50 % relative error reduction compared with char - CRNN in these datasets .,Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",12,0.1411764705882353,179,0.6937984496124031,12,1.0,1,results,Experimental Results,document_classification8
1070,181,Comparison with RNN and CNN,Experimental Results,,document_classification,8,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",13,0.1529411764705882,180,0.6976744186046512,0,0.0,1,results,Experimental Results,document_classification8
1071,186,shows that DRNN performs far better than CNN .,Experimental Results,Comparison with RNN and CNN,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",18,0.2117647058823529,185,0.7170542635658915,5,0.4166666666666667,1,results,Experimental Results: Comparison with RNN and CNN,document_classification8
1072,193,Our model DRNN achieves much better performance than GRU and LSTM .,Experimental Results,The experimental results are shown in .,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",25,0.2941176470588235,192,0.7441860465116279,12,1.0,1,results,Experimental Results: The experimental results are shown in .,document_classification8
1073,222,We find that the disconnected naive RNN performs just a little worse than disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU ) when the window size is lower than 5 .,Experimental Results,Recurrent Unit,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",54,0.6352941176470588,221,0.8565891472868217,6,0.2727272727272727,1,results,Experimental Results: Recurrent Unit,document_classification8
1074,226,"DGRU achieves the best performance when the window size is 15 , while the best window size for DLSTM is 5 .",Experimental Results,Recurrent Unit,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",58,0.6823529411764706,225,0.872093023255814,10,0.4545454545454545,1,results,Experimental Results: Recurrent Unit,document_classification8
1075,233,We still conduct the experiments on AG dataset .,Experimental Results,,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",65,0.7647058823529411,232,0.8992248062015504,17,0.7727272727272727,1,results,Experimental Results,document_classification8
1076,235,"From ( b ) , we can see that the DRNN model with max pooling performs better than the others .",Experimental Results,We still conduct the experiments on AG dataset .,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O']",67,0.788235294117647,234,0.9069767441860463,19,0.8636363636363636,1,results,Experimental Results: We still conduct the experiments on AG dataset .,document_classification8
1077,237,We find attentive pooling is not significantly affected by window sizes .,Experimental Results,We still conduct the experiments on AG dataset .,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-n', 'I-n', 'I-p', 'B-n', 'I-n', 'O']",69,0.8117647058823529,236,0.9147286821705426,21,0.9545454545454546,1,results,Experimental Results: We still conduct the experiments on AG dataset .,document_classification8
1078,239,Window size analysis,Experimental Results,,document_classification,8,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",71,0.8352941176470589,238,0.9224806201550388,0,0.0,1,results,Experimental Results,document_classification8
1079,2,Investigating Capsule Networks with Dynamic Routing for Text Classification,title,,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0041152263374485,1,0.0,1,research-problem,title,document_classification9
1080,10,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,abstract,abstract,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,1.0,9,0.037037037037037,7,1.0,1,code,abstract,document_classification9
1081,12,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,Introduction,Introduction,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037037037,11,0.0452674897119341,1,0.037037037037037,1,research-problem,Introduction,document_classification9
1082,25,It then hierarchically builds such pattern extraction pipelines at multiple levels .,Introduction,Introduction,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",14,0.5185185185185185,24,0.0987654320987654,14,0.5185185185185185,1,model,Introduction,document_classification9
1083,139,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",Implementation Details,Implementation Details,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",1,0.25,138,0.5679012345679012,1,0.25,1,hyperparameters,Implementation Details,document_classification9
1084,140,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,Implementation Details,Implementation Details,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",2,0.5,139,0.5720164609053497,2,0.5,1,hyperparameters,Implementation Details,document_classification9
1085,141,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,Implementation Details,Implementation Details,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",3,0.75,140,0.5761316872427984,3,0.75,1,hyperparameters,Implementation Details,document_classification9
1086,142,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,Implementation Details,Implementation Details,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",4,1.0,141,0.5802469135802469,4,1.0,1,hyperparameters,Implementation Details,document_classification9
1087,144,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .",Baseline methods,Baseline methods,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0,143,0.588477366255144,1,0.0,1,baselines,Baseline methods,document_classification9
1088,149,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",Quantitative Evaluation,Quantitative Evaluation,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'I-n', 'O']",3,0.75,148,0.6090534979423868,3,0.75,1,results,Quantitative Evaluation,document_classification9
1089,2,Deep Joint Entity Disambiguation with Local Neural Attention,title,,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0038910505836575,1,0.0,1,research-problem,title,entity_linking0
1090,9,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .,Introduction,Introduction,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0909090909090909,8,0.0311284046692607,1,0.0909090909090909,1,research-problem,Introduction,entity_linking0
1091,15,"In recent years , many text and language understanding tasks have been advanced by neural network architectures .",Introduction,Introduction,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,14,0.0544747081712062,7,0.6363636363636364,1,research-problem,Introduction,entity_linking0
1092,179,All models are implemented in the Torch framework .,Experiments,Training Details and ( Hyper ) Parameters,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",10,0.2439024390243902,178,0.6926070038910506,9,0.225,1,experimental-setup,Experiments: Training Details and ( Hyper ) Parameters,entity_linking0
1093,186,We use Adagrad with a learning rate of 0.3 .,Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",17,0.4146341463414634,185,0.7198443579766537,16,0.4,1,tasks,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
1094,187,"We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .",Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4390243902439024,186,0.7237354085603113,17,0.425,1,tasks,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
1095,199,"Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.7317073170731707,198,0.7704280155642024,29,0.725,1,tasks,Experiments: Local and Global Model Training .,entity_linking0
1096,200,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",31,0.7560975609756098,199,0.77431906614786,30,0.75,1,tasks,Experiments: Local and Global Model Training .,entity_linking0
1097,201,Variable size mini-batches consisting of all mentions in a document are used during training .,Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",32,0.7804878048780488,200,0.7782101167315175,31,0.775,1,experimental-setup,Experiments: Local and Global Model Training .,entity_linking0
1098,203,"Hyper- parameters of the best validated global model are : ? = 0.01 , K = 100 , R = 25 , S = 7 , ? = 0.5 , T = 10 .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8292682926829268,202,0.7859922178988327,33,0.825,1,tasks,Experiments: Local and Global Model Training .,entity_linking0
1099,204,"For the local model , R = 50 was best .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",35,0.8536585365853658,203,0.7898832684824902,34,0.85,1,tasks,Experiments: Local and Global Model Training .,entity_linking0
1100,206,"To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",37,0.902439024390244,205,0.7976653696498055,36,0.9,1,tasks,Experiments: Local and Global Model Training .,entity_linking0
1101,208,"By using diagonal matrices A , B , C , we keep the number of parameters very low ( approx. 1.2 K parameters ) .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'I-n', 'I-n', 'I-n', 'O', 'O']",39,0.951219512195122,207,0.8054474708171206,38,0.95,1,experimental-setup,Experiments: Local and Global Model Training .,entity_linking0
1102,210,"We also experimented with diagonal plus low - rank matrices , but encountered quality degradation .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",41,1.0,209,0.8132295719844358,40,1.0,1,tasks,Experiments: Local and Global Model Training .,entity_linking0
1103,213,Our method outperforms the well established Wikipedia link measure and the method of using less information ( only word - entity statistics ) .,Entity Similarity Results,Entity Similarity Results,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",2,0.0512820512820512,212,0.8249027237354085,2,0.25,1,results,Entity Similarity Results,entity_linking0
1104,218,"We emphasize that our global ED model outperforms Huang 's ED model , likely due to the power of our local and joint neural network architectures .",Entity Similarity Results,Entity Similarity Results,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.1794871794871795,217,0.8443579766536965,7,0.875,1,results,Entity Similarity Results,entity_linking0
1105,2,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation,title,title,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0078740157480314,1,0.0,1,research-problem,title,entity_linking1
1106,5,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .",abstract,abstract,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.2857142857142857,4,0.0314960629921259,2,0.2857142857142857,1,research-problem,abstract,entity_linking1
1107,12,Named entity disambiguation ( NED ) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base ( KB ) .,Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0666666666666666,11,0.0866141732283464,1,0.0666666666666666,1,research-problem,Introduction,entity_linking1
1108,18,"In this paper , we describe a new contextualized embedding model for words and entities for NED .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O']",7,0.4666666666666667,17,0.1338582677165354,7,0.4666666666666667,1,model,Introduction,entity_linking1
1109,19,"Following , the proposed model is based on the bidirectional transformer encoder .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",8,0.5333333333333333,18,0.1417322834645669,8,0.5333333333333333,1,model,Introduction,entity_linking1
1110,20,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.6,19,0.1496062992125984,9,0.6,1,model,Introduction,entity_linking1
1111,21,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",10,0.6666666666666666,20,0.1574803149606299,10,0.6666666666666666,1,model,Introduction,entity_linking1
1112,24,The NED model addresses the task by capturing word - based and entity - based contextual information using the trained contextualized embeddings .,Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",13,0.8666666666666667,23,0.1811023622047244,13,0.8666666666666667,1,model,Introduction,entity_linking1
1113,63,"We also set the feed - forward / filter size to 4096 , the dropout probability applied to all layers was 0.1 , and the maximum word length in an input sequence was set to 512 .",Training,Training,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",3,0.1578947368421052,62,0.4881889763779528,3,0.1764705882352941,1,experiments,Training,entity_linking1
1114,66,"Other parameters , namely the parameters in the MEP and the embeddings for entities , were initialized randomly .",Training,Training,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']",6,0.3157894736842105,65,0.5118110236220472,6,0.3529411764705882,1,hyperparameters,Training,entity_linking1
1115,73,"We used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .",Training,Training,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",13,0.6842105263157895,72,0.5669291338582677,13,0.7647058823529411,1,experiments,Training,entity_linking1
1116,74,The batch size was set to 252 .,Training,Training,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",14,0.7368421052631579,73,0.5748031496062992,14,0.8235294117647058,1,hyperparameters,Training,entity_linking1
1117,106,"We set the batch size to 32 , and used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .",Model,Model,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",11,0.7857142857142857,105,0.8267716535433071,11,0.7857142857142857,1,experiments,Model,entity_linking1
1118,113,"As shown , our models outperformed all previously proposed models .",Results,Results,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']",3,0.75,112,0.8818897637795275,3,0.75,1,results,Results,entity_linking1
1119,114,"Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .",Results,Results,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",4,1.0,113,0.889763779527559,4,1.0,1,results,Results,entity_linking1
1120,2,Deep contextualized word representations,title,,entity_linking,10,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0046728971962616,1,0.0,1,research-problem,title,entity_linking10
1121,12,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O']",4,0.1052631578947368,11,0.0514018691588785,4,0.1081081081081081,1,model,Introduction,entity_linking10
1122,13,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.131578947368421,12,0.0560747663551401,5,0.1351351351351351,1,model,Introduction,entity_linking10
1123,14,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.1578947368421052,13,0.0607476635514018,6,0.1621621621621621,1,model,Introduction,entity_linking10
1124,15,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.1842105263157894,14,0.0654205607476635,7,0.1891891891891892,1,model,Introduction,entity_linking10
1125,16,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",8,0.2105263157894736,15,0.0700934579439252,8,0.2162162162162162,1,model,Introduction,entity_linking10
1126,17,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.2368421052631578,16,0.0747663551401869,9,0.2432432432432432,1,model,Introduction,entity_linking10
1127,20,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision that are most useful for each end task .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",12,0.3157894736842105,19,0.0887850467289719,12,0.3243243243243243,1,model,Introduction,entity_linking10
1128,31,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",23,0.6052631578947368,30,0.1401869158878504,23,0.6216216216216216,1,model,Introduction,entity_linking10
1129,33,context2vec uses a bidirectional Long Short Term Memory ( LSTM ; to encode the context around a pivot word .,Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",25,0.6578947368421053,32,0.1495327102803738,25,0.6756756756756757,1,model,Introduction,entity_linking10
1130,43,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'I-n', 'O', 'O']",35,0.9210526315789472,42,0.1962616822429906,35,0.945945945945946,1,model,Introduction,entity_linking10
1131,45,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",37,0.9736842105263158,44,0.205607476635514,37,1.0,1,model,Introduction,entity_linking10
1132,99,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .",Bidirectional language models,See supplemental material for details .,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",52,0.9629629629629628,98,0.4579439252336448,16,0.8888888888888888,1,experiments,Bidirectional language models: See supplemental material for details .,entity_linking10
1133,107,"After adding ELMo to the baseline model , test set F 1 improved by 4.7 % from 81.1 % to 85.8 % , a 24.9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1.4 % .",Evaluation,Question answering,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.0458715596330275,106,0.4953271028037383,5,0.1724137931034483,1,experiments,Evaluation: Question answering,entity_linking10
1134,116,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",Evaluation,Textual entailment,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",14,0.128440366972477,115,0.5373831775700935,14,0.4827586206896552,1,experiments,Evaluation: Textual entailment,entity_linking10
1135,128,"As shown in , our ELMo enhanced biLSTM - CRF achieves 92. 22 % F 1 averaged over five runs .",Evaluation,Named entity extraction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",26,0.2385321100917431,127,0.5934579439252337,26,0.896551724137931,1,experiments,Evaluation: Named entity extraction,entity_linking10
1136,131,"As shown in Sec. 5.1 , using all layers instead of just the last layer improves performance across multiple tasks .",Evaluation,Named entity extraction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",29,0.2660550458715596,130,0.6074766355140186,29,1.0,1,experiments,Evaluation: Named entity extraction,entity_linking10
1137,132,Sentiment analysis,Evaluation,,entity_linking,10,"['O', 'O']","['B-n', 'I-n']",30,0.2752293577981651,131,0.6121495327102804,0,0.0,1,experiments,Evaluation,entity_linking10
1138,148,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to = 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( = 1 vs. = 0.001 ) .",Evaluation,Alternate layer weighting schemes,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.4220183486238532,147,0.6869158878504673,7,0.1489361702127659,1,experiments,Evaluation: Alternate layer weighting schemes,entity_linking10
1139,155,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves overall results for some tasks .",Evaluation,Alternate layer weighting schemes,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",53,0.4862385321100917,154,0.719626168224299,14,0.2978723404255319,1,experiments,Evaluation: Alternate layer weighting schemes,entity_linking10
1140,156,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .",Evaluation,Alternate layer weighting schemes,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O']",54,0.4954128440366973,155,0.7242990654205608,15,0.3191489361702128,1,experiments,Evaluation: Alternate layer weighting schemes,entity_linking10
1141,161,the task - specific context representations are likely more important than those from the biLM .,Evaluation,All - words fine grained WSD F 1 .,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-n', 'I-p', 'O', 'I-p', 'O', 'B-n', 'O']",59,0.5412844036697247,160,0.7476635514018691,20,0.425531914893617,1,results,Evaluation: All - words fine grained WSD F 1 .,entity_linking10
1142,164,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .",Evaluation,Since adding,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.5688073394495413,163,0.7616822429906542,23,0.4893617021276595,1,experiments,Evaluation: Since adding,entity_linking10
1143,178,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .",Evaluation,Since adding,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",76,0.6972477064220184,177,0.8271028037383178,37,0.7872340425531915,1,experiments,Evaluation: Since adding,entity_linking10
1144,185,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .",Evaluation,POS tagging,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",83,0.7614678899082569,184,0.8598130841121495,44,0.9361702127659576,1,experiments,Evaluation: POS tagging,entity_linking10
1145,195,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .",Evaluation,Sample efficiency,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-p', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'I-n', 'O']",93,0.8532110091743119,194,0.9065420560747663,6,0.6,1,results,Evaluation: Sample efficiency,entity_linking10
1146,199,"The output layer weights are relatively balanced , with a slight preference for the lower layers .",Evaluation,Sample efficiency,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",97,0.8899082568807339,198,0.9252336448598132,10,1.0,1,ablation-analysis,Evaluation: Sample efficiency,entity_linking10
1147,205,"Replacing the Glo Ve vectors with the biLM character layer gives a slight improvement for all tasks ( e.g. from 80.8 to 81.4 F 1 for SQuAD ) , but overall the improvements are small compared to the full ELMo model .",Evaluation,Contextual vs. sub - word information,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",103,0.944954128440367,204,0.9532710280373832,5,0.8333333333333334,1,results,Evaluation: Contextual vs. sub - word information,entity_linking10
1148,206,"From this , we conclude that most of the gains in the downstream tasks are due to the contextual information and not the sub-word information .",Evaluation,Contextual vs. sub - word information,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",104,0.9541284403669724,205,0.9579439252336448,6,1.0,1,results,Evaluation: Contextual vs. sub - word information,entity_linking10
1149,211,"As shown in the two right hand columns of , adding Glo Ve to models with ELMo generally provides a marginal improvement over ELMo only models ( e.g. 0.2 % F 1 improvement for SRL from 84.5 to 84.7 ) .",Evaluation,Are pre-trained vectors necessary with,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",109,1.0,210,0.9813084112149532,4,1.0,1,tasks,Evaluation: Are pre-trained vectors necessary with,entity_linking10
1150,2,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,title,title,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.005181347150259,1,0.0,1,research-problem,title,entity_linking11
1151,4,"In this article , we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation , by exploiting the semantic relationships between senses such as synonymy , hypernymy and hyponymy , in order to compress the sense vocabulary of Princeton WordNet , and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database .",abstract,abstract,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",1,0.3333333333333333,3,0.0155440414507772,1,0.3333333333333333,1,research-problem,abstract,entity_linking11
1152,6,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .",abstract,abstract,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",3,1.0,5,0.0259067357512953,3,1.0,1,research-problem,abstract,entity_linking11
1153,8,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",Introduction,Introduction,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.05,7,0.0362694300518134,1,0.05,1,research-problem,Introduction,entity_linking11
1154,24,"Therefore , we propose two different methods for building this subset and we call them sense vocabulary compression methods .",Introduction,Introduction,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.85,23,0.1191709844559585,17,0.85,1,model,Introduction,entity_linking11
1155,65,"Current state of the art supervised WSD systems such as , , and are all confronted to the following issues :",Methods,Compression,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1594202898550724,64,0.3316062176165803,27,0.6136363636363636,1,research-problem,Methods: Compression,entity_linking11
1156,71,"In order to overcome all these issues , we propose a method for grouping together multiple sense tags that refer in fact to the same concept .",Methods,Compression,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",17,0.2463768115942029,70,0.3626943005181347,33,0.75,1,model,Methods: Compression,entity_linking11
1157,73,From Senses to Synsets : A Vocabulary Compression Based on Synonymy,Methods,Compression,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",19,0.2753623188405797,72,0.3730569948186528,35,0.7954545454545454,1,research-problem,Methods: Compression,entity_linking11
1158,132,"For BERT , we used the model named "" bert - largecased "" of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .",Implementation details,Implementation details,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",1,0.25,131,0.6787564766839378,1,0.25,1,experimental-setup,Implementation details,entity_linking11
1159,134,"For the Transformer encoder layers , we used the same parameters as the "" base "" model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .",Implementation details,Implementation details,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O']",3,0.75,133,0.689119170984456,3,0.75,1,experimental-setup,Implementation details,entity_linking11
1160,140,We performed every training for 20 epochs .,Training,Training,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.2666666666666666,139,0.7202072538860104,4,0.2666666666666666,1,experiments,Training,entity_linking11
1161,147,"3 . A "" all relations "" system which applies our second vocabulary compression through all relations on the training corpus .",Training,Training,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",11,0.7333333333333333,146,0.7564766839378239,11,0.7333333333333333,1,baselines,Training,entity_linking11
1162,148,"We trained with mini-batches of 100 sentences , truncated to 80 words , and we used Adam with a learning rate of 0.0001 as the optimization method .",Training,Training,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.8,147,0.7616580310880829,12,0.8,1,experiments,Training,entity_linking11
1163,149,All models have been trained on one Nvidia 's Titan X GPU .,Training,Training,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.8666666666666667,148,0.7668393782383419,13,0.8666666666666667,1,experimental-setup,Training,entity_linking11
1164,164,"In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .",Evaluation,Evaluation,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.7058823529411765,163,0.844559585492228,12,0.7058823529411765,1,results,Evaluation,entity_linking11
1165,169,"In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .",Evaluation,Evaluation,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",17,1.0,168,0.8704663212435233,17,1.0,1,results,Evaluation,entity_linking11
1166,180,"As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.625,179,0.927461139896373,10,0.625,1,ablation-analysis,Ablation Study,entity_linking11
1167,181,"Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",11,0.6875,180,0.932642487046632,11,0.6875,1,ablation-analysis,Ablation Study,entity_linking11
1168,185,"Finally , through the scores obtained by invidual models ( without ensemble ) , we can observe on the standard deviations that the vocabulary compression method through hypernyms never impact significantly the final score .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'O']",15,0.9375,184,0.9533678756476685,15,0.9375,1,ablation-analysis,Ablation Study,entity_linking11
1169,186,"However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",16,1.0,185,0.9585492227979274,16,1.0,1,ablation-analysis,Ablation Study,entity_linking11
1170,2,Incorporating Glosses into Neural Word Sense Disambiguation,title,title,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0037593984962406,0,0.0,1,research-problem,title,entity_linking12
1171,4,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,abstract,abstract,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.0112781954887218,1,0.125,1,research-problem,abstract,entity_linking12
1172,9,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .",abstract,abstract,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.75,8,0.0300751879699248,6,0.75,1,research-problem,abstract,entity_linking12
1173,13,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,Introduction,Introduction,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037037037,12,0.0451127819548872,1,0.037037037037037,1,research-problem,Introduction,entity_linking12
1174,30,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",18,0.6666666666666666,29,0.1090225563909774,18,0.6666666666666666,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
1175,31,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.7037037037037037,30,0.112781954887218,19,0.7037037037037037,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
1176,32,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.7407407407407407,31,0.1165413533834586,20,0.7407407407407407,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
1177,36,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .",Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",24,0.8888888888888888,35,0.131578947368421,24,0.8888888888888888,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
1178,38,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",26,0.9629629629629628,37,0.1390977443609022,26,0.9629629629629628,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
1179,195,"We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",2,0.2222222222222222,194,0.7293233082706767,2,0.2222222222222222,1,hyperparameters,Implementation Details,entity_linking12
1180,196,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3333333333333333,195,0.7330827067669173,3,0.3333333333333333,1,hyperparameters,Implementation Details,entity_linking12
1181,197,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",4,0.4444444444444444,196,0.7368421052631579,4,0.4444444444444444,1,hyperparameters,Implementation Details,entity_linking12
1182,198,We assign gloss expansion depth K the value of 4 .,Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']",5,0.5555555555555556,197,0.7406015037593985,5,0.5555555555555556,1,hyperparameters,Implementation Details,entity_linking12
1183,199,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']",6,0.6666666666666666,198,0.7443609022556391,6,0.6666666666666666,1,experiments,Implementation Details,entity_linking12
1184,200,We use Adam optimizer in the training process with 0.001 initial learning rate .,Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.7777777777777778,199,0.7481203007518797,7,0.7777777777777778,1,hyperparameters,Implementation Details,entity_linking12
1185,201,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",8,0.8888888888888888,200,0.7518796992481203,8,0.8888888888888888,1,hyperparameters,Implementation Details,entity_linking12
1186,212,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,Knowledge - based Systems,Knowledge - based Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,1.0,211,0.793233082706767,9,1.0,1,experiments,Knowledge - based Systems,entity_linking12
1187,216,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,Supervised Systems,Supervised Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,1.0,215,0.8082706766917294,3,0.3,1,baselines,Supervised Systems,entity_linking12
1188,219,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,Neural - based Systems,Neural - based Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.3333333333333333,218,0.8195488721804511,6,0.6,1,baselines,Neural - based Systems,entity_linking12
1189,225,English all - words results,Results and Discussion,,entity_linking,12,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0294117647058823,224,0.8421052631578947,1,0.0588235294117647,1,results,Results and Discussion,entity_linking12
1190,226,"In this section , we show the performance of our proposed model in the English all - words task .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0588235294117647,225,0.8458646616541353,2,0.1176470588235294,1,results,Results and Discussion: English all - words results,entity_linking12
1191,230,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.1764705882352941,229,0.8609022556390977,6,0.3529411764705882,1,results,Results and Discussion: English all - words results,entity_linking12
1192,231,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.2058823529411764,230,0.8646616541353384,7,0.4117647058823529,1,results,Results and Discussion: English all - words results,entity_linking12
1193,235,". fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.3235294117647059,234,0.8796992481203008,11,0.6470588235294118,1,results,Results and Discussion: English all - words results,entity_linking12
1194,237,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.3823529411764705,236,0.8872180451127819,13,0.7647058823529411,1,results,Results and Discussion: English all - words results,entity_linking12
1195,238,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O']",14,0.4117647058823529,237,0.8909774436090225,14,0.8235294117647058,1,results,Results and Discussion: English all - words results,entity_linking12
1196,239,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",15,0.4411764705882353,238,0.8947368421052632,15,0.8823529411764706,1,results,Results and Discussion: English all - words results,entity_linking12
1197,240,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",16,0.4705882352941176,239,0.8984962406015038,16,0.9411764705882352,1,results,Results and Discussion: English all - words results,entity_linking12
1198,252,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.8235294117647058,251,0.943609022556391,10,0.625,1,results,Results and Discussion: English all - words results,entity_linking12
1199,2,Word Sense Disambiguation using a Bidirectional LSTM,title,title,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0086206896551724,1,0.0,1,research-problem,title,entity_linking13
1200,4,"In this paper we present a clean , yet effective , model for word sense disambiguation .",abstract,abstract,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",1,0.1428571428571428,3,0.0258620689655172,1,0.1428571428571428,1,research-problem,abstract,entity_linking13
1201,23,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .",Introduction,Introduction,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",12,0.631578947368421,22,0.1896551724137931,12,0.631578947368421,1,model,Introduction,entity_linking13
1202,83,"The source code , implemented using TensorFlow , has been released as open source 1 .",Experimental settings,Experimental settings,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.1333333333333333,82,0.7068965517241379,2,1.0,1,experimental-setup,Experimental settings,entity_linking13
1203,87,The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,Experimental settings,Embeddings,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",6,0.4,86,0.7413793103448276,3,0.4285714285714285,1,experimental-setup,Experimental settings: Embeddings,entity_linking13
1204,88,"Words not included in this set are initialized from N ( 0 , 0.1 ) .",Experimental settings,Embeddings,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.4666666666666667,87,0.75,4,0.5714285714285714,1,experimental-setup,Experimental settings: Embeddings,entity_linking13
1205,101,htsa 3 by was the winner of the SE3 lexical sample task with a F 1 score of 72.9 .,Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",4,0.3076923076923077,100,0.8620689655172413,4,0.3333333333333333,1,results,Results,entity_linking13
1206,106,Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",9,0.6923076923076923,105,0.9051724137931034,9,0.75,1,results,Results,entity_linking13
1207,107,"Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .",Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O']",10,0.7692307692307693,106,0.913793103448276,10,0.8333333333333334,1,results,Results,entity_linking13
1208,108,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .",Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",11,0.8461538461538461,107,0.9224137931034484,11,0.9166666666666666,1,results,Results,entity_linking13
1209,2,Knowledge - based Word Sense Disambiguation using Topic Models,title,title,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0051282051282051,1,0.0,1,research-problem,title,entity_linking14
1210,6,"In this paper , we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context .",abstract,abstract,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'B-n', 'O']",3,0.4285714285714285,5,0.0256410256410256,3,0.4285714285714285,1,research-problem,abstract,entity_linking14
1211,12,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .,Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0555555555555555,11,0.0564102564102564,1,0.0555555555555555,1,research-problem,Introduction,entity_linking14
1212,14,"WSD , being AI - complete ( Navigli 2009 ) , is still an open problem after over two decades of research .",Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1666666666666666,13,0.0666666666666666,3,0.1666666666666666,1,research-problem,Introduction,entity_linking14
1213,22,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .",Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6111111111111112,21,0.1076923076923077,11,0.6111111111111112,1,model,Introduction,entity_linking14
1214,23,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .",Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.6666666666666666,22,0.1128205128205128,12,0.6666666666666666,1,model,Introduction,entity_linking14
1215,25,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",14,0.7777777777777778,24,0.123076923076923,14,0.7777777777777778,1,model,Introduction,entity_linking14
1216,26,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .",Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",15,0.8333333333333334,25,0.1282051282051282,15,0.8333333333333334,1,model,Introduction,entity_linking14
1217,169,"The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .",Experiments & Results,Experiments & Results,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3076923076923077,168,0.8615384615384616,8,0.6666666666666666,1,results,Experiments & Results,entity_linking14
1218,170,"We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .",Experiments & Results,Experiments & Results,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.3461538461538461,169,0.8666666666666667,9,0.75,1,results,Experiments & Results,entity_linking14
1219,172,The proposed system outperforms all previous knowledgebased systems overall parts of speech .,Experiments & Results,Experiments & Results,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.4230769230769231,171,0.8769230769230769,11,0.9166666666666666,1,results,Experiments & Results,entity_linking14
1220,2,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,title,title,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0038610038610038,1,0.0,1,research-problem,title,entity_linking15
1221,7,"Our approach outperforms the previous state - of - the - art system on this data , resulting in an average 8 % improvement of the final score .",abstract,abstract,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",4,0.8,6,0.0231660231660231,4,0.8,1,research-problem,abstract,entity_linking15
1222,10,Knowledge base question answering ( QA ) requires a precise modeling of the question semantics through the entities and relations available in the knowledge base ( KB ) in order to retrieve the correct answer .,Introduction,Introduction,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0126582278481012,9,0.0347490347490347,1,0.0909090909090909,1,research-problem,Introduction,entity_linking15
1223,27,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .",Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.2278481012658227,26,0.1003861003861003,5,0.0757575757575757,1,model,Introduction: PERFORMER,entity_linking15
1224,29,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .",Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",20,0.2531645569620253,28,0.1081081081081081,7,0.106060606060606,1,model,Introduction: PERFORMER,entity_linking15
1225,32,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .",Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",23,0.2911392405063291,31,0.1196911196911196,10,0.1515151515151515,1,model,Introduction: PERFORMER,entity_linking15
1226,47,"In the recent years , EL on Twitter data has emerged as a branch of entity linking research .",Introduction,Code and datasets,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.4810126582278481,46,0.1776061776061776,25,0.3787878787878788,1,research-problem,Introduction: Code and datasets,entity_linking15
1227,178,We compile two new datasets for entity linking on questions that we derive from publicly available question answering data : WebQSP and GraphQuestions .,Architecture comparison,Datasets,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']",2,0.0952380952380952,177,0.6833976833976834,1,0.05,1,results,Architecture comparison: Datasets,entity_linking15
1228,216,We also include a heuristics baseline that ranks candidate entities according to their frequency in Wikipedia .,Existing systems,Existing systems,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'I-n', 'B-p', 'B-n', 'O']",4,0.1818181818181818,215,0.8301158301158301,5,0.4545454545454545,1,baselines,Existing systems,entity_linking15
1229,238,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,Results,Results,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",3,0.1764705882352941,237,0.915057915057915,3,0.1764705882352941,1,results,Results,entity_linking15
1230,239,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,Results,Results,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.2352941176470588,238,0.918918918918919,4,0.2352941176470588,1,results,Results,entity_linking15
1231,2,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,title,title,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0063694267515923,1,0.0,1,research-problem,title,entity_linking16
1232,11,"Word Sense Disambiguation ( WSD ) is an important problem in Natural Language Processing ( NLP ) , both in its own right and as a steppingstone to other advanced tasks in the NLP pipeline , applications such as machine translation and question answering .",Introduction,Introduction,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0769230769230769,10,0.0636942675159235,1,0.0769230769230769,1,research-problem,Introduction,entity_linking16
1233,16,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .",Introduction,Introduction,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.4615384615384615,15,0.0955414012738853,6,0.4615384615384615,1,approach,Introduction,entity_linking16
1234,17,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .",Introduction,Introduction,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",7,0.5384615384615384,16,0.1019108280254777,7,0.5384615384615384,1,approach,Introduction,entity_linking16
1235,106,This results in a vocabulary size of | V | = 29044 .,Experimental Settings,Experimental Settings,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.6,105,0.6687898089171974,3,0.6,1,hyperparameters,Experimental Settings,entity_linking16
1236,114,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .",Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.125,113,0.7197452229299363,5,0.1724137931034483,1,results,Results,entity_linking16
1237,119,IMS + adapted CW is another WSD model that considers deep neural networks and also uses pre-trained word embeddings as inputs .,Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",10,0.25,118,0.7515923566878981,10,0.3448275862068966,1,baselines,Results,entity_linking16
1238,122,htsa 3 was the winner of the SensEval - 3 lexical sample .,Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.325,121,0.7707006369426752,13,0.4482758620689655,1,results,Results,entity_linking16
1239,124,"IRST - Kernels utilizes kernel methods for pattern abstraction , paradigmatic and syntagmatic information and unsupervised term proximity on British National Corpus ( BNC ) , in SVM classifiers .",Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",15,0.375,123,0.7834394904458599,15,0.5172413793103449,1,baselines,Results,entity_linking16
1240,2,Neural Sequence Learning Models for Word Sense Disambiguation,title,,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0049261083743842,1,0.0,1,research-problem,title,entity_linking2
1241,9,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .",Introduction,Introduction,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0666666666666666,8,0.0394088669950738,1,0.0666666666666666,1,research-problem,Introduction,entity_linking2
1242,18,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .",Introduction,Introduction,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O']",10,0.6666666666666666,17,0.083743842364532,10,0.6666666666666666,1,model,Introduction,entity_linking2
1243,20,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .",Introduction,Introduction,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.8,19,0.0935960591133004,12,0.8,1,model,Introduction,entity_linking2
1244,70,LSTM Tagger,Training .,,entity_linking,2,"['O', 'O']","['B-n', 'I-n']",7,0.1590909090909091,69,0.3399014778325123,31,0.4078947368421052,1,hyperparameters,Training .,entity_linking2
1245,141,"6 Finally , we carried out an experiment on multilingual WSD using the Italian , German , French and Spanish data of SE13 .",Experimental Setup,Experimental Setup,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",7,0.875,140,0.6896551724137931,7,0.3888888888888889,1,results,Experimental Setup,entity_linking2
1246,151,"All models were trained fora fixed number of epochs E = 40 using Adadelta ( Zeiler , 2012 ) with learning rate 1.0 and batch size 32 .",Training .,Training .,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'O']",4,0.8,150,0.7389162561576355,17,0.9444444444444444,1,hyperparameters,Training .,entity_linking2
1247,161,"11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.1951219512195122,160,0.7881773399014779,8,0.2962962962962963,1,results,Experimental Results,entity_linking2
1248,162,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O']",9,0.2195121951219512,161,0.7931034482758621,9,0.3333333333333333,1,results,Experimental Results,entity_linking2
1249,163,"Furthermore , introducing LEX ( cf. Section 4 ) as auxiliary task was generally helpful ; on the other hand , POS did not seem to help , corroborating previous findings ( Alonso .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2439024390243902,162,0.7980295566502463,10,0.3703703703703703,1,results,Experimental Results,entity_linking2
1250,164,English All - words WSD,Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",11,0.2682926829268293,163,0.8029556650246306,11,0.4074074074074074,1,results,Experimental Results,entity_linking2
1251,169,"It is worth noting that RNN - based architectures outperformed classical supervised approaches when dealing with verbs , which are shown to be highly ambiguous .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",16,0.3902439024390244,168,0.8275862068965517,16,0.5925925925925926,1,results,Experimental Results,entity_linking2
1252,172,Multilingual All - words WSD,Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",19,0.4634146341463415,171,0.8423645320197044,19,0.7037037037037037,1,results,Experimental Results,entity_linking2
1253,179,"F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",26,0.6341463414634146,178,0.8768472906403941,26,0.9629629629629628,1,results,Experimental Results,entity_linking2
1254,180,"We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6585365853658537,179,0.8817733990147784,27,1.0,1,results,Experimental Results,entity_linking2
1255,2,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings,title,title,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0045454545454545,1,0.0,1,research-problem,title,entity_linking3
1256,13,Word Sense Disambiguation ( WSD ) is the task to identify the correct sense of the usage of a word from a ( usually ) fixed inventory of sense identifiers .,abstract,abstract,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1923076923076923,12,0.0545454545454545,10,0.1923076923076923,1,research-problem,abstract,entity_linking3
1257,137,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,Experimental Results,Contextualized Embeddings,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1454545454545454,136,0.6181818181818182,3,0.1875,1,results,Experimental Results: Contextualized Embeddings,entity_linking3
1258,143,We are using Version 2.1 : https://github.com/getalp/,Experimental Results,,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n']",14,0.2545454545454545,142,0.6454545454545455,9,0.5625,1,code,Experimental Results,entity_linking3
1259,144,UFSAC 4 BERT performed best in experiment one .,Experimental Results,,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']",15,0.2727272727272727,143,0.65,10,0.625,1,results,Experimental Results,entity_linking3
1260,187,shows that including the POS restriction increases the F 1 scores for S7 - T7 and S7 - T17 .,Post - evaluation experiment,Post - evaluation experiment,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.2857142857142857,186,0.8454545454545455,36,0.8780487804878049,1,results,Post - evaluation experiment,entity_linking3
1261,2,Learning Distributed Representations of Texts and Entities from Knowledge Base,title,title,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0043668122270742,1,0.0,1,research-problem,title,entity_linking4
1262,22,Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base ( KB ) such as Wikipedia and Freebase .,Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0857142857142857,21,0.0917030567685589,3,0.0857142857142857,1,code,Introduction,entity_linking4
1263,26,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",7,0.2,25,0.1091703056768559,7,0.2,1,model,Introduction,entity_linking4
1264,27,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",8,0.2285714285714285,26,0.1135371179039301,8,0.2285714285714285,1,model,Introduction,entity_linking4
1265,31,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'I-n', 'O']",12,0.3428571428571428,30,0.131004366812227,12,0.3428571428571428,1,research-problem,Introduction,entity_linking4
1266,40,"In both tasks , we adopt a simple multi -layer perceptron ( MLP ) classifier with the learned representations as features .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",21,0.6,39,0.1703056768558952,21,0.6,1,model,Introduction,entity_linking4
1267,44,"Our work differs from these works because we aim to map texts ( i.e. , sentences and paragraphs ) and entities into the same vector space .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7142857142857143,43,0.1877729257641921,25,0.7142857142857143,1,model,Introduction,entity_linking4
1268,47,We train the model using a large amount of entity annotations extracted directly from Wikipedia .,Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",28,0.8,46,0.2008733624454148,28,0.8,1,model,Introduction,entity_linking4
1269,126,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.3636363636363636,125,0.5458515283842795,4,0.3636363636363636,1,baselines,Baselines,entity_linking4
1270,127,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",5,0.4545454545454545,126,0.5502183406113537,5,0.4545454545454545,1,baselines,Baselines,entity_linking4
1271,128,The method also uses the LR classifier with the derived representations as features .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.5454545454545454,127,0.5545851528384279,6,0.5454545454545454,1,baselines,Baselines,entity_linking4
1272,129,FTS - BRNN is based on the bidirectional recurrent neural network ( RNN ) with gated recurrent units ( GRU ) .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.6363636363636364,128,0.5589519650655022,7,0.6363636363636364,1,baselines,Baselines,entity_linking4
1273,135,The experimental results show that our NTEE model achieved the best performance compared to the other proposed models and all the baseline methods on both the history and the literature datasets . :,Results,Results,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.0769230769230769,134,0.5851528384279476,1,0.1428571428571428,1,results,Results,entity_linking4
1274,137,"In particular , despite the simplicity of the neural network architecture of our method compared to the state - of - the - art methods ( i.e. , QANTA and FTS - BRNN ) , our method clearly outperformed these methods .",Results,Results,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'O']",3,0.2307692307692307,136,0.5938864628820961,3,0.4285714285714285,1,results,Results,entity_linking4
1275,2,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,title,title,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0051020408163265,1,0.0,1,research-problem,title,entity_linking5
1276,10,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",Introduction,Introduction,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0416666666666666,9,0.0459183673469387,1,0.0454545454545454,1,research-problem,Introduction,entity_linking5
1277,11,"Various approaches have been proposed to achieve WSD , and they are generally ordered by the type and the quantity of resources they use :",Introduction,Introduction,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0833333333333333,10,0.0510204081632653,2,0.0909090909090909,1,research-problem,Introduction,entity_linking5
1278,19,"Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing , but in this work , the idea is to solve this issue by taking advantage of one of the multiple semantic relationships between senses included in WordNet : the hypernymy and hyponymy relationships .",Introduction,Introduction,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.4166666666666667,18,0.0918367346938775,10,0.4545454545454545,1,model,Introduction,entity_linking5
1279,31,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,Introduction,Introduction,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",22,0.9166666666666666,30,0.1530612244897959,22,1.0,1,code,Introduction,entity_linking5
1280,34,Language Model Based WSD,,,entity_linking,5,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",0,0.0,33,0.1683673469387755,0,0.0,1,experiments,,entity_linking5
1281,149,We performed every training for 20 epochs .,Training,Training,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",6,0.3,148,0.7551020408163265,6,0.4615384615384615,1,experiments,Training,entity_linking5
1282,154,"We trained with mini-batches of 100 sentences , truncated to 80 words , and padded with zero vectors from the end , and we used Adam ( Kingma and Ba , 2014 ) , with the same default parameters described in their article as the optimization method , except for the learning rate that we set to 0.0001 ( 10 times smaller than the default value ) .",Training,Training,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,153,0.7806122448979592,11,0.8461538461538461,1,experiments,Training,entity_linking5
1283,155,All models have been trained on Nvidia 's Titan X GPUs .,Training,Training,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.6,154,0.7857142857142857,12,0.9230769230769232,1,experimental-setup,Training,entity_linking5
1284,185,"Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",8,0.4210526315789473,184,0.9387755102040816,8,0.4210526315789473,1,results,Results,entity_linking5
1285,189,"In comparison with the other works , our systems trained on the SemCor alone expose results comparable with the best system of , which is trained on the same corpus and augmented with a semi-supervised method .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",12,0.631578947368421,188,0.9591836734693876,12,0.631578947368421,1,results,Results,entity_linking5
1286,190,"When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O']",13,0.6842105263157895,189,0.9642857142857144,13,0.6842105263157895,1,results,Results,entity_linking5
1287,191,"Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'I-n', 'O', 'I-n', 'O', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'O']",14,0.7368421052631579,190,0.9693877551020408,14,0.7368421052631579,1,results,Results,entity_linking5
1288,193,"As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O']",16,0.8421052631578947,192,0.979591836734694,16,0.8421052631578947,1,results,Results,entity_linking5
1289,194,"Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",17,0.8947368421052632,193,0.9846938775510204,17,0.8947368421052632,1,results,Results,entity_linking5
1290,2,Incorporating Glosses into Neural Word Sense Disambiguation,title,title,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0037174721189591,0,0.0,1,research-problem,title,entity_linking6
1291,4,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,abstract,abstract,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.0111524163568773,1,0.125,1,research-problem,abstract,entity_linking6
1292,9,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .",abstract,abstract,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.75,8,0.0297397769516728,6,0.75,1,research-problem,abstract,entity_linking6
1293,13,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,Introduction,Introduction,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0357142857142857,12,0.0446096654275092,1,0.0357142857142857,1,research-problem,Introduction,entity_linking6
1294,31,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",19,0.6785714285714286,30,0.1115241635687732,19,0.6785714285714286,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1295,32,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",20,0.7142857142857143,31,0.1152416356877323,20,0.7142857142857143,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1296,33,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.75,32,0.1189591078066914,21,0.75,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1297,37,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .",Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",25,0.8928571428571429,36,0.1338289962825278,25,0.8928571428571429,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1298,39,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",27,0.9642857142857144,38,0.1412639405204461,27,0.9642857142857144,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1299,196,"We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",2,0.2222222222222222,195,0.724907063197026,2,0.2222222222222222,1,hyperparameters,Implementation Details,entity_linking6
1300,197,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3333333333333333,196,0.7286245353159851,3,0.3333333333333333,1,hyperparameters,Implementation Details,entity_linking6
1301,198,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",4,0.4444444444444444,197,0.7323420074349443,4,0.4444444444444444,1,hyperparameters,Implementation Details,entity_linking6
1302,199,We assign gloss expansion depth K the value of 4 .,Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']",5,0.5555555555555556,198,0.7360594795539034,5,0.5555555555555556,1,hyperparameters,Implementation Details,entity_linking6
1303,200,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']",6,0.6666666666666666,199,0.7397769516728625,6,0.6666666666666666,1,experiments,Implementation Details,entity_linking6
1304,201,We use Adam optimizer in the training process with 0.001 initial learning rate .,Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.7777777777777778,200,0.7434944237918215,7,0.7777777777777778,1,hyperparameters,Implementation Details,entity_linking6
1305,202,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",8,0.8888888888888888,201,0.7472118959107806,8,0.8888888888888888,1,hyperparameters,Implementation Details,entity_linking6
1306,214,This work shows that glosses are important to WSD and enriching gloss information via its semantic relations can help to WSD .,Knowledge - based Systems,Basile et al.,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",4,0.8,213,0.79182156133829,10,0.9090909090909092,1,experiments,Knowledge - based Systems: Basile et al.,entity_linking6
1307,215,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,Knowledge - based Systems,Basile et al.,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,1.0,214,0.7955390334572491,11,1.0,1,experiments,Knowledge - based Systems: Basile et al.,entity_linking6
1308,219,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,Supervised Systems,Supervised Systems,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,1.0,218,0.8104089219330854,3,0.3,1,baselines,Supervised Systems,entity_linking6
1309,222,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,Neural - based Systems,Neural - based Systems,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.3333333333333333,221,0.8215613382899628,6,0.6,1,baselines,Neural - based Systems,entity_linking6
1310,228,English all - words results,Results and Discussion,,entity_linking,6,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0294117647058823,227,0.8438661710037175,1,0.0555555555555555,1,results,Results and Discussion,entity_linking6
1311,229,"In this section , we show the performance of our proposed model in the English all - words task .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0588235294117647,228,0.8475836431226765,2,0.1111111111111111,1,results,Results and Discussion: English all - words results,entity_linking6
1312,233,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.1764705882352941,232,0.862453531598513,6,0.3333333333333333,1,results,Results and Discussion: English all - words results,entity_linking6
1313,237,". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",10,0.2941176470588235,236,0.8773234200743495,10,0.5555555555555556,1,results,Results and Discussion: English all - words results,entity_linking6
1314,238,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.3235294117647059,237,0.8810408921933085,11,0.6111111111111112,1,results,Results and Discussion: English all - words results,entity_linking6
1315,240,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.3823529411764705,239,0.8884758364312267,13,0.7222222222222222,1,results,Results and Discussion: English all - words results,entity_linking6
1316,241,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O']",14,0.4117647058823529,240,0.8921933085501859,14,0.7777777777777778,1,results,Results and Discussion: English all - words results,entity_linking6
1317,242,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",15,0.4411764705882353,241,0.895910780669145,15,0.8333333333333334,1,results,Results and Discussion: English all - words results,entity_linking6
1318,243,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",16,0.4705882352941176,242,0.8996282527881041,16,0.8888888888888888,1,results,Results and Discussion: English all - words results,entity_linking6
1319,2,Semi-supervised Word Sense Disambiguation with Neural Models,title,title,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0044642857142857,1,0.0,1,research-problem,title,entity_linking7
1320,4,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,abstract,abstract,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.0133928571428571,1,0.125,1,research-problem,abstract,entity_linking7
1321,13,Word sense disambiguation ( WSD ) is a long - standing problem in natural language processing ( NLP ) with broad applications .,Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0666666666666666,12,0.0535714285714285,1,0.0666666666666666,1,research-problem,Introduction,entity_linking7
1322,20,"In this paper , we describe two novel WSD algorithms .",Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.5333333333333333,19,0.0848214285714285,8,0.5333333333333333,1,model,Introduction,entity_linking7
1323,21,The first is based on a Long Short Term Memory ( LSTM ) ) .,Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.6,20,0.0892857142857142,9,0.6,1,model,Introduction,entity_linking7
1324,23,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.7333333333333333,22,0.0982142857142857,11,0.7333333333333333,1,model,Introduction,entity_linking7
1325,130,Sem Eval Tasks,Experiments,Experiments,entity_linking,7,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",4,0.0677966101694915,129,0.5758928571428571,4,0.1379310344827586,1,experiments,Experiments,entity_linking7
1326,136,Our proposed algorithms achieve the highest all - words F 1 scores except for Sem - Eval 2013 .,Experiments,Experiments,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.1694915254237288,135,0.6026785714285714,10,0.3448275862068966,1,results,Experiments,entity_linking7
1327,141,"Our self - trained word embeddings have similar performance to the pre-trained embeddings , as shown in .",Experiments,Settings,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",15,0.2542372881355932,140,0.625,15,0.5172413793103449,1,results,Experiments: Settings,entity_linking7
1328,145,The learning rate is 0.1 .,Experiments,,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",19,0.3220338983050847,144,0.6428571428571429,19,0.6551724137931034,1,hyperparameters,Experiments,entity_linking7
1329,146,"We experimented with other learning rates , and observed no significant performance difference after the training converges .",Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",20,0.3389830508474576,145,0.6473214285714286,20,0.6896551724137931,1,experiments,Experiments: The learning rate is 0.1 .,entity_linking7
1330,148,Word2 Vec vectors Vs. LSTM,Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",22,0.3728813559322034,147,0.65625,22,0.7586206896551724,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1331,151,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",25,0.423728813559322,150,0.6696428571428571,25,0.8620689655172413,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1332,152,Sem Cor Vs. OMSTI,Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",26,0.4406779661016949,151,0.6741071428571429,26,0.896551724137931,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1333,153,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .",Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'O']",27,0.4576271186440678,152,0.6785714285714286,27,0.9310344827586208,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1334,159,The algorithm performs similarly on the different data sets .,Experiments,We use the implementation of DIST EXPANDER .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",33,0.559322033898305,158,0.7053571428571429,3,1.0,1,tasks,Experiments: We use the implementation of DIST EXPANDER .,entity_linking7
1335,182,"Word 2 Vec : a nearest - neighbor classifier with Word2 Vec word embedding , which has similar performance to cutting - edge algorithms studied in on SemEval tasks .",Experiments,LSTM classifier,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O']",56,0.9491525423728814,181,0.8080357142857143,4,0.2222222222222222,1,baselines,Experiments: LSTM classifier,entity_linking7
1336,189,"Across all part of speech tags and datasets , F1 scores increase after adding more training data .",Change of training data,Change of training data,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O']",3,0.1034482758620689,188,0.8392857142857143,11,0.6111111111111112,1,results,Change of training data,entity_linking7
1337,191,The SemCor ( or MASC ) trained classifier is on a par with the NOAD trained classifier on F1 score .,Change of training data,Change of training data,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.1724137931034483,190,0.8482142857142857,13,0.7222222222222222,1,results,Change of training data,entity_linking7
1338,2,LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS FROM TEXT,title,title,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.5,1,0.0049504950495049,1,0.5,1,research-problem,title,entity_linking8
1339,3,Work done as a Google AI Resident,title,,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",2,1.0,2,0.0099009900990099,2,1.0,1,research-problem,title,entity_linking8
1340,21,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .",INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.125,20,0.099009900990099,12,0.48,1,model,INTRODUCTION,entity_linking8
1341,22,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .",INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1354166666666666,21,0.1039603960396039,13,0.52,1,model,INTRODUCTION,entity_linking8
1342,24,RELIC accurately captures categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies .,INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.15625,23,0.1138613861386138,15,0.6,1,research-problem,INTRODUCTION,entity_linking8
1343,26,We also show that given just a few exemplar entities of a given category such as Scottish footballers we can use RELIC to recover the remaining entities of that category with good precision .,INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",17,0.1770833333333333,25,0.1237623762376237,17,0.68,1,model,INTRODUCTION,entity_linking8
1344,27,Using RELIC for entity linking can match state - of - the - art approaches that make use of non-local and non-linguistic information about entities .,INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'I-n', 'O', 'O', 'O']",18,0.1875,26,0.1287128712871287,18,0.72,1,research-problem,INTRODUCTION,entity_linking8
1345,30,"RELIC learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned , and not the surface form of the mention itself .",INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",21,0.21875,29,0.1435643564356435,21,0.84,1,experiments,INTRODUCTION,entity_linking8
1346,112,"We also apply RELIC 's context encoder and entity embeddings to the task of end - to - end trivia question answering , and we show that this approach can capture more than half of the answers identified by the best existing reading comprehension systems .",EVALUATION,EVALUATION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2727272727272727,111,0.5495049504950495,6,1.0,1,experiments,EVALUATION,entity_linking8
1347,141,"RELIC outperforms prior work , even with only 5 % of the training data .",System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.2033898305084746,140,0.693069306930693,4,0.3076923076923077,1,results,System: F1,entity_linking8
1348,196,We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,Results,Trivia,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",7,0.5384615384615384,195,0.9653465346534652,7,0.5384615384615384,1,results,Results: Trivia,entity_linking8
1349,198,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .",Results,Trivia,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6923076923076923,197,0.9752475247524752,9,0.6923076923076923,1,results,Results: Trivia,entity_linking8
1350,2,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,title,title,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.003690036900369,1,0.0,1,research-problem,title,entity_linking9
1351,4,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",abstract,abstract,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.011070110701107,1,0.1666666666666666,1,research-problem,abstract,entity_linking9
1352,9,"By combining contexts based on the proposed embedding with standard NED features , we achieved state - of - theart accuracy of 93.1 % on the standard CoNLL dataset and 85.2 % on the TAC 2010 dataset .",abstract,abstract,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,1.0,8,0.029520295202952,6,1.0,1,research-problem,abstract,entity_linking9
1353,11,"Named Entity Disambiguation ( NED ) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0227272727272727,10,0.03690036900369,1,0.04,1,research-problem,Introduction,entity_linking9
1354,20,The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low - dimensional vector space .,Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.2272727272727272,19,0.070110701107011,10,0.4,1,model,Introduction,entity_linking9
1355,21,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.25,20,0.07380073800738,11,0.44,1,model,Introduction,entity_linking9
1356,26,Our model consists of the following three models based on the skip - gram model :,Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.3636363636363636,25,0.092250922509225,16,0.64,1,model,Introduction,entity_linking9
1357,27,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'O', 'I-n', 'O', 'O', 'O', 'I-n', 'B-p', 'B-n', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O']",17,0.3863636363636363,26,0.0959409594095941,17,0.68,1,model,Introduction,entity_linking9
1358,28,"By jointly optimizing these models , our method simultaneously learns the embedding of words and entities .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",18,0.4090909090909091,27,0.0996309963099631,18,0.72,1,model,Introduction,entity_linking9
1359,29,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O']",19,0.4318181818181818,28,0.1033210332103321,19,0.76,1,model,Introduction,entity_linking9
1360,32,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",22,0.5,31,0.1143911439114391,22,0.88,1,model,Introduction,entity_linking9
1361,40,Skip- gram Model for Word Similarity,Introduction,,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n']",30,0.6818181818181818,39,0.1439114391143911,4,0.2222222222222222,1,research-problem,Introduction,entity_linking9
1362,99,We use stochastic gradient descent ( SGD ) for the optimization .,Training,Training,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",16,0.4848484848484848,98,0.3616236162361623,16,0.5714285714285714,1,hyperparameters,Training,entity_linking9
1363,164,"Following , we also used learning rate ? = 0.025 which linearly decreased with the iterations of the Wikipedia dump .",Experiments,Training for the Proposed Embedding,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",14,0.8235294117647058,163,0.6014760147601476,10,0.7692307692307693,1,experiments,Experiments: Training for the Proposed Embedding,entity_linking9
1364,225,"Surprisingly , we attained results comparable with those of some state - of - the - art methods on the both datasets by only using base features .",Results,Feature Study,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'I-p', 'B-n', 'I-n', 'O']",6,0.2608695652173913,224,0.8265682656826568,5,0.4166666666666667,1,results,Results: Feature Study,entity_linking9
1365,226,Adding string similarity features slightly further improved performance .,Results,Feature Study,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.3043478260869565,225,0.8302583025830258,6,0.5,1,results,Results: Feature Study,entity_linking9
1366,228,Our method outperformed some state - of - the - art methods without using coherence .,Results,Feature Study,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",9,0.391304347826087,227,0.8376383763837638,8,0.6666666666666666,1,results,Results: Feature Study,entity_linking9
1367,2,"3D Face Morphable Models "" In - the - Wild """,title,title,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0042194092827004,1,0.0,1,research-problem,title,face_alignment0
1368,18,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1052631578947368,17,0.0717299578059071,4,0.1052631578947368,1,research-problem,Introduction,face_alignment0
1369,21,The method requires the construction of a 3 DMM which is a statistical model of facial texture and shape in a space where there are explicit correspondences .,Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",7,0.1842105263157894,20,0.0843881856540084,7,0.1842105263157894,1,model,Introduction,face_alignment0
1370,28,"3 D facial shape recovery from a single image under "" inthe - wild "" conditions is still an open and challenging problem in computer vision mainly due to the fact that :",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3684210526315789,27,0.1139240506329113,14,0.3684210526315789,1,research-problem,Introduction,face_alignment0
1371,29,The general problem of extracting the 3D facial shape from a single image is an ill - posed problem which is notoriously difficult to be solved without the use of any statistical priors for the shape and texture of faces .,Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3947368421052631,28,0.1181434599156118,15,0.3947368421052631,1,research-problem,Introduction,face_alignment0
1372,33,"Learning statistical priors of the 3D facial shape and texture for "" in - the - wild "" images is currently very difficult by using modern acquisition devices .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.5,32,0.1350210970464135,19,0.5,1,research-problem,Introduction,face_alignment0
1373,41,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",27,0.7105263157894737,40,0.1687763713080168,27,0.7105263157894737,1,model,Introduction,face_alignment0
1374,42,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",28,0.7368421052631579,41,0.1729957805907173,28,0.7368421052631579,1,model,Introduction,face_alignment0
1375,43,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.7631578947368421,42,0.1772151898734177,29,0.7631578947368421,1,model,Introduction,face_alignment0
1376,44,"By capitalising on the recent advancements in fitting statistical deformable models , we propose a novel and fast algorithm for fitting "" in - the -wild "" 3 DMMs .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",30,0.7894736842105263,43,0.1814345991561181,30,0.7894736842105263,1,model,Introduction,face_alignment0
1377,151,Gauss - Newton Optimization,Model Fitting,,face_alignment,0,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",30,0.3614457831325301,150,0.6329113924050633,11,0.4230769230769231,1,hyperparameters,Model Fitting,face_alignment0
1378,206,"To train our model , which we label as ITW , we use a variant of the Basel Face Model ( BFM ) that we trained to contain both identities drawn from the original BFM model along with expressions provided by .",Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0384615384615384,205,0.8649789029535865,1,0.25,1,hyperparameters,Experiments,face_alignment0
1379,210,3D Shape Recovery,Experiments,Experiments,face_alignment,0,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",5,0.1923076923076923,209,0.8818565400843882,0,0.0,1,experiments,Experiments,face_alignment0
1380,211,"Herein , we evaluate our "" in - the -wild "" 3 DMM ( ITW ) in terms of 3D shape estimation accuracy against two popular state - of - the - art alternative 3 DMM formulations .",Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.2307692307692307,210,0.8860759493670886,1,0.0666666666666666,1,experiments,Experiments,face_alignment0
1381,212,The first one is a classic 3 DMM with the original Basel laboratory texture model and full lighting equation which we term Classic .,Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']",7,0.2692307692307692,211,0.890295358649789,2,0.1333333333333333,1,baselines,Experiments,face_alignment0
1382,213,The second is the texture - less linear model proposed in which we refer to as Linear .,Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']",8,0.3076923076923077,212,0.8945147679324894,3,0.2,1,baselines,Experiments,face_alignment0
1383,216,"The mean mesh of each model under testis landmarked with the same 49 point markup used in the dataset , and is registered against the ground truth mesh by performing a Procrustes alignment using the sparse annotations followed by Non-Rigid Iterative Closest Point ( N - ICP ) to iteratively deform the two surfaces until they are brought into correspondence .",Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",11,0.4230769230769231,215,0.9071729957805909,6,0.4,1,hyperparameters,Experiments,face_alignment0
1384,223,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .",Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.6923076923076923,222,0.9367088607594936,13,0.8666666666666667,1,results,Experiments,face_alignment0
1385,231,ITW slightly outperforms IMM even though both IMM and PS - NL use all four available images of each subject .,Experiments,Quantitative Normal Recovery,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,1.0,230,0.9704641350210972,5,1.0,1,experiments,Experiments: Quantitative Normal Recovery,face_alignment0
1386,2,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,title,title,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0045662100456621,1,0.0,1,research-problem,title,face_alignment1
1387,9,"Our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry ( including the non-visible parts of the face ) bypassing the construction ( during training ) and fitting ( during testing ) of a 3D Morphable Model .",abstract,abstract,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.6666666666666666,8,0.0365296803652968,6,0.6666666666666666,1,research-problem,abstract,face_alignment1
1388,14,3 D face reconstruction is the problem of recovering the 3D facial geometry from 2D images .,Introduction,Introduction,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0333333333333333,13,0.0593607305936073,1,0.0555555555555555,1,research-problem,Introduction,face_alignment1
1389,17,This work is on 3D face reconstruction using only a single image .,Introduction,Introduction,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'I-n', 'I-n', 'O']",4,0.1333333333333333,16,0.0730593607305936,4,0.2222222222222222,1,research-problem,Introduction,face_alignment1
1390,19,"In this paper , we propose to approach it , for the first time to the best of our knowledge , by directly learning a mapping from pixels to 3D coordinates using a Convolutional Neural Network ( CNN ) .",Introduction,Introduction,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.2,18,0.0821917808219178,6,0.3333333333333333,1,model,Introduction,face_alignment1
1391,20,"Besides its simplicity , our approach works with totally unconstrained images downloaded from the web , including facial images of arbitrary poses , facial expressions and occlusions , as shown in .",Introduction,Introduction,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']",7,0.2333333333333333,19,0.0867579908675799,7,0.3888888888888889,1,model,Introduction,face_alignment1
1392,147,"Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .",Training,Training,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.2,146,0.6666666666666666,1,0.2,1,hyperparameters,Training,face_alignment1
1393,148,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .",Training,Training,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,147,0.6712328767123288,2,0.4,1,hyperparameters,Training,face_alignment1
1394,160,"3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1818181818181818,159,0.726027397260274,1,0.0344827586206896,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1395,161,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.2045454545454545,160,0.730593607305936,2,0.0689655172413793,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1396,162,"Also , there are no significant performance discrepancies across different datasets ( ALFW2000 - 3D seems to be slightly more difficult ) .",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2272727272727272,161,0.7351598173515982,3,0.1034482758620689,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1397,164,VRN - Guided uses another stacked hourglass network for landmark localization .,Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",12,0.2727272727272727,163,0.7442922374429224,5,0.1724137931034483,1,baselines,Results: Volumetric Regression Networks largely outperform,face_alignment1
1398,165,"4 . VRN - Multitask does not always perform particularly better than the plain VRN ( in fact on BU - 4 DFE it performs worse ) , not justifying the increase of network complexity .",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2954545454545454,164,0.7488584474885844,6,0.2068965517241379,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1399,199,"For all experiments reported , we used the best performing VRN - Guided .",Ablation studies,Ablation studies,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O']",2,0.1176470588235294,198,0.9041095890410958,2,0.1176470588235294,1,ablation-analysis,Ablation studies,face_alignment1
1400,211,"The performance of the 3D reconstruction dropped by a negligible amount , suggesting that as long as the Gaussians are of a sensible size , guidance will always help .",Ablation studies,Effect of Gaussian size for guidance .,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.8235294117647058,210,0.958904109589041,14,0.8235294117647058,1,ablation-analysis,Ablation studies: Effect of Gaussian size for guidance .,face_alignment1
1401,2,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,title,title,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0034246575342465,1,0.0,1,research-problem,title,face_alignment10
1402,14,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .",Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0333333333333333,13,0.0445205479452054,1,0.0333333333333333,1,research-problem,Introduction,face_alignment10
1403,17,"The existing challenge is to achieve robust and accurate landmark localisation of unconstrained faces that are impacted by a variety of appearance variations , e.g. in pose , expression , illumination , image blurring and occlusion .",Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",4,0.1333333333333333,16,0.0547945205479452,4,0.1333333333333333,1,research-problem,Introduction,face_alignment10
1404,30,"To further address the issue , we propose a new loss function , namely Wing loss ( ) , for robust facial landmark localisation .",Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.5666666666666667,29,0.0993150684931506,17,0.5666666666666667,1,model,Introduction,face_alignment10
1405,34,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .",Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.7,33,0.1130136986301369,21,0.7,1,model,Introduction,face_alignment10
1406,209,"In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.0135135135135135,208,0.7123287671232876,1,0.04,1,experimental-setup,Implementation details,face_alignment10
1407,210,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.027027027027027,209,0.7157534246575342,2,0.08,1,experimental-setup,Implementation details,face_alignment10
1408,212,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.054054054054054,211,0.7226027397260274,4,0.16,1,experimental-setup,Implementation details,face_alignment10
1409,213,Each model was trained for 120 k iterations .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.0675675675675675,212,0.726027397260274,5,0.2,1,experimental-setup,Implementation details,face_alignment10
1410,215,"The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O']",7,0.0945945945945946,214,0.7328767123287672,7,0.28,1,experimental-setup,Implementation details,face_alignment10
1411,216,"For the convolutional layer , we used 3 3 kernels with the stride of 1 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",8,0.1081081081081081,215,0.7363013698630136,8,0.32,1,experimental-setup,Implementation details,face_alignment10
1412,218,"For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']",10,0.1351351351351351,217,0.7431506849315068,10,0.4,1,experimental-setup,Implementation details,face_alignment10
1413,219,"For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",11,0.1486486486486486,218,0.7465753424657534,11,0.44,1,experimental-setup,Implementation details,face_alignment10
1414,220,"The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",12,0.1621621621621621,219,0.75,12,0.48,1,experimental-setup,Implementation details,face_alignment10
1415,221,The parameters of the Wing loss were set tow = 15 and = 3 .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.1756756756756756,220,0.7534246575342466,13,0.52,1,experimental-setup,Implementation details,face_alignment10
1416,222,"To perform data augmentation , we randomly rotated each training image between [ ? 30 , 30 ] degrees for CNN - 6 and between [ ? 10 , 10 ] degrees for CNN - 7 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",14,0.1891891891891892,221,0.7568493150684932,14,0.56,1,experimental-setup,Implementation details,face_alignment10
1417,223,"In addition , we randomly flipped each training image with the probability of 50 % .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O']",15,0.2027027027027027,222,0.7602739726027398,15,0.6,1,experimental-setup,Implementation details,face_alignment10
1418,224,"For bounding box perturbation , we applied random translations to the upper-left and bottom - right corners of the face bounding box within 5 % of the bounding .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",16,0.2162162162162162,223,0.7636986301369864,16,0.64,1,experimental-setup,Implementation details,face_alignment10
1419,227,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']",19,0.2567567567567567,226,0.773972602739726,19,0.76,1,experimental-setup,Implementation details,face_alignment10
1420,234,Comparison with state of the art 7.2.1 AFLW,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",26,0.3513513513513513,233,0.797945205479452,0,0.0,1,results,Implementation details,face_alignment10
1421,240,"In our experiments , we used our two - stage facial landmark localisation framework by stacking the CNN - 6 and CNN - 7 networks ( denoted by CNN - 6 / 7 ) , as introduced in Section 6 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4324324324324324,239,0.8184931506849316,6,0.2068965517241379,1,experimental-setup,Implementation details,face_alignment10
1422,241,"In addition , the proposed Pose - based Data Balancing ( PDB ) strategy was adopted , as presented in Section 5 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4459459459459459,240,0.821917808219178,7,0.2413793103448276,1,experimental-setup,Implementation details,face_alignment10
1423,246,"As shown in , our CNN - 6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function ( magenta solid line ) .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",38,0.5135135135135135,245,0.839041095890411,12,0.4137931034482758,1,results,Implementation details,face_alignment10
1424,248,"Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",40,0.5405405405405406,247,0.8458904109589042,14,0.4827586206896552,1,experiments,Implementation details,face_alignment10
1425,249,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",41,0.5540540540540541,248,0.8493150684931506,15,0.5172413793103449,1,results,Implementation details,face_alignment10
1426,253,The face images involved in 300W have been semi-automatically annotated by 68 facial landmarks .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",45,0.6081081081081081,252,0.863013698630137,19,0.6551724137931034,1,experimental-setup,Implementation details,face_alignment10
1427,257,The final size of the test set is 689 .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",49,0.6621621621621622,256,0.8767123287671232,23,0.7931034482758621,1,experimental-setup,Implementation details,face_alignment10
1428,262,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",54,0.7297297297297297,261,0.8938356164383562,28,0.9655172413793104,1,results,Implementation details,face_alignment10
1429,263,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",55,0.7432432432432432,262,0.8972602739726028,29,1.0,1,results,Implementation details,face_alignment10
1430,264,Run time and network architectures,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",56,0.7567567567567568,263,0.9006849315068494,0,0.0,1,experiments,Implementation details,face_alignment10
1431,265,"Facial landmark localisation has been widely used in many real - time practical applications , hence the speed together with accuracy of an algorithm is crucial for the deployment of the algorithm in commercial use cases .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.7702702702702703,264,0.9041095890410958,1,0.0555555555555555,1,experiments,Implementation details,face_alignment10
1432,269,The input for ResNet is a 224 224 3 colour image .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",61,0.8243243243243243,268,0.9178082191780822,5,0.2777777777777778,1,experimental-setup,Implementation details,face_alignment10
1433,271,"For both AFLW and 300 W , by replacing the CNN - 6/7 network with ResNet - 50 , the performance has been further improved by around 10 % , as shown in .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",63,0.8513513513513513,270,0.9246575342465754,7,0.3888888888888889,1,results,Implementation details,face_alignment10
1434,280,The speed of TR - DRN is 83 fps on an NVIDIA GeForce GTX Titan X card .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",72,0.972972972972973,279,0.9554794520547946,16,0.8888888888888888,1,experiments,Implementation details,face_alignment10
1435,282,"It should be noted that our CNN - 6 / 7 still outperforms the state - of - the - art approaches by a significant margin while running at 170 fps on a GPU card , as shown in .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",74,1.0,281,0.9623287671232876,18,1.0,1,results,Implementation details,face_alignment10
1436,2,Unsupervised Training for 3D Morphable Model Regression,title,,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0037593984962406,1,0.0,1,research-problem,title,face_alignment11
1437,10,"Finding the coordinates of a person in this space from a single image of that person is a common task for applications such as 3D avatar creation , facial animation transfer , and video editing ( e.g. ) .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1,9,0.0338345864661654,2,0.1,1,research-problem,Introduction,face_alignment11
1438,18,We map features from a facial recognition network into identity parameters for the Basel 2017 Morphable Face Model .,Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.5,17,0.0639097744360902,10,0.5,1,model,Introduction,face_alignment11
1439,20,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",12,0.6,19,0.0714285714285714,12,0.6,1,model,Introduction,face_alignment11
1440,21,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",13,0.65,20,0.075187969924812,13,0.65,1,model,Introduction,face_alignment11
1441,23,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",15,0.75,22,0.0827067669172932,15,0.75,1,model,Introduction,face_alignment11
1442,26,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",18,0.9,25,0.093984962406015,18,0.9,1,model,Introduction,face_alignment11
1443,27,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.95,26,0.0977443609022556,19,0.95,1,model,Introduction,face_alignment11
1444,105,We use the Phong reflection model for shading .,Illumination Model,Illumination Model,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.0327868852459016,104,0.3909774436090225,2,0.2857142857142857,1,hyperparameters,Illumination Model,face_alignment11
1445,138,Identity prediction can be further enhanced by using multiple poses for each face .,Illumination Model,Identity Loss,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",35,0.5737704918032787,137,0.5150375939849624,7,0.3333333333333333,1,results,Illumination Model: Identity Loss,face_alignment11
1446,142,Batch Distribution,Illumination Model,,face_alignment,11,"['O', 'O']","['B-n', 'I-n']",39,0.639344262295082,141,0.5300751879699248,11,0.5238095238095238,1,hyperparameters,Illumination Model,face_alignment11
1447,171,"Our method shows improved likeness and color fidelity over competing methods , especially in the shape of the eyes , eyebrows , and nose .",Experiments,Experiments,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'I-n', 'O', 'I-n', 'I-n', 'O']",6,0.2,170,0.6390977443609023,6,0.75,1,results,Experiments,face_alignment11
1448,184,Neutral Pose Reconstruction on MICC,Experiments,,face_alignment,11,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",19,0.6333333333333333,183,0.6879699248120301,0,0.0,1,results,Experiments,face_alignment11
1449,185,We quantitatively evaluate the ground - truth accuracy of our models on the MICC Florence 3D Faces dataset ( MICC ) in .,Experiments,Neutral Pose Reconstruction on MICC,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-p', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",20,0.6666666666666666,184,0.6917293233082706,1,0.0909090909090909,1,results,Experiments: Neutral Pose Reconstruction on MICC,face_alignment11
1450,190,We instead average our encoder embeddings before making a single reconstruction .,Experiments,Neutral Pose Reconstruction on MICC,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",25,0.8333333333333334,189,0.7105263157894737,6,0.5454545454545454,1,experiments,Experiments: Neutral Pose Reconstruction on MICC,face_alignment11
1451,194,"Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .",Experiments,Neutral Pose Reconstruction on MICC,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.9666666666666668,193,0.7255639097744361,10,0.9090909090909092,1,results,Experiments: Neutral Pose Reconstruction on MICC,face_alignment11
1452,195,"We are also more stable across changing environments , with similar results for all three test sets .",Experiments,Neutral Pose Reconstruction on MICC,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,1.0,194,0.7293233082706767,11,1.0,1,results,Experiments: Neutral Pose Reconstruction on MICC,face_alignment11
1453,196,Face Recognition Results,,,face_alignment,11,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,195,0.7330827067669173,0,0.0,1,results,,face_alignment11
1454,202,Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .,Face Recognition Results,Face Recognition Results,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3529411764705882,201,0.7556390977443609,6,0.2307692307692307,1,results,Face Recognition Results,face_alignment11
1455,205,"Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .",Face Recognition Results,Face Recognition Results,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.5294117647058824,204,0.7669172932330827,9,0.3461538461538461,1,results,Face Recognition Results,face_alignment11
1456,211,"Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .",Face Recognition Results,Face Recognition Results,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.8823529411764706,210,0.7894736842105263,15,0.5769230769230769,1,results,Face Recognition Results,face_alignment11
1457,251,Facial features smoothly degrade as the necessary information is no longer present in the input image .,Method,Occlusion Stress,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'O']",37,0.7115384615384616,250,0.9398496240601504,13,0.7647058823529411,1,research-problem,Method: Occlusion Stress,face_alignment11
1458,4,Face alignment is a classic problem in the computer vision field .,abstract,abstract,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.011070110701107,1,0.1428571428571428,1,research-problem,abstract,face_alignment12
1459,6,"In this paper , for the first time , we aim at providing a very dense 3D alignment for largepose face images .",abstract,abstract,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",3,0.4285714285714285,5,0.018450184501845,3,0.4285714285714285,1,research-problem,abstract,face_alignment12
1460,21,"Moreover , DeFA should offer dense correspondence not only between two face images , but also between the face image and the canonical 3 D face model .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.25,20,0.07380073800738,10,0.25,1,approach,Introduction,face_alignment12
1461,24,"In this work , we choose to develop the idea of fitting a dense 3 D face model to an image , where the model with thousands of vertexes makes it possible for face alignment to go very "" dense "" .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.325,23,0.084870848708487,13,0.325,1,approach,Introduction,face_alignment12
1462,38,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",27,0.675,37,0.1365313653136531,27,0.675,1,approach,Introduction,face_alignment12
1463,41,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'O']",30,0.75,40,0.1476014760147601,30,0.75,1,approach,Introduction,face_alignment12
1464,42,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",31,0.775,41,0.1512915129151291,31,0.775,1,approach,Introduction,face_alignment12
1465,43,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",32,0.8,42,0.1549815498154981,32,0.8,1,approach,Introduction,face_alignment12
1466,46,"Generally , our main contributions can be summarized as : 1 . We identify and define anew problem of dense face alignment , which seeks alignment of face - region pixels beyond the sparse set of landmarks .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",35,0.875,45,0.1660516605166051,35,0.875,1,approach,Introduction,face_alignment12
1467,48,"To achieve dense face alignment , we develop a novel 3 D face model fitting algorithm that adopts multiple constraints and leverages multiple datasets .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",37,0.925,47,0.1734317343173431,37,0.925,1,approach,Introduction,face_alignment12
1468,196,"At stage 1 , we use 300W - LP to train our DeFA network with parameter constraint ( PL ) .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.1290322580645161,195,0.7195571955719557,4,0.1290322580645161,1,hyperparameters,Experimental setup,face_alignment12
1469,197,"At stage 2 , we additionally include samples from the Caltech10K , and COFW to continue the training of our network with the additional landmark fitting constraint ( LFC ) .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.1612903225806451,196,0.7232472324723247,5,0.1612903225806451,1,hyperparameters,Experimental setup,face_alignment12
1470,198,"At stage 3 , we fine - tune the model with SPC and CFC constraints .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.1935483870967742,197,0.7269372693726938,6,0.1935483870967742,1,baselines,Experimental setup,face_alignment12
1471,199,"For large - pose face alignment , we fine - tune the model with AFLW - LFPA training set .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.2258064516129032,198,0.7306273062730627,7,0.2258064516129032,1,experiments,Experimental setup,face_alignment12
1472,200,"For near - frontal face alignment , we fine - tune the model with 300 W training set .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.2580645161290322,199,0.7343173431734318,8,0.2580645161290322,1,hyperparameters,Experimental setup,face_alignment12
1473,201,"All samples at the third stage are augmented 20 times with up to 20 random in - plain rotation and 15 % random noise on the center , width , and length of the initial bounding box .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",9,0.2903225806451613,200,0.7380073800738007,9,0.2903225806451613,1,hyperparameters,Experimental setup,face_alignment12
1474,203,"To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.3548387096774194,202,0.7453874538745388,11,0.3548387096774194,1,experiments,Experimental setup,face_alignment12
1475,204,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",12,0.3870967741935484,203,0.7490774907749077,12,0.3870967741935484,1,hyperparameters,Experimental setup,face_alignment12
1476,205,"The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",13,0.4193548387096774,204,0.7527675276752768,13,0.4193548387096774,1,experiments,Experimental setup,face_alignment12
1477,206,"In stage 2 , the regularization weights ?",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",14,0.4516129032258064,205,0.7564575645756457,14,0.4516129032258064,1,hyperparameters,Experimental setup,face_alignment12
1478,208,"lm for LFC is 5 ; In stage 3 , the regularization weights ? lm , ? s , ?",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5161290322580645,207,0.7638376383763837,16,0.5161290322580645,1,hyperparameters,Experimental setup,face_alignment12
1479,209,"c for LFC , SPC and CFC are set as 5 , 1 and 1 , respectively .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",17,0.5483870967741935,208,0.7675276752767528,17,0.5483870967741935,1,hyperparameters,Experimental setup,face_alignment12
1480,218,"For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",26,0.8387096774193549,217,0.8007380073800738,26,0.8387096774193549,1,experiments,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1481,219,"For AFLW2000 - 3D , our method also shows a large improvement .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",27,0.8709677419354839,218,0.8044280442804428,27,0.8709677419354839,1,experiments,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1482,220,"Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.9032258064516128,219,0.8081180811808119,28,0.9032258064516128,1,experiments,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1483,221,"For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.935483870967742,220,0.8118081180811808,29,0.935483870967742,1,experiments,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1484,223,The consistently superior performance of our DeFA indicates that we have advanced the state of the art in large - pose face alignment .,Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",31,1.0,222,0.8191881918819188,31,1.0,1,experiments,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1485,225,"Even though the proposed method can handle largepose alignment , to show its performance on the near- frontal datasets , we evaluate our method on the 300W dataset .",Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",1,0.125,224,0.8265682656826568,1,0.125,1,results,Experiments on Near-frontal Datasets,face_alignment12
1486,227,"4 . To find the corresponding landmarks on the cheek , we apply the landmark marching algorithm to move contour landmarks from self - occluded location to the silhouette .",Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",3,0.375,226,0.8339483394833949,3,0.375,1,baselines,Experiments on Near-frontal Datasets,face_alignment12
1487,228,Our method is the second best method on the challenging set .,Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",4,0.5,227,0.8376383763837638,4,0.5,1,results,Experiments on Near-frontal Datasets,face_alignment12
1488,229,"In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .",Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,228,0.8413284132841329,5,0.625,1,results,Experiments on Near-frontal Datasets,face_alignment12
1489,232,"It is a strong testimony of our model in that DeFA , without further finetuning , outperforms both 3DDFA and its fine tuned version with SDM .",Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",8,1.0,231,0.8523985239852399,8,1.0,1,results,Experiments on Near-frontal Datasets,face_alignment12
1490,238,The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.1666666666666666,237,0.8745387453874539,5,0.1666666666666666,1,ablation-analysis,Ablation Study,face_alignment12
1491,239,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .",Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",6,0.2,238,0.8782287822878229,6,0.2,1,ablation-analysis,Ablation Study,face_alignment12
1492,240,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .",Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.2333333333333333,239,0.8819188191881919,7,0.2333333333333333,1,ablation-analysis,Ablation Study,face_alignment12
1493,242,5 shows that the effectiveness of CFC and SPC is more than LFC .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",9,0.3,241,0.8892988929889298,9,0.3,1,ablation-analysis,Ablation Study,face_alignment12
1494,248,Comparing LFC + SPC and LFC + CFC performances shows that the CFC is more helpful than the SPC .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",15,0.5,247,0.911439114391144,15,0.5,1,ablation-analysis,Ablation Study,face_alignment12
1495,250,Using all constraints achieves the best performance .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",17,0.5666666666666667,249,0.918819188191882,17,0.5666666666666667,1,ablation-analysis,Ablation Study,face_alignment12
1496,254,This result shows that for the images with NME - lp between 5 % and 15 % the SPC is helpful .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'O']",21,0.7,253,0.933579335793358,21,0.7,1,ablation-analysis,Ablation Study,face_alignment12
1497,262,"As shown in , SPC utilizes SIFT points to cover the whole 3D shape and the points in the highly textured areas are substantially used .",Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",29,0.9666666666666668,261,0.96309963099631,29,0.9666666666666668,1,ablation-analysis,Ablation Study,face_alignment12
1498,2,Nonlinear 3D Face Morphable Model,title,,face_alignment,13,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0033670033670033,1,0.0,1,research-problem,title,face_alignment13
1499,4,"As a classic statistical model of 3D facial shape and texture , 3D Morphable Model ( 3 DMM ) is widely used in facial analysis , e.g. , model fitting , image synthesis .",abstract,abstract,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",1,0.1,3,0.0101010101010101,1,0.1,1,research-problem,abstract,face_alignment13
1500,11,The entire network is end - to - end trainable with only weak supervision .,abstract,abstract,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.8,10,0.0336700336700336,8,0.8,1,model,abstract,face_alignment13
1501,16,"The morphable model framework provides two key benefits : first , a point - to - point correspondence between the reconstruction and all other models , enabling morphing , and second , modeling underlying transformations between types of faces ( male to female , neutral to smile , etc . ) .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.025,15,0.0505050505050505,2,0.0425531914893617,1,model,Introduction,face_alignment13
1502,17,"3 DMM has been widely applied in numerous areas , such as computer vision , graphics , human behavioral analysis and craniofacial surgery .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0375,16,0.0538720538720538,3,0.0638297872340425,1,research-problem,Introduction,face_alignment13
1503,19,We propose a nonlinear 3 DMM to model shape / texture via deep neural networks ( DNNs ) .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.0625,18,0.0606060606060606,5,0.1063829787234042,1,model,Introduction,face_alignment13
1504,20,"It can be trained from in - the - wild face images without 3 D scans , and also better reconstructs the original images due to the inherent nonlinearity .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",6,0.075,19,0.0639730639730639,6,0.1276595744680851,1,model,Introduction,face_alignment13
1505,22,"To model highly variable 3 D face shapes , a large amount of high - quality 3 D face scans is required .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",8,0.1,21,0.0707070707070707,8,0.1702127659574468,1,research-problem,Introduction,face_alignment13
1506,26,"Hence , it is fragile to large variances in the face identity .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.15,25,0.0841750841750841,12,0.2553191489361702,1,model,Introduction,face_alignment13
1507,45,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",31,0.3875,44,0.1481481481481481,31,0.6595744680851063,1,model,Introduction,face_alignment13
1508,46,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",32,0.4,45,0.1515151515151515,32,0.6808510638297872,1,model,Introduction,face_alignment13
1509,48,These two decoders are essentially the nonlinear 3 DMM .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",34,0.425,47,0.1582491582491582,34,0.723404255319149,1,model,Introduction,face_alignment13
1510,49,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",35,0.4375,48,0.1616161616161616,35,0.7446808510638298,1,model,Introduction,face_alignment13
1511,50,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'O']",36,0.45,49,0.1649831649831649,36,0.7659574468085106,1,model,Introduction,face_alignment13
1512,52,"Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",38,0.475,51,0.1717171717171717,38,0.8085106382978723,1,model,Introduction,face_alignment13
1513,53,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",39,0.4875,52,0.1750841750841751,39,0.8297872340425532,1,model,Introduction,face_alignment13
1514,54,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",40,0.5,53,0.1784511784511784,40,0.851063829787234,1,model,Introduction,face_alignment13
1515,58,1 ) We learn a nonlinear 3 DMM model that has greater representation power than its traditional linear counterpart .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",44,0.55,57,0.1919191919191919,44,0.9361702127659576,1,model,Introduction,face_alignment13
1516,59,"2 ) We jointly learn the model and the model fitting algorithm via weak supervision , by leveraging a large collection of 2D images without 3D scans .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",45,0.5625,58,0.1952861952861952,45,0.9574468085106383,1,model,Introduction,face_alignment13
1517,60,The novel rendering layer enables the end - to - end training .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",46,0.575,59,0.1986531986531986,46,0.9787234042553192,1,model,Introduction,face_alignment13
1518,207,"Using facial mesh triangle definition by Basel Face Model ( BFM ) , we train our 3 DMM using 300W - LP dataset .",Experimental Results,Experimental Results,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0454545454545454,206,0.6936026936026936,2,0.5,1,results,Experimental Results,face_alignment13
1519,208,"The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.",Experimental Results,Experimental Results,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n']",3,0.0681818181818181,207,0.696969696969697,3,0.75,1,ablation-analysis,Experimental Results,face_alignment13
1520,209,"We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .",Experimental Results,Experimental Results,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",4,0.0909090909090909,208,0.7003367003367004,4,1.0,1,ablation-analysis,Experimental Results,face_alignment13
1521,210,Expressiveness,Experimental Results,,face_alignment,13,['O'],['B-n'],5,0.1136363636363636,209,0.7037037037037037,0,0.0,1,results,Experimental Results,face_alignment13
1522,247,"Our nonlinear model has a significantly smaller reconstruction error than the linear model , 0.0196 vs. 0.0241 ( Tab. 3 ) .",Experimental Results,Texture .,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O']",42,0.9545454545454546,246,0.8282828282828283,21,0.9130434782608696,1,results,Experimental Results: Texture .,face_alignment13
1523,253,visualizes our 3 DMM fitting results on CelebA dataset .,Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.1,252,0.8484848484848485,3,0.1,1,experiments,Applications,face_alignment13
1524,255,We can recover personal facial characteristic in both shape and texture .,Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.1666666666666666,254,0.8552188552188552,5,0.1666666666666666,1,experiments,Applications,face_alignment13
1525,258,Face alignment is a critical step for any facial analysis task such as face recognition .,Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2666666666666666,257,0.8653198653198653,8,0.2666666666666666,1,experiments,Applications,face_alignment13
1526,265,We obtain a low error that is comparable to optimization - based methods .,Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.5,264,0.8888888888888888,15,0.5,1,experiments,Applications,face_alignment13
1527,2,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses,title,title,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0037735849056603,1,0.0,1,research-problem,title,face_alignment14
1528,20,The non-visible regions of the face are determined by the estimated camera center and the estimated 3D shape .,Introduction,Introduction,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.2592592592592592,19,0.0716981132075471,7,0.3043478260869565,1,model,Introduction,face_alignment14
1529,33,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .",Introduction,Introduction,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",20,0.7407407407407407,32,0.120754716981132,20,0.8695652173913043,1,model,Introduction,face_alignment14
1530,49,Landmark locations can be directly predicted by a regression from a learned feature space .,Methods,Methods,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",2,0.0150375939849624,48,0.1811320754716981,8,0.3809523809523809,1,model,Methods,face_alignment14
1531,51,The objective function in GSDM is divided into multiple regions of similar gradient directions .,Methods,Methods,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.0300751879699248,50,0.1886792452830188,10,0.4761904761904761,1,model,Methods,face_alignment14
1532,52,It then constructs a separate cascaded shape regressor for each region .,Methods,Methods,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.037593984962406,51,0.1924528301886792,11,0.5238095238095238,1,model,Methods,face_alignment14
1533,57,3DDFA fits a dense 3 D face model to the image via CNN and DDN proposes a novel cascaded framework incorporating geometric constraints for localizing landmarks in faces and other non-rigid objects .,Methods,Methods,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.075187969924812,56,0.2113207547169811,16,0.7619047619047619,1,baselines,Methods,face_alignment14
1534,60,Nonlinear statistical model approaches are impractical in real - time applications .,Methods,Methods,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.0977443609022556,59,0.2226415094339622,19,0.9047619047619048,1,research-problem,Methods,face_alignment14
1535,63,CNNs for 3D Object Modeling,Methods,,face_alignment,14,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n']",16,0.1203007518796992,62,0.2339622641509434,0,0.0,1,research-problem,Methods,face_alignment14
1536,203,Our network is implemented in the Caffe framework .,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",1,0.0476190476190476,202,0.7622641509433963,1,0.0476190476190476,1,hyperparameters,Implementation Details,face_alignment14
1537,204,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",2,0.0952380952380952,203,0.7660377358490567,2,0.0952380952380952,1,ablation-analysis,Implementation Details,face_alignment14
1538,205,All modules are differentiable so that the whole network can be trained end - to - end .,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.1428571428571428,204,0.769811320754717,3,0.1428571428571428,1,hyperparameters,Implementation Details,face_alignment14
1539,206,"We adopt two architectures , AlexNet and VGG - 16 , as the pre-trained models for our shared feature extraction networks in , i.e. we use the convolution layers from the pre-trained models to initialize ours .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'I-n', 'B-p', 'I-p', 'O', 'O']",4,0.1904761904761904,205,0.7735849056603774,4,0.1904761904761904,1,hyperparameters,Implementation Details,face_alignment14
1540,207,"Since these networks already extract informative low - level features and we do not want to lose this information , we freeze some of the earlier convolution layers and finetune the rest .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O']",5,0.238095238095238,206,0.7773584905660378,5,0.238095238095238,1,hyperparameters,Implementation Details,face_alignment14
1541,208,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.2857142857142857,207,0.7811320754716982,6,0.2857142857142857,1,hyperparameters,Implementation Details,face_alignment14
1542,209,The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.3333333333333333,208,0.7849056603773585,7,0.3333333333333333,1,hyperparameters,Implementation Details,face_alignment14
1543,210,"With N landmarks to regress , we need NFC layers to compute the offsets for each individual landmark .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.3809523809523809,209,0.7886792452830189,8,0.3809523809523809,1,hyperparameters,Implementation Details,face_alignment14
1544,211,"While it 's possible to setup N individual FC layers , here we implement this by adding one Scaling layer followed by a Reduction layer and Bias layer .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.4285714285714285,210,0.7924528301886793,9,0.4285714285714285,1,hyperparameters,Implementation Details,face_alignment14
1545,212,During training only the new layers are updated and all previous layers are frozen .,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",10,0.4761904761904761,211,0.7962264150943397,10,0.4761904761904761,1,hyperparameters,Implementation Details,face_alignment14
1546,213,Training on 300W - LP,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'I-n', 'I-n']",11,0.5238095238095238,212,0.8,11,0.5238095238095238,1,experiments,Implementation Details,face_alignment14
1547,216,"For the AlexNet architecture , we train for 100,000 iterations with a batch size of 50 .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.6666666666666666,215,0.8113207547169812,14,0.6666666666666666,1,experiments,Implementation Details,face_alignment14
1548,217,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 50,000 iterations .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.7142857142857143,216,0.8150943396226416,15,0.7142857142857143,1,hyperparameters,Implementation Details,face_alignment14
1549,218,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 40,000 iterations .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",16,0.7619047619047619,217,0.8188679245283019,16,0.7619047619047619,1,hyperparameters,Implementation Details,face_alignment14
1550,219,"For the VGG - 16 architecture , we train for 200,000 iterations with a batch size of 25 .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",17,0.8095238095238095,218,0.8226415094339623,17,0.8095238095238095,1,experiments,Implementation Details,face_alignment14
1551,220,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 100,000 iterations .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.8571428571428571,219,0.8264150943396227,18,0.8571428571428571,1,hyperparameters,Implementation Details,face_alignment14
1552,221,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 70,000 iterations .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",19,0.9047619047619048,220,0.8301886792452831,19,0.9047619047619048,1,hyperparameters,Implementation Details,face_alignment14
1553,222,The momentum for all experiments is set to 0.9 .,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",20,0.9523809523809524,221,0.8339622641509434,20,0.9523809523809524,1,hyperparameters,Implementation Details,face_alignment14
1554,229,The VGG - 16 model outperforms the AlexNet model in all three pose ranges on the AFLW detected set as shown in .,Ablation Experiments,Ablation Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",5,0.5555555555555556,228,0.8603773584905661,5,0.5555555555555556,1,ablation-analysis,Ablation Experiments,face_alignment14
1555,233,shows that the landmark regression step greatly helps to improve the accuracy .,Ablation Experiments,Ablation Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",9,1.0,232,0.8754716981132076,9,1.0,1,ablation-analysis,Ablation Experiments,face_alignment14
1556,235,"AFLW : Since the CMS - RCNN approach may only detect the easier to landmark faces , we use the provided bounding box anytime the face is not detected by the detector .",Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']",1,0.0384615384615384,234,0.8830188679245283,1,0.0526315789473684,1,baselines,Comparison Experiments,face_alignment14
1557,238,"We compare against baseline methods used by on the same dataset , namely Cascaded Deformable Shape Models ( CDM ) , Robust Cascaded Pose Regression ( RCPR ) , Explicit Shape Regression ( ESR ) , SDM and 3DDFA .",Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']",4,0.1538461538461538,237,0.8943396226415095,4,0.2105263157894736,1,baselines,Comparison Experiments,face_alignment14
1558,239,All methods except for CDM were retrained on the 300W - LP dataset .,Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.1923076923076923,238,0.8981132075471698,5,0.2631578947368421,1,baselines,Comparison Experiments,face_alignment14
1559,241,"clearly shows that our model using the VGG - 16 architecture has achieved better accuracy in all pose ranges , especially the ( 60 , 90 ] category , and has achieved a smaller standard deviation in the error .",Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",7,0.2692307692307692,240,0.9056603773584906,7,0.3684210526315789,1,results,Comparison Experiments,face_alignment14
1560,242,"This means that not only are the landmarks more accurate , they are more consistent than the other methods ..",Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",8,0.3076923076923077,241,0.909433962264151,8,0.4210526315789473,1,results,Comparison Experiments,face_alignment14
1561,247,AFLW2000 - 3D :,Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O']",13,0.5,246,0.9283018867924528,13,0.6842105263157895,1,baselines,Comparison Experiments,face_alignment14
1562,251,"Here we see that though 3DDFA + SDM performs well , the VGG - 16 architecture of our model still performs best in both the [ 0 , 30 ] and ( 60 , 90 ] ranges .",Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.6538461538461539,250,0.9433962264150944,17,0.8947368421052632,1,results,Comparison Experiments,face_alignment14
1563,252,"While the VGG - 16 model is only second best in the ( 30 , 60 ] range by a small amount , the improvement in ( 60 , 90 ] means that , once again , our method generates more accurate and more consistent landmarks , even in a 3D sense .",Comparison Experiments,Comparison Experiments,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6923076923076923,251,0.9471698113207548,18,0.9473684210526316,1,results,Comparison Experiments,face_alignment14
1564,254,Running Speed,Comparison Experiments,,face_alignment,14,"['O', 'O']","['B-n', 'I-n']",20,0.7692307692307693,253,0.9547169811320756,0,0.0,1,results,Comparison Experiments,face_alignment14
1565,257,The models are evaluated on a 3.40 GHz Intel Core i7-6700 CPU and an NVIDIA GeForce GTX TITAN X GPU .,Comparison Experiments,Running Speed,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",23,0.8846153846153846,256,0.9660377358490566,3,0.5,1,results,Comparison Experiments: Running Speed,face_alignment14
1566,2,Face Alignment Across Large Poses : A 3D Solution,title,,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0034965034965034,1,0.0,1,research-problem,title,face_alignment15
1567,18,The latter extracts features around key points and regresses it to the ground truth landmarks .,Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.108695652173913,17,0.0594405594405594,5,0.1666666666666666,1,model,Introduction,face_alignment15
1568,45,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",32,0.6956521739130435,44,0.1538461538461538,1,0.0666666666666666,1,model,Introduction,face_alignment15
1569,47,We call this method 3D Dense Face Alignment ( 3DDFA ) .,Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",34,0.7391304347826086,46,0.1608391608391608,3,0.2,1,research-problem,Introduction,face_alignment15
1570,50,"To resolve the fitting process in 3 DDFA , we propose a cascaded convolutional neutral network ( CNN ) based regression method .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",37,0.8043478260869565,49,0.1713286713286713,6,0.4,1,model,Introduction,face_alignment15
1571,52,"In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",39,0.8478260869565217,51,0.1783216783216783,8,0.5333333333333333,1,model,Introduction,face_alignment15
1572,53,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",40,0.8695652173913043,52,0.1818181818181818,9,0.6,1,model,Introduction,face_alignment15
1573,57,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",44,0.9565217391304348,56,0.1958041958041958,13,0.8666666666666667,1,model,Introduction,face_alignment15
1574,59,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/.",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",46,1.0,58,0.2027972027972027,15,1.0,1,code,Introduction,face_alignment15
1575,239,"In this paper , we test the performance of 3DDFA on three different tasks , including the large - pose face alignment on AFLW , 3 D face alignment on AFLW2000 - 3D and mediumpose face alignment on 300W .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",1,0.0256410256410256,238,0.8321678321678322,1,0.0384615384615384,1,baselines,Comparison Experiments,face_alignment15
1576,240,Large Pose Face Alignment in AFLW Protocol :,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O']",2,0.0512820512820512,239,0.8356643356643356,2,0.0769230769230769,1,baselines,Comparison Experiments,face_alignment15
1577,242,The bounding boxes provided by AFLW are used for initialization ( which are not the ground truth ) .,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1025641025641025,241,0.8426573426573427,4,0.1538461538461538,1,hyperparameters,Comparison Experiments,face_alignment15
1578,243,"During training , for 2D methods we use the projected 3D landmarks as the ground truth and for 3DDFA we directly regress the 3 DMM parameters .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.1282051282051282,242,0.8461538461538461,5,0.1923076923076923,1,hyperparameters,Comparison Experiments,face_alignment15
1579,250,CDM is the first one claimed to perform pose - free face alignment .,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.3076923076923077,249,0.8706293706293706,12,0.4615384615384615,1,baselines,Comparison Experiments,face_alignment15
1580,251,RCPR is a occlusion - robust method with the potential to deal with selfocclusion and we train it with landmark visibility labels computed by .,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",13,0.3333333333333333,250,0.8741258741258742,13,0.5,1,baselines,Comparison Experiments,face_alignment15
1581,258,"Firstly , the results indicate that all the methods benefits substantially from face profiling when dealing with large poses .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",20,0.5128205128205128,257,0.8986013986013986,20,0.7692307692307693,1,results,Comparison Experiments,face_alignment15
1582,259,"The improvements in [ 60 , 90 ] are 44.06 % for RCPR , 40.36 % for ESR and 42.10 % for SDM .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",21,0.5384615384615384,258,0.902097902097902,21,0.8076923076923077,1,results,Comparison Experiments,face_alignment15
1583,262,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",24,0.6153846153846154,261,0.9125874125874126,24,0.9230769230769232,1,results,Comparison Experiments,face_alignment15
1584,263,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O']",25,0.6410256410256411,262,0.916083916083916,25,0.9615384615384616,1,results,Comparison Experiments,face_alignment15
1585,265,3D Face Alignment in AFLW2000-3D,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O']",27,0.6923076923076923,264,0.9230769230769232,0,0.0,1,baselines,Comparison Experiments,face_alignment15
1586,269,"Compared with the results in AFLW , we can seethe defect of barely evaluating visible landmarks .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",31,0.7948717948717948,268,0.9370629370629372,4,0.6666666666666666,1,results,Comparison Experiments,face_alignment15
1587,270,"For all the methods , despite with ground truth bounding boxes the performance in [ 60 , 90 ] and the standard deviation are obviously reduced when considering all the landmarks .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",32,0.8205128205128205,269,0.9405594405594404,5,0.8333333333333334,1,results,Comparison Experiments,face_alignment15
1588,2,Deep Multi- Center Learning for Face Alignment,title,,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0030395136778115,1,0.0,1,research-problem,title,face_alignment16
1589,11,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,abstract,abstract,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",8,0.6666666666666666,10,0.0303951367781155,8,0.1666666666666666,1,code,abstract,face_alignment16
1590,27,"However , it needs extra labels of facial attributes for training samples , which limits its universality .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O']",11,0.1264367816091954,26,0.0790273556231003,24,0.5,1,model,I. INTRODUCTION,face_alignment16
1591,32,It is observed that the nose can be localized roughly with the locations of eyes and mouth .,I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.1839080459770115,31,0.094224924012158,29,0.6041666666666666,1,model,I. INTRODUCTION,face_alignment16
1592,35,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",19,0.2183908045977011,34,0.1033434650455927,32,0.6666666666666666,1,model,I. INTRODUCTION,face_alignment16
1593,36,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",20,0.2298850574712643,35,0.1063829787234042,33,0.6875,1,model,I. INTRODUCTION,face_alignment16
1594,37,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O']",21,0.2413793103448276,36,0.1094224924012158,34,0.7083333333333334,1,model,I. INTRODUCTION,face_alignment16
1595,38,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.2528735632183908,37,0.1124620060790273,35,0.7291666666666666,1,model,I. INTRODUCTION,face_alignment16
1596,39,The entire framework reinforces the learning process of each landmark with a low model complexity .,I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",23,0.264367816091954,38,0.1155015197568389,36,0.75,1,model,I. INTRODUCTION,face_alignment16
1597,41,We propose a novel multi-center learning framework for exploiting the strong correlations among landmarks .,I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",25,0.2873563218390804,40,0.121580547112462,38,0.7916666666666666,1,model,I. INTRODUCTION,face_alignment16
1598,42,We propose a model assembling method which ensures a low model complexity .,I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",26,0.2988505747126437,41,0.1246200607902735,39,0.8125,1,model,I. INTRODUCTION,face_alignment16
1599,87,C. Face Alignment via Deep Learning,I. INTRODUCTION,,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O']",71,0.8160919540229885,86,0.2613981762917933,0,0.0,1,research-problem,I. INTRODUCTION,face_alignment16
1600,208,"We enhance the diversity of raw training data on account of their limited variation patterns , using five steps : rotation , uniform scaling , translation , horizontal flip , and JPEG compression .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",104,0.6933333333333334,207,0.6291793313069909,73,0.453416149068323,1,hyperparameters,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1601,213,We train our MCL using an open source deep learning framework Caffe .,A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",109,0.7266666666666667,212,0.6443768996960486,78,0.484472049689441,1,hyperparameters,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1602,217,"The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']",113,0.7533333333333333,216,0.6565349544072948,82,0.5093167701863354,1,hyperparameters,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1603,234,"FLD + PDE performs facial landmark detection , pose and deformation estimation simultaneously , in which the training data of pose and deformation estimation are used .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",130,0.8666666666666667,233,0.7082066869300911,99,0.6149068322981367,1,baselines,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1604,237,"Our method MCL outperforms most of the state - of - the - art methods , especially on AFLW dataset where a relative error reduction of 3.93 % is achieved compared to RecNet .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O']",133,0.8866666666666667,236,0.7173252279635258,102,0.6335403726708074,1,results,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1605,256,1 ) Global Average Pooling vs. Full Connection :,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0153846153846153,255,0.7750759878419453,121,0.7515527950310559,1,ablation-analysis,C. Ablation Study,face_alignment16
1606,266,2 ) Robustness of Weighting :,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O']",11,0.1692307692307692,265,0.8054711246200608,131,0.8136645962732919,1,ablation-analysis,C. Ablation Study,face_alignment16
1607,273,"When ? is 0.4 , WM can still achieves good performance .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'O', 'I-p', 'B-n', 'I-n', 'O']",18,0.2769230769230769,272,0.8267477203647416,138,0.8571428571428571,1,ablation-analysis,C. Ablation Study,face_alignment16
1608,279,"Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",24,0.3692307692307692,278,0.8449848024316109,144,0.8944099378881988,1,ablation-analysis,C. Ablation Study,face_alignment16
1609,282,"Taking the left eye model as an example , it additionally reduces the errors of landmarks of right eye , mouth , and chin , which is due to the correlations among different facial parts .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.4153846153846154,281,0.8541033434650456,147,0.9130434782608696,1,ablation-analysis,C. Ablation Study,face_alignment16
1610,283,"Moreover , for the right eye cluster , the right eye model improves the accuracy more significantly than the left eye model .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",28,0.4307692307692308,282,0.8571428571428571,148,0.9192546583850932,1,ablation-analysis,C. Ablation Study,face_alignment16
1611,287,"Note that Simplified AM has already acquired good results , which verifies the effectiveness of the multicenter fine - tuning stage .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4923076923076923,286,0.8693009118541033,152,0.9440993788819876,1,ablation-analysis,C. Ablation Study,face_alignment16
1612,290,It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG .,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",35,0.5384615384615384,289,0.878419452887538,155,0.9627329192546584,1,ablation-analysis,C. Ablation Study,face_alignment16
1613,2,Aggregation via Separation : Boosting Facial Landmark Detector with Semi-Supervised,title,title,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.5,1,0.003584229390681,1,0.5,1,research-problem,title,face_alignment17
1614,3,Style Translation,title,,face_alignment,17,"['O', 'O']","['B-n', 'I-n']",2,1.0,2,0.007168458781362,2,1.0,1,research-problem,title,face_alignment17
1615,8,"With these augmented synthetic samples , our semi-supervised model surprisingly outperforms the fully - supervised one by a large margin .",abstract,abstract,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",4,0.5714285714285714,7,0.025089605734767,4,0.5714285714285714,1,research-problem,abstract,face_alignment17
1616,11,The code is made publicly available at https://github.com/thesouthfrog/stylealign.,abstract,abstract,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",7,1.0,10,0.03584229390681,7,1.0,1,code,abstract,face_alignment17
1617,13,"Facial landmark detection is a fundamentally important step in many face applications , such as face recognition , 3 D face reconstruction , face tracking and face editing .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0285714285714285,12,0.043010752688172,1,0.0285714285714285,1,research-problem,Introduction,face_alignment17
1618,24,"We instead utilize style transfer and disentangled representation learning to tackle the face alignment problem , since style transfer aims at altering style while preserving content .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3428571428571428,23,0.082437275985663,12,0.3428571428571428,1,model,Introduction,face_alignment17
1619,26,"Our idea is based on the purpose of facial landmark detection , which is to regress "" facial content "" - the principal component of facial geometry - by filtering unconstrained "" styles "" .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.4,25,0.089605734767025,14,0.4,1,model,Introduction,face_alignment17
1620,30,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",18,0.5142857142857142,29,0.1039426523297491,18,0.5142857142857142,1,model,Introduction,face_alignment17
1621,31,"Instead of directly generating images , we first map face images into the space of structure and style .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.5428571428571428,30,0.1075268817204301,19,0.5428571428571428,1,model,Introduction,face_alignment17
1622,32,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']",20,0.5714285714285714,31,0.1111111111111111,20,0.5714285714285714,1,model,Introduction,face_alignment17
1623,33,"By factoring these features , we perform visual style translation between existing facial geometry .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",21,0.6,32,0.1146953405017921,21,0.6,1,model,Introduction,face_alignment17
1624,40,A novel semi-supervised framework based on conditional variational auto - encoder is built upon this new perspective .,Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.8,39,0.1397849462365591,28,0.8,1,model,Introduction,face_alignment17
1625,41,"By disentangling style and structure , our model generates style - augmented images via style translation , further boosting facial landmark detection .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",29,0.8285714285714286,40,0.1433691756272401,29,0.8285714285714286,1,model,Introduction,face_alignment17
1626,168,The Res - 18 baseline receives strong enhancement using synthetic images .,Evaluation Metrics,WFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",19,0.5588235294117647,167,0.5985663082437276,20,0.5714285714285714,1,experiments,Evaluation Metrics: WFLW,face_alignment17
1627,171,"By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .",Evaluation Metrics,WFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",22,0.6470588235294118,170,0.6093189964157706,23,0.6571428571428571,1,experiments,Evaluation Metrics: WFLW,face_alignment17
1628,172,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .",Evaluation Metrics,WFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",23,0.6764705882352942,171,0.6129032258064516,24,0.6857142857142857,1,experiments,Evaluation Metrics: WFLW,face_alignment17
1629,215,"It shows when the data is limited , our separation component tends to capture weak style information , such as color and lighting .",Ablation Study,Improvement on Limited Data,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",16,0.5161290322580645,214,0.7670250896057348,15,0.8823529411764706,1,ablation-analysis,Ablation Study: Improvement on Limited Data,face_alignment17
1630,2,Deep Alignment Network : A convolutional neural network for robust face alignment,title,title,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0041152263374485,1,0.0,1,research-problem,title,face_alignment18
1631,13,"Face alignment is an important component of many computer vision applications , such as face verification , facial emotion recognition , humancomputer interaction and facial motion capture .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0571428571428571,12,0.0493827160493827,2,0.0571428571428571,1,research-problem,Introduction,face_alignment18
1632,16,The features are then used to iteratively refine the estimates of landmark locations .,Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.1428571428571428,15,0.0617283950617283,5,0.1428571428571428,1,model,Introduction,face_alignment18
1633,19,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.2285714285714285,18,0.074074074074074,8,0.2285714285714285,1,model,Introduction,face_alignment18
1634,20,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",9,0.2571428571428571,19,0.0781893004115226,9,0.2571428571428571,1,model,Introduction,face_alignment18
1635,22,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'O']",11,0.3142857142857143,21,0.0864197530864197,11,0.3142857142857143,1,model,Introduction,face_alignment18
1636,24,The convolutional neural network can use the heatmaps to infer the current estimates of landmark locations in the image and thus refine them .,Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O']",13,0.3714285714285714,23,0.0946502057613168,13,0.3714285714285714,1,model,Introduction,face_alignment18
1637,29,We introduce landmark heatmaps which transfer the information about current landmark location estimates between the stages of our method .,Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.5142857142857142,28,0.1152263374485596,18,0.5142857142857142,1,model,Introduction,face_alignment18
1638,30,"This improvement allows our method to make use of the entire image of a face , instead of local patches , and avoid falling into local minima .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",19,0.5428571428571428,29,0.1193415637860082,19,0.5428571428571428,1,model,Introduction,face_alignment18
1639,191,"We train two models , DAN which is trained on the training subset of the 300W competition data and DAN - Menpo which is trained on both the above mentioned dataset and the Menpo challenge training set .",Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0256410256410256,190,0.7818930041152263,40,0.6451612903225806,1,baselines,Implementation,face_alignment18
1640,192,"Data augmentation is performed by mirroring around the Y axis as well as random translation , rotation and scaling , all sampled from normal distributions .",Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",2,0.0512820512820512,191,0.7860082304526749,41,0.6612903225806451,1,experimental-setup,Implementation,face_alignment18
1641,194,Both models ( DAN and DAN - Menpo ) consist of two stages .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",4,0.1025641025641025,193,0.7942386831275721,43,0.6935483870967742,1,baselines,Implementation,face_alignment18
1642,195,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",5,0.1282051282051282,194,0.7983539094650206,44,0.7096774193548387,1,experimental-setup,Implementation,face_alignment18
1643,196,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",6,0.1538461538461538,195,0.8024691358024691,45,0.7258064516129032,1,experimental-setup,Implementation,face_alignment18
1644,198,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.2051282051282051,197,0.8106995884773662,47,0.7580645161290323,1,experimental-setup,Implementation,face_alignment18
1645,206,For each test set we initialize our method using the face detector bounding boxes provided with the datasets .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",16,0.4102564102564102,205,0.8436213991769548,55,0.8870967741935484,1,experimental-setup,Implementation,face_alignment18
1646,214,Results on the Menpo challenge test set,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",24,0.6153846153846154,213,0.8765432098765432,0,0.0,1,results,Implementation,face_alignment18
1647,218,The first step performs face alignment using a square initialization bounding box placed in the middle of the image with a size set to a percentage of image height .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.717948717948718,217,0.8930041152263375,4,0.2666666666666666,1,experimental-setup,Implementation,face_alignment18
1648,224,The chosen bounding box size was 46 % of the image height .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",34,0.8717948717948718,223,0.9176954732510288,10,0.6666666666666666,1,experimental-setup,Implementation,face_alignment18
1649,227,For the AUC and the failure rate we have chosen a threshold of 0.03 of the bounding box diagonal as it is approximately equivalent to 0.08 of the interocular distance used in the previous chapter .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.9487179487179488,226,0.9300411522633744,13,0.8666666666666667,1,experimental-setup,Implementation,face_alignment18
1650,234,The addition of the second stage increases the AUC 0.08 by 20 % while the mean error and failure rate are reduced by 14 % and 56 % respectively .,Further evaluation,Further evaluation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",4,0.8,233,0.9588477366255144,4,0.8,1,results,Further evaluation,face_alignment18
1651,235,The addition of a third stage does not bring significant benefit in any of the metrics .,Further evaluation,Further evaluation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'O']",5,1.0,234,0.9629629629629628,5,1.0,1,results,Further evaluation,face_alignment18
1652,2,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild,title,title,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",1,0.0,1,0.0048543689320388,1,0.0,1,research-problem,title,face_alignment2
1653,4,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .",abstract,abstract,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.0145631067961165,1,0.125,1,research-problem,abstract,face_alignment2
1654,6,"In this paper , we introduce DeCaFA , an end - to - end deep convolutional cascade architecture for face alignment .",abstract,abstract,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.375,5,0.0242718446601941,3,0.375,1,research-problem,abstract,face_alignment2
1655,10,"We show experimentally that DeCaFA significantly outperforms existing approaches on 300W , CelebA and WFLW databases .",abstract,abstract,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.875,9,0.0436893203883495,7,0.875,1,research-problem,abstract,face_alignment2
1656,17,"This allows to robustly learn rigid transformations , such as translation and rotation , in the first cascade stages , while learning non-rigid deformation ( e.g. due to facial expression or non-planar rotation ) later on .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",5,0.2631578947368421,16,0.0776699029126213,5,0.2631578947368421,1,model,Introduction,face_alignment2
1657,22,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.5263157894736842,21,0.1019417475728155,10,0.5263157894736842,1,model,Introduction,face_alignment2
1658,23,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",11,0.5789473684210527,22,0.1067961165048543,11,0.5789473684210527,1,model,Introduction,face_alignment2
1659,27,"We introduce a fully - convolutional Deep Cascade for Face Alignment ( DeCaFA ) that unifies cascaded regression and end - to - end deep approaches , by using landmark - wise attention maps fused to extract local information around a current landmark estimate .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",15,0.7894736842105263,26,0.1262135922330097,15,0.7894736842105263,1,model,Introduction,face_alignment2
1660,28,"We show that intermediate supervision with increasing weights helps DeCaFA to learn coarse attention maps in its early stages , that are refined in the later stages .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8421052631578947,27,0.1310679611650485,16,0.8421052631578947,1,model,Introduction,face_alignment2
1661,29,"Through chaining multiple transfer layers , DeCaFA integrates heterogeneous data annotated with different numbers of landmarks and model the intrinsic relationship between these tasks .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O']",17,0.8947368421052632,28,0.1359223300970873,17,0.8947368421052632,1,model,Introduction,face_alignment2
1662,130,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .",Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.1666666666666666,129,0.6262135922330098,1,0.1666666666666666,1,hyperparameters,Implementation details,face_alignment2
1663,132,Each convolution is followed by a batch normalization layer with ReLU activation .,Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.5,131,0.6359223300970874,3,0.5,1,hyperparameters,Implementation details,face_alignment2
1664,133,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.6666666666666666,132,0.6407766990291263,4,0.6666666666666666,1,hyperparameters,Implementation details,face_alignment2
1665,134,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.8333333333333334,133,0.6456310679611651,5,0.8333333333333334,1,hyperparameters,Implementation details,face_alignment2
1666,135,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .",Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,134,0.6504854368932039,6,1.0,1,experiments,Implementation details,face_alignment2
1667,153,"The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.073170731707317,152,0.7378640776699029,3,0.0909090909090909,1,ablation-analysis,Ablation study,face_alignment2
1668,154,"On IBUG , this difference is more conspicuous , thus there is for improvement by stacking more cascade stages .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O']",4,0.0975609756097561,153,0.7427184466019418,4,0.1212121212121212,1,ablation-analysis,Ablation study,face_alignment2
1669,156,"Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1463414634146341,155,0.7524271844660194,6,0.1818181818181818,1,ablation-analysis,Ablation study,face_alignment2
1670,160,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2439024390243902,159,0.7718446601941747,10,0.303030303030303,1,ablation-analysis,Ablation study,face_alignment2
1671,161,F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2682926829268293,160,0.7766990291262136,11,0.3333333333333333,1,ablation-analysis,Ablation study,face_alignment2
1672,162,"Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.2926829268292683,161,0.7815533980582524,12,0.3636363636363636,1,ablation-analysis,Ablation study,face_alignment2
1673,165,shows a comparison between DeCaFA and recent state - of - the - art approaches on 300W database .,Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",15,0.3658536585365853,164,0.7961165048543689,15,0.4545454545454545,1,ablation-analysis,Ablation study,face_alignment2
1674,166,"Our approach performs better than most existing approaches on the common subset , and performs very close to its best contenders on the challenging subset .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",16,0.3902439024390244,165,0.8009708737864077,16,0.4848484848484848,1,ablation-analysis,Ablation study,face_alignment2
1675,167,"Note that DeCaFA trained only on 300 W trainset has a ME of 3.69 % and is already very competitive with recent approaches , thanks to its end - to - end cascade architecture .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.4146341463414634,166,0.8058252427184466,17,0.5151515151515151,1,ablation-analysis,Ablation study,face_alignment2
1676,168,"DeCaFA is competitive with the best approaches , LAB and DAN - MENPO as well as JMFA - MENPO , which also use external data .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",18,0.4390243902439024,167,0.8106796116504854,18,0.5454545454545454,1,ablation-analysis,Ablation study,face_alignment2
1677,172,DeCaFA performs better than LAB and Wing by a significant margin on every subset .,Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",22,0.5365853658536586,171,0.8300970873786407,22,0.6666666666666666,1,ablation-analysis,Ablation study,face_alignment2
1678,173,"Also , note that DeCaFA trained solely on WFLW already as a ME of 5.01 on the whole test set , which is still better that these two methods .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",23,0.5609756097560976,172,0.8349514563106796,23,0.696969696969697,1,ablation-analysis,Ablation study,face_alignment2
1679,179,"Method Mean error ( % ) SDM 4.35 CFSS 3,95 DSRN 3.08 AAN 2.99 DeCaFA 2.10 approach is the best by a significant margin .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",29,0.7073170731707317,178,0.8640776699029126,29,0.8787878787878788,1,ablation-analysis,Ablation study,face_alignment2
1680,181,"Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",31,0.7560975609756098,180,0.8737864077669902,31,0.9393939393939394,1,ablation-analysis,Ablation study,face_alignment2
1681,188,"A allows to substantially improve the landmark localization on both datasets , most notably when the number of training images is very low .",Ablation study,Using coarsely annotated data from Celeb,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",38,0.926829268292683,187,0.9077669902912622,4,0.5714285714285714,1,ablation-analysis,Ablation study: Using coarsely annotated data from Celeb,face_alignment2
1682,190,"DeCaFA trained with 15 % of 300 W trainset and 6 % of WFLW trainset is on par with SAN on 300W ( , see ) , and is substantially better than DVLN on WFLW .",Ablation study,Using coarsely annotated data from Celeb,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",40,0.975609756097561,189,0.9174757281553398,6,0.8571428571428571,1,ablation-analysis,Ablation study: Using coarsely annotated data from Celeb,face_alignment2
1683,195,"Also notice that the predicted landmarks are close to the corresponding ground truth , even in the presence of rotations and occlusions ( WFLW ) or facial expressions ( CelebA ) .",Qualitative results,Qualitative results,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,1.0,194,0.941747572815534,3,1.0,1,results,Qualitative results,face_alignment2
1684,2,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,title,title,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0029585798816568,1,0.0,1,research-problem,title,face_alignment3
1685,13,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.,abstract,,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",10,1.0,12,0.0355029585798816,10,1.0,1,code,abstract,face_alignment3
1686,15,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037037037,14,0.0414201183431952,1,0.037037037037037,1,research-problem,Introduction,face_alignment3
1687,16,"Face alignment plays an essential role in many face related applications such as face recognition , face frontalization and 3D face reconstruction .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.074074074074074,15,0.044378698224852,2,0.074074074074074,1,research-problem,Introduction,face_alignment3
1688,26,"As a result of i ) and ii ) , models trained with the MSE loss tend to predict a blurry and dilated heatmap with low intensity on foreground pixels compared to the ground truth ( .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']",12,0.4444444444444444,25,0.0739644970414201,12,0.4444444444444444,1,experiments,Introduction,face_alignment3
1689,30,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",16,0.5925925925925926,29,0.0857988165680473,16,0.5925925925925926,1,model,Introduction,face_alignment3
1690,31,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.6296296296296297,30,0.0887573964497041,17,0.6296296296296297,1,model,Introduction,face_alignment3
1691,32,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'B-n', 'B-p', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.6666666666666666,31,0.0917159763313609,18,0.6666666666666666,1,model,Introduction,face_alignment3
1692,33,The encoded coordinate information further improves the performance of our approach .,Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",19,0.7037037037037037,32,0.0946745562130177,19,0.7037037037037037,1,model,Introduction,face_alignment3
1693,34,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",20,0.7407407407407407,33,0.0976331360946745,20,0.7407407407407407,1,model,Introduction,face_alignment3
1694,38,With proposed Weighted Loss Map it is also able to focus on foreground pixels and difficult background pixels during training .,Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",24,0.8888888888888888,37,0.1094674556213017,24,0.8888888888888888,1,model,Introduction,face_alignment3
1695,39,"Encode coordinate information , including coordinates on boundary , into the face alignment algorithm using CoordConv .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",25,0.925925925925926,38,0.1124260355029585,25,0.925925925925926,1,model,Introduction,face_alignment3
1696,128,"The reduced influence of correct estimations helps the network to stay converged , instead of oscillating like the L1 and the Wing loss .",Our Model,Thus pixels with small errors will be amplified .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",52,0.4094488188976378,127,0.3757396449704142,44,0.8627450980392157,1,experiments,Our Model: Thus pixels with small errors will be amplified .,face_alignment3
1697,190,"Difficult background pixels should also be focused on since these pixels are relatively difficult to regress , accurately regressing them could help narrow down the area of foreground pixels to improve localization accuracy . :",Our Model,In our experiments we use W = 10 .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O']",114,0.8976377952755905,189,0.5591715976331361,17,0.8947368421052632,1,ablation-analysis,Our Model: In our experiments we use W = 10 .,face_alignment3
1698,227,"For the WFLW dataset , the provided bounding boxes are not very accurate , to ensure all landmarks are preserved from cropping , we enlarge the bounding boxes by 10 % on both dimensions .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.1111111111111111,226,0.6686390532544378,2,0.1111111111111111,1,hyperparameters,Implementation details,face_alignment3
1699,229,"The input of the network is 256 256 , the output of each stacked HG is 64 64 .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.2222222222222222,228,0.6745562130177515,4,0.2222222222222222,1,hyperparameters,Implementation details,face_alignment3
1700,231,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",6,0.3333333333333333,230,0.6804733727810651,6,0.3333333333333333,1,hyperparameters,Implementation details,face_alignment3
1701,232,We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",7,0.3888888888888889,231,0.6834319526627219,7,0.3888888888888889,1,hyperparameters,Implementation details,face_alignment3
1702,233,"We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.4444444444444444,232,0.6863905325443787,8,0.4444444444444444,1,hyperparameters,Implementation details,face_alignment3
1703,234,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.5,233,0.6893491124260355,9,0.5,1,hyperparameters,Implementation details,face_alignment3
1704,235,"Random Gaussian blur , noise and occlusion are also used .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",10,0.5555555555555556,234,0.6923076923076923,10,0.5555555555555556,1,hyperparameters,Implementation details,face_alignment3
1705,240,"14 . Our approach outperforms previous state - of - the - art by a significant margin , especially on the failure rate .",Implementation details,All models are trained from scratch .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",15,0.8333333333333334,239,0.7071005917159763,15,0.8333333333333334,1,results,Implementation details: All models are trained from scratch .,face_alignment3
1706,241,We are able to reduce the failure rate measured at 10 % NME from 3.73 % to 0.99 % .,Implementation details,All models are trained from scratch .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",16,0.8888888888888888,240,0.7100591715976331,16,0.8888888888888888,1,experiments,Implementation details: All models are trained from scratch .,face_alignment3
1707,243,Our performance on the COFW shows the robustness of our approach against faces with large pose and heavy occlusion .,Implementation details,All models are trained from scratch .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,1.0,242,0.7159763313609467,18,1.0,1,results,Implementation details: All models are trained from scratch .,face_alignment3
1708,245,"Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .",Evaluation on 300W,Evaluation on 300W,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.2,244,0.7218934911242604,1,0.2,1,results,Evaluation on 300W,face_alignment3
1709,246,"For the challenge subset ( iBug dataset ) , we are able to outperform",Evaluation on 300W,Evaluation on 300W,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n']",2,0.4,245,0.7248520710059172,2,0.4,1,results,Evaluation on 300W,face_alignment3
1710,251,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .",Evaluation on WFLW,Evaluation on WFLW,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,250,0.7396449704142012,1,0.0555555555555555,1,results,Evaluation on WFLW,face_alignment3
1711,252,On every subset we outperform the previous state - of - the - art ap - :,Evaluation on WFLW,Evaluation on WFLW,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",2,0.3333333333333333,251,0.742603550295858,2,0.1111111111111111,1,results,Evaluation on WFLW,face_alignment3
1712,255,We are also able to reduce the failure rate and increase the AUC dramatically and hence improving the overall localization quality significantly .,Evaluation on WFLW,Evaluation on WFLW,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O']",5,0.8333333333333334,254,0.7514792899408284,5,0.2777777777777778,1,results,Evaluation on WFLW,face_alignment3
1713,256,"All in all , our approach fails on only 2.84 % of all images , more than a two times improvement compared with 7.6 .",Evaluation on WFLW,Evaluation on WFLW,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,255,0.7544378698224852,6,0.3333333333333333,1,results,Evaluation on WFLW,face_alignment3
1714,273,Note the baseline model ( model trained with MSE ) underperforms the state - of - theart .,Evaluation of different modules,The dataset used for ablation study is WFLW .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.1904761904761904,272,0.8047337278106509,4,0.5,1,results,Evaluation of different modules: The dataset used for ablation study is WFLW .,face_alignment3
1715,274,"To compare with a naive weight mask without focus on hard negative pixels , we introduced a baseline weight map W M base =? W + 1 , where W = 10 . The major contribution comes from Adaptive Wing loss , which improves the benchmark by 0.74 % .",Evaluation of different modules,The dataset used for ablation study is WFLW .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.238095238095238,273,0.8076923076923077,5,0.625,1,results,Evaluation of different modules: The dataset used for ablation study is WFLW .,face_alignment3
1716,275,"All other modules contributed incrementally to the localization performance , our Weighted Loss Map improves 0.25 % , boundary prediction and coordinates encoding are able to contribute another 0.09 % .",Evaluation of different modules,The dataset used for ablation study is WFLW .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O']",6,0.2857142857142857,274,0.8106508875739645,6,0.75,1,results,Evaluation of different modules: The dataset used for ablation study is WFLW .,face_alignment3
1717,290,"Our proposed Adaptive Wing loss significantly boosts performance compared with MSE , which proves the general applicability of the proposed Adaptive Wing loss on more heatmap regression tasks .",Evaluation of different modules,Results are shown in .,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,1.0,289,0.8550295857988166,12,1.0,1,results,Evaluation of different modules: Results are shown in .,face_alignment3
1718,331,"Even with only one HG block , our approach still outperforms previous state - of - the - arts in all datasets except the common subset and the full dataset of 300W .",Method,Experiment on different number of HG stacks,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O']",5,0.7142857142857143,330,0.9763313609467456,3,0.6,1,results,Method: Experiment on different number of HG stacks,face_alignment3
1719,338,Runtime is evaluated on Nvidia GTX 1080 Ti graphics card with batch size of 1 .,Result Visualization,Result Visualization,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,1.0,337,0.9970414201183432,4,1.0,1,experiments,Result Visualization,face_alignment3
1720,2,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network,title,title,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0047393364928909,1,0.0,1,research-problem,title,face_alignment4
1721,15,"It aims to detect the facial landmarks such as eyes , nose and mouth , namely predicting the location parameters of landmarks .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.0714285714285714,14,0.0663507109004739,2,0.0714285714285714,1,research-problem,Introduction,face_alignment4
1722,16,Researchers usually regard this task as atypical non -linear least squares problem .,Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.1071428571428571,15,0.0710900473933649,3,0.1071428571428571,1,research-problem,Introduction,face_alignment4
1723,22,( b ) Self - Iterative Regression . :,Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",9,0.3214285714285714,21,0.0995260663507109,9,0.3214285714285714,1,model,Introduction,face_alignment4
1724,24,"To predict the landmarks ' location parameters , the CR based methods require multiple regressors , while SIR just need one regressor and updates parameters iteratively .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O']",11,0.3928571428571428,23,0.1090047393364928,11,0.3928571428571428,1,model,Introduction,face_alignment4
1725,31,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6428571428571429,30,0.1421800947867298,18,0.6428571428571429,1,model,Introduction,face_alignment4
1726,33,"The training data is obtained by random sampling in the parameter space , and in the test - ing process , parameters are updated iteratively by calling the same regressor , which is dubbed Self - Iterative Regression .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.7142857142857143,32,0.1516587677725118,20,0.7142857142857143,1,model,Introduction,face_alignment4
1727,36,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'O']",23,0.8214285714285714,35,0.1658767772511848,23,0.8214285714285714,1,model,Introduction,face_alignment4
1728,37,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.8571428571428571,36,0.1706161137440758,24,0.8571428571428571,1,model,Introduction,face_alignment4
1729,40,"2 . The Landmarks - Attention Network ( LAN ) is developed to independently learn discriminative features around each landmarks , which significantly reduces the dimension of feature layer and the number of model parameters .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.9642857142857144,39,0.1848341232227488,27,0.9642857142857144,1,model,Introduction,face_alignment4
1730,138,"As illustrated in , SIR is more robust than CR because the former can cover more training space and is n't affected by the optimization path .",Training,Training,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2727272727272727,137,0.6492890995260664,6,0.2727272727272727,1,experiments,Training,face_alignment4
1731,141,"Once one regressor predicts the false direction , the final result is prone to drift away ; ( b ) SIR Descent Direction Map : the training space of SIR includes distribution from coarse stages to fine stages and all descent directions are pointed to ground truth .",Training,Training,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O']",9,0.4090909090909091,140,0.6635071090047393,9,0.4090909090909091,1,experiments,Training,face_alignment4
1732,175,The NME results shows that SIR performs comparatively with RAR ) and outperform other existing methods .,Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O']",20,0.392156862745098,174,0.8246445497630331,20,0.7692307692307693,1,results,Experiments: Metrics .,face_alignment4
1733,182,Comparison with Cascaded Regression,Experiments,,face_alignment,4,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",27,0.5294117647058824,181,0.8578199052132701,0,0.0,1,results,Experiments,face_alignment4
1734,184,"Different from them , our method obtains state - of - the - art performance by iterative call the same regressor rather than adding anymore regressors .",Experiments,Comparison with Cascaded Regression,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",29,0.5686274509803921,183,0.8672985781990521,2,0.2857142857142857,1,results,Experiments: Comparison with Cascaded Regression,face_alignment4
1735,2,Look at Boundary : A Boundary - Aware Face Alignment Algorithm,title,title,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0035087719298245,1,0.0,1,research-problem,title,face_alignment5
1736,4,We present a novel boundary - aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation .,abstract,abstract,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.0909090909090909,3,0.0105263157894736,1,0.0909090909090909,1,research-problem,abstract,face_alignment5
1737,12,"By utilising boundary information of 300 - W dataset , our method achieves 3.92 % mean error with 0.39 % failure rate on COFW dataset , and 1.25 % mean error on AFLW - Full dataset .",abstract,abstract,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.8181818181818182,11,0.0385964912280701,9,0.8181818181818182,1,research-problem,abstract,face_alignment5
1738,30,"In this work , we represent facial structure using 13 boundary lines .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",15,0.4166666666666667,29,0.1017543859649122,15,0.4166666666666667,1,dataset,Introduction,face_alignment5
1739,31,"Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets , which will not suffer from inconsistency of the annotation schemes .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",16,0.4444444444444444,30,0.1052631578947368,16,0.4444444444444444,1,model,Introduction,face_alignment5
1740,32,Our boundary - aware face alignment algorithm contains two stages .,Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",17,0.4722222222222222,31,0.1087719298245614,17,0.4722222222222222,1,model,Introduction,face_alignment5
1741,33,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",18,0.5,32,0.1122807017543859,18,0.5,1,model,Introduction,face_alignment5
1742,35,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.5555555555555556,34,0.119298245614035,20,0.5555555555555556,1,model,Introduction,face_alignment5
1743,37,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.6111111111111112,36,0.1263157894736842,22,0.6111111111111112,1,model,Introduction,face_alignment5
1744,39,"After generating facial boundary heatmaps , the next step is deriving facial landmarks using boundaries .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",24,0.6666666666666666,38,0.1333333333333333,24,0.6666666666666666,1,model,Introduction,face_alignment5
1745,43,"To fully utilise the structure information , we apply boundary heatmaps at multiple stages in the landmark regression network .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",28,0.7777777777777778,42,0.1473684210526315,28,0.7777777777777778,1,model,Introduction,face_alignment5
1746,50,Each image is annotated with 98 landmarks and 6 attributes .,Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",35,0.9722222222222222,49,0.1719298245614035,35,0.9722222222222222,1,dataset,Introduction,face_alignment5
1747,207,AFLW dataset : AFLW contains 24386 in - the - wild faces with large head pose up to 120 for yaw and 90 for pitch and roll .,Experiments,Datesets .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",10,0.5555555555555556,206,0.7228070175438597,10,0.3448275862068966,1,tasks,Experiments: Datesets .,face_alignment5
1748,224,All our models are trained with Caffe [ 24 ] on 4 Titan X GPUs .,Evaluation metric .,Evaluation metric .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.4705882352941176,223,0.7824561403508772,27,0.9310344827586208,1,experimental-setup,Evaluation metric .,face_alignment5
1749,231,Our method performs best among all of the state - of - the - art methods .,Evaluation metric .,Comparison with existing approaches 4.1.1 Evaluation on 300W,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.8823529411764706,230,0.8070175438596491,4,0.6666666666666666,1,results,Evaluation metric .: Comparison with existing approaches 4.1.1 Evaluation on 300W,face_alignment5
1750,240,shows the CED curves of our method against state - of - the - art methods on the COFW - 68 dataset .,Evaluation on WFLW,Cross - dataset evaluation on COFW and AFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.3529411764705882,239,0.8385964912280702,6,0.3529411764705882,1,results,Evaluation on WFLW: Cross - dataset evaluation on COFW and AFLW,face_alignment5
1751,241,Our model outperforms previous results with a large margin .,Evaluation on WFLW,Cross - dataset evaluation on COFW and AFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",7,0.4117647058823529,240,0.8421052631578947,7,0.4117647058823529,1,results,Evaluation on WFLW: Cross - dataset evaluation on COFW and AFLW,face_alignment5
1752,242,We achieve 4.62 % mean error with 2.17 % failure rate .,Evaluation on WFLW,Cross - dataset evaluation on COFW and AFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.4705882352941176,241,0.8456140350877193,8,0.4705882352941176,1,results,Evaluation on WFLW: Cross - dataset evaluation on COFW and AFLW,face_alignment5
1753,243,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .",Evaluation on WFLW,Cross - dataset evaluation on COFW and AFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5294117647058824,242,0.8491228070175438,9,0.5294117647058824,1,results,Evaluation on WFLW: Cross - dataset evaluation on COFW and AFLW,face_alignment5
1754,247,There is a clear boost between our method without and with using boundary information .,Evaluation on WFLW,The results are reported in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",13,0.7647058823529411,246,0.8631578947368421,13,0.7647058823529411,1,results,Evaluation on WFLW: The results are reported in .,face_alignment5
1755,249,"Moreover , our method uses boundary information achieves 29 % , 32 % and 29 % relative performance improve- ment over the baseline method ( "" LAB without boundary "" ) on COFW - 29 , AFLW - Full and AFLW - Frontal respectively .",Evaluation on WFLW,The results are reported in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",15,0.8823529411764706,248,0.8701754385964913,15,0.8823529411764706,1,results,Evaluation on WFLW: The results are reported in .,face_alignment5
1756,253,"Our framework consists of several pivotal components , i.e. , boundary information fusion , message passing and adversarial learning .",Ablation study,Ablation study,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",1,0.0769230769230769,252,0.8842105263157894,1,0.0769230769230769,1,ablation-analysis,Ablation study,face_alignment5
1757,263,"As indicated in , our final model that fuses boundary information in all four levels improves mean error from 7.12 % to 6.13 % .",Ablation study,The overall results are shown in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.8461538461538461,262,0.9192982456140352,11,0.8461538461538461,1,ablation-analysis,Ablation study: The overall results are shown in .,face_alignment5
1758,281,"In this paper , we present a novel use of facial boundary to derive facial landmarks .",Method,Unconstrained face alignment is an emerging topic .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",15,0.7894736842105263,280,0.9824561403508772,2,0.3333333333333333,1,model,Method: Unconstrained face alignment is an emerging topic .,face_alignment5
1759,285,The runtime of our algorithm is 60 ms on TITAN X GPU .,Method,Unconstrained face alignment is an emerging topic .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",19,1.0,284,0.9964912280701754,6,1.0,1,model,Method: Unconstrained face alignment is an emerging topic .,face_alignment5
1760,2,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees,title,title,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0028490028490028,1,0.0,1,research-problem,title,face_alignment6
1761,6,"In this paper we present 3DDE , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ensembles of regression trees .",abstract,abstract,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.3333333333333333,5,0.0142450142450142,3,0.3333333333333333,1,research-problem,abstract,face_alignment6
1762,11,"In the experiments performed , 3 DDE improves the state - of - the - art in 300W , COFW , AFLW and WFLW data sets .",abstract,abstract,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.8888888888888888,10,0.0284900284900284,8,0.8888888888888888,1,research-problem,abstract,face_alignment6
1763,23,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-n', 'O']",10,0.4347826086956521,22,0.0626780626780626,10,0.4347826086956521,1,model,Introduction,face_alignment6
1764,24,"It is a hybrid approach that inherits good properties of ERT , such as the ability to impose a face shape prior , and the robustness of deep models .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O']",11,0.4782608695652174,23,0.0655270655270655,11,0.4782608695652174,1,model,Introduction,face_alignment6
1765,25,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",12,0.5217391304347826,24,0.0683760683760683,12,0.5217391304347826,1,model,Introduction,face_alignment6
1766,26,"With this initialization we tackle one of the main drawbacks of ERT , namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.5652173913043478,25,0.0712250712250712,13,0.5652173913043478,1,model,Introduction,face_alignment6
1767,27,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",14,0.6086956521739131,26,0.074074074074074,14,0.6086956521739131,1,model,Introduction,face_alignment6
1768,28,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.6521739130434783,27,0.0769230769230769,15,0.6521739130434783,1,model,Introduction,face_alignment6
1769,31,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,Introduction,A preliminary version of our work appeared in .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']",18,0.782608695652174,30,0.0854700854700854,18,0.782608695652174,1,model,Introduction: A preliminary version of our work appeared in .,face_alignment6
1770,207,"For each data set , we train from scratch the CNN selecting the model parameters with lowest validation error .",Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.0869565217391304,206,0.5868945868945868,2,0.0869565217391304,1,experimental-setup,Implementation,face_alignment6
1771,208,We crop faces using the ground truth bounding boxes annotations enlarged by 30 % .,Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",3,0.1304347826086956,207,0.5897435897435898,3,0.1304347826086956,1,experimental-setup,Implementation,face_alignment6
1772,209,"We generate different training samples in each epoch by applying random in plane rotations between 45 , scale changes by 15 % and translations by 5 % of bounding box size , randomly mirroring images horizontally and generating random rectangular occlusions .",Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.1739130434782608,208,0.5925925925925926,4,0.1739130434782608,1,experimental-setup,Implementation,face_alignment6
1773,210,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .",Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.217391304347826,209,0.5954415954415955,5,0.217391304347826,1,experimental-setup,Implementation,face_alignment6
1774,211,We train until convergence with an initial learning rate ? = 0.001 .,Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.2608695652173913,210,0.5982905982905983,6,0.2608695652173913,1,experimental-setup,Implementation,face_alignment6
1775,212,"When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .",Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",7,0.3043478260869565,211,0.6011396011396012,7,0.3043478260869565,1,experimental-setup,Implementation,face_alignment6
1776,213,In the CNN the cropped input face is reduced from 160160 to 11 pixels gradually dividing by half their size across B = 8 branches applying astride 2 convolution with kernel size 22 1 .,Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']",8,0.3478260869565217,212,0.603988603988604,8,0.3478260869565217,1,experimental-setup,Implementation,face_alignment6
1777,214,We apply batch normalization after each convolution .,Implementation,,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",9,0.391304347826087,213,0.6068376068376068,9,0.391304347826087,1,experimental-setup,Implementation,face_alignment6
1778,216,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .",Implementation,We apply batch normalization after each convolution .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O']",11,0.4782608695652174,215,0.6125356125356125,11,0.4782608695652174,1,experimental-setup,Implementation: We apply batch normalization after each convolution .,face_alignment6
1779,219,The depth of trees is set to 4 .,Implementation,,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",14,0.6086956521739131,218,0.6210826210826211,14,0.6086956521739131,1,experimental-setup,Implementation,face_alignment6
1780,220,"The number of tests to choose the best split parameters , ? , is set to 200 .",Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",15,0.6521739130434783,219,0.6239316239316239,15,0.6521739130434783,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1781,221,We resize each image to set the face size to 160160 pixels .,Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",16,0.6956521739130435,220,0.6267806267806267,16,0.6956521739130435,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1782,222,"For feature extraction , the FREAK pattern diameter is reduced gradually in each stage ( i.e. , in the last stages the pixel pairs for each feature are closer ) .",Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7391304347826086,221,0.6296296296296297,17,0.7391304347826086,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1783,223,We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.782608695652174,222,0.6324786324786325,18,0.782608695652174,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1784,225,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",20,0.8695652173913043,224,0.6381766381766382,20,0.8695652173913043,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1785,227,"Training the CNN and the coarse - to - fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20 GHz ( 210 cores / 20 threads , 128 GB of RAM ) with a batch size of 32 images .",Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O']",22,0.9565217391304348,226,0.6438746438746439,22,0.9565217391304348,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1786,236,"The selected algorithms are representative of the three main families of solutions : a ) ensembles of regression trees ( c GPRT , RCPR , ERT ) , b) CNN - based approaches ( LAB , DAN , RCN ) and c ) mixed approaches with deep nets and ensembles of regression trees ( 3DDE , DCFE ) .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'I-n', 'O', 'O']",7,0.5384615384615384,235,0.6695156695156695,7,0.5384615384615384,1,baselines,Experiments using public code,face_alignment6
1787,237,"Overall , 3 DDE is better than any other providing a public implementation in the literature .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",8,0.6153846153846154,236,0.6723646723646723,8,0.6153846153846154,1,results,Experiments using public code,face_alignment6
1788,239,"In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.7692307692307693,238,0.6780626780626781,10,0.7692307692307693,1,results,Experiments using public code,face_alignment6
1789,240,"We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-n', 'O', 'B-p', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.8461538461538461,239,0.6809116809116809,11,0.8461538461538461,1,results,Experiments using public code,face_alignment6
1790,247,Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition ( see ) and in the full subset of the 300W public test set ( see ) .,Experiments using published results,Experiments using published results,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",4,0.0975609756097561,246,0.7008547008547008,4,0.0975609756097561,1,results,Experiments using published results,face_alignment6
1791,249,"In the challenging subset of the 300W public competition , SHN gets better results than 3DDE .",Experiments using published results,Experiments using published results,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.1463414634146341,248,0.7065527065527065,6,0.1463414634146341,1,results,Experiments using published results,face_alignment6
1792,255,"3 DDE obtains the best results , NME 5.11 , establishing anew state - of - the - art .",Experiments using published results,This is the standard to evaluate occlusions .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.2926829268292683,254,0.7236467236467237,12,0.2926829268292683,1,results,Experiments using published results: This is the standard to evaluate occlusions .,face_alignment6
1793,266,"In terms of landmark visibility estimation , we have obtained better precision with an overall better recall than the best previous approach , DCFE .",Experiments using published results,This is the standard to evaluate occlusions .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",23,0.5609756097560976,265,0.7549857549857549,23,0.5609756097560976,1,results,Experiments using published results: This is the standard to evaluate occlusions .,face_alignment6
1794,267,"Again , the regularization together with the new initialization contributes to improve DCFE .",Experiments using published results,This is the standard to evaluate occlusions .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",24,0.5853658536585366,266,0.7578347578347578,24,0.5853658536585366,1,experiments,Experiments using published results: This is the standard to evaluate occlusions .,face_alignment6
1795,270,"Although the results in are not strictly comparable , because each paper uses its own train and test subsets , we get an NME of 2.06 with the full 21 landmarks set .",Experiments using published results,This is the standard to evaluate occlusions .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",27,0.6585365853658537,269,0.7663817663817664,27,0.6585365853658537,1,results,Experiments using published results: This is the standard to evaluate occlusions .,face_alignment6
1796,277,3DDE outperforms its competitors in all the WFLW subsets by a large margin .,Experiments using published results,This is the standard to evaluate occlusions .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",34,0.8292682926829268,276,0.7863247863247863,34,0.8292682926829268,1,results,Experiments using published results: This is the standard to evaluate occlusions .,face_alignment6
1797,286,"3DDE is based on three key ideas : 3D initialization , a cascaded ERT regressor operating on probabilistic CNN features and a coarse - to - fine scheme .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'O', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0476190476190476,285,0.811965811965812,1,0.0476190476190476,1,ablation-analysis,Ablation study,face_alignment6
1798,288,In we show the results obtained by different configurations of our framework when evaluated on WFLW .,Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",3,0.1428571428571428,287,0.8176638176638177,3,0.1428571428571428,1,ablation-analysis,Ablation study,face_alignment6
1799,296,"When combined with the cascaded ERT , the 3D initialization is key to achieve top overall performance , see CNN + MS + DE vs CNN + 3D + DE in the full subset .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",11,0.5238095238095238,295,0.8404558404558404,11,0.5238095238095238,1,ablation-analysis,Ablation study,face_alignment6
1800,300,"So , it provides the largest improvement in the pose subset .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",15,0.7142857142857143,299,0.8518518518518519,15,0.7142857142857143,1,ablation-analysis,Ablation study,face_alignment6
1801,301,The use of CNN probability maps improves the NME in the full data set in about 20 % ( see CNN+ 3D + SE vs CNN + 3D + DE ) .,Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.7619047619047619,300,0.8547008547008547,16,0.7619047619047619,1,ablation-analysis,Ablation study,face_alignment6
1802,303,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",18,0.8571428571428571,302,0.8603988603988604,18,0.8571428571428571,1,ablation-analysis,Ablation study,face_alignment6
1803,322,"The smallest database , COFW , has the worst cross - dataset results .",Cross - dataset evaluation,In we show the results of our evaluation .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.5333333333333333,321,0.9145299145299144,15,0.6818181818181818,1,results,Cross - dataset evaluation: In we show the results of our evaluation .,face_alignment6
1804,323,"On the other hand , the data set with greatest diversity , WFLW , has the best results .",Cross - dataset evaluation,In we show the results of our evaluation .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",9,0.6,322,0.9173789173789174,16,0.7272727272727273,1,results,Cross - dataset evaluation: In we show the results of our evaluation .,face_alignment6
1805,324,"Moreover , the model All , trained with the training sets of all data bases , is able to improve , in all cross - dataset experiments , the models trained in a single data set .",Cross - dataset evaluation,In we show the results of our evaluation .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",10,0.6666666666666666,323,0.9202279202279202,17,0.7727272727272727,1,results,Cross - dataset evaluation: In we show the results of our evaluation .,face_alignment6
1806,329,"The landmarks with highest NME are those related to the ears , the bottom of the mouth and the chin .",Cross - dataset evaluation,Ina final experiment we use model,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O']",15,1.0,328,0.9344729344729344,22,1.0,1,results,Cross - dataset evaluation: Ina final experiment we use model,face_alignment6
1807,2,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,title,title,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0046728971962616,1,0.0,1,research-problem,title,face_alignment7
1808,10,Code is available at https://github.com/YadiraF/PRNet.,abstract,abstract,face_alignment,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n']",7,1.0,9,0.0420560747663551,7,1.0,1,code,abstract,face_alignment7
1809,12,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .,Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0344827586206896,11,0.0514018691588785,1,0.0344827586206896,1,research-problem,Introduction,face_alignment7
1810,21,"trains a complex network to regress 68 facial landmarks with 2D coordinates from a single image , but needs an extra network to estimate the depth value .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3448275862068966,20,0.0934579439252336,10,0.3448275862068966,1,model,Introduction,face_alignment7
1811,27,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.5517241379310345,26,0.1214953271028037,16,0.5517241379310345,1,model,Introduction,face_alignment7
1812,29,"Meanwhile , our method is straightforward with a very light - weighted model which provides the result in one pass with 9.8 ms .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.6206896551724138,28,0.1308411214953271,18,0.6206896551724138,1,model,Introduction,face_alignment7
1813,31,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",20,0.6896551724137931,30,0.1401869158878504,20,0.6896551724137931,1,model,Introduction,face_alignment7
1814,32,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.7241379310344828,31,0.1448598130841121,21,0.7241379310344828,1,model,Introduction,face_alignment7
1815,33,"Figure1 shows our method is robust to poses , illuminations and occlusions .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.7586206896551724,32,0.1495327102803738,22,0.7586206896551724,1,experiments,Introduction,face_alignment7
1816,36,"- To directly regress the 3D facial structure and dense alignment , we develop a novel representation called UV position map , which records the position information of 3 D face and provides dense correspondence to the semantic meaning of each point on UV space .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",25,0.8620689655172413,35,0.1635514018691588,25,0.8620689655172413,1,model,Introduction,face_alignment7
1817,37,"- For training , we proposed a weight mask which assigns different weight to each point on position map and compute a weighted loss .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",26,0.896551724137931,36,0.1682242990654205,26,0.896551724137931,1,model,Introduction,face_alignment7
1818,39,- We finally provide a light - weighted framework that runs at over 100 FPS to directly obtain 3 D face reconstruction and alignment result from a single 2 D facial image .,Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.9655172413793104,38,0.1775700934579439,28,0.9655172413793104,1,model,Introduction,face_alignment7
1819,142,"Like , we also augment our training data by scaling color channels with scale range from 0.6 to 1.4 .",Training Details,Training Details,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-n', 'O']",7,0.5384615384615384,141,0.6588785046728972,7,0.5384615384615384,1,experimental-setup,Training Details,face_alignment7
1820,146,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .",Training Details,Training Details,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",11,0.8461538461538461,145,0.677570093457944,11,0.8461538461538461,1,experimental-setup,Training Details,face_alignment7
1821,147,The batch size is set as 16 .,Training Details,Training Details,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",12,0.9230769230769232,146,0.6822429906542056,12,0.9230769230769232,1,experimental-setup,Training Details,face_alignment7
1822,148,All of our training codes are implemented with TensorFlow .,Training Details,Training Details,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",13,1.0,147,0.6869158878504673,13,1.0,1,experimental-setup,Training Details,face_alignment7
1823,207,Network trained without using weight mask has worst performance compared with other two settings .,Ablation Study,Ablation Study,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.8,206,0.9626168224299064,4,0.8,1,ablation-analysis,Ablation Study,face_alignment7
1824,208,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .",Ablation Study,Ablation Study,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,1.0,207,0.9672897196261684,5,1.0,1,ablation-analysis,Ablation Study,face_alignment7
1825,2,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning,title,title,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0035211267605633,1,0.0,1,research-problem,title,face_alignment8
1826,9,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,abstract,Best viewed in color .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3333333333333333,8,0.028169014084507,6,0.3333333333333333,1,research-problem,abstract: Best viewed in color .,face_alignment8
1827,13,"Specifically , taking the sparse 2 D facial landmarks as additional information , 2 DSAL introduces four novel self - supervision schemes that view the 2D landmark and 3D landmark prediction as a self - mapping process , including the 2D and 3D landmark self - prediction consistency , cycle - consistency over the 2D landmark prediction and self - critic over the predicted 3 DMM coefficients based on landmark predictions .",abstract,Best viewed in color .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",10,0.5555555555555556,12,0.0422535211267605,10,0.5555555555555556,1,model,abstract: Best viewed in color .,face_alignment8
1828,14,"Using these four self - supervision schemes , the 2DASL method significantly relieves demands on the the conventional paired 2D - to - 3D annotations and gives much higher - quality 3 D face models without requiring any additional 3D annotations .",abstract,Best viewed in color .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",11,0.6111111111111112,13,0.0457746478873239,11,0.6111111111111112,1,research-problem,abstract: Best viewed in color .,face_alignment8
1829,23,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,Abstract,Abstract,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,22,0.0774647887323943,1,0.1428571428571428,1,research-problem,Abstract,face_alignment8
1830,31,3 D face reconstruction is an important task in the field of computer vision and graphics .,Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.03125,30,0.1056338028169014,1,0.03125,1,research-problem,Introduction,face_alignment8
1831,41,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.34375,40,0.1408450704225352,11,0.34375,1,model,Introduction,face_alignment8
1832,45,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.46875,44,0.1549295774647887,15,0.46875,1,model,Introduction,face_alignment8
1833,49,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.59375,48,0.1690140845070422,19,0.59375,1,model,Introduction,face_alignment8
1834,51,"To facilitate the overall learning procedure , our method also exploits self - critic learning .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.65625,50,0.176056338028169,21,0.65625,1,model,Introduction,face_alignment8
1835,52,"It takes as input both the latent representation and 3 DMM coefficients of an face image and learns a critic model to evaluate the intrinsic consistency between the predicted 3 DMM coefficients and the corresponding face image , offering another supervision for 3 D face model learning .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.6875,51,0.1795774647887324,22,0.6875,1,model,Introduction,face_alignment8
1836,57,"We propose anew scheme that aims to fully utilize the abundant "" in - the - wild "" 2 D face images to assist 3 D face model learning .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",27,0.84375,56,0.1971830985915492,27,0.84375,1,model,Introduction,face_alignment8
1837,59,We introduce anew method that is able to train 3 D face models with 2 D face images by self - supervised learning .,Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",29,0.90625,58,0.204225352112676,29,0.90625,1,model,Introduction,face_alignment8
1838,61,"We develop anew self - critic learning based approach which could effectively improve the 3D face model learning procedure and give a better model , even though the 2D landmark annotations are noisy .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.96875,60,0.2112676056338028,31,0.96875,1,model,Introduction,face_alignment8
1839,154,2D assisted self - supervised learning,Model overview,Best viewed in color .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",36,0.4337349397590361,153,0.5387323943661971,36,0.4337349397590361,1,research-problem,Model overview: Best viewed in color .,face_alignment8
1840,170,"Since the contour landmarks of a 2 D face are inaccurate to represent the corresponding points of 3 D face , we discard them and sample 18 landmarks from the 68 2D facial landmarks .",Model overview,Best viewed in color .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",52,0.6265060240963856,169,0.5950704225352113,52,0.6265060240963856,1,experiments,Model overview: Best viewed in color .,face_alignment8
1841,205,Our proposed 2 DASL is implemented with Pytorch .,Training details and datasets,,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",1,0.0188679245283018,204,0.7183098591549296,1,0.0454545454545454,1,experimental-setup,Training details and datasets,face_alignment8
1842,206,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .",Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0377358490566037,205,0.721830985915493,2,0.0909090909090909,1,experimental-setup,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1843,211,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .",Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",7,0.1320754716981132,210,0.7394366197183099,7,0.3181818181818182,1,experimental-setup,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1844,215,The 2D facial landmarks of all the face images are detected by an advanced 2 D facial landmarks detector .,Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.2075471698113207,214,0.7535211267605634,11,0.5,1,model,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1845,218,Each face is annotated with its corresponding 3 DMM coefficients and the 68 3D facial landmarks .,Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.2641509433962264,217,0.7640845070422535,14,0.6363636363636364,1,experimental-setup,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1846,223,Each image is annotated with 34 facial landmarks .,Training details and datasets,,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",19,0.3584905660377358,222,0.7816901408450704,19,0.8636363636363636,1,experimental-setup,Training details and datasets,face_alignment8
1847,230,"As can be seen , our results are more accurate than the ground truth in some cases .",Training details and datasets,Dense face alignment,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",26,0.490566037735849,229,0.8063380281690141,3,0.1,1,experiments,Training details and datasets: Dense face alignment,face_alignment8
1848,239,"The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .",Training details and datasets,Dense face alignment,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'O']",35,0.660377358490566,238,0.8380281690140845,12,0.4,1,experiments,Training details and datasets: Dense face alignment,face_alignment8
1849,240,"For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .",Training details and datasets,Dense face alignment,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",36,0.6792452830188679,239,0.8415492957746479,13,0.4333333333333333,1,experiments,Training details and datasets: Dense face alignment,face_alignment8
1850,246,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",42,0.7924528301886793,245,0.8626760563380281,19,0.6333333333333333,1,experiments,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1851,252,"Following , we first employ the Iterative Closest Points ( ICP ) algorithm to find the corresponding nearest points between the reconstructed 3 D face and the ground truth point cloud .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",48,0.9056603773584906,251,0.8838028169014085,25,0.8333333333333334,1,experiments,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1852,255,"As can be seen , the 3D reconstruction results of 2 DASL outperforms 3 DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'I-n', 'O']",51,0.9622641509433962,254,0.8943661971830986,28,0.9333333333333332,1,experiments,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1853,257,"As can be seen , the reconstructed shape of our 2 DASL are more smooth , however , both PRNet and VRN - Guided introduce some artifacts into the reconstructed results , which makes the reconstructed faces look unnaturally .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",53,1.0,256,0.9014084507042254,30,1.0,1,experiments,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1854,260,"( 2 ) 2DASL ( cyc ) , which takes as input the combination of RGB face images and the corresponding 2D FLMs with self - supervison , however without self - critic supervision ; ( 3 ) 2DASL ( sc ) , which takes as input the RGB face images only using self - critic learning .",Ablation study,Ablation study,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.1,259,0.9119718309859156,2,0.1,1,baselines,Ablation study,face_alignment8
1855,261,"( 4 ) 2DASL ( cyc+sc ) , which contains both self - supervision and self - critic supervision .",Ablation study,Ablation study,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.15,260,0.9154929577464788,3,0.15,1,baselines,Ablation study,face_alignment8
1856,265,"2 . Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages , respectively .",Ablation study,The ablation study results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",7,0.35,264,0.9295774647887324,7,0.35,1,ablation-analysis,Ablation study: The ablation study results are shown in Tab .,face_alignment8
1857,267,"If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .",Ablation study,The ablation study results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",9,0.45,266,0.9366197183098592,9,0.45,1,ablation-analysis,Ablation study: The ablation study results are shown in Tab .,face_alignment8
1858,269,The best result is achieved when both these two modules are used .,Ablation study,The ablation study results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O']",11,0.55,268,0.943661971830986,11,0.55,1,ablation-analysis,Ablation study: The ablation study results are shown in Tab .,face_alignment8
1859,2,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection,title,title,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0033898305084745,1,0.0,1,research-problem,title,face_alignment9
1860,12,"In addition , to recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) to correct outliers by considering the global face shape as a constraint .",abstract,abstract,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",9,0.9,11,0.0372881355932203,9,0.9,1,model,abstract,face_alignment9
1861,37,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .",Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O']",23,0.9583333333333334,36,0.1220338983050847,20,0.9523809523809524,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1862,181,"1 . The GHCU implicitly learns the whole face shape constraint from the training data and always gives facialshape landmarks , as shown in .",Network likelihood model,Global heatmap correction unit,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",58,0.9830508474576272,180,0.6101694915254238,9,0.9,1,experiments,Network likelihood model: Global heatmap correction unit,face_alignment9
1863,201,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.1428571428571428,200,0.6779661016949152,18,0.6,1,experimental-setup,Implementation Details .,face_alignment9
1864,202,We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",3,0.2142857142857142,201,0.6813559322033899,19,0.6333333333333333,1,experimental-setup,Implementation Details .,face_alignment9
1865,203,"As described in Section 4 , our algorithm comprises two parts : network training and real groundtruth searching , which are alternatively optimized .",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",4,0.2857142857142857,202,0.6847457627118644,20,0.6666666666666666,1,ablation-analysis,Implementation Details .,face_alignment9
1866,204,"Specifically , at each epoch , we first search the real ground - trut ?",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']",5,0.3571428571428571,203,0.688135593220339,21,0.7,1,ablation-analysis,Implementation Details .,face_alignment9
1867,207,"When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",8,0.5714285714285714,206,0.6983050847457627,24,0.8,1,experimental-setup,Implementation Details .,face_alignment9
1868,208,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",9,0.6428571428571429,207,0.7016949152542373,25,0.8333333333333334,1,experimental-setup,Implementation Details .,face_alignment9
1869,211,We set batch size to 10 for network training .,Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",12,0.8571428571428571,210,0.711864406779661,28,0.9333333333333332,1,experimental-setup,Implementation Details .,face_alignment9
1870,213,1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .,Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",14,1.0,212,0.7186440677966102,30,1.0,1,experimental-setup,Implementation Details .,face_alignment9
1871,218,"2 ) uses the hourglass architecture with human annotations , which is actually the traditional landmark detector training .",Comparison experiment,Comparison experiment,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O']",4,0.1379310344827586,217,0.735593220338983,4,0.1379310344827586,1,baselines,Comparison experiment,face_alignment9
1872,220,"2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2068965517241379,219,0.7423728813559322,6,0.2068965517241379,1,results,Comparison experiment: From Tab.,face_alignment9
1873,221,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.2413793103448276,220,0.7457627118644068,7,0.2413793103448276,1,results,Comparison experiment: From Tab.,face_alignment9
1874,223,"Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.3103448275862069,222,0.752542372881356,9,0.3103448275862069,1,results,Comparison experiment: From Tab.,face_alignment9
1875,224,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3448275862068966,223,0.7559322033898305,10,0.3448275862068966,1,results,Comparison experiment: From Tab.,face_alignment9
1876,233,It is also observed that HGs + SA + GHCU works better than HGs + SA .,Comparison experiment,"HGs , 1.62 % vs 1.95 % .",face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",19,0.6551724137931034,232,0.7864406779661017,19,0.6551724137931034,1,results,"Comparison experiment: HGs , 1.62 % vs 1.95 % .",face_alignment9
1877,236,The subset Category 3 is the most challenging one .,Comparison experiment,"HGs , 1.62 % vs 1.95 % .",face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",22,0.7586206896551724,235,0.7966101694915254,22,0.7586206896551724,1,results,"Comparison experiment: HGs , 1.62 % vs 1.95 % .",face_alignment9
1878,238,"4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O']",24,0.8275862068965517,237,0.8033898305084746,24,0.8275862068965517,1,results,Comparison experiment: From Tab.,face_alignment9
1879,239,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.8620689655172413,238,0.8067796610169492,25,0.8620689655172413,1,results,Comparison experiment: From Tab.,face_alignment9
1880,240,Comparison with state of the art on AFLW dataset .,Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",26,0.896551724137931,239,0.8101694915254237,26,0.896551724137931,1,results,Comparison experiment: From Tab.,face_alignment9
1881,243,"GHCU considers the global face shape as constraint , being robust to such challenging factors .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,1.0,242,0.8203389830508474,29,1.0,1,baselines,Comparison experiment: From Tab.,face_alignment9
1882,285,"As shown in Tab. 8 , our CNN based GHCU outperforms PCA based method in terms of both accuracy and efficiency .",Self evaluations,Global heatmap correction unit .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",41,1.0,284,0.9627118644067796,41,0.8723404255319149,1,results,Self evaluations: Global heatmap correction unit .,face_alignment9
1883,290,"9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .",Ablation study .,As shown in Tab .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,289,0.9796610169491524,46,0.9787234042553192,1,ablation-analysis,Ablation study .: As shown in Tab .,face_alignment9
1884,291,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .",Ablation study .,As shown in Tab .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,1.0,290,0.9830508474576272,47,1.0,1,ablation-analysis,Ablation study .: As shown in Tab .,face_alignment9
1885,2,Accurate Face Detection for High Performance,title,,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0067567567567567,1,0.0,1,research-problem,title,face_detection0
1886,4,Face detection has witnessed significant progress due to the advances of deep convolutional neural networks ( CNNs ) .,abstract,abstract,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.0202702702702702,1,0.1666666666666666,1,research-problem,abstract,face_detection0
1887,11,"Face detection is a tremendously important field in computer vision needed for face recognition , sentiment analysis , video surveillance , and many other fields .",Introduction,Introduction,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,10,0.0675675675675675,1,0.0625,1,research-problem,Introduction,face_detection0
1888,23,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .",Introduction,Introduction,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",13,0.8125,22,0.1486486486486486,13,0.8125,1,model,Introduction,face_detection0
1889,24,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,Introduction,Introduction,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",14,0.875,23,0.1554054054054054,14,0.875,1,model,Introduction,face_detection0
1890,25,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,Introduction,Introduction,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",15,0.9375,24,0.1621621621621621,15,0.9375,1,model,Introduction,face_detection0
1891,74,The focal loss is the reshaping of cross entropy loss such that it down - weights the loss assigned to well - classified examples .,RetinaNet Baseline,RetinaNet Baseline,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.2040816326530612,73,0.4932432432432432,10,0.4347826086956521,1,baselines,RetinaNet Baseline,face_detection0
1892,93,STR performs is two - step regression on three high level detection layers to adjust anchors and provide better initialization for the subsequent regressor .,RetinaNet Baseline,Selective Refinement Network,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",29,0.5918367346938775,92,0.6216216216216216,5,0.5555555555555556,1,baselines,RetinaNet Baseline: Selective Refinement Network,face_detection0
1893,134,The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset .,Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.7037037037037037,133,0.8986486486486487,1,0.1111111111111111,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1894,135,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.7407407407407407,134,0.9054054054054054,2,0.2222222222222222,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1895,136,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",21,0.7777777777777778,135,0.9121621621621622,3,0.3333333333333333,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1896,137,The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",22,0.8148148148148148,136,0.918918918918919,4,0.4444444444444444,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1897,138,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",23,0.8518518518518519,137,0.9256756756756755,5,0.5555555555555556,1,experiments,Experimental Dataset: Optimization Detail,face_detection0
1898,139,The full training and testing codes are built on the PyTorch library .,Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",24,0.8888888888888888,138,0.9324324324324323,6,0.6666666666666666,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1899,141,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",26,0.9629629629629628,140,0.945945945945946,8,0.8888888888888888,1,results,Experimental Dataset: Optimization Detail,face_detection0
1900,2,EXTD : Extremely Tiny Face Detector via Iterative Filter Reuse,title,title,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0043290043290043,1,0.0,1,research-problem,title,face_detection1
1901,4,"In this paper , we propose a new multi-scale face detector having an extremely tiny number of parameters ( EXTD ) , less than 0.1 million , as well as achieving comparable performance to deep heavy detectors .",abstract,abstract,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.2,3,0.0129870129870129,1,0.2,1,research-problem,abstract,face_detection1
1902,28,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.5757575757575758,27,0.1168831168831168,19,0.5757575757575758,1,model,Introduction,face_detection1
1903,29,"The main discovery is that we can share the network in generating each feature - map , as shown in .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",20,0.6060606060606061,28,0.1212121212121212,20,0.6060606060606061,1,model,Introduction,face_detection1
1904,37,"We note that our model does not require any extra layer commonly defined as in , and is trained from scratch .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",28,0.8484848484848485,36,0.1558441558441558,28,0.8484848484848485,1,model,Introduction,face_detection1
1905,40,"We propose an iterative network sharing model for multi-stage face detection which can significantly reduce the parameter size , as well as provide abundant object semantic information to the lower stage feature maps .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",31,0.9393939393939394,39,0.1688311688311688,31,0.9393939393939394,1,model,Introduction,face_detection1
1906,135,"Using the hard negative mining technique , we balance the ratio of positive and negative samples N neg / N pos to 3 and the balancing parameter ?",Training,Training,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']",13,0.65,134,0.5800865800865801,13,0.65,1,experimental-setup,Training,face_detection1
1907,136,is set to 4 .,Training,Training,face_detection,1,"['O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'O']",14,0.7,135,0.5844155844155844,14,0.7,1,experimental-setup,Training,face_detection1
1908,140,The proposed method is implemented with PyTorch and NAVER Smart Machine Learning ( NSML ) system .,Training,Training,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.9,139,0.6017316017316018,18,0.9,1,experimental-setup,Training,face_detection1
1909,142,Code will be available at https ://github.com/clovaai.,Training,Training,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",20,1.0,141,0.6103896103896104,20,1.0,1,code,Training,face_detection1
1910,159,"For the model , we designed three variations which have a different number of parameters , lighter one having 0.063 M parameters with 32 channels for each feature maps , intermediate one having 0.1 M parameters with 48 channels , and the heavier one with 64 channels and 0.16M parameters when designed as FPN .",Experimental Setting,Experimental Setting,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'O', 'B-n', 'O']",12,0.1666666666666666,158,0.683982683982684,12,0.5714285714285714,1,experimental-setup,Experimental Setting,face_detection1
1911,163,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .",Experimental Setting,See Appendix,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",16,0.2222222222222222,162,0.7012987012987013,16,0.7619047619047619,1,experimental-setup,Experimental Setting: See Appendix,face_detection1
1912,176,"For a fair comparison , all the inference processes of the models are implemented by PyTorch 1.0 .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",29,0.4027777777777778,175,0.7575757575757576,7,0.2333333333333333,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1913,177,Comparison to the Existing Methods :,Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",30,0.4166666666666667,176,0.7619047619047619,8,0.2666666666666666,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1914,179,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",32,0.4444444444444444,178,0.7705627705627706,10,0.3333333333333333,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1915,180,The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",33,0.4583333333333333,179,0.7748917748917749,11,0.3666666666666666,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1916,182,"The m AP gap to DSFD , which is tremendously heavier , is about 5.0 % , but it would be safe to suggest that the proposed method offers more decent trade - off in that DSFD uses about 2860 times more parameters than the proposed method .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",35,0.4861111111111111,181,0.7835497835497836,13,0.4333333333333333,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1917,186,"When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",39,0.5416666666666666,185,0.8008658008658008,17,0.5666666666666667,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1918,187,"However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",40,0.5555555555555556,186,0.8051948051948052,18,0.6,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1919,194,Detection performance regarding the Face Scale :,Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",47,0.6527777777777778,193,0.8354978354978355,25,0.8333333333333334,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1920,196,"From the table , we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",49,0.6805555555555556,195,0.8441558441558441,27,0.9,1,experiments,Experimental Setting: Performance Analysis,face_detection1
1921,204,"First , for all the different channel width , FPN based architecture achieved better detection performance compared to SSD based architecture , especially for detecting small faces .",Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",57,0.7916666666666666,203,0.8787878787878788,4,0.2105263157894736,1,experiments,Experimental Setting: Variation Analysis,face_detection1
1922,209,"As the channel width increased by 32 to 64 , we can see that the detection accuracy significantly enhanced for all the cases ; Easy , Medium , and Hard .",Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",62,0.8611111111111112,208,0.9004329004329005,9,0.4736842105263157,1,experiments,Experimental Setting: Variation Analysis,face_detection1
1923,213,"In all the cases including FPN based and SSD based structures , PReLU was the most effective choice when it comes to mAP , but the gap between Leaky - ReLU was not that significant for the FPN variants .",Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",66,0.9166666666666666,212,0.9177489177489178,13,0.6842105263157895,1,experiments,Experimental Setting: Variation Analysis,face_detection1
1924,214,"When tested with SSD based architecture , PReLU outperformed Leaky - ReLU with larger margin than those using FPN structure .",Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",67,0.9305555555555556,213,0.922077922077922,14,0.7368421052631579,1,experiments,Experimental Setting: Variation Analysis,face_detection1
1925,215,It is worth noting that ReLU occurred notable performance decreases especially when the channel width was small for both SSD and FPN cases .,Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",68,0.9444444444444444,214,0.9264069264069263,15,0.7894736842105263,1,experiments,Experimental Setting: Variation Analysis,face_detection1
1926,216,"When the channel width was set to 32 , m AP for all the three cases were lower than 10 % to 20 % compared to those using other activation functions .",Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",69,0.9583333333333334,215,0.9307359307359307,16,0.8421052631578947,1,experiments,Experimental Setting: Variation Analysis,face_detection1
1927,226,"For training the proposed architecture , a stochastic gradient descent optimizer ( SGD ) with learning rate 1e ? 3 , with 0.9 momentum , 0.0005 weight decay , and batch size 16 is used .",Appendix A. Implementation detail,Appendix A. Implementation detail,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.1666666666666666,225,0.974025974025974,1,0.1666666666666666,1,experimental-setup,Appendix A. Implementation detail,face_detection1
1928,228,"The maximum iteration number is basically set to 240K , and we drop the learning rate to 1e ? 4 and 1e ? 5 at 120 K and 180K iterations .",Appendix A. Implementation detail,Appendix A. Implementation detail,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",3,0.5,227,0.9826839826839828,3,0.5,1,experimental-setup,Appendix A. Implementation detail,face_detection1
1929,2,DSFD : Dual Shot Face Detector,title,,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0051546391752577,1,0.0,1,research-problem,title,face_detection10
1930,9,"Face detection is a fundamental step for various facial applications , like face alignment , parsing , recognition , and verification .",Introduction,Introduction,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0294117647058823,8,0.0412371134020618,1,0.0294117647058823,1,research-problem,Introduction,face_detection10
1931,13,The first one is mainly based on the Region Proposal Network ( RPN ) adopted in Faster RCNN and employs two stage detection schemes .,Introduction,Introduction,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.1470588235294117,12,0.0618556701030927,5,0.1470588235294117,1,model,Introduction,face_detection10
1932,14,RPN is trained end - to - end and generates highquality region proposals which are further refined by Fast R - CNN detector .,Introduction,Introduction,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.1764705882352941,13,0.0670103092783505,6,0.1764705882352941,1,model,Introduction,face_detection10
1933,15,"The other one is Single Shot Detector ( SSD ) based one - stage methods , which get rid of RPN , and directly predict the bounding boxes and confidence .",Introduction,Introduction,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'O']",7,0.2058823529411764,14,0.0721649484536082,7,0.2058823529411764,1,research-problem,Introduction,face_detection10
1934,127,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.0317460317460317,126,0.6494845360824743,2,0.1666666666666666,1,hyperparameters,Implementation Details,face_detection10
1935,128,All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.0476190476190476,127,0.654639175257732,3,0.25,1,hyperparameters,Implementation Details,face_detection10
1936,129,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .",Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.0634920634920634,128,0.6597938144329897,4,0.3333333333333333,1,hyperparameters,Implementation Details,face_detection10
1937,130,The batch size is set to 16 .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",5,0.0793650793650793,129,0.6649484536082474,5,0.4166666666666667,1,hyperparameters,Implementation Details,face_detection10
1938,131,"The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .",Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.0952380952380952,130,0.6701030927835051,6,0.5,1,hyperparameters,Implementation Details,face_detection10
1939,133,Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.1269841269841269,132,0.6804123711340206,8,0.6666666666666666,1,hyperparameters,Implementation Details,face_detection10
1940,135,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",10,0.1587301587301587,134,0.6907216494845361,10,0.8333333333333334,1,code,Implementation Details,face_detection10
1941,138,Analysis on DSFD,Implementation Details,,face_detection,10,"['O', 'O', 'O']","['O', 'O', 'B-n']",13,0.2063492063492063,137,0.7061855670103093,0,0.0,1,results,Implementation Details,face_detection10
1942,154,"Finally , we can improve our DSFD to 96.6 % , 95.7 % , 90.4 % with ResNet 152 as the backbone . Besides , shows that our improved anchor matching strategy greatly increases the number of ground truth faces that are closed to the anchor , which can reduce the contradiction between the discrete anchor scales and continuous face scales .",Implementation Details,Improved Anchor Matching,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4603174603174603,153,0.788659793814433,3,0.2,1,experiments,Implementation Details: Improved Anchor Matching,face_detection10
1943,156,Comparison with RFB,Implementation Details,,face_detection,10,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",31,0.492063492063492,155,0.7989690721649485,5,0.3333333333333333,1,results,Implementation Details,face_detection10
1944,164,"2 ) Auxiliary loss based on progressive anchor is used to train all 12 different scale detection feature maps , and it improves the performance on easy , medium and hard faces simultaneously .",Implementation Details,Comparison with RFB,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",39,0.6190476190476191,163,0.8402061855670103,13,0.8666666666666667,1,experiments,Implementation Details: Comparison with RFB,face_detection10
1945,165,"3 ) Our improved anchor matching provides better initial anchors and ground - truth faces to regress anchor from faces , which achieves the improvements of 0.3 % , 0.1 % , 0.3 % on three settings , respectively .",Implementation Details,Comparison with RFB,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.6349206349206349,164,0.845360824742268,14,0.9333333333333332,1,results,Implementation Details: Comparison with RFB,face_detection10
1946,166,"Additionally , when we enlarge the training batch size ( i.e. , Large BS ) , the result in hard setting can get 91.2 % AP .",Implementation Details,Comparison with RFB,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",41,0.6507936507936508,165,0.8505154639175257,15,1.0,1,results,Implementation Details: Comparison with RFB,face_detection10
1947,170,"From , DSFD with SE - ResNeXt101 324d got 95.7 % , 94.8 % , 88.9 % , on easy , medium and hard settings respectively , which indicates that more complexity model and higher Top - 1 I ma - geNet classification accuracy may not benefit face detection AP .",Implementation Details,Effects of Different Backbones,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.7142857142857143,169,0.8711340206185567,3,0.1428571428571428,1,results,Implementation Details: Effects of Different Backbones,face_detection10
1948,172,Our DSFD enjoys high inference speed benefited from simply using the second shot detection results .,Implementation Details,Effects of Different Backbones,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",47,0.746031746031746,171,0.8814432989690721,5,0.238095238095238,1,results,Implementation Details: Effects of Different Backbones,face_detection10
1949,182,"As shown in , our DSFD achieves the best performance among all of the state - of - the - art face detectors based on the average precision ( AP ) across the three subsets , i.e. , 96.6 % ( Easy ) , 95.7 % ( Medium ) and 90.4 % ( Hard ) on validation set , and 96.0 % ( Easy ) , 95.3 % ( Medium ) and 90.0 % ( Hard ) on test set .",Implementation Details,Effects of Different Backbones,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",57,0.9047619047619048,181,0.9329896907216496,15,0.7142857142857143,1,results,Implementation Details: Effects of Different Backbones,face_detection10
1950,186,"Since WIDER FACE has bounding box annotation while faces in FDDB are represented by ellipses , we learn a post - hoc ellipses regressor to transform the final prediction results .",Implementation Details,FDDB Dataset,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",61,0.9682539682539684,185,0.9536082474226804,19,0.9047619047619048,1,experiments,Implementation Details: FDDB Dataset,face_detection10
1951,187,"As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .",Implementation Details,FDDB Dataset,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",62,0.984126984126984,186,0.9587628865979382,20,0.9523809523809524,1,experiments,Implementation Details: FDDB Dataset,face_detection10
1952,188,"After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .",Implementation Details,FDDB Dataset,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O']",63,1.0,187,0.963917525773196,21,1.0,1,experiments,Implementation Details: FDDB Dataset,face_detection10
1953,2,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,title,title,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0035587188612099,1,0.0,1,research-problem,title,face_detection11
1954,7,These complementary scale - specific detectors are combined to produce a strong multi-scale object detector .,abstract,abstract,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.5714285714285714,6,0.0213523131672597,4,0.5714285714285714,1,model,abstract,face_detection11
1955,16,"The R - CNN samples object proposals at multiple scales , using a preliminary attention stage , and then warps these proposals to the size ( e.g. 224224 ) supported by the CNN .",Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",5,0.1612903225806451,15,0.0533807829181494,5,0.1612903225806451,1,model,Introduction,face_detection11
1956,29,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .",Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",18,0.5806451612903226,28,0.099644128113879,18,0.5806451612903226,1,model,Introduction,face_detection11
1957,31,Both of them are learned end - to - end and share computations .,Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",20,0.6451612903225806,30,0.1067615658362989,20,0.6451612903225806,1,model,Introduction,face_detection11
1958,35,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",24,0.7741935483870968,34,0.1209964412811387,24,0.7741935483870968,1,model,Introduction,face_detection11
1959,36,"This is shown to produce accurate object proposals on detection benchmarks with large variation of scale , such as KITTI , achieving a recall of over 95 % for only 100 proposals .",Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",25,0.8064516129032258,35,0.1245551601423487,25,0.8064516129032258,1,model,Introduction,face_detection11
1960,37,A second contribution of this work is the use of feature upsampling as an alternative to input upsampling .,Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",26,0.8387096774193549,36,0.1281138790035587,26,0.8387096774193549,1,model,Introduction,face_detection11
1961,142,"Learning is initialized with the model generated by the first learning stage of the proposal network , described in Section 3.4 .",Implementation Details,Implementation Details,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0208333333333333,141,0.501779359430605,1,0.1666666666666666,1,experimental-setup,Implementation Details,face_detection11
1962,143,"The learning rate is set to 0.0005 , and reduced by a factor of 10 times after every 10,000 iterations .",Implementation Details,Implementation Details,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.0416666666666666,142,0.505338078291815,2,0.3333333333333333,1,experimental-setup,Implementation Details,face_detection11
1963,144,"Learning stops after 25,000 iterations .",Implementation Details,Implementation Details,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.0625,143,0.5088967971530249,3,0.5,1,experimental-setup,Implementation Details,face_detection11
1964,145,The joint optimization of ( 6 ) is solved by back - propagation throughout the unified network .,Implementation Details,Implementation Details,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",4,0.0833333333333333,144,0.5124555160142349,4,0.6666666666666666,1,experimental-setup,Implementation Details,face_detection11
1965,147,"1 . Following , the parameters of layers "" conv 1 - 1 "" to "" conv2 - 2 "" are fixed during learning , for faster training .",Implementation Details,Implementation Details,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",6,0.125,146,0.5195729537366548,6,1.0,1,experimental-setup,Implementation Details,face_detection11
1966,165,"Simply forwarding object patches , at the original scale , through the CNN impairs performance ( especially for small ones ) , since the pre-trained CNN models have a natural scale ( e.g. 224224 ) .",Implementation Details,Feature Map Approximation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5,164,0.5836298932384342,17,0.5151515151515151,1,experiments,Implementation Details: Feature Map Approximation,face_detection11
1967,182,Context Embedding,Implementation Details,,face_detection,11,"['O', 'O']","['B-n', 'I-n']",41,0.8541666666666666,181,0.6441281138790036,0,0.0,1,experiments,Implementation Details,face_detection11
1968,199,"Following , a model was trained for car detection and another for pedestrian / cyclist detection .",Experimental Evaluation,Experimental Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.6428571428571429,198,0.7046263345195729,9,0.6428571428571429,1,baselines,Experimental Evaluation,face_detection11
1969,202,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn.",Experimental Evaluation,Experimental Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",12,0.8571428571428571,201,0.7153024911032029,12,0.8571428571428571,1,code,Experimental Evaluation,face_detection11
1970,204,An NVIDIA Titan GPU was used for CNN computations .,Experimental Evaluation,Experimental Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",14,1.0,203,0.7224199288256228,14,1.0,1,experimental-setup,Experimental Evaluation,face_detection11
1971,210,"As expected , each layer has highest accuracy for the objects that match its scale .",Proposal Evaluation,Proposal Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.25,209,0.7437722419928826,5,0.25,1,results,Proposal Evaluation,face_detection11
1972,211,"While the individual recall across scales is low , the combination of all detectors achieves high recall for all object scales .",Proposal Evaluation,Proposal Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",6,0.3,210,0.7473309608540926,6,0.3,1,results,Proposal Evaluation,face_detection11
1973,212,The effect of input size shows that the proposal network is fairly robust to the size of input images for cars and pedestrians .,Proposal Evaluation,Proposal Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",7,0.35,211,0.7508896797153025,7,0.35,1,results,Proposal Evaluation,face_detection11
1974,217,"shows that , for the MS - CNN , detection can substantially benefit proposal generation , especially for pedestrians .",Proposal Evaluation,Proposal Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",12,0.6,216,0.7686832740213523,12,0.6,1,results,Proposal Evaluation,face_detection11
1975,218,"Comparison with the state - of - the - art compares the proposal generation network to BING , Selective Search , EdgeBoxes , MCG , 3 DOP and RPN .",Proposal Evaluation,Proposal Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",13,0.65,217,0.7722419928825622,13,0.65,1,results,Proposal Evaluation,face_detection11
1976,219,The top row of the figure shows that the MS - CNN achieves a recall about 98 % with only 100 proposals .,Proposal Evaluation,Proposal Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",14,0.7,218,0.7758007117437722,14,0.7,1,results,Proposal Evaluation,face_detection11
1977,241,"For pedestrian , bootstrapping and mixture are close , but random is much worse .",Object Detection Evaluation,Object Detection Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.3333333333333333,240,0.8540925266903915,15,0.3333333333333333,1,results,Object Detection Evaluation,face_detection11
1978,248,"As shown in , the deconvoltion layer helps inmost cases .",Object Detection Evaluation,Object Detection Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",22,0.4888888888888889,247,0.8790035587188612,22,0.4888888888888889,1,results,Object Detection Evaluation,face_detection11
1979,8,"In WSMA - Seg , multimodal annotations are proposed to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .",abstract,abstract,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']",5,0.8333333333333334,7,0.0507246376811594,5,0.8333333333333334,1,research-problem,abstract,face_detection12
1980,9,"In addition , we propose a multi-scale pooling segmentation ( MSP - Seg ) as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg. Experimental results on multiple datasets show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors .",abstract,abstract,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,1.0,8,0.0579710144927536,6,1.0,1,research-problem,abstract,face_detection12
1981,11,Object detection in images is one of the most widely explored tasks in computer vision .,Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0081967213114754,10,0.072463768115942,1,0.05,1,research-problem,Introduction,face_detection12
1982,19,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",9,0.0737704918032786,18,0.1304347826086956,9,0.45,1,approach,Introduction,face_detection12
1983,20,"It consists of two phases , namely , a training and a testing phase .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']",10,0.081967213114754,19,0.1376811594202898,10,0.5,1,approach,Introduction,face_detection12
1984,21,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",11,0.0901639344262295,20,0.144927536231884,11,0.55,1,approach,Introduction,face_detection12
1985,22,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.0983606557377049,21,0.1521739130434782,12,0.6,1,approach,Introduction,face_detection12
1986,23,"WSMA - Seg has the following advantages : ( i ) as an NMS - free solution , WSMA - Seg avoids all hyperparameters related to anchor boxes and NMS ; so , the above - mentioned threshold selection problem is also avoided ; ( ii ) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation - like multimodal annotations ; and ( iii ) multimodal annotations are pixel - level annotations ; so , they can describe the objects more accurately and overcome the above - mentioned environment noise problem .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1065573770491803,22,0.1594202898550724,13,0.65,1,approach,Introduction,face_detection12
1987,25,"Therefore , in this work , we further propose a multi-scale pooling segmentation ( MSP - Seg ) model , which is used as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation ( especially for extreme cases , e.g. , very small objects ) , and consequently enhances the detection accuracy of WSMA - Seg .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",15,0.1229508196721311,24,0.1739130434782608,15,0.75,1,approach,Introduction,face_detection12
1988,27,"We propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach to achieve an accurate and robust object detection without NMS , which is the first anchor-free and NMS - free object detection approach .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1393442622950819,26,0.1884057971014492,17,0.85,1,approach,Introduction,face_detection12
1989,28,We propose multimodal annotations to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .,Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",18,0.1475409836065573,27,0.1956521739130435,18,0.9,1,approach,Introduction,face_detection12
1990,29,We propose a multi-scale pooling segmentation ( MSP - Seg ) model to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg .,Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",19,0.1557377049180328,28,0.2028985507246377,19,0.95,1,approach,Introduction,face_detection12
1991,61,Object Detection Using Segmentation Results and Contour Tracing,Introduction,,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.4180327868852459,60,0.4347826086956521,0,0.0,1,research-problem,Introduction,face_detection12
1992,98,"As shown in , our proposed method with Stack = 2 , Base = 40 , Depth = 5 has achieved the best performance among all solutions in terms of F1 Score .",Introduction,Rebar Head Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",88,0.7213114754098361,97,0.7028985507246377,8,0.8,1,experiments,Introduction: Rebar Head Detection,face_detection12
1993,2,RetinaFace : Single - stage Dense Face Localisation in the Wild,title,title,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0041666666666666,1,0.0,1,research-problem,title,face_detection13
1994,4,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .",abstract,abstract,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.0125,1,0.1,1,research-problem,abstract,face_detection13
1995,5,"This paper presents a robust single - stage face detector , named RetinaFace , which performs pixel - wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self - supervised multi-task learning .",abstract,abstract,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'I-n', 'O', 'I-n', 'O', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.2,4,0.0166666666666666,2,0.2,1,research-problem,abstract,face_detection13
1996,9,( 2 ) We further add a selfsupervised mesh decoder branch for predicting a pixel - wise 3D shape face information in parallel with the existing supervised branches .,abstract,"Specifically ,",face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.6,8,0.0333333333333333,6,0.6,1,approach,"abstract: Specifically ,",face_detection13
1997,15,Automatic face localisation is the prerequisite step of facial image analysis for many applications such as facial attribute ( e.g. expression and age ) and facial identity recognition .,Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0294117647058823,14,0.0583333333333333,1,0.0294117647058823,1,research-problem,Introduction,face_detection13
1998,18,The proposed single - stage pixel - wise face localisation method employs extra-supervised and self - supervised multi-task learning in parallel with the existing box classification and regression branches .,Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.1176470588235294,17,0.0708333333333333,4,0.1176470588235294,1,approach,Introduction,face_detection13
1999,19,"Each positive anchor outputs ( 1 ) a face score , ( 2 ) a face box , ( 3 ) five facial landmarks , and ( 4 ) dense 3 D face vertices projected on the image plane .",Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",5,0.1470588235294117,18,0.075,5,0.1470588235294117,1,approach,Introduction,face_detection13
2000,25,"Following this route , we improve the single - stage face detection framework and propose a state - of - the - art dense face localisation method by exploiting multi-task losses coming from strongly supervised and self - supervised signals .",Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.3235294117647059,24,0.1,11,0.3235294117647059,1,approach,Introduction,face_detection13
2001,29,"Inspired by , MTCNN and STN simultaneously detected faces and five facial landmarks .",Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",15,0.4411764705882353,28,0.1166666666666666,15,0.4411764705882353,1,approach,Introduction,face_detection13
2002,41,"In this paper , we employ a mesh decoder branch through self - supervision learning for predicting a pixel - wise 3 D face shape in parallel with the existing supervised branches .",Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",27,0.7941176470588235,40,0.1666666666666666,27,0.7941176470588235,1,approach,Introduction,face_detection13
2003,43,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .",Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",29,0.8529411764705882,42,0.175,29,0.8529411764705882,1,approach,Introduction,face_detection13
2004,132,"For negative anchors , only classification loss is applied .",Implementation details,Loss Head .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O']",10,0.3846153846153846,131,0.5458333333333333,10,0.303030303030303,1,experimental-setup,Implementation details: Loss Head .,face_detection13
2005,134,"We employ a shared loss head ( 1 1 conv ) across different feature maps H n W n 256 , n ? { 2 , . . . , 6 }.",Implementation details,Loss Head .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.4615384615384615,133,0.5541666666666667,12,0.3636363636363636,1,experimental-setup,Implementation details: Loss Head .,face_detection13
2006,140,"We set the scale step at 2 1 / 3 and the aspect ratio at 1 : During training , anchors are matched to a ground - truth box when IoU is larger than 0.5 , and to the background when IoU is less than 0.3 .",Implementation details,Anchor Settings .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6923076923076923,139,0.5791666666666667,18,0.5454545454545454,1,experimental-setup,Implementation details: Anchor Settings .,face_detection13
2007,142,"Since most of the anchors ( > 99 % ) are negative after the matching step , we employ standard OHEM to alleviate significant imbalance between the positive and negative training examples .",Implementation details,Anchor Settings .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.7692307692307693,141,0.5875,20,0.6060606060606061,1,experimental-setup,Implementation details: Anchor Settings .,face_detection13
2008,143,"More specifically , we sort negative anchors by the loss values and select the top ones so that the ratio between the negative and positive samples is at least 3:1 .",Implementation details,Anchor Settings .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",21,0.8076923076923077,142,0.5916666666666667,21,0.6363636363636364,1,experimental-setup,Implementation details: Anchor Settings .,face_detection13
2009,150,"We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .",Training Details .,Training Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1666666666666666,149,0.6208333333333333,28,0.8484848484848485,1,experimental-setup,Training Details .,face_detection13
2010,151,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .",Training Details .,Training Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.3333333333333333,150,0.625,29,0.8787878787878788,1,experimental-setup,Training Details .,face_detection13
2011,152,The training process terminates at 80 epochs .,Training Details .,Training Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",3,0.5,151,0.6291666666666667,30,0.9090909090909092,1,experimental-setup,Training Details .,face_detection13
2012,155,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,Training Details .,Testing Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,1.0,154,0.6416666666666667,33,1.0,1,experimental-setup,Training Details .: Testing Details .,face_detection13
2013,160,"By applying the practices of state - of - the - art techniques ( i.e. FPN , context module , and deformable convolution ) , we setup a strong baseline ( 91.286 % ) , which is slightly better than ISRN ( 90.9 % ) .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.5,159,0.6625,4,0.5,1,ablation-analysis,Ablation Study,face_detection13
2014,161,"Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,160,0.6666666666666666,5,0.625,1,ablation-analysis,Ablation Study,face_detection13
2015,162,"By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.75,161,0.6708333333333333,6,0.75,1,ablation-analysis,Ablation Study,face_detection13
2016,163,"Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.875,162,0.675,7,0.875,1,ablation-analysis,Ablation Study,face_detection13
2017,164,"This demonstrates that landmark regression does help dense regression , which in turn boosts face detection performance even further .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",8,1.0,163,0.6791666666666667,8,1.0,1,ablation-analysis,Ablation Study,face_detection13
2018,171,Our approach outper - forms these state - of - the - art methods in terms of AP .,Method,Face box Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-p', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",6,0.15,170,0.7083333333333334,4,0.4,1,results,Method: Face box Accuracy,face_detection13
2019,172,"More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .",Method,Face box Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.175,171,0.7125,5,0.5,1,results,Method: Face box Accuracy,face_detection13
2020,173,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .",Method,Face box Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.2,172,0.7166666666666667,6,0.6,1,results,Method: Face box Accuracy,face_detection13
2021,176,"Besides accurate bounding boxes , the five facial landmarks predicted by Retina Face are also very robust under the variations of pose , occlusion and resolution .",Method,Face box Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.275,175,0.7291666666666666,9,0.9,1,results,Method: Face box Accuracy,face_detection13
2022,183,RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .,Method,Five Facial Landmark Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",18,0.45,182,0.7583333333333333,5,0.7142857142857143,1,results,Method: Five Facial Landmark Accuracy,face_detection13
2023,185,"Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .",Method,Five Facial Landmark Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5,184,0.7666666666666667,7,1.0,1,results,Method: Five Facial Landmark Accuracy,face_detection13
2024,199,"In this paper , we demonstrate how our face detection method can boost the performance of a state - of - the - art publicly available face recognition method , i.e. ArcFace .",Method,Face Recognition Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']",34,0.85,198,0.825,2,0.25,1,results,Method: Face Recognition Accuracy,face_detection13
2025,204,"The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .",Method,Face Recognition Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",39,0.975,203,0.8458333333333333,7,0.875,1,results,Method: Face Recognition Accuracy,face_detection13
2026,209,We employ two tricks ( i.e. flip test and face detection score to weigh samples within templates ) to progressively improve the face verification accuracy .,Methods,Methods,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",3,0.12,208,0.8666666666666667,3,0.375,1,approach,Methods,face_detection13
2027,210,"Under fair comparison , TAR ( at FAR = 1 e ? 6 ) significantly improves from 88 . 29 % to 89.59 % simply by replacing MTCNN with RetinaFace .",Methods,Methods,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O']",4,0.16,209,0.8708333333333333,4,0.5,1,results,Methods,face_detection13
2028,215,Inference Efficiency,Methods,,face_detection,13,"['O', 'O']","['B-n', 'I-n']",9,0.36,214,0.8916666666666667,0,0.0,1,results,Methods,face_detection13
2029,2,WIDER FACE : A Face Detection Benchmark,title,title,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.0,1,0.0037174721189591,1,0.0,1,research-problem,title,face_detection14
2030,32,We introduce a large - scale face detection dataset called WIDER FACE .,Introduction,Introduction,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.72,31,0.1152416356877323,18,0.72,1,dataset,Introduction,face_detection14
2031,36,"We show an example of using WIDER FACE through proposing a multi-scale two - stage cascade framework , which uses divide and conquer strategy to deal with large scale variations .",Introduction,Introduction,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",22,0.88,35,0.1301115241635687,22,0.88,1,model,Introduction,face_detection14
2032,158,"Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .",Experimental Results,Overall .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.1368421052631579,157,0.5836431226765799,12,0.2727272727272727,1,results,Experimental Results: Overall .,face_detection14
2033,166,The results of small scale are abysmal : none of the algorithms is able to achieve more than 12 % AP .,Experimental Results,Scale .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2210526315789473,165,0.6133828996282528,20,0.4545454545454545,1,results,Experimental Results: Scale .,face_detection14
2034,170,"In , we show the impact of occlusion on detecting faces with a height of at least 30 pixels .",Experimental Results,Occlusion .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",25,0.2631578947368421,169,0.6282527881040892,24,0.5454545454545454,1,results,Experimental Results: Occlusion .,face_detection14
2035,173,The maximum AP is only 26.5 % achieved by Faceness .,Experimental Results,Occlusion .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",28,0.2947368421052631,172,0.6394052044609665,27,0.6136363636363636,1,results,Experimental Results: Occlusion .,face_detection14
2036,175,The best performance of baseline methods drops to 14.4 % .,Experimental Results,Occlusion .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-p', 'B-n', 'I-n', 'O']",30,0.3157894736842105,174,0.6468401486988847,29,0.6590909090909091,1,results,Experimental Results: Occlusion .,face_detection14
2037,176,"It is worth noting that Faceness and DPM , which are part based models , already perform relatively better than other methods on occlusion handling .",Experimental Results,Occlusion .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",31,0.3263157894736842,175,0.6505576208178439,30,0.6818181818181818,1,results,Experimental Results: Occlusion .,face_detection14
2038,183,"The best performance is achieved by Faceness , with a recall below 20 % .",Experimental Results,Pose .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",38,0.4,182,0.6765799256505576,37,0.8409090909090909,1,results,Experimental Results: Pose .,face_detection14
2039,186,"Among the four baseline methods , Faceness tends to outperform the other methods .",Experimental Results,Summary .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O']",41,0.431578947368421,185,0.6877323420074349,40,0.9090909090909092,1,results,Experimental Results: Summary .,face_detection14
2040,191,WIDER FACE as an Effective Training Source,Experimental Results,,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",46,0.4842105263157895,190,0.7063197026022305,0,0.0,1,results,Experimental Results,face_detection14
2041,203,"As shown in , the retrained models perform consistently better than the baseline models .",Experimental Results,WIDER FACE as an Effective Training Source,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",58,0.6105263157894737,202,0.7509293680297398,12,0.3428571428571428,1,results,Experimental Results: WIDER FACE as an Effective Training Source,face_detection14
2042,204,The average AP improvement of retrained ACF detector is 5.4 % in comparison to baseline ACF detector .,Experimental Results,WIDER FACE as an Effective Training Source,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",59,0.6210526315789474,203,0.7546468401486989,13,0.3714285714285714,1,results,Experimental Results: WIDER FACE as an Effective Training Source,face_detection14
2043,205,"For the Faceness , the retrained Faceness model obtain 4.2 % improvement on WIDER hard test set .",Experimental Results,WIDER FACE as an Effective Training Source,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",60,0.631578947368421,204,0.758364312267658,14,0.4,1,results,Experimental Results: WIDER FACE as an Effective Training Source,face_detection14
2044,209,"The retrained ACF detector achieves a recall rate of 87.48 % , outperforms the baseline ACF by a considerable margin of 1.4 % .",Experimental Results,WIDER FACE as an Effective Training Source,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",64,0.6736842105263158,208,0.7732342007434945,18,0.5142857142857142,1,results,Experimental Results: WIDER FACE as an Effective Training Source,face_detection14
2045,218,The recall rate improvement of the retrained Faceness detector is 0.8 % in comparison to the baseline Faceness detector .,Experimental Results,WIDER FACE as an Effective Training Source,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",73,0.7684210526315789,217,0.8066914498141264,27,0.7714285714285715,1,results,Experimental Results: WIDER FACE as an Effective Training Source,face_detection14
2046,236,"As shown in , the multi-scale cascade CNN obtains 8.5 % AP improvement on the WIDER Hard subset compared to the retrained Faceness , suggesting its superior capability in handling faces with different scales .",Experimental Results,Evaluation of Multi-scale Detection Cascade,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.9578947368421052,235,0.8736059479553904,9,0.6923076923076923,1,results,Experimental Results: Evaluation of Multi-scale Detection Cascade,face_detection14
2047,239,"For the WIDER Medium subset , the multi-scale cascade CNN outperforms other baseline methods with a considerable margin .",Experimental Results,Evaluation of Multi-scale Detection Cascade,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",94,0.9894736842105264,238,0.8847583643122676,12,0.9230769230769232,1,results,Experimental Results: Evaluation of Multi-scale Detection Cascade,face_detection14
2048,2,FaceBoxes : A CPU Real - time Face Detector with High Accuracy,title,title,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0042372881355932,1,0.0,1,research-problem,title,face_detection15
2049,5,"To address this challenge , we propose a novel face detector , named FaceBoxes , with superior performance on both speed and accuracy .",abstract,abstract,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.2,4,0.0169491525423728,2,0.2,1,research-problem,abstract,face_detection15
2050,13,Code is available at https://github.com/sfzhang15/FaceBoxes .,abstract,,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'O']",10,1.0,12,0.0508474576271186,10,1.0,1,code,abstract,face_detection15
2051,15,Face detection is one of the fundamental problems in computer vision and pattern recognition .,Introduction,Introduction,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.024390243902439,14,0.059322033898305,1,0.024390243902439,1,research-problem,Introduction,face_detection15
2052,41,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .",Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",27,0.6585365853658537,40,0.1694915254237288,27,0.6585365853658537,1,model,Introduction: These two ways have their own advantages .,face_detection15
2053,42,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .",Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.6829268292682927,41,0.173728813559322,28,0.6829268292682927,1,model,Introduction: These two ways have their own advantages .,face_detection15
2054,50,We design the Rapidly Digested Convolutional Layers ( RDCL ) to enable face detection to achieve real - time speed on the CPU ; We introduce the Multiple Scale Convolutional Layers ( MSCL ) to handle various scales of face via enriching receptive fields and discretizing anchors over layers .,Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O']",36,0.8780487804878049,49,0.2076271186440678,36,0.8780487804878049,1,model,Introduction: These two ways have their own advantages .,face_detection15
2055,165,"All the parameters are randomly initialized with the "" xavier "" method .",Other implementation details .,Other implementation details .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.25,164,0.6949152542372882,25,0.8928571428571429,1,hyperparameters,Other implementation details .,face_detection15
2056,166,"We finetune the resulting model using SGD with 0.9 momentum , 0.0005 weight decay and batch size 32 .",Other implementation details .,Other implementation details .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O']",2,0.5,165,0.6991525423728814,26,0.9285714285714286,1,hyperparameters,Other implementation details .,face_detection15
2057,167,"The maximum number of iterations is 120 k and we use 10 ? 3 learning rate for the first 80 k iterations , then continue training for 20 k iterations with 10 ? 4 and 10 ? 5 , respectively .",Other implementation details .,Other implementation details .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",3,0.75,166,0.7033898305084746,27,0.9642857142857144,1,hyperparameters,Other implementation details .,face_detection15
2058,168,Our method is implemented in the Caffe library .,Other implementation details .,,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",4,1.0,167,0.7076271186440678,28,1.0,1,hyperparameters,Other implementation details .,face_detection15
2059,178,"As listed in Tab. 1 , comparing with recent CNN - based methods , our FaceBoxes can run at 20 FPS on the CPU with state - of - the - art accuracy .",Experiments,Runtime efficiency,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.9,177,0.75,7,0.875,1,results,Experiments: Runtime efficiency,face_detection15
2060,201,"2 indicates that MSCL effectively increases the m AP by 1.0 % , owning to the diverse receptive fields and the multi -scale anchor tiling mechanism .",Model analysis,MSCL is better .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8,200,0.847457627118644,8,0.6153846153846154,1,ablation-analysis,Model analysis: MSCL is better .,face_detection15
2061,202,RDCL is efficient and accuracy - preserving .,Model analysis,,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.84,201,0.8516949152542372,9,0.6923076923076923,1,ablation-analysis,Model analysis,face_detection15
2062,212,"As illustrated in , our FaceBoxes outperforms all others by a large margin .",Evaluation on benchmark,It has 205 images with 473 faces .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",5,0.238095238095238,211,0.8940677966101694,5,0.238095238095238,1,results,Evaluation on benchmark: It has 205 images with 473 faces .,face_detection15
2063,213,shows some qualitative results on the AFW dataset .,Evaluation on benchmark,It has 205 images with 473 faces .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",6,0.2857142857142857,212,0.8983050847457628,6,0.2857142857142857,1,results,Evaluation on benchmark: It has 205 images with 473 faces .,face_detection15
2064,217,"Our method significantly outperforms all other methods and commercial face detectors ( e.g. , SkyBiometry , Face + + and Picasa ) .",Evaluation on benchmark,PASCAL face dataset .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4761904761904761,216,0.9152542372881356,10,0.4761904761904761,1,results,Evaluation on benchmark: PASCAL face dataset .,face_detection15
2065,218,shows some qualitative results on the PASCAL face dataset .,Evaluation on benchmark,PASCAL face dataset .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",11,0.5238095238095238,217,0.9194915254237288,11,0.5238095238095238,1,results,Evaluation on benchmark: PASCAL face dataset .,face_detection15
2066,2,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition",title,title,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",1,0.0,1,0.0021459227467811,1,0.0,1,research-problem,title,face_detection16
2067,11,"D ETECTION and analysis of faces is a challenging problem in computer vision , and has been actively researched for applications such as face verification , face tracking , person identification , etc .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0095238095238095,10,0.0214592274678111,1,0.027027027027027,1,research-problem,INTRODUCTION,face_detection16
2068,15,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",5,0.0476190476190476,14,0.0300429184549356,5,0.1351351351351351,1,model,INTRODUCTION,face_detection16
2069,16,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O']",6,0.0571428571428571,15,0.0321888412017167,6,0.1621621621621621,1,model,INTRODUCTION,face_detection16
2070,17,We exploit the fact that information contained in features is hierarchically distributed throughout the network as demonstrated in .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O']",7,0.0666666666666666,16,0.0343347639484978,7,0.1891891891891892,1,model,INTRODUCTION,face_detection16
2071,27,Features fusion aims to transform the features to a common subspace where they can be combined linearly or non-linearly .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1619047619047619,26,0.055793991416309,17,0.4594594594594595,1,model,INTRODUCTION,face_detection16
2072,29,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",19,0.1809523809523809,28,0.0600858369098712,19,0.5135135135135135,1,model,INTRODUCTION,face_detection16
2073,37,Fusing the intermediate layer features provides additional performance boost .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",27,0.2571428571428571,36,0.0772532188841201,27,0.7297297297297297,1,model,INTRODUCTION,face_detection16
2074,51,"Since then , several approaches have adopted MTL for solving different problems in computer vision .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3904761904761905,50,0.1072961373390558,3,0.0517241379310344,1,research-problem,INTRODUCTION,face_detection16
2075,53,This method is based on a mixture of trees with a shared pool of parts in the sense that every facial landmark is modeled as apart and uses global mixtures to capture the topological changes due to viewpoint variations .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",43,0.4095238095238095,52,0.111587982832618,5,0.0862068965517241,1,model,INTRODUCTION,face_detection16
2076,60,It fuses all the intermediate layers of a CNN at three different scales of the image pyramid for multi-task training on diverse sets .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",50,0.4761904761904761,59,0.1266094420600858,12,0.2068965517241379,1,model,INTRODUCTION,face_detection16
2077,66,"Instead , we strategically design the network architecture such that the tasks exploit low level as well as high level features of the network .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",56,0.5333333333333333,65,0.1394849785407725,18,0.3103448275862069,1,model,INTRODUCTION,face_detection16
2078,67,We also jointly predict the task of face detection and landmark localization .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",57,0.5428571428571428,66,0.1416309012875536,19,0.3275862068965517,1,research-problem,INTRODUCTION,face_detection16
2079,84,Landmarks localization :,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O']","['B-n', 'I-n', 'O']",74,0.7047619047619048,83,0.1781115879828326,36,0.6206896551724138,1,model,INTRODUCTION,face_detection16
2080,88,"While the former learns the shape increment given a mean initial shape , the latter trains an appearance model to predict the keypoint locations .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",78,0.7428571428571429,87,0.186695278969957,40,0.6896551724137931,1,model,INTRODUCTION,face_detection16
2081,100,Gender recognition :,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O']","['B-n', 'I-n', 'O']",90,0.8571428571428571,99,0.2124463519313304,52,0.896551724137931,1,experiments,INTRODUCTION,face_detection16
2082,144,"It provides annotations for 21 landmark points per face , along with the face bounding - box , face pose ( yaw , pitch and roll ) and gender information .",Training,Training,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",3,0.0120481927710843,143,0.3068669527896995,3,0.0625,1,hyperparameters,Training,face_detection16
2083,148,We use the Selective Search algorithm in R - CNN to generate region proposals for faces in an image .,Training,Training,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-n', 'B-n', 'O']",7,0.0281124497991967,147,0.315450643776824,7,0.1458333333333333,1,experiments,Training,face_detection16
2084,157,We use 21 point markups for face landmarks locations as provided in the AFLW dataset .,Training,Training,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O']",16,0.0642570281124498,156,0.334763948497854,16,0.3333333333333333,1,hyperparameters,Training,face_detection16
2085,170,We also learn the visibility factor in order to test the presence of the predicted landmark .,Training,Training,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",29,0.1164658634538152,169,0.3626609442060086,29,0.6041666666666666,1,experiments,Training,face_detection16
2086,178,Gender Recognition :,Training,Training,face_detection,16,"['O', 'O', 'O']","['B-n', 'I-n', 'O']",37,0.1485943775100401,177,0.3798283261802575,37,0.7708333333333334,1,experiments,Training,face_detection16
2087,179,Predicting gender is a two class problem similar to face detection .,Training,Training,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",38,0.1526104417670682,178,0.3819742489270386,38,0.7916666666666666,1,experiments,Training,face_detection16
2088,211,IRP improves the recall by generating more candidate proposals by using the predicted landmarks information from the initial set of region proposals .,Training,Testing,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",70,0.2811244979919678,210,0.4506437768240343,21,0.525,1,experiments,Training: Testing,face_detection16
2089,263,R- CNN,Training,,face_detection,16,"['O', 'O']","['B-n', 'I-n']",122,0.4899598393574297,262,0.5622317596566524,5,0.0892857142857142,1,baselines,Training,face_detection16
2090,271,We also perform a linear bounding box regression to localize the face co-ordinates .,Training,R- CNN,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",130,0.5220883534136547,270,0.5793991416309013,13,0.2321428571428571,1,experiments,Training: R- CNN,face_detection16
2091,433,Fast - HyperFace,Method,Effect of Post - Processing,face_detection,16,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",42,0.5915492957746479,432,0.927038626609442,23,0.6216216216216216,1,research-problem,Method: Effect of Post - Processing,face_detection16
2092,434,The Hyperface method is tested on a machine with 8 cores and GTX TITAN - X GPU .,Method,Effect of Post - Processing,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",43,0.6056338028169014,433,0.9291845493562232,24,0.6486486486486487,1,experiments,Method: Effect of Post - Processing,face_detection16
2093,438,We also propose a fast version of HyperFace which uses a high recall fast face detector instead of Selective Search to generate candidate region proposals .,Method,Effect of Post - Processing,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",47,0.6619718309859155,437,0.9377682403433476,28,0.7567567567567568,1,model,Method: Effect of Post - Processing,face_detection16
2094,439,We implement a face detector using Single Shot Detector ( SSD ) framework .,Method,Effect of Post - Processing,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",48,0.676056338028169,438,0.9399141630901288,29,0.7837837837837838,1,model,Method: Effect of Post - Processing,face_detection16
2095,445,"Fast - HyperFace consumes a total time of 0.15s ( 0.05 s for SSD face detector , and 0.1s for HyperFace ) on a GTX TITAN X GPU .",Method,Effect of Post - Processing,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",54,0.7605633802816901,444,0.9527896995708156,35,0.945945945945946,1,experiments,Method: Effect of Post - Processing,face_detection16
2096,446,"The Fast - HyperFace achieves a m AP of 97.6 % on AFW face detection task , which is comparable to the HyperFace m AP of 97.9 % .",Method,Effect of Post - Processing,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.7746478873239436,445,0.9549356223175964,36,0.972972972972973,1,results,Method: Effect of Post - Processing,face_detection16
2097,447,"Thus , Fast - HyperFace improves the speed by a factor of 12 with negligible degradation in performance .",Method,Effect of Post - Processing,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",56,0.7887323943661971,446,0.9570815450643776,37,1.0,1,experiments,Method: Effect of Post - Processing,face_detection16
2098,2,Pyramid Box : A Context - assisted Single Shot Face Detector,title,title,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0041666666666666,1,0.0,1,research-problem,title,face_detection17
2099,12,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,abstract,abstract,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",9,0.9,11,0.0458333333333333,9,0.9,1,code,abstract,face_detection17
2100,15,Face detection is a fundamental and essential task in various face applications .,Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.027027027027027,14,0.0583333333333333,1,0.027027027027027,1,research-problem,Introduction,face_detection17
2101,24,Face R - FCN re-weights embedding responses on score maps and eliminates the effect of non-uniformed contribution in each facial part using a position - sensitive average pooling .,Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.2702702702702703,23,0.0958333333333333,10,0.2702702702702703,1,model,Introduction,face_detection17
2102,25,FAN proposes an anchor - level attention by highlighting the features from the face region to detect the occluded faces .,Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",11,0.2972972972972973,24,0.1,11,0.2972972972972973,1,model,Introduction,face_detection17
2103,31,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.4594594594594595,30,0.125,17,0.4594594594594595,1,model,Introduction,face_detection17
2104,36,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.5945945945945946,35,0.1458333333333333,22,0.5945945945945946,1,model,Introduction,face_detection17
2105,37,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",23,0.6216216216216216,36,0.15,23,0.6216216216216216,1,model,Introduction,face_detection17
2106,38,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",24,0.6486486486486487,37,0.1541666666666666,24,0.6486486486486487,1,model,Introduction,face_detection17
2107,42,"We propose an anchor-based context assisted method , called PyramidAnchors , to introduce supervised information on learning contextual features for small , blurred and partially occluded faces .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.7567567567567568,41,0.1708333333333333,28,0.7567567567567568,1,model,Introduction,face_detection17
2108,43,2 . We design the Low - level Feature Pyramid Networks ( LFPN ) to merge contextual features and facial features better .,Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O']",29,0.7837837837837838,42,0.175,29,0.7837837837837838,1,model,Introduction,face_detection17
2109,45,"3 . We introduce a context - sensitive prediction module , consisting of a mixed network structure and max - in - out layer to learn accurate location and classification from the merged features .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",31,0.8378378378378378,44,0.1833333333333333,31,0.8378378378378378,1,model,Introduction,face_detection17
2110,157,"Data sampling is a classical subject in statistics , machine learning and pattern recognition , it achieves great development in recent years .",Training,Training,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.131578947368421,156,0.65,5,0.131578947368421,1,experiments,Training,face_detection17
2111,158,"For the task of objection detection , Focus Loss address the class imbalance by reshaping the standard cross entropy loss .",Training,Training,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.1578947368421052,157,0.6541666666666667,6,0.1578947368421052,1,experiments,Training,face_detection17
2112,159,Here we utilize a data augment sample method named Data - anchor - sampling .,Training,Training,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.1842105263157894,158,0.6583333333333333,7,0.1842105263157894,1,baselines,Training,face_detection17
2113,168,2 ) generate smaller face samples through larger ones to increase the diversity of face samples of smaller scales .,Training,Training,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",16,0.4210526315789473,167,0.6958333333333333,16,0.4210526315789473,1,experiments,Training,face_detection17
2114,187,"As for the parameter initialization , our PyramidBox use the pre-trained parameters from VGG16 .",Training,Optimization .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",35,0.9210526315789472,186,0.775,35,0.9210526315789472,1,hyperparameters,Training: Optimization .,face_detection17
2115,188,"The parameters of conv fc 67 and conv fc 7 are initialized by sub - sampling parameters from fc 6 and fc 7 of VGG16 and the other additional layers are randomly initialized with "" xavier "" in .",Training,Optimization .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O']",36,0.9473684210526316,187,0.7791666666666667,36,0.9473684210526316,1,hyperparameters,Training: Optimization .,face_detection17
2116,189,"We use a learning rate of 10 ? 3 for 80 k iterations , and 10 ? 4 for the next 20 k iterations , and 10 ? 5 for the last 20 k iterations on the WIDER FACE training set with batch size 16 .",Training,Optimization .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",37,0.9736842105263158,188,0.7833333333333333,37,0.9736842105263158,1,hyperparameters,Training: Optimization .,face_detection17
2117,190,We also use a momentum of 0.9 and a weight decay of 0.0005 .,Training,Optimization .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",38,1.0,189,0.7875,38,1.0,1,hyperparameters,Training: Optimization .,face_detection17
2118,196,Our Pyramid,Baseline .,,face_detection,17,"['O', 'O']","['B-n', 'I-n']",1,0.0476190476190476,195,0.8125,3,0.1304347826086956,1,baselines,Baseline .,face_detection17
2119,201,"The results listed in prove that LFPN started from a middle layer , using conv fc7 in our Pyramid Box , is more powerful , which implies that features with large gap in scale may not help each other .",Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2857142857142857,200,0.8333333333333334,8,0.3478260869565217,1,results,Baseline .: Contrast Study .,face_detection17
2120,202,The comparison between the first and forth column of indicates that LFPN increases the m AP by 1.9 % on hard subset .,Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.3333333333333333,201,0.8375,9,0.391304347826087,1,results,Baseline .: Contrast Study .,face_detection17
2121,205,We employ Data - anchor - sampling based on LFPN network and the result shows that our data - anchor - sampling effectively improves the performance .,Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'O']",10,0.4761904761904761,204,0.85,12,0.5217391304347826,1,results,Baseline .: Contrast Study .,face_detection17
2122,206,"The mAP is increased by 0.4 % , 0.4 % and 0.6 % on easy , medium and hard subset , respectively .",Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",11,0.5238095238095238,205,0.8541666666666666,13,0.5652173913043478,1,results,Baseline .: Contrast Study .,face_detection17
2123,209,"By comparing the first and last column in , one can see that PyamidAnchor effectively improves the performance , i.e. , 0.7 % , 0.6 % and 0.9 % on easy , medium and hard , respectively .",Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-n', 'O', 'B-n', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",14,0.6666666666666666,208,0.8666666666666667,16,0.6956521739130435,1,results,Baseline .: Contrast Study .,face_detection17
2124,211,Wider and deeper context prediction module is better .,Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",16,0.7619047619047619,210,0.875,18,0.782608695652174,1,results,Baseline .: Contrast Study .,face_detection17
2125,212,shows that the performance of CPM is better than both DSSD module and SSH context module .,Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.8095238095238095,211,0.8791666666666667,19,0.8260869565217391,1,results,Baseline .: Contrast Study .,face_detection17
2126,213,"Notice that the combination of SSH and DSSD gains very little compared to SSH alone , which indicates that large receptive field is more important to predict the accurate location and classification .",Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.8571428571428571,212,0.8833333333333333,20,0.8695652173913043,1,results,Baseline .: Contrast Study .,face_detection17
2127,214,"In addition , by comparing the last two column of , one can find that the method of Max - in - out improves the mAP on WIDER FACE validation set about + 0.2 % ( Easy ) , + 0.3 % ( Medium ) and + 0.1 % ( Hard ) , respectively .",Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",19,0.9047619047619048,213,0.8875,21,0.9130434782608696,1,results,Baseline .: Contrast Study .,face_detection17
2128,215,"To conclude this section , we summarize our results in , from which one can see that m AP increase 2.1 % , 2.3 % and 4.7 % on easy , medium and hard subset , respectively .",Baseline .,Contrast Study .,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",20,0.9523809523809524,214,0.8916666666666667,22,0.9565217391304348,1,results,Baseline .: Contrast Study .,face_detection17
2129,228,"Our PyramidBox outperforms others across all three subsets , i.e. 0.961 ( easy ) , 0.950 ( medium ) , 0.889 ( hard ) for validation set , and 0.956 ( easy ) , 0.946 ( medium ) , 0.887 ( hard ) for testing set .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1.0,227,0.9458333333333332,11,1.0,1,results,Evaluation on Benchmark,face_detection17
2130,2,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection,title,title,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0031347962382445,1,0.0,1,research-problem,title,face_detection18
2131,14,"Detection and analysis on human subjects using facial feature based biometrics for access control , surveillance systems and other security applications have gained popularity over the past few years .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0144927536231884,13,0.0407523510971786,1,0.0357142857142857,1,research-problem,INTRODUCTION,face_detection18
2132,21,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'O', 'O', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1159420289855072,20,0.0626959247648902,8,0.2857142857142857,1,model,INTRODUCTION,face_detection18
2133,22,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",9,0.1304347826086956,21,0.0658307210031347,9,0.3214285714285714,1,model,INTRODUCTION,face_detection18
2134,25,"Therefore , it is able to robustly deal with the challenges in the problem of unconstrained face detection .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.1739130434782608,24,0.0752351097178683,12,0.4285714285714285,1,model,INTRODUCTION,face_detection18
2135,26,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",13,0.1884057971014492,25,0.0783699059561128,13,0.4642857142857143,1,model,INTRODUCTION,face_detection18
2136,78,"Inside the network , skip pooling is used to extract information at multiple scales and levels of abstraction .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",65,0.9420289855072465,77,0.2413793103448276,36,0.9,1,model,INTRODUCTION,face_detection18
2137,82,"Unlike all the previous approaches that select a feature extractor beforehand and incorporate a linear classifier with the depth descriptor beside RGB channels , our method solves the problem under a deep learning framework where the global and the local context features , i.e. multi scaling , are synchronized to Faster Region - based Convolutional Neural Networks in order to robustly achieve semantic detection .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",69,1.0,81,0.2539184952978056,40,1.0,1,model,INTRODUCTION,face_detection18
2138,228,Our CMS - RCNN is implemented in the Caffe deep learning framework .,Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0666666666666666,227,0.7115987460815048,1,0.0666666666666666,1,hyperparameters,Implementation Details,face_detection18
2139,229,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.1333333333333333,228,0.7147335423197492,2,0.1333333333333333,1,hyperparameters,Implementation Details,face_detection18
2140,236,"Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",9,0.6,235,0.7366771159874608,9,0.6,1,hyperparameters,Implementation Details,face_detection18
2141,239,"Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.8,238,0.7460815047021944,12,0.8,1,hyperparameters,Implementation Details,face_detection18
2142,240,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",13,0.8666666666666667,239,0.7492163009404389,13,0.8666666666666667,1,experiments,Implementation Details,face_detection18
2143,241,"Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O']",14,0.9333333333333332,240,0.7523510971786834,14,0.9333333333333332,1,hyperparameters,Implementation Details,face_detection18
2144,242,Therefore the channel size of final feature map is at the same size as the original fifth convolution layer in Faster R - CNN .,Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",15,1.0,241,0.7554858934169278,15,1.0,1,hyperparameters,Implementation Details,face_detection18
2145,246,"Using this database , our proposed approach robustly outperforms strong baseline methods , including Two - stage CNN , Multi-scale Cascade CNN , Faceness and Aggregate Channel Features ( ACF ) , by a large margin .",EXPERIMENTS,EXPERIMENTS,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",3,0.0394736842105263,245,0.768025078369906,3,0.6,1,results,EXPERIMENTS,face_detection18
2146,249,Experiments on WIDER FACE Dataset,EXPERIMENTS,EXPERIMENTS,face_detection,18,"['O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n']",6,0.0789473684210526,248,0.7774294670846394,0,0.0,1,results,EXPERIMENTS,face_detection18
2147,267,Our method outperforms those strong baselines by a large margin .,EXPERIMENTS,Testing and Comparison,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",24,0.3157894736842105,266,0.8338557993730408,6,0.6,1,results,EXPERIMENTS: Testing and Comparison,face_detection18
2148,268,"It achieves the best average precision in all level faces , i.e. AP = 0.902 ( Easy ) , 0.874 ( Medium ) and 0.643 ( Hard ) , and outperforms the second best baseline by 26.0 % ( Easy ) , 37.4 % ( Medium ) and 60.8 % ( Hard ) .",EXPERIMENTS,Testing and Comparison,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-n', 'O', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'I-n', 'O', 'B-n', 'O', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",25,0.3289473684210526,267,0.8369905956112853,7,0.7,1,results,EXPERIMENTS: Testing and Comparison,face_detection18
2149,269,"These results suggest that as the difficulty level goes up , CMS - RCNN can detect challenging faces better .",EXPERIMENTS,Testing and Comparison,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'O']",26,0.3421052631578947,268,0.8401253918495298,8,0.8,1,results,EXPERIMENTS: Testing and Comparison,face_detection18
2150,272,With Context v.s. Without Context,EXPERIMENTS,,face_detection,18,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",29,0.3815789473684211,271,0.8495297805642633,0,0.0,1,results,EXPERIMENTS,face_detection18
2151,284,"Additionally , the context model produces a longer PR curve , which means that contextual reasoning can help finding more faces .",EXPERIMENTS,With Context v.s. Without Context,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5394736842105263,283,0.8871473354231975,12,1.0,1,results,EXPERIMENTS: With Context v.s. Without Context,face_detection18
2152,295,Experiments on FDDB Face Database,EXPERIMENTS,,face_detection,18,"['O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n']",52,0.6842105263157895,294,0.9216300940438872,0,0.0,1,results,EXPERIMENTS,face_detection18
2153,304,Our method achieves the best recall rate on this database .,EXPERIMENTS,Experiments on FDDB Face Database,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",61,0.8026315789473685,303,0.9498432601880876,9,0.6428571428571429,1,results,EXPERIMENTS: Experiments on FDDB Face Database,face_detection18
2154,307,The proposed CMS - RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods ( as shown ) .,EXPERIMENTS,Experiments on FDDB Face Database,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",64,0.8421052631578947,306,0.9592476489028212,12,0.8571428571428571,1,results,EXPERIMENTS: Experiments on FDDB Face Database,face_detection18
2155,311,"This paper has presented our proposed CMS - RCNN approach to robustly detect human facial regions from images collected under various challenging conditions , e.g. highly occlusions , low resolutions , facial expressions , illumination variations , etc .",EXPERIMENTS,Experiments on FDDB Face Database,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.8947368421052632,310,0.9717868338557992,1,0.1111111111111111,1,baselines,EXPERIMENTS: Experiments on FDDB Face Database,face_detection18
2156,313,The experimental results show that our proposed approach outperforms strong baselines on the WIDER FACE and consistently achieves very competitive results against state - of - the - art methods on the FDDB .,EXPERIMENTS,Experiments on FDDB Face Database,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",70,0.9210526315789472,312,0.9780564263322884,3,0.3333333333333333,1,results,EXPERIMENTS: Experiments on FDDB Face Database,face_detection18
2157,2,Face Detection Using Improved Faster RCNN,title,,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.009090909090909,1,0.0,1,research-problem,title,face_detection19
2158,7,"Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy set , medium set , hard set ) .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0975609756097561,6,0.0545454545454545,4,0.0975609756097561,1,research-problem,abstract,face_detection19
2159,25,"( 3 ) Our framework achieves two 1st places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy , medium , hard ) , one illustrative example of our results in the crowd case can be found in .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5365853658536586,24,0.2181818181818181,22,0.5365853658536586,1,research-problem,abstract,face_detection19
2160,26,"Face detection is one of the most fundamental and challenging problems in computer vision , and has been extensively studied for decades .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5609756097560976,25,0.2272727272727272,23,0.5609756097560976,1,research-problem,abstract,face_detection19
2161,35,Dense - Box employs a fully deep convolutional neural network to directly predict face confidence and corresponding bounding box .,abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",32,0.7804878048780488,34,0.3090909090909091,32,0.7804878048780488,1,research-problem,abstract,face_detection19
2162,36,"UnitBox introduces a novel intersection - over - union ( IoU ) loss to predict bounding box , which regresses the four bounds of a predicted box as a whole unit .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.8048780487804879,35,0.3181818181818182,33,0.8048780487804879,1,research-problem,abstract,face_detection19
2163,38,S 3 FD presents a single shot scale - invariant face detector which achieves good result on WIDER FACE datasets .,abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",35,0.8536585365853658,37,0.3363636363636363,35,0.8536585365853658,1,research-problem,abstract,face_detection19
2164,84,Single NVIDIA Tesla K80 is used for training and testing .,Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.0476190476190476,83,0.7545454545454545,1,0.0588235294117647,1,experimental-setup,Implementation Details,face_detection19
2165,85,Mini batch size is set to 1 considering memory consumption .,Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.0952380952380952,84,0.7636363636363637,2,0.1176470588235294,1,experimental-setup,Implementation Details,face_detection19
2166,88,"A deformable layer is used to output a "" thin "" feature map with exploiting image context .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.238095238095238,87,0.7909090909090909,5,0.2941176470588235,1,baselines,Implementation Details,face_detection19
2167,89,"Aspect ratios ( 1 , 1.5 , 2 ) and scales ( 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 ) are carefully designed to capture better locations of faces in the RPN stage , and the number of filters for the RPN layer is set as 512 .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",6,0.2857142857142857,88,0.8,6,0.3529411764705882,1,experimental-setup,Implementation Details,face_detection19
2168,94,"By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",11,0.5238095238095238,93,0.8454545454545455,11,0.6470588235294118,1,experimental-setup,Implementation Details,face_detection19
2169,95,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",12,0.5714285714285714,94,0.8545454545454545,12,0.7058823529411765,1,experimental-setup,Implementation Details,face_detection19
2170,96,Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",13,0.6190476190476191,95,0.8636363636363636,13,0.7647058823529411,1,experimental-setup,Implementation Details,face_detection19
2171,97,"In testing stage , multi-scale testing strategy is adapted to be robust to different scale faces .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",14,0.6666666666666666,96,0.8727272727272727,14,0.8235294117647058,1,experimental-setup,Implementation Details,face_detection19
2172,100,"We also find top - ranked 6000 proposals are directly selected without NMS during testing can boost 0.1 % , 0.3 % and 0.6 % on easy set , medium set and hard set respectively .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",17,0.8095238095238095,99,0.9,17,1.0,1,results,Implementation Details,face_detection19
2173,103,"Compared with the recently published top approaches , FDNet1.0 wins two 1st places ( easy set = 95.9 % , medium set = 94.5 % ) and one 2nd place ( hard set = 87.9 % ) on the validation set , as illustrated in .",Implementation Details,Comparison on Benchmarks,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",20,0.9523809523809524,102,0.9272727272727272,2,0.6666666666666666,1,results,Implementation Details: Comparison on Benchmarks,face_detection19
2174,2,Selective Refinement Network for High Performance Face Detection,title,,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0042194092827004,1,0.0,1,research-problem,title,face_detection2
2175,4,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .",abstract,abstract,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.0126582278481012,1,0.1428571428571428,1,research-problem,abstract,face_detection2
2176,12,"Face detection is a long - standing problem in computer vision with extensive applications including face alignment , face analysis , face recognition , etc .",Introduction,Introduction,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0227272727272727,11,0.0464135021097046,1,0.0227272727272727,1,research-problem,Introduction,face_detection2
2177,15,To further improve the performance of face detection has become a challenging issue .,Introduction,Introduction,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0909090909090909,14,0.0590717299578059,4,0.0909090909090909,1,research-problem,Introduction,face_detection2
2178,27,R - CNN - like detectors ) address the class imbalance by a two - stage cascade and sampling heuristics .,Introduction,Introduction,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-n', 'O']",16,0.3636363636363636,26,0.1097046413502109,16,0.3636363636363636,1,approach,Introduction,face_detection2
2179,28,"As for single - shot detectors , RetinaNet proposes the focal loss to focus training on a sparse set of hard examples and down - weight the loss assigned to well - classified examples .",Introduction,Introduction,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.3863636363636363,27,0.1139240506329113,17,0.3863636363636363,1,approach,Introduction,face_detection2
2180,38,"As shown in ( d ) , as the IoU threshold increases , the AP drops dramatically , indicating that the accuracy of the bounding box location needs to be improved .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-n', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6136363636363636,37,0.1561181434599156,27,0.6136363636363636,1,approach,"Introduction: However , Retina",face_detection2
2181,40,Cascade R - CNN addresses this issue by cascading R - CNN with different IoU thresholds .,Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",29,0.6590909090909091,39,0.1645569620253164,29,0.6590909090909091,1,approach,"Introduction: However , Retina",face_detection2
2182,41,RefineDet ) applies two - step regression to single - shot detector .,Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",30,0.6818181818181818,40,0.1687763713080168,30,0.6818181818181818,1,approach,"Introduction: However , Retina",face_detection2
2183,43,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",32,0.7272727272727273,42,0.1772151898734177,32,0.7272727272727273,1,approach,"Introduction: However , Retina",face_detection2
2184,46,"As shown in , RetinaNet with STC improves the recall efficiency to a certain extent .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",35,0.7954545454545454,45,0.189873417721519,35,0.7954545454545454,1,experiments,"Introduction: However , Retina",face_detection2
2185,48,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",37,0.8409090909090909,47,0.1983122362869198,37,0.8409090909090909,1,approach,"Introduction: However , Retina",face_detection2
2186,52,We present a STC module to filter out most simple negative samples from low level layers to reduce the classification search space .,Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",41,0.9318181818181818,51,0.2151898734177215,41,0.9318181818181818,1,approach,"Introduction: However , Retina",face_detection2
2187,159,"It consists of 393 , 703 annotated face bounding boxes in 32 , 203 images with variations in pose , scale , facial expression , occlusion , and lighting condition .",Training Dataset .,Training Dataset .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O']",2,0.0769230769230769,158,0.6666666666666666,3,0.1111111111111111,1,experimental-setup,Training Dataset .,face_detection2
2188,176,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .",Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.7307692307692307,175,0.7383966244725738,20,0.7407407407407407,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
2189,177,"We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .",Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-n', 'O']",20,0.7692307692307693,176,0.7426160337552743,21,0.7777777777777778,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
2190,178,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .",Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",21,0.8076923076923077,177,0.7468354430379747,22,0.8148148148148148,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
2191,179,We implement SRN using the Py - Torch library .,Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.8461538461538461,178,0.7510548523206751,23,0.8518518518518519,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
2192,181,"In the inference phase , the STC first filters the regularly tiled anchors on the selected pyramid levels with the negative confidence scores larger than the threshold ? = 0.99 , and then STR adjusts the locations and sizes of selected anchors .",Training Dataset .,Inference .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",24,0.9230769230769232,180,0.759493670886076,25,0.925925925925926,1,experimental-setup,Training Dataset .: Inference .,face_detection2
2193,183,"Finally , we apply the non-maximum suppression ( NMS ) with jaccard overlap of 0.5 to generate the top 750 high confident detections per image as the final results .",Training Dataset .,Inference .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-n', 'O']",26,1.0,182,0.7679324894514767,27,1.0,1,experiments,Training Dataset .: Inference .,face_detection2
2194,193,"Firstly , we use the ordinary prediction head in ) instead of the proposed RFE .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",2,0.1,192,0.810126582278481,6,0.25,1,ablation-analysis,Ablation Setting .,face_detection2
2195,197,"Experimental results of applying two - step classification to each pyramid level are shown in , indicating that applying two - step classification to the low pyramid levels improves the performance , especially on tiny faces .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3,196,0.8270042194092827,10,0.4166666666666667,1,ablation-analysis,Ablation Setting .,face_detection2
2196,198,"Therefore , the STC module selectively applies the two - step classification on the low pyramid levels ( i.e. , P2 , P3 , and P4 ) , since these levels are associated with lots of small anchors , which are the main source of false positives .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,197,0.8312236286919831,11,0.4583333333333333,1,ablation-analysis,Ablation Setting .,face_detection2
2197,200,"As listed in , our STC effectively reduces the false positives across different recall rates , demonstrating the effectiveness of the STC module . Selective Two - step Regression .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.45,199,0.8396624472573839,13,0.5416666666666666,1,ablation-analysis,Ablation Setting .,face_detection2
2198,202,"As shown in , it produces much better results than the baseline , with 0.8 % , 0.9 % and 0.8 % AP improvements on the Easy , Medium , and Hard subsets .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.55,201,0.8481012658227848,15,0.625,1,results,Ablation Setting .,face_detection2
2199,203,Experimental results of applying two - step regression to each pyramid level ( see ) confirm our previous analysis .,Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,202,0.8523206751054853,16,0.6666666666666666,1,results,Ablation Setting .,face_detection2
2200,205,"As shown in , the STR module produces consistently accurate detection results than the baseline method .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",14,0.7,204,0.8607594936708861,18,0.75,1,ablation-analysis,Ablation Setting .,face_detection2
2201,206,"The gap between the AP across all three subsets increases as the IoU threshold increases , which indicate that the STR module is important to produce more accurate detections .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,205,0.8649789029535865,19,0.7916666666666666,1,ablation-analysis,Ablation Setting .,face_detection2
2202,207,"In addition , coupled with the STC module , the performance is further improved to 96.1 % , 95.0 % and 90.1 % on the Easy , Medium and Hard subsets , respectively .",Ablation Setting .,Ablation Setting .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",16,0.8,206,0.869198312236287,20,0.8333333333333334,1,results,Ablation Setting .,face_detection2
2203,209,The RFE is used to diversify the receptive fields of detection layers in order to capture faces with extreme poses .,Ablation Setting .,Receptive Field Enhancement .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.9,208,0.8776371308016878,22,0.9166666666666666,1,ablation-analysis,Ablation Setting .: Receptive Field Enhancement .,face_detection2
2204,210,"Comparing the detection results between fourth and fifth columns in , we notice that RFE consistently improves the AP scores in different subsets , i.e. , 0.3 % , 0.3 % , and 0.1 % APs on the Easy , Medium , and Hard categories .",Ablation Setting .,Receptive Field Enhancement .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.95,209,0.8818565400843882,23,0.9583333333333334,1,ablation-analysis,Ablation Setting .: Receptive Field Enhancement .,face_detection2
2205,217,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.2631578947368421,216,0.9113924050632912,5,0.2631578947368421,1,results,Evaluation on Benchmark,face_detection2
2206,221,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.4736842105263157,220,0.9282700421940928,9,0.4736842105263157,1,results,Evaluation on Benchmark,face_detection2
2207,225,"As shown in ( c ) , our SRN sets a new state - of - the - art performance , i.e. , 98.8 % true positive rate when the number of false positives is equal to 1000 .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",13,0.6842105263157895,224,0.9451476793248944,13,0.6842105263157895,1,results,Evaluation on Benchmark,face_detection2
2208,230,"As shown in , we find that SRN performs favourably against the state - of - the - art based on the average precision ( AP ) across the three subsets , especially on the Hard subset which contains a large amount of small faces .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'I-n', 'O']",18,0.9473684210526316,229,0.9662447257383966,18,0.9473684210526316,1,results,Evaluation on Benchmark,face_detection2
2209,231,"Specifically , it produces the best AP scores in all subsets of both validation and testing sets , i.e. , 96.4 % ( Easy ) , 95.3 % ( Medium ) and 90.2 % ( Hard ) for validation set , and 95.9 % ( Easy ) , 94.9 % ( Medium ) and 89.7 % ( Hard ) for testing set , surpassing all approaches , which demonstrates the superiority of the proposed detector .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,1.0,230,0.9704641350210972,19,1.0,1,results,Evaluation on Benchmark,face_detection2
2210,2,Aggregate Channel Features for Multi-view Face Detection,title,,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0036764705882352,1,0.0,1,research-problem,title,face_detection20
2211,24,The classifier learning process follows the VJ framework pipeline .,Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",14,0.4827586206896552,23,0.0845588235294117,14,0.4827586206896552,1,model,Introduction,face_detection20
2212,25,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .",Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.5172413793103449,24,0.088235294117647,15,0.5172413793103449,1,model,Introduction,face_detection20
2213,30,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .",Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.6896551724137931,29,0.1066176470588235,20,0.6896551724137931,1,model,Introduction,face_detection20
2214,164,"Summary : Based on observations above , we choose 2048 as the number of weak classifiers contained in the soft cascade .",Training design,Training design,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",15,0.1704545454545454,163,0.5992647058823529,15,0.375,1,hyperparameters,Training design,face_detection20
2215,242,Evaluation on benchmark face database,Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n']",4,0.1379310344827586,241,0.8860294117647058,0,0.0,1,results,Experiments,face_detection20
2216,243,"As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .",Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",5,0.1724137931034483,242,0.8897058823529411,1,0.1111111111111111,1,results,Experiments,face_detection20
2217,248,"In discrete score where evaluation metric is the same as in AFW , our detector achieves 83.7 % , which is a little better than Yan et al ..",Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3448275862068966,247,0.9080882352941176,6,0.6666666666666666,1,results,Experiments,face_detection20
2218,250,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .",Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.4137931034482758,249,0.9154411764705882,8,0.8888888888888888,1,results,Experiments,face_detection20
2219,2,Supervised Transformer Network for Efficient Face Detection,title,title,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0033898305084745,1,0.0,1,research-problem,title,face_detection21
2220,34,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .",Introduction,"Besides these model - based methods ,",face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",20,0.1342281879194631,33,0.111864406779661,20,0.4444444444444444,1,model,"Introduction: Besides these model - based methods ,",face_detection21
2221,52,It first uses a conventional boosting cascade to obtain a set of face candidate areas .,Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",38,0.2550335570469799,51,0.1728813559322033,38,0.8444444444444444,1,model,Introduction: This helps increase detection recall .,face_detection21
2222,160,"Instead , we use the ROI masks , so that different samples can share the feature in the overlapping area .",Introduction,Motivation,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",146,0.9798657718120806,159,0.5389830508474577,20,0.8695652173913043,1,model,Introduction: Motivation,face_detection21
2223,177,We use a Real - Boost algorithm for the cascade classification learning .,Implementation details,Each fern contains 8 binary nodes .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",13,0.2653061224489796,176,0.5966101694915255,13,0.2653061224489796,1,hyperparameters,Implementation details: Each fern contains 8 binary nodes .,face_detection21
2224,224,We use GoogleNet in both the RPN and RCNN networks .,Experiments,Some sample images are shown in .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.131578947368421,223,0.7559322033898305,10,0.5263157894736842,1,experiments,Experiments: Some sample images are shown in .,face_detection21
2225,255,"As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .",Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",41,0.5394736842105263,254,0.8610169491525423,12,0.5714285714285714,1,results,Experiments: There are 6 different ablative settings in total .,face_detection21
2226,257,"In the training phase , in order to increase the variation of training samples , we randomly select K positive / negative samples from each image for the RCNN network .",Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",43,0.5657894736842105,256,0.8677966101694915,14,0.6666666666666666,1,hyperparameters,Experiments: There are 6 different ablative settings in total .,face_detection21
2227,262,We found that NMS tend to include too much noisy low confidence candidates .,Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",48,0.631578947368421,261,0.8847457627118644,19,0.9047619047619048,1,experiments,Experiments: There are 6 different ablative settings in total .,face_detection21
2228,263,"We also compare the PR curves of using all candidates , NMS , and non-top K suppression .",Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",49,0.6447368421052632,262,0.888135593220339,20,0.9523809523809524,1,experiments,Experiments: There are 6 different ablative settings in total .,face_detection21
2229,264,"Our non - top K suppression is very close to using all candidates , and achieved consistently better results than NMS under the same number of candidates .",Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'I-p', 'B-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",50,0.6578947368421053,263,0.8915254237288136,21,1.0,1,experiments,Experiments: There are 6 different ablative settings in total .,face_detection21
2230,273,We also compare with the standard network without ROI convolution .,Experiments,We conduct the experiments on the FDDB database .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O']",59,0.7763157894736842,272,0.9220338983050848,8,0.32,1,tasks,Experiments: We conduct the experiments on the FDDB database .,face_detection21
2231,274,Non- top K ( K = 3 ) suppression is adopted in all settings to make RCNN network more efficiency .,Experiments,We conduct the experiments on the FDDB database .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'I-n', 'O']",60,0.7894736842105263,273,0.9254237288135592,9,0.36,1,tasks,Experiments: We conduct the experiments on the FDDB database .,face_detection21
2232,278,The original DNN detector can run at 10 FPS on CPU fora VGA image .,Experiments,We conduct the experiments on the FDDB database .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",64,0.8421052631578947,277,0.9389830508474576,13,0.52,1,tasks,Experiments: We conduct the experiments on the FDDB database .,face_detection21
2233,282,"BC D. Qualitative face detection results on ( a ) FDDB , ( b ) AFW , ( c ) PASCAL faces datasets .",Experiments,We conduct the experiments on the FDDB database .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']",68,0.8947368421052632,281,0.952542372881356,17,0.68,1,tasks,Experiments: We conduct the experiments on the FDDB database .,face_detection21
2234,285,"On the FDDB dataset , we compare with all public methods .",Experiments,Comparing with state - of - the - art,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.9342105263157896,284,0.9627118644067796,20,0.8,1,results,Experiments: Comparing with state - of - the - art,face_detection21
2235,287,"On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .",Experiments,Comparing with state - of - the - art,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",73,0.9605263157894736,286,0.9694915254237289,22,0.88,1,baselines,Experiments: Comparing with state - of - the - art,face_detection21
2236,289,"We learn a global regression from 5 facial points to face rectangles to match the annotation for each dataset , and use toolbox from for the evaluation .",Experiments,Comparing with state - of - the - art,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.986842105263158,288,0.976271186440678,24,0.96,1,baselines,Experiments: Comparing with state - of - the - art,face_detection21
2237,2,A Fast and Accurate Unconstrained Face Detector,title,,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0024449877750611,1,0.0,1,research-problem,title,face_detection3
2238,4,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .",abstract,abstract,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.0073349633251833,1,0.125,1,research-problem,abstract,face_detection3
2239,13,The objective of face detection is to find and locate faces in an image .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0047393364928909,12,0.0293398533007334,1,0.0217391304347826,1,research-problem,INTRODUCTION,face_detection3
2240,14,It is the first step in automatic face recognition applications .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0094786729857819,13,0.0317848410757946,2,0.0434782608695652,1,research-problem,INTRODUCTION,face_detection3
2241,19,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0331753554502369,18,0.0440097799511002,7,0.1521739130434782,1,research-problem,INTRODUCTION,face_detection3
2242,34,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.1042654028436018,33,0.0806845965770171,22,0.4782608695652174,1,model,INTRODUCTION,face_detection3
2243,36,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",24,0.1137440758293838,35,0.0855745721271393,24,0.5217391304347826,1,model,INTRODUCTION,face_detection3
2244,37,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",25,0.1184834123222748,36,0.0880195599022004,25,0.5434782608695652,1,model,INTRODUCTION,face_detection3
2245,38,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",26,0.1232227488151658,37,0.0904645476772616,26,0.5652173913043478,1,model,INTRODUCTION,face_detection3
2246,39,"While individual NPD features may have "" weak "" discriminative ability , our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",27,0.1279620853080568,38,0.0929095354523227,27,0.5869565217391305,1,model,INTRODUCTION,face_detection3
2247,40,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",28,0.1327014218009478,39,0.0953545232273838,28,0.6086956521739131,1,model,INTRODUCTION,face_detection3
2248,41,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",29,0.1374407582938388,40,0.097799511002445,29,0.6304347826086957,1,model,INTRODUCTION,face_detection3
2249,42,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",30,0.1421800947867298,41,0.1002444987775061,30,0.6521739130434783,1,model,INTRODUCTION,face_detection3
2250,44,"A new type of feature , called NPD is proposed , which is efficient to compute and has several desirable properties , including scale invariance , boundedness , and enabling reconstruction of the original image .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-p', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",32,0.1516587677725118,43,0.1051344743276283,32,0.6956521739130435,1,model,INTRODUCTION,face_detection3
2251,45,A deep quadratic tree learner is proposed to learn and combine an optimal subset of NPD features to boost their discriminability .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",33,0.1563981042654028,44,0.1075794621026894,33,0.717391304347826,1,model,INTRODUCTION,face_detection3
2252,46,"In this way , only a single soft - cascade AdaBoost classifier is needed to handle unconstrained faces with occlusions and arbitrary viewpoints , without pose labeling or clustering in the training stage .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",34,0.1611374407582938,45,0.1100244498777506,34,0.7391304347826086,1,model,INTRODUCTION,face_detection3
2253,50,The unconstrained face detector does not depend on pose specific cascade structure design ; pose labeling or clustering in the training stage is also not required .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",38,0.1800947867298578,49,0.1198044009779951,38,0.8260869565217391,1,model,INTRODUCTION,face_detection3
2254,52,The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",40,0.1895734597156398,51,0.1246943765281173,40,0.8695652173913043,1,code,INTRODUCTION,face_detection3
2255,90,"In this paper , we show that the optimal ordinal / contrastive features and their combinations can be learned by integrating the proposed NPD features in a deep quadratic tree .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",78,0.3696682464454976,89,0.2176039119804401,31,0.2695652173913043,1,model,INTRODUCTION,face_detection3
2256,91,"In this way , unconstrained face variations can be automatically partitioned into different leaves of the learned quadratic tree classifier .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",79,0.3744075829383886,90,0.2200488997555012,32,0.2782608695652174,1,model,INTRODUCTION,face_detection3
2257,144,The NPD feature measures the relative difference between two pixel values .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",132,0.6255924170616114,143,0.3496332518337408,85,0.7391304347826086,1,model,INTRODUCTION,face_detection3
2258,147,"Compared to the absolute difference | x ? y| , NPD is invariant to scale change of the pixel intensities .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'O']",135,0.6398104265402843,146,0.3569682151589242,88,0.7652173913043478,1,model,INTRODUCTION,face_detection3
2259,175,NPD FOR FACE DETECTION,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n']",163,0.7725118483412322,174,0.4254278728606357,0,0.0,1,research-problem,INTRODUCTION,face_detection3
2260,231,"For bootstrapping nonface images , we also used the AFLW images , but masked the facial regions with random images containing no faces , as shown in .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",7,0.3043478260869565,230,0.5623471882640587,7,0.4375,1,experiments,Implementation Details,face_detection3
2261,232,We used a detection template of 24 24 pixels .,Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.3478260869565217,231,0.5647921760391198,8,0.5,1,hyperparameters,Implementation Details,face_detection3
2262,233,"We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.391304347826087,232,0.5672371638141809,9,0.5625,1,hyperparameters,Implementation Details,face_detection3
2263,235,"Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",11,0.4782608695652174,234,0.5721271393643031,11,0.6875,1,hyperparameters,Implementation Details,face_detection3
2264,239,The detection template is 20 20 pixels .,Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",15,0.6521739130434783,238,0.5819070904645477,15,0.9375,1,hyperparameters,Implementation Details,face_detection3
2265,240,"The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",16,0.6956521739130435,239,0.5843520782396088,16,1.0,1,hyperparameters,Implementation Details,face_detection3
2266,251,"In the test stage , a scale factor of 1.2 was set for multiscale detection .",EXPERIMENTS,EXPERIMENTS,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",3,0.6,250,0.6112469437652812,3,0.6,1,hyperparameters,EXPERIMENTS,face_detection3
2267,266,The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.3,265,0.6479217603911981,12,0.3,1,results,Evaluation on FDDB Database,face_detection3
2268,274,"It can be observed that the proposed NPD detector is among the top performers for the discrete metric , though it is not as good as the four recent methods for the continuous metric .",Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5,273,0.6674816625916871,20,0.5,1,results,Evaluation on FDDB Database,face_detection3
2269,278,"Compared to recent methods , the Joint Cascade algorithm is the most competitive one to us in terms of accuracy and speed ( see Sec. 5.6 ) .",Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.6,277,0.6772616136919315,24,0.6,1,results,Evaluation on FDDB Database,face_detection3
2270,286,The performance of the Zhu-Ramanan model is quite impressive considering such a small training data .,Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.8,285,0.6968215158924206,32,0.8,1,results,Evaluation on FDDB Database,face_detection3
2271,291,"Many rotated , occluded , and out - of - focus faces can be successfully detected by the proposed method .",Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",37,0.925,290,0.7090464547677262,37,0.925,1,results,Evaluation on FDDB Database,face_detection3
2272,307,The results show that the proposed NPD face detector significantly outperforms both the Viola - Jones and PittPatt face detectors .,Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.1935483870967742,306,0.7481662591687042,12,0.4285714285714285,1,results,Evaluation on GENKI Database,face_detection3
2273,308,Evaluation on CMU - MIT Database,Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",13,0.2096774193548387,307,0.7506112469437652,13,0.4642857142857143,1,results,Evaluation on GENKI Database,face_detection3
2274,318,"detector is better when FP < 3 , but SURF cascade method outperforms NPD at higher FPs .",Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",23,0.3709677419354839,317,0.7750611246943765,23,0.8214285714285714,1,results,Evaluation on GENKI Database,face_detection3
2275,321,"In addition , the proposed NPD method is not as good as the Soft cascade , the state - of - the - art method on the CMU - MIT dataset .",Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",26,0.4193548387096774,320,0.78239608801956,26,0.9285714285714286,1,results,Evaluation on GENKI Database,face_detection3
2276,322,"Still , the proposed NPD method can detect about 80 % of the frontal faces without any false positives , which is promising .",Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",27,0.4354838709677419,321,0.784841075794621,27,0.9642857142857144,1,results,Evaluation on GENKI Database,face_detection3
2277,339,"The NPD detector performs better than the Haar , LBP , and POF detectors with the same CART based weak learners .",Evaluation on GENKI Database,Analysis of the Proposed Face Detector,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",44,0.7096774193548387,338,0.8264058679706602,15,0.4545454545454545,1,results,Evaluation on GENKI Database: Analysis of the Proposed Face Detector,face_detection3
2278,340,"The performance improvements due to NPD features over Haar , LBP , and POF features are about 6 % , 19 % , and 15 % , respectively , for discrete metric , and about 4 % , 13 % , and 10 % , respectively , for continuous metric , at FP = 1 .",Evaluation on GENKI Database,Analysis of the Proposed Face Detector,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O']",45,0.7258064516129032,339,0.8288508557457213,16,0.4848484848484848,1,results,Evaluation on GENKI Database: Analysis of the Proposed Face Detector,face_detection3
2279,341,"NPD is better than POF , because with NPD features the regression tree learns optimal thresholds to form more robust ordinal rules .",Evaluation on GENKI Database,Analysis of the Proposed Face Detector,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.7419354838709677,340,0.8312958435207825,17,0.5151515151515151,1,results,Evaluation on GENKI Database: Analysis of the Proposed Face Detector,face_detection3
2280,342,"NPD performs better than Haar and LBP , especially at low false positives , indicating that combining optimal pixel - level features in regression trees provides better discrimination between faces and nonfaces .",Evaluation on GENKI Database,Analysis of the Proposed Face Detector,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.7580645161290323,341,0.8337408312958435,18,0.5454545454545454,1,results,Evaluation on GENKI Database: Analysis of the Proposed Face Detector,face_detection3
2281,352,"As illustrated , using CART instead of stump classifier improves the face detection performance by about 0 % - 17 % for discrete metric and 0 % - 11 % for continuous metric .",Evaluation on GENKI Database,Analysis of the Proposed Face Detector,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",57,0.9193548387096774,351,0.8581907090464548,28,0.8484848484848485,1,results,Evaluation on GENKI Database: Analysis of the Proposed Face Detector,face_detection3
2282,355,"Besides , the DQT based detector further improves the performance , due to its quadratic splitting capability compared to linear splitting .",Evaluation on GENKI Database,Analysis of the Proposed Face Detector,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",60,0.967741935483871,354,0.8655256723716381,31,0.9393939393939394,1,results,Evaluation on GENKI Database: Analysis of the Proposed Face Detector,face_detection3
2283,363,"shows that the NPD face detector performs the best on the pose and illumination subsets , thanks to the scale - invariant NPD features and the deep quadratic trees .",Evaluation Under Specific Challenges,Evaluation Under Specific Challenges,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.0980392156862745,362,0.8850855745721271,5,0.4166666666666667,1,results,Evaluation Under Specific Challenges,face_detection3
2284,381,The original resolution is 1280 720 .,Evaluation Under Specific Challenges,Detection Speed,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",23,0.4509803921568627,380,0.9290953545232272,10,0.3333333333333333,1,results,Evaluation Under Specific Challenges: Detection Speed,face_detection3
2285,399,The NPD detector achieves similar speed as that of Joint Cascade method .,Evaluation Under Specific Challenges,Detection Speed,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",41,0.803921568627451,398,0.9731051344743276,28,0.9333333333333332,1,results,Evaluation Under Specific Challenges: Detection Speed,face_detection3
2286,403,We have proposed a fast and accurate method for face detection in cluttered scenes .,Evaluation Under Specific Challenges,Detection Speed,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",45,0.8823529411764706,402,0.982885085574572,1,0.1428571428571428,1,baselines,Evaluation Under Specific Challenges: Detection Speed,face_detection3
2287,2,LFFD : A Light and Fast Face Detector for Edge Devices,title,title,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0029498525073746,1,0.0,1,research-problem,title,face_detection4
2288,4,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .",abstract,abstract,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,3,0.0088495575221238,1,0.0625,1,research-problem,abstract,face_detection4
2289,14,"Under the new schema , the proposed method can achieve superior accuracy ( WIDER FACE Val / Test - Easy : 0.910/0.896 , Medium : 0.881/0.865 , Hard : 0.780/0.770 ; FDDB - discontinuous : 0.973 , continuous : 0.724 ) .",abstract,abstract,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,13,0.0383480825958702,11,0.6875,1,research-problem,abstract,face_detection4
2290,21,Face detection is a long - standing problem in computer vision .,Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0222222222222222,20,0.0589970501474926,1,0.0222222222222222,1,research-problem,Introduction,face_detection4
2291,31,Face detection is a fast - growing branch of general object detection in the past decade .,Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2444444444444444,30,0.0884955752212389,11,0.2444444444444444,1,research-problem,Introduction,face_detection4
2292,33,One of its well - known followers is aggregate channel features ( ACF ) which can take advantages of channel features effectively .,Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O']",13,0.2888888888888888,32,0.0943952802359882,13,0.2888888888888888,1,model,Introduction,face_detection4
2293,37,"Two - stage methods consist of proposal selection and localization regression , which are mainly originated from R - CNN series .",Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3777777777777777,36,0.1061946902654867,17,0.3777777777777777,1,model,Introduction,face_detection4
2294,42,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .",Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",22,0.4888888888888889,41,0.1209439528023598,22,0.4888888888888889,1,model,Introduction,face_detection4
2295,211,We flip the cropped image with probability of 0.5 .,Training Details,Randomly horizontal flip .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",15,0.5172413793103449,210,0.6194690265486725,15,0.3846153846153846,1,experimental-setup,Training Details: Randomly horizontal flip .,face_detection4
2296,214,"For face classification , we use softmax with cross - entropy loss over two classes .",Training Details,Loss function .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",18,0.6206896551724138,213,0.6283185840707964,18,0.4615384615384615,1,experimental-setup,Training Details: Loss function .,face_detection4
2297,218,"For bbox regression , we adopt L2 loss directly .",Training Details,Loss function .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O']",22,0.7586206896551724,217,0.640117994100295,22,0.5641025641025641,1,experimental-setup,Training Details: Loss function .,face_detection4
2298,227,We initialize all parameters with xavier method and train the network from scratch .,Training parameters .,Training parameters .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-n', 'O']",1,0.1111111111111111,226,0.6666666666666666,31,0.7948717948717948,1,experimental-setup,Training parameters .,face_detection4
2299,229,"The optimization method is SGD with 0.9 momentum , zero weight decay and batch size 32 .",Training parameters .,Training parameters .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O']",3,0.3333333333333333,228,0.672566371681416,33,0.8461538461538461,1,experimental-setup,Training parameters .,face_detection4
2300,232,The initial learning rate is 0.1 .,Training parameters .,,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.6666666666666666,231,0.6814159292035398,36,0.9230769230769232,1,experimental-setup,Training parameters .,face_detection4
2301,233,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .",Training parameters .,The initial learning rate is 0.1 .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.7777777777777778,232,0.6843657817109144,37,0.9487179487179488,1,experimental-setup,Training parameters .: The initial learning rate is 0.1 .,face_detection4
2302,234,The training time is about 5 days with two NVIDIA GTX 1080 TI .,Training parameters .,The initial learning rate is 0.1 .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.8888888888888888,233,0.6873156342182891,38,0.9743589743589745,1,experimental-setup,Training parameters .: The initial learning rate is 0.1 .,face_detection4
2303,268,"DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .",Evaluation on Benchmarks,And the second criterion directly uses IOU ratios .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",27,0.3,267,0.7876106194690266,27,0.5192307692307693,1,results,Evaluation on Benchmarks: And the second criterion directly uses IOU ratios .,face_detection4
2304,285,"Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .",Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.4888888888888889,284,0.8377581120943953,44,0.8461538461538461,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
2305,289,We can see that the results on Medium and Hard parts are improved remarkably .,Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'O']",48,0.5333333333333333,288,0.8495575221238938,48,0.9230769230769232,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
2306,292,"Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .",Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",51,0.5666666666666667,291,0.8584070796460177,51,0.9807692307692308,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
2307,298,"For fair comparison , FaceBoxes 3.2 is used here instead of FaceBoxes .",Evaluation on Benchmarks,Running Efficiency,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",57,0.6333333333333333,297,0.8761061946902655,4,0.1081081081081081,1,results,Evaluation on Benchmarks: Running Efficiency,face_detection4
2308,305,"The proposed LFFD runs the fastest at 38402160 , and FaceBoxes 3.2 obtains the highest speed at other three resolutions .",Evaluation on Benchmarks,Running Efficiency,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",64,0.7111111111111111,304,0.8967551622418879,11,0.2972972972972973,1,results,Evaluation on Benchmarks: Running Efficiency,face_detection4
2309,5,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",abstract,abstract,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3333333333333333,4,0.0263157894736842,2,0.25,1,research-problem,abstract,face_detection5
2310,76,2 ) Bounding box regression :,C. Training,C. Training,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.1714285714285714,75,0.4934210526315789,6,0.1428571428571428,1,baselines,C. Training,face_detection5
2311,87,4 ) Multi-source training :,C. Training,C. Training,face_detection,5,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O']",17,0.4857142857142857,86,0.5657894736842105,17,0.4047619047619047,1,baselines,C. Training,face_detection5
2312,98,5 ) Online Hard sample mining :,C. Training,C. Training,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.8,97,0.6381578947368421,28,0.6666666666666666,1,baselines,C. Training,face_detection5
2313,117,"1 ) P- Net : We randomly crop several patches from WIDER FACE to collect positives , negatives and part face .",A. Training Data,A. Training Data,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'I-n', 'O', 'B-n', 'I-n', 'O']",4,0.1142857142857142,116,0.7631578947368421,4,0.2105263157894736,1,baselines,A. Training Data,face_detection5
2314,119,"2 ) R - Net : We use first stage of our framework to detect faces from WIDER FACE to collect positives , negatives and part face while landmark faces are detected from CelebA .",A. Training Data,A. Training Data,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",6,0.1714285714285714,118,0.7763157894736842,6,0.3157894736842105,1,baselines,A. Training Data,face_detection5
2315,120,3 ) O - Net : Similar to R - Net to collect data but we use first two stages of our framework to detect faces .,A. Training Data,A. Training Data,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",7,0.2,119,0.7828947368421053,7,0.3684210526315789,1,baselines,A. Training Data,face_detection5
2316,132,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,A. Training Data,C.,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.5428571428571428,131,0.8618421052631579,19,1.0,1,experiments,A. Training Data: C.,face_detection5
2317,2,Robust Face Detection via Learning Small Faces on Hard Images,title,title,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0040322580645161,1,0.0,1,research-problem,title,face_detection6
2318,13,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0277777777777777,12,0.0483870967741935,1,0.0277777777777777,1,research-problem,Introduction,face_detection6
2319,20,"In , we show that , even on the train set of WIDER FACE , the official pre-trained SSH 1 still fails on some of the images with extremely hard faces .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.2222222222222222,19,0.0766129032258064,8,0.2222222222222222,1,experiments,Introduction,face_detection6
2320,24,"To address this issue , in this paper , we propose a robust face detector by putting more training focus on those hard images .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",12,0.3333333333333333,23,0.0927419354838709,12,0.3333333333333333,1,model,Introduction,face_detection6
2321,27,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",15,0.4166666666666667,26,0.1048387096774193,15,0.4166666666666667,1,model,Introduction,face_detection6
2322,28,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O']",16,0.4444444444444444,27,0.1088709677419354,16,0.4444444444444444,1,model,Introduction,face_detection6
2323,31,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",19,0.5277777777777778,30,0.1209677419354838,19,0.5277777777777778,1,model,Introduction,face_detection6
2324,37,"To conclude , in this paper , we propose a novel face detector with the following contributions :",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",25,0.6944444444444444,36,0.1451612903225806,25,0.6944444444444444,1,model,Introduction,face_detection6
2325,39,"This is done without any extra modules , parameters or computation overhead added on the existing detector .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",27,0.75,38,0.1532258064516129,27,0.75,1,model,Introduction,face_detection6
2326,40,"We design a single shot detector with only one detection feature map , which focuses on small faces with a specific range of sizes .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.7777777777777778,39,0.157258064516129,28,0.7777777777777778,1,model,Introduction,face_detection6
2327,163,"We flip all images horizontally , to double the size of our training dataset to 25760 .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.2,162,0.6532258064516129,2,0.2,1,experimental-setup,Experimental settings,face_detection6
2328,165,"We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-p', 'B-n', 'I-n', 'O']",4,0.4,164,0.6612903225806451,4,0.4,1,experimental-setup,Experimental settings,face_detection6
2329,166,"We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",5,0.5,165,0.6653225806451613,5,0.5,1,experimental-setup,Experimental settings,face_detection6
2330,167,"During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.6,166,0.6693548387096774,6,0.6,1,experimental-setup,Experimental settings,face_detection6
2331,168,"The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.7,167,0.6733870967741935,7,0.7,1,experimental-setup,Experimental settings,face_detection6
2332,175,"As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",3,0.0441176470588235,174,0.7016129032258065,3,0.2,1,results,Experiment results,face_detection6
2333,177,"Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - thearts .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0735294117647058,176,0.7096774193548387,5,0.3333333333333333,1,results,Experiment results,face_detection6
2334,185,"We show the PR curve at compared with , and our method achieves a new the state - of - the - art performance of AP = 99.0 .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",13,0.1911764705882352,184,0.7419354838709677,13,0.8666666666666667,1,results,Experiment results,face_detection6
2335,187,"As shown in compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",15,0.2205882352941176,186,0.75,15,1.0,1,results,Experiment results,face_detection6
2336,191,"From , we can see that our single level baseline model can achieve performance comparable to the current : Ablation experiments .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.2794117647058823,190,0.7661290322580645,3,0.2307692307692307,1,results,Experiment results,face_detection6
2337,192,Baseline - Three is a face detector similar to SSH with three detection feature maps .,Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.2941176470588235,191,0.7701612903225806,4,0.3076923076923077,1,baselines,Experiment results,face_detection6
2338,193,Baseline - Single is our proposed detector with single detection feature map shown in .,Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",21,0.3088235294117647,192,0.7741935483870968,5,0.3846153846153846,1,baselines,Experiment results,face_detection6
2339,196,"Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3529411764705882,195,0.7862903225806451,8,0.6153846153846154,1,results,Experiment results,face_detection6
2340,201,Combining HIM and DH together can improve further towards the state - of - the - art performance .,Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",29,0.4264705882352941,200,0.8064516129032258,13,1.0,1,results,Experiment results,face_detection6
2341,210,The ablation results evaluated on WIDER FACE val dataset are shown in .,Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",38,0.5588235294117647,209,0.842741935483871,2,0.6666666666666666,1,results,Experiment results,face_detection6
2342,217,"Our full evaluation resizes the image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels respectively , to build an image pyramid .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",45,0.6617647058823529,216,0.8709677419354839,5,0.5555555555555556,1,results,Experiment results,face_detection6
2343,220,"Without resizing the short side to contain 100 and 300 pixels , the performance on easy subset is only 78.2 , which is even lower than the performance on medium and hard which contain much harder faces .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.7058823529411765,219,0.8830645161290323,8,0.8888888888888888,1,results,Experiment results,face_detection6
2344,225,"For fair comparison , we run all methods on the same machine , with one Titan X ( Maxwell ) GPU , and Intel",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n']",53,0.7794117647058824,224,0.9032258064516128,3,0.1666666666666666,1,results,Experiment results,face_detection6
2345,228,"All methods except for Pyramid Box are based on Caffe1 implementation , which is compiled with CUDA 9.0 and CUDNN 7 .",Experiment results,Core i7-4770K,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",56,0.8235294117647058,227,0.9153225806451613,6,0.3333333333333333,1,results,Experiment results: Core i7-4770K,face_detection6
2346,230,We use the officially built Pad - dlePaddle with CUDA 9.0 and CUDNN 7 .,Experiment results,Core i7-4770K,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",58,0.8529411764705882,229,0.9233870967741936,8,0.4444444444444444,1,results,Experiment results: Core i7-4770K,face_detection6
2347,235,"As shown in , our detector can outperform SSH , S 3 FD and PyramidBox significantly with a smaller inference time .",Experiment results,Pyramid,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",63,0.9264705882352942,234,0.9435483870967742,13,0.7222222222222222,1,results,Experiment results: Pyramid,face_detection6
2348,2,Recurrent Scale Approximation for Object Detection in CNN,title,,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O']",1,0.0,1,0.0035971223021582,1,0.0,1,research-problem,title,face_detection7
2349,14,Object detection is one of the most important tasks in computer vision .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0263157894736842,13,0.0467625899280575,1,0.0263157894736842,1,research-problem,Introduction,face_detection7
2350,18,"Most of the appearance variations can now be handled in CNN , benefiting from the invariance property of convolution and pooling operations .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O']",5,0.131578947368421,17,0.0611510791366906,5,0.131578947368421,1,model,Introduction,face_detection7
2351,19,"The location variations can be naturally solved via sliding windows , which can be efficiently incorporated into CNN in a fully convolutional manner .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.1578947368421052,18,0.0647482014388489,6,0.1578947368421052,1,model,Introduction,face_detection7
2352,22,"The first way , as shown in , handles objects of different scales independently by resizing the input into different scales and then forwarding the resized images multiple times for detection .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",9,0.2368421052631578,21,0.0755395683453237,9,0.2368421052631578,1,model,Introduction,face_detection7
2353,24,"The second way , as depicted in , forwards the image only once and then directly regresses objects at multiple scales .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.2894736842105263,23,0.0827338129496402,11,0.2894736842105263,1,model,Introduction,face_detection7
2354,31,"Our solution to the feature pyramid in CNN descends from the observations of modern CNN - based detectors , including Faster - RCNN , R - FCN , SSD , YOLO and STN , where feature maps are first computed and the detection results are decoded from the maps afterwards .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4736842105263157,30,0.1079136690647482,18,0.4736842105263157,1,model,Introduction,face_detection7
2355,35,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O']",22,0.5789473684210527,34,0.1223021582733813,22,0.5789473684210527,1,model,Introduction,face_detection7
2356,36,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",23,0.6052631578947368,35,0.1258992805755395,23,0.6052631578947368,1,model,Introduction,face_detection7
2357,37,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-p', 'O', 'B-n', 'O']",24,0.631578947368421,36,0.1294964028776978,24,0.631578947368421,1,model,Introduction,face_detection7
2358,40,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",27,0.7105263157894737,39,0.1402877697841726,27,0.7105263157894737,1,model,Introduction,face_detection7
2359,42,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",29,0.7631578947368421,41,0.1474820143884892,29,0.7631578947368421,1,model,Introduction,face_detection7
2360,46,The three components can be incorporated into a unified CNN framework and trained end - to - end .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",33,0.868421052631579,45,0.1618705035971223,33,0.868421052631579,1,model,Introduction,face_detection7
2361,49,"1 ) We prove that deep CNN features for an image can be approximated from different scales using a portable recurrent unit ( RSA ) , which fully leverages efficiency and accuracy .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O']",36,0.9473684210526316,48,0.1726618705035971,36,0.9473684210526316,1,model,Introduction,face_detection7
2362,179,We use this model in scale - forecast network and LRN .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.1067961165048543,178,0.6402877697841727,8,0.2857142857142857,1,baselines,Setup and Implementation Details: Face Detection,face_detection7
2363,180,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.116504854368932,179,0.6438848920863309,9,0.3214285714285714,1,hyperparameters,Setup and Implementation Details: Face Detection,face_detection7
2364,181,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.1262135922330097,180,0.6474820143884892,10,0.3571428571428571,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2365,184,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1553398058252427,183,0.658273381294964,13,0.4642857142857143,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2366,185,"The maximum training iteration is 1,000,000 .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",17,0.1650485436893204,184,0.6618705035971223,14,0.5,1,hyperparameters,Setup and Implementation Details: Face Detection,face_detection7
2367,186,We use stochastic gradient descent as the optimizer .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",18,0.174757281553398,185,0.6654676258992805,15,0.5357142857142857,1,hyperparameters,Setup and Implementation Details: Face Detection,face_detection7
2368,190,"We can observe from the results that our trained scale network recalls almost 99 % at x = 1 , indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2135922330097087,189,0.6798561151079137,19,0.6785714285714286,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2369,195,"knowledge , during inference , we set the threshold for predicting potential scales of the input so that it has approximately two predictions .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",27,0.2621359223300971,194,0.697841726618705,24,0.8571428571428571,1,hyperparameters,Setup and Implementation Details: Face Detection,face_detection7
2370,199,Performance of Scale - forecast Network,Setup and Implementation Details,,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",31,0.3009708737864077,198,0.7122302158273381,28,1.0,1,results,Setup and Implementation Details,face_detection7
2371,200,Ablative Evaluation on RSA Unit,Setup and Implementation Details,,face_detection,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n']",32,0.3106796116504854,199,0.7158273381294964,0,0.0,1,baselines,Setup and Implementation Details,face_detection7
2372,203,"The image is first resized to higher dimension being 2048 and the RSA unit predicts six scales defined in Section 3.1 ( 1024 , 512 , 256 , 128 and 64 ) .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.3398058252427184,202,0.7266187050359713,3,0.125,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2373,207,"However , results from the figure indicate that as we plug RSA at deeper layers , its performance decades .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",39,0.3786407766990291,206,0.7410071942446043,7,0.2916666666666667,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2374,209,"For example , in case final feature which means RSA is plugged at the final convolution layer after res3c , the error rate is almost 100 % , indicating RSA 's incapability of handling the insufficient information in this layer .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3980582524271844,208,0.7482014388489209,9,0.375,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2375,210,The error rate decreases in shallower cases .,Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-n', 'I-p', 'B-n', 'I-n', 'O']",42,0.4077669902912621,209,0.7517985611510791,10,0.4166666666666667,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2376,216,The path during one - time forward from image to the input map right before RSA is shorter ; and the rolling out time increases accordingly .,Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'O']",48,0.4660194174757281,215,0.7733812949640287,16,0.6666666666666666,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2377,219,Most of the computation happens before layer res2 b and it has an acceptable error rate of 3.44 % .,Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",51,0.4951456310679611,218,0.7841726618705036,19,0.7916666666666666,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2378,225,Our Algorithm vs. Baseline RPN,Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",57,0.5533980582524272,224,0.8057553956834532,0,0.0,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2379,226,"We compare our model ( denoted as RSA + LRN ) , a combination of the RSA unit and a landmark retracing network , with the region proposal network ( RPN ) .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",58,0.5631067961165048,225,0.8093525179856115,1,0.037037037037037,1,baselines,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2380,227,"In the first setting , we use the original RPN with multiple anchors ( denoted as RPN m ) to detect faces of various scales .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",59,0.5728155339805825,226,0.8129496402877698,2,0.074074074074074,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2381,242,"On AFW , our algorithm achieves an AP of 99.17 % using the original annotation and an AP of 99 . 96 % using the revised annotation 7 ( c ) .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",74,0.7184466019417476,241,0.8669064748201439,17,0.6296296296296297,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2382,243,"On FDDB , RSA + LRN recalls 93.0 % faces with 50 false positives 7 ( a ) .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",75,0.7281553398058253,242,0.8705035971223022,18,0.6666666666666666,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2383,244,"On MALF , our method recalls 82.4 % faces with zero false positive 7 ( d ) .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",76,0.7378640776699029,243,0.8741007194244604,19,0.7037037037037037,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2384,247,"To address this , we learn a transformer to fit each annotation from the landmarks .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",79,0.7669902912621359,246,0.8848920863309353,22,0.8148148148148148,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2385,249,"Our proposed model can detect faces at various scales , including the green annotations provided in AFW as well as faces marked in red that are of small sizes and not labeled in the dataset ..",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O']",81,0.7864077669902912,248,0.8920863309352518,24,0.8888888888888888,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2386,251,"The proposed algorithm ( Scale - forecast network with RSA + LRN , tagged by LRN + RSA ) outperforms other methods by a large margin .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",83,0.8058252427184466,250,0.8992805755395683,26,0.9629629629629628,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2387,254,RSA on Generic Object Proposal,Setup and Implementation Details,,face_detection,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n']",86,0.8349514563106796,253,0.9100719424460432,0,0.0,1,experiments,Setup and Implementation Details,face_detection7
2388,255,We now verify that the scale approximation learning by RSA unit also generalizes comparably well on the generic region proposal task .,Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",87,0.8446601941747572,254,0.9136690647482014,1,0.0588235294117647,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2389,257,ILSVRC DET is a challenging dataset for generic object detection .,Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",89,0.8640776699029126,256,0.920863309352518,3,0.1764705882352941,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2390,260,We choose the single anchor RPN with ResNet - 101 as the baseline .,Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",92,0.8932038834951457,259,0.9316546762589928,6,0.3529411764705882,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2391,262,"The anchors are of size 128 ? 2 squared , 128256 and 256128 .",Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']",94,0.912621359223301,261,0.9388489208633094,8,0.4705882352941176,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2392,264,Scale - forecast network is also employed to predict the higher dimension of objects in the image .,Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",96,0.9320388349514565,263,0.946043165467626,10,0.5882352941176471,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2393,267,"Without loss of recall , RPN + RSA reduces around 61.05 % computation cost compared with the single - scale RPN , when the number of boxes is over 100 .",Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",99,0.9611650485436892,266,0.9568345323741008,13,0.7647058823529411,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2394,268,RPN + RSA is also more efficient and recalls more objects than original RPN .,Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",100,0.970873786407767,267,0.960431654676259,14,0.8235294117647058,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2395,269,Our model and the single - anchor RPN both perform better than the original RPN .,Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",101,0.9805825242718448,268,0.9640287769784172,15,0.8823529411764706,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2396,271,"Overall , our scheme of using RSA plus LRN competes comparably with the standard RPN method in terms of computation efficiency and accuracy .",Setup and Implementation Details,RSA on Generic Object Proposal,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",103,1.0,270,0.9712230215827338,17,1.0,1,experiments,Setup and Implementation Details: RSA on Generic Object Proposal,face_detection7
2397,2,Detecting Faces Using Region - based Fully Convolutional Networks,title,,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0064102564102564,1,0.0,1,research-problem,title,face_detection8
2398,5,"In this report , we propose a region - based face detector applying deep networks in a fully convolutional fashion , named Face R - FCN .",abstract,abstract,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.3333333333333333,4,0.0256410256410256,2,0.3333333333333333,1,research-problem,abstract,face_detection8
2399,6,"Based on Region - based Fully Convolutional Networks ( R - FCN ) , our face detector is more accurate and computationally efficient compared with the previous R - CNN based face detectors .",abstract,abstract,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.5,5,0.032051282051282,3,0.5,1,research-problem,abstract,face_detection8
2400,11,Face detection plays an important role in the modern face - relevant applications .,Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0454545454545454,10,0.0641025641025641,1,0.0454545454545454,1,research-problem,Introduction,face_detection8
2401,18,"The ConvNet of R - FCN is built with the computations shared on the entire image , which leads to the improvement of training and testing efficiency .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.3636363636363636,17,0.1089743589743589,8,0.3636363636363636,1,approach,Introduction,face_detection8
2402,22,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O']",12,0.5454545454545454,21,0.1346153846153846,12,0.5454545454545454,1,approach,Introduction,face_detection8
2403,24,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",14,0.6363636363636364,23,0.1474358974358974,14,0.6363636363636364,1,approach,Introduction,face_detection8
2404,26,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",16,0.7272727272727273,25,0.1602564102564102,16,0.7272727272727273,1,approach,Introduction,face_detection8
2405,29,"The proposed approach is based on R - FCN and is well suited for face detection , thus we call it Face R - FCN .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.8636363636363636,28,0.1794871794871795,19,0.8636363636363636,1,approach,Introduction,face_detection8
2406,108,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",2,0.048780487804878,107,0.6858974358974359,2,0.1333333333333333,1,hyperparameters,Implementation Details,face_detection8
2407,109,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",3,0.073170731707317,108,0.6923076923076923,3,0.2,1,hyperparameters,Implementation Details,face_detection8
2408,110,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']",4,0.0975609756097561,109,0.6987179487179487,4,0.2666666666666666,1,baselines,Implementation Details,face_detection8
2409,111,We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.1219512195121951,110,0.7051282051282052,5,0.3333333333333333,1,hyperparameters,Implementation Details,face_detection8
2410,115,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",9,0.2195121951219512,114,0.7307692307692307,9,0.6,1,baselines,Implementation Details,face_detection8
2411,116,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",10,0.2439024390243902,115,0.7371794871794872,10,0.6666666666666666,1,hyperparameters,Implementation Details,face_detection8
2412,117,The proposals are processed by OHEM to train with hard examples .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",11,0.2682926829268293,116,0.7435897435897436,11,0.7333333333333333,1,hyperparameters,Implementation Details,face_detection8
2413,118,We set the 256 for the size of RPN mini-batch and 128 for R - FCN respectively .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O']",12,0.2926829268292683,117,0.75,12,0.8,1,hyperparameters,Implementation Details,face_detection8
2414,120,"We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.3414634146341463,119,0.7628205128205128,14,0.9333333333333332,1,hyperparameters,Implementation Details,face_detection8
2415,121,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.3658536585365853,120,0.7692307692307693,15,1.0,1,hyperparameters,Implementation Details,face_detection8
2416,125,"As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .",Implementation Details,Comparison on Benchmarks,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",19,0.4634146341463415,124,0.7948717948717948,2,0.6666666666666666,1,results,Implementation Details: Comparison on Benchmarks,face_detection8
2417,126,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .",Implementation Details,Comparison on Benchmarks,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4878048780487805,125,0.8012820512820513,3,1.0,1,results,Implementation Details: Comparison on Benchmarks,face_detection8
2418,130,We use the training set of the WIDER FACE dataset to train our model ( denoted as Model - A in ) and compare against the recently published top approaches on FDDB .,Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']",24,0.5853658536585366,129,0.8269230769230769,3,0.15,1,experiments,Implementation Details: FDDB,face_detection8
2419,133,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .",Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",27,0.6585365853658537,132,0.8461538461538461,6,0.3,1,experiments,Implementation Details: FDDB,face_detection8
2420,134,Our discrete ROC curve is superior to the prior best - performing method .,Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.6829268292682927,133,0.8525641025641025,7,0.35,1,experiments,Implementation Details: FDDB,face_detection8
2421,135,We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",29,0.7073170731707317,134,0.8589743589743589,8,0.4,1,experiments,Implementation Details: FDDB,face_detection8
2422,139,But the competitive result we achieved is still noticeable .,Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'O']",33,0.8048780487804879,138,0.8846153846153846,12,0.6,1,experiments,Implementation Details: FDDB,face_detection8
2423,141,"Face R - FCN shows the superior performance over the prior methods across the three subsets ( easy , medium and hard ) in both validation and test sets .",Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",35,0.8536585365853658,140,0.8974358974358975,14,0.7,1,experiments,Implementation Details: FDDB,face_detection8
2424,146,"As expected , the performance of Face R - FCN is further improved .",Implementation Details,"Furthermore ,",face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",40,0.975609756097561,145,0.9294871794871796,19,0.95,1,results,"Implementation Details: Furthermore ,",face_detection8
2425,2,Finding Tiny Faces,title,title,face_detection,9,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",1,0.0,1,0.003610108303249,1,0.0,1,research-problem,title,face_detection9
2426,5,"We describe a detector that can find around 800 faces out of the reportedly 1000 present , by making use of novel characterizations of scale , resolution , and context to find small objects .",abstract,abstract,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",2,0.1538461538461538,4,0.0144404332129963,2,0.1538461538461538,1,research-problem,abstract,face_detection9
2427,23,We make use of a coarse image pyramid to capture extreme scale challenges in ( c ) .,Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",6,0.1276595744680851,22,0.0794223826714801,6,0.1276595744680851,1,approach,Introduction,face_detection9
2428,24,"Finally , to improve performance on small faces , we model additional context , which is efficiently implemented as a fixed - size receptive field across all scale - specific templates ( d ) .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",7,0.1489361702127659,23,0.0830324909747292,7,0.1489361702127659,1,approach,Introduction,face_detection9
2429,29,Multi - task modeling of scales :,Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.2553191489361702,28,0.1010830324909747,12,0.2553191489361702,1,research-problem,Introduction,face_detection9
2430,33,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.3404255319148936,32,0.1155234657039711,16,0.3404255319148936,1,approach,Introduction,face_detection9
2431,35,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.3829787234042553,34,0.1227436823104693,18,0.3829787234042553,1,approach,Introduction,face_detection9
2432,48,"In , we present a simple human experiment where users attempt to classify true and false positive faces ( as given by our detector ) .",Introduction,"This is often formulated as "" context "" .",face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6595744680851063,47,0.1696750902527075,31,0.6595744680851063,1,approach,"Introduction: This is often formulated as "" context "" .",face_detection9
2433,52,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .",Introduction,"This is often formulated as "" context "" .",face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",35,0.7446808510638298,51,0.1841155234657039,35,0.7446808510638298,1,approach,"Introduction: This is often formulated as "" context "" .",face_detection9
2434,53,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,Introduction,"This is often formulated as "" context "" .",face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.7659574468085106,52,0.187725631768953,36,0.7659574468085106,1,approach,"Introduction: This is often formulated as "" context "" .",face_detection9
2435,57,"In particular , when compared to prior art on WIDER FACE , our results reduce error by a factor of 2 ( our models produce an AP of 82 % while prior art ranges from 29 - 64 % ) . :",Introduction,"This is often formulated as "" context "" .",face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.851063829787234,56,0.2021660649819494,40,0.851063829787234,1,approach,"Introduction: This is often formulated as "" context "" .",face_detection9
2436,62,Adding a fixed contextual window of 300 pixels dramatically reduces error on small faces by 20 % .,Introduction,"This is often formulated as "" context "" .",face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",45,0.9574468085106383,61,0.2202166064981949,45,0.9574468085106383,1,experiments,"Introduction: This is often formulated as "" context "" .",face_detection9
2437,209,"We use a fixed learning rate of 10 ? 4 , a weight decay of 0.0005 , and a momentum of 0.9 .",Method,Other hyper - parameters,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'O']",14,0.6666666666666666,208,0.7509025270758123,14,0.6666666666666666,1,hyperparameters,Method: Other hyper - parameters,face_detection9
2438,220,"As shows , our hybrid - resolution model ( HR ) achieves state - of - the - art performance on all difficulty levels , but most importantly , reduces error on the "" hard "" set by 2X .",Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",3,0.0769230769230769,219,0.7906137184115524,3,0.081081081081081,1,results,Experiments,face_detection9
2439,226,"Our out - of - the - box detector ( HR ) outperforms all published results on the discrete score , which uses a standard 50 % intersection - over - union threshold to define correctness .",Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",9,0.2307692307692307,225,0.8122743682310469,9,0.2432432432432432,1,results,Experiments,face_detection9
2440,229,Our regressor is trained with 10 - fold cross validation .,Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.3076923076923077,228,0.8231046931407943,12,0.3243243243243243,1,hyperparameters,Experiments,face_detection9
2441,234,Our Resnet 101 - based detector runs at 1.4 FPS on 1080 p resolution and 3.1 FPS on 720 p resolution .,Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",17,0.4358974358974359,233,0.8411552346570397,17,0.4594594594594595,1,results,Experiments,face_detection9
2442,238,"We propose a simple yet effective framework for finding small objects , demonstrating that both large context and scale - variant representations are crucial .",Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5384615384615384,237,0.855595667870036,21,0.5675675675675675,1,baselines,Experiments,face_detection9
2443,239,We specifically show that massively - large receptive fields can be effectively encoded as a foveal descriptor that captures both coarse context ( necessary for detecting small objects ) and high - resolution image features ( helpful for localizing small objects ) .,Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5641025641025641,238,0.8592057761732852,22,0.5945945945945946,1,ablation-analysis,Experiments,face_detection9
2444,240,"We also explore the encoding of scale in existing pre-trained deep networks , suggesting a simple way to extrapolate networks tuned for limited scales to more extreme scenarios in a scale - variant fashion .",Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5897435897435898,239,0.8628158844765343,23,0.6216216216216216,1,ablation-analysis,Experiments,face_detection9
2445,244,"By learning a post - hoc regressor that converts bounding boxes to ellipses , our approach ( HR - ER ) produces state - of the - art continuous overlaps as well ( right ) .",Experiments,ROC curves on FDDB - test .,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6923076923076923,243,0.8772563176895307,27,0.7297297297297297,1,results,Experiments: ROC curves on FDDB - test .,face_detection9
2446,248,"Our proposed detector is able to detect faces at a continuous range of scales , while being robust to challenges such as expression , blur , illumination etc .",Experiments,Qualitative results on WIDER FACE .,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O']",31,0.7948717948717948,247,0.8916967509025271,31,0.8378378378378378,1,results,Experiments: Qualitative results on WIDER FACE .,face_detection9
2447,269,Online hard mining and balanced sampling,B. Experimental details,,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",12,0.6,268,0.967509025270758,7,0.4666666666666667,1,experiments,B. Experimental details,face_detection9
2448,272,We set a small threshold ( 0.03 ) on classification loss to filter out easy locations .,B. Experimental details,Online hard mining and balanced sampling,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O']",15,0.75,271,0.9783393501805054,10,0.6666666666666666,1,experiments,B. Experimental details: Online hard mining and balanced sampling,face_detection9
2449,277,Our detector is mostly affected by object scale ( from 0.044 to 0.896 ) and blur ( from 0.259 to 0.798 ) .,B. Experimental details,Summary of sensitivity plot .,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1.0,276,0.9963898916967509,15,1.0,1,results,B. Experimental details: Summary of sensitivity plot .,face_detection9
2450,2,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,title,title,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0125,1,0.0,1,research-problem,title,hypernym_discovery0
2451,4,This paper describes a simple but competitive unsupervised system for hypernym discovery .,abstract,abstract,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.1666666666666666,3,0.0375,1,0.1666666666666666,1,research-problem,abstract,hypernym_discovery0
2452,16,"This shared task differs from recent taxonomy evaluation tasks by concentrating on Hypernym Discovery : the task of predicting ( discovering ) n hypernym candidates for a given input word , within the vocabulary of a specific domain .",Introduction,Introduction,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1935483870967742,15,0.1875,6,0.3,1,research-problem,Introduction,hypernym_discovery0
2453,34,There are several competing approaches for producing word embedding vectors .,Introduction,Introduction,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",24,0.7741935483870968,33,0.4125,3,0.3,1,research-problem,Introduction,hypernym_discovery0
2454,61,Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 .,Results,Results,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",1,0.125,60,0.75,1,0.125,1,results,Results,hypernym_discovery0
2455,63,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .",Results,Results,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",3,0.375,62,0.775,3,0.375,1,results,Results,hypernym_discovery0
2456,2,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings,title,title,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0092592592592592,1,0.0,1,research-problem,title,hypernym_discovery1
2457,4,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .",abstract,abstract,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.0277777777777777,1,0.2,1,research-problem,abstract,hypernym_discovery1
2458,12,"Various natural language processing ( NLP ) tasks , especially those semantically intensive ones aiming for inference and reasoning with generalization capability , such as question answering and textual entailment , can benefit from identifying semantic relations between words beyond synonymy .",Introduction,Introduction,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O']",3,0.1666666666666666,11,0.1018518518518518,3,0.1666666666666666,1,research-problem,Introduction,hypernym_discovery1
2459,13,The hypernym discovery task aims to discover the most appropriate hypernym ( s ) for input concepts or entities from a pre-defined corpus .,Introduction,Introduction,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2222222222222222,12,0.1111111111111111,4,0.2222222222222222,1,research-problem,Introduction,hypernym_discovery1
2460,17,"The other challenge is representation for terms , including words and phrases , where the phrase embedding could not be obtained byword embeddings directly .",Introduction,Introduction,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-n', 'O', 'O']",8,0.4444444444444444,16,0.1481481481481481,8,0.4444444444444444,1,research-problem,Introduction,hypernym_discovery1
2461,23,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .",Introduction,Introduction,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",14,0.7777777777777778,22,0.2037037037037037,14,0.7777777777777778,1,model,Introduction,hypernym_discovery1
2462,83,Our model was implemented using the Theano 1 .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O']",7,0.4375,82,0.7592592592592593,5,0.3571428571428571,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2463,84,The diagonal variant of Ada - Grad is used for neural network training .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.5,83,0.7685185185185185,6,0.4285714285714285,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2464,87,The hidden dimension of all neural models are 200 .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",11,0.6875,86,0.7962962962962963,9,0.6428571428571429,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2465,88,The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",12,0.75,87,0.8055555555555556,10,0.7142857142857143,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2466,89,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .",Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",13,0.8125,88,0.8148148148148148,11,0.7857142857142857,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2467,97,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,96,0.8888888888888888,4,0.4,1,results,Result and analysis,hypernym_discovery1
2468,100,"All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,99,0.9166666666666666,7,0.7,1,results,Result and analysis,hypernym_discovery1
2469,101,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,100,0.925925925925926,8,0.8,1,results,Result and analysis,hypernym_discovery1
2470,2,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection,title,title,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0040650406504065,1,0.0,1,research-problem,title,hypernym_discovery2
2471,131,"The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .",Experiments,Comparing Unsupervised Measures,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1176470588235294,130,0.5284552845528455,13,0.6842105263157895,1,results,Experiments: Comparing Unsupervised Measures,hypernym_discovery2
2472,132,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .",Experiments,Comparing Unsupervised Measures,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O']",15,0.1260504201680672,131,0.532520325203252,14,0.7368421052631579,1,results,Experiments: Comparing Unsupervised Measures,hypernym_discovery2
2473,143,The inclusion hypothesis seems to be most effective in discriminating between hypernyms and meronyms under syntactic contexts .,Experiments,Hypernym vs. Meronym,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",26,0.2184873949579832,142,0.5772357723577236,1,0.0625,1,results,Experiments: Hypernym vs. Meronym,hypernym_discovery2
2474,158,"For instance , on EVALution , SLQS performs worse ( ranked only as high as 13th ) , as this dataset has no such restriction on the basic level concepts , and may contain pairs like ( eye , animal ) .",Experiments,Hypernym vs. Meronym,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3445378151260504,157,0.6382113821138211,16,1.0,1,results,Experiments: Hypernym vs. Meronym,hypernym_discovery2
2475,159,Hypernym vs. Attribute,Experiments,,hypernym_discovery,2,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",42,0.3529411764705882,158,0.6422764227642277,0,0.0,1,results,Experiments,hypernym_discovery2
2476,168,"Hypernym vs. Synonym SLQS performs well also in discriminating between hypernyms and synonyms , in which y is also not more general than x .",Experiments,Hypernym vs. Antonym,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.4285714285714285,167,0.6788617886178862,5,0.5,1,results,Experiments: Hypernym vs. Antonym,hypernym_discovery2
2477,169,"We observed that in the joint context type , the difference in SLQS scores between synonyms and hypernyms was the largest .",Experiments,Hypernym vs. Antonym,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",52,0.4369747899159664,168,0.6829268292682927,6,0.6,1,results,Experiments: Hypernym vs. Antonym,hypernym_discovery2
2478,174,Hypernym vs. Coordination,Experiments,,hypernym_discovery,2,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",57,0.4789915966386555,173,0.7032520325203252,0,0.0,1,results,Experiments,hypernym_discovery2
2479,176,"On Weeds , inclusion - based measures ( ClarkeDE , invCL and Weeds ) showed the best results .",Experiments,Hypernym vs. Coordination,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",59,0.4957983193277311,175,0.7113821138211383,2,0.0408163265306122,1,results,Experiments: Hypernym vs. Coordination,hypernym_discovery2
2480,191,"The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .",Experiments,Hypernym vs. Coordination,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",74,0.6218487394957983,190,0.7723577235772358,17,0.3469387755102041,1,results,Experiments: Hypernym vs. Coordination,hypernym_discovery2
2481,192,"As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .",Experiments,Hypernym vs. Coordination,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.6302521008403361,191,0.7764227642276422,18,0.3673469387755102,1,results,Experiments: Hypernym vs. Coordination,hypernym_discovery2
2482,2,Supervised Distributional Hypernym Discovery via Domain Adaptation,title,,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0059171597633136,1,0.0,1,research-problem,title,hypernym_discovery3
2483,6,"In this paper , we propose a supervised distributional framework for hypernym discovery which operates at the sense level , enabling large - scale automatic acquisition of dis ambiguated taxonomies .",abstract,abstract,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",3,0.5,5,0.029585798816568,3,0.5,1,research-problem,abstract,hypernym_discovery3
2484,7,"By exploiting semantic regularities between hyponyms and hypernyms in embeddings spaces , and integrating a domain clustering algorithm , our model becomes sensitive to the target data .",abstract,abstract,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'O']",4,0.6666666666666666,6,0.0355029585798816,4,0.6666666666666666,1,research-problem,abstract,hypernym_discovery3
2485,12,"By embedding cues about how we perceive concepts , and how these concepts generalize in a domain of knowledge , these resources bear a capacity for generalization that lies at the core of human cognition and have become key in Natural Language Processing ( NLP ) tasks where inference and reasoning have proved to be essential .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0869565217391304,11,0.0650887573964497,2,0.0869565217391304,1,research-problem,Introduction,hypernym_discovery3
2486,25,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",15,0.6521739130434783,24,0.1420118343195266,15,0.6521739130434783,1,model,Introduction,hypernym_discovery3
2487,26,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.6956521739130435,25,0.1479289940828402,16,0.6956521739130435,1,model,Introduction,hypernym_discovery3
2488,32,"Compared to word - level taxonomy learning , TAXO - EMBED results in more refined and unambiguous hypernymic relations at the sense level , with a direct application in tasks such as semantic search .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.9565217391304348,31,0.1834319526627219,22,0.9565217391304348,1,research-problem,Introduction,hypernym_discovery3
2489,67,"KB - UNIFY 6 ( Delli Bovi et al. , 2015 a ) ( KB - U ) is a knowledge - based approach , based on BabelNet , for integrating the output of different OIE systems into a single unified and dis ambiguated knowledge repository .",Training Data,Training Data,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.3125,66,0.3905325443786982,5,0.5,1,baselines,Training Data,hypernym_discovery3
2490,68,"The unification algorithm takes as input a set K of OIE - derived resources , each of which is modeled as a set of entity , relation , entity triples , and comprises two subsequent stages : in the first dis ambiguation stage , each KB in K is linked to the sense inventory of Babel Net by dis ambiguating its relation argument pairs ; in the following alignment stage , equivalent relations across different KB in K are merged together .",Training Data,Training Data,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,67,0.3964497041420118,6,0.6,1,baselines,Training Data,hypernym_discovery3
2491,139,"As expected , Yago and WiBi achieve the best over all results .",Experimental setting,Experimental setting,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.7857142857142857,138,0.8165680473372781,11,0.7857142857142857,1,results,Experimental setting,hypernym_discovery3
2492,149,Experiment 2 : Extra-Coverage,Results and discussion,Results and discussion,hypernym_discovery,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n']",6,0.75,148,0.8757396449704142,6,0.75,1,results,Results and discussion,hypernym_discovery3
2493,4,"This paper proposes a simple but effective method for the discovery of hypernym sets based on word embedding , which can be used to measure the contextual similarities between words .",abstract,abstract,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.5,3,0.0258620689655172,2,0.5,1,research-problem,abstract,hypernym_discovery4
2494,13,"In the SemEval 2018 Task 9 , the task has shifted to "" Hypernym Discovery "" , i.e. , given the search space of a domain 's vocabulary and an input hyponym , discover its best ( set of ) candidate hypernyms .",Introduction,Introduction,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6666666666666666,12,0.1034482758620689,6,0.6666666666666666,1,research-problem,Introduction,hypernym_discovery4
2495,82,Word2vec is used to produce the word embeddings .,Experimental Setup,Experimental Setup,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",1,0.0588235294117647,81,0.6982758620689655,1,0.2,1,hyperparameters,Experimental Setup,hypernym_discovery4
2496,83,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,Experimental Setup,Experimental Setup,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1176470588235294,82,0.7068965517241379,2,0.4,1,hyperparameters,Experimental Setup,hypernym_discovery4
2497,87,Results Based on Projection Learning,Experimental Setup,,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n']",6,0.3529411764705882,86,0.7413793103448276,0,0.0,1,results,Experimental Setup,hypernym_discovery4
2498,91,"By using the same evaluating metrics as PRF in the cited paper , our best F - value on the validation set is 0.68 ( the paper result is 0.73 ) when the best cluster number is 2 and the threshold is ( 17.7 , 17.3 ) .",Experimental Setup,Results Based on Projection Learning,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.5882352941176471,90,0.7758620689655172,4,0.3636363636363636,1,results,Experimental Setup: Results Based on Projection Learning,hypernym_discovery4
2499,2,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery,title,title,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-n', 'I-n']",1,0.0,1,0.0053191489361702,1,0.0,1,research-problem,title,hypernym_discovery5
2500,8,The goal of the hypernym discovery task at Sem - Eval 2018 is to predict the hypernyms of a query given a large vocabulary of candidate hypernyms .,Introduction,Introduction,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0188679245283018,7,0.0372340425531914,1,0.025,1,research-problem,Introduction,hypernym_discovery5
2501,12,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .",Introduction,Introduction,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.0943396226415094,11,0.0585106382978723,5,0.125,1,model,Introduction,hypernym_discovery5
2502,14,Pattern - Based Hypernym Discovery,Introduction,,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n']",7,0.1320754716981132,13,0.0691489361702127,7,0.175,1,research-problem,Introduction,hypernym_discovery5
2503,48,Learning Projections for Hypernym Discovery,Introduction,,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n']",41,0.7735849056603774,47,0.25,0,0.0,1,research-problem,Introduction,hypernym_discovery5
2504,134,Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O']",12,0.5454545454545454,133,0.7074468085106383,12,0.5454545454545454,1,results,Experiments and Results,hypernym_discovery5
2505,137,"If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",15,0.6818181818181818,136,0.723404255319149,15,0.6818181818181818,1,results,Experiments and Results,hypernym_discovery5
2506,138,"Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.7272727272727273,137,0.7287234042553191,16,0.7272727272727273,1,results,Experiments and Results,hypernym_discovery5
2507,140,"Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task , and even outperformed the supervised baseline on 2A .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",18,0.8181818181818182,139,0.7393617021276596,18,0.8181818181818182,1,results,Experiments and Results,hypernym_discovery5
2508,141,"Combining the outputs of the 2 systems improves the best score of either system on all test sets , sometimes by as much as 10 points .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.8636363636363636,140,0.7446808510638298,19,0.8636363636363636,1,results,Experiments and Results,hypernym_discovery5
2509,142,"Notice also that the results obtained using only the supervised system indicate that data augmentation had a positive effect on our 2A scores only ( compare runs 1 and 2 ) , although our tests on the trial set suggested it would also have a positive effect on our 1A scores .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.9090909090909092,141,0.75,20,0.9090909090909092,1,results,Experiments and Results,hypernym_discovery5
2510,143,"Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O']",21,0.9545454545454546,142,0.7553191489361702,21,0.9545454545454546,1,results,Experiments and Results,hypernym_discovery5
2511,151,No subsampling : we sample positive examples uniformly from the training set .,Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",6,0.1395348837209302,150,0.7978723404255319,6,0.25,1,ablation-analysis,Ablation Tests,hypernym_discovery5
2512,153,"No MTL : instead of multi - task learning ( MTL ) , we use a single classifier for both named entities and concepts .",Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'O', 'O', 'I-p', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.1860465116279069,152,0.8085106382978723,8,0.3333333333333333,1,baselines,Ablation Tests,hypernym_discovery5
2513,163,Frozen embeddings : the word embeddings are not fine - tuned during training .,Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-p', 'B-n', 'O']",18,0.4186046511627907,162,0.8617021276595744,18,0.75,1,ablation-analysis,Ablation Tests,hypernym_discovery5
2514,172,"It is worth noting that our supervised model outperforms the supervised baseline provided for this task ( see ) even when it exploits a single projection matrix , however the difference in scores between these 2 systems is only 2 or 3 points , depending on the evaluation metric .",Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.627906976744186,171,0.9095744680851064,2,0.6666666666666666,1,ablation-analysis,Ablation Tests,hypernym_discovery5
2515,2,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery,title,title,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0078125,1,0.0,1,research-problem,title,hypernym_discovery6
2516,4,"In this paper , we present our proposed system ( EXPR ) to participate in the hypernym discovery task of SemEval 2018 .",abstract,abstract,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.125,3,0.0234375,1,0.125,1,research-problem,abstract,hypernym_discovery6
2517,5,The task addresses the challenge of discovering hypernym relations from a text corpus .,abstract,abstract,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.25,4,0.03125,2,0.25,1,research-problem,abstract,hypernym_discovery6
2518,25,Hypernym detection focuses on deciding whether a hypernymic relation holds between a given pair of terms or not .,Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5909090909090909,24,0.1875,13,0.5909090909090909,1,research-problem,Introduction,hypernym_discovery6
2519,26,Hypernym discovery focuses on discovering a set containing the best hypernyms for a given term from a given vocabulary search space .,Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6363636363636364,25,0.1953125,14,0.6363636363636364,1,research-problem,Introduction,hypernym_discovery6
2520,27,The task is divided into two subtasks : General - Purpose Hypernym Discovery and Domain - Specific Hypernym Discovery .,Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.6818181818181818,26,0.203125,15,0.6818181818181818,1,research-problem,Introduction,hypernym_discovery6
2521,29,"The second consists of discovering hypernym in a domain - specific corpus , thus they provide the participants with data for two specific domains : Medical and Music .",Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",17,0.7727272727272727,28,0.21875,17,0.7727272727272727,1,dataset,Introduction,hypernym_discovery6
2522,32,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .",Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",20,0.9090909090909092,31,0.2421875,20,0.9090909090909092,1,model,Introduction,hypernym_discovery6
2523,33,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .",Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",21,0.9545454545454546,32,0.25,21,0.9545454545454546,1,model,Introduction,hypernym_discovery6
2524,101,"For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .",Results and Analysis,Results and Analysis,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",3,0.15,100,0.78125,3,0.15,1,results,Results and Analysis,hypernym_discovery6
2525,114,"As shown in the table 2 , the candidate hypernym extraction ( CHE ) coverage for English testing terms is 950 ( 63 % ) , that means our system is unable to extract any candidate hypernym for 550 ( 37 % ) terms ( 398 entities and 152 concepts ) .",Results and Analysis,Results and Analysis,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,113,0.8828125,16,0.8,1,results,Results and Analysis,hypernym_discovery6
2526,13,utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other .,Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",8,0.1066666666666666,12,0.096,8,0.32,1,model,Introduction,hypernym_discovery7
2527,14,Here we apply sparse feature pairs to hypernym extraction .,Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",9,0.12,13,0.104,9,0.36,1,model,Introduction,hypernym_discovery7
2528,18,The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis ( FCA ) is relatively new .,Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",13,0.1733333333333333,17,0.136,13,0.52,1,research-problem,Introduction,hypernym_discovery7
2529,31,Formal concept analysis,Introduction,,hypernym_discovery,7,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",26,0.3466666666666667,30,0.24,0,0.0,1,experiments,Introduction,hypernym_discovery7
2530,117,Generating more negative samples also provides some additional performance boost .,Post - evaluation analysis,We call this strategy as candidate filtering .,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.8,116,0.928,20,0.8333333333333334,1,tasks,Post - evaluation analysis: We call this strategy as candidate filtering .,hypernym_discovery7
2531,2,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies,title,title,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0151515151515151,1,0.0,1,research-problem,title,hypernym_discovery8
2532,4,"This paper presents the participation of Apollo 's team in the SemEval - 2018 Task 9 "" Hypernym Discovery "" , Subtask 1 : "" General - Purpose Hypernym Discovery "" , which tries to produce a ranked list of hypernyms for a specific term .",abstract,abstract,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.3333333333333333,3,0.0454545454545454,1,0.3333333333333333,1,research-problem,abstract,hypernym_discovery8
2533,5,We propose a novel approach for automatic extraction of hypernymy relations from a corpus by using dependency patterns .,abstract,abstract,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",2,0.6666666666666666,4,0.0606060606060606,2,0.6666666666666666,1,research-problem,abstract,hypernym_discovery8
2534,8,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,Introduction,Introduction,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.0192307692307692,7,0.106060606060606,1,0.0625,1,research-problem,Introduction,hypernym_discovery8
2535,9,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,Introduction,Introduction,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.0384615384615384,8,0.1212121212121212,2,0.125,1,model,Introduction,hypernym_discovery8
2536,15,"It is well known that in natural language processing ( NLP ) , one of the biggest challenges is to understand the meaning of words .",Introduction,Introduction,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O']",8,0.1538461538461538,14,0.2121212121212121,8,0.5,1,research-problem,Introduction,hypernym_discovery8
2537,24,A new Approach to Detect Hypernymy Relation,Introduction,,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",17,0.3269230769230769,23,0.3484848484848485,0,0.0,1,research-problem,Introduction,hypernym_discovery8
2538,2,Neural Models for Reasoning over Multiple Mentions using Coreference,title,title,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.0,1,0.0057142857142857,1,0.0,1,research-problem,title,natural_language_inference0
2539,11,One important form of reasoning for Question Answering ( QA ) models is the ability to aggregate information from multiple mentions of entities .,Introduction,Introduction,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1111111111111111,10,0.0571428571428571,2,0.1111111111111111,1,research-problem,Introduction,natural_language_inference0
2540,12,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .",Introduction,Introduction,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.1666666666666666,11,0.0628571428571428,3,0.1666666666666666,1,model,Introduction,natural_language_inference0
2541,20,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .",Introduction,Introduction,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6111111111111112,19,0.1085714285714285,11,0.6111111111111112,1,model,Introduction,natural_language_inference0
2542,100,In each case we see clear improvements of using C - GRU layers over GRU layers .,Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.3055555555555556,99,0.5657142857142857,11,0.3055555555555556,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2543,102,"The Bi - C - GRU model significantly improves on this baseline , which shows that , with less data , coreference annotations can provide a useful bias for a memory network on how to read and write memories .",Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3611111111111111,101,0.5771428571428572,13,0.3611111111111111,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2544,103,"A break - down of task - wise performance is given in Appendix C. Comparing C - GRU to the GRU based method , we find that the main gains are on tasks 2 ( two supporting facts ) , 3 ( three supporting facts ) and 16 ( basic induction ) .",Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O']",14,0.3888888888888889,102,0.5828571428571429,14,0.3888888888888889,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2545,105,"Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .",Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4444444444444444,104,0.5942857142857143,16,0.4444444444444444,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2546,107,"On the other hand , C - GRU was significantly better than QRN on task 16 ( basic induction ) .",Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5,106,0.6057142857142858,18,0.5,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2547,108,We also include a baseline which uses coreference features as 1 - hot vectors appended to the input word vectors ( GA w/ GRU + 1 - hot ) .,Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.5277777777777778,107,0.6114285714285714,19,0.5277777777777778,1,model,Method: Experiments on more natural data are described below .,natural_language_inference0
2548,111,"In both cases there is a sharp drop in performance , showing that specifically using coreference for connecting mentions is important .",Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6111111111111112,110,0.6285714285714286,22,0.6111111111111112,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2549,120,"We see higher performance for the C - GRU model in the low data regime , and better generalization throughout the training curve for all three settings .",Method,Wikihop dataset .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",31,0.8611111111111112,119,0.68,31,0.8611111111111112,1,research-problem,Method: Wikihop dataset .,natural_language_inference0
2550,125,"Lastly , we note that both models vastly outperform the best reported result of BiDAf from 1 . We believe this is because the GA models select answers from the list of candidatees , whereas BiDAF ignores those candidates .",Method,Wikihop dataset .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,1.0,124,0.7085714285714285,36,1.0,1,experiments,Method: Wikihop dataset .,natural_language_inference0
2551,138,We see a significant gain in performance when using the layer with coreference bias .,Method overall context,Method overall context,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",12,0.7058823529411765,137,0.7828571428571428,12,0.7058823529411765,1,experiments,Method overall context,natural_language_inference0
2552,139,"Furthermore , the 1 - hot baseline which uses the same coreference information , but with sequential recency bias fails to improve over the regular GRU layer .",Method overall context,Method overall context,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",13,0.7647058823529411,138,0.7885714285714286,13,0.7647058823529411,1,experiments,Method overall context,natural_language_inference0
2553,160,The maximum number of coreference clusters across all tasks was C = 13 .,B Implementation details,B Implementation details,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.25,159,0.9085714285714286,5,0.25,1,hyperparameters,B Implementation details,natural_language_inference0
2554,166,"We used dropout of 0.2 in between the intermediate layers , and initialized word embeddings with Glove .",B Implementation details,B Implementation details,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",11,0.55,165,0.9428571428571428,11,0.55,1,hyperparameters,B Implementation details,natural_language_inference0
2555,2,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension,title,title,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0067567567567567,1,0.0,1,research-problem,title,natural_language_inference1
2556,4,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .,abstract,abstract,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.2,3,0.0202702702702702,1,0.2,1,research-problem,abstract,natural_language_inference1
2557,10,Building Artificial Intelligence ( AI ) algorithms to teach machines to read and to comprehend text is a long - standing challenge in Natural Language Processing ( NLP ) .,Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0454545454545454,9,0.0608108108108108,1,0.0454545454545454,1,research-problem,Introduction,natural_language_inference1
2558,19,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .",Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.4545454545454545,18,0.1216216216216216,10,0.4545454545454545,1,model,Introduction,natural_language_inference1
2559,20,The ConZNet architecture consists of two phases .,Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",11,0.5,19,0.1283783783783783,11,0.5,1,model,Introduction,natural_language_inference1
2560,21,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",12,0.5454545454545454,20,0.1351351351351351,12,0.5454545454545454,1,model,Introduction,natural_language_inference1
2561,23,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .",Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",14,0.6363636363636364,22,0.1486486486486486,14,0.6363636363636364,1,model,Introduction,natural_language_inference1
2562,28,"Moreover , our decoder combines span prediction and sequence generation .",Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.8636363636363636,27,0.1824324324324324,19,0.8636363636363636,1,model,Introduction,natural_language_inference1
2563,119,In both baselines we replace the span prediction layer with an answer generation layer .,Baselines,Baselines,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",3,0.6,118,0.7972972972972973,3,0.6,1,baselines,Baselines,natural_language_inference1
2564,120,In Baseline 1 we use an 1 please refer for more details attention based seq2seq layer without using copy mechanism in the answer generation unit similar to .,Baselines,Baselines,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",4,0.8,119,0.8040540540540541,4,0.8,1,baselines,Baselines,natural_language_inference1
2565,123,We split each document into sentences using the sentence tokenizer of the NLTK toolkit .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",1,0.0769230769230769,122,0.8243243243243243,1,0.0769230769230769,1,experimental-setup,Implementation Details,natural_language_inference1
2566,124,"Similarly , we further tokenize each sentence , corresponding question and answer using the word tokenizer of NLTK .",Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",2,0.1538461538461538,123,0.831081081081081,2,0.1538461538461538,1,experimental-setup,Implementation Details,natural_language_inference1
2567,125,The model is implemented using Python and Tensorflow .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",3,0.2307692307692307,124,0.8378378378378378,3,0.2307692307692307,1,experimental-setup,Implementation Details,natural_language_inference1
2568,126,All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",4,0.3076923076923077,125,0.8445945945945946,4,0.3076923076923077,1,experimental-setup,Implementation Details,natural_language_inference1
2569,127,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .",Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']",5,0.3846153846153846,126,0.8513513513513513,5,0.3846153846153846,1,experimental-setup,Implementation Details,natural_language_inference1
2570,128,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",6,0.4615384615384615,127,0.8581081081081081,6,0.4615384615384615,1,experimental-setup,Implementation Details,natural_language_inference1
2571,129,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.5384615384615384,128,0.8648648648648649,7,0.5384615384615384,1,experimental-setup,Implementation Details,natural_language_inference1
2572,130,The number of hidden units are set to 100 .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",8,0.6153846153846154,129,0.8716216216216216,8,0.6153846153846154,1,experimental-setup,Implementation Details,natural_language_inference1
2573,131,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .",Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",9,0.6923076923076923,130,0.8783783783783784,9,0.6923076923076923,1,experiments,Implementation Details,natural_language_inference1
2574,132,The hyperparameter ' sample size ' ( number of relevant sentences ) is chosen based on the model performance on the devset .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",10,0.7692307692307693,131,0.8851351351351351,10,0.7692307692307693,1,experimental-setup,Implementation Details,natural_language_inference1
2575,133,shows the performance of various models on Narrative QA .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",11,0.8461538461538461,132,0.8918918918918919,11,0.8461538461538461,1,results,Implementation Details,natural_language_inference1
2576,134,It can be noted that our model with sample size 5 ( choosing 5 relevant sentences ) outperforms the best ROUGE - L score available so far by 12.62 % compared to .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O']",12,0.9230769230769232,133,0.8986486486486487,12,0.9230769230769232,1,results,Implementation Details,natural_language_inference1
2577,135,"The low performance of Baseline 1 shows that the hybrid approach ( ConZNet ) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models ( Seq2Seq , ASR , BiDAF , MRU ) .",Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,1.0,134,0.9054054054054054,13,1.0,1,results,Implementation Details,natural_language_inference1
2578,2,A Simple and Effective Approach to the Story Cloze Test,title,title,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0092592592592592,1,0.0,1,research-problem,title,natural_language_inference10
2579,6,"Following this approach , we present a simpler fully - neural approach to the Story Cloze Test using skip - thought embeddings of the stories in a feed - forward network that achieves close to state - of - the - art performance on this task without any feature engineering .",abstract,abstract,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",3,0.0666666666666666,5,0.0462962962962962,3,0.2307692307692307,1,research-problem,abstract,natural_language_inference10
2580,7,We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach .,abstract,abstract,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.0888888888888888,6,0.0555555555555555,4,0.3076923076923077,1,research-problem,abstract,natural_language_inference10
2581,64,We use two layer and three layer fully connected networks with Rectified Linear ( ReLU ) non-linearities ( refer to Appendix A for model - specific architecture ) .,Models,Models,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,63,0.5833333333333334,4,0.3333333333333333,1,experiments,Models,natural_language_inference10
2582,71,"Full Context ( FC ) Here , we use a Gated Recurrent Unit ( GRU ) to encode the entire story prompt into a 4800 - dimensional vector , add it to the skipthought embedding of the story ending , and pass it as input to the neural network .",Models,Models,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",11,0.9166666666666666,70,0.6481481481481481,11,0.9166666666666666,1,baselines,Models,natural_language_inference10
2583,83,We use cross-entropy loss and SGD with learning rate of 0.01 .,Experimental Method,Experimental Method,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",3,0.4285714285714285,82,0.7592592592592593,3,0.4285714285714285,1,hyperparameters,Experimental Method,natural_language_inference10
2584,89,The 3 - layer feed - forward neural network trained on the validation set by summing the skip - thought embeddings of the last sentence ( LS ) of the story prompt and the ending gives the best accuracy ( 76.5 % ) .,Results and Discussion,Results and Discussion,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O']",1,0.0666666666666666,88,0.8148148148148148,1,0.0666666666666666,1,results,Results and Discussion,natural_language_inference10
2585,90,"This approach is far simpler than previous approaches in the literature ; it requires no feature engineering , nor intricate neural network architecture , and achieves close to state - of - the - art accuracy .",Results and Discussion,Results and Discussion,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.1333333333333333,89,0.8240740740740741,2,0.1333333333333333,1,ablation-analysis,Results and Discussion,natural_language_inference10
2586,95,"We note that the model trained using only the last sentence ( LS ) of the story context has higher accuracy compared to the model that uses a GRU to encode the full context ( FC ) , and even the model which encodes the entire context .",Results and Discussion,Results and Discussion,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",7,0.4666666666666667,94,0.8703703703703703,7,0.4666666666666667,1,results,Results and Discussion,natural_language_inference10
2587,2,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING,title,title,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0040322580645161,1,0.0,1,research-problem,title,natural_language_inference11
2588,4,Several deep learning models have been proposed for question answering .,abstract,abstract,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.1428571428571428,3,0.0120967741935483,1,0.1428571428571428,1,research-problem,abstract,natural_language_inference11
2589,12,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,INTRODUCTION,INTRODUCTION,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0089285714285714,11,0.0443548387096774,1,0.0526315789473684,1,research-problem,INTRODUCTION,natural_language_inference11
2590,22,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .",INTRODUCTION,INTRODUCTION,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.0982142857142857,21,0.0846774193548387,11,0.5789473684210527,1,model,INTRODUCTION,natural_language_inference11
2591,23,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .",INTRODUCTION,INTRODUCTION,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",12,0.1071428571428571,22,0.0887096774193548,12,0.631578947368421,1,model,INTRODUCTION,natural_language_inference11
2592,127,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.25,126,0.5080645161290323,2,0.1818181818181818,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2593,128,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.3333333333333333,127,0.5120967741935484,3,0.2727272727272727,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2594,129,We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",5,0.4166666666666667,128,0.5161290322580645,4,0.3636363636363636,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2595,131,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",7,0.5833333333333334,130,0.5241935483870968,6,0.5454545454545454,1,experiments,EXPERIMENTS,natural_language_inference11
2596,132,All LSTMs have randomly initialized parameters and an initial state of zero .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.6666666666666666,131,0.5282258064516129,7,0.6363636363636364,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2597,133,Sentinel vectors are randomly initialized and optimized during training .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",9,0.75,132,0.532258064516129,8,0.7272727272727273,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2598,134,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",10,0.8333333333333334,133,0.5362903225806451,9,0.8181818181818182,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2599,135,"We use dropout to regularize our network during training , and optimize the model using ADAM .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",11,0.9166666666666666,134,0.5403225806451613,10,0.9090909090909092,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2600,136,All models are implemented and trained with Chainer .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']",12,1.0,135,0.5443548387096774,11,1.0,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2601,145,"The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .",RESULTS,RESULTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",8,0.2758620689655172,144,0.5806451612903226,8,0.2758620689655172,1,results,RESULTS,natural_language_inference11
2602,169,"On the decoder side , we experiment with various pool sizes for the HMN maxout layers , using a 2 - layer MLP instead of a HMN , and forcing the HMN decoder to a single iteration .",Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",2,0.0833333333333333,168,0.6774193548387096,2,0.0833333333333333,1,ablation-analysis,Model Ablation,natural_language_inference11
2603,170,"Empirically , we achieve the best performance on the development set with an iterative HMN with pool size 16 , and find that the model consistently benefits from a deeper , iterative decoder network .",Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.125,169,0.6814516129032258,3,0.125,1,ablation-analysis,Model Ablation,natural_language_inference11
2604,171,"The performance improves as the number of maximum allowed iterations increases , with little improvement after 4 iterations .",Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O']",4,0.1666666666666666,170,0.6854838709677419,4,0.1666666666666666,1,ablation-analysis,Model Ablation,natural_language_inference11
2605,172,"On the encoder side , replacing the coattention mechanism with an attention mechanism similar to Wang & Jiang ( 2016 b ) by setting CD to QA D in equation 3 results in a 1.9 point F1 drop .",Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.2083333333333333,171,0.6895161290322581,5,0.2083333333333333,1,ablation-analysis,Model Ablation,natural_language_inference11
2606,175,Performance across length,Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",8,0.3333333333333333,174,0.7016129032258065,8,0.3333333333333333,1,ablation-analysis,Model Ablation,natural_language_inference11
2607,178,"However , as in shown in , there is no notable performance degradation for longer documents and questions contrary to our expectations .",Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",11,0.4583333333333333,177,0.7137096774193549,11,0.4583333333333333,1,ablation-analysis,Model Ablation,natural_language_inference11
2608,182,"Namely , it becomes increasingly challenging to compute the correct word span as the number of words increases .",Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'O']",15,0.625,181,0.7298387096774194,15,0.625,1,ablation-analysis,Model Ablation,natural_language_inference11
2609,185,"In , we note that the mean F1 of DCN exceeds those of previous systems across all question types .",Model Ablation,Model Ablation,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",18,0.75,184,0.7419354838709677,18,0.75,1,ablation-analysis,Model Ablation,natural_language_inference11
2610,2,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING,title,title,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0048780487804878,1,0.0,1,research-problem,title,natural_language_inference12
2611,4,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .",abstract,abstract,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.125,3,0.0146341463414634,1,0.125,1,research-problem,abstract,natural_language_inference12
2612,21,The external memory enables the model to deal with a knowledge base without loss of information .,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",9,0.0491803278688524,20,0.0975609756097561,9,0.25,1,model,INTRODUCTION,natural_language_inference12
2613,23,Generalization updates old memories given the new input and output feature map finds relevant information from the memory .,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.0601092896174863,22,0.1073170731707317,11,0.3055555555555556,1,model,INTRODUCTION,natural_language_inference12
2614,25,"Based on the memory network architecture , neural network based models like end - to - end memory network ( Mem N2N ) , gated end - to - end memory network ( GMe m N2N ) , dynamic memory network ( DMN ) , and dynamic memory network + ( DMN + ) are proposed .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",13,0.0710382513661202,24,0.1170731707317073,13,0.3611111111111111,1,model,INTRODUCTION,natural_language_inference12
2615,42,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'O']",30,0.1639344262295081,41,0.2,30,0.8333333333333334,1,model,INTRODUCTION,natural_language_inference12
2616,43,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O']",31,0.1693989071038251,42,0.2048780487804878,31,0.8611111111111112,1,model,INTRODUCTION,natural_language_inference12
2617,46,"Relation Memory Network ( RMN ) is composed of four components - embedding , attention , updating , and reasoning .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'O']",34,0.1857923497267759,45,0.2195121951219512,34,0.9444444444444444,1,model,INTRODUCTION,natural_language_inference12
2618,51,EMBEDDING COMPONENT,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O']","['B-n', 'I-n']",39,0.2131147540983606,50,0.2439024390243902,0,0.0,1,model,INTRODUCTION,natural_language_inference12
2619,61,"To constitute the attention component , we applied simple MLP represented as gt ? .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",49,0.2677595628415301,60,0.2926829268292683,3,0.2307692307692307,1,model,INTRODUCTION,natural_language_inference12
2620,67,"1 ) to control the intensity of attention , inspired by the way Neural Turing Machine reads from the memory .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",55,0.3005464480874317,66,0.3219512195121951,9,0.6923076923076923,1,model,INTRODUCTION,natural_language_inference12
2621,73,"To forget the information already used , we use intuitive updating component to renew the memory .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']",61,0.3333333333333333,72,0.3512195121951219,1,0.25,1,model,INTRODUCTION,natural_language_inference12
2622,77,REASONING COMPONENT,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O']","['B-n', 'I-n']",65,0.3551912568306011,76,0.3707317073170731,0,0.0,1,experiments,INTRODUCTION,natural_language_inference12
2623,84,"MemN2N first calculates the relatedness of sentences in the question and memory by taking the inner product , and the sentence with the highest relatedness is selected as the first supporting sentence for the given question .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",72,0.3934426229508196,83,0.4048780487804878,7,0.5833333333333334,1,model,INTRODUCTION,natural_language_inference12
2624,125,bAbI story - based QA dataset,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",113,0.6174863387978142,124,0.6048780487804878,1,0.0344827586206896,1,research-problem,INTRODUCTION,natural_language_inference12
2625,132,"For regularization , we use batch normalization for all MLPs .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",120,0.6557377049180327,131,0.6390243902439025,8,0.2758620689655172,1,model,INTRODUCTION,natural_language_inference12
2626,133,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",121,0.6612021857923497,132,0.6439024390243903,9,0.3103448275862069,1,model,INTRODUCTION,natural_language_inference12
2627,148,"While trained jointly , RMN learns these different solutions for each task .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",136,0.7431693989071039,147,0.7170731707317073,24,0.8275862068965517,1,model,INTRODUCTION,natural_language_inference12
2628,149,"For the task 3 , the only failed task , attention component still functions well ; it focuses sequentially on the supporting sentences .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-n', 'I-n', 'O', 'I-n', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'O']",137,0.7486338797814208,148,0.7219512195121951,25,0.8620689655172413,1,experiments,INTRODUCTION,natural_language_inference12
2629,163,"With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",151,0.825136612021858,162,0.7902439024390244,9,0.375,1,experiments,INTRODUCTION,natural_language_inference12
2630,171,"Different from other tasks , RMN yields the same error rate 25.1 % with MemN2N and GMe m N2N on the task 3 .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",159,0.8688524590163934,170,0.8292682926829268,17,0.7083333333333334,1,experiments,INTRODUCTION,natural_language_inference12
2631,194,"Overall , the number of hops is correlated with the number of supporting sentences .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",182,0.994535519125683,193,0.9414634146341464,15,0.9375,1,experiments,INTRODUCTION,natural_language_inference12
2632,2,Natural Language Comprehension with the EpiReader,title,title,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0036496350364963,1,0.0,1,research-problem,title,natural_language_inference13
2633,4,"We present the EpiReader , a novel model for machine comprehension of text .",abstract,abstract,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.2,3,0.010948905109489,1,0.2,1,research-problem,abstract,natural_language_inference13
2634,5,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",abstract,abstract,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,4,0.0145985401459854,2,0.4,1,research-problem,abstract,natural_language_inference13
2635,12,"In this paper , we argue that the same principle can be applied to machine comprehension of natural language .",Introduction,Introduction,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.0160427807486631,11,0.0401459854014598,3,0.0681818181818181,1,model,Introduction,natural_language_inference13
2636,13,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .",Introduction,Introduction,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O']",4,0.0213903743315508,12,0.0437956204379562,4,0.0909090909090909,1,model,Introduction,natural_language_inference13
2637,16,Machine comprehension ( MC ) has therefore garnered significant attention from the machine learning research community .,Introduction,Introduction,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0374331550802139,15,0.0547445255474452,7,0.1590909090909091,1,research-problem,Introduction,natural_language_inference13
2638,26,The EpiReader factors into two components .,Introduction,,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-p', 'B-n', 'I-n', 'O']",17,0.0909090909090909,25,0.0912408759124087,17,0.3863636363636363,1,model,Introduction,natural_language_inference13
2639,40,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .",Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",31,0.1657754010695187,39,0.1423357664233576,31,0.7045454545454546,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2640,63,Children 's Book Test,Introduction,,natural_language_inference,13,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",54,0.2887700534759358,62,0.2262773722627737,9,0.4736842105263157,1,experiments,Introduction,natural_language_inference13
2641,94,The Extractor is a Pointer Network .,Introduction,,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",85,0.4545454545454545,93,0.3394160583941605,1,0.04,1,model,Introduction,natural_language_inference13
2642,109,"At each step the biGRU outputs two d-dimensional encoding vectors , one for the forward direction and one for the backward direction .",Introduction,We refer to the combination as a biGRU .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",100,0.5347593582887701,108,0.3941605839416058,16,0.64,1,model,Introduction: We refer to the combination as a biGRU .,natural_language_inference13
2643,221,"To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",1,0.0833333333333333,220,0.8029197080291971,1,0.0833333333333333,1,experimental-setup,Implementation and training details,natural_language_inference13
2644,222,"The word embeddings were initialized randomly , drawing from the uniform distribution over .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O']",2,0.1666666666666666,221,0.8065693430656934,2,0.1666666666666666,1,experimental-setup,Implementation and training details,natural_language_inference13
2645,223,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.25,222,0.8102189781021898,3,0.25,1,experiments,Implementation and training details,natural_language_inference13
2646,224,Our model was implement in Theano using the Keras framework .,Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",4,0.3333333333333333,223,0.8138686131386861,4,0.3333333333333333,1,experimental-setup,Implementation and training details,natural_language_inference13
2647,229,"All our models used 2 - regularization at 0.001 , ? = 50 , and ? = 0.04 .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,228,0.8321167883211679,9,0.75,1,experimental-setup,Implementation and training details,natural_language_inference13
2648,236,The EpiReader achieves state - of - the - art performance across the board for both datasets .,Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O']",3,0.0833333333333333,235,0.8576642335766423,3,0.25,1,results,Results,natural_language_inference13
2649,237,"On CNN , we score 2.2 % higher on test than the best previous model of .",Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']",4,0.1111111111111111,236,0.8613138686131386,4,0.3333333333333333,1,results,Results,natural_language_inference13
2650,243,The improvement on CBT - NE is more modest at 1.1 % .,Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",10,0.2777777777777778,242,0.8832116788321168,10,0.8333333333333334,1,results,Results,natural_language_inference13
2651,133,"For each mini-batch update , the 2 norm of the whole gradient of all parameters is measured 5 and if larger than L = 50 , then it is scaled down to have norm L.",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'B-n', 'I-n']",2,0.1052631578947368,132,0.6567164179104478,2,0.1052631578947368,1,hyperparameters,Training Details,natural_language_inference14
2652,135,"We use the learning rate annealing schedule from , namely , if the validation cost has not decreased after one epoch , then the learning rate is scaled down by a factor 1.5 .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",4,0.2105263157894736,134,0.6666666666666666,4,0.2105263157894736,1,hyperparameters,Training Details,natural_language_inference14
2653,136,"Training terminates when the learning rate drops below 10 ? 5 , i.e. after 50 epochs or so .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2631578947368421,135,0.6716417910447762,5,0.2631578947368421,1,hyperparameters,Training Details,natural_language_inference14
2654,137,"Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",6,0.3157894736842105,136,0.6766169154228856,6,0.3157894736842105,1,hyperparameters,Training Details,natural_language_inference14
2655,138,"On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.3684210526315789,137,0.681592039800995,7,0.3684210526315789,1,hyperparameters,Training Details,natural_language_inference14
2656,141,Note that the baseline architectures were tuned in to give optimal perplexity 6 .,Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O']",10,0.5263157894736842,140,0.6965174129353234,10,0.5263157894736842,1,hyperparameters,Training Details,natural_language_inference14
2657,144,"We also vary the number of hops and memory size of our MemN2N , showing the contribution of both to performance ; note in particular that increasing the number of hops helps .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.6842105263157895,143,0.7114427860696517,13,0.6842105263157895,1,experiments,Training Details,natural_language_inference14
2658,145,"In , we show how Mem N2N operates on memory with multiple hops .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",14,0.7368421052631579,144,0.7164179104477612,14,0.7368421052631579,1,experiments,Training Details,natural_language_inference14
2659,153,"MemNN : The strongly supervised AM + NG + NL Memory Networks approach , proposed in .",Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",2,0.1818181818181818,152,0.7562189054726368,2,0.1818181818181818,1,baselines,Baselines,natural_language_inference14
2660,155,It uses a max operation ( rather than softmax ) at each layer which is trained directly with supporting facts ( strong supervision ) .,Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O']",4,0.3636363636363636,154,0.7661691542288557,4,0.3636363636363636,1,baselines,Baselines,natural_language_inference14
2661,156,"It employs n-gram modeling , nonlinear layers and an adaptive number of hops per query .",Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",5,0.4545454545454545,155,0.7711442786069652,5,0.4545454545454545,1,baselines,Baselines,natural_language_inference14
2662,161,"LSTM : A standard LSTM model , trained using question / answer pairs only ( i.e. also weakly supervised ) .",Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O']",10,0.9090909090909092,160,0.7960199004975125,10,0.9090909090909092,1,baselines,Baselines,natural_language_inference14
2663,164,Language Modeling Experiments,,,natural_language_inference,14,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,163,0.8109452736318408,0,0.0,1,experiments,,natural_language_inference14
2664,174,"To aid training , we apply ReLU operations to half of the units in each layer .",Language Modeling Experiments,Language Modeling Experiments,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",10,0.5,173,0.8606965174129353,10,0.5,1,experiments,Language Modeling Experiments,natural_language_inference14
2665,175,"We use layer - wise ( RNN - like ) weight sharing , i.e. the query weights of each layer are the same ; the output weights of each layer are the same .",Language Modeling Experiments,Language Modeling Experiments,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'I-n', 'O']",11,0.55,174,0.8656716417910447,11,0.55,1,experiments,Language Modeling Experiments,natural_language_inference14
2666,2,Neural Natural Language Inference Models Enhanced with External Knowledge,title,,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0043103448275862,1,0.0,1,research-problem,title,natural_language_inference15
2667,4,Modeling natural language inference is a very challenging task .,abstract,abstract,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.0129310344827586,1,0.1666666666666666,1,research-problem,abstract,natural_language_inference15
2668,12,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",Introduction,Introduction,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.125,11,0.0474137931034482,2,0.125,1,research-problem,Introduction,natural_language_inference15
2669,23,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .",Introduction,Introduction,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",13,0.8125,22,0.0948275862068965,13,0.8125,1,model,Introduction,natural_language_inference15
2670,25,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .",Introduction,Introduction,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,24,0.1034482758620689,15,0.9375,1,experiments,Introduction,natural_language_inference15
2671,160,The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",3,0.1304347826086956,159,0.6853448275862069,3,0.1304347826086956,1,hyperparameters,Training Details,natural_language_inference15
2672,161,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'B-n', 'O']",4,0.1739130434782608,160,0.6896551724137931,4,0.1739130434782608,1,hyperparameters,Training Details,natural_language_inference15
2673,162,All word embeddings are updated during training .,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",5,0.217391304347826,161,0.6939655172413793,5,0.217391304347826,1,hyperparameters,Training Details,natural_language_inference15
2674,163,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.2608695652173913,162,0.6982758620689655,6,0.2608695652173913,1,hyperparameters,Training Details,natural_language_inference15
2675,164,The mini - batch size is set to 32 .,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",7,0.3043478260869565,163,0.7025862068965517,7,0.3043478260869565,1,hyperparameters,Training Details,natural_language_inference15
2676,166,ESIM is a strong NLI baseline framework with the source code made available at https://github.com/lukecq1231/nli,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",9,0.391304347826087,165,0.7112068965517241,9,0.391304347826087,1,code,Training Details,natural_language_inference15
2677,170,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",13,0.5652173913043478,169,0.728448275862069,13,0.5652173913043478,1,results,Training Details,natural_language_inference15
2678,173,"In addition to that , we also use 15 semantic relation features , which does not bring additional gains in performance .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-n', 'O', 'O', 'O', 'B-n', 'O']",16,0.6956521739130435,172,0.7413793103448276,16,0.6956521739130435,1,hyperparameters,Training Details,natural_language_inference15
2679,175,"To further investigate external knowledge , we add TransE relation embedding , and again no further improvement is observed on both the development and test sets when TransE relation embedding is used ( concatenated ) with the semantic relation vectors .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'I-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",18,0.782608695652174,174,0.75,18,0.782608695652174,1,experiments,Training Details,natural_language_inference15
2680,178,"The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",21,0.9130434782608696,177,0.7629310344827587,21,0.9130434782608696,1,results,Training Details,natural_language_inference15
2681,179,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",22,0.9565217391304348,178,0.7672413793103449,22,0.9565217391304348,1,results,Training Details,natural_language_inference15
2682,205,"Especially under the condition of restricted training data ( 0.8 % ) , the model obtains a large gain when using more than half of external knowledge . :",Model In Cross,Model In Cross,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",19,0.4634146341463415,204,0.8793103448275862,19,0.9047619047619048,1,results,Model In Cross,natural_language_inference15
2683,222,"Especially , for antonym category in cross - domain set , KIM outperform ESIM significantly (+ absolute 5.0 % ) as expected , because antonym feature captured by external knowledge would help unseen cross - domain samples .",Model In Cross,Analysis by Inference Categories,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'B-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.8780487804878049,221,0.9525862068965516,7,0.6363636363636364,1,results,Model In Cross: Analysis by Inference Categories,natural_language_inference15
2684,2,Text Understanding with the Attention Sum Reader Network,title,title,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0041666666666666,1,0.0,1,research-problem,title,natural_language_inference16
2685,8,Ensemble of our models sets new state of the art on all evaluated datasets .,abstract,abstract,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,1.0,7,0.0291666666666666,5,1.0,1,research-problem,abstract,natural_language_inference16
2686,11,Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence .,Introduction,Introduction,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0238095238095238,10,0.0416666666666666,2,0.0769230769230769,1,research-problem,Introduction,natural_language_inference16
2687,56,Children 's Book Test,Introduction,,natural_language_inference,16,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",47,0.5595238095238095,55,0.2291666666666666,8,0.3076923076923077,1,experiments,Introduction,natural_language_inference16
2688,79,We call this the contextual embedding .,Introduction,,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O']",70,0.8333333333333334,78,0.325,4,0.2222222222222222,1,model,Introduction,natural_language_inference16
2689,172,To train the model we used stochastic gradient descent with the ADAM update rule and learning rate of 0.001 or 0.0005 .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",1,0.0526315789473684,171,0.7125,1,0.0526315789473684,1,hyperparameters,Training Details,natural_language_inference16
2690,177,"2 . The initial weights in the word embedding matrix were drawn randomly uniformly from the interval [ ? 0.1 , 0.1 ] .",Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.3157894736842105,176,0.7333333333333333,6,0.3157894736842105,1,hyperparameters,Training Details,natural_language_inference16
2691,178,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",7,0.3684210526315789,177,0.7375,7,0.3684210526315789,1,hyperparameters,Training Details,natural_language_inference16
2692,179,We also used a gradient clipping threshold of 10 and batches of size 32 .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'I-p', 'B-n', 'O']",8,0.4210526315789473,178,0.7416666666666667,8,0.4210526315789473,1,hyperparameters,Training Details,natural_language_inference16
2693,180,During training we randomly shuffled all examples in each epoch .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",9,0.4736842105263157,179,0.7458333333333333,9,0.4736842105263157,1,hyperparameters,Training Details,natural_language_inference16
2694,183,For each batch of the CNN and Daily Mail datasets we randomly reshuffled the assignment of named entities to the corresponding word embedding vectors to match the procedure proposed in .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O']",12,0.631578947368421,182,0.7583333333333333,12,0.631578947368421,1,hyperparameters,Training Details,natural_language_inference16
2695,211,Ensembles of our models set new state - of - the - art results on all evaluated datasets .,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.1379310344827586,210,0.875,3,0.2307692307692307,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2696,214,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .",Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2413793103448276,213,0.8875,6,0.4615384615384615,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2697,215,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",8,0.2758620689655172,214,0.8916666666666667,7,0.5384615384615384,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2698,216,The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.3103448275862069,215,0.8958333333333334,8,0.6153846153846154,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2699,218,Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets ..,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.3793103448275862,217,0.9041666666666668,10,0.7692307692307693,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2700,220,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .",Results,CBT .,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4482758620689655,219,0.9125,12,0.9230769230769232,1,results,Results: CBT .,natural_language_inference16
2701,221,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,Results,CBT .,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4827586206896552,220,0.9166666666666666,13,1.0,1,results,Results: CBT .,natural_language_inference16
2702,2,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING,title,title,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0024752475247524,1,0.0,1,research-problem,title,natural_language_inference17
2703,4,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .",abstract,abstract,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.0074257425742574,1,0.1428571428571428,1,research-problem,abstract,natural_language_inference17
2704,34,"( ii ) An online evaluation platform and leaderboard , based primarily on privately - held test data .",INTRODUCTION,INTRODUCTION,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",23,0.2233009708737864,33,0.0816831683168316,23,0.8846153846153846,1,dataset,INTRODUCTION,natural_language_inference17
2705,47,"It is model - agnostic , allowing for any kind of representation or contextualization , including models that use no explicit vector or symbolic representations for sentences whatsoever .",INTRODUCTION,INTRODUCTION,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'I-n', 'O', 'B-p', 'B-n', 'O', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'I-n', 'B-p', 'B-n', 'O', 'O']",36,0.3495145631067961,46,0.1138613861386138,9,0.5294117647058824,1,model,INTRODUCTION,natural_language_inference17
2706,48,GLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite .,INTRODUCTION,INTRODUCTION,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']",37,0.3592233009708738,47,0.1163366336633663,10,0.5882352941176471,1,experiments,INTRODUCTION,natural_language_inference17
2707,145,"Interannotator agreement is high , with a Fleiss 's ? of 0.73 .",Evaluation,Evaluation,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.5454545454545454,144,0.3564356435643564,13,0.9285714285714286,1,results,Evaluation,natural_language_inference17
2708,156,We implement our models in the AllenNLP library .,BASELINES,See Appendix,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']",5,0.8333333333333334,155,0.3836633663366336,5,0.3571428571428571,1,hyperparameters,BASELINES: See Appendix,natural_language_inference17
2709,157,Original code for the baselines is available at https://github.com/nyu-mll/GLUE-baselines and a newer version is available at https://github.com/jsalt18-sentence-repl/jiant.,BASELINES,See Appendix,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']",6,1.0,156,0.3861386138613861,6,0.4285714285714285,1,code,BASELINES: See Appendix,natural_language_inference17
2710,172,"We train our models with Adam ( Kingma & Ba , 2015 ) with initial learning rate 10 ? 4 and batch size 128 .",Pre-Training,Pre-Training,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O']",6,0.6666666666666666,171,0.4232673267326732,6,0.5,1,hyperparameters,Pre-Training,natural_language_inference17
2711,2,Parameter Re-Initialization through Cyclical Batch Size Schedules,title,,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0049751243781094,1,0.0,1,research-problem,title,natural_language_inference18
2712,19,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",9,0.36,18,0.0895522388059701,9,0.36,1,model,Introduction,natural_language_inference18
2713,23,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .",Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",13,0.52,22,0.1094527363184079,13,0.52,1,model,Introduction,natural_language_inference18
2714,29,"We explore different cyclical batch size ( CBS ) schedules for training neural networks inspired by Bayesian statistics , particularly adaptive MCMC methods .",Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.76,28,0.1393034825870646,19,0.76,1,model,Introduction,natural_language_inference18
2715,34,We propose a simple but effective ensembling method that combines models saved during different cycles at no additional training cost .,Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.96,33,0.1641791044776119,24,0.96,1,model,Introduction,natural_language_inference18
2716,82,Language Results,,,natural_language_inference,18,"['O', 'O']","['B-n', 'I-n']",0,0.0,81,0.4029850746268656,0,0.0,1,experiments,,natural_language_inference18
2717,83,Language modeling is a challenging problem due to the complex and long - range interactions between distant words .,Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0526315789473684,82,0.4079601990049751,1,0.0526315789473684,1,results,Language Results,natural_language_inference18
2718,88,"As we can see , the best performing CBS schedules result in significant improvements in perplexity ( up to 7.91 ) over the baseline schedules and also offer reductions in the number of SGD training iterations ( up to 33 % ) .",Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O']",6,0.3157894736842105,87,0.4328358208955223,6,0.3157894736842105,1,results,Language Results,natural_language_inference18
2719,90,Notice that almost all CBS schedules outperform the baseline schedule .,Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'O']",8,0.4210526315789473,89,0.4427860696517413,8,0.4210526315789473,1,results,Language Results,natural_language_inference18
2720,101,"We see that the CBS schedules match baseline performance , but the number of training iterations used in CBS schedules is up to 2 fewer .",Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",19,1.0,100,0.4975124378109453,19,1.0,1,results,Language Results,natural_language_inference18
2721,105,We observe that CBS achieves similar performance to the baseline .,Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",3,0.1428571428571428,104,0.5174129353233831,3,0.3,1,results,Image Classification Results,natural_language_inference18
2722,107,"With CBS - 15 , we see 90.71 % training accuracy and 56. 44 % testing accuracy , which is a larger improvement than that offered by CBS on convolutional models on Cifar - 10 .",Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.238095238095238,106,0.527363184079602,5,0.5,1,results,Image Classification Results,natural_language_inference18
2723,109,Combining CBS - 15 on C2 with this strategy improves accuracy to 94.82 % .,Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",7,0.3333333333333333,108,0.5373134328358209,7,0.7,1,results,Image Classification Results,natural_language_inference18
2724,111,Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % .,Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",9,0.4285714285714285,110,0.5472636815920398,9,0.9,1,results,Image Classification Results,natural_language_inference18
2725,112,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .",Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",10,0.4761904761904761,111,0.5522388059701493,10,1.0,1,results,Image Classification Results,natural_language_inference18
2726,155,"During training , we crop the image to 224 224 . PTB ( language modeling ) .",A Training Details,Natural Language Inference .,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.6,154,0.7661691542288557,15,0.2542372881355932,1,experiments,A Training Details: Natural Language Inference .,natural_language_inference18
2727,158,"The total vocabulary size is 10 k , and all words outside the vocabulary are replaced by a placeholder token . WikiText 2 ( language modeling ) .",A Training Details,Natural Language Inference .,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.72,157,0.7810945273631841,18,0.3050847457627119,1,experiments,A Training Details: Natural Language Inference .,natural_language_inference18
2728,2,Natural Language Inference by Tree - Based Convolution and Heuristic Matching,title,title,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.00625,1,0.0,1,research-problem,title,natural_language_inference19
2729,4,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .",abstract,abstract,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.3333333333333333,3,0.01875,1,0.3333333333333333,1,research-problem,abstract,natural_language_inference19
2730,8,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .,Introduction,Introduction,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",1,0.0294117647058823,7,0.04375,1,0.1428571428571428,1,research-problem,Introduction,natural_language_inference19
2731,33,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .",Introduction,E Hypothesis,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",26,0.7647058823529411,32,0.2,18,0.6923076923076923,1,model,Introduction: E Hypothesis,natural_language_inference19
2732,128,"All our neural layers , including embeddings , were set to 300 dimensions .",Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",9,0.45,127,0.79375,1,0.125,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2733,129,"The model is mostly robust when the dimension is large , e.g. , several hundred .",Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.5,128,0.8,2,0.25,1,results,Evaluation: Hyperparameter Settings,natural_language_inference19
2734,130,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-p', 'B-n', 'I-n', 'O']",11,0.55,129,0.80625,3,0.375,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2735,131,We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'O', 'B-p', 'B-n', 'O']",12,0.6,130,0.8125,4,0.5,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2736,134,"Initial learning rate was set to 1 , and a power decay was applied .",Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O']",15,0.75,133,0.83125,7,0.875,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2737,135,We used stochastic gradient descent with a batch size of 50 .,Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",16,0.8,134,0.8375,8,1.0,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2738,138,"As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of humanengineered features , long short term memory .",Evaluation,Performance,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.95,137,0.85625,2,0.6666666666666666,1,results,Evaluation: Performance,natural_language_inference19
2739,140,Model Variant,,,natural_language_inference,19,"['O', 'O']","['B-n', 'I-n']",0,0.0,139,0.86875,0,0.0,1,experiments,,natural_language_inference19
2740,144,We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,143,0.89375,4,0.4,1,results,Model Variant,natural_language_inference19
2741,145,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .",Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.3125,144,0.9,5,0.5,1,results,Model Variant,natural_language_inference19
2742,148,Further applying element - wise product improves the accuracy by another 0.5 % .,Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.5,147,0.91875,8,0.8,1,results,Model Variant,natural_language_inference19
2743,149,"The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .",Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.5625,148,0.925,9,0.9,1,results,Model Variant,natural_language_inference19
2744,2,Stochastic Answer Networks for Natural Language Inference,title,,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0072992700729927,1,0.0,1,research-problem,title,natural_language_inference2
2745,9,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .",abstract,Motivation,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0909090909090909,8,0.0583941605839416,6,0.0909090909090909,1,research-problem,abstract: Motivation,natural_language_inference2
2746,17,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .",abstract,Motivation,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']",14,0.2121212121212121,16,0.1167883211678832,14,0.2121212121212121,1,research-problem,abstract: Motivation,natural_language_inference2
2747,80,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",1,0.0833333333333333,79,0.5766423357664233,1,0.0833333333333333,1,experimental-setup,Implementation details,natural_language_inference2
2748,81,We fix word embedding with 300 - dimensional GloVe word vectors .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.1666666666666666,80,0.583941605839416,2,0.1666666666666666,1,experimental-setup,Implementation details,natural_language_inference2
2749,82,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.25,81,0.5912408759124088,3,0.25,1,experimental-setup,Implementation details,natural_language_inference2
2750,84,So lexicon embeddings are d =600 - dimensions .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.4166666666666667,83,0.6058394160583942,5,0.4166666666666667,1,experimental-setup,Implementation details,natural_language_inference2
2751,85,The embedding for the out - of - vocabulary is zeroed .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.5,84,0.6131386861313869,6,0.5,1,experimental-setup,Implementation details,natural_language_inference2
2752,86,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",7,0.5833333333333334,85,0.6204379562043796,7,0.5833333333333334,1,experimental-setup,Implementation details,natural_language_inference2
2753,87,The projection size in the attention layer is set to 256 .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",8,0.6666666666666666,86,0.6277372262773723,8,0.6666666666666666,1,experimental-setup,Implementation details,natural_language_inference2
2754,88,"To speedup training , we use weight normalization .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",9,0.75,87,0.635036496350365,9,0.75,1,experimental-setup,Implementation details,natural_language_inference2
2755,89,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",10,0.8333333333333334,88,0.6423357664233577,10,0.8333333333333334,1,experimental-setup,Implementation details,natural_language_inference2
2756,90,The mini - batch size is set to 32 .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",11,0.9166666666666666,89,0.6496350364963503,11,0.9166666666666666,1,experimental-setup,Implementation details,natural_language_inference2
2757,91,Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",12,1.0,90,0.656934306569343,12,1.0,1,experimental-setup,Implementation details,natural_language_inference2
2758,128,"Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .",Model,Model,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.7333333333333333,127,0.927007299270073,11,0.7333333333333333,1,results,Model,natural_language_inference2
2759,129,"We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .",Model,Model,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.8,128,0.9343065693430656,12,0.8,1,results,Model,natural_language_inference2
2760,130,"Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .",Model,Model,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.8666666666666667,129,0.9416058394160584,13,0.8666666666666667,1,results,Model,natural_language_inference2
2761,2,Neural Tree Indexers for Text Understanding,title,,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0032679738562091,1,0.0,1,research-problem,title,natural_language_inference20
2762,8,NTI constructs a full n-ary tree by processing the input text with its node function in a bottom - up fashion .,abstract,abstract,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.7142857142857143,7,0.022875816993464,5,0.7142857142857143,1,research-problem,abstract,natural_language_inference20
2763,19,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .",Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1702127659574468,18,0.0588235294117647,8,0.3636363636363636,1,model,Introduction,natural_language_inference20
2764,22,"Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling .",Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2340425531914893,21,0.0686274509803921,11,0.5,1,model,Introduction,natural_language_inference20
2765,24,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :",Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",13,0.2765957446808511,23,0.0751633986928104,13,0.5909090909090909,1,model,Introduction,natural_language_inference20
2766,29,( b ) NTI learns representations for the premise and hypothesis sentences and then attentively combines them for classification .,Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",18,0.3829787234042553,28,0.0915032679738562,18,0.8181818181818182,1,model,Introduction,natural_language_inference20
2767,136,We trained NTI using Adam with hyperparameters selected on development set .,Experiments,Experiments,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0125786163522012,135,0.4411764705882353,2,0.1666666666666666,1,hyperparameters,Experiments,natural_language_inference20
2768,137,The pre-trained 300 - D Glove 840B vectors were obtained for the word embeddings,Experiments,Experiments,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n']",3,0.0188679245283018,136,0.4444444444444444,3,0.25,1,hyperparameters,Experiments,natural_language_inference20
2769,139,The word embeddings are fixed during training .,Experiments,Experiments,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",5,0.0314465408805031,138,0.4509803921568627,5,0.4166666666666667,1,hyperparameters,Experiments,natural_language_inference20
2770,140,The embeddings for out - ofvocabulary words were set to zero vector .,Experiments,Experiments,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",6,0.0377358490566037,139,0.4542483660130719,6,0.5,1,hyperparameters,Experiments,natural_language_inference20
2771,144,The size of hidden units of the NTI modules were set to 300 .,Experiments,Experiments,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",10,0.0628930817610062,143,0.4673202614379085,10,0.8333333333333334,1,hyperparameters,Experiments,natural_language_inference20
2772,145,The models were regularized by using dropouts and an l 2 weight decay .,Experiments,Experiments,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.0691823899371069,144,0.4705882352941176,11,0.9166666666666666,1,hyperparameters,Experiments,natural_language_inference20
2773,147,Natural Language Inference,Experiments,,natural_language_inference,20,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",13,0.0817610062893081,146,0.477124183006536,0,0.0,1,experiments,Experiments,natural_language_inference20
2774,152,"For each model , we set the batch size to 32 .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",18,0.1132075471698113,151,0.4934640522875817,5,0.0892857142857142,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2775,153,"The initial learning , the regularization strength and the number of epoch to be trained are varied for each model .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-n', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",19,0.1194968553459119,152,0.4967320261437908,6,0.1071428571428571,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2776,154,NTI - SLSTM : this model does not rely on f leaf transformer but uses the S - LSTM units for the non-leaf node function .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",20,0.1257861635220125,153,0.5,7,0.125,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2777,155,"We set the initial learning rate to 1e - 3 and l 2 regularizer strength to 3 e - 5 , and train the model for 90 epochs .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",21,0.1320754716981132,154,0.5032679738562091,8,0.1428571428571428,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2778,156,The neural net was regularized by 10 % input dropouts and the 20 % output dropouts .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.1383647798742138,155,0.5065359477124183,9,0.1607142857142857,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2779,157,NTI - SLSTM - LSTM : we use LSTM for the leaf node function f leaf .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",23,0.1446540880503144,156,0.5098039215686274,10,0.1785714285714285,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2780,162,NTI - SLSTM node - by - node global attention :,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",28,0.1761006289308176,161,0.5261437908496732,15,0.2678571428571428,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2781,165,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 40 epochs .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",31,0.1949685534591195,164,0.5359477124183006,18,0.3214285714285714,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2782,166,The neural net was regularized by 15 % input dropouts and the 15 % output dropouts .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",32,0.2012578616352201,165,0.5392156862745098,19,0.3392857142857143,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2783,167,NTI - SLSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.2075471698113207,166,0.5424836601307189,20,0.3571428571428571,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2784,171,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 10 epochs .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",37,0.2327044025157232,170,0.5555555555555556,24,0.4285714285714285,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2785,172,The neural net was regularized by 10 % input dropouts and the 15 % output dropouts .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",38,0.2389937106918239,171,0.5588235294117647,25,0.4464285714285714,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2786,173,NTI - SLSTM - LSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.2452830188679245,172,0.5620915032679739,26,0.4642857142857143,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2787,175,Tree matching NTI - SLSTM - LSTM global attention : this model first constructs the premise and hypothesis trees simultaneously with the NTI - SLSTM - LSTM model and then computes their matching vector by using the global attention and an additional LSTM .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",41,0.2578616352201258,174,0.5686274509803921,28,0.5,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2788,180,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train the model for 20 epochs .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",46,0.2893081761006289,179,0.5849673202614379,33,0.5892857142857143,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2789,181,The neural net was regularized by 20 % input dropouts and the 20 % output dropouts .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",47,0.2955974842767295,180,0.5882352941176471,34,0.6071428571428571,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2790,182,Tree matching NTI - SLSTM - LSTM tree attention : we replace the global attention with the tree attention .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",48,0.3018867924528302,181,0.5915032679738562,35,0.625,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2791,184,"Full tree matching NTI - SLSTM - LSTM global attention : this model produces two sets of the attention vectors , one by attending over the premise tree regarding each hypothesis tree node and another by attending over the hypothesis tree regarding each premise tree node .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",50,0.3144654088050314,183,0.5980392156862745,37,0.6607142857142857,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2792,193,Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",59,0.3710691823899371,192,0.6274509803921569,46,0.8214285714285714,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2793,195,Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",61,0.3836477987421384,194,0.6339869281045751,48,0.8571428571428571,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2794,196,"Not surprisingly , using LSTM as leaf node function helps in learning better representations .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O']",62,0.389937106918239,195,0.6372549019607843,49,0.875,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2795,197,Our NTI - SLSTM - LSTM is a hybrid model which encodes a sequence sequentially through its leaf node function and then hierarchically composes the output representations .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",63,0.3962264150943396,196,0.6405228758169934,50,0.8928571428571429,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2796,198,"The node - by - node attention models improve the performance , indicating that modeling inter-sentence interaction is an important element in NLI .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.4025157232704403,197,0.6437908496732027,51,0.9107142857142856,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2797,221,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .",Experiments,Answer Sentence Selection,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",87,0.5471698113207547,220,0.7189542483660131,17,0.85,1,experiments,Experiments: Answer Sentence Selection,natural_language_inference20
2798,222,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,Experiments,Answer Sentence Selection,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",88,0.5534591194968553,221,0.7222222222222222,18,0.9,1,experiments,Experiments: Answer Sentence Selection,natural_language_inference20
2799,263,"Surprisingly , attention degree for the single word expression like "" stone "" , "" wall "" and "" leaves "" is lower to compare with multiword phrases .",Experiments,Attention and Compositionality,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']",129,0.8113207547169812,262,0.8562091503267973,6,0.6,1,results,Experiments: Attention and Compositionality,natural_language_inference20
2800,276,Overall the NTI model is robust to the length of the phrases being matched .,Experiments,Learned Representations of Phrases and Sentences,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'O']",142,0.8930817610062893,275,0.8986928104575164,8,0.6153846153846154,1,experiments,Experiments: Learned Representations of Phrases and Sentences,natural_language_inference20
2801,288,"When the padding size is less ( up to 10 ) , the NTI - SLSTM - LSTM model performs better .",Experiments,Effects of Padding Size,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",154,0.9685534591194968,287,0.937908496732026,6,0.5454545454545454,1,results,Experiments: Effects of Padding Size,natural_language_inference20
2802,290,Overall we do not observe any significant performance drop for both models as the padding size increases .,Experiments,Effects of Padding Size,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']",156,0.981132075471698,289,0.9444444444444444,8,0.7272727272727273,1,results,Experiments: Effects of Padding Size,natural_language_inference20
2803,2,Attention - over - Attention Neural Networks for Reading Comprehension,title,title,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.004524886877828,1,0.0,1,research-problem,title,natural_language_inference21
2804,8,"In addition to the primary model , we also propose an N - best re-ranking strategy to double check the validity of the candidates and further improve the performance .",abstract,abstract,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O']",5,0.8333333333333334,7,0.0316742081447963,5,0.8333333333333334,1,model,abstract,natural_language_inference21
2805,20,"In this paper , we present a novel neural network architecture , called attention - over - attention model .",Introduction,Introduction,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.0787401574803149,19,0.085972850678733,10,0.4,1,model,Introduction,natural_language_inference21
2806,26,We also propose an N - best re-ranking strategy to re-score the candidates in various aspects and further improve the performance .,Introduction,Introduction,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O']",16,0.1259842519685039,25,0.1131221719457013,16,0.64,1,model,Introduction,natural_language_inference21
2807,57,Children 's Book Test,Introduction,,natural_language_inference,21,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",47,0.3700787401574803,56,0.253393665158371,16,0.2253521126760563,1,experiments,Introduction,natural_language_inference21
2808,141,Embedding Layer :,Experimental Setups,Experimental Setups,natural_language_inference,21,"['O', 'O', 'O']","['B-n', 'I-n', 'O']",2,0.1,140,0.6334841628959276,2,0.1,1,baselines,Experimental Setups,natural_language_inference21
2809,142,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ].",Experimental Setups,Experimental Setups,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",3,0.15,141,0.6380090497737556,3,0.15,1,experimental-setup,Experimental Setups,natural_language_inference21
2810,145,CB Test NE CB Test CN Valid Test Valid Test Valid Test,Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O']",6,0.3,144,0.6515837104072398,6,0.3,1,experiments,Experimental Setups: News,natural_language_inference21
2811,148,Hidden Layer : Internal weights of GRUs are initialized with random orthogonal matrices .,Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",9,0.45,147,0.665158371040724,9,0.45,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2812,150,"We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",11,0.55,149,0.6742081447963801,11,0.55,1,experiments,Experimental Setups: News,natural_language_inference21
2813,151,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",12,0.6,150,0.6787330316742082,12,0.6,1,experiments,Experimental Setups: News,natural_language_inference21
2814,152,We used batched training strategy of 32 samples .,Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",13,0.65,151,0.6832579185520362,13,0.65,1,experiments,Experimental Setups: News,natural_language_inference21
2815,154,"In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,153,0.6923076923076923,15,0.75,1,experiments,Experimental Setups: News,natural_language_inference21
2816,155,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",16,0.8,154,0.6968325791855203,16,0.8,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2817,157,"The ensemble model is made up of four best models , which are trained using different random seed .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",18,0.9,156,0.7058823529411765,18,0.9,1,experiments,Experimental Setups: News,natural_language_inference21
2818,158,"Implementation is done with Theano ( Theano Development Team , 2016 ) and Keras , and all models are trained on Tesla K40 GPU . :",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O']",19,0.95,157,0.7104072398190046,19,0.95,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2819,163,"As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0909090909090909,162,0.7330316742081447,3,0.375,1,results,Overall Results,natural_language_inference21
2820,164,"Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.1212121212121212,163,0.7375565610859729,4,0.5,1,results,Overall Results,natural_language_inference21
2821,165,"We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.1515151515151515,164,0.7420814479638009,5,0.625,1,results,Overall Results,natural_language_inference21
2822,166,"When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.1818181818181818,165,0.746606334841629,6,0.75,1,results,Overall Results,natural_language_inference21
2823,168,"Instead of using pre-defined merging heuristics , and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance , where 4.1 % and 3.7 % improvements can be made in CNN validation and test set against CAS Reader .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",8,0.2424242424242424,167,0.755656108597285,8,1.0,1,results,Overall Results,natural_language_inference21
2824,2,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems,title,title,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",1,0.0,1,0.0045454545454545,1,0.0,1,research-problem,title,natural_language_inference22
2825,4,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,abstract,abstract,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.0136363636363636,1,0.032258064516129,1,research-problem,abstract,natural_language_inference22
2826,5,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,abstract,abstract,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",2,0.3333333333333333,4,0.0181818181818181,2,0.064516129032258,1,research-problem,abstract,natural_language_inference22
2827,8,"In addition , we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor ( ANN ) technology , which enables us to find the most similar question from all available candidates very quickly during online serving .",abstract,abstract,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",5,0.8333333333333334,7,0.0318181818181818,5,0.1612903225806451,1,research-problem,abstract,natural_language_inference22
2828,11,Question answering systems have been widely studied in both the academic and industrial community and are widely applied to various scenarios .,I. INTRODUCTION,I. INTRODUCTION,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0217391304347826,10,0.0454545454545454,8,0.2580645161290322,1,research-problem,I. INTRODUCTION,natural_language_inference22
2829,14,"In this work , we focus on building an IR - based QA system to answer the Frequently Asked Questions ( FAQ ) .",I. INTRODUCTION,I. INTRODUCTION,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.0869565217391304,13,0.059090909090909,11,0.3548387096774194,1,model,I. INTRODUCTION,natural_language_inference22
2830,15,"The critical part of IRbased QA system is to find the most similar question from a massive QA knowledge base , which could be further reformulated as a Paraphrase Identification ( PI ) problem , also known as sentence matching .",I. INTRODUCTION,I. INTRODUCTION,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",5,0.108695652173913,14,0.0636363636363636,12,0.3870967741935484,1,research-problem,I. INTRODUCTION,natural_language_inference22
2831,28,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .",I. INTRODUCTION,I. INTRODUCTION,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",18,0.391304347826087,27,0.1227272727272727,25,0.8064516129032258,1,model,I. INTRODUCTION,natural_language_inference22
2832,29,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .",I. INTRODUCTION,I. INTRODUCTION,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",19,0.4130434782608695,28,0.1272727272727272,26,0.8387096774193549,1,model,I. INTRODUCTION,natural_language_inference22
2833,36,Natural language sentence matching ( NLSM ) has gone through substantial developments in recent years .,I. INTRODUCTION,A. Natural Language Sentence Matching,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5652173913043478,35,0.1590909090909091,1,0.0833333333333333,1,research-problem,I. INTRODUCTION: A. Natural Language Sentence Matching,natural_language_inference22
2834,38,"For the paraphrase identification ( PI ) task , NLSM is utilized to determine whether two sentences are paraphrases or not .",I. INTRODUCTION,A. Natural Language Sentence Matching,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.6086956521739131,37,0.1681818181818181,3,0.25,1,research-problem,I. INTRODUCTION: A. Natural Language Sentence Matching,natural_language_inference22
2835,149,Bank Question ( BQ ) dataset is a specific - domain Chinese dataset for sentence semantic equivalence identification ( SSEI ) .,EXPERIMENTS,Quora Question,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.8181818181818182,148,0.6727272727272727,8,0.8,1,experiments,EXPERIMENTS: Quora Question,natural_language_inference22
2836,153,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",1,0.0434782608695652,152,0.6909090909090909,1,0.125,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2837,154,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",2,0.0869565217391304,153,0.6954545454545454,2,0.25,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2838,155,We set = 0.8 in the multi - task loss function .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.1304347826086956,154,0.7,3,0.375,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2839,157,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",5,0.217391304347826,156,0.7090909090909091,5,0.625,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2840,158,An Adam optimizer is used to optimize all the trainable weights .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.2608695652173913,157,0.7136363636363636,6,0.75,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2841,159,The learning rate is set to 4e - 4 and the batch size is set to 200 .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",7,0.3043478260869565,158,0.7181818181818181,7,0.875,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2842,160,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",8,0.3478260869565217,159,0.7227272727272728,8,1.0,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2843,163,ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",11,0.4782608695652174,162,0.7363636363636363,2,0.1428571428571428,1,baselines,B. Settings of Experiments,natural_language_inference22
2844,164,It uses BiLSTM to encode sentence contexts and uses the attention mechanism to calculate the information between two sentences .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",12,0.5217391304347826,163,0.740909090909091,3,0.2142857142857142,1,baselines,B. Settings of Experiments,natural_language_inference22
2845,165,ESIM has shown excellent performance on the SNLI dataset .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",13,0.5652173913043478,164,0.7454545454545455,4,0.2857142857142857,1,results,B. Settings of Experiments,natural_language_inference22
2846,166,BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-n', 'O']",14,0.6086956521739131,165,0.75,5,0.3571428571428571,1,baselines,B. Settings of Experiments,natural_language_inference22
2847,167,"The model uses a BiLSTM layer to learn the sentence representation , four different types of multiperspective matching layers to match two sentences , an additional BiLSTM layer to aggregate the matching results , and a two - layer feed - forward network for prediction .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",15,0.6521739130434783,166,0.7545454545454545,6,0.4285714285714285,1,baselines,B. Settings of Experiments,natural_language_inference22
2848,168,"SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.6956521739130435,167,0.759090909090909,7,0.5,1,baselines,B. Settings of Experiments,natural_language_inference22
2849,170,DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",18,0.782608695652174,169,0.7681818181818182,9,0.6428571428571429,1,baselines,B. Settings of Experiments,natural_language_inference22
2850,171,It hierarchically extracts semantic features from interaction space to achieve a high - level understanding of the sentence pair .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",19,0.8260869565217391,170,0.7727272727272727,10,0.7142857142857143,1,baselines,B. Settings of Experiments,natural_language_inference22
2851,179,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,178,0.8090909090909091,3,0.25,1,results,D. Results of Experiments,natural_language_inference22
2852,182,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .",D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.5,181,0.8227272727272728,6,0.5,1,results,D. Results of Experiments,natural_language_inference22
2853,184,As shown in show that our MSEM model achieves the best performance .,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",8,0.6666666666666666,183,0.8318181818181818,8,0.6666666666666666,1,results,D. Results of Experiments,natural_language_inference22
2854,187,And the model with multi-task learning further improved performance ranging from 0.4 % to 1 % .,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-n', 'I-n', 'O']",11,0.9166666666666666,186,0.8454545454545455,11,0.9166666666666666,1,results,D. Results of Experiments,natural_language_inference22
2855,188,"Compared with existing models , our model shows great advantages on datasets with low average overlap rate , which is known to be very common in realworld question answering scenarios .",D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1.0,187,0.85,12,1.0,1,results,D. Results of Experiments,natural_language_inference22
2856,196,It turns out that the attentive pooling is better than max pooling .,E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",7,0.7,195,0.8863636363636364,7,0.7,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2857,197,"Then if we remove the highway network , the accuracy will drop to 88.36 % .",E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",8,0.8,196,0.8909090909090909,8,0.8,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2858,198,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .",E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",9,0.9,197,0.8954545454545455,9,0.9,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2859,206,"As shown in , the F 1 score of the new system is 14 . 26 % higher than the baseline system .",F. Online System Evaluation,F. Online System Evaluation,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",6,0.375,205,0.9318181818181818,6,0.8571428571428571,1,results,F. Online System Evaluation,natural_language_inference22
2860,2,Deep Fusion LSTMs for Text Semantic Matching,title,,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",1,0.0,1,0.0046948356807511,1,0.0,1,research-problem,title,natural_language_inference23
2861,4,"Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .",abstract,abstract,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.0140845070422535,1,0.1111111111111111,1,research-problem,abstract,natural_language_inference23
2862,14,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .",Introduction,Introduction,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",1,0.125,13,0.0610328638497652,1,0.0384615384615384,1,research-problem,Introduction,natural_language_inference23
2863,25,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.0384615384615384,24,0.1126760563380281,12,0.4615384615384615,1,model,Strong Interaction Models,natural_language_inference23
2864,28,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",6,0.0769230769230769,27,0.1267605633802817,15,0.5769230769230769,1,research-problem,Strong Interaction Models,natural_language_inference23
2865,29,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O']",7,0.0897435897435897,28,0.1314553990610328,16,0.6153846153846154,1,model,Strong Interaction Models,natural_language_inference23
2866,30,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'O']",8,0.1025641025641025,29,0.136150234741784,17,0.6538461538461539,1,model,Strong Interaction Models,natural_language_inference23
2867,31,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",9,0.1153846153846153,30,0.1408450704225352,18,0.6923076923076923,1,model,Strong Interaction Models,natural_language_inference23
2868,34,"Different with previous models , DF - LSTMs model the strong interactions of two texts in a recursive matching way , which consist of two inter -and intra-dependent LSTMs .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.1538461538461538,33,0.1549295774647887,21,0.8076923076923077,1,model,Strong Interaction Models,natural_language_inference23
2869,40,Recursively Text Semantic Matching,Strong Interaction Models,,natural_language_inference,23,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",18,0.2307692307692307,39,0.1830985915492957,0,0.0,1,research-problem,Strong Interaction Models,natural_language_inference23
2870,53,Long Short - Term Memory Network,Strong Interaction Models,,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",31,0.3974358974358974,52,0.244131455399061,13,0.5,1,model,Strong Interaction Models,natural_language_inference23
2871,67,Deep Fusion LSTMs for Recursively Semantic Matching,Strong Interaction Models,,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']",45,0.5769230769230769,66,0.3098591549295774,0,0.0,1,research-problem,Strong Interaction Models,natural_language_inference23
2872,139,Neural bag - of - words ( NBOW ) :,Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0714285714285714,138,0.647887323943662,1,0.0714285714285714,1,baselines,Competitor Methods,natural_language_inference23
2873,140,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",2,0.1428571428571428,139,0.6525821596244131,2,0.1428571428571428,1,baselines,Competitor Methods,natural_language_inference23
2874,141,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2142857142857142,140,0.6572769953051644,3,0.2142857142857142,1,baselines,Competitor Methods,natural_language_inference23
2875,142,"Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']",4,0.2857142857142857,141,0.6619718309859155,4,0.2857142857142857,1,baselines,Competitor Methods,natural_language_inference23
2876,143,"Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",5,0.3571428571428571,142,0.6666666666666666,5,0.3571428571428571,1,baselines,Competitor Methods,natural_language_inference23
2877,144,"Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4285714285714285,143,0.6713615023474179,6,0.4285714285714285,1,baselines,Competitor Methods,natural_language_inference23
2878,146,Experiment - I : Recognizing Textual Entailment,Competitor Methods,,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",8,0.5714285714285714,145,0.6807511737089202,8,0.5714285714285714,1,results,Competitor Methods,natural_language_inference23
2879,152,"The results of DF - LSTMs outperform all the competitor models with the same number of hidden states while achieving comparable results to the state - of - the - art and using much fewer parameters , which indicate that it is effective to model the strong interactions of two texts in a recursive matching way .",Competitor Methods,Experiment - I : Recognizing Textual Entailment,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,1.0,151,0.7089201877934272,14,1.0,1,results,Competitor Methods: Experiment - I : Recognizing Textual Entailment,natural_language_inference23
2880,156,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .",Results,Results,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0697674418604651,155,0.7276995305164319,3,0.1428571428571428,1,results,Results,natural_language_inference23
2881,13,Using Wikipedia articles as the knowledge source causes the task of question answering ( QA ) to combine the challenges of both large - scale open - domain QA and of machine comprehension of text .,Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2272727272727272,12,0.0535714285714285,5,0.2272727272727272,1,research-problem,Introduction,natural_language_inference24
2882,15,"We term this setting , machine reading at scale ( MRS ) .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.3181818181818182,14,0.0625,7,0.3181818181818182,1,research-problem,Introduction,natural_language_inference24
2883,16,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3636363636363636,15,0.0669642857142857,8,0.3636363636363636,1,model,Introduction,natural_language_inference24
2884,24,"Instead MRS is focused on simultaneously maintaining the challenge of machine comprehension , which requires the deep understanding of text , while keeping the realistic constraint of searching over a large open resource .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.7272727272727273,23,0.1026785714285714,16,0.7272727272727273,1,model,Introduction,natural_language_inference24
2885,25,"In this paper , we show how multiple existing QA datasets can be used to evaluate MRS by requiring an open - domain system to perform well on all of them at once .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",17,0.7727272727272727,24,0.1071428571428571,17,0.7727272727272727,1,model,Introduction,natural_language_inference24
2886,26,"We develop DrQA , a strong system for question answering from Wikipedia composed of : ( 1 ) Document Retriever , a module using bigram hashing and TF - IDF matching designed to , given a question , efficiently return a subset of relevant articles and ( 2 ) Document Reader , a multi - layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'O']",18,0.8181818181818182,25,0.1116071428571428,18,0.8181818181818182,1,model,Introduction,natural_language_inference24
2887,59,Our System : DrQA,,,natural_language_inference,24,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-n']",0,0.0,58,0.2589285714285714,28,0.9655172413793104,1,experiments,,natural_language_inference24
2888,136,WikiMovies,Our System : DrQA,,natural_language_inference,24,['O'],['B-n'],77,0.7403846153846154,135,0.6026785714285714,24,0.9230769230769232,1,results,Our System : DrQA,natural_language_inference24
2889,174,We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,Implementation details,Implementation details,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1428571428571428,173,0.7723214285714286,2,0.25,1,experimental-setup,Implementation details,natural_language_inference24
2890,175,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .",Implementation details,Implementation details,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",2,0.2857142857142857,174,0.7767857142857143,3,0.375,1,experimental-setup,Implementation details,natural_language_inference24
2891,177,We use Adamax for optimization as described in .,Implementation details,,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O']",4,0.5714285714285714,176,0.7857142857142857,5,0.625,1,experimental-setup,Implementation details,natural_language_inference24
2892,178,Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,Implementation details,We use Adamax for optimization as described in .,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",5,0.7142857142857143,177,0.7901785714285714,6,0.75,1,experimental-setup,Implementation details: We use Adamax for optimization as described in .,natural_language_inference24
2893,182,"Our system ( single model ) can achieve 70.0 % exact match and 79.0 % F 1 scores on the test set , which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing .",Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0666666666666666,181,0.8080357142857143,1,0.1666666666666666,1,results,Result and analysis,natural_language_inference24
2894,185,As shown in all the features contribute to the performance of our final system .,Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.2666666666666666,184,0.8214285714285714,4,0.6666666666666666,1,results,Result and analysis,natural_language_inference24
2895,186,"Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .",Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",5,0.3333333333333333,185,0.8258928571428571,5,0.8333333333333334,1,results,Result and analysis,natural_language_inference24
2896,191,SQuAD : A single Document Reader model is trained on the SQuAD training set only and used on all evaluation sets .,Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",10,0.6666666666666666,190,0.8482142857142857,3,0.375,1,baselines,Result and analysis,natural_language_inference24
2897,192,Fine-tune ( DS ) : A Document Reader model is pre-trained on SQuAD and then fine - tuned for each dataset independently using its distant supervision ( DS ) training set .,Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.7333333333333333,191,0.8526785714285714,4,0.5,1,baselines,Result and analysis,natural_language_inference24
2898,193,Multitask ( DS ) :,Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.8,192,0.8571428571428571,5,0.625,1,baselines,Result and analysis,natural_language_inference24
2899,201,"Despite the difficulty of the task compared to machine comprehension ( where you are given the right paragraph ) and unconstrained QA ( using redundant resources ) , Dr QA still provides reasonable performance across all four datasets .",Method Dev,Results presents the results .,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",4,0.2222222222222222,200,0.8928571428571429,4,0.2222222222222222,1,results,Method Dev: Results presents the results .,natural_language_inference24
2900,2,A Deep Cascade Model for Multi - Document Reading Comprehension,title,title,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0038910505836575,1,0.0,1,research-problem,title,natural_language_inference25
2901,13,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0188679245283018,12,0.046692607003891,1,0.0285714285714285,1,research-problem,Introduction,natural_language_inference25
2902,29,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",17,0.320754716981132,28,0.1089494163424124,17,0.4857142857142857,1,model,Introduction,natural_language_inference25
2903,32,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",20,0.3773584905660377,31,0.1206225680933852,20,0.5714285714285714,1,model,Introduction,natural_language_inference25
2904,33,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3962264150943396,32,0.1245136186770428,21,0.6,1,model,Introduction,natural_language_inference25
2905,34,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",22,0.4150943396226415,33,0.1284046692607003,22,0.6285714285714286,1,model,Introduction,natural_language_inference25
2906,35,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O']",23,0.4339622641509434,34,0.1322957198443579,23,0.6571428571428571,1,model,Introduction,natural_language_inference25
2907,36,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",24,0.4528301886792453,35,0.1361867704280155,24,0.6857142857142857,1,model,Introduction,natural_language_inference25
2908,38,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",26,0.490566037735849,37,0.1439688715953307,26,0.7428571428571429,1,model,Introduction,natural_language_inference25
2909,39,"For each of the first two modules , we define a ranking function and an extraction function .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",27,0.5094339622641509,38,0.1478599221789883,27,0.7714285714285715,1,model,Introduction,natural_language_inference25
2910,40,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",28,0.5283018867924528,39,0.1517509727626459,28,0.8,1,model,Introduction,natural_language_inference25
2911,41,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",29,0.5471698113207547,40,0.1556420233463035,29,0.8285714285714286,1,model,Introduction,natural_language_inference25
2912,43,"The main contributions can be summarized as follow : We propose a deep cascade learning framework to address the practical multi-document machine reading comprehension task , which considers both the effectiveness and efficiency in a coarse - to - fine manner .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']",31,0.5849056603773585,42,0.1634241245136186,31,0.8857142857142857,1,model,Introduction,natural_language_inference25
2913,44,"We incorporate the auxiliary document extraction and paragraph extraction tasks to the pure answer span prediction , which helps to narrow down the search space and improves the final extraction result in multi-document MRC scenario .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6037735849056604,43,0.1673151750972762,32,0.9142857142857144,1,model,Introduction,natural_language_inference25
2914,48,Related Work Machine Reading Comprehension,Introduction,,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n']",36,0.6792452830188679,47,0.1828793774319066,0,0.0,1,research-problem,Introduction,natural_language_inference25
2915,59,Cascade Learning,Introduction,,natural_language_inference,25,"['O', 'O']","['B-n', 'I-n']",47,0.8867924528301887,58,0.22568093385214,0,0.0,1,model,Introduction,natural_language_inference25
2916,209,"Since the Trivia QA documents often contain many small paragraphs , we also restructure the documents by merging consecutive paragraphs to a maximum size of 600 words for each paragraph as in ( Clark and Gardner 2017 ) .",Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,208,0.8093385214007782,3,0.073170731707317,1,experimental-setup,Implementation Details,natural_language_inference25
2917,211,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .",Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",5,0.4166666666666667,210,0.8171206225680934,5,0.1219512195121951,1,experimental-setup,Implementation Details,natural_language_inference25
2918,212,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",6,0.5,211,0.8210116731517509,6,0.1463414634146341,1,experimental-setup,Implementation Details,natural_language_inference25
2919,213,The word embeddings are fixed during training .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",7,0.5833333333333334,212,0.8249027237354085,7,0.1707317073170731,1,experimental-setup,Implementation Details,natural_language_inference25
2920,214,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O']",8,0.6666666666666666,213,0.8287937743190662,8,0.1951219512195122,1,experimental-setup,Implementation Details,natural_language_inference25
2921,215,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",9,0.75,214,0.8326848249027238,9,0.2195121951219512,1,experimental-setup,Implementation Details,natural_language_inference25
2922,217,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",11,0.9166666666666666,216,0.8404669260700389,11,0.2682926829268293,1,experimental-setup,Implementation Details,natural_language_inference25
2923,221,"We can see that by adopting the deep cascade learning framework , the proposed model outperforms the previous state - of - the - art methods by an evident margin on both datasets , which validates the effectiveness of the proposed method in addressing the challenging multi-document MRC task .",Main Results,Main Results,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'O']",2,1.0,220,0.8560311284046692,15,0.3658536585365853,1,results,Main Results,natural_language_inference25
2924,226,"From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2105263157894736,225,0.8754863813229572,20,0.4878048780487805,1,ablation-analysis,Ablation Study,natural_language_inference25
2925,228,"2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",6,0.3157894736842105,227,0.8832684824902723,22,0.5365853658536586,1,ablation-analysis,Ablation Study,natural_language_inference25
2926,230,"Jointly training the three extraction tasks can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",8,0.4210526315789473,229,0.8910505836575876,24,0.5853658536585366,1,ablation-analysis,Ablation Study,natural_language_inference25
2927,231,Effectiveness v.s. Efficiency Trade - off,Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",9,0.4736842105263157,230,0.8949416342412452,25,0.6097560975609756,1,ablation-analysis,Ablation Study,natural_language_inference25
2928,233,The result on DuReader development set is presented in .,Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",11,0.5789473684210527,232,0.9027237354085604,27,0.6585365853658537,1,ablation-analysis,Ablation Study,natural_language_inference25
2929,234,"We can see that : 1 ) By properly taking more documents or paragraphs into consideration , the performance of the model gradually increases when it reaches 4 documents and 2 paragraphs , and then the performance decreases slightly which maybe due to that much noisy data is introduced .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947368421,233,0.906614785992218,28,0.6829268292682927,1,ablation-analysis,Ablation Study,natural_language_inference25
2930,235,"2 ) The time cost can be largely reduced by removing more irrelevant documents and paragraphs in the cascade ranking stage , while keeping the performance not change that much .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.6842105263157895,234,0.9105058365758756,29,0.7073170731707317,1,ablation-analysis,Ablation Study,natural_language_inference25
2931,239,The performance of jointly training the answer extraction module with different auxiliary tasks on DuReader development set is shown in Table 5 .,Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.8947368421052632,238,0.926070038910506,33,0.8048780487804879,1,ablation-analysis,Ablation Study,natural_language_inference25
2932,240,"We can see that by incorporating the auxiliary document extraction or paragraph extraction task in the joint learning framework , the performance can always improve which again shows the advantage of introducing auxiliary tasks for helping to learn shared bottom representations .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9473684210526316,239,0.9299610894941636,34,0.8292682926829268,1,ablation-analysis,Ablation Study,natural_language_inference25
2933,241,"Besides , the performance gain by adding document extraction task is larger , which maybe due to that it can better lay the foundation of the model with that information from different documents can be distinguished .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,1.0,240,0.933852140077821,35,0.8536585365853658,1,ablation-analysis,Ablation Study,natural_language_inference25
2934,243,Results on E-commerce and Tax data,On- line Evaluation,,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.1,242,0.9416342412451362,37,0.902439024390244,1,results,On- line Evaluation,natural_language_inference25
2935,247,"Besides , the performance with respect to F 1 score is also largely improved with the proposed multi-document MRC model , which demonstrates the effectiveness of our method for removing the rich irrelevant noisy content in our online scenario .",On- line Evaluation,Results on E-commerce and Tax data,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,246,0.9571984435797666,41,1.0,1,results,On- line Evaluation: Results on E-commerce and Tax data,natural_language_inference25
2936,248,Results on Different Document Lengths,On- line Evaluation,,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n']",6,0.6,247,0.9610894941634242,0,0.0,1,results,On- line Evaluation,natural_language_inference25
2937,251,"We can see that without incorporating with the cascade ranking module , the answer extraction module performs rather poorly both in effectiveness and efficiency as the document length increases .",On- line Evaluation,Results on Different Document Lengths,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']",9,0.9,250,0.9727626459143968,3,0.75,1,results,On- line Evaluation: Results on Different Document Lengths,natural_language_inference25
2938,2,U - Net : Machine Reading Comprehension with Unanswerable Questions,title,title,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.0035087719298245,1,0.0,1,research-problem,title,natural_language_inference26
2939,4,Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing .,abstract,abstract,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.0105263157894736,1,0.1428571428571428,1,research-problem,abstract,natural_language_inference26
2940,12,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .",Introduction,Introduction,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0263157894736842,11,0.0385964912280701,1,0.0263157894736842,1,research-problem,Introduction,natural_language_inference26
2941,60,"Glove embedding ( Pennington , Socher , and Manning 2014 ) and Elmo embedding ) are used as basic embeddings .",Proposed Model,Proposed Model,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",10,0.09009009009009,59,0.2070175438596491,10,0.09009009009009,1,experimental-setup,Proposed Model,natural_language_inference26
2942,180,"We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",1,0.1,179,0.6280701754385964,1,0.1,1,experimental-setup,Implementation Details,natural_language_inference26
2943,181,"We use 12 dimensions to embed POS tags , 8 for NER tags .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.2,180,0.631578947368421,2,0.2,1,experimental-setup,Implementation Details,natural_language_inference26
2944,182,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-n', 'O']",3,0.3,181,0.6350877192982456,3,0.3,1,experimental-setup,Implementation Details,natural_language_inference26
2945,183,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.4,182,0.6385964912280702,4,0.4,1,experimental-setup,Implementation Details,natural_language_inference26
2946,184,All the LSTM blocks are bi-directional with one single layer .,Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.5,183,0.6421052631578947,5,0.5,1,experimental-setup,Implementation Details,natural_language_inference26
2947,185,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']",6,0.6,184,0.6456140350877193,6,0.6,1,experimental-setup,Implementation Details,natural_language_inference26
2948,186,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",7,0.7,185,0.6491228070175439,7,0.7,1,experimental-setup,Implementation Details,natural_language_inference26
2949,187,We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,186,0.6526315789473685,8,0.8,1,experimental-setup,Implementation Details,natural_language_inference26
2950,191,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .",Main Results,Main Results,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,190,0.6666666666666666,1,0.2,1,results,Main Results,natural_language_inference26
2951,192,Our model outperforms most of the previous approaches .,Main Results,,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.4,191,0.6701754385964912,2,0.4,1,results,Main Results,natural_language_inference26
2952,193,"Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .",Main Results,Our model outperforms most of the previous approaches .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.6,192,0.6736842105263158,3,0.6,1,results,Main Results: Our model outperforms most of the previous approaches .,natural_language_inference26
2953,194,"In fact , among all the end - to - end models , we achieve the best F1 scores .",Main Results,Our model outperforms most of the previous approaches .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",4,0.8,193,0.6771929824561403,4,0.8,1,results,Main Results: Our model outperforms most of the previous approaches .,natural_language_inference26
2954,206,"Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .",Ablation Study,"First , we remove the universal node U .",natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1851851851851851,205,0.7192982456140351,10,0.2564102564102564,1,ablation-analysis,"Ablation Study: First , we remove the universal node U .",natural_language_inference26
2955,217,"Compared to the bi-attention model , the F1 - score decreases 0.5 % .",Ablation Study,Self - Attn Only 69.7 73.5 - 0.5,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'O']",21,0.3888888888888889,216,0.7578947368421053,21,0.5384615384615384,1,ablation-analysis,Ablation Study: Self - Attn Only 69.7 73.5 - 0.5,natural_language_inference26
2956,218,Multi- task Study,Ablation Study,,natural_language_inference,26,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",22,0.4074074074074074,217,0.7614035087719299,22,0.5641025641025641,1,ablation-analysis,Ablation Study,natural_language_inference26
2957,225,Results ( the first two rows in ) show that there is a large gain when using the multi - task model .,Ablation Study,Multi- task Study,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",29,0.5370370370370371,224,0.7859649122807018,29,0.7435897435897436,1,ablation-analysis,Ablation Study: Multi- task Study,natural_language_inference26
2958,227,"For the answer boundary detection task , we find that the multi -task setup ( i.e. , the classification layer participates in the training process ) does not help its performance .",Ablation Study,Multi- task Study,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'B-n', 'O']",31,0.5740740740740741,226,0.7929824561403509,31,0.7948717948717948,1,ablation-analysis,Ablation Study: Multi- task Study,natural_language_inference26
2959,234,"But as shown above , our model achieves a good score in SQuAD 2.0 test , which shows this model has the potential to achieve higher performance by making progress on both the answer detection and classification tasks .",Ablation Study,But this is not the case .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.7037037037037037,233,0.8175438596491228,38,0.9743589743589745,1,ablation-analysis,Ablation Study: But this is not the case .,natural_language_inference26
2960,235,"Overall , we can conclude that our multi-task model works well since the performance of unanswerability classification improves significantly when the answer pointer and answer verifier work simultaneously .",Ablation Study,But this is not the case .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",39,0.7222222222222222,234,0.8210526315789474,39,1.0,1,ablation-analysis,Ablation Study: But this is not the case .,natural_language_inference26
2961,245,"As we can see , when the threshold is set to 0.5 , F1 score of answerable questions is similar to that of unanswerable questions .",Ablation Study,But this is not the case .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O']",49,0.9074074074074074,244,0.856140350877193,9,0.6428571428571429,1,ablation-analysis,Ablation Study: But this is not the case .,natural_language_inference26
2962,246,"When we increase the threshold ( i.e. , more likely to predict the question as unanswerable ) , performance for answerable questions degrades , and improves for unanswerable questions .",Ablation Study,But this is not the case .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-n', 'O', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'O']",50,0.925925925925926,245,0.8596491228070176,10,0.7142857142857143,1,ablation-analysis,Ablation Study: But this is not the case .,natural_language_inference26
2963,248,"We can see that the overall F 1 score is slightly better , which is consistent with the idea from SQ uAD 2.0 .",Ablation Study,This is as expected .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.9629629629629628,247,0.8666666666666667,12,0.8571428571428571,1,ablation-analysis,Ablation Study: This is as expected .,natural_language_inference26
2964,250,"Finally , we set the threshold to be 0.7 for the submission system to SQuAD evaluation .",Ablation Study,This is as expected .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",54,1.0,249,0.8736842105263158,14,1.0,1,ablation-analysis,Ablation Study: This is as expected .,natural_language_inference26
2965,2,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING,title,title,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']",1,0.0,1,0.006578947368421,1,0.0,1,research-problem,title,natural_language_inference27
2966,4,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,abstract,abstract,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.0197368421052631,1,0.1428571428571428,1,research-problem,abstract,natural_language_inference27
2967,5,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",abstract,abstract,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.0263157894736842,2,0.2857142857142857,1,research-problem,abstract,natural_language_inference27
2968,12,Traditional machine reading comprehension ( MRC ) tasks share the single - turn setting of answering a single question related to a passage .,INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0588235294117647,11,0.0723684210526315,1,0.0588235294117647,1,research-problem,INTRODUCTION,natural_language_inference27
2969,19,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .",INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",8,0.4705882352941176,18,0.1184210526315789,8,0.4705882352941176,1,model,INTRODUCTION,natural_language_inference27
2970,22,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .",INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",11,0.6470588235294118,21,0.1381578947368421,11,0.6470588235294118,1,model,INTRODUCTION,natural_language_inference27
2971,107,"For training , we use all questions / answers for one passage as a batch .",Training .,Training .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",1,0.1111111111111111,106,0.6973684210526315,14,0.6363636363636364,1,hyperparameters,Training .,natural_language_inference27
2972,122,"We compare SDNet with the following baseline models : PGNet ( Seq2 Seq with copy mechanism ) , DrQA , DrQA + PGNet , BiDAF ++ and .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O']",6,0.75,121,0.7960526315789473,6,0.2142857142857142,1,baselines,EXPERIMENTS,natural_language_inference27
2973,128,"As shown , SDNet achieves significantly better results than baseline models .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.4285714285714285,127,0.8355263157894737,12,0.4285714285714285,1,experiments,Results .,natural_language_inference27
2974,129,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'I-n', 'I-n', 'O']",4,0.5714285714285714,128,0.8421052631578947,13,0.4642857142857143,1,experiments,Results .,natural_language_inference27
2975,130,"Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",5,0.7142857142857143,129,0.8486842105263158,14,0.5,1,experiments,Results .,natural_language_inference27
2976,132,"As seen , SDNet overpasses all but one baseline models after the second epoch , and achieves state - of - the - art results only after 8 epochs .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O']",7,1.0,131,0.8618421052631579,16,0.5714285714285714,1,experiments,Results .,natural_language_inference27
2977,135,The results show that removing BERT can reduce the F 1 score on development set by 7.15 % .,Ablation Studies .,Ablation Studies .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-p', 'B-n', 'O', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",2,0.1818181818181818,134,0.881578947368421,19,0.6785714285714286,1,ablation-analysis,Ablation Studies .,natural_language_inference27
2978,136,"Our proposed weight sum of per-layer output from BERT is crucial , which can boost the performance by 1.75 % , compared with using only last layer 's output .",Ablation Studies .,Ablation Studies .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.2727272727272727,135,0.8881578947368421,20,0.7142857142857143,1,ablation-analysis,Ablation Studies .,natural_language_inference27
2979,139,"Using BERT - base instead of BERT - large pretrained model hurts the F 1 score by 2.61 % , which manifests the superiority of BERT - large model .",Ablation Studies .,Ablation Studies .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5454545454545454,138,0.9078947368421052,23,0.8214285714285714,1,ablation-analysis,Ablation Studies .,natural_language_inference27
2980,140,"Variational dropout and self attention can each improve the performance by 0.24 % and 0.75 % , respectively .",Ablation Studies .,Ablation Studies .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",7,0.6363636363636364,139,0.9144736842105264,24,0.8571428571428571,1,ablation-analysis,Ablation Studies .,natural_language_inference27
2981,2,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS,title,title,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0040983606557377,1,0.0,1,research-problem,title,natural_language_inference28
2982,27,"It may also learn basic rules of approximate ( logical ) inference , such as the fact that objects belonging to the same category tend to have similar properties ( light objects can be carried over from rooms to rooms for instance ) .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5172413793103449,26,0.1065573770491803,15,0.5172413793103449,1,model,INTRODUCTION,natural_language_inference28
2983,28,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",16,0.5517241379310345,27,0.110655737704918,16,0.5517241379310345,1,model,INTRODUCTION,natural_language_inference28
2984,127,SYNTHETIC WORLD MODEL TASK,EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",3,0.1071428571428571,126,0.5163934426229508,0,0.0,1,results,EXPERIMENTS,natural_language_inference28
2985,134,"For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",10,0.3571428571428571,133,0.5450819672131147,7,0.3684210526315789,1,hyperparameters,EXPERIMENTS,natural_language_inference28
2986,137,"All models were trained with ADAM with initial learning rates set by grid search over { 0.1 , 0.01 , 0.001 } and divided by 2 every 10,000 updates .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.4642857142857143,136,0.5573770491803278,10,0.5263157894736842,1,hyperparameters,EXPERIMENTS,natural_language_inference28
2987,139,"The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O']",15,0.5357142857142857,138,0.5655737704918032,12,0.631578947368421,1,results,EXPERIMENTS,natural_language_inference28
2988,140,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-n', 'O']",16,0.5714285714285714,139,0.569672131147541,13,0.6842105263157895,1,results,EXPERIMENTS,natural_language_inference28
2989,141,"In contrast , the EntNet is able to solve the task in all cases .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",17,0.6071428571428571,140,0.5737704918032787,14,0.7368421052631579,1,results,EXPERIMENTS,natural_language_inference28
2990,146,We see that the model is able to achieve good performance several times past its training horizon .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O']",22,0.7857142857142857,145,0.5942622950819673,19,1.0,1,results,EXPERIMENTS,natural_language_inference28
2991,147,BABI TASKS,EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O']","['B-n', 'I-n']",23,0.8214285714285714,146,0.5983606557377049,0,0.0,1,experiments,EXPERIMENTS,natural_language_inference28
2992,155,"All models were trained with ADAM using a learning rate of ? = 0.01 , which was divided by 2 every 25 epochs until 200 epochs were reached .",Training Details,Training Details,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'O']",2,0.0869565217391304,154,0.6311475409836066,2,0.0869565217391304,1,hyperparameters,Training Details,natural_language_inference28
2993,158,"In all experiments , our model had embedding dimension size d = 100 and 20 memory slots .",Training Details,Training Details,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.217391304347826,157,0.6434426229508197,5,0.217391304347826,1,hyperparameters,Training Details,natural_language_inference28
2994,160,"Our model is able to solve all the tasks , outperforming the other models in terms of both the number of solved tasks and the average error .",Training Details,Training Details,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",7,0.3043478260869565,159,0.6516393442622951,7,0.3043478260869565,1,experiments,Training Details,natural_language_inference28
2995,170,"Note that it does not store useful or correct information in the memory slots corresponding to locations , most likely because this task does not contain questions about locations ( such as "" who is in the kitchen ? "" ) .",Training Details,Training Details,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7391304347826086,169,0.6926229508196722,17,0.7391304347826086,1,experiments,Training Details,natural_language_inference28
2996,171,CHILDRE N'S BOOK TEST ( CBT ),Training Details,Training Details,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",18,0.782608695652174,170,0.6967213114754098,18,0.782608695652174,1,experiments,Training Details,natural_language_inference28
2997,181,All models were trained using standard stochastic gradient descent ( SGD ) with a fixed learning rate of 0.001 .,Training Details,C .,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",4,0.4,180,0.7377049180327869,4,0.4,1,hyperparameters,Training Details: C .,natural_language_inference28
2998,182,"We used separate input encodings for the update and gating functions , and applied a dropout rate of 0.5 to the word embedding dimensions .",Training Details,C .,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.5,181,0.7418032786885246,5,0.5,1,hyperparameters,Training Details: C .,natural_language_inference28
2999,195,"The general EntNet performs better than the LSTMs and n-gram model on the Named Entities Task , but lags behind on the Common Nouns task .",Results,Results,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.7,194,0.7950819672131147,7,0.7,1,results,Results,natural_language_inference28
3000,196,"The simplified EntNet outperforms all other single - pass models on both tasks , and also performs better than the Memory Network which does not use the self - supervision heuristic .",Results,Results,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.8,195,0.7991803278688525,8,0.8,1,results,Results,natural_language_inference28
3001,198,The fact that the simplified EntNet is able to obtain decent performance is encouraging since it indicates that the model is able to build an internal representation of the story which it can then use to answer a relatively diverse set of queries .,Results,Results,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1.0,197,0.8073770491803278,10,1.0,1,results,Results,natural_language_inference28
3002,19,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .",INTRODUCTION,INTRODUCTION,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",8,0.3809523809523809,18,0.1176470588235294,8,0.3809523809523809,1,model,INTRODUCTION,natural_language_inference29
3003,23,"This perspective leads to a different attention - based architecture containing two sequential phases , question - aware passage representation phase and evidence propagation phase. , RNET , MReader , and PhaseCond ( our proposed model ) .",INTRODUCTION,INTRODUCTION,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5714285714285714,22,0.1437908496732026,12,0.5714285714285714,1,model,INTRODUCTION,natural_language_inference29
3004,32,"Moreover , we observe several meaningful trends : a ) during the questionpassage attention phase , repeatedly attending the passage with the same question representation "" forces "" each passage word to become increasingly closer to the original question representation , and therefore increasing the number of layers has a risk of degrading the network performance , b ) during the self - attention phase , the self - attention 's alignment weights of the second layer become noticeably "" sharper "" than the first layer , suggesting the importance of fully propagating evidence through the passage itself .",INTRODUCTION,INTRODUCTION,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,1.0,31,0.2026143790849673,21,1.0,1,model,INTRODUCTION,natural_language_inference29
3005,104,"We use pre-trained GloVe 100 - dimensional word vectors , parts - of - speech tag features , named - entity tag feature , and binary features of exact matching which indicate if a passage word can be exactly matched to any question word and vice versa .",TRAINING DETAILS,TRAINING DETAILS,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0454545454545454,103,0.673202614379085,2,0.2857142857142857,1,hyperparameters,TRAINING DETAILS,natural_language_inference29
3006,105,"Following , we also use question type ( what , how , who , when , which , where , why , be , and other ) features where each type is represented by a trainable embedding .",TRAINING DETAILS,TRAINING DETAILS,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",3,0.0681818181818181,104,0.6797385620915033,3,0.4285714285714285,1,hyperparameters,TRAINING DETAILS,natural_language_inference29
3007,106,We use CNN with 100 one - dimensional filters with width 5 to encode character level embedding .,TRAINING DETAILS,TRAINING DETAILS,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",4,0.0909090909090909,105,0.6862745098039216,4,0.5714285714285714,1,hyperparameters,TRAINING DETAILS,natural_language_inference29
3008,107,The hidden size is set as 128 for all the LSTM layers .,TRAINING DETAILS,TRAINING DETAILS,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.1136363636363636,106,0.6928104575163399,5,0.7142857142857143,1,hyperparameters,TRAINING DETAILS,natural_language_inference29
3009,108,Dropout are used for all the learnable parameters with a ratio as 0.2 .,TRAINING DETAILS,TRAINING DETAILS,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'O']",6,0.1363636363636363,107,0.6993464052287581,6,0.8571428571428571,1,hyperparameters,TRAINING DETAILS,natural_language_inference29
3010,109,"We use the Adam optimizer ( Kingma & Ba , 2014 ) with an initial learning rate of 0.0006 , which is halved when a bad checkpoint is met .",TRAINING DETAILS,TRAINING DETAILS,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']",7,0.1590909090909091,108,0.7058823529411765,7,1.0,1,hyperparameters,TRAINING DETAILS,natural_language_inference29
3011,123,Single Model Ensemble Models Dev Set,TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",21,0.4772727272727273,122,0.7973856209150327,0,0.0,1,results,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
3012,126,"The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .",TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5454545454545454,125,0.8169934640522876,3,0.3,1,results,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
3013,130,"For the question - passage attention phase , using single layer does n't degrade the performance significantly from the default setting of two layers , resulting in a different conclusion from ; .",TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.6363636363636364,129,0.8431372549019608,7,0.7,1,results,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
3014,134,ANALYSIS ON ATTENTION LAYERS,TRAINING DETAILS,,natural_language_inference,29,"['O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n']",32,0.7272727272727273,133,0.869281045751634,0,0.0,1,experiments,TRAINING DETAILS,natural_language_inference29
3015,138,"First , the first layer of the question - passage attention phase can successfully align question keywords with the corresponding passage keywords , as shown in .",TRAINING DETAILS,ANALYSIS ON ATTENTION LAYERS,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",36,0.8181818181818182,137,0.8954248366013072,4,0.3333333333333333,1,experiments,TRAINING DETAILS: ANALYSIS ON ATTENTION LAYERS,natural_language_inference29
3016,2,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering,title,title,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'I-n', 'O', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0046296296296296,1,0.0,1,research-problem,title,natural_language_inference3
3017,4,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .,abstract,abstract,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.2,3,0.0138888888888888,1,0.2,1,research-problem,abstract,natural_language_inference3
3018,19,The question - answer pairs are annotated through crowdsourcing .,Introduction,Introduction,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",10,0.0662251655629139,18,0.0833333333333333,10,0.4,1,dataset,Introduction,natural_language_inference3
3019,28,The bi-directional attention flow ( BIDAF ) used the bi-directional attention to obtain a question - aware context representation .,Introduction,Introduction,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",19,0.1258278145695364,27,0.125,19,0.76,1,model,Introduction,natural_language_inference3
3020,29,"In this paper , we introduce syntactic information to encode questions with a specific form of recursive neural networks .",Introduction,Introduction,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",20,0.1324503311258278,28,0.1296296296296296,20,0.8,1,model,Introduction,natural_language_inference3
3021,30,"More specifically , we explore a tree - structured LSTM which extends the linear - chain long short - term memory ( LSTM ) ] to a recursive structure , which has the potential to capture long - distance interactions over the structures .",Introduction,Introduction,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",21,0.1390728476821192,29,0.1342592592592592,21,0.84,1,model,Introduction,natural_language_inference3
3022,35,Word embedding,Introduction,,natural_language_inference,3,"['O', 'O']","['B-n', 'I-n']",26,0.1721854304635761,34,0.1574074074074074,0,0.0,1,experiments,Introduction,natural_language_inference3
3023,168,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']",7,0.35,167,0.7731481481481481,7,0.35,1,results,Experiment Results: Set - Up,natural_language_inference3
3024,169,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",8,0.4,168,0.7777777777777778,8,0.4,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
3025,170,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .",Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",9,0.45,169,0.7824074074074074,9,0.45,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
3026,172,The cluster number K in discriminative block is 100 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",11,0.55,171,0.7916666666666666,11,0.55,1,results,Experiment Results: Set - Up,natural_language_inference3
3027,173,The Adam method is used for optimization .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",12,0.6,172,0.7962962962962963,12,0.6,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
3028,175,The initial learning rate is 0.0004 and the batch size is 32 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']",14,0.7,174,0.8055555555555556,14,0.7,1,results,Experiment Results: Set - Up,natural_language_inference3
3029,178,"All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .",Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",17,0.85,177,0.8194444444444444,17,0.85,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
3030,179,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .",Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",18,0.9,178,0.8240740740740741,18,0.9,1,results,Experiment Results: Set - Up,natural_language_inference3
3031,181,We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",20,1.0,180,0.8333333333333334,20,1.0,1,results,Experiment Results: Set - Up,natural_language_inference3
3032,192,"Our model achieves a 68.73 % EM score and 77.39 % F1 score , which is ranked among the state of the art single models ( without model ensembling shows the ablation performances of various Q- code on the development set .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0952380952380952,191,0.8842592592592593,2,0.0952380952380952,1,results,Results,natural_language_inference3
3033,194,"Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",4,0.1904761904761904,193,0.8935185185185185,4,0.1904761904761904,1,results,Results,natural_language_inference3
3034,195,"When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.238095238095238,194,0.8981481481481481,5,0.238095238095238,1,results,Results,natural_language_inference3
3035,196,"We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O']",6,0.2857142857142857,195,0.9027777777777778,6,0.2857142857142857,1,results,Results,natural_language_inference3
3036,198,"When letting the number of hidden question types ( K ) to be 20 , the performance improves to 68.73%/77.74 % on EM and F1 , respectively , which corresponds to the results of our model reported in .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'B-n', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3809523809523809,197,0.9120370370370372,8,0.3809523809523809,1,results,Results,natural_language_inference3
3037,2,Convolutional Neural Network Architectures for Matching Natural Language Sentences,title,,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0052083333333333,1,0.0,1,research-problem,title,natural_language_inference30
3038,4,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",abstract,abstract,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.015625,1,0.1666666666666666,1,research-problem,abstract,natural_language_inference30
3039,11,Matching two potentially heterogenous language objects is central to many natural language applications .,Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,10,0.0520833333333333,1,0.0625,1,research-problem,Introduction,natural_language_inference30
3040,12,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .",Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.125,11,0.0572916666666666,2,0.125,1,model,Introduction,natural_language_inference30
3041,16,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']",6,0.375,15,0.078125,6,0.375,1,model,Introduction,natural_language_inference30
3042,17,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.4375,16,0.0833333333333333,7,0.4375,1,model,Introduction,natural_language_inference30
3043,128,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,Training,Training,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",10,0.5555555555555556,127,0.6614583333333334,10,0.5555555555555556,1,experiments,Training,natural_language_inference30
3044,129,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .",Training,Training,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",11,0.6111111111111112,128,0.6666666666666666,11,0.6111111111111112,1,experiments,Training,natural_language_inference30
3045,131,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .",Training,Training,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.7222222222222222,130,0.6770833333333334,13,0.7222222222222222,1,hyperparameters,Training,natural_language_inference30
3046,136,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .",Training,Training,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-n', 'O']",18,1.0,135,0.703125,18,1.0,1,experiments,Training,natural_language_inference30
3047,142,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,Competitor Methods,Competitor Methods,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",1,0.0256410256410256,141,0.734375,1,0.0344827586206896,1,baselines,Competitor Methods,natural_language_inference30
3048,143,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :",Competitor Methods,Competitor Methods,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",2,0.0512820512820512,142,0.7395833333333334,2,0.0689655172413793,1,baselines,Competitor Methods,natural_language_inference30
3049,144,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",Competitor Methods,Competitor Methods,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.0769230769230769,143,0.7447916666666666,3,0.1034482758620689,1,baselines,Competitor Methods,natural_language_inference30
3050,145,We use the SENNA - type sentence model for sentence representation ;,Competitor Methods,Competitor Methods,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",4,0.1025641025641025,144,0.75,4,0.1379310344827586,1,baselines,Competitor Methods,natural_language_inference30
3051,146,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",Competitor Methods,Competitor Methods,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",5,0.1282051282051282,145,0.7552083333333334,5,0.1724137931034483,1,baselines,Competitor Methods,natural_language_inference30
3052,148,Experiment I : Sentence Completion,Competitor Methods,,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n']",7,0.1794871794871795,147,0.765625,7,0.2413793103448276,1,results,Competitor Methods,natural_language_inference30
3053,156,"The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .",Competitor Methods,Experiment I : Sentence Completion,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",15,0.3846153846153846,155,0.8072916666666666,15,0.5172413793103449,1,results,Competitor Methods: Experiment I : Sentence Completion,natural_language_inference30
3054,157,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .",Competitor Methods,Experiment I : Sentence Completion,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4102564102564102,156,0.8125,16,0.5517241379310345,1,results,Competitor Methods: Experiment I : Sentence Completion,natural_language_inference30
3055,158,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .",Competitor Methods,Experiment I : Sentence Completion,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.4358974358974359,157,0.8177083333333334,17,0.5862068965517241,1,results,Competitor Methods: Experiment I : Sentence Completion,natural_language_inference30
3056,162,"Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .",Competitor Methods,Experiment I : Sentence Completion,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'O', 'O', 'O']",21,0.5384615384615384,161,0.8385416666666666,21,0.7241379310344828,1,results,Competitor Methods: Experiment I : Sentence Completion,natural_language_inference30
3057,2,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes,title,title,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.002932551319648,1,0.0,1,research-problem,title,natural_language_inference31
3058,16,"External memory allows MANNs to learn algorithmic solutions to problems that have eluded the capabilities of traditional LSTMs , and to generalize to longer sequence lengths .",Introduction,Introduction,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'I-p', 'O', 'B-n', 'O', 'O', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",5,0.2631578947368421,15,0.0439882697947214,5,0.2631578947368421,1,approach,Introduction,natural_language_inference31
3059,23,"In this paper , we present a MANN named SAM ( sparse access memory ) .",Introduction,Introduction,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",12,0.631578947368421,22,0.064516129032258,12,0.631578947368421,1,approach,Introduction,natural_language_inference31
3060,24,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .",Introduction,Introduction,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.6842105263157895,23,0.0674486803519061,13,0.6842105263157895,1,approach,Introduction,natural_language_inference31
3061,30,"This Sparse Differentiable Neural Computer ( SDNC ) is over 400 faster than the canonical dense variant fora memory size of 2,000 slots , and achieves the best reported result in the Babi tasks without supervising the memory access .",Introduction,"Further , in Supplementary",natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",19,1.0,29,0.0850439882697947,19,1.0,1,approach,"Introduction: Further , in Supplementary",natural_language_inference31
3062,136,Speed and memory benchmarks,Results,Results,natural_language_inference,31,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",1,0.0076923076923076,135,0.3958944281524926,0,0.0,1,results,Results,natural_language_inference31
3063,164,Question answering on the Babi tasks,Results,,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",29,0.2230769230769231,163,0.4780058651026393,0,0.0,1,results,Results,natural_language_inference31
3064,169,"The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",34,0.2615384615384615,168,0.4926686217008797,5,0.5,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
3065,170,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.2692307692307692,169,0.4956011730205278,6,0.6,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
3066,172,"More directly comparable previous work with end - to - end memory networks , which did not use supervision , fails at 6 of the tasks .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",37,0.2846153846153846,171,0.501466275659824,8,0.8,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
3067,173,"Both the sparse and dense perform comparably at this task , again indicating the sparse approximations do not impair learning .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.2923076923076923,172,0.5043988269794721,9,0.9,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
3068,187,"SAM outperformed other models , presumably due to its much larger memory capacity .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.4,186,0.5454545454545454,12,0.6,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
3069,195,"SAM is able to outperform other approaches , presumably because it can utilize a much larger memory .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.4615384615384615,194,0.5689149560117303,20,1.0,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
3070,329,G Babi results,,,natural_language_inference,31,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,328,0.9618768328445748,0,0.0,1,results,,natural_language_inference31
3071,339,"SDNC achieves the best reported result on this task with unsupervised memory access , solving all but 1 task .",G Babi results,G Babi results,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.8333333333333334,338,0.9912023460410556,10,0.8333333333333334,1,results,G Babi results,natural_language_inference31
3072,2,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller,title,title,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",1,0.0,1,0.0051282051282051,1,0.0,1,research-problem,title,natural_language_inference4
3073,6,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .",abstract,abstract,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",3,0.4285714285714285,5,0.0256410256410256,3,0.4285714285714285,1,research-problem,abstract,natural_language_inference4
3074,13,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0909090909090909,12,0.0615384615384615,2,0.0909090909090909,1,research-problem,Introduction,natural_language_inference4
3075,21,"Inspired by these approaches , we develop a customized memory controller along with an external memory augmentation for complicated RC tasks .",Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",10,0.4545454545454545,20,0.1025641025641025,10,0.4545454545454545,1,model,Introduction,natural_language_inference4
3076,24,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']",13,0.5909090909090909,23,0.1179487179487179,13,0.5909090909090909,1,model,Introduction,natural_language_inference4
3077,25,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",14,0.6363636363636364,24,0.123076923076923,14,0.6363636363636364,1,model,Introduction,natural_language_inference4
3078,31,( 1 ) We propose an extended memory controller module for RC tasks .,Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",20,0.9090909090909092,30,0.1538461538461538,20,0.9090909090909092,1,model,Introduction,natural_language_inference4
3079,121,NLTK is used for tokenizing words .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",18,0.75,120,0.6153846153846154,18,0.75,1,experiments,Experimental Setup: In Trivia,natural_language_inference4
3080,122,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .",Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",19,0.7916666666666666,121,0.6205128205128205,19,0.7916666666666666,1,experiments,Experimental Setup: In Trivia,natural_language_inference4
3081,123,The hidden vector dimension l is set to 200 .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O']",20,0.8333333333333334,122,0.6256410256410256,20,0.8333333333333334,1,experiments,Experimental Setup: In Trivia,natural_language_inference4
3082,124,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .",Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",21,0.875,123,0.6307692307692307,21,0.875,1,experiments,Experimental Setup: In Trivia,natural_language_inference4
3083,125,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'B-n', 'O', 'I-n', 'O']",22,0.9166666666666666,124,0.6358974358974359,22,0.9166666666666666,1,experiments,Experimental Setup: In Trivia,natural_language_inference4
3084,126,We use an exponential moving average of weights with a decaying factor of 0.001 .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",23,0.9583333333333334,125,0.6410256410256411,23,0.9583333333333334,1,experiments,Experimental Setup: In Trivia,natural_language_inference4
3085,129,"For our quantitative comparisons , we use BiDAF with self attention ) as a baseline , which maintains the best results published on both TriviaQA and SQuAD datasets .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.0185185185185185,128,0.6564102564102564,1,0.0227272727272727,1,results,Quantitative Results,natural_language_inference4
3086,130,"In TriviaQA and QUASAR - T dataset , we compare our model with BiDAF as well as its variant called ' BiDAF + DNC , ' which is augmented with an existing external memory architecture just before the answer prediction layer in the BiDAF .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']",2,0.037037037037037,129,0.6615384615384615,2,0.0454545454545454,1,results,Quantitative Results,natural_language_inference4
3087,131,"Overall , in lengthy - document cases such as Trivi aQA and QUASAR - T , our model outperforms all the published results , as seen in Tables 2 and 3 , while in the short - document case such as SQuAD , we mostly achieve the best results , as seen in .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",3,0.0555555555555555,130,0.6666666666666666,3,0.0681818181818181,1,results,Quantitative Results,natural_language_inference4
3088,133,TriviaQA .,Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O']","['B-n', 'O']",5,0.0925925925925925,132,0.676923076923077,5,0.1136363636363636,1,results,Quantitative Results,natural_language_inference4
3089,134,"As shown in , our model , even without DEBS , outperforms the existing state - of - the - art method such as ' BiDAF + SA + SN ' by a large margin in all the cases .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",6,0.1111111111111111,133,0.6820512820512821,6,0.1363636363636363,1,results,Quantitative Results,natural_language_inference4
3090,135,"Our model with DEBS , which replaces BiGRU encoder blocks , performs even better than that without it in all the cases except for the combination of the ' full ' and ' Wikipedia ' case , which involves documents containing no relevant information for the answer .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1296296296296296,134,0.6871794871794872,7,0.1590909090909091,1,results,Quantitative Results,natural_language_inference4
3091,137,"We note that our method achieves these outstanding results without any additional features. , our simple baseline ' BiDAF + DNC , ' which involves an existing memory architecture , improves performance over BiDAF , indicating the efficacy of incorporating an external memory .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1666666666666666,136,0.6974358974358974,9,0.2045454545454545,1,results,Quantitative Results,natural_language_inference4
3092,138,"Moreover , our model with the proposed memory controller achieves significantly better results compared to other models .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",10,0.1851851851851851,137,0.7025641025641025,10,0.2272727272727272,1,results,Quantitative Results,natural_language_inference4
3093,141,"First , we adopt ELMo to our model ( without DEBS ) , which uses word embedding as the weighted sum of the hidden layers of a language model with regularization as an additional feature to our word embeddings .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",13,0.2407407407407407,140,0.717948717948718,13,0.2954545454545454,1,ablation-analysis,Quantitative Results,natural_language_inference4
3094,142,"This improves the F 1 score of our model up to 85.13 and EM to 77. 44 , showing the highest performances among all the methods without using self attention .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2592592592592592,141,0.7230769230769231,14,0.3181818181818182,1,results,Quantitative Results,natural_language_inference4
3095,143,"Due to the relatively short document length in SQuAD compared to TriviaQA and QUASAR - T , our model without DEBS performs worse than the baseline ' BiDAF + Self Attention + ELMo. '",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.2777777777777778,142,0.7282051282051282,15,0.3409090909090909,1,results,Quantitative Results,natural_language_inference4
3096,146,QUASAR - T .,Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",18,0.3333333333333333,145,0.7435897435897436,18,0.4090909090909091,1,results,Quantitative Results,natural_language_inference4
3097,166,"As can be seen in , using DEBS in all the places improves the performance most , and furthermore , the memory controller with DEBS gives the largest performance margin .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",38,0.7037037037037037,165,0.8461538461538461,38,0.8636363636363636,1,results,Quantitative Results,natural_language_inference4
3098,2,Sentence Similarity Learning by Lexical Decomposition and Composition,title,,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0039370078740157,1,0.0,1,research-problem,title,natural_language_inference5
3099,13,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,abstract,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2,12,0.0472440944881889,10,0.2,1,research-problem,abstract,natural_language_inference5
3100,166,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,Experimental Setting,Experimental Setting,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5555555555555556,165,0.6496062992125984,5,0.5555555555555556,1,research-problem,Experimental Setting,natural_language_inference5
3101,170,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .",Experimental Setting,Experimental Setting,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",9,1.0,169,0.6653543307086615,9,1.0,1,hyperparameters,Experimental Setting,natural_language_inference5
3102,2,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding,title,title,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']",1,0.0,1,0.0072463768115942,1,0.0,1,research-problem,title,natural_language_inference6
3103,10,"The vector is then used for various downstream tasks , e.g. , sentiment analysis , natural language inference , etc .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']",2,0.064516129032258,9,0.0652173913043478,2,0.0909090909090909,1,model,Introduction,natural_language_inference6
3104,15,Self - attention computes attention weights by the inner product between words and the learnable weight vector .,Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",7,0.2258064516129032,14,0.1014492753623188,7,0.3181818181818182,1,model,Introduction,natural_language_inference6
3105,16,"The weight vector is important in that it detects informative words , yet it is static during inference .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'O']",8,0.2580645161290322,15,0.108695652173913,8,0.3636363636363636,1,model,Introduction,natural_language_inference6
3106,23,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",15,0.4838709677419355,22,0.1594202898550724,15,0.6818181818181818,1,model,Introduction,natural_language_inference6
3107,24,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']",16,0.5161290322580645,23,0.1666666666666666,16,0.7272727272727273,1,model,Introduction,natural_language_inference6
3108,28,"We design and implement Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",20,0.6451612903225806,27,0.1956521739130435,20,0.9090909090909092,1,model,Introduction,natural_language_inference6
3109,29,We devise the dynamic weight vector with which DSA computes attention weights .,Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']",21,0.6774193548387096,28,0.2028985507246377,21,0.9545454545454546,1,model,Introduction,natural_language_inference6
3110,96,"We implement single DSA , multiple DSA and self - attention in Eq. 1 as a baseline .",Experiments,Experiments,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,95,0.6884057971014492,2,0.2857142857142857,1,baselines,Experiments,natural_language_inference6
3111,97,Both DSA and self - attention are stacked on CNN with Dense Connection for fair comparison .,Experiments,Experiments,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O']",3,0.4285714285714285,96,0.6956521739130435,3,0.4285714285714285,1,baselines,Experiments,natural_language_inference6
3112,98,"For our implementations , we initialize word embeddings by 300D Glo Ve 840B pretrained vectors , and fix them during training .",Experiments,Experiments,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']",4,0.5714285714285714,97,0.7028985507246377,4,0.5714285714285714,1,hyperparameters,Experiments,natural_language_inference6
3113,99,We use cross-entropy loss as an objective function for both tasks .,Experiments,Experiments,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']",5,0.7142857142857143,98,0.7101449275362319,5,0.7142857142857143,1,hyperparameters,Experiments,natural_language_inference6
3114,100,"We set do = 600 , m = 1 for single DSA and do = 300 , m = 8 for multiple DSA .",Experiments,Experiments,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.8571428571428571,99,0.717391304347826,6,0.8571428571428571,1,experiments,Experiments,natural_language_inference6
3115,102,Natural Language Inference Results,,,natural_language_inference,6,"['O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n']",0,0.0,101,0.7318840579710145,0,0.0,1,experiments,,natural_language_inference6
3116,115,"With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .",Natural Language Inference Results,"Entailment , Contradiction and Neutral .",natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.8125,114,0.8260869565217391,13,0.8125,1,results,"Natural Language Inference Results: Entailment , Contradiction and Neutral .",natural_language_inference6
3117,116,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .",Natural Language Inference Results,"Entailment , Contradiction and Neutral .",natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.875,115,0.8333333333333334,14,0.875,1,results,"Natural Language Inference Results: Entailment , Contradiction and Neutral .",natural_language_inference6
3118,118,"Note that our implementation of the baseline , selfattention stacked on CNN with Dense Connection , shows better performance ( + 0.4 % ) than the one stacked on BiLSTM .",Natural Language Inference Results,"Entailment , Contradiction and Neutral .",natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']",16,1.0,117,0.8478260869565217,16,1.0,1,results,"Natural Language Inference Results: Entailment , Contradiction and Neutral .",natural_language_inference6
3119,119,Sentiment Analysis Results,,,natural_language_inference,6,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",0,0.0,118,0.855072463768116,0,0.0,1,results,,natural_language_inference6
3120,127,"Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .",Sentiment Analysis Results,Sentiment Analysis Results,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,126,0.9130434782608696,8,0.8,1,results,Sentiment Analysis Results,natural_language_inference6
3121,128,"In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .",Sentiment Analysis Results,Sentiment Analysis Results,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'I-n', 'I-n', 'O', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']",9,0.9,127,0.9202898550724636,9,0.9,1,results,Sentiment Analysis Results,natural_language_inference6
3122,2,Teaching Machines to Read and Comprehend,title,,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0043478260869565,1,0.0,1,research-problem,title,natural_language_inference7
3123,4,Teaching machines to read natural language documents remains an elusive challenge .,abstract,abstract,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.0130434782608695,1,0.25,1,research-problem,abstract,natural_language_inference7
3124,5,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",abstract,abstract,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,4,0.017391304347826,2,0.5,1,research-problem,abstract,natural_language_inference7
3125,16,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",8,0.1333333333333333,15,0.0652173913043478,8,0.4705882352941176,1,model,Introduction,natural_language_inference7
3126,18,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",10,0.1666666666666666,17,0.0739130434782608,10,0.5882352941176471,1,dataset,Introduction,natural_language_inference7
3127,32,"Here we propose a methodology for creating real - world , large scale supervised training data for learning reading comprehension models .",Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'O', 'I-n', 'I-n', 'O', 'I-n', 'I-n', 'I-n', 'O', 'I-n', 'O', 'B-n', 'B-n', 'I-n', 'I-n', 'O']",24,0.4,31,0.1347826086956521,6,0.4615384615384615,1,model,Introduction,natural_language_inference7
3128,33,"Inspired by work in summarisation , we create two machine reading corpora by exploiting online newspaper articles and their matching summaries .",Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']",25,0.4166666666666667,32,0.1391304347826087,7,0.5384615384615384,1,model,Introduction,natural_language_inference7
3129,95,We tune the maximum penalty per word ( m = 8 ) on the validation data .,Symbolic Matching Models,Word Distance Benchmark,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",21,1.0,94,0.408695652173913,4,1.0,1,experiments,Symbolic Matching Models: Word Distance Benchmark,natural_language_inference7
3130,156,Word distance benchmark,Empirical Evaluation,Empirical Evaluation,natural_language_inference,7,"['O', 'O', 'O']","['B-n', 'I-n', 'I-n']",20,0.7407407407407407,155,0.6739130434782609,20,0.4651162790697674,1,results,Empirical Evaluation,natural_language_inference7
3131,2,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention,title,title,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0083333333333333,1,0.0,1,research-problem,title,natural_language_inference8
3132,4,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .",abstract,abstract,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",1,0.125,3,0.025,1,0.125,1,research-problem,abstract,natural_language_inference8
3133,11,"With less number of parameters , our model outperformed the existing best sentence encoding - based approach by a large margin .",abstract,abstract,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",8,1.0,10,0.0833333333333333,8,1.0,1,research-problem,abstract,natural_language_inference8
3134,13,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0416666666666666,12,0.1,1,0.0416666666666666,1,research-problem,Introduction,natural_language_inference8
3135,30,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .",Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.75,29,0.2416666666666666,18,0.75,1,approach,Introduction,natural_language_inference8
3136,31,The basic model is based on building biL - STM models on both premises and hypothesis .,Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.7916666666666666,30,0.25,19,0.7916666666666666,1,approach,Introduction,natural_language_inference8
3137,77,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .",Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O']",9,0.3214285714285714,76,0.6333333333333333,1,0.0909090909090909,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
3138,78,The batch size is 128 .,Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",10,0.3571428571428571,77,0.6416666666666667,2,0.1818181818181818,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
3139,79,A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']",11,0.3928571428571428,78,0.65,3,0.2727272727272727,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
3140,80,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .",Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",12,0.4285714285714285,79,0.6583333333333333,4,0.3636363636363636,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
3141,81,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .",Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",13,0.4642857142857143,80,0.6666666666666666,5,0.4545454545454545,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
3142,85,"2 . Keep their representation stays close to unseen similar words in inference time , which improved the model 's generation ability .",Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O']",17,0.6071428571428571,84,0.7,9,0.8181818181818182,1,ablation-analysis,Experiments: Parameter Setting,natural_language_inference8
3143,6,"This baseline has been submitted to the official NQ leaderboard . Code , preprocessed data and pretrained model are available . https://ai.google.com/research/NaturalQuestions https://github.com/google-research/language/tree/",abstract,abstract,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,5,0.0769230769230769,3,0.75,1,code,abstract,natural_language_inference9
3144,17,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .",Introduction,Introduction,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'I-p', 'O', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n']",9,0.25,16,0.2461538461538461,9,0.8181818181818182,1,approach,Introduction,natural_language_inference9
3145,18,We refer to our model as BERT joint to emphasize the fact that we are modeling short and long answers in a single model rather than in a pipeline of two models .,Introduction,Introduction,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'I-n', 'O']",10,0.2777777777777778,17,0.2615384615384615,10,0.9090909090909092,1,approach,Introduction,natural_language_inference9
3146,54,We initialized our model from a BERT model already finetuned on SQ u AD 1.1 .,Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",1,0.1111111111111111,53,0.8153846153846154,1,0.1111111111111111,1,hyperparameters,Experiments,natural_language_inference9
3147,56,"We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .",Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']",3,0.3333333333333333,55,0.8461538461538461,3,0.3333333333333333,1,hyperparameters,Experiments,natural_language_inference9
3148,60,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']",7,0.7777777777777778,59,0.9076923076923076,7,0.7777777777777778,1,results,Experiments,natural_language_inference9
