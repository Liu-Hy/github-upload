{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brilliant-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in the hyperparameter space with W&B sweep\n",
    "import logging\n",
    "from ast import literal_eval as load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import sklearn\n",
    "\n",
    "from simpletransformers.classification import (\n",
    "    ClassificationArgs,\n",
    "    ClassificationModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interracial-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "df = pd.read_csv('triples.csv')\n",
    "df.insert(loc=0, column='idx', value=np.arange(len(df)))\n",
    "sent_num=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eleven-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(arr):\n",
    "    ls=[]\n",
    "    trip_list=[]\n",
    "    for i in range(len(arr)):\n",
    "        lth = len(load(arr[i,4]))\n",
    "        for p1 in range(lth-1):\n",
    "            for p2 in range(p1+1, p1+2): #p1+2 or lth\n",
    "                phrase1=load(arr[i,4])[p1]\n",
    "                phrase2=load(arr[i,4])[p2]\n",
    "                a_ls= load(arr[i,5])\n",
    "                possible=1\n",
    "                for a in a_ls:\n",
    "                    if (a[0]==phrase1[0] and a[2]==phrase2[0]) or (a[0]==phrase2[0] and a[2]==phrase1[0]):\n",
    "                        possible=0\n",
    "                        break\n",
    "                if possible==1:\n",
    "                    word_ls=arr[i,2].split(' ')\n",
    "                    triple=[phrase1[0],'has',phrase2[0]]\n",
    "                    trip_list.append(triple)\n",
    "                    word_ls.insert(phrase1[1][0], '<<')\n",
    "                    word_ls.insert(phrase1[1][1]+1, '>>')\n",
    "                    word_ls.insert(phrase2[1][0]+2, '[[')\n",
    "                    word_ls.insert(phrase2[1][1]+3, ']]')\n",
    "                    flg=0\n",
    "                    trip_ls=load(arr[i,6])\n",
    "                    for trip in trip_ls:\n",
    "                        if trip[0]==phrase1[0] and trip[2]==phrase2[0]:\n",
    "                            if trip[1]=='has':\n",
    "                                flg=1\n",
    "                                break\n",
    "                            elif trip[1]=='name':\n",
    "                                flg=2\n",
    "                                break\n",
    "                    ls.append([int(arr[i, 0]),phrase1[0],phrase2[0],trip_ls,' '.join(word_ls),flg])\n",
    "    dataframe = pd.DataFrame(ls)\n",
    "    dataframe.columns = ['idx','phrase1', 'phrase2', 'triples', 'text', 'labels']\n",
    "    return dataframe,trip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "south-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.values\n",
    "df,trip_list=convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "corporate-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.manual_seed = 1\n",
    "model_args.fp16 = False\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.do_lower_case = True  # when using uncased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attended-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_F1(ref, pred):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "going-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c3b518d895498c9c56e4592f0a79cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbec216e5d0144a9962f301826220e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hl/lib/python3.9/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'F1_score': 0, 'eval_loss': 0.6911000259916659}\n"
     ]
    }
   ],
   "source": [
    "# Create a TransformerModel\n",
    "model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"../rel/outputsB/best_model\",\n",
    "    args=model_args,\n",
    ")\n",
    "result, model_outputs, wrong_predictions = model.eval_model(df, F1_score=triple_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competent-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(model_outputs.argmax(axis=1))\n",
    "df['preds']=preds\n",
    "df['cand']=trip_list\n",
    "df.loc[df['preds']==0,'cand']=None\n",
    "#df.loc[df['preds']==2,'cand'][1]='name'\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i,'preds']==2:\n",
    "        df.loc[i,'cand'][1]='name'\n",
    "data=[]\n",
    "for i in range(sent_num):\n",
    "    temp = list(df[df['idx']==i]['cand'])\n",
    "    temp = [t for t in temp if t]\n",
    "    data.append(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floral-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['two variants', 'has', 'algorithm']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['pretraining', 'has', 'bi-directional transformer model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['bi-directional transformer architecture', 'has', 'predicts'], ['predicts', 'has', 'every token']]\",\n",
       " \"[['cloze - style training objective', 'has', 'model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['dimensionality', 'has', '1024'], ['dimensionality', 'has', '256'], ['momentum', 'has', '0.99']]\",\n",
       " \"[['learning rate', 'has', 'linearly warmed up']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['much better', 'name', 'MRPC']]\",\n",
       " '[]',\n",
       " \"[['fine tuning', 'has', 'biggest gain']]\",\n",
       " '[]',\n",
       " \"[['scaling', 'has', 'bilm term']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Seq2seq approach', 'has', 'stronger baseline']]\",\n",
       " '[]',\n",
       " \"[['L = 2 , H = 200 , and B = 5', 'has', 'adequate']]\",\n",
       " \"[['subword split', 'has', 'input token unit']]\",\n",
       " \"[['subword information', 'has', 'features']]\",\n",
       " \"[['current top - notch methods', 'name', 'RNNG']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['sequence - to - sequence model', 'has', 'attention']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['embedding layer', 'has', '90K vocabulary']]\",\n",
       " '[]',\n",
       " \"[['single attention model', 'has', '88.3']]\",\n",
       " \"[['large high - confidence corpus', 'has', 'single LSTM + A model'], ['outperforms', 'has', 'best single model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['two state - of - the - art generative neural parsing models', 'name', 'Recurrent Neural Network Grammar generative parser ( RG )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['RG', 'has', 'decreases'], ['decreases', 'has', 'performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['93.94 F1 .', 'has', '93.94 F1']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['PTB setting', 'has', 'ensembling']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['bottom - up parser and the top - down parser', 'has', 'similar results'], ['in - order parser', 'has', 'outperforms']]\",\n",
       " '[]',\n",
       " \"[['fully - supervise setting', 'has', 'inorder parser']]\",\n",
       " '[]',\n",
       " \"[['our final model', 'has', 'best results']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['very good results', 'has', '93.8 F 1']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['forget gate bias', 'has', 'one']]\",\n",
       " '[]',\n",
       " \"[['learning rate', 'has', '0.25 0.85 max']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['new state of the art', 'has', '93.8 F 1']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['RNNG composition function', 'name', 'novel gated attention mechanism'], ['novel gated attention mechanism', 'name', 'GA - RNNG']]\",\n",
       " \"[['role', 'has', 'individual heads']]\",\n",
       " '[]',\n",
       " \"[['stack', 'has', 'worst'], ['worst', 'has', 'new results']]\",\n",
       " \"[['language modeling', 'has', 'stack - only RNNG']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['attention - based tree output', 'has', 'high error rate ( ? 90 % )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['test data', 'has', 'usual split'], ['usual split', 'has', 'GA - RNNG']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['mixed effect', 'has', 'improves'], ['improves', 'has', 'performance']]\",\n",
       " \"[['8 of the 9 languages', 'has', 'test set result']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['cluster - pair representations', 'has', 'our network']]\",\n",
       " \"[['coreference clusters', 'has', 'incrementally']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['current state', 'name', 'partially completed coreference clustering']]\",\n",
       " '[]',\n",
       " \"[['cluster - ranking model', 'has', 'results'], ['results', 'has', 'further']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['neural architecture', 'name', 'policy network'], ['learning', 'has', 'span representation']]\",\n",
       " \"[['sequence of linking actions', 'has', 'linking actions'], ['how good', 'has', 'generated coreference clusters']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['number of sampled trajectories N s', 'has', '100'], ['regularization parameter', 'has', '{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }']]\",\n",
       " '[]',\n",
       " \"[['model', 'has', 'ELMo']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Coreference resolution systems', 'has', 'Coreference resolution']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['REINFORCE', 'has', 'slightly better']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['each iteration', 'has', 'antecedent distribution']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['first feed - forward layer size', 'has', 'value']]\",\n",
       " \"[['learning rate', 'has', '10 ? 4'], ['maximal batch size', 'has', '64']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[[\\'s@1 score\\', \\'has\\', \\'MR - LSTM\\'], [\\'MR - LSTM\\', \\'has\\', \\'outperforms\\'], [\\'outperforms\\', \\'has\\', \"KZH13 \\'s results\"]]',\n",
       " '[]',\n",
       " \"[['Performance', 'has', '68.10 s@1 score']]\",\n",
       " '[]',\n",
       " \"[['MR - LSTM', 'has', 'more successful']]\",\n",
       " '[[\\'shell noun resolution\\', \\'has\\', \"KZH13 \\'s dataset\"], [\\'range\\', \\'has\\', \\'76.09-93.14\\']]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['modeling', 'has', 'global information']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['model', 'has', 'local classifier']]\",\n",
       " '[]',\n",
       " \"[['initial learning rate', 'has', 'AdaGrad']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['rate', 'has', '0.3']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['RNN', 'has', 'improves'], ['improves', 'has', 'performance']]\",\n",
       " \"[['RNN performance', 'has', 'significantly better']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['context memory block', 'has', 'proposed model'], ['target sentences and context sentences', 'has', 'into consideration']]\",\n",
       " \"[['LSTM modules', 'has', '200 output units']]\",\n",
       " '[]',\n",
       " \"[['initial learning rate', 'has', '0.001']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['baseline model', 'has', '67.2 % F1 score']]\",\n",
       " \"[['models', 'has', 'cross - sentence dependency']]\",\n",
       " \"[['ASL model', 'has', 'better performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['first end - to - end coreference resolution model', 'has', 'significantly outperforms']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['word embeddings', 'has', 'fixed concatenation']]\",\n",
       " '[]',\n",
       " \"[['character CNN', 'has', 'characters']]\",\n",
       " \"[['convolutions', 'has', 'window sizes']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['our 5 - model ensemble', 'has', 'improves']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['mention candidates', 'has', 'rule - based system']]\",\n",
       " \"[['oracle mentions', 'has', 'improvement']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['dropout', 'has', '0.3']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['BERT - base', 'has', 'improvement']]\",\n",
       " '[]',\n",
       " \"[['overlap variant', 'has', 'no improvement']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Wiseman', 'has', 'standard encoder - decoder system']]\",\n",
       " \"[['Li', 'has', 'standard encoder - decoder'], ['standard encoder - decoder', 'has', 'delayed copy mechanism'], ['delayed copy mechanism', 'has', 'text']]\",\n",
       " \"[['Puduppully - plan', 'has', 'two steps'], ['two steps', 'has', 'first standard encoder - decoder']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['each decoding step', 'has', 'gated recurrent network'], ['gated recurrent network', 'has', 'computes'], ['computes', 'has', 'records']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['initial learning rate', 'has', '0.001']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['lower results', 'has', 'Flat scenario']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['significant higher BLEU score (', 'has', '16.7 vs. 14.5 )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['decoder', 'has', 'all models'], ['all models', 'has', '4 - layer RNN decoder']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['official E2E test set', 'has', 'our ensemble model'], ['baseline model', 'name', 'TGen']]\",\n",
       " '[]',\n",
       " \"[['outperforms', 'has', 'Laptop dataset']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['model', 'has', 'GCN encoder']]\",\n",
       " \"[['GCN model', 'has', 'more stable']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['neural models', 'has', 'upper bound results']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['generated strings', 'has', 'selected'], ['behav', 'has', '5 out of 5 customer rating']]\",\n",
       " \"[['canonical presentation', 'name', 'RSA framework'], ['reference resolution', 'has', 'referents']]\",\n",
       " '[]',\n",
       " \"[['pragmatic methods', 'has', 'improvements']]\",\n",
       " \"[['SD 1', 'has', 'strong'], ['Coverage ratios', 'has', 'attribute type']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['initial learning rate', 'has', '0.15'], ['batch size', 'has', '5']]\",\n",
       " \"[['truncation size', 'has', '100']]\",\n",
       " \"[['beam size', 'has', '5']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['best reported system', 'has', 'absolute improvement'], ['content selection precision', 'has', 'improves'], ['content ordering', 'has', 'increases']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['generating', 'has', 'fluent language']]\",\n",
       " \"[['explicit , symbolic , text planning stage', 'has', 'output']]\",\n",
       " '[]',\n",
       " \"[['plan', 'has', 'neural generation system']]\",\n",
       " '[]',\n",
       " \"[['WebNLG challenge', 'has', 'Melbourne'], ['Melbourne', 'has', 'end - to - end system'], ['scored', 'has', 'best'], ['UPF - FORGe', 'has', 'classic grammar - based NLG system']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['two important features', 'has', 'state - of - art architecture']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['E2E +', 'has', 'TGen']]\",\n",
       " \"[['E2E', 'has', 'outperforms'], ['outperforms', 'has', 'EDA_CS']]\",\n",
       " \"[['EDA_CS TL', 'has', 'bleu increment']]\",\n",
       " \"[['baseline model', 'name', 'EDA']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['jointly learning', 'has', 'POS tagging and dependency paring']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['character and POS tag embeddings', 'has', 'randomly initialized']]\",\n",
       " \"[['dropout', 'has', '67 % keep probability']]\",\n",
       " '[]',\n",
       " \"[['initial learning rate', 'has', '0.001']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['two parsing architectures', 'name', 'transition - based']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['our parsers', 'has', 'very competitive']]\",\n",
       " \"[['external embeddings', 'has', 'first - order graph - based parser'], ['2 features', 'has', 'outperforms'], ['outperforms', 'has', 'all other systems']]\",\n",
       " \"[['greedy transition based parser', 'has', '4 features']]\",\n",
       " '[]',\n",
       " \"[['external word embeddings', 'has', 'accuracy']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['German', 'has', 'best published UAS scores']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['three BiLSTM - CRF - based models', 'name', 'Stanford - NNdep']]\",\n",
       " \"[['traditional feature - based models', 'name', 'MarMoT']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['number of BiLSTM layers', 'has', '2']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['MarMoT', 'has', 'accuracy results']]\",\n",
       " \"[['BiLSTM - CRF', 'has', 'accuracies']]\",\n",
       " \"[['PTB', 'has', 'CNN - based character - level word embeddings']]\",\n",
       " \"[['GENIA and CRAFT', 'has', 'BiLSTM - CRF']]\",\n",
       " '[]',\n",
       " \"[['GENIA', 'has', 'pre-trained models'], ['pre-trained models', 'has', 'BLLIP']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['STACKPTR', 'has', 'transition - based architecture'], ['transition - based architecture', 'has', 'asymptotic efficiency']]\",\n",
       " \"[['STACKPTR parser', 'has', 'pointer network'], ['pointer network', 'has', 'backbone']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['UAS and LAS', 'has', 'Full variation of STACKPTR']]\",\n",
       " \"[['Full model', 'has', 'best accuracy']]\",\n",
       " \"[['LCM and UCM', 'has', 'STACKPTR']]\",\n",
       " \"[['results', 'has', 'our parser']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['German', 'has', 'performance'], ['performance', 'has', 'BIAF']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['transition - based parsing', 'has', 'sentences']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['our neural network parser', 'has', 'significantly more']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['our model', 'has', 'competitive']]\",\n",
       " '[]',\n",
       " \"[['network', 'has', 'larger']]\",\n",
       " \"[['optimize', 'has', 'Adam']]\",\n",
       " \"[['Our model', 'has', 'nearly the same UAS performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['parser states', 'has', 'drawn']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Chinese score', 'has', 'state - of - the - art']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['label bias problem', 'has', 'locally normalized models']]\",\n",
       " '[]',\n",
       " \"[['gradients', 'has', 'approximate global normalization']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['beam search', 'has', 'locally normalized model']]\",\n",
       " \"[['character ngrams feature', 'has', 'very important']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['achieving', 'has', 'performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['accuracy', 'has', 'slightly'], ['trigrams', 'has', 'performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Tagspace', 'has', 'tag prediction model']]\",\n",
       " '[]',\n",
       " \"[['Both models', 'has', 'similar performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Fine-tune ( Ft )', 'has', 'Fine - tuning'], ['Fine - tuning', 'has', 'pre-trained model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Self - training', 'has', 'UDA model ( UDA + Self )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['UDA algorithm and MLM pre-training', 'has', 'significant improvements']]\",\n",
       " \"[['sentiment classification task', 'has', 'unlabeled data size'], ['unlabeled data size', 'has', 'larger']]\",\n",
       " \"[['MLM method', 'has', 'relatively more resource intensive']]\",\n",
       " \"[['MLdoc dataset', 'has', 'size'], ['UDA method', 'has', 'more helpful']]\",\n",
       " \"[['sentiment classification task', 'has', 'self - training technique'], ['self - training technique', 'has', 'consistently improves']]\",\n",
       " '[]',\n",
       " \"[['MLdoc dataset', 'has', 'self - training']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Neural Attentive Bagof - Entities ( NABoE ) model', 'has', 'neural network model']]\",\n",
       " \"[['each entity name', 'has', 'document']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['mini-batch size', 'has', '32']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['NTEE', 'has', 'state - of - the - art model']]\",\n",
       " '[]',\n",
       " \"[['baselines', 'has', 'our models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[[\"words \\' contextual information and task information\", \\'has\\', \\'inherently jointed\\']]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['weighting scheme', 'has', 'TFIDF']]\",\n",
       " \"[['Word2 Vec method', 'has', 'neural network language method']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Our method', 'has', 'better performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Graph Convolutional Network ( GCN )', 'has', 'simple and effective graph neural network']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['TF - IDF + LR', 'has', 'bag - of - words model']]\",\n",
       " '[]',\n",
       " \"[['CNN', 'has', 'Convolutional Neural Network']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Bi- LSTM', 'has', 'bi-directional LSTM']]\",\n",
       " \"[['PV - DBOW', 'has', 'paragraph vector model'], ['paragraph vector model', 'has', 'orders'], ['orders', 'has', 'orders of words']]\",\n",
       " '[]',\n",
       " \"[['PV - DM', 'has', 'paragraph vector model']]\",\n",
       " '[]',\n",
       " \"[['PTE', 'has', 'predictive text embedding']]\",\n",
       " '[]',\n",
       " \"[['SWEM', 'has', 'simple word embedding models']]\",\n",
       " \"[['LEAM', 'has', 'label - embedding attentive models']]\",\n",
       " '[]',\n",
       " \"[['Graph - CNN - C', 'has', 'graph CNN model']]\",\n",
       " '[]',\n",
       " \"[['Graph - CNN - F', 'has', 'Graph - CNN - C']]\",\n",
       " \"[['window size', 'has', '20']]\",\n",
       " \"[['learning rate', 'has', '0.02']]\",\n",
       " '[]',\n",
       " \"[['significantly outperforms', 'has', 'all baseline models']]\",\n",
       " \"[['pre-trained Glo Ve word embeddings', 'has', 'CNN']]\",\n",
       " '[]',\n",
       " \"[['PV - DBOW', 'has', 'comparable results']]\",\n",
       " '[]',\n",
       " \"[['Graph - CNN models', 'has', 'competitive performances']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['discrete text', 'has', 'continuous representation']]\",\n",
       " \"[['network depth', 'has', 'meta-parameter']]\",\n",
       " \"[['computational complexity', 'has', 'network']]\",\n",
       " '[]',\n",
       " \"[['generalizes', 'has', 'commonly used word embedding'], ['text regions', 'has', 'covering'], ['covering', 'has', 'one or more words']]\",\n",
       " '[]',\n",
       " \"[['final pooling layer', 'has', 'aggregates'], ['aggregates', 'has', 'internal data']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['all the five datasets', 'has', 'DPCNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Zhang', 'has', 'best linear model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['general framework', 'name', 'region embedding + pooling']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['elimination', 'has', 'word embedding layer']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['mini-batch size', 'has', '50 or 100'], ['optionally', 'has', 'rmsprop']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['three out of the four datasets', 'has', 'oh - 2 LSTMp']]\",\n",
       " \"[['RCV1', 'has', 'n-gram SVM'], ['RCV1', 'has', 'bow - CNN']]\",\n",
       " \"[['one - hot CNN', 'has', 'surprising well']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['our tasks', 'has', 'wv - 2 LSTMp']]\",\n",
       " \"[['performance', 'has', 'one - hot CNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['text classification', 'has', 'input']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['test performance', 'has', 'Elec and RCV1 datasets']]\",\n",
       " \"[['improved', 'has', 'test performance']]\",\n",
       " \"[['bidirectional LSTM', 'has', 'improves'], ['improves', 'has', 'results']]\",\n",
       " '[]',\n",
       " \"[['Adversarial training', 'has', 'improve']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Theano', 'has', 'python library']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['TREC', 'has', 'number of filters'], ['number of filters', 'has', '300'], ['memory dimension', 'has', '300']]\",\n",
       " \"[['word vector layer and the LSTM layer', 'has', 'probability']]\",\n",
       " \"[['L2 regularization', 'has', 'factor']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['consistently outperforms', 'has', 'all published neural baseline models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['multiple convolutional layers', 'has', 'in parallel']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['dictionary', 'has', 'following characters'], ['following characters', 'has', 'abcdefghijklmnopqrstuvwxyz0123456']]\",\n",
       " \"[['larger text', 'has', 'truncated']]\",\n",
       " \"[['character embedding', 'has', 'size'], ['size', 'has', '16']]\",\n",
       " \"[['initial learning rate', 'has', '0.01'], ['momentum', 'has', '0.9']]\",\n",
       " \"[['Torch', 'has', 'Torch 7']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['deep architecture', 'has', 'well']]\",\n",
       " \"[['smallest depth', 'has', 'our model']]\",\n",
       " '[]',\n",
       " \"[['small depth', 'has', 'temporal max - pooling']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['each dataset', 'has', 'bag - of - words model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['dataset', 'has', 'of size']]\",\n",
       " \"[['Conv Nets', 'has', 'well']]\",\n",
       " \"[['Choice of alphabet', 'has', 'difference']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['mini-batch size', 'has', '10'], ['default value', 'has', '1.0']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Subj and MR datasets', 'has', 'BLSTM - 2DCNN'], ['BLSTM - 2DCNN', 'has', 'second higher accuracies']]\",\n",
       " \"[['RCNN', 'has', 'BLSTM - 2DCNN']]\",\n",
       " '[]',\n",
       " \"[['DSCNN', 'has', 'BLSTM - 2DCNN'], ['BLSTM - 2DCNN', 'has', 'outperforms'], ['outperforms', 'has', 'five datasets']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Reuters', 'has', 'SVM']]\",\n",
       " \"[['AAPD', 'has', 'SVM']]\",\n",
       " \"[['SVM', 'has', 'LR baseline'], ['single - label datasets', 'has', 'IMDB and Yelp 2014']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Transformer', 'has', 'close']]\",\n",
       " '[]',\n",
       " \"[['our models', 'has', 'outperform'], ['outperform', 'has', 'Watson']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['deep learning architectures', 'has', 'Transformer and m LSTM']]\",\n",
       " \"[['Our models', 'has', 'lower F 1 scores']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )', 'has', 'text classification model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['size batch', 'has', '64']]\",\n",
       " '[]',\n",
       " \"[['NVIDIA GTX 1060 GPU', 'has', 'Intel Core i 7 4770s CPU']]\",\n",
       " \"[['network reduction', 'has', 'GAP']]\",\n",
       " \"[['dataset', 'has', 'four target classes']]\",\n",
       " \"[['Char - CNN', 'has', 'proposed model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['jointly embedding', 'has', 'word and label']]\",\n",
       " \"[['label embedding framework', 'has', 'Label - attentive text representation'], ['Label - attentive text representation', 'has', 'informative']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['initial learning rate', 'has', '0.001'], ['minibatch size', 'has', '100']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['LEAM', 'has', 'best AUC score']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['HDLTex', 'name', 'deep learning architectures']]\",\n",
       " '[]',\n",
       " \"[['processing', 'has', 'Xeon E5 ? 2640 ( 2.6 GHz )'], ['GPU cards', 'has', 'N vidia Quadro K620']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['nave Bayes', 'has', 'much worse']]\",\n",
       " '[]',\n",
       " \"[['data set W OS ? 11967', 'has', 'best accuracy']]\",\n",
       " \"[['86 %', 'has', 'over all']]\",\n",
       " \"[['data set W OS ? 46985', 'has', 'best scores']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['EXplicit interAction Model', 'name', 'EXAM']]\",\n",
       " \"[['three main components', 'name', 'word - level encoder']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['last layer', 'has', 'aggregates'], ['aggregates', 'has', 'matching scores']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['region size', 'has', '7'], ['embedding size', 'has', '128']]\",\n",
       " \"[['initial learning rate', 'has', '0.0001'], ['batch size', 'has', '16']]\",\n",
       " \"[['2 times', 'has', '2 times interaction feature length']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['baselines', 'has', 'three variants']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Char - based models', 'has', 'highest over all scores']]\",\n",
       " \"[['Word - based baselines', 'has', 'other variants']]\",\n",
       " \"[['five baselines', 'has', 'W.C Region Emb'], ['W.C Region Emb', 'has', 'best']]\",\n",
       " \"[['three datasets', 'name', 'AG']]\",\n",
       " \"[['Yah.A.', 'has', 'EXAM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['embedding size', 'has', '256']]\",\n",
       " \"[['each GRU Cell', 'has', '1,024 hidden states']]\",\n",
       " \"[['accumulated MLP', 'has', '60 hidden units']]\",\n",
       " \"[['batch size', 'has', '1000'], ['initial learning rate', 'has', '0.001']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['scores', 'has', 'best']]\",\n",
       " \"[['transfer accuracies', 'has', 'quite low']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['novel model', 'name', 'Disconnected Recurrent Neural Network ( DRNN )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['batch size', 'has', '128'], ['our proposed model', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all the other models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['size', 'has', '50'], ['size', 'has', '25']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['capability', 'has', 'capsule network']]\",\n",
       " \"[['capsule networks', 'has', 'substantial and significant improvement']]\",\n",
       " '[]',\n",
       " \"[['capsule network', 'has', 'much stronger transferring capability']]\",\n",
       " \"[['Reuters - Full', 'has', 'capsule network'], ['capsule network', 'has', 'robust superiority']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['window size', 'has', '20']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['validation accuracy', 'has', 'does not increase']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['four of the five datasets', 'name', 'MSNBC']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['ELMo representations', 'has', 'deep']]\",\n",
       " '[]',\n",
       " \"[['intrinsic evaluations', 'has', 'higher - level LSTM states']]\",\n",
       " '[]',\n",
       " \"[['ELMo', 'has', 'baseline model'], ['baseline model', 'has', 'test set F 1']]\",\n",
       " \"[['significantly larger', 'has', '1.8 % improvement']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['two different methods', 'has', 'greatly reduce'], ['greatly reduce', 'has', 'size']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['model', 'name', 'bert - largecased']]\",\n",
       " '[[\\'Transformer encoder layers\\', \\'has\\', \\'same parameters\\'], [\\'same parameters\\', \\'has\\', \\'\" base \" model\\']]',\n",
       " '[]',\n",
       " \"[['systematically', 'has', 'state of the art']]\",\n",
       " \"[['scores', 'has', 'state of the art']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['novel model GAS', 'has', 'gloss - augmented WSD neural network']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['gloss expansion depth K', 'has', 'value']]\",\n",
       " \"[['number of passes | T M |', 'has', '1 to 5']]\",\n",
       " '[]',\n",
       " \"[['drop rate', 'has', '0.5']]\",\n",
       " '[[\\'validation loss\\', \\'has\\', \"does n\\'t improve\"]]',\n",
       " '[]',\n",
       " \"[['Babelfy', 'has', 'semantic network structure']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['IMS +emb', 'has', 'IMS']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['best results', 'has', '70.6']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['increasing number of passes', 'has', 'F1 - score'], ['F1 - score', 'has', 'increases']]\",\n",
       " \"[['number of passes', 'has', '3']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Our proposed model', 'has', 'top score']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "executive-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data,columns=['triple_B'])\n",
    "data.to_csv('B.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-device",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
