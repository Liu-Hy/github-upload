{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designing-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "from ast import literal_eval as load\n",
    "import logging\n",
    "from collections import Counter\n",
    "from simpletransformers.ner import (\n",
    "    NERArgs,\n",
    "    NERModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "productive-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_type=0\n",
    "labelset = [\"B\", \"I\", \"O\"]\n",
    "type_ls = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "animal-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720\n",
      "2720\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "pos = pd.read_csv('pos_sent.csv')\n",
    "print(len(pos))\n",
    "col_name = pos.columns[6+BIO_type]\n",
    "pos = pos.dropna(axis=0, subset=[col_name])\n",
    "print(len(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriented-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(pos)):\n",
    "    words = pos.loc[i,'text'].split(' ')\n",
    "    tags = load(pos.loc[i,'BIO'])\n",
    "    for j in range(len(words)):\n",
    "        data.append([i, words[j], tags[j]])\n",
    "df = pd.DataFrame(data, columns=['sentence_id', 'words', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "upset-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_spans(ls):\n",
    "    # given a sequence of BIO tags, get the list of tuples representing spans of entities\n",
    "    spans = []\n",
    "    for type in type_ls:\n",
    "        for i in range(len(ls)):\n",
    "            st, ed = 0, 0\n",
    "            if ls[i] == 'B'+type:\n",
    "                st, ed = i, i + 1\n",
    "                for j in range(i+1, len(ls)):\n",
    "                    if ls[j] == 'I'+type:\n",
    "                        ed += 1\n",
    "                    else:\n",
    "                        break\n",
    "                spans.append((st, ed))\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "superior-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_F1(ref, pred):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../ner/'\n",
    "model_ls = []\n",
    "for i in range(43):\n",
    "    folder = os.path.join(base_dir,'Nmoutputs'+str(i))\n",
    "    models = os.listdir(folder)\n",
    "    models = [os.path.join(folder,model) for model in models if model[:11]=='checkpoint-' and model[-1]!='1']\n",
    "    model_ls += models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hispanic-accessory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legendary-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.fp16 = False\n",
    "model_args.manual_seed = 1\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.do_lower_case = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86d082aa37a43758c5d7a76be89e827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8afa8390d1846459779a08264ee1476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hl/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "INFO:simpletransformers.ner.ner_model:{'eval_loss': 3.0725177056649153, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'F1_score': 0}\n",
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef3fddb070640a2b7984dc0a7f58ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8026981ec46a42d6951dd3ba0ee3e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model:{'eval_loss': 2.4773749745943965, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'F1_score': 0}\n",
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea45378b8a641068e9ae60b11d9bcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e18c50177064b5a9fe49724b6eb1fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model:{'eval_loss': 2.770830958906342, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'F1_score': 0}\n",
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851d07e923394042855387e846fb48f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f41510cc924c93a36b8e3e884dcf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model:{'eval_loss': 2.4743904015597176, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'F1_score': 0}\n",
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d78fba41be4c998715067c7bfbaebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5a2e6c7b5f4eb2b0a0b6dcf6ac991e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "each_pred = [[] for k in range(len(model_ls))]\n",
    "for i in range(len(model_ls)):\n",
    "    # Create a TransformerModel\n",
    "    model = NERModel(\n",
    "        'bert',\n",
    "        model_ls[i],\n",
    "        labels=labelset,\n",
    "        args=model_args,\n",
    "    )\n",
    "    result, model_outputs, pred_label = model.eval_model(df, F1_score=phrase_F1)\n",
    "    each_pred[i] = [get_entity_spans(label) for label in pred_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls = [[] for i in range(len(pos))]\n",
    "for i in range(len(pos)):\n",
    "    preds = []\n",
    "    for j in range(len(model_ls)):\n",
    "        preds += each_pred[j][i]\n",
    "    cnt = Counter(preds)\n",
    "    pred_ls[i] = [k for k,v in cnt.items() if v>62]\n",
    "    pred_ls[i] = sorted(pred_ls[i], key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos)==len(pred_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[str(l) for l in pred_ls]\n",
    "pos = pd.read_csv('pos_sent.csv')\n",
    "pos['BIO_1']=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.to_csv('pos_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls1 = []\n",
    "for i in range(len(pred_ls)):\n",
    "    for j in range(len(pred_ls[i])):\n",
    "        tup = pred_ls[i][j]\n",
    "        word_ls=pos.loc[i,'text'].split(' ')\n",
    "        word_ls.insert(tup[0],'<<')\n",
    "        word_ls.insert(tup[1]+1,'>>')\n",
    "        ls1.append([i,j,' '.join(word_ls),0])\n",
    "dataframe = pd.DataFrame(ls1)\n",
    "dataframe.columns = ['sent_idx','phrase_idx','text','labels']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (\n",
    "    ClassificationArgs,\n",
    "    ClassificationModel,\n",
    ")\n",
    "model_args1 = ClassificationArgs()\n",
    "\n",
    "model_args1.overwrite_output_dir = True\n",
    "model_args1.reprocess_input_data = True\n",
    "model_args1.manual_seed = 1\n",
    "model_args1.fp16 = False\n",
    "model_args1.use_multiprocessing = True\n",
    "model_args1.do_lower_case = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ClassificationModel(\n",
    "    \"xlmroberta\",\n",
    "    \"../ner/outputs23/best_model\",\n",
    "    args=model_args1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model1.eval_model(dataframe, F1_score=sklearn.metrics.f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(model_outputs.argmax(axis=1))\n",
    "dataframe['pred'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = [[[],[]] for i in range(len(pos))]\n",
    "for i in range(len(dataframe)):\n",
    "    s_id = dataframe.loc[i,'sent_idx']\n",
    "    p_id = dataframe.loc[i,'phrase_idx']\n",
    "    st,ed = pred_ls[s_id][p_id]\n",
    "    word_ls = pos.loc[s_id,'text'].split(' ')\n",
    "    phrase = ' '.join(word_ls[st:ed])\n",
    "    tup = (phrase, (st,ed))\n",
    "    if dataframe.loc[i,'pred'] == 0:\n",
    "        all[s_id][1].append(tup)\n",
    "    else:\n",
    "        all[s_id][0].append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=pd.read_csv('pos_sent.csv')\n",
    "#dt=dt['labels','text']\n",
    "a=pd.DataFrame(all)\n",
    "a.columns=['predicates', 'subj/obj']\n",
    "a=pd.concat([dt[['labels','text']],a],axis=1)\n",
    "a['triple_A']='[]'\n",
    "a['triple_B']='[]'\n",
    "a['triple_C']='[]'\n",
    "a['triple_D']='[]'\n",
    "a=a.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('triples.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-origin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
