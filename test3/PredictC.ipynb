{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cross-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in the hyperparameter space with W&B sweep\n",
    "import logging\n",
    "from ast import literal_eval as load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import sklearn\n",
    "\n",
    "from simpletransformers.classification import (\n",
    "    ClassificationArgs,\n",
    "    ClassificationModel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "df = pd.read_csv('triples.csv')\n",
    "df.insert(loc=0, column='idx', value=np.arange(len(df)))\n",
    "sent_num=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collectible-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(arr):\n",
    "    trip_list=[] # not trip_ls\n",
    "    ls=[]\n",
    "    for i in range(len(arr)):\n",
    "        uni_name=arr[i,1]\n",
    "        uni_name=(uni_name[0].upper()+uni_name[1:]).replace('-',' ')\n",
    "        if arr[i,3]!='[]' and arr[i,4]!='[]':\n",
    "            pre = load(arr[i,3])[0]\n",
    "            np = load(arr[i,4])[0]\n",
    "            if pre[1][0]<np[1][0]:\n",
    "                triple=[uni_name,pre[0],np[0]]\n",
    "                trip_list.append(triple)\n",
    "                \n",
    "                word_ls = arr[i,2].split(' ')\n",
    "                word_ls.insert(pre[1][0], '<<')\n",
    "                word_ls.insert(pre[1][1]+1, '>>')\n",
    "                word_ls.insert(np[1][0]+2, '[[')\n",
    "                word_ls.insert(np[1][1]+3, ']]')\n",
    "                unit = arr[i,1]\n",
    "                unit = (unit[0].upper()+unit[1:]).replace('-',' ')\n",
    "                unit_ls = ['[[']+(unit.split(' '))+[']]']\n",
    "                word_ls = unit_ls+[':']+word_ls\n",
    "                flg=0\n",
    "                if arr[i,7]=='[]':\n",
    "                    trip_ls = []\n",
    "                else:\n",
    "                    trip_ls = load(arr[i,7])\n",
    "                    for trip in trip_ls:\n",
    "                        if trip[1]==pre[0] and trip[2]==np[0]:\n",
    "                            flg=1\n",
    "                            break\n",
    "                ls.append([int(arr[i, 0]), unit, pre[0], np[0], trip_ls, ' '.join(word_ls), flg])\n",
    "    dataframe = pd.DataFrame(ls)\n",
    "    dataframe.columns = ['idx', 'info_unit', 'pre', 'np', 'triples', 'text', 'labels']\n",
    "    return dataframe,trip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "framed-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.values\n",
    "df,trip_list=convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welsh-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.manual_seed = 1\n",
    "model_args.fp16 = False\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.do_lower_case = True  # when using uncased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "missing-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_F1(ref, pred):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olympic-orbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2a77afee654aa4acebf2662d6f492c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4baa8dce3c314e5d9a41adb50ad2d8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hl/lib/python3.9/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 0, 'tn': 169, 'fp': 1059, 'fn': 0, 'F1_score': 0, 'eval_loss': 2.765305299263496}\n"
     ]
    }
   ],
   "source": [
    "# Create a TransformerModel\n",
    "model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"../rel/outputsC/best_model\",\n",
    "    args=model_args,\n",
    ")\n",
    "result, model_outputs, wrong_predictions = model.eval_model(df, F1_score=triple_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brief-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(model_outputs.argmax(axis=1))\n",
    "df['preds']=preds\n",
    "df['cand']=trip_list\n",
    "df.loc[df['preds']==0,'cand']=None\n",
    "data=[]\n",
    "for i in range(sent_num):\n",
    "    temp = list(df[df['idx']==i]['cand'])\n",
    "    temp = [t for t in temp if t]\n",
    "    data.append(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enabling-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[]',\n",
       " \"[['Model', 'introduce', 'recurrent neural network grammars ( RNNGs']]\",\n",
       " '[]',\n",
       " \"[['Model', 'give', 'two variants']]\",\n",
       " '[]',\n",
       " \"[['Model', 'present', 'simple importance sampling algorithm']]\",\n",
       " \"[['Hyperparameters', 'For', 'discriminative model']]\",\n",
       " \"[['Hyperparameters', 'For', 'generative model']]\",\n",
       " \"[['Hyperparameters', 'tuned', 'dropout rate']]\",\n",
       " \"[['Hyperparameters', 'For', 'sequential LSTM baseline']]\",\n",
       " \"[['Hyperparameters', 'For', 'training']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'show', 'even larger performance gains']]\",\n",
       " '[]',\n",
       " \"[['Model', 'introducing', 'cloze - style training objective']]\",\n",
       " \"[['Model', 'separately computes', 'forward and backward states']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'run', 'experiments']]\",\n",
       " \"[['Experimental setup', 'use', 'NCCL2 library']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'with comparison to', 'fine tuning']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'results in', 'bilm loss']]\",\n",
       " \"[['Results', 'shows', 'cloze loss']]\",\n",
       " '[]',\n",
       " \"[['Model', 'update', 'Seq2seq approach']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'using', 'subword split']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'work', 'poorly']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'used', 'model']]\",\n",
       " \"[['Hyperparameters', 'Training on', 'small dataset']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'pre-trained', 'skip - gram embeddings']]\",\n",
       " '[]',\n",
       " \"[['Results', 'trained on', 'large high - confidence corpus']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'present', 'beam - based search procedure']]\",\n",
       " \"[['Results', 'taking', 'weighted average']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'combining', 'scores']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'novel transition system']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'find that', 'bottom - up parser and the top - down parser']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'present', 'neural - net parse reranker']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'initialize', 'starting states']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'vanilla softmax']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'focus on', 'RNNGs']]\",\n",
       " \"[['Approach', 'manipulates', 'inductive bias']]\",\n",
       " \"[['Approach', 'begin with', 'importance']]\",\n",
       " \"[['Approach', 'augment', 'RNNG composition function']]\",\n",
       " \"[['Approach', 'Using', 'GA - RNNG']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Ablating', 'stack']]\",\n",
       " '[]',\n",
       " \"[['Results', 'modeling', 'syntax']]\",\n",
       " \"[['Results', 'remark', 'stack - only results']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'On', 'test data']]\",\n",
       " '[]',\n",
       " \"[['Model', 'introduce', 'parser']]\",\n",
       " \"[['Model', 'present', 'character LSTM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'train', 'deep neural network']]\",\n",
       " \"[['Model', 'captures', 'entity - level information']]\",\n",
       " \"[['Model', 'Using', 'cluster - pair representations']]\",\n",
       " \"[['Model', 'At', 'test time']]\",\n",
       " \"[['Model', 'makes', 'novel easy - first cluster - ranking procedure']]\",\n",
       " \"[['Model', 'using', 'learning - to - search algorithm']]\",\n",
       " \"[['Model', 'allows', 'model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'goal - directed endto - end deep reinforcement learning framework']]\",\n",
       " \"[['Model', 'leverage', 'neural architecture']]\",\n",
       " '[]',\n",
       " \"[['Model', 'introduce', 'entropy regularization term']]\",\n",
       " \"[['Model', 'update', 'regularized policy network parameters']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'pretrain', 'our model']]\",\n",
       " \"[['Hyperparameters', 'set', 'number of sampled trajectories N s']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Built on top of', 'model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'introducing', 'context - dependent ELMo embedding']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'modify', 'max-margin coreference objective']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'is', 'neural mention - ranking model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'find', 'REINFORCE']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'introduce', 'approximation']]\",\n",
       " \"[['Approach', 'At', 'each iteration']]\",\n",
       " \"[['Approach', 'propose', 'coarseto - fine approach']]\",\n",
       " \"[['Approach', 'introduce', 'less accurate but more efficient coarse factor']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'cheaply computes', 'rough sketch']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'observe', 'further improvement']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'inspired by', 'mention - ranking model']]\",\n",
       " \"[['Model', 'Given', 'anaphoric sentence']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'consider', 'one anaphor at a time']]\",\n",
       " \"[['Model', 'show', 'model']]\",\n",
       " \"[['Model', 'produces', 'large amounts of instances']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'report', 'preceding sentence baseline ( PS BL )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'initialized', 'weight matrices']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'trained', 'our model']]\",\n",
       " \"[['Hyperparameters', 'clip', 'gradients']]\",\n",
       " \"[['Hyperparameters', 'train for', '10 epochs']]\",\n",
       " \"[['Hyperparameters', 'used', 'l 2 - regularization']]\",\n",
       " '[]',\n",
       " \"[['Results', 'In terms of', 's@1 score']]\",\n",
       " \"[['Results', 'with', 'HPs']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'for', 'shell noun resolution']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'posit', 'global context']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'no', 'manually defined cluster features']]\",\n",
       " \"[['Approach', 'incorporate', 'mention - ranking style coreference system']]\",\n",
       " \"[['Approach', 'including', 'recurrent neural network']]\",\n",
       " \"[['Approach', 'train', 'model']]\",\n",
       " \"[['Experimental setup', 'For', 'training']]\",\n",
       " \"[['Experimental setup', 'find', 'initial learning rate']]\",\n",
       " \"[['Experimental setup', 'set', 'ha ( x n ) , h c ( x n ) , and h ( m )']]\",\n",
       " \"[['Experimental setup', 'use', 'single - layer LSTM']]\",\n",
       " \"[['Experimental setup', 'For', 'regularization']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'makes use of', 'GPU']]\",\n",
       " '[]',\n",
       " \"[['Results', 'see', 'statistically significant improvement']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'present', 'word embedding model']]\",\n",
       " \"[['Research problem', 'by running', 'attentional sentence linking models']]\",\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'cross - sentence encoder']]\",\n",
       " '[]',\n",
       " \"[['Model', 'With', 'context memory block']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'In', 'ASL']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'randomly select', 'up to 40 continuous sentences']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Comparing with', 'baseline model']]\",\n",
       " \"[['Results', 'show', 'models']]\",\n",
       " \"[['Results', 'indicated', 'ASL model']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'propose', 'entity - level representation']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'uses', 'contextual embeddings']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'done by using', 'BERT']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Removing', 'second - order span - representations']]\",\n",
       " \"[['Results', 'Replacing', 'secondorder span representations']]\",\n",
       " \"[['Results', 'set', 'new state of the art']]\",\n",
       " \"[['Research problem', 'End - to - end', 'Neural Coreference Resolution']]\",\n",
       " \"[['Research problem', 'introduce', 'first end - to - end coreference resolution model']]\",\n",
       " \"[['Model', 'present', 'end - toend']]\",\n",
       " \"[['Model', 'demonstrate', 'performance']]\",\n",
       " '[]',\n",
       " \"[['Model', 'includes', 'span - ranking model']]\",\n",
       " \"[['Model', 'are', 'vector embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'In', 'character CNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'ADAM']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'apply', '0.5 dropout']]\",\n",
       " \"[['Hyperparameters', 'apply', '0.2 dropout']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'trained for', 'up to 150 epochs']]\",\n",
       " \"[['Hyperparameters', 'implemented in', 'Tensor - Flow']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'contribute', '3.8 F1']]\",\n",
       " \"[['Ablation analysis', 'see', 'contribution']]\",\n",
       " \"[['Ablation analysis', 'show', '1.3 F1 degradation']]\",\n",
       " \"[['Ablation analysis', 'keeping', 'mention candidates']]\",\n",
       " \"[['Ablation analysis', 'With', 'oracle mentions']]\",\n",
       " '[]',\n",
       " \"[['Model', 'fine - tune', 'BERT']]\",\n",
       " \"[['Model', 'extending', 'c 2f - coref model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'fine tune', 'all models']]\",\n",
       " \"[['Hyperparameters', 'trained', 'separate models']]\",\n",
       " '[]',\n",
       " \"[['Results', 'shows', 'BERT']]\",\n",
       " '[]',\n",
       " \"[['Results', 'shows', 'BERT - base']]\",\n",
       " '[]',\n",
       " \"[['Results', 'observe', 'overlap variant']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'new structured - data encoder']]\",\n",
       " \"[['Model', 'focuses on', 'encoding']]\",\n",
       " \"[['Model', 'model', 'general structure']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'dropout']]\",\n",
       " \"[['Hyperparameters', 'trained with', 'batch size']]\",\n",
       " \"[['Hyperparameters', 'follow', 'training procedure']]\",\n",
       " \"[['Hyperparameters', 'trained with', 'Adam optimizer']]\",\n",
       " \"[['Hyperparameters', 'used', 'beam search']]\",\n",
       " \"[['Hyperparameters', 'implemented in', 'Open NMT - py']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'shows', 'Flat scenario']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'present', 'neural ensemble natural language generator']]\",\n",
       " \"[['Model', 'explore', 'novel ways']]\",\n",
       " \"[['Experimental setup', 'built', 'ensemble model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'show', 'LSTM and the CNN models']]\",\n",
       " \"[['Results', 'On', 'official E2E test set']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'cast', 'language generation and interpretation']]\",\n",
       " \"[['Approach', 'builds on', 'learned Rational Speech Acts ( RSA ) models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'learns', 'content plan']]\",\n",
       " \"[['Model', 'train', 'end - to - end']]\",\n",
       " \"[['Hyperparameters', 'used', 'one - layer pointer networks']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'applied', 'dropout']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'text decoding']]\",\n",
       " \"[['Hyperparameters', 'set', 'beam size']]\",\n",
       " \"[['Hyperparameters', 'implemented in', 'Open NMT - py']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Compared to', 'best reported system']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'explicit , symbolic , text planning stage']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'developed', 'end - to - end neural baseline']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'constitute', 'more general approach']]\",\n",
       " \"[['Model', 'present', 'character - level sequence - to - sequence model']]\",\n",
       " '[]',\n",
       " \"[['Model', 'achieves', 'rather interesting performance results']]\",\n",
       " \"[['Model', 'shows', 'two important features']]\",\n",
       " \"[['Experimental setup', 'developed', 'our system']]\",\n",
       " \"[['Experimental setup', 'minimize', 'negative log - likelihood loss']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'propose', 'novel neural network model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'present', 'novel neural network - based model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'apply', 'dropout']]\",\n",
       " \"[['Experimental setup', 'apply', 'word dropout']]\",\n",
       " \"[['Experimental setup', 'optimize', 'objective loss']]\",\n",
       " \"[['Experimental setup', 'use', '100 - dimensional word embeddings']]\",\n",
       " \"[['Experimental setup', 'fix', 'number of hidden nodes']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'centered around', 'BiRNNs']]\",\n",
       " \"[['Approach', 'represent', 'each word']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'demonstrate', 'effectiveness']]\",\n",
       " \"[['Approach', 'In', 'graphbased parser']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'LSTM variant']]\",\n",
       " \"[['Results', 'clear that', 'our parsers']]\",\n",
       " \"[['Results', 'not using', 'external embeddings']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Moving from', 'simple ( 4 features )']]\",\n",
       " \"[['Results', 'adding', 'external word embeddings']]\",\n",
       " \"[['Model', 'give', 'probabilistic interpretation']]\",\n",
       " \"[['Model', 'distilling', 'ensemble']]\",\n",
       " \"[['Model', 'derive', 'cost']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'consider', 'neural FOG parser']]\",\n",
       " '[]',\n",
       " \"[['Results', 'training', 'same model']]\",\n",
       " \"[['Results', 'For', 'English']]\",\n",
       " \"[['Results', 'For', 'Chinese']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'For', 'three BiLSTM - CRF - based models']]\",\n",
       " \"[['Experimental setup', 'For', 'traditional feature - based models']]\",\n",
       " \"[['Experimental setup', 'For', 'BiLSTM - CRF - based models']]\",\n",
       " \"[['Experimental setup', 'For', 'Stanford - NNdep']]\",\n",
       " \"[['Experimental setup', 'For', 'jPTDP']]\",\n",
       " \"[['Experimental setup', 'fix', 'number of BiLSTM layers']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'for', 'PTB']]\",\n",
       " \"[['Results', 'On', 'GENIA and CRAFT']]\",\n",
       " '[]',\n",
       " \"[['Results', 'On', 'GENIA']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'novel neural network architecture']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 're-implemented', 'graph - based Deep Biaffine ( BIAF ) parser']]\",\n",
       " '[]',\n",
       " \"[['Results', 'On', 'UAS and LAS']]\",\n",
       " '[]',\n",
       " \"[['Results', 'On', 'LCM and UCM']]\",\n",
       " '[]',\n",
       " \"[['Results', 're-implementation of', 'BIAF']]\",\n",
       " '[]',\n",
       " \"[['Results', 'On', 'German']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'combine', 'representational power']]\",\n",
       " \"[['Model', 'train', 'neural network']]\",\n",
       " \"[['Model', 'use', 'activations']]\",\n",
       " \"[['Model', 'generate', 'high - confidence parse trees']]\",\n",
       " \"[['Model', 'known', 'tri-training']]\",\n",
       " \"[['Hyperparameters', 'used', 'publicly available word2vec 2 tool']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'modify', 'neural graphbased approach']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'adapt', 'training criterion']]\",\n",
       " \"[['Model', 'use', 'method']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'introduce', 'globally normalized transition - based neural network model']]\",\n",
       " \"[['Model', 'demonstrate', 'simple feed - forward networks']]\",\n",
       " \"[['Model', 'uses', 'transition system']]\",\n",
       " \"[['Model', 'perform', 'beam search']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Using', 'beam search']]\",\n",
       " \"[['Ablation analysis', 'set of', 'character ngrams feature']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'significantly outperform', 'LSTM - based approaches']]\",\n",
       " '[]',\n",
       " \"[['Model', 'explore', 'very large corpus']]\",\n",
       " \"[['Research problem', 'show', 'linear models']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'use', '10 hidden units']]\",\n",
       " \"[['Results', 'adding', 'bigram information']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'tune', 'hyperparameters']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'compare with', 'Tagspace']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'combine', 'state - of - the - art cross - lingual methods']]\",\n",
       " \"[['Model', 'for', 'domain adaptation']]\",\n",
       " \"[['Model', 'based on', 'masked language model ( MLM ) pre-training']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'In', 'sentiment classification task']]\",\n",
       " '[]',\n",
       " \"[['Results', 'in', 'MLdoc dataset']]\",\n",
       " \"[['Results', 'In', 'sentiment classification task']]\",\n",
       " '[]',\n",
       " \"[['Results', 'In', 'MLdoc dataset']]\",\n",
       " \"[['Results', 'comparing with', 'best cross - lingual results']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'Leveraging', 'unlabeled data']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'proposes', 'Neural Attentive Bagof - Entities ( NABoE ) model']]\",\n",
       " \"[['Model', 'For', 'each entity name']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'trained using', 'mini-batch SGD']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Relative to', 'baselines']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'task - oriented word embedding method']]\",\n",
       " '[]',\n",
       " \"[['Model', 'To model', 'task information']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'new graph neural networkbased method']]\",\n",
       " \"[['Model', 'construct', 'single large graph']]\",\n",
       " \"[['Model', 'model', 'graph']]\",\n",
       " '[]',\n",
       " \"[['Model', 'turn', 'text classification problem']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'compare', 'Text GCN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'explored', 'CNN -rand']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'used', 'Logistic Regression']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'used', 'Logistic Regression']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'Text GCN']]\",\n",
       " \"[['Hyperparameters', 'tuned', 'other parameters']]\",\n",
       " \"[['Hyperparameters', 'For', 'baseline models']]\",\n",
       " '[]',\n",
       " \"[['Results', 'When', 'pre-trained Glo Ve word embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'call', 'deep pyramid CNN ( DPCNN )']]\",\n",
       " \"[['Model', 'After converting', 'discrete text']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'show', 'DPCNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'max pooling']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'On', 'all the five datasets']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'build on', 'general framework']]\",\n",
       " \"[['Model', 'designed to enable', 'learning']]\",\n",
       " '[]',\n",
       " \"[['Model', 'to', 'model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'On', 'our tasks']]\",\n",
       " \"[['Results', 'review', 'performance']]\",\n",
       " '[]',\n",
       " \"[['Results', 'Increasing', 'dimensionality']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'to', 'text classification tasks']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'define', 'perturbation']]\",\n",
       " \"[['Model', 'propose', 'text classifier']]\",\n",
       " \"[['Experimental setup', 'used', 'TensorFlow']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'trained for', '100,000 steps']]\",\n",
       " \"[['Experimental setup', 'applied', 'gradient clipping']]\",\n",
       " \"[['Experimental setup', 'For', 'regularization']]\",\n",
       " \"[['Experimental setup', 'For', 'bidirectional LSTM model']]\",\n",
       " \"[['Results', 'saw that', 'cosine distance']]\",\n",
       " \"[['Results', 'shows', 'test performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'shows', 'test performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'introduce', 'new architecture']]\",\n",
       " \"[['Model', 'design', 'simple end - to - end , unified architecture']]\",\n",
       " '[]',\n",
       " \"[['Model', 'to learn', 'sequential correlations']]\",\n",
       " \"[['Model', 'choose', 'sequence - based input']]\",\n",
       " \"[['Baselines', 'implement', 'our model']]\",\n",
       " \"[['Experimental setup', 'of', 'tensors']]\",\n",
       " \"[['Experimental setup', 'use', 'one convolutional layer and one LSTM layer']]\",\n",
       " \"[['Experimental setup', 'For', 'TREC']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'add', 'L2 regularization']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'achieve', 'fourth best published result']]\",\n",
       " \"[['Results', 'For', 'binary classification task']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'close to', 'state - of - the - art SVM']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'shown', 'single convolutional layer']]\",\n",
       " \"[['Ablation analysis', 'For', 'multiple convolutional layers']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'performed on', 'single NVidia K40 GPU']]\",\n",
       " \"[['Experimental setup', 'use', 'temporal batch norm']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'observe', 'small depth']]\",\n",
       " '[]',\n",
       " \"[['Model', 'treating', 'text']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'For', 'each dataset']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'is', 'character - level ConvNets']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'proposes', 'Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )']]\",\n",
       " \"[['Model', 'utilizes', 'Bidirectional Long Short - Term Memory Networks ( BLSTM )']]\",\n",
       " '[]',\n",
       " \"[['Model', 'applies', '2D convolution ( BLSTM - 2DCNN )']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', '100 convolutional filters']]\",\n",
       " \"[['Hyperparameters', 'set', 'mini-batch size']]\",\n",
       " \"[['Hyperparameters', 'For', 'regularization']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Compared with', 'RCNN']]\",\n",
       " \"[['Results', 'Compared with', 'ReNN']]\",\n",
       " \"[['Results', 'Compared with', 'DSCNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'find', 'simple bi-directional LSTM ( BiLSTM ) architecture']]\",\n",
       " \"[['Baselines', 'conduct', 'large - scale reproducibility study']]\",\n",
       " \"[['Baselines', 'compare', 'neural approaches']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'performed on', 'Nvidia GTX 1080 and RTX 2080 Ti GPUs']]\",\n",
       " \"[['Experimental setup', 'use', 'Scikitlearn 0.19.2']]\",\n",
       " \"[['Results', 'see that', 'our simple LSTM reg model']]\",\n",
       " \"[['Results', 'observe', 'LSTM reg']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'On', 'AAPD']]\",\n",
       " \"[['Results', 'Compared to', 'SVM']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'train', 'mLSTM and Transformer language models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'find that', 'our models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'compare', 'deep learning architectures']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'investigate', 'modifications']]\",\n",
       " \"[['Model', 'used', 'Temporal Depthwise Separable Convolution']]\",\n",
       " \"[['Model', 'propose', 'Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )']]\",\n",
       " \"[['Experimental setup', 'For', 'SVDCNN and Char - CNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'performed on', 'NVIDIA GTX 1060 GPU']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'Considering', 'dataset']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'use', '300 - dimensional Glo Ve word embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'train', 'Adam Optimizer']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'implemented using', 'Tensorflow']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'compare against', 'logistic regression model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'perform', 'hierarchical classification']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'using', 'central processing units ( CPUs )']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'implemented', 'Python']]\",\n",
       " \"[['Experimental setup', 'used', 'Keras and Tensor Flow libraries']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'For', 'data set W OS ? 11967']]\",\n",
       " '[]',\n",
       " \"[['Results', 'For', 'data set W OS ? 46985']]\",\n",
       " '[]',\n",
       " \"[['Model', 'introduce', 'interaction mechanism']]\",\n",
       " \"[['Model', 'devise', 'EXplicit interAction Model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'For', 'multi -class task']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'used', 'adam ( Kingma and Ba 2014 )']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'implemented and trained by', 'MXNet ( Chen et al. )']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'For', 'five baselines']]\",\n",
       " '[]',\n",
       " \"[['Results', 'For', 'Yah.A.']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'implemented', 'baseline models']]\",\n",
       " \"[['Experimental setup', 'used', 'matrix']]\",\n",
       " \"[['Experimental setup', 'adopted', 'GRU']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'applied', 'Adam']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'use', 'data']]\",\n",
       " \"[['Model', 'For', 'each language']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'Training on', 'German or French']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'leads to', 'important improvement']]\",\n",
       " '[]',\n",
       " \"[['Model', 'incorporate', 'positioninvariance']]\",\n",
       " \"[['Model', 'To maintain', 'position - invariance']]\",\n",
       " \"[['Model', 'regarded as', 'special 1D CNN']]\",\n",
       " \"[['Hyperparameters', 'utilize', '300D Glo Ve 840B vectors']]\",\n",
       " \"[['Hyperparameters', 'use', 'Adadelta ( Zeiler , 2012 )']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'To avoid', 'gradient explosion problem']]\",\n",
       " '[]',\n",
       " \"[['Results', 'see that', 'very deep CNN ( VDCNN )']]\",\n",
       " \"[['Results', 'shows', 'our model']]\",\n",
       " \"[['Results', 'shows', 'DRNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'called', 'capsule network']]\",\n",
       " \"[['Model', 'introduce', 'iterative routing process']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', '300 - dimensional word2vec vectors']]\",\n",
       " \"[['Hyperparameters', 'conduct', 'mini-batch']]\",\n",
       " \"[['Hyperparameters', 'use', 'Adam optimization algorithm']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'observe', 'capsule networks']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'investigate', 'capability']]\",\n",
       " \"[['Ablation analysis', 'observe that', 'capsule networks']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'observe that', 'capsule networks']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'implemented in', 'Torch framework']]\",\n",
       " \"[['Experimental setup', 'For', 'entity embeddings only']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'use', 'Adam']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'Training on', 'single GPU']]\",\n",
       " \"[['Results', 'obtain', 'state of the art accuracy']]\",\n",
       " \"[['Results', 'analyzed', 'accuracy']]\",\n",
       " \"[['Results', 'shows', 'our method']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Research problem', 'propose', 'words and entities']]\",\n",
       " \"[['Model', 'addressing', 'NED']]\",\n",
       " \"[['Model', 'describe', 'new contextualized embedding model']]\",\n",
       " \"[['Model', 'based on', 'bidirectional transformer encoder']]\",\n",
       " \"[['Model', 'takes', 'sequence of words and entities']]\",\n",
       " \"[['Model', 'propose', 'masked entity prediction']]\",\n",
       " \"[['Model', 'trained', 'model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'using', 'pseudo entity annotations']]\",\n",
       " \"[['Results', 'achieved', 'new state - of - the - art results']]\",\n",
       " \"[['Results', 'using', 'pseudo entity annotations']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'differ from', 'traditional word type embeddings']]\",\n",
       " \"[['Model', 'use', 'vectors']]\",\n",
       " \"[['Model', 'call', 'ELMo ( Embeddings from Language Models ) representations']]\",\n",
       " '[]',\n",
       " \"[['Model', 'learn', 'linear combination of the vectors']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'propose', 'two different methods']]\",\n",
       " \"[['Approach', 'present', 'WSD system']]\",\n",
       " '[]',\n",
       " \"[['Approach', 'by taking advantage of', 'semantic relationships']]\",\n",
       " \"[['Approach', 'based on', 'observation']]\",\n",
       " \"[['Experimental setup', 'For', 'BERT']]\",\n",
       " \"[['Experimental setup', 'For', 'Transformer encoder layers']]\",\n",
       " \"[['Results', 'observe', 'our systems']]\",\n",
       " \"[['Results', 'thanks to', 'Princeton WordNet Gloss Corpus']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'Using', 'BERT']]\",\n",
       " \"[['Ablation analysis', 'using', 'ensembles']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'propose', 'novel model GAS']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'use', 'pre-trained word embeddings']]\",\n",
       " \"[['Hyperparameters', 'employ', '256 hidden units']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'assign', 'gloss expansion depth K']]\",\n",
       " \"[['Hyperparameters', 'experiment with', 'number of passes | T M |']]\",\n",
       " \"[['Hyperparameters', 'use', 'Adam optimizer']]\",\n",
       " \"[['Hyperparameters', 'to avoid', 'overfitting']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'find that', 'GAS ext']]\",\n",
       " \"[['Results', 'shows', 'appropriate number of passes']]\",\n",
       " '[]',\n",
       " \"[['Results', 'shows that', 'multiple passes operation']]\",\n",
       " \"[['Results', 'with', 'increasing number of passes']]\",\n",
       " \"[['Results', 'when', 'number of passes']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'modeling', 'sequence of words']]\",\n",
       " \"[['Experimental setup', 'implemented using', 'TensorFlow']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'see that', 'dropword']]\",\n",
       " \"[['Results', 'Randomizing', 'input words']]\",\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fewer-diameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2720"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mobile-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>triple_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[['Model', 'introduce', 'recurrent neural netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[['Model', 'give', 'two variants']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>[['Experimental setup', 'trained', 'model']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>[['Experimental setup', 'tuned', 'number of ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2720 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               triple_C\n",
       "0                                                    []\n",
       "1     [['Model', 'introduce', 'recurrent neural netw...\n",
       "2                                                    []\n",
       "3                   [['Model', 'give', 'two variants']]\n",
       "4                                                    []\n",
       "...                                                 ...\n",
       "2715       [['Experimental setup', 'trained', 'model']]\n",
       "2716  [['Experimental setup', 'tuned', 'number of ep...\n",
       "2717                                                 []\n",
       "2718                                                 []\n",
       "2719                                                 []\n",
       "\n",
       "[2720 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(data,columns=['triple_C'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "serious-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('C.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-union",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
