triple_B
[]
[]
[]
"[['two variants', 'has', 'algorithm']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['pretraining', 'has', 'bi-directional transformer model']]"
[]
[]
"[['bi-directional transformer architecture', 'has', 'predicts'], ['predicts', 'has', 'every token']]"
"[['cloze - style training objective', 'has', 'model']]"
[]
[]
"[['dimensionality', 'has', '1024'], ['dimensionality', 'has', '256'], ['momentum', 'has', '0.99']]"
"[['learning rate', 'has', 'linearly warmed up']]"
[]
[]
[]
[]
[]
"[['much better', 'name', 'MRPC']]"
[]
"[['fine tuning', 'has', 'biggest gain']]"
[]
"[['scaling', 'has', 'bilm term']]"
[]
[]
"[['Seq2seq approach', 'has', 'stronger baseline']]"
[]
"[['L = 2 , H = 200 , and B = 5', 'has', 'adequate']]"
"[['subword split', 'has', 'input token unit']]"
"[['subword information', 'has', 'features']]"
"[['current top - notch methods', 'name', 'RNNG']]"
[]
[]
"[['sequence - to - sequence model', 'has', 'attention']]"
[]
[]
[]
[]
"[['embedding layer', 'has', '90K vocabulary']]"
[]
"[['single attention model', 'has', '88.3']]"
"[['large high - confidence corpus', 'has', 'single LSTM + A model'], ['outperforms', 'has', 'best single model']]"
[]
[]
[]
[]
[]
[]
"[['two state - of - the - art generative neural parsing models', 'name', 'Recurrent Neural Network Grammar generative parser ( RG )']]"
[]
[]
[]
"[['RG', 'has', 'decreases'], ['decreases', 'has', 'performance']]"
[]
[]
[]
"[['93.94 F1 .', 'has', '93.94 F1']]"
[]
[]
"[['PTB setting', 'has', 'ensembling']]"
[]
[]
[]
[]
[]
[]
[]
"[['bottom - up parser and the top - down parser', 'has', 'similar results'], ['in - order parser', 'has', 'outperforms']]"
[]
"[['fully - supervise setting', 'has', 'inorder parser']]"
[]
"[['our final model', 'has', 'best results']]"
[]
[]
[]
"[['very good results', 'has', '93.8 F 1']]"
[]
[]
"[['forget gate bias', 'has', 'one']]"
[]
"[['learning rate', 'has', '0.25 0.85 max']]"
[]
[]
"[['new state of the art', 'has', '93.8 F 1']]"
[]
[]
[]
[]
[]
[]
"[['RNNG composition function', 'name', 'novel gated attention mechanism'], ['novel gated attention mechanism', 'name', 'GA - RNNG']]"
"[['role', 'has', 'individual heads']]"
[]
"[['stack', 'has', 'worst'], ['worst', 'has', 'new results']]"
"[['language modeling', 'has', 'stack - only RNNG']]"
[]
[]
[]
[]
[]
[]
"[['attention - based tree output', 'has', 'high error rate ( ? 90 % )']]"
[]
[]
"[['test data', 'has', 'usual split'], ['usual split', 'has', 'GA - RNNG']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['mixed effect', 'has', 'improves'], ['improves', 'has', 'performance']]"
"[['8 of the 9 languages', 'has', 'test set result']]"
[]
[]
[]
"[['cluster - pair representations', 'has', 'our network']]"
"[['coreference clusters', 'has', 'incrementally']]"
[]
[]
"[['current state', 'name', 'partially completed coreference clustering']]"
[]
"[['cluster - ranking model', 'has', 'results'], ['results', 'has', 'further']]"
[]
[]
"[['neural architecture', 'name', 'policy network'], ['learning', 'has', 'span representation']]"
"[['sequence of linking actions', 'has', 'linking actions'], ['how good', 'has', 'generated coreference clusters']]"
[]
[]
[]
[]
"[['number of sampled trajectories N s', 'has', '100'], ['regularization parameter', 'has', '{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }']]"
[]
"[['model', 'has', 'ELMo']]"
[]
[]
[]
"[['Coreference resolution systems', 'has', 'Coreference resolution']]"
[]
[]
[]
[]
[]
"[['REINFORCE', 'has', 'slightly better']]"
[]
[]
[]
"[['each iteration', 'has', 'antecedent distribution']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['first feed - forward layer size', 'has', 'value']]"
"[['learning rate', 'has', '10 ? 4'], ['maximal batch size', 'has', '64']]"
[]
[]
[]
[]
"[['s@1 score', 'has', 'MR - LSTM'], ['MR - LSTM', 'has', 'outperforms'], ['outperforms', 'has', ""KZH13 's results""]]"
[]
"[['Performance', 'has', '68.10 s@1 score']]"
[]
"[['MR - LSTM', 'has', 'more successful']]"
"[['shell noun resolution', 'has', ""KZH13 's dataset""], ['range', 'has', '76.09-93.14']]"
[]
[]
"[['modeling', 'has', 'global information']]"
[]
[]
[]
[]
[]
"[['model', 'has', 'local classifier']]"
[]
"[['initial learning rate', 'has', 'AdaGrad']]"
[]
[]
"[['rate', 'has', '0.3']]"
[]
[]
[]
[]
"[['RNN', 'has', 'improves'], ['improves', 'has', 'performance']]"
"[['RNN performance', 'has', 'significantly better']]"
[]
[]
[]
[]
[]
[]
"[['context memory block', 'has', 'proposed model'], ['target sentences and context sentences', 'has', 'into consideration']]"
"[['LSTM modules', 'has', '200 output units']]"
[]
"[['initial learning rate', 'has', '0.001']]"
[]
[]
[]
"[['baseline model', 'has', '67.2 % F1 score']]"
"[['models', 'has', 'cross - sentence dependency']]"
"[['ASL model', 'has', 'better performance']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['first end - to - end coreference resolution model', 'has', 'significantly outperforms']]"
[]
[]
[]
[]
[]
[]
[]
"[['word embeddings', 'has', 'fixed concatenation']]"
[]
"[['character CNN', 'has', 'characters']]"
"[['convolutions', 'has', 'window sizes']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['our 5 - model ensemble', 'has', 'improves']]"
[]
[]
[]
[]
[]
"[['mention candidates', 'has', 'rule - based system']]"
"[['oracle mentions', 'has', 'improvement']]"
[]
[]
[]
[]
[]
"[['dropout', 'has', '0.3']]"
[]
[]
[]
[]
"[['BERT - base', 'has', 'improvement']]"
[]
"[['overlap variant', 'has', 'no improvement']]"
[]
[]
[]
[]
[]
[]
"[['Wiseman', 'has', 'standard encoder - decoder system']]"
"[['Li', 'has', 'standard encoder - decoder'], ['standard encoder - decoder', 'has', 'delayed copy mechanism'], ['delayed copy mechanism', 'has', 'text']]"
"[['Puduppully - plan', 'has', 'two steps'], ['two steps', 'has', 'first standard encoder - decoder']]"
[]
[]
"[['each decoding step', 'has', 'gated recurrent network'], ['gated recurrent network', 'has', 'computes'], ['computes', 'has', 'records']]"
[]
[]
[]
[]
"[['initial learning rate', 'has', '0.001']]"
[]
[]
[]
"[['lower results', 'has', 'Flat scenario']]"
[]
[]
"[['significant higher BLEU score (', 'has', '16.7 vs. 14.5 )']]"
[]
[]
[]
[]
[]
[]
[]
"[['decoder', 'has', 'all models'], ['all models', 'has', '4 - layer RNN decoder']]"
[]
[]
[]
"[['official E2E test set', 'has', 'our ensemble model'], ['baseline model', 'name', 'TGen']]"
[]
"[['outperforms', 'has', 'Laptop dataset']]"
[]
[]
[]
[]
[]
"[['model', 'has', 'GCN encoder']]"
"[['GCN model', 'has', 'more stable']]"
[]
[]
"[['neural models', 'has', 'upper bound results']]"
[]
[]
[]
[]
[]
"[['generated strings', 'has', 'selected'], ['behav', 'has', '5 out of 5 customer rating']]"
"[['canonical presentation', 'name', 'RSA framework'], ['reference resolution', 'has', 'referents']]"
[]
"[['pragmatic methods', 'has', 'improvements']]"
"[['SD 1', 'has', 'strong'], ['Coverage ratios', 'has', 'attribute type']]"
[]
[]
[]
[]
[]
[]
"[['initial learning rate', 'has', '0.15'], ['batch size', 'has', '5']]"
"[['truncation size', 'has', '100']]"
"[['beam size', 'has', '5']]"
[]
[]
[]
[]
[]
"[['best reported system', 'has', 'absolute improvement'], ['content selection precision', 'has', 'improves'], ['content ordering', 'has', 'increases']]"
[]
[]
"[['generating', 'has', 'fluent language']]"
"[['explicit , symbolic , text planning stage', 'has', 'output']]"
[]
"[['plan', 'has', 'neural generation system']]"
[]
"[['WebNLG challenge', 'has', 'Melbourne'], ['Melbourne', 'has', 'end - to - end system'], ['scored', 'has', 'best'], ['UPF - FORGe', 'has', 'classic grammar - based NLG system']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['two important features', 'has', 'state - of - art architecture']]"
[]
[]
[]
[]
"[['E2E +', 'has', 'TGen']]"
"[['E2E', 'has', 'outperforms'], ['outperforms', 'has', 'EDA_CS']]"
"[['EDA_CS TL', 'has', 'bleu increment']]"
"[['baseline model', 'name', 'EDA']]"
[]
[]
[]
[]
"[['jointly learning', 'has', 'POS tagging and dependency paring']]"
[]
[]
"[['character and POS tag embeddings', 'has', 'randomly initialized']]"
"[['dropout', 'has', '67 % keep probability']]"
[]
"[['initial learning rate', 'has', '0.001']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['two parsing architectures', 'name', 'transition - based']]"
[]
[]
[]
[]
"[['our parsers', 'has', 'very competitive']]"
"[['external embeddings', 'has', 'first - order graph - based parser'], ['2 features', 'has', 'outperforms'], ['outperforms', 'has', 'all other systems']]"
"[['greedy transition based parser', 'has', '4 features']]"
[]
"[['external word embeddings', 'has', 'accuracy']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['German', 'has', 'best published UAS scores']]"
[]
[]
[]
[]
"[['three BiLSTM - CRF - based models', 'name', 'Stanford - NNdep']]"
"[['traditional feature - based models', 'name', 'MarMoT']]"
[]
[]
[]
"[['number of BiLSTM layers', 'has', '2']]"
[]
[]
[]
"[['MarMoT', 'has', 'accuracy results']]"
"[['BiLSTM - CRF', 'has', 'accuracies']]"
"[['PTB', 'has', 'CNN - based character - level word embeddings']]"
"[['GENIA and CRAFT', 'has', 'BiLSTM - CRF']]"
[]
"[['GENIA', 'has', 'pre-trained models'], ['pre-trained models', 'has', 'BLLIP']]"
[]
[]
[]
[]
"[['STACKPTR', 'has', 'transition - based architecture'], ['transition - based architecture', 'has', 'asymptotic efficiency']]"
"[['STACKPTR parser', 'has', 'pointer network'], ['pointer network', 'has', 'backbone']]"
[]
[]
[]
"[['UAS and LAS', 'has', 'Full variation of STACKPTR']]"
"[['Full model', 'has', 'best accuracy']]"
"[['LCM and UCM', 'has', 'STACKPTR']]"
"[['results', 'has', 'our parser']]"
[]
[]
"[['German', 'has', 'performance'], ['performance', 'has', 'BIAF']]"
[]
[]
[]
"[['transition - based parsing', 'has', 'sentences']]"
[]
[]
[]
[]
"[['our neural network parser', 'has', 'significantly more']]"
[]
[]
[]
"[['our model', 'has', 'competitive']]"
[]
"[['network', 'has', 'larger']]"
"[['optimize', 'has', 'Adam']]"
"[['Our model', 'has', 'nearly the same UAS performance']]"
[]
[]
[]
"[['parser states', 'has', 'drawn']]"
[]
[]
[]
[]
"[['Chinese score', 'has', 'state - of - the - art']]"
[]
[]
[]
[]
"[['label bias problem', 'has', 'locally normalized models']]"
[]
"[['gradients', 'has', 'approximate global normalization']]"
[]
[]
[]
[]
[]
"[['beam search', 'has', 'locally normalized model']]"
"[['character ngrams feature', 'has', 'very important']]"
[]
[]
[]
[]
[]
[]
"[['achieving', 'has', 'performance']]"
[]
[]
[]
[]
"[['accuracy', 'has', 'slightly'], ['trigrams', 'has', 'performance']]"
[]
[]
[]
"[['Tagspace', 'has', 'tag prediction model']]"
[]
"[['Both models', 'has', 'similar performance']]"
[]
[]
[]
[]
[]
[]
"[['Fine-tune ( Ft )', 'has', 'Fine - tuning'], ['Fine - tuning', 'has', 'pre-trained model']]"
[]
[]
"[['Self - training', 'has', 'UDA model ( UDA + Self )']]"
[]
[]
[]
"[['UDA algorithm and MLM pre-training', 'has', 'significant improvements']]"
"[['sentiment classification task', 'has', 'unlabeled data size'], ['unlabeled data size', 'has', 'larger']]"
"[['MLM method', 'has', 'relatively more resource intensive']]"
"[['MLdoc dataset', 'has', 'size'], ['UDA method', 'has', 'more helpful']]"
"[['sentiment classification task', 'has', 'self - training technique'], ['self - training technique', 'has', 'consistently improves']]"
[]
"[['MLdoc dataset', 'has', 'self - training']]"
[]
[]
[]
[]
[]
"[['Neural Attentive Bagof - Entities ( NABoE ) model', 'has', 'neural network model']]"
"[['each entity name', 'has', 'document']]"
[]
[]
"[['mini-batch size', 'has', '32']]"
[]
[]
[]
[]
[]
[]
"[['NTEE', 'has', 'state - of - the - art model']]"
[]
"[['baselines', 'has', 'our models']]"
[]
[]
[]
[]
"[[""words ' contextual information and task information"", 'has', 'inherently jointed']]"
[]
[]
"[['weighting scheme', 'has', 'TFIDF']]"
"[['Word2 Vec method', 'has', 'neural network language method']]"
[]
[]
[]
"[['Our method', 'has', 'better performance']]"
[]
[]
[]
[]
"[['Graph Convolutional Network ( GCN )', 'has', 'simple and effective graph neural network']]"
[]
[]
[]
[]
[]
"[['TF - IDF + LR', 'has', 'bag - of - words model']]"
[]
"[['CNN', 'has', 'Convolutional Neural Network']]"
[]
[]
"[['Bi- LSTM', 'has', 'bi-directional LSTM']]"
"[['PV - DBOW', 'has', 'paragraph vector model'], ['paragraph vector model', 'has', 'orders'], ['orders', 'has', 'orders of words']]"
[]
"[['PV - DM', 'has', 'paragraph vector model']]"
[]
"[['PTE', 'has', 'predictive text embedding']]"
[]
"[['SWEM', 'has', 'simple word embedding models']]"
"[['LEAM', 'has', 'label - embedding attentive models']]"
[]
"[['Graph - CNN - C', 'has', 'graph CNN model']]"
[]
"[['Graph - CNN - F', 'has', 'Graph - CNN - C']]"
"[['window size', 'has', '20']]"
"[['learning rate', 'has', '0.02']]"
[]
"[['significantly outperforms', 'has', 'all baseline models']]"
"[['pre-trained Glo Ve word embeddings', 'has', 'CNN']]"
[]
"[['PV - DBOW', 'has', 'comparable results']]"
[]
"[['Graph - CNN models', 'has', 'competitive performances']]"
[]
[]
"[['discrete text', 'has', 'continuous representation']]"
"[['network depth', 'has', 'meta-parameter']]"
"[['computational complexity', 'has', 'network']]"
[]
"[['generalizes', 'has', 'commonly used word embedding'], ['text regions', 'has', 'covering'], ['covering', 'has', 'one or more words']]"
[]
"[['final pooling layer', 'has', 'aggregates'], ['aggregates', 'has', 'internal data']]"
[]
[]
[]
"[['all the five datasets', 'has', 'DPCNN']]"
[]
[]
"[['Zhang', 'has', 'best linear model']]"
[]
[]
"[['general framework', 'name', 'region embedding + pooling']]"
[]
[]
"[['elimination', 'has', 'word embedding layer']]"
[]
[]
"[['mini-batch size', 'has', '50 or 100'], ['optionally', 'has', 'rmsprop']]"
[]
[]
"[['three out of the four datasets', 'has', 'oh - 2 LSTMp']]"
"[['RCV1', 'has', 'n-gram SVM'], ['RCV1', 'has', 'bow - CNN']]"
"[['one - hot CNN', 'has', 'surprising well']]"
[]
[]
[]
[]
"[['our tasks', 'has', 'wv - 2 LSTMp']]"
"[['performance', 'has', 'one - hot CNN']]"
[]
[]
[]
[]
[]
[]
"[['text classification', 'has', 'input']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['test performance', 'has', 'Elec and RCV1 datasets']]"
"[['improved', 'has', 'test performance']]"
"[['bidirectional LSTM', 'has', 'improves'], ['improves', 'has', 'results']]"
[]
"[['Adversarial training', 'has', 'improve']]"
[]
[]
[]
[]
[]
[]
[]
"[['Theano', 'has', 'python library']]"
[]
[]
"[['TREC', 'has', 'number of filters'], ['number of filters', 'has', '300'], ['memory dimension', 'has', '300']]"
"[['word vector layer and the LSTM layer', 'has', 'probability']]"
"[['L2 regularization', 'has', 'factor']]"
[]
[]
[]
[]
[]
"[['consistently outperforms', 'has', 'all published neural baseline models']]"
[]
[]
[]
"[['multiple convolutional layers', 'has', 'in parallel']]"
[]
[]
[]
"[['dictionary', 'has', 'following characters'], ['following characters', 'has', 'abcdefghijklmnopqrstuvwxyz0123456']]"
"[['larger text', 'has', 'truncated']]"
"[['character embedding', 'has', 'size'], ['size', 'has', '16']]"
"[['initial learning rate', 'has', '0.01'], ['momentum', 'has', '0.9']]"
"[['Torch', 'has', 'Torch 7']]"
[]
[]
"[['deep architecture', 'has', 'well']]"
"[['smallest depth', 'has', 'our model']]"
[]
"[['small depth', 'has', 'temporal max - pooling']]"
[]
[]
[]
[]
"[['each dataset', 'has', 'bag - of - words model']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['dataset', 'has', 'of size']]"
"[['Conv Nets', 'has', 'well']]"
"[['Choice of alphabet', 'has', 'difference']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['mini-batch size', 'has', '10'], ['default value', 'has', '1.0']]"
[]
[]
[]
[]
[]
[]
"[['Subj and MR datasets', 'has', 'BLSTM - 2DCNN'], ['BLSTM - 2DCNN', 'has', 'second higher accuracies']]"
"[['RCNN', 'has', 'BLSTM - 2DCNN']]"
[]
"[['DSCNN', 'has', 'BLSTM - 2DCNN'], ['BLSTM - 2DCNN', 'has', 'outperforms'], ['outperforms', 'has', 'five datasets']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['Reuters', 'has', 'SVM']]"
"[['AAPD', 'has', 'SVM']]"
"[['SVM', 'has', 'LR baseline'], ['single - label datasets', 'has', 'IMDB and Yelp 2014']]"
[]
[]
[]
[]
"[['Transformer', 'has', 'close']]"
[]
"[['our models', 'has', 'outperform'], ['outperform', 'has', 'Watson']]"
[]
[]
"[['deep learning architectures', 'has', 'Transformer and m LSTM']]"
"[['Our models', 'has', 'lower F 1 scores']]"
[]
[]
[]
"[['Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )', 'has', 'text classification model']]"
[]
[]
"[['size batch', 'has', '64']]"
[]
"[['NVIDIA GTX 1060 GPU', 'has', 'Intel Core i 7 4770s CPU']]"
"[['network reduction', 'has', 'GAP']]"
"[['dataset', 'has', 'four target classes']]"
"[['Char - CNN', 'has', 'proposed model']]"
[]
[]
[]
[]
[]
"[['jointly embedding', 'has', 'word and label']]"
"[['label embedding framework', 'has', 'Label - attentive text representation'], ['Label - attentive text representation', 'has', 'informative']]"
[]
[]
[]
[]
"[['initial learning rate', 'has', '0.001'], ['minibatch size', 'has', '100']]"
[]
[]
[]
[]
[]
"[['LEAM', 'has', 'best AUC score']]"
[]
[]
[]
[]
[]
"[['HDLTex', 'name', 'deep learning architectures']]"
[]
"[['processing', 'has', 'Xeon E5 ? 2640 ( 2.6 GHz )'], ['GPU cards', 'has', 'N vidia Quadro K620']]"
[]
[]
[]
[]
[]
[]
"[['nave Bayes', 'has', 'much worse']]"
[]
"[['data set W OS ? 11967', 'has', 'best accuracy']]"
"[['86 %', 'has', 'over all']]"
"[['data set W OS ? 46985', 'has', 'best scores']]"
[]
[]
"[['EXplicit interAction Model', 'name', 'EXAM']]"
"[['three main components', 'name', 'word - level encoder']]"
[]
[]
"[['last layer', 'has', 'aggregates'], ['aggregates', 'has', 'matching scores']]"
[]
[]
[]
"[['region size', 'has', '7'], ['embedding size', 'has', '128']]"
"[['initial learning rate', 'has', '0.0001'], ['batch size', 'has', '16']]"
"[['2 times', 'has', '2 times interaction feature length']]"
[]
[]
"[['baselines', 'has', 'three variants']]"
[]
[]
[]
"[['Char - based models', 'has', 'highest over all scores']]"
"[['Word - based baselines', 'has', 'other variants']]"
"[['five baselines', 'has', 'W.C Region Emb'], ['W.C Region Emb', 'has', 'best']]"
"[['three datasets', 'name', 'AG']]"
"[['Yah.A.', 'has', 'EXAM']]"
[]
[]
"[['embedding size', 'has', '256']]"
"[['each GRU Cell', 'has', '1,024 hidden states']]"
"[['accumulated MLP', 'has', '60 hidden units']]"
"[['batch size', 'has', '1000'], ['initial learning rate', 'has', '0.001']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['scores', 'has', 'best']]"
"[['transfer accuracies', 'has', 'quite low']]"
[]
[]
[]
[]
[]
[]
"[['novel model', 'name', 'Disconnected Recurrent Neural Network ( DRNN )']]"
[]
[]
[]
[]
[]
[]
"[['batch size', 'has', '128'], ['our proposed model', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all the other models']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['size', 'has', '50'], ['size', 'has', '25']]"
[]
[]
[]
[]
[]
"[['capability', 'has', 'capsule network']]"
"[['capsule networks', 'has', 'substantial and significant improvement']]"
[]
"[['capsule network', 'has', 'much stronger transferring capability']]"
"[['Reuters - Full', 'has', 'capsule network'], ['capsule network', 'has', 'robust superiority']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['window size', 'has', '20']]"
[]
[]
[]
"[['validation accuracy', 'has', 'does not increase']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['four of the five datasets', 'name', 'MSNBC']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['ELMo representations', 'has', 'deep']]"
[]
"[['intrinsic evaluations', 'has', 'higher - level LSTM states']]"
[]
"[['ELMo', 'has', 'baseline model'], ['baseline model', 'has', 'test set F 1']]"
"[['significantly larger', 'has', '1.8 % improvement']]"
[]
[]
[]
[]
[]
[]
[]
"[['two different methods', 'has', 'greatly reduce'], ['greatly reduce', 'has', 'size']]"
[]
[]
[]
[]
"[['model', 'name', 'bert - largecased']]"
"[['Transformer encoder layers', 'has', 'same parameters'], ['same parameters', 'has', '"" base "" model']]"
[]
"[['systematically', 'has', 'state of the art']]"
"[['scores', 'has', 'state of the art']]"
[]
[]
[]
[]
[]
[]
"[['novel model GAS', 'has', 'gloss - augmented WSD neural network']]"
[]
[]
[]
[]
[]
[]
"[['gloss expansion depth K', 'has', 'value']]"
"[['number of passes | T M |', 'has', '1 to 5']]"
[]
"[['drop rate', 'has', '0.5']]"
"[['validation loss', 'has', ""does n't improve""]]"
[]
"[['Babelfy', 'has', 'semantic network structure']]"
[]
[]
"[['IMS +emb', 'has', 'IMS']]"
[]
[]
[]
[]
[]
"[['best results', 'has', '70.6']]"
[]
[]
[]
"[['increasing number of passes', 'has', 'F1 - score'], ['F1 - score', 'has', 'increases']]"
"[['number of passes', 'has', '3']]"
[]
[]
[]
[]
[]
[]
[]
"[['Our proposed model', 'has', 'top score']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['proposed method', 'name', 'WSD - TM']]"
"[['performance', 'has', 'proposed model'], ['best supervised system', 'name', 'Melamud16 ( 69.4 )']]"
"[['all previous knowledgebased systems', 'has', 'overall parts of speech']]"
[]
[]
[]
"[['entity mention detection and entity disambiguation', 'has', 'jointly'], ['whole process', 'has', 'end - to - end differentiable']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['features', 'has', 'cover'], ['cover', 'has', 'frequency']]"
"[['VCG model', 'has', 'overall F- score result']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['results', 'has', 'top - performing and low - performing supervised algorithms']]"
[]
"[['reverse', 'has', 'sequential follow'], ['sequential follow', 'has', 'information']]"
[]
[]
[]
[]
[]
"[['layer of word embeddings', 'has', 'pre-trained']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['CWEs', 'has', 'utilized directly']]"
"[['semantic capabilities', 'has', 'CWEs']]"
[]
[]
"[['BERT', 'has', 'outperforms'], ['outperforms', 'has', 'all others']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['ELMo embedding space', 'has', 'major senses'], ['major senses', 'has', 'slightly more separated']]"
[]
"[['Neural Text - Entity Encoder ( NTEE )', 'has', 'neural network model']]"
"[['every text', 'has', 'KB']]"
[]
[]
[]
[]
"[['placing', 'has', 'texts and entities']]"
[]
"[['BOW', 'has', 'conventional approach']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['supervised systems', 'has', 'overall WordNet vocabulary']]"
"[['coverage improvement', 'has', 'holds true']]"
"[['difference of scores', 'has', 'our system'], ['overall', 'has', 'not significant']]"
[]
"[['WordNet Gloss Tagged', 'has', 'training data']]"
[]
"[['ensembling', 'has', 'very efficient method']]"
"[['ensembles', 'has', 'scores'], ['scores', 'has', 'significantly higher']]"
[]
[]
[]
"[['novel model GAS', 'has', 'gloss - augmented WSD neural network']]"
[]
[]
[]
[]
[]
"[['gloss expansion depth K', 'has', 'value']]"
"[['number of passes | T M |', 'has', '1 to 5']]"
[]
"[['drop rate', 'has', '0.5']]"
"[['validation loss', 'has', ""does n't improve""]]"
[]
[]
"[['best results', 'has', '70.6']]"
"[['other three neural - based methods', 'has', 'our best model']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['Unified WSD', 'has', 'highest F 1 score']]"
[]
"[['IMS + Word2 Vec ( T: SemCor )', 'has', 'SVM - based classifier']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['SemCor ( or MASC ) trained classifier', 'has', 'NOAD trained classifier']]"
[]
[]
[]
"[['LP', 'has', 'clear benefits']]"
"[['significant improvements', 'has', '6.3 % increase']]"
[]
"[['LP', 'has', 'substantially improves'], ['substantially improves', 'has', 'classifier F1']]"
[]
[]
"[['F1 scores', 'has', 'relatively stable'], ['percentile', 'has', 'ranges']]"
[]
[]
[]
[]
[]
"[['entity embedding size', 'has', 'd = 300']]"
[]
[]
[]
"[[""RELIC 's"", 'has', 'entity linking performance'], ['entity linking performance', 'has', 'boosted even higher'], ['adoption of', 'has', 'commonly used entity linking features']]"
[]
[]
"[['RELIC', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'prior results']]"
"[['TypeNet', 'has', 'aggregate mention - level types'], ['aggregate mention - level types', 'has', 'aggregate'], ['aggregate', 'has', 'mention - level types']]"
[]
[]
[]
"[['mask rate', 'has', '10 %'], ['10 %', 'has', 'RELIC']]"
[]
"[[""RELIC 's performance"", 'has', 'upper bound']]"
[]
[]
"[['ORQA', 'has', 'BERT based reading comprehension model']]"
"[['significant', 'has', 'RELIC'], ['RELIC', 'has', 'outperforms'], ['outperforms', 'has', 'reading comprehension baseline']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['proposed embedding', 'name', 'textual context similarity']]"
"[['NED method', 'has', 'contexts']]"
[]
"[['candidate generation method', 'has', 'considerably affected'], ['considerably affected', 'has', 'performance']]"
[]
[]
[]
[]
[]
"[['"" in - the -wild "" feature - based texture model', 'has', 'fitting strategy'], ['fitting strategy', 'has', 'simplified']]"
[]
"[['Classic model', 'has', 'struggles']]"
"[['texture - free Linear model', 'has', 'better'], ['better', 'has', 'ITW model']]"
[]
[]
"[['ITW', 'has', 'slightly outperforms'], ['slightly outperforms', 'has', 'IMM']]"
[]
[]
[]
[]
"[['training', 'has', 'random augmentation']]"
[]
[]
"[['VRN - Guided', 'has', 'landmark detection module']]"
"[['Volumetric Regression Networks', 'has', 'largely outperform']]"
"[['3DDFA and EOS', 'has', 'all datasets']]"
[]
[]
[]
[]
"[['performance', 'has', 'our method'], ['our method', 'has', 'decreases'], ['pose', 'has', 'increases']]"
[]
"[['training set', 'has', 'performance variation'], ['performance variation', 'has', 'quite minor']]"
[]
[]
[]
[]
"[['novel loss function', 'has', 'Wing loss']]"
"[['data augmentation strategy', 'name', 'pose - based data balancing']]"
[]
[]
[]
[]
"[['weight decay', 'has', '5 10 ? 4'], ['momentum', 'has', '0.9'], ['batch size', 'has', '8']]"
[]
"[['downsize', 'has', 'feature maps']]"
[]
"[['proposed PDB strategy', 'has', 'number of bins K'], ['number of bins K', 'has', '17']]"
"[['CNN - 6', 'has', 'input image size'], ['input image size', 'has', '64 64 3'], ['learning rate', 'has', '3 10 ? 6']]"
"[['parameters', 'has', 'Wing loss'], ['CNN - 7', 'has', 'input image size'], ['input image size', 'has', '128 128 3'], ['learning rate', 'has', '1 10 ? 6 to 1 10 ?8']]"
[]
[]
[]
"[['probability', 'has', '50 %']]"
[]
[]
[]
[]
[]
"[['two - stage landmark localisation framework', 'has', 'PDB strategy']]"
[]
[]
"[['training', 'has', 'regression network']]"
[]
"[['invariance', 'has', 'loss']]"
"[['three novel losses', 'name', 'batch distribution loss']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['contour constraint', 'has', 'contour']]"
[]
"[['3D face model fitting approach', 'has', 'inherent advantage']]"
[]
[]
"[['initial global learning rate', 'has', '1 e ? 3']]"
"[['minibatch size', 'has', '32'], ['weight decay', 'has', '0.005']]"
"[['AFLW - LFPA', 'has', 'our method']]"
"[['AFLW2000 - 3D', 'has', 'our method'], ['our method', 'has', 'large improvement']]"
[]
"[['IJB - A dataset', 'has', 'our method']]"
[]
"[['Our method', 'has', 'second best method']]"
"[['performance', 'has', 'our method']]"
[]
"[['AFLW - PIFA dataset', 'has', 'our method']]"
"[['datasets', 'has', 'second and third stages']]"
"[['LFC + SPC and LFC + CFC performances', 'has', 'CFC'], ['CFC', 'has', 'more helpful']]"
"[['all constraints', 'has', 'best performance']]"
"[['5 % and 15 %', 'has', 'SPC'], ['SPC', 'has', 'helpful']]"
[]
[]
[]
[]
"[['fitting algorithm', 'has', 'nonlinear 3 DMM']]"
[]
[]
[]
[]
[]
"[['significantly improved', 'has', 'shape and texture representation power']]"
[]
"[['losses', 'has', 'similar magnitudes']]"
[]
[]
[]
"[['nonlinear model', 'has', 'significantly lower L 1 reconstruction error']]"
"[['nonlinear model', 'has', 'significantly smaller reconstruction error']]"
[]
[]
[]
"[['Garrido et al.', 'has', 'offline optimization method']]"
"[['global image - based discriminator', 'has', 'redundant']]"
[]
[]
[]
[]
[]
[]
[]
"[['two architectures', 'name', 'AlexNet and VGG - 16']]"
"[['VGG - 16 architecture', 'has', 'first 4 layers'], ['first 4 layers', 'has', 'frozen']]"
"[['attaching', 'has', 'additional layers']]"
[]
[]
[]
[]
"[['fitting process', 'has', '3 DDFA']]"
"[['specifically designed feature', 'name', 'Projected Normalized Coordinate Code ( PNCC )']]"
"[['Weighted Parameter Distance Cost ( WPDC )', 'has', 'cost function']]"
[]
"[['training', 'has', '3DDFA']]"
[]
[]
[]
[]
"[['testing error', 'has', 'reduced']]"
"[['generic cascade process', 'has', 'training and testing errors'], ['training and testing errors', 'has', 'fast']]"
"[['initialization regeneration', 'has', 'training error'], ['training error', 'has', 'updated'], ['beginning of', 'has', 'each iteration'], ['testing error', 'has', 'descend']]"
[]
[]
"[['adaptively optimizes', 'has', 'parameter weights']]"
[]
[]
"[['3DDFA', 'has', 'state of the art'], ['state of the art', 'has', 'all the 2D methods']]"
[]
"[['performance', 'has', '3DDFA']]"
[]
"[['all the methods', 'has', 'performance']]"
[]
[]
[]
[]
"[['novel deep learning framework', 'name', 'Multi - Center Learning ( MCL )']]"
[]
"[['focused', 'has', 'firstly']]"
"[['model complexity', 'has', 'model assembling method']]"
[]
"[['crop', 'has', 'face']]"
[]
[]
"[['input face patch', 'has', '50 50 grayscale image'], ['multiplying', 'has', '0.0078125']]"
[]
"[['type of solver', 'has', 'SGD']]"
[]
"[['learning rate', 'has', 'factor']]"
"[['state - of - the - art methods', 'name', 'ESR']]"
[]
[]
"[['MCL', 'has', 'superior capability']]"
[]
[]
[]
[]
"[['Global Average Pooling', 'has', 'more advantageous']]"
[]
"[['0.4', 'has', 'WM']]"
[]
"[['WM', 'has', 'left eye model and the right eye model']]"
[]
[]
[]
[]
"[['Weighting Simplified AM', 'has', 'improves slightly']]"
"[['AM', 'has', 'higher accuracy']]"
[]
[]
[]
"[['WM and AM', 'has', 'increase']]"
"[['results', 'has', 'landmarks']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['existing facial structure', 'has', 'faces'], ['further train', 'has', 'facial landmark detectors']]"
"[['training', 'has', 'all images']]"
"[['downsampling', 'has', 'input feature maps'], ['batch normalization', 'has', 'removed']]"
"[['training', 'has', 'disentangling step'], ['initial learning rate', 'has', '0.01']]"
[]
"[['detector architecture', 'has', 'simple baseline network']]"
[]
[]
"[['stronger baseline', 'has', 'our model'], ['outperforms', 'has', 'state - of the - art entries']]"
"[['strong baselines', 'has', 'our method']]"
"[['additional "" style- augmented "" synthetic training samples', 'has', 'our model'], ['simple backbone', 'has', 'outperforms'], ['outperforms', 'has', 'previous state - of - the - art methods']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['improvement', 'has', 'even larger'], ['number of training images', 'has', 'quite small']]"
[]
"[['number of augmented styles', 'has', 'model']]"
[]
[]
"[['novel face alignment method', 'name', 'Deep Alignment Network ( DAN )']]"
"[['multistage neural network', 'has', 'each stage']]"
[]
"[['each stage', 'has', 'landmark heatmap']]"
"[['data augmentation', 'has', 'total of 10 images']]"
[]
"[['mini batch size', 'has', '64']]"
[]
[]
"[['failure rate reduction', 'has', '60 %']]"
"[['failure rate reduction', 'has', '72 %']]"
[]
[]
[]
[]
[]
"[['attention maps', 'has', 'refined']]"
[]
[]
[]
[]
"[['learning rate', 'has', 'learning rate annealing'], ['learning rate annealing', 'has', 'annealing']]"
"[['batch size', 'has', 'batch size 8']]"
"[['accuracy', 'has', 'steadily increases'], ['saturates', 'has', 'third']]"
[]
[]
[]
[]
[]
[]
"[['DeCaFA', 'has', 'new state - of - the - art']]"
[]
[]
[]
[]
"[['bottom - up and top - down CNN structures', 'name', 'stacked Hourglass ( HG )']]"
"[['our model', 'has', 'full coordinate information']]"
[]
[]
"[['momentum', 'has', '0'], ['weight decay', 'has', '1 10 ?5']]"
"[['learning rate', 'has', '1 10 ?5 and 1 10 ? 6']]"
"[['rescaling', 'has', '15 % )']]"
[]
[]
[]
"[['challenge subset ( iBug dataset )', 'has', 'outperform']]"
[]
"[['300 W private test dataset', 'has', 'previous state - of - theart'], ['variant metrics', 'name', 'NME']]"
[]
[]
[]
"[['our approach', 'has', 'fails']]"
[]
[]
[]
"[['discriminative landmarks features', 'name', 'Landmarks - Attention Network ( LAN )']]"
[]
[]
"[['location patch size', 'has', '57 57']]"
"[['CNN structure', 'has', 'Rectified Linear Unit ( ReLU )'], ['optimizer', 'has', 'Adadelta ( Zeiler 2012 ) approach'], ['learning rate', 'has', '0.1']]"
[]
"[['NME results', 'has', 'SIR']]"
"[['more challenging IBUG subset', 'has', 'our method']]"
"[['SIR method', 'has', 'state - of - the - art methods']]"
[]
[]
[]
[]
[]
"[['boundary - aware face alignment algorithm', 'has', 'two stages']]"
"[['regress', 'has', 'landmarks']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['boundary map ( "" BM "" )', 'has', 'most effective one']]"
[]
[]
"[['performance', 'has', 'improved consistently']]"
[]
"[['comparison', 'has', 'effectiveness'], ['effectiveness', 'has', 'using hourglass structure design']]"
[]
[]
[]
"[['3 DDE ( 3D Deeply - initialized Ensemble ) regressor', 'has', 'robust and efficient face alignment algorithm']]"
"[['robustly fitting', 'has', '3 D face model']]"
[]
[]
[]
"[['early stopping', 'has', 'early stopping and better data augmentation techniques']]"
[]
"[['until convergence', 'has', 'convergence']]"
[]
[]
"[['All layers', 'has', '68 filters']]"
"[['initialization', 'has', 'initialization , g 0']]"
[]
[]
[]
[]
[]
[]
"[['set', 'name', 'SA']]"
[]
[]
[]
"[['improve', 'has', 'large margin'], ['large margin', 'has', 'other ERT methods'], ['other ERT methods', 'name', 'RCPR']]"
"[['RCN', 'has', 'CNN architecture']]"
[]
[]
"[['challenging subset', 'has', '300W public competition'], ['300W public competition', 'has', 'SHN']]"
"[['cascaded ERT', 'has', '3D initialization'], ['3D initialization', 'has', 'key']]"
[]
[]
[]
[]
[]
[]
"[['end - to - end method', 'name', 'Position map Regression Network ( PRN )']]"
"[['UV position map', 'has', '2D image']]"
"[['weighted loss', 'has', 'focuses']]"
"[['decays', 'has', 'half']]"
"[['batch size', 'has', '16']]"
[]
[]
"[['our result', 'has', 'slightly outperforms'], ['slightly outperforms', 'has', 'state - of - the - art method 3D - FAN']]"
"[['depth value', 'has', 'performance discrepancy'], ['our method and 3D - FAN', 'has', 'increases']]"
[]
"[['AFLW2000 - 3 D dataset', 'has', 'our predictions'], ['our predictions', 'has', 'more accurate']]"
"[['our method', 'has', 'best methods']]"
[]
"[['weights', 'has', 'specific regions'], ['weight ratio', 'has', 'considerable improvement']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['3 DMM - based methods', 'name', '3 DDFA and DeFA'], ['our method', 'has', 'outperforms'], ['outperforms', 'has', 'large margin']]"
[]
[]
"[['large poses', 'has', '2 DASL']]"
[]
[]
[]
"[['self - critic learning', 'has', 'NME'], ['NME', 'has', 'increases']]"
"[['self - supervision scheme', 'has', 'NME']]"
"[['FLMs', 'has', 'input']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['training', 'has', 'roughly converged model'], ['initial learning rate', 'has', '2.5 10 ?4']]"
"[['Semantic Alignment', 'has', 'beginning'], ['initial learning rate', 'has', '2.5 10 ? 6']]"
"[['batch size', 'has', '10']]"
[]
[]
"[['our Semantic Alignment ( HGs + SA )', 'has', 'greatly outperform'], ['greatly outperform', 'has', 'hourglass ( HGs ) only']]"
"[['GHCU', 'has', 'HGs + SA + GHCU']]"
[]
"[['state of the art method', 'has', '6.38 % ( HGs + SA +GHCU + Norm )']]"
"[['AFLW . 300W', 'has', '68 facial points']]"
"[['HGs + SA', 'has', 'outperforms']]"
"[['HGs', 'has', '1.62 % vs 1.95 %']]"
[]
[]
"[['HGs + SA', 'has', 'greatly outperforms'], ['greatly outperforms', 'has', 'HGs']]"
"[['HGs + SA', 'has', 'HGs + SA + GHCU']]"
[]
[]
"[['GHCU', 'has', 'more effective']]"
[]
[]
"[['high performance face detector', 'name', 'AInnoFace']]"
[]
"[['backbone network', 'has', 'proposed AInnoFace detector']]"
[]
"[['batch size', 'has', '32']]"
"[['gradually ramp up', 'has', 'learning rate']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['recurrently passing', 'has', 'network']]"
[]
"[['proposed iterative architecture', 'has', 'network']]"
[]
[]
"[['different activation functions', 'name', 'ReLU']]"
[]
"[['results', 'has', '138 times lighter']]"
"[['SOTA face detectors', 'name', 'Pyra - midBox']]"
[]
[]
"[['our SSD - based variations', 'has', 'lower mAP results']]"
[]
[]
"[['all the different channel width', 'has', 'FPN based architecture']]"
"[['SSD based architecture', 'has', 'PReLU']]"
"[['novel face detection network', 'has', 'three novel contributions']]"
[]
[]
[]
[]
[]
"[['proposed network', 'name', 'Dual Shot Face Detector ( DSFD )']]"
[]
[]
[]
"[['batch size', 'has', '16']]"
"[['learning rate', 'has', '10 ?3']]"
"[['inference', 'has', ""first shot 's outputs""], [""first shot 's outputs"", 'has', 'ignored']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['additional annotations', 'has', 'unlabeled faces'], ['outperform', 'has', 'all other methods']]"
[]
"[['unified deep neural network', 'name', 'multi -scale CNN ( MS - CNN )']]"
[]
"[['unified multi-scale deep CNN', 'name', 'multi -scale CNN ( MS - CNN )']]"
"[['two sub-networks', 'name', 'object proposal network']]"
[]
[]
[]
"[['performance', 'has', 'MS - CNN detector']]"
[]
[]
[]
"[['MS - CNN', 'has', 'new record']]"
[]
"[['Pedestrian detection', 'has', 'Caltech']]"
"[['MS - CNN', 'has', 'state - of the - art performance']]"
[]
[]
[]
"[['training phase', 'has', 'WSMA - Seg']]"
"[['testing phase', 'has', 'resulting heatmaps'], ['contour tracing operation', 'has', 'conducted']]"
[]
"[['our proposed method', 'has', 'Stack = 2 , Base = 40 , Depth = 5'], ['Stack = 2 , Base = 40 , Depth = 5', 'has', 'best performance']]"
"[['state - of - the - art baselines', 'has', 'WSMA - Seg'], ['WSMA - Seg', 'has', 'much simpler']]"
[]
[]
[]
[]
[]
"[['other metrics', 'has', 'performance']]"
[]
[]
[]
[]
"[['novel pixel - wise face localisation method', 'name', 'Reti- naFace']]"
[]
"[['SGD optimiser', 'has', 'momentum'], ['weight decay', 'has', '0.0005'], ['batch size', 'has', '8 4']]"
[]
[]
[]
"[['WIDER FACE', 'has', 'standard practices']]"
"[['IoU threshold', 'has', '0.4']]"
[]
"[['slightly deteriorates', 'has', 'results']]"
"[['landmark and dense regression', 'has', 'jointly']]"
[]
"[['RetinaFace', 'has', 'best AP']]"
"[['recent best performed method', 'has', 'Reti - na Face'], ['new impressive record (', 'has', '91.4 % v.s. 90.3 % )']]"
[]
[]
[]
"[['MTCNN', 'has', 'RetinaFace']]"
[]
[]
[]
"[['single - stage features', 'name', 'RetinaFace'], ['employing', 'has', '( Region of Interest ) RoI features']]"
[]
"[['CFP - FP', 'has', 'Reti - na Face']]"
"[['WIDER FACE', 'has', 'Face Detection']]"
[]
[]
[]
"[['multiple attributes', 'name', 'occlusion']]"
[]
[]
[]
"[['easy set', 'has', 'average precision ( AP )']]"
"[['performance', 'has', 'drops'], ['drops', 'has', '10 %']]"
"[['hard set', 'has', 'even more challenging']]"
[]
"[['results', 'has', 'small scale'], ['small scale', 'has', 'abysmal']]"
[]
"[['partial occlusion', 'has', 'performance'], ['performance', 'has', 'drops'], ['drops', 'has', 'significantly']]"
"[['maximum AP', 'has', 'only 26.5 %']]"
[]
"[['recall', 'has', '20 %']]"
[]
[]
[]
"[['novel face detector', 'name', 'FaceBoxes']]"
"[['proposed method', 'has', 'lightweight yet powerful network structure']]"
"[['enriching', 'has', 'receptive fields']]"
"[['different types of anchors', 'has', 'same density']]"
[]
[]
[]
[]
[]
"[['RDCL', 'has', 'efficient and accuracy - preserving']]"
[]
[]
[]
[]
[]
"[['outperforms', 'has', 'all others']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['recently published deep learning - based face detection methods', 'name', 'DP2MFD']]"
[]
[]
"[['consistently accurate', 'has', 'overall pose angles']]"
[]
[]
[]
[]
[]
[]
[]
"[['LFWA dataset', 'has', 'our method']]"
[]
[]
[]
[]
"[['series of anchors', 'name', 'PyramidAnchors']]"
[]
[]
[]
"[['training strategy', 'name', 'Data - anchor - sampling']]"
[]
[]
[]
"[['Our PyramidBox', 'has', 'outperforms'], ['outperforms', 'has', 'others']]"
[]
[]
[]
[]
[]
[]
"[['CMS - RCNN method', 'name', 'Multi - Scale Region Proposal Network ( MS - RPN )']]"
[]
"[['first 5 sets of convolution layers', 'has', 'same architecture'], ['training', 'has', 'parameters']]"
[]
[]
"[[""' conv3 ' , ' conv4 ' , and ' conv5 '"", 'has', 'normalized']]"
"[['training convergence', 'has', 'initial re-weighting scale'], ['initial re-weighting scale', 'has', 'carefully set']]"
[]
"[['CMS - CNN', 'has', 'RoI pooling layer']]"
[]
[]
"[['11 convolution layer', 'has', 'employed']]"
[]
[]
[]
[]
[]
[]
"[['detailed design Faster RCNN method', 'name', 'FDNet1.0']]"
"[['deformable layer', 'has', 'fewer channels']]"
[]
[]
[]
"[['Mini batch size', 'has', '1']]"
[]
[]
[]
"[['initial learning rate', 'has', '1e - 3']]"
[]
[]
[]
[]
"[['novel face detection framework', 'name', 'Selective Refinement Network ( SRN )']]"
"[['two key modules', 'name', 'Selective Two - step Classification ( STC ) module']]"
"[['filter out', 'has', 'most simple negative samples']]"
[]
[]
[]
"[['batch size', 'has', '32']]"
"[['learning rate', 'has', '10 ?2']]"
[]
[]
"[['SRN', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art methods']]"
[]
"[['SRN', 'has', 'state - of - the - art results']]"
[]
"[['our SRN', 'has', 'new state - of - the - art performance']]"
[]
[]
[]
[]
[]
"[['variant of channel features', 'name', 'aggregate channel features']]"
"[['aggregate channel features', 'has', 'bottleneck']]"
[]
"[['deep exploration', 'has', 'multi-scaling'], ['multi-scaling', 'has', 'feature representation'], ['performance', 'has', 'greatly'], ['face detection', 'has', 'color channel']]"
"[['AFW', 'has', 'our multi-scale detector'], ['our multi-scale detector', 'has', 'ap value']]"
"[['commercial systems', 'has', 'ours']]"
"[['discrete score', 'has', 'evaluation metric'], ['evaluation metric', 'has', 'AFW']]"
"[['continuous score', 'has', 'overlap ratio']]"
[]
[]
[]
[]
"[['first stage', 'name', 'multi-task Region Proposal Network ( RPN )']]"
[]
[]
"[['second - stage network', 'name', 'RCNN']]"
"[['K face candidate regions', 'has', 'top responses']]"
[]
[]
"[['end - to - end training', 'has', 'training']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['Our non - top K suppression', 'has', 'very close']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['NPD feature', 'has', 'several desirable properties']]"
"[['resulting', 'has', 'face detection template']]"
[]
[]
"[['""', 'has', 'divide and conquer "" strategy']]"
[]
[]
"[['detection template', 'has', '24 24 pixels']]"
[]
[]
[]
[]
[]
"[['detection template', 'has', '20 20 pixels']]"
"[['detector cascade', 'has', '15 stages'], ['each stage', 'has', 'target false accept rate'], ['target false accept rate', 'has', '0.5']]"
[]
[]
"[['proposed NPD face detector', 'has', 'second best one']]"
"[['proposed NPD detector', 'has', 'top performers']]"
"[['Joint Cascade algorithm', 'has', 'most competitive one']]"
[]
[]
[]
"[['Viola - Jones frontal face detector', 'has', 'NPD detector']]"
[]
[]
[]
"[['SSD', 'has', 'pre-defined anchor boxes']]"
[]
"[['optimization method', 'has', 'SGD'], ['batch size', 'has', '32']]"
"[['initial learning rate', 'has', '0.1']]"
[]
"[['training time', 'has', 'about 5 days']]"
[]
[]
[]
[]
[]
"[['LFFD', 'has', 'superior']]"
[]
"[['performance drop', 'has', 'evident']]"
"[['Pyramid Box', 'has', 'best results']]"
[]
[]
[]
[]
[]
[]
"[['proposed CNNs', 'has', 'three stages']]"
"[['candidate windows', 'has', 'quickly']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['our method', 'has', 'outperforms'], ['outperforms', 'has', 'all the state - of - the - art methods']]"
[]
[]
[]
[]
"[['images', 'has', 'not perfectly detected']]"
[]
"[['our detector', 'has', 'more efficient']]"
[]
"[['learning rate', 'has', '0.004']]"
[]
[]
[]
[]
[]
[]
[]
"[['discontinuous ROC curve', 'has', 'our method']]"
"[['PR curve', 'has', 'our method'], ['our method', 'has', 'new']]"
"[['AP', 'has', '99.60']]"
[]
"[['hard subset', 'has', 'significantly']]"
[]
"[['HIM and DH', 'has', 'HIM and DH together']]"
[]
[]
[]
"[['feature maps', 'has', 'multiple times']]"
[]
[]
[]
[]
[]
[]
[]
"[['identifying', 'has', 'face']]"
[]
"[['structure', 'has', 'shallow version'], ['first seven ResNet blocks', 'has', 'used']]"
[]
[]
[]
[]
"[['batch size', 'has', '4'], ['base learning rate', 'has', '0.001']]"
"[['maximum training iteration', 'has', '1,000,000']]"
[]
[]
[]
"[['deeper', 'has', 'RSA'], ['RSA', 'has', 'branched out'], ['worse', 'has', 'feature approximation']]"
[]
"[['minimum operation', 'has', 'each component'], ['maximum operation', 'has', 'amount']]"
[]
"[['recurrent operation', 'has', 'increase'], ['increase', 'has', 'error rate']]"
[]
[]
[]
"[['size', 'has', 'general face']]"
[]
[]
[]
[]
[]
[]
"[['RPN stage', 'has', 'Face R - FCN']]"
[]
[]
[]
[]
[]
"[['testing stage', 'has', 'multi-scale testing']]"
[]
"[['WIDER FACE hard subset', 'has', 'our approach']]"
[]
[]
[]
[]
"[['Face R - FCN', 'has', 'superior performance']]"
"[['true positive rate', 'has', '98. 99 %']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['final approach', 'has', 'delicate mixture']]"
[]
[]
[]
"[['reduces', 'has', 'error']]"
[]
[]
"[['post - hoc regressor', 'has', 'our detector']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['music industry domain subtask', 'has', 'our system']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['dropout probability', 'has', 'CNN filter width']]"
[]
"[['batch size', 'has', '20'], ['word embedding and sense embedding sizes', 'has', '300']]"
"[['0.5h', 'has', 'domain - specific domain - specific ones']]"
[]
[]
[]
"[['outperform', 'has', 'term embedding averaging']]"
"[['word embedding', 'has', 'sense embedding'], ['sense embedding', 'has', 'much poorer result']]"
[]
[]
[]
"[['performance', 'has', 'measures']]"
[]
[]
[]
[]
"[['feature weighting', 'has', 'consistency'], ['raw frequency', 'has', 'successful']]"
"[['new SLQS variants', 'has', 'list']]"
[]
[]
[]
[]
[]
[]
"[['TAXOEMBED', 'has', 'hypernym detection algorithm']]"
[]
[]
[]
"[['jointly', 'has', 'hypernym extraction and dis ambiguation']]"
[]
"[['taxonomy learning and Information Extraction systems', 'name', 'WiBi']]"
"[['DefIE', 'has', 'automaic OIE system']]"
[]
"[['Yago and WiBi', 'has', 'best over all results']]"
"[['surpassing', 'has', 'Yago']]"
[]
[]
[]
[]
[]
[]
"[['PRF', 'has', 'best F - value'], ['best cluster number', 'has', '2']]"
"[['projection learning method', 'has', 'not very well']]"
[]
[]
[]
"[['cross validation', 'has', 'performance'], ['test data', 'has', 'dropped'], ['dropped', 'has', 'significantly'], ['English', 'has', 'MAP']]"
[]
[]
"[['hybrid system', 'has', 'ranked 1st']]"
"[['scores', 'has', 'much higher'], ['much higher', 'has', 'strongest baselines']]"
[]
"[['our scores', 'has', 'slightly']]"
"[['cross-evaluation results', 'has', 'supervised baseline']]"
[]
"[['run 1', 'has', 'best']]"
[]
[]
[]
[]
[]
"[['two feature vectors', 'name', 'feature vector']]"
[]
[]
[]
"[['three corpora', 'has', 'our system']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['original sequence', 'has', 'in parallel']]"
[]
[]
[]
"[['QRN baseline', 'has', 'C - GRU'], ['C - GRU', 'has', 'significantly worse']]"
"[['C - GRU', 'has', 'significantly better']]"
[]
[]
[]
[]
[]
[]
"[['ConZNet architecture', 'has', 'two phases']]"
[]
"[['second phase', 'has', 'encoder - decoder architecture']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['number of hidden units', 'has', '100']]"
"[['initial learning rate', 'has', '0.1']]"
"[['our model', 'has', 'gradually dropped']]"
[]
"[['performance', 'has', 'model'], ['model', 'has', 'improved dramatically'], ['sample sizes', 'has', '3 and 5']]"
[]
[]
[]
"[['context', 'has', 'outperforms'], ['outperforms', 'has', 'models']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['vocabulary', 'has', 'words']]"
"[['max sequence length', 'has', '600'], ['hidden state size', 'has', '200']]"
[]
"[['Sentinel vectors', 'has', 'randomly initialized and optimized']]"
"[['dynamic decoder', 'has', 'maximum number of iterations'], ['maximum number of iterations', 'has', '4'], ['maxout pool size', 'has', '16']]"
[]
[]
"[['performance', 'has', 'Dynamic Coattention Network'], ['DCN', 'has', 'capability'], ['answer span', 'has', 'multiple times']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['learning rate', 'has', '2 e ?4']]"
[]
"[['sentences', 'has', 'to be memorized']]"
[]
[]
[]
[]
"[['number of unnecessary object pairs', 'has', 'RN']]"
"[['match type feature', 'has', 'all models'], ['all models', 'has', 'RMN']]"
"[['RMN', 'has', 'same error rate'], ['same error rate', 'has', '25.1 %']]"
[]
[]
[]
[]
"[['deep , end - to - end , neural comprehension model', 'name', 'EpiReader']]"
[]
[]
[]
[]
"[['Extractor', 'has', 'important function']]"
[]
[]
[]
[]
"[['estimates', 'has', 'measure']]"
[]
[]
"[['word embeddings', 'has', 'initialized randomly'], ['initialized', 'has', 'randomly']]"
[]
[]
[]
[]
[]
"[['CBT - CN', 'has', '4.0 %']]"
[]
"[['CBT - NE results', 'has', 'validation and test accuracies'], ['validation and test accuracies', 'has', 'relatively high variance']]"
[]
"[['novel recurrent neural network ( RNN ) architecture', 'has', 'recurrence'], ['possibly large external memory', 'has', 'multiple times']]"
[]
[]
[]
"[['each mini-batch update', 'has', '2 norm'], ['larger than L = 50', 'has', 'scaled down']]"
"[['learning rate annealing schedule', 'has', 'validation cost'], ['validation cost', 'has', 'not decreased'], ['learning rate', 'has', 'scaled down']]"
"[['batch size', 'has', '128']]"
"[['each training', 'has', '10 times']]"
"[['MemNN', 'has', 'strongly supervised AM + NG + NL Memory Networks approach']]"
[]
"[['weakly supervised heuristic version', 'has', 'MemNN']]"
"[['LSTM', 'has', 'standard LSTM model']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['initialized', 'has', 'randomly']]"
[]
"[['mini - batch size', 'has', '32']]"
[]
"[['difference', 'has', 'ESIM and KIM']]"
"[['performance', 'has', 'models']]"
[]
[]
[]
[]
"[['fact', 'has', 'answer'], ['answer', 'has', 'word']]"
[]
[]
"[['dot product', 'has', 'question embedding and the contextual embedding']]"
[]
[]
"[['gradient clipping threshold', 'has', '10']]"
[]
[]
"[['CNN dataset', 'has', 'single model']]"
[]
[]
"[['named entity prediction', 'has', 'best single model']]"
"[['common noun prediction', 'has', '0.4 % absolute better']]"
[]
[]
[]
[]
[]
"[['GLUE tasks', 'has', 'training data']]"
"[['feature', 'has', 'privately - held test data']]"
[]
[]
[]
[]
[]
"[['GenSen', 'has', 'outperforms'], ['outperforms', 'has', 'all but the two best']]"
[]
"[['STS - B', 'has', 'models'], ['task', 'has', 'lag']]"
[]
[]
[]
[]
"[['adapting', 'has', 'weight initialization']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['training curves', 'has', 'CBS schedules'], ['aforementioned', 'has', 'cyclical spikes']]"
[]
[]
[]
[]
"[['performance', 'has', 'increases'], ['increases', 'has', '76.401 %']]"
[]
[]
[]
[]
[]
[]
"[['TBCNN', 'has', 'more robust']]"
"[['pooling layer', 'has', 'aggregates'], ['aggregates', 'has', 'information']]"
[]
[]
[]
[]
"[['Initial learning rate', 'has', '1']]"
[]
"[['existing sentence encoding - based approaches', 'name', 'feature - rich method']]"
[]
[]
"[['different matching heuristics', 'has', 'result'], ['result', 'has', 'TBCNN - pair model']]"
[]
[]
[]
[]
[]
[]
"[['tokenize', 'has', 'all the dataset']]"
[]
"[['character encoding', 'has', 'concatenation'], ['hidden size', 'has', '50 , 100 , 150']]"
"[['lexicon embeddings', 'has', 'd =600 - dimensions']]"
[]
"[['hidden size', 'has', 'LSTM']]"
[]
[]
"[['dropout rate', 'has', '0.2']]"
"[['mini - batch size', 'has', '32']]"
[]
[]
"[['SciTail dataset', 'has', 'SAN']]"
"[['SciTail dataset', 'has', 'SAN']]"
"[['Single - step baseline', 'has', 'proposed model']]"
[]
[]
"[[""Chen 's model"", 'has', 'biggest improvement'], ['SAN', 'has', '50 % vs 77 % and 58 % vs 85 %']]"
"[['most challenging "" Long Sentence "" and "" Quantity / Time "" categories', 'has', ""SAN 's result""], [""SAN 's result"", 'has', 'substantially better']]"
[]
[]
[]
"[['node transformation functions', 'name', 'leaf node mapping']]"
[]
"[['sequential leaf node transformer', 'name', 'LSTM'], ['chosen', 'has', 'NTI network']]"
[]
"[['best score', 'has', '87.3 % accuracy']]"
[]
"[['node - by - node attention models', 'has', 'performance']]"
[]
"[['large margin', 'has', 'nearly 5 - 6 %']]"
[]
"[['NTI model', 'has', 'NASM']]"
[]
[]
[]
[]
[]
[]
"[['novel neural network architecture', 'name', 'attention - over - attention model']]"
[]
[]
[]
"[['embedding weights', 'has', 'randomly initialized']]"
"[['Hidden Layer', 'has', 'Internal weights']]"
"[['initial learning rate', 'has', '0.001']]"
"[['gradient clipping threshold', 'has', '5']]"
[]
[]
[]
[]
[]
"[['another significant boost', 'has', '2.0 % to 3.7 %']]"
[]
"[['ensemble model', 'has', 'our AoA Reader']]"
"[['pre-defined merging heuristics', 'has', 'model']]"
"[['NE and CN category', 'has', 'benefit a lot']]"
"[['NE category', 'has', 'performance']]"
[]
[]
[]
[]
"[['most similar question', 'has', 'very quickly']]"
[]
"[['character embedding', 'has', 'randomly initialized']]"
[]
"[['dropout rate', 'has', '0.1']]"
[]
"[['learning rate', 'has', '4e - 4'], ['batch size', 'has', '200']]"
"[['performance', 'has', 'model'], ['learning rate', 'has', '1e - 3']]"
"[['ESIM', 'has', 'Enhanced Sequential Inference Model']]"
"[['BiMPM', 'has', 'Bilateral Multi- Perspective Matching model']]"
"[['SSE', 'has', 'Shortcut - Stacked Sentence Encoder'], ['Shortcut - Stacked Sentence Encoder', 'has', 'encodingbased sentence - matching model']]"
"[['DIIN', 'has', 'Densely Interactive Inference Network']]"
"[['Quora dataset', 'has', 'BiMPM and ESIM models']]"
[]
"[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'state - of the - art models']]"
"[['BQ dataset', 'has', 'specific - domain dataset']]"
"[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art models']]"
[]
[]
[]
"[['accuracy', 'has', 'decreases'], ['decreases', 'has', 'accuracy'], ['accuracy', 'has', 'drop']]"
[]
"[['highway network', 'has', 'accuracy'], ['accuracy', 'has', 'drop']]"
"[['character - level embedding', 'has', 'accuracy']]"
[]
[]
[]
"[['text matching', 'has', 'modelling'], ['modelling', 'has', 'interaction']]"
"[['interactions', 'has', 'recursively']]"
"[['models', 'has', 'text']]"
[]
[]
[]
"[['Single LSTM', 'has', 'Two sequences']]"
"[['Parallel LSTMs', 'has', 'Two sequences']]"
"[['Attention LSTMs', 'has', 'Two sequences']]"
[]
"[['proposed model', 'has', 'superiority'], ['both metrics', 'has', 'large margin']]"
[]
[]
[]
[]
"[['scan', 'has', 'carefully']]"
[]
[]
"[['our approach', 'has', 'generic']]"
[]
[]
[]
[]
[]
"[['Dropout', 'has', 'p = 0.3']]"
[]
"[['aligned question embedding feature', 'has', 'our system']]"
"[['f aligned and f exact match', 'has', 'performance'], ['performance', 'has', 'drops'], ['drops', 'has', 'dramatically']]"
[]
[]
[]
[]
[]
"[['early stages', 'has', 'simple features and ranking functions']]"
[]
[]
[]
[]
"[['three modules', 'name', 'document retrieval']]"
[]
"[['module', 'has', 'each subsequent stage']]"
"[['ranking function', 'has', 'preliminary filter']]"
[]
[]
"[['mini-batch size', 'has', '32'], ['initial learning rate', 'has', '0.0005']]"
[]
[]
[]
[]
[]
[]
"[['deep cascade learning framework', 'has', 'proposed model']]"
"[['shared LSTM', 'has', 'important role']]"
"[['manual features', 'has', 'performance'], ['performance', 'has', 'further improved slightly']]"
[]
[]
[]
[]
[]
"[['three sub - tasks', 'name', 'answer pointer']]"
"[['U - Net', 'name', 'three sub - tasks']]"
[]
[]
[]
[]
[]
[]
[]
"[['LSTM blocks', 'has', 'bi-directional']]"
"[['hidden layer dimension', 'has', '125'], ['attention layer dimension', 'has', '250']]"
"[['dropout layer', 'has', 'modeling layers'], ['dropout rate', 'has', '0.3']]"
[]
[]
[]
"[['best - performing systems', 'has', 'our model'], ['our model', 'has', 'simple architecture']]"
[]
"[['node U', 'has', 'shared'], ['not shared', 'has', 'performance'], ['performance', 'has', 'slightly degraded']]"
"[['performance', 'has', 'dropped slightly']]"
"[['plausible answer pointer', 'has', 'performance'], ['performance', 'has', 'dropped']]"
"[['answer verifier', 'has', 'performance'], ['performance', 'has', 'dropped greatly'], ['dropped greatly', 'has', 'greatly']]"
[]
[]
[]
[]
"[['SDNet', 'has', 'contextual attention - based deep neural network']]"
[]
[]
[]
"[['previous rounds of questions and answers', 'has', 'previous rounds']]"
[]
[]
[]
[]
[]
[]
"[['distributed memory and processor architecture', 'name', 'Recurrent Entity Network ( EntNet )']]"
[]
"[['own "" processor ""', 'has', 'simple gated recurrent network']]"
"[['EntNet', 'has', 'bank of gated RNNs'], ['bank of gated RNNs', 'has', 'hidden states']]"
[]
[]
[]
"[['number of hops', 'has', 'T ? 2'], ['embedding dimension', 'has', 'd = 20']]"
"[['LSTM', 'has', '50 hidden units']]"
"[['2', 'has', 'every 10,000 updates']]"
"[['MemN2N', 'has', 'worst performance'], ['sequence', 'has', 'increases']]"
"[['sequence', 'has', 'increases']]"
[]
"[['methods', 'has', 'limited memory']]"
[]
[]
[]
"[['alternatively captures', 'has', 'question - aware passage representations']]"
[]
[]
[]
"[['performance', 'has', 'different number of layers']]"
"[['question - passage attention phase', 'has', 'single layer'], ['performance', 'has', 'significantly']]"
"[['multiple stacking layers', 'has', 'needed']]"
[]
[]
"[['modeling questions', 'has', 'end - to - end neural network framework']]"
[]
"[['proposed', 'has', 'adaptation models']]"
[]
[]
[]
[]
"[['CharCNN filter length', 'has', '1 , 3 , 5']]"
[]
[]
"[['first momentum', 'has', '0.9'], ['second', 'has', '0.999']]"
"[['initial learning rate', 'has', '0.0004'], ['batch size', 'has', '32']]"
"[['word - level embedding d w', 'has', '300 dimensions']]"
[]
"[['Explicit question - type dimension d ET', 'has', '50']]"
[]
[]
[]
"[['performance', 'has', 'improved slightly']]"
"[['TreeLSTM', 'has', 'syntactic parses']]"
[]
[]
[]
[]
"[['Natural language sentences', 'has', 'complicated structures']]"
[]
[]
"[['novel model', 'has', 'naturally host'], ['naturally host', 'has', 'hierarchical composition']]"
[]
[]
"[['ARC - I', 'has', 'significantly']]"
[]
[]
[]
[]
[]
"[['MANN', 'name', 'SAM ( sparse access memory )']]"
"[['memory modifications', 'has', 'sparse subset'], ['our model', 'has', 'optimal']]"
[]
"[['some tasks', 'has', 'notably'], ['notably', 'has', 'priority sort']]"
[]
"[['all tasks', 'has', 'SAM'], ['advance', 'has', 'further'], ['associative recall task', 'has', 'SAM']]"
[]
[]
[]
"[['NTM', 'has', 'poorly']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['memory controller', 'has', 'residual connection']]"
"[['gated recurrent unit ( GRU )', 'has', 'dense connection']]"
[]
[]
"[['memory size', 'has', '100 36']]"
"[['hidden vector dimension l', 'has', '200']]"
"[['learning rate', 'has', '0.5']]"
"[['batch size', 'has', '20']]"
[]
[]
"[['lengthy - document cases', 'has', 'our model'], ['short - document case', 'has', 'SQuAD']]"
[]
"[['DEBS', 'has', 'all the places'], ['memory controller', 'has', 'DEBS']]"
[]
[]
[]
[]
"[['semantic matching vector', 'has', 'each word vector'], ['two components', 'name', 'similar component']]"
[]
[]
[]
[]
"[['performance', 'has', 'improved significantly']]"
"[['our model', 'has', 'best MAP']]"
[]
"[['best performance', 'has', 'bigram CNN model']]"
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
"[['tradeoffs', 'has', 'multiple DSA']]"
"[['baseline', 'has', 'single DSA']]"
"[['our implementation', 'name', 'baseline'], ['baseline', 'name', 'selfattention']]"
[]
[]
"[['SST dataset', 'has', 'marginal differences']]"
[]
[]
[]
[]
[]
[]
[]
[]
"[['relatively strong performance', 'has', 'word distance benchmark']]"
[]
[]
[]
"[['Reader', 'has', 'surprisingly well']]"
[]
[]
[]
"[['Entailment', 'has', 'inferred to be true'], ['Contradiction', 'has', 'inferred to be false']]"
[]
"[['building', 'has', 'biL - STM models']]"
"[['basic mean pooling encoder', 'has', 'intuition']]"
[]
"[['training objective', 'has', 'cross - entropy loss']]"
"[['batch size', 'has', '128']]"
"[['dropout rate', 'has', '0.25']]"
[]
"[['sampling values', 'has', 'values']]"
[]
"[['mean pooling', 'has', 'each word']]"
[]
[]
"[['rank', 'has', 'spans'], ['spans', 'has', 'at'], ['at', 'has', 'inference time'], ['inference time', 'has', 'by'], ['by', 'has', 'difference']]"
[]
[]
[]
[]
[]
"[['Our model', 'has', 'gap']]"
