,labels,text,predicates,subj/obj,triple_A,triple_B,triple_C,triple_D,topic,paper_idx,idx
0,research-problem,Recurrent Neural Network Grammars,[],"[('Grammars', (3, 4))]",[],[],[],[],constituency_parsing,0,2
1,model,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .","[('introduce', (5, 6)), ('of', (21, 22)), ('explicitly models', (24, 26)), ('among', (30, 31))]","[('recurrent neural network grammars', (6, 10)), ('new generative probabilistic model', (17, 21)), ('sentences', (22, 23)), ('nested , hierarchical relationships', (26, 30)), ('words and phrases', (31, 34))]","[['new generative probabilistic model', 'of', 'sentences'], ['sentences', 'explicitly models', 'nested , hierarchical relationships'], ['nested , hierarchical relationships', 'among', 'words and phrases']]","[['recurrent neural network grammars', 'has', 'new generative probabilistic model']]","[['Model', 'introduce', 'recurrent neural network grammars']]",[],constituency_parsing,0,12
2,model,"RNNGs operate via a recursive syntactic process reminiscent of probabilistic context - free grammar generation , but decisions are parameterized using RNNs that condition on the entire syntactic derivation history , greatly relaxing context - free independence assumptions .","[('operate via', (1, 3)), ('reminiscent of', (7, 9))]","[('RNNGs', (0, 1)), ('recursive syntactic process', (4, 7)), ('probabilistic context - free grammar generation', (9, 15))]","[['RNNGs', 'operate via', 'recursive syntactic process'], ['recursive syntactic process', 'reminiscent of', 'probabilistic context - free grammar generation']]",[],[],"[['Model', 'has', 'RNNGs']]",constituency_parsing,0,13
3,model,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .","[('give', (1, 2)), ('of', (4, 5)), ('for', (9, 10))]","[('two variants', (2, 4)), ('algorithm', (6, 7)), ('parsing', (10, 11)), ('generation', (27, 28))]","[['two variants', 'of', 'algorithm']]","[['two variants', 'has', 'algorithm']]","[['Model', 'give', 'two variants']]",[],constituency_parsing,0,15
4,model,"The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences , and this is used to solve a second practical challenge with RNNGs : approximating the marginal likelihood and MAP tree of a sentence under the generative model .","[('use', (6, 7)), ('to obtain', (9, 11)), ('of', (12, 13)), ('for', (15, 16))]","[('discriminative model', (1, 3)), ('ancestor sampling', (7, 9)), ('samples', (11, 12)), ('parse trees', (13, 15)), ('sentences', (16, 17))]","[['discriminative model', 'use', 'ancestor sampling'], ['ancestor sampling', 'to obtain', 'samples'], ['samples', 'of', 'parse trees'], ['parse trees', 'for', 'sentences']]",[],[],"[['Model', 'has', 'discriminative model']]",constituency_parsing,0,24
5,model,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,"[('present', (1, 2)), ('uses', (8, 9)), ('from', (10, 11)), ('to solve', (14, 16)), ('in', (18, 19))]","[('simple importance sampling algorithm', (3, 7)), ('samples', (9, 10)), ('discriminative parser', (12, 14)), ('inference problems', (16, 18)), ('generative model', (20, 22))]","[['simple importance sampling algorithm', 'uses', 'samples'], ['samples', 'from', 'discriminative parser'], ['samples', 'to solve', 'inference problems'], ['discriminative parser', 'to solve', 'inference problems'], ['inference problems', 'in', 'generative model']]",[],"[['Model', 'present', 'simple importance sampling algorithm']]",[],constituency_parsing,0,25
6,hyperparameters,"For the discriminative model , we used hidden dimensions of 128 and 2 - layer LSTMs ( larger numbers of dimensions reduced validation set performance ) .","[('For', (0, 1)), ('used', (6, 7)), ('of', (9, 10))]","[('discriminative model', (2, 4)), ('hidden dimensions', (7, 9)), ('128', (10, 11)), ('2 - layer LSTMs', (12, 16))]","[['discriminative model', 'used', 'hidden dimensions'], ['discriminative model', 'used', '2 - layer LSTMs'], ['hidden dimensions', 'of', '128']]",[],"[['Hyperparameters', 'For', 'discriminative model']]",[],constituency_parsing,0,160
7,hyperparameters,"For the generative model , we used 256 dimensions and 2 layer LSTMs .","[('used', (6, 7))]","[('generative model', (2, 4)), ('256 dimensions', (7, 9)), ('2 layer LSTMs', (10, 13))]","[['generative model', 'used', '256 dimensions'], ['generative model', 'used', '2 layer LSTMs']]",[],[],[],constituency_parsing,0,161
8,hyperparameters,"For both models , we tuned the dropout rate to maximize validation set likelihood , obtaining optimal rates of 0.2 ( discriminative ) and 0.3 ( generative ) .","[('tuned', (5, 6)), ('to maximize', (9, 11)), ('obtaining', (15, 16)), ('of', (18, 19))]","[('both models', (1, 3)), ('dropout rate', (7, 9)), ('validation set likelihood', (11, 14)), ('optimal rates', (16, 18)), ('0.2 ( discriminative )', (19, 23)), ('0.3 ( generative )', (24, 28))]","[['both models', 'tuned', 'dropout rate'], ['dropout rate', 'to maximize', 'validation set likelihood'], ['dropout rate', 'obtaining', 'optimal rates'], ['validation set likelihood', 'obtaining', 'optimal rates'], ['optimal rates', 'of', '0.2 ( discriminative )'], ['optimal rates', 'of', '0.3 ( generative )']]",[],[],[],constituency_parsing,0,162
9,hyperparameters,"For the sequential LSTM baseline for the language model , we also found an optimal dropout rate of 0.3 .","[('for', (5, 6)), ('found', (12, 13)), ('of', (17, 18))]","[('sequential LSTM baseline', (2, 5)), ('language model', (7, 9)), ('optimal dropout rate', (14, 17)), ('0.3', (18, 19))]","[['sequential LSTM baseline', 'for', 'language model'], ['sequential LSTM baseline', 'found', 'optimal dropout rate'], ['optimal dropout rate', 'of', '0.3']]",[],[],[],constituency_parsing,0,163
10,hyperparameters,For training we used stochastic gradient descent with a learning rate of 0.1 .,"[('used', (3, 4)), ('with', (7, 8)), ('of', (11, 12))]","[('training', (1, 2)), ('stochastic gradient descent', (4, 7)), ('learning rate', (9, 11)), ('0.1', (12, 13))]","[['training', 'used', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'learning rate'], ['learning rate', 'of', '0.1']]",[],[],[],constituency_parsing,0,164
11,research-problem,Cloze - driven Pretraining of Self - attention Networks,[],"[('Pretraining of Self - attention Networks', (3, 9))]",[],[],[],[],constituency_parsing,1,2
12,research-problem,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .,[],"[('pretraining a bi-directional transformer model', (6, 11))]",[],[],[],[],constituency_parsing,1,4
13,research-problem,Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems .,[],"[('Language model pretraining', (0, 3))]",[],[],[],[],constituency_parsing,1,9
14,model,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .","[('show', (5, 6)), ('by', (13, 14)), ('jointly pretraining', (14, 16)), ('of', (18, 19))]","[('even larger performance gains', (7, 11)), ('possible', (12, 13)), ('both directions', (16, 18)), ('large language - model - inspired self - attention cloze model', (20, 31))]","[['even larger performance gains', 'jointly pretraining', 'both directions'], ['possible', 'jointly pretraining', 'both directions'], ['both directions', 'of', 'large language - model - inspired self - attention cloze model']]","[['even larger performance gains', 'has', 'possible']]","[['Model', 'show', 'even larger performance gains']]",[],constituency_parsing,1,11
15,model,Our bi-directional transformer architecture predicts every token in the training data ( ) .,"[('in', (7, 8))]","[('Our bi-directional transformer architecture', (0, 4)), ('predicts', (4, 5)), ('every token', (5, 7)), ('training data', (9, 11))]","[['every token', 'in', 'training data']]","[['Our bi-directional transformer architecture', 'has', 'predicts'], ['predicts', 'has', 'every token']]",[],"[['Model', 'has', 'Our bi-directional transformer architecture']]",constituency_parsing,1,12
16,model,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,"[('introducing', (4, 5)), ('where', (11, 12)), ('predict', (15, 16)), ('given', (19, 20))]","[('cloze - style training objective', (6, 11)), ('model', (13, 14)), ('center word', (17, 19)), ('left - to - right and right - to - left context representations', (20, 33))]","[['cloze - style training objective', 'where', 'model'], ['model', 'predict', 'center word'], ['center word', 'given', 'left - to - right and right - to - left context representations']]","[['cloze - style training objective', 'has', 'model']]","[['Model', 'introducing', 'cloze - style training objective']]",[],constituency_parsing,1,13
17,model,Our model separately computes both forward and backward states with * Equal contribution . :,"[('separately computes', (2, 4)), ('with', (9, 10))]","[('Our model', (0, 2)), ('both forward and backward states', (4, 9)), ('Equal contribution', (11, 13))]","[['Our model', 'separately computes', 'both forward and backward states'], ['both forward and backward states', 'with', 'Equal contribution']]",[],[],"[['Model', 'has', 'Our model']]",constituency_parsing,1,14
18,experiments,Experimental setup,[],"[('Experimental setup', (0, 2))]",[],[],[],[],constituency_parsing,1,110
19,experimental-setup,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .","[('use', (2, 3)), ('in', (6, 7)), ('contains', (12, 13)), ('with', (18, 19)), ('followed by', (22, 24)), ('with', (28, 29))]","[('CNN models', (0, 2)), ('adaptive softmax', (4, 6)), ('output', (8, 9)), ('headband', (11, 12)), ('60K most frequent types', (14, 18)), ('dimensionality', (19, 20)), ('1024', (20, 21)), ('160 K band', (25, 28)), ('dimensionality', (29, 30)), ('256', (30, 31))]","[['CNN models', 'use', 'adaptive softmax'], ['adaptive softmax', 'in', 'output'], ['headband', 'contains', '60K most frequent types'], ['60K most frequent types', 'with', 'dimensionality'], ['dimensionality', 'followed by', '160 K band'], ['1024', 'followed by', '160 K band'], ['160 K band', 'with', 'dimensionality']]","[['dimensionality', 'has', '1024'], ['dimensionality', 'has', '256']]",[],"[['Experimental setup', 'has', 'CNN models']]",constituency_parsing,1,128
20,experimental-setup,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,"[('from', (7, 8)), ('to', (11, 12)), ('for', (13, 14)), ('using', (20, 21)), ('with', (26, 27)), ('to', (30, 31))]","[('learning rate', (1, 3)), ('linearly warmed up', (4, 7)), ('10 ? 7', (8, 11)), ('1', (12, 13)), ('16 K steps', (14, 17)), ('annealed', (19, 20)), ('cosine learning rate schedule', (22, 26)), ('single phase', (28, 30)), ('0.0001', (31, 32))]","[['linearly warmed up', 'from', '10 ? 7'], ['10 ? 7', 'to', '1'], ['single phase', 'to', '0.0001'], ['1', 'for', '16 K steps'], ['annealed', 'using', 'cosine learning rate schedule'], ['cosine learning rate schedule', 'with', 'single phase'], ['single phase', 'to', '0.0001']]","[['learning rate', 'has', 'linearly warmed up']]",[],"[['Experimental setup', 'has', 'learning rate']]",constituency_parsing,1,129
21,experimental-setup,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,"[('run', (1, 2)), ('on', (3, 4)), ('with', (8, 9)), ('interconnected by', (16, 18))]","[('experiments', (2, 3)), ('DGX - 1 machines', (4, 8)), ('8 NVIDIA V100 GPUs', (9, 13)), ('machines', (14, 15)), ('Infiniband', (18, 19))]","[['experiments', 'on', 'DGX - 1 machines'], ['DGX - 1 machines', 'with', '8 NVIDIA V100 GPUs'], ['machines', 'interconnected by', 'Infiniband']]",[],"[['Experimental setup', 'run', 'experiments']]",[],constituency_parsing,1,130
22,experimental-setup,We also use the NCCL2 library and the torch .,"[('use', (2, 3))]","[('NCCL2 library', (4, 6)), ('torch', (8, 9))]",[],[],"[['Experimental setup', 'use', 'NCCL2 library']]",[],constituency_parsing,1,131
23,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],constituency_parsing,1,135
24,experiments,GLUE,[],"[('GLUE', (0, 1))]",[],[],[],[],constituency_parsing,1,136
25,results,"All our models outperform the uni-directional transformer ( OpenAI GPT ) of , however , our model is about 50 % larger than their model .","[('outperform', (3, 4))]","[('All our models', (0, 3)), ('uni-directional transformer', (5, 7))]","[['All our models', 'outperform', 'uni-directional transformer']]",[],[],[],constituency_parsing,1,151
26,results,"Our CNN base model performs as well as STILTs in aggregate , however , on some tasks involving sentence - pairs , STILTs performs much better ( MRPC , RTE ) ; there is a similar trend for BERT .","[('performs', (4, 5)), ('as', (5, 6)), ('in', (9, 10))]","[('Our CNN base model', (0, 4)), ('well', (6, 7)), ('STILTs', (8, 9)), ('aggregate', (10, 11))]","[['Our CNN base model', 'performs', 'well'], ['Our CNN base model', 'performs', 'STILTs'], ['Our CNN base model', 'as', 'well'], ['STILTs', 'in', 'aggregate']]",[],[],"[['Results', 'has', 'Our CNN base model']]",constituency_parsing,1,153
27,results,Named Entity Recognition,[],"[('Named Entity Recognition', (0, 3))]",[],[],[],"[['Results', 'has', 'Named Entity Recognition']]",constituency_parsing,1,163
28,results,"shows the results , with comparison to previous published ELMo BASE results the art , but fine tuning gives the biggest gain .","[('gives', (18, 19))]","[('fine tuning', (16, 18)), ('biggest gain', (20, 22))]","[['fine tuning', 'gives', 'biggest gain']]","[['fine tuning', 'has', 'biggest gain']]",[],"[['Results', 'has', 'fine tuning']]",constituency_parsing,1,167
29,results,Constituency Parsing,[],"[('Constituency Parsing', (0, 2))]",[],[],[],"[['Results', 'has', 'Constituency Parsing']]",constituency_parsing,1,168
30,ablation-analysis,This results in the bilm loss dominating the triplet loss and we found that scaling the bilm term by a factor of 0.15 results in better performance .,"[('results in', (1, 3)), ('found that', (12, 14)), ('by', (18, 19)), ('of', (21, 22))]","[('scaling', (14, 15)), ('bilm term', (16, 18)), ('factor', (20, 21)), ('0.15', (22, 23)), ('better performance', (25, 27))]","[['scaling', 'by', 'factor'], ['bilm term', 'by', 'factor'], ['factor', 'of', '0.15']]","[['scaling', 'has', 'bilm term']]","[['Ablation analysis', 'results in', 'scaling']]",[],constituency_parsing,1,180
31,results,shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself .,"[('shows that', (0, 2)), ('performs', (5, 6)), ('than', (8, 9)), ('combining', (14, 15)), ('over', (22, 23))]","[('cloze loss', (3, 5)), ('significantly better', (6, 8)), ('bilm loss', (10, 12)), ('two loss types', (16, 19)), ('does not improve', (19, 22)), ('cloze loss', (24, 26))]","[['cloze loss', 'performs', 'significantly better'], ['significantly better', 'than', 'bilm loss'], ['cloze loss', 'combining', 'two loss types'], ['does not improve', 'over', 'cloze loss']]","[['two loss types', 'has', 'does not improve']]","[['Results', 'shows that', 'cloze loss']]",[],constituency_parsing,1,181
32,research-problem,An Empirical Study of Building a Strong Baseline for Constituency Parsing,[],"[('Constituency Parsing', (9, 11))]",[],[],[],[],constituency_parsing,2,2
33,model,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,"[('update', (4, 5)), ('as', (10, 11)), ('of', (14, 15))]","[('Seq2seq approach', (6, 8)), ('stronger baseline', (12, 14)), ('constituency parsing', (15, 17))]","[['Seq2seq approach', 'as', 'stronger baseline'], ['stronger baseline', 'of', 'constituency parsing']]","[['Seq2seq approach', 'has', 'stronger baseline']]","[['Model', 'update', 'Seq2seq approach']]",[],constituency_parsing,2,13
34,results,( 1 ) Smaller mini-batch size M and gradient clipping G provided the better performance .,"[('provided', (11, 12))]","[('Smaller', (3, 4)), ('mini-batch size M and gradient clipping G', (4, 11)), ('better performance', (13, 15))]","[['mini-batch size M and gradient clipping G', 'provided', 'better performance']]","[['Smaller', 'has', 'mini-batch size M and gradient clipping G']]",[],[],constituency_parsing,2,111
35,results,"( 2 ) Larger layer size , hidden state dimension , and beam size have little impact on the performance ; our setting , L = 2 , H = 200 , and B = 5 looks adequate in terms of speed / performance trade - off .","[('have', (14, 15)), ('on', (17, 18))]","[('Larger layer size , hidden state dimension , and beam size', (3, 14)), ('little impact', (15, 17)), ('performance', (19, 20))]","[['Larger layer size , hidden state dimension , and beam size', 'have', 'little impact'], ['little impact', 'on', 'performance']]","[['Larger layer size , hidden state dimension , and beam size', 'has', 'little impact']]",[],"[['Results', 'has', 'Larger layer size , hidden state dimension , and beam size']]",constituency_parsing,2,113
36,results,"As often demonstrated in the NMT literature , using subword split as input token unit instead of standard tokenized word unit has potential to improve the performance .","[('using', (8, 9)), ('as', (11, 12)), ('instead of', (15, 17)), ('to improve', (23, 25))]","[('subword split', (9, 11)), ('input token unit', (12, 15)), ('standard tokenized word unit', (17, 21)), ('potential', (22, 23)), ('performance', (26, 27))]","[['subword split', 'as', 'input token unit'], ['input token unit', 'instead of', 'standard tokenized word unit'], ['potential', 'to improve', 'performance']]","[['subword split', 'has', 'input token unit']]","[['Results', 'using', 'subword split']]",[],constituency_parsing,2,115
37,baselines,"Thus , using subword information as features is one promising approach for leveraging subword information into constituency parsing .","[('as', (5, 6)), ('for leveraging', (11, 13)), ('into', (15, 16))]","[('subword information', (3, 5)), ('features', (6, 7)), ('promising approach', (9, 11)), ('subword information', (13, 15)), ('constituency parsing', (16, 18))]","[['subword information', 'as', 'features'], ['promising approach', 'for leveraging', 'subword information'], ['subword information', 'into', 'constituency parsing']]","[['subword information', 'has', 'features']]",[],[],constituency_parsing,2,119
38,results,Our Seq2seq approach successfully achieved the competitive level as the current top - notch methods : RNNG and its variants .,"[('achieved', (4, 5)), ('as', (8, 9))]","[('Our Seq2seq approach', (0, 3)), ('competitive level', (6, 8)), ('current top - notch methods', (10, 15)), ('RNNG and its variants', (16, 20))]","[['Our Seq2seq approach', 'achieved', 'competitive level'], ['competitive level', 'as', 'current top - notch methods']]","[['current top - notch methods', 'name', 'RNNG and its variants']]",[],"[['Results', 'has', 'Our Seq2seq approach']]",constituency_parsing,2,126
39,research-problem,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .,[],"[('Syntactic constituency parsing', (0, 3))]",[],[],[],[],constituency_parsing,3,4
40,model,"We found this model to work poorly when we trained it on standard human - annotated parsing datasets ( 1M tokens ) , so we constructed an artificial dataset by labelling a large corpus with the BerkeleyParser .","[('constructed', (25, 26)), ('by labelling', (29, 31)), ('with', (34, 35))]","[('an artificial dataset', (26, 29)), ('large corpus', (32, 34)), ('BerkeleyParser', (36, 37))]","[['an artificial dataset', 'by labelling', 'large corpus'], ['large corpus', 'with', 'BerkeleyParser']]",[],"[['Model', 'constructed', 'an artificial dataset']]",[],constituency_parsing,3,21
41,experiments,"We trained a sequence - to - sequence model with attention on the small human - annotated parsing dataset and were able to achieve an F 1 score of 88.3 on section 23 of the WSJ without the use of an ensemble and 90.5 with an ensemble , which matches the performance of the BerkeleyParser ( 90.4 ) when trained on the same data .","[('trained', (1, 2)), ('with', (9, 10)), ('on', (11, 12))]","[('sequence - to - sequence model', (3, 9)), ('attention', (10, 11)), ('small human - annotated parsing dataset', (13, 19))]","[['sequence - to - sequence model', 'with', 'attention'], ['attention', 'on', 'small human - annotated parsing dataset']]","[['sequence - to - sequence model', 'has', 'attention']]",[],[],constituency_parsing,3,25
42,experiments,"Finally , we constructed a second artificial dataset consisting of only high - confidence parse trees , as measured by the agreement of two parsers .","[('consisting of', (8, 10)), ('measured by', (18, 20)), ('of', (22, 23))]","[('second artificial dataset', (5, 8)), ('only high - confidence parse trees', (10, 16)), ('agreement', (21, 22)), ('two parsers', (23, 25))]","[['second artificial dataset', 'consisting of', 'only high - confidence parse trees'], ['only high - confidence parse trees', 'measured by', 'agreement'], ['agreement', 'of', 'two parsers']]",[],[],[],constituency_parsing,3,26
43,experiments,We trained a sequence - to - sequence model with attention on this data and achieved an F 1 score of 92.5 on section 23 of the WSJ - a new state - of - the - art .,"[('trained', (1, 2)), ('with', (9, 10))]","[('sequence - to - sequence model', (3, 9)), ('attention', (10, 11))]","[['sequence - to - sequence model', 'with', 'attention']]",[],[],[],constituency_parsing,3,27
44,model,"In our experiments we used a model with 3 LSTM layers and 256 units in each layer , which we call LSTM + A .","[('used', (4, 5)), ('with', (7, 8)), ('in', (14, 15)), ('call', (20, 21))]","[('model', (6, 7)), ('3 LSTM layers', (8, 11)), ('256 units', (12, 14)), ('each layer', (15, 17)), ('LSTM + A', (21, 24))]","[['model', 'with', '3 LSTM layers'], ['model', 'with', '256 units'], ['256 units', 'in', 'each layer']]",[],"[['Model', 'used', 'model']]",[],constituency_parsing,3,67
45,hyperparameters,"Training on a small dataset we additionally used 2 dropout layers , one between LSTM 1 and LSTM 2 , and one between LSTM 2 and LSTM 3 .","[('Training on', (0, 2)), ('additionally used', (6, 8)), ('between', (13, 14))]","[('small dataset', (3, 5)), ('2 dropout layers', (8, 11)), ('LSTM 1 and LSTM 2', (14, 19)), ('LSTM 2 and LSTM 3', (23, 28))]","[['small dataset', 'additionally used', '2 dropout layers']]",[],"[['Hyperparameters', 'Training on', 'small dataset']]",[],constituency_parsing,3,70
46,hyperparameters,The embedding layer for our 90K vocabulary can be initialized randomly or using pre-trained word - vector embeddings .,"[('for', (3, 4)), ('initialized', (9, 10)), ('using', (12, 13))]","[('embedding layer', (1, 3)), ('our 90K vocabulary', (4, 7)), ('randomly', (10, 11)), ('pre-trained word - vector embeddings', (13, 18))]","[['embedding layer', 'for', 'our 90K vocabulary'], ['our 90K vocabulary', 'initialized', 'randomly'], ['embedding layer', 'using', 'pre-trained word - vector embeddings']]",[],[],"[['Hyperparameters', 'has', 'embedding layer']]",constituency_parsing,3,82
47,hyperparameters,We pre-trained skip - gram embeddings of size 512 using word2vec [ 6 ] on a 10B - word corpus .,"[('pre-trained', (1, 2)), ('of', (6, 7)), ('using', (9, 10)), ('on', (14, 15))]","[('skip - gram embeddings', (2, 6)), ('size 512', (7, 9)), ('word2vec', (10, 11)), ('10B - word corpus', (16, 20))]","[['skip - gram embeddings', 'of', 'size 512'], ['skip - gram embeddings', 'using', 'word2vec'], ['word2vec', 'on', '10B - word corpus']]",[],"[['Hyperparameters', 'pre-trained', 'skip - gram embeddings']]",[],constituency_parsing,3,83
48,results,But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,"[('gets to', (5, 7)), ('achieves', (17, 18)), ('matching', (19, 20)), ('on', (25, 26))]","[('single attention model', (2, 5)), ('88.3', (7, 8)), ('ensemble of 5 LSTM + A+D models', (10, 17)), ('90.5', (18, 19)), ('single - model BerkeleyParser', (21, 25)), ('WSJ', (26, 27))]","[['single attention model', 'gets to', '88.3'], ['ensemble of 5 LSTM + A+D models', 'achieves', '90.5'], ['90.5', 'matching', 'single - model BerkeleyParser'], ['single - model BerkeleyParser', 'on', 'WSJ']]","[['single attention model', 'has', '88.3']]",[],[],constituency_parsing,3,118
49,results,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .","[('trained on', (1, 3)), ('achieves', (16, 17))]","[('large high - confidence corpus', (4, 9)), ('single LSTM + A model', (11, 16)), ('92.5', (17, 18)), ('outperforms', (20, 21)), ('best single model', (24, 27)), ('best ensemble result', (31, 34))]","[['single LSTM + A model', 'achieves', '92.5']]","[['large high - confidence corpus', 'has', 'single LSTM + A model'], ['outperforms', 'has', 'best single model']]","[['Results', 'trained on', 'large high - confidence corpus']]",[],constituency_parsing,3,119
50,results,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,"[('further improves', (7, 9)), ('to', (11, 12))]","[('ensemble of 5 LSTM+ A models', (1, 7)), ('score', (10, 11)), ('92.8', (12, 13))]","[['ensemble of 5 LSTM+ A models', 'further improves', 'score'], ['score', 'to', '92.8']]","[['ensemble of 5 LSTM+ A models', 'has', 'score']]",[],[],constituency_parsing,3,120
51,results,"The LSTM + A model trained on WSJ dataset only produced malformed trees for 25 of the 1700 sentences in our development set ( 1.5 % of all cases ) , and the model trained on full high - confidence dataset did this for 14 sentences ( 0.8 % ) .","[('trained on', (5, 7)), ('produced', (10, 11)), ('for', (13, 14)), ('of', (15, 16)), ('in', (19, 20)), ('for', (43, 44))]","[('LSTM + A model', (1, 5)), ('WSJ dataset', (7, 9)), ('malformed trees', (11, 13)), ('25', (14, 15)), ('1700 sentences', (17, 19)), ('our development set ( 1.5 % of all cases )', (20, 30)), ('full high - confidence dataset', (36, 41)), ('14 sentences ( 0.8 % )', (44, 50))]","[['LSTM + A model', 'trained on', 'WSJ dataset'], ['WSJ dataset', 'produced', 'malformed trees'], ['malformed trees', 'for', '25'], ['25', 'of', '1700 sentences'], ['1700 sentences', 'in', 'our development set ( 1.5 % of all cases )'], ['full high - confidence dataset', 'for', '14 sentences ( 0.8 % )']]",[],[],"[['Results', 'has', 'LSTM + A model']]",constituency_parsing,3,122
52,results,"The difference between the F 1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser , 1.7 for the baseline LSTM , and 0.7 for LSTM + A .","[('between', (2, 3)), ('on', (7, 8)), ('of', (9, 10)), ('for', (19, 20)), ('for', (24, 25)), ('for', (31, 32))]","[('difference', (1, 2)), ('F 1 score', (4, 7)), ('sentences', (8, 9)), ('length', (10, 11)), ('upto 30 and that upto 70', (11, 17)), ('1.3', (18, 19)), ('BerkeleyParser', (21, 22)), ('1.7', (23, 24)), ('baseline LSTM', (26, 28)), ('0.7', (30, 31)), ('LSTM + A', (32, 35))]","[['difference', 'between', 'F 1 score'], ['F 1 score', 'on', 'sentences'], ['sentences', 'of', 'length'], ['1.3', 'for', 'BerkeleyParser'], ['1.3', 'for', 'baseline LSTM'], ['1.3', 'for', '0.7'], ['1.7', 'for', 'baseline LSTM'], ['0.7', 'for', 'LSTM + A']]",[],[],"[['Results', 'has', 'difference']]",constituency_parsing,3,130
53,results,LSTM + A trained on the high - confidence corpus ( which only includes text from news ) achieved an F 1 score of 95.7 on QTB and 84.6 on WEB .,"[('on', (4, 5)), ('achieved', (18, 19)), ('of', (23, 24)), ('on', (25, 26))]","[('LSTM + A', (0, 3)), ('trained', (3, 4)), ('high - confidence corpus', (6, 10)), ('F 1 score', (20, 23)), ('95.7', (24, 25)), ('QTB', (26, 27)), ('84.6', (28, 29)), ('WEB', (30, 31))]","[['LSTM + A', 'on', 'high - confidence corpus'], ['trained', 'on', 'high - confidence corpus'], ['95.7', 'on', 'QTB'], ['high - confidence corpus', 'achieved', 'F 1 score'], ['F 1 score', 'of', '95.7'], ['F 1 score', 'of', '84.6'], ['95.7', 'on', 'QTB'], ['95.7', 'on', '84.6']]","[['LSTM + A', 'has', 'trained']]",[],"[['Results', 'has', 'LSTM + A']]",constituency_parsing,3,157
54,research-problem,Recent work has proposed several generative neural models for constituency parsing that achieve state - of - the - art results .,[],"[('constituency parsing', (9, 11))]",[],[],[],[],constituency_parsing,4,4
55,research-problem,Recent work on neural constituency parsing has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler .,[],"[('neural constituency parsing', (3, 6))]",[],[],[],[],constituency_parsing,4,10
56,ablation-analysis,"In this paper , we present experiments to isolate the degree to which each gain occurs for each of two state - of - the - art generative neural parsing models : the Recurrent Neural Network Grammar generative parser ( RG ) of , and the LSTM language modeling generative parser ( LM ) of .","[('present', (5, 6)), ('to isolate', (7, 9)), ('for', (16, 17))]","[('experiments', (6, 7)), ('degree', (10, 11)), ('gain', (14, 15)), ('each of two state - of - the - art generative neural parsing models', (17, 31)), ('the Recurrent Neural Network Grammar generative parser ( RG )', (32, 42)), ('LSTM language modeling generative parser ( LM )', (46, 54))]","[['experiments', 'to isolate', 'degree'], ['gain', 'for', 'each of two state - of - the - art generative neural parsing models']]","[['degree', 'has', 'gain'], ['each of two state - of - the - art generative neural parsing models', 'name', 'the Recurrent Neural Network Grammar generative parser ( RG )']]","[['Ablation analysis', 'present', 'experiments']]",[],constituency_parsing,4,35
57,approach,"In particular , we present and use a beam - based search procedure with an augmented state space that can search directly in the generative models , allowing us to explore A ?","[('with', (13, 14))]","[('beam - based search procedure', (8, 13)), ('augmented state space', (15, 18)), ('search', (20, 21)), ('generative models', (24, 26))]","[['beam - based search procedure', 'with', 'augmented state space']]",[],[],[],constituency_parsing,4,36
58,results,"We find that this is indeed the case : simply taking a weighted average of the scores of both models when selecting a parse from the base parser 's candidate list improves over using only the score of the generative model , in many cases substantially ( Section 3.2 ) .","[('taking', (10, 11)), ('of', (14, 15)), ('of', (17, 18)), ('when selecting', (20, 22)), ('from', (24, 25))]","[('weighted average', (12, 14)), ('scores', (16, 17)), ('both models', (18, 20)), ('parse', (23, 24)), (""base parser 's candidate list"", (26, 31))]","[['weighted average', 'of', 'scores'], ['scores', 'of', 'both models'], ['scores', 'of', 'both models'], ['scores', 'when selecting', 'parse'], ['both models', 'when selecting', 'parse'], ['parse', 'from', ""base parser 's candidate list""]]",[],"[['Results', 'taking', 'weighted average']]",[],constituency_parsing,4,43
59,experiments,Augmenting the candidate set,[],"[('Augmenting the candidate set', (0, 4))]",[],[],[],[],constituency_parsing,4,90
60,ablation-analysis,RG decreases performance from 93.45 F1 to 92.78 F1 on the development set .,"[('from', (3, 4)), ('to', (6, 7))]","[('RG', (0, 1)), ('decreases', (1, 2)), ('performance', (2, 3)), ('93.45 F1', (4, 6)), ('92.78 F1', (7, 9))]","[['performance', 'from', '93.45 F1'], ['93.45 F1', 'to', '92.78 F1']]","[['RG', 'has', 'decreases'], ['decreases', 'has', 'performance']]",[],"[['Ablation analysis', 'has', 'RG']]",constituency_parsing,4,98
61,experiments,Score combination,[],"[('Score combination', (0, 2))]",[],[],[],[],constituency_parsing,4,107
62,results,"We find that combining the scores of both models improves on using the score of either model alone , regardless of the source of candidates .","[('combining', (3, 4)), ('of', (6, 7)), ('using', (11, 12)), ('of', (14, 15))]","[('scores', (5, 6)), ('both models', (7, 9)), ('improves', (9, 10)), ('score', (13, 14)), ('either model alone', (15, 18))]","[['scores', 'of', 'both models'], ['score', 'of', 'either model alone'], ['improves', 'using', 'score'], ['score', 'of', 'either model alone']]",[],"[['Results', 'combining', 'scores']]",[],constituency_parsing,4,111
63,experiments,Strengthening model combination,[],"[('Strengthening model combination', (0, 3))]",[],[],[],[],constituency_parsing,4,119
64,ablation-analysis,"2 Combining candidates and scores from all three models ( row 9 ) , we obtain 93.94 F1 . :","[('Combining', (1, 2)), ('from', (5, 6)), ('obtain', (15, 16))]","[('candidates and scores', (2, 5)), ('all three models', (6, 9)), ('93.94 F1', (16, 18))]","[['candidates and scores', 'from', 'all three models'], ['candidates and scores', 'obtain', '93.94 F1'], ['all three models', 'obtain', '93.94 F1']]",[],[],[],constituency_parsing,4,123
65,baselines,"Ensembling Finally , we compare to another commonly used model combination method : ensembling multiple instances of the same model type trained from different random initializations .",[],"[('Ensembling', (0, 1))]",[],[],[],"[['Baselines', 'has', 'Ensembling']]",constituency_parsing,4,131
66,ablation-analysis,"Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .","[('using', (2, 3)), ('than', (14, 15)), ('rescoring', (15, 16))]","[('ensembled RD models', (5, 8)), ('lower', (13, 14)), ('single RD model', (17, 20))]","[['lower', 'rescoring', 'single RD model']]",[],"[['Ablation analysis', 'using', 'ensembled RD models']]",[],constituency_parsing,4,134
67,results,"In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .","[('In', (0, 1)), ('with', (6, 7)), ('achieves', (9, 10)), ('of', (15, 16))]","[('PTB setting', (2, 4)), ('ensembling', (5, 6)), ('score combination', (7, 9)), ('best over all result', (11, 15)), ('94.25', (16, 17))]","[['ensembling', 'with', 'score combination'], ['ensembling', 'achieves', 'best over all result'], ['score combination', 'achieves', 'best over all result'], ['best over all result', 'of', '94.25']]","[['PTB setting', 'has', 'ensembling']]",[],[],constituency_parsing,4,135
68,research-problem,In- Order Transition - based Constituent Parsing,[],"[('Transition - based Constituent Parsing', (2, 7))]",[],[],[],[],constituency_parsing,5,2
69,research-problem,Both bottom - up and top - down strategies have been used for neural transition - based constituent parsing .,[],"[('neural transition - based constituent parsing', (13, 19))]",[],[],[],[],constituency_parsing,5,4
70,model,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .","[('propose', (5, 6)), ('for', (10, 11)), ('mitigating', (14, 15)), ('of both', (16, 18)), ('by finding', (26, 28)), ('between', (30, 31))]","[('novel transition system', (7, 10)), ('constituent parsing', (11, 13)), ('issues', (15, 16)), ('bottom - up and top - down systems', (18, 26)), ('compromise', (29, 30)), ('bottom - up constituent information', (31, 36)), ('top - down lookahead information', (37, 42))]","[['novel transition system', 'for', 'constituent parsing'], ['novel transition system', 'mitigating', 'issues'], ['issues', 'of both', 'bottom - up and top - down systems'], ['bottom - up and top - down systems', 'by finding', 'compromise'], ['compromise', 'between', 'bottom - up constituent information'], ['compromise', 'between', 'top - down lookahead information']]",[],"[['Model', 'propose', 'novel transition system']]",[],constituency_parsing,5,32
71,code,We release our code at https://github.com/LeonCrashCode/InOrderParser .,[],"[('https://github.com/LeonCrashCode/InOrderParser', (5, 6))]",[],[],[],[],constituency_parsing,5,49
72,experiments,Reranking experiments,[],"[('Reranking experiments', (0, 2))]",[],[],[],[],constituency_parsing,5,131
73,results,The bottom - up system performs slightly better than the top - down system .,"[('performs', (5, 6)), ('than', (8, 9))]","[('bottom - up system', (1, 5)), ('slightly better', (6, 8)), ('top - down system', (10, 14))]","[['bottom - up system', 'performs', 'slightly better'], ['slightly better', 'than', 'top - down system']]",[],[],"[['Results', 'has', 'bottom - up system']]",constituency_parsing,5,133
74,results,The inorder system outperforms both the bottom - up and the top - down system .,"[('outperforms', (3, 4))]","[('inorder system', (1, 3)), ('both the bottom - up and the top - down system', (4, 15))]","[['inorder system', 'outperforms', 'both the bottom - up and the top - down system']]",[],[],"[['Results', 'has', 'inorder system']]",constituency_parsing,5,134
75,results,"We find that the bottom - up parser and the top - down parser have similar results under the greedy setting , and the in - order parser outperforms both of them .","[('find', (1, 2)), ('have', (14, 15)), ('under', (17, 18))]","[('the bottom - up parser and the top - down parser', (3, 14)), ('similar results', (15, 17)), ('greedy setting', (19, 21))]","[['the bottom - up parser and the top - down parser', 'have', 'similar results'], ['similar results', 'under', 'greedy setting']]","[['the bottom - up parser and the top - down parser', 'has', 'similar results']]","[['Results', 'find', 'the bottom - up parser and the top - down parser']]",[],constituency_parsing,5,136
76,results,English constituent results,[],"[('English constituent results', (0, 3))]",[],[],[],"[['Results', 'has', 'English constituent results']]",constituency_parsing,5,138
77,results,"With the fully - supervise setting 5 , the inorder parser outperforms the state - of - the - art discrete parser , the state - of - the - art neural parsers","[('With', (0, 1)), ('outperforms', (11, 12))]","[('fully - supervise setting', (2, 6)), ('inorder parser', (9, 11)), ('state - of - the - art discrete parser', (13, 22)), ('state - of - the - art neural parsers', (24, 33))]","[['inorder parser', 'outperforms', 'state - of - the - art discrete parser'], ['inorder parser', 'outperforms', 'state - of - the - art neural parsers']]","[['fully - supervise setting', 'has', 'inorder parser']]",[],[],constituency_parsing,5,140
78,results,Chinese dependency results,[],"[('Chinese dependency results', (0, 3))]",[],[],[],"[['Results', 'has', 'Chinese dependency results']]",constituency_parsing,5,145
79,results,"As shown in , by converting the results to dependencies 6 , our final model achieves the best results among transitionbased parsing , and obtains comparable results to the state - of - the - art graph - based models . 85.5 84.0 87.7 86.2","[('to', (8, 9)), ('achieves', (15, 16)), ('among', (19, 20)), ('obtains', (24, 25))]","[('final model', (13, 15)), ('best results', (17, 19)), ('transitionbased parsing', (20, 22)), ('comparable results', (25, 27)), ('state - of - the - art graph - based models', (29, 40))]","[['comparable results', 'to', 'state - of - the - art graph - based models'], ['final model', 'achieves', 'best results'], ['best results', 'among', 'transitionbased parsing'], ['final model', 'obtains', 'comparable results']]","[['final model', 'has', 'best results']]",[],[],constituency_parsing,5,146
80,experiments,Parsing as Language Modeling,[],"[('Parsing', (0, 1))]",[],[],[],[],constituency_parsing,6,2
81,research-problem,"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .",[],"[('syntactic parsing', (2, 4))]",[],[],[],[],constituency_parsing,6,4
82,research-problem,"Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 .",[],"[('deep learning syntactic parsing', (3, 7))]",[],[],[],[],constituency_parsing,6,7
83,model,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .","[('present', (14, 15)), ('with', (31, 32))]","[('neural - net parse reranker', (16, 21)), ('simple architecture', (34, 36))]",[],[],"[['Model', 'present', 'neural - net parse reranker']]",[],constituency_parsing,6,8
84,hyperparameters,"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50 .","[('with', (6, 7)), ('trained with', (11, 13)), ('through', (15, 16)), ('with', (17, 18))]","[('three LSTM layers', (3, 6)), ('1,500 units', (7, 9)), ('truncated backpropagation', (13, 15)), ('time', (16, 17)), ('mini-batch size', (18, 20)), ('20', (20, 21)), ('step size', (22, 24)), ('50', (24, 25))]","[['three LSTM layers', 'with', '1,500 units'], ['three LSTM layers', 'trained with', 'truncated backpropagation'], ['truncated backpropagation', 'through', 'time'], ['time', 'with', 'mini-batch size'], ['time', 'with', 'step size']]","[['mini-batch size', 'has', '20'], ['step size', 'has', '50']]",[],"[['Hyperparameters', 'has', 'three LSTM layers']]",constituency_parsing,6,51
85,hyperparameters,We initialize starting states with previous minibatch 's last hidden states .,"[('initialize', (1, 2)), ('with', (4, 5))]","[('starting states', (2, 4)), (""previous minibatch 's last hidden states"", (5, 11))]","[['starting states', 'with', ""previous minibatch 's last hidden states""]]",[],"[['Hyperparameters', 'initialize', 'starting states']]",[],constituency_parsing,6,52
86,hyperparameters,"The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .","[('initialized to be', (5, 8)), ('sampled from', (16, 18))]","[('forget gate bias', (1, 4)), ('one and the rest of model parameters', (8, 15)), ('U ( ? 0.05 , 0.05 )', (18, 25))]","[['forget gate bias', 'initialized to be', 'one and the rest of model parameters'], ['one and the rest of model parameters', 'sampled from', 'U ( ? 0.05 , 0.05 )']]","[['forget gate bias', 'has', 'one and the rest of model parameters']]",[],"[['Hyperparameters', 'has', 'forget gate bias']]",constituency_parsing,6,53
87,hyperparameters,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,"[('applied to', (2, 4)), ('when', (10, 11))]","[('Dropout', (0, 1)), ('non-recurrent connections', (4, 6)), ('gradients', (7, 8)), ('clipped', (9, 10)), ('norm', (12, 13)), ('bigger than 20', (14, 17))]","[['Dropout', 'applied to', 'non-recurrent connections'], ['clipped', 'when', 'norm']]","[['norm', 'has', 'bigger than 20']]",[],"[['Hyperparameters', 'has', 'Dropout']]",constituency_parsing,6,54
88,hyperparameters,The learning rate is 0.25 0.85 max where is an epoch number .,"[('is', (3, 4))]","[('learning rate', (1, 3)), ('0.25 0.85 max', (4, 7)), ('epoch number', (10, 12))]","[['learning rate', 'is', '0.25 0.85 max']]","[['learning rate', 'has', '0.25 0.85 max']]",[],"[['Hyperparameters', 'has', 'learning rate']]",constituency_parsing,6,55
89,hyperparameters,"For simplicity , we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax or noise contrastive estimation ( Gutmann and Hyvrinen , 2012 ) .","[('use', (4, 5)), ('over', (7, 8))]","[('vanilla softmax', (5, 7)), ('entire vocabulary', (9, 11))]","[['vanilla softmax', 'over', 'entire vocabulary']]",[],"[['Hyperparameters', 'use', 'vanilla softmax']]",[],constituency_parsing,6,56
90,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],constituency_parsing,6,72
91,results,"A single LSTM - LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .","[('with', (9, 10)), ('reaches', (14, 15)), ('with', (27, 28)), ('achieves', (32, 33))]","[('single LSTM - LM ( GS )', (1, 8)), ('Charniak ( GS )', (10, 14)), ('93.6', (15, 16)), ('ensemble of eight LSTM - LMs ( GS )', (18, 27)), ('Charniak ( GS )', (28, 32)), ('new state of the art', (34, 39)), ('93.8 F 1', (40, 43))]","[['single LSTM - LM ( GS )', 'with', 'Charniak ( GS )'], ['Charniak ( GS )', 'reaches', '93.6'], ['ensemble of eight LSTM - LMs ( GS )', 'with', 'Charniak ( GS )'], ['ensemble of eight LSTM - LMs ( GS )', 'achieves', 'new state of the art'], ['Charniak ( GS )', 'achieves', 'new state of the art']]","[['new state of the art', 'has', '93.8 F 1']]",[],[],constituency_parsing,6,77
92,results,"When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .","[('When', (0, 1)), ('converted to', (3, 5))]","[('trees', (1, 2)), ('Stanford dependencies', (5, 7)), ('5 UAS and LAS', (8, 12)), ('95.9 % and 94.1 %', (13, 18))]","[['trees', 'converted to', 'Stanford dependencies']]",[],"[['Results', 'When', 'trees']]",[],constituency_parsing,6,78
93,research-problem,What Do Recurrent Neural Network Grammars Learn About Syntax ?,[],"[('Recurrent Neural Network Grammars', (2, 6))]",[],[],[],[],constituency_parsing,7,2
94,research-problem,Recurrent neural network grammars ( RNNG ) area recently proposed probabilistic generative modeling family for natural language .,[],"[('Recurrent neural network grammars ( RNNG )', (0, 7))]",[],[],[],[],constituency_parsing,7,4
95,approach,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .",[],"[('RNNGs', (3, 4))]",[],[],[],[],constituency_parsing,7,12
96,approach,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,"[('manipulates', (2, 3)), ('of', (6, 7)), ('to test', (8, 10))]","[('inductive bias', (4, 6)), ('RNNGs', (7, 8)), ('linguistic hypotheses', (10, 12))]","[['inductive bias', 'of', 'RNNGs'], ['inductive bias', 'to test', 'linguistic hypotheses'], ['RNNGs', 'to test', 'linguistic hypotheses']]",[],"[['Approach', 'manipulates', 'inductive bias']]",[],constituency_parsing,7,21
97,approach,We begin with an ablation study to discover the importance of the composition function in 3 .,"[('begin with', (1, 3)), ('to discover', (6, 8)), ('of', (10, 11))]","[('ablation study', (4, 6)), ('importance', (9, 10)), ('composition function', (12, 14))]","[['ablation study', 'to discover', 'importance'], ['importance', 'of', 'composition function']]",[],"[['Approach', 'begin with', 'ablation study']]",[],constituency_parsing,7,22
98,approach,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .","[('augment', (6, 7)), ('with', (11, 12)), ('to incorporate', (25, 27))]","[('RNNG composition function', (8, 11)), ('novel gated attention mechanism', (13, 17)), ('more interpretability', (27, 29))]","[['RNNG composition function', 'with', 'novel gated attention mechanism'], ['novel gated attention mechanism', 'to incorporate', 'more interpretability']]","[['RNNG composition function', 'name', 'novel gated attention mechanism']]","[['Approach', 'augment', 'RNNG composition function']]",[],constituency_parsing,7,23
99,approach,"Using the GA - RNNG , we proceed by investigating the role that individual heads play in phrasal representation ( 5 ) and the role that nonterminal category labels play ( 6 ) .","[('Using', (0, 1)), ('investigating', (9, 10)), ('play in', (15, 17)), ('play', (29, 30))]","[('GA - RNNG', (2, 5)), ('role', (11, 12)), ('individual heads', (13, 15)), ('phrasal representation', (17, 19)), ('nonterminal category labels', (26, 29))]","[['GA - RNNG', 'investigating', 'role'], ['individual heads', 'play in', 'phrasal representation']]","[['role', 'has', 'individual heads']]","[['Approach', 'Using', 'GA - RNNG']]",[],constituency_parsing,7,24
100,results,"The RNNG with only a stack is the strongest of the ablations , and it even outperforms the "" full "" RNNG with all three data structures .","[('with', (2, 3)), ('of', (9, 10)), ('outperforms', (16, 17)), ('with', (22, 23))]","[('RNNG', (1, 2)), ('only a stack', (3, 6)), ('strongest', (8, 9)), ('ablations', (11, 12)), ('"" full "" RNNG', (18, 22)), ('all three data structures', (23, 27))]","[['RNNG', 'with', 'only a stack'], ['"" full "" RNNG', 'with', 'all three data structures'], ['strongest', 'of', 'ablations'], ['RNNG', 'outperforms', '"" full "" RNNG'], ['"" full "" RNNG', 'with', 'all three data structures']]",[],[],"[['Results', 'has', 'RNNG']]",constituency_parsing,7,88
101,results,Ablating the stack gives the worst among the new results .,"[('Ablating', (0, 1)), ('gives', (3, 4)), ('among', (6, 7))]","[('stack', (2, 3)), ('worst', (5, 6)), ('new results', (8, 10))]","[['stack', 'gives', 'worst'], ['worst', 'among', 'new results']]","[['stack', 'has', 'worst'], ['worst', 'has', 'new results']]","[['Results', 'Ablating', 'stack']]",[],constituency_parsing,7,89
102,results,"A similar performance degradation is seen in language modeling : the stack - only RNNG achieves the best performance , and ablating the stack is most harmful .","[('seen in', (5, 7)), ('achieves', (15, 16)), ('ablating', (21, 22))]","[('language modeling', (7, 9)), ('stack - only RNNG', (11, 15)), ('best performance', (17, 19)), ('stack', (23, 24)), ('most harmful', (25, 27))]","[['stack - only RNNG', 'achieves', 'best performance']]","[['language modeling', 'has', 'stack - only RNNG']]","[['Results', 'seen in', 'language modeling']]",[],constituency_parsing,7,93
103,results,"Indeed , modeling syntax without explicit composition ( the stackablated RNNG ) provides little benefit over a sequential LSTM language model .","[('modeling', (2, 3)), ('without', (4, 5)), ('provides', (12, 13)), ('over', (15, 16))]","[('syntax', (3, 4)), ('explicit composition', (5, 7)), ('little benefit', (13, 15)), ('sequential LSTM language model', (17, 21))]","[['syntax', 'without', 'explicit composition'], ['syntax', 'provides', 'little benefit'], ['little benefit', 'over', 'sequential LSTM language model']]",[],"[['Results', 'modeling', 'syntax']]",[],constituency_parsing,7,94
104,results,We remark that the stack - only results are the best published PTB results for both phrasestructure and dependency parsing among supervised models .,"[('for', (14, 15))]","[('stack - only results', (4, 8)), ('best published PTB results', (10, 14)), ('phrasestructure and dependency parsing', (16, 20))]","[['best published PTB results', 'for', 'phrasestructure and dependency parsing']]",[],[],[],constituency_parsing,7,95
105,results,Gated Attention RNNG,[],"[('Gated Attention RNNG', (0, 3))]",[],[],[],"[['Results', 'has', 'Gated Attention RNNG']]",constituency_parsing,7,96
106,results,"It is clear that the model outperforms the baseline RNNG with all three structures present and achieves competitive performance with the strongest , stack - only , RNNG variant .","[('clear that', (2, 4)), ('outperforms', (6, 7)), ('with', (10, 11)), ('achieves', (16, 17)), ('with', (19, 20))]","[('model', (5, 6)), ('baseline RNNG', (8, 10)), ('all three structures', (11, 14)), ('competitive performance', (17, 19)), ('strongest , stack - only , RNNG variant', (21, 29))]","[['model', 'outperforms', 'baseline RNNG'], ['baseline RNNG', 'with', 'all three structures'], ['competitive performance', 'with', 'strongest , stack - only , RNNG variant'], ['model', 'achieves', 'competitive performance'], ['competitive performance', 'with', 'strongest , stack - only , RNNG variant']]",[],"[['Results', 'clear that', 'model']]",[],constituency_parsing,7,134
107,results,Headedness in Phrases,[],"[('Headedness in Phrases', (0, 3))]",[],[],[],"[['Results', 'has', 'Headedness in Phrases']]",constituency_parsing,7,135
108,results,The model has a higher overlap with the conversion using Collins head rules ( 49.8 UAS ) rather than the Stanford head rules ( 40.4 UAS ) .,"[('with', (6, 7)), ('using', (9, 10)), ('than', (18, 19))]","[('model', (1, 2)), ('higher overlap', (4, 6)), ('conversion', (8, 9)), ('Collins head rules', (10, 13)), ('49.8 UAS', (14, 16)), ('Stanford head rules', (20, 23)), ('40.4 UAS', (24, 26))]","[['higher overlap', 'with', 'conversion'], ['conversion', 'using', 'Collins head rules']]","[['model', 'has', 'higher overlap'], ['Collins head rules', 'has', '49.8 UAS'], ['Stanford head rules', 'has', '40.4 UAS']]",[],"[['Results', 'has', 'model']]",constituency_parsing,7,173
109,results,"In general , the attention - based tree output has a high error rate ( ? 90 % ) when the dependent is a verb , since the constituent with the highest attention weight in a verb phrase is often the noun phrase instead of the verb , as discussed above .","[('when', (19, 20)), ('is', (22, 23))]","[('attention - based tree output', (4, 9)), ('high error rate ( ? 90 % )', (11, 19)), ('dependent', (21, 22)), ('verb', (24, 25))]","[['high error rate ( ? 90 % )', 'when', 'dependent'], ['dependent', 'is', 'verb']]","[['attention - based tree output', 'has', 'high error rate ( ? 90 % )']]",[],"[['Results', 'has', 'attention - based tree output']]",constituency_parsing,7,175
110,results,"The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .","[('for', (5, 6)), ('for', (17, 18)), ('with respect to', (29, 32))]","[('conversion accuracy', (1, 3)), ('better', (4, 5)), ('nouns', (6, 7)), ('? 50 % error', (8, 12)), ('much better', (15, 17)), ('determiners', (18, 19)), ('30 %', (20, 22)), ('particles', (24, 25)), ('6 %', (26, 28)), ('Collins head rules', (33, 36))]","[['better', 'for', 'nouns'], ['much better', 'for', 'determiners'], ['much better', 'for', 'particles']]","[['conversion accuracy', 'has', 'better'], ['determiners', 'has', '30 %'], ['particles', 'has', '6 %']]",[],"[['Results', 'has', 'conversion accuracy']]",constituency_parsing,7,176
111,ablation-analysis,The Role of Nonterminal Labels,[],"[('The Role of Nonterminal Labels', (0, 5))]",[],[],[],"[['Ablation analysis', 'has', 'The Role of Nonterminal Labels']]",constituency_parsing,7,183
112,results,"On test data ( with the usual split ) , the GA - RNNG achieves 94.2 % , while the U - GA - RNNG achieves 93.5 % .","[('On', (0, 1)), ('with', (4, 5)), ('achieves', (14, 15)), ('achieves', (25, 26))]","[('test data', (1, 3)), ('usual split', (6, 8)), ('GA - RNNG', (11, 14)), ('94.2 %', (15, 17)), ('U - GA - RNNG', (20, 25)), ('93.5 %', (26, 28))]","[['test data', 'with', 'usual split'], ['GA - RNNG', 'achieves', '94.2 %'], ['U - GA - RNNG', 'achieves', '93.5 %']]","[['test data', 'has', 'usual split'], ['usual split', 'has', 'GA - RNNG']]","[['Results', 'On', 'test data']]",[],constituency_parsing,7,192
113,research-problem,Constituency Parsing with a Self - Attentive Encoder,[],"[('Constituency Parsing', (0, 2))]",[],[],[],[],constituency_parsing,8,2
114,model,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .","[('introduce', (5, 6)), ('combines', (9, 10)), ('built using', (12, 14)), ('with', (21, 22)), ('customized for', (24, 26))]","[('parser', (7, 8)), ('encoder', (11, 12)), ('self - attentive architecture', (17, 21)), ('decoder', (23, 24)), ('parsing', (26, 27))]","[['parser', 'combines', 'encoder'], ['self - attentive architecture', 'with', 'decoder'], ['decoder', 'customized for', 'parsing']]",[],"[['Model', 'introduce', 'parser']]",[],constituency_parsing,8,16
115,model,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .","[('present', (2, 3)), ('uses', (9, 10)), ('performs', (15, 16)), ('than', (17, 18)), ('if', (21, 22))]","[('a version of our model', (3, 8)), ('character LSTM', (11, 13)), ('better', (16, 17)), ('other lexical representationseven', (18, 21)), ('word embeddings', (22, 24)), ('removed', (25, 26))]","[['a version of our model', 'uses', 'character LSTM'], ['character LSTM', 'performs', 'better'], ['better', 'than', 'other lexical representationseven'], ['better', 'if', 'word embeddings'], ['other lexical representationseven', 'if', 'word embeddings']]","[['word embeddings', 'has', 'removed']]","[['Model', 'present', 'a version of our model']]",[],constituency_parsing,8,24
116,experiments,6 Results,[],"[('Results', (1, 2))]",[],[],[],[],constituency_parsing,8,163
117,experiments,English ( WSJ ),[],"[('English ( WSJ )', (0, 4))]",[],[],[],[],constituency_parsing,8,164
118,experiments,"The test score of 93.55 F1 for our CharLSTM parser exceeds the previous best numbers for single - system parsers trained on the Penn Treebank ( without the use of any external data , such as pre-trained word embeddings ) .","[('of', (3, 4)), ('for', (6, 7)), ('exceeds', (10, 11)), ('for', (15, 16)), ('trained on', (20, 22))]","[('test score', (1, 3)), ('93.55 F1', (4, 6)), ('CharLSTM parser', (8, 10)), ('previous best numbers', (12, 15)), ('single - system parsers', (16, 20)), ('Penn Treebank', (23, 25))]","[['test score', 'of', '93.55 F1'], ['93.55 F1', 'for', 'CharLSTM parser'], ['previous best numbers', 'for', 'single - system parsers'], ['CharLSTM parser', 'exceeds', 'previous best numbers'], ['previous best numbers', 'for', 'single - system parsers'], ['single - system parsers', 'trained on', 'Penn Treebank']]",[],[],[],constituency_parsing,8,168
119,experiments,"When our parser is augmented with ELMo word representations , it achieves a new state - of - the - art score of 95.13 F1 on the WSJ test set .","[('augmented with', (4, 6)), ('achieves', (11, 12)), ('of', (22, 23)), ('on', (25, 26))]","[('our parser', (1, 3)), ('ELMo word representations', (6, 9)), ('new state - of - the - art score', (13, 22)), ('95.13 F1', (23, 25)), ('WSJ test set', (27, 30))]","[['our parser', 'augmented with', 'ELMo word representations'], ['our parser', 'achieves', 'new state - of - the - art score'], ['ELMo word representations', 'achieves', 'new state - of - the - art score'], ['new state - of - the - art score', 'of', '95.13 F1'], ['95.13 F1', 'on', 'WSJ test set']]",[],[],[],constituency_parsing,8,169
120,experiments,Multilingual ( SPMRL ),[],"[('Multilingual ( SPMRL )', (0, 4))]",[],[],[],[],constituency_parsing,8,171
121,experiments,"Development set results show that the addition of word embeddings to a model that uses a character LSTM has a mixed effect : it improves performance for some languages , but hurts for others .","[('show', (3, 4)), ('of', (7, 8)), ('to', (10, 11)), ('uses', (14, 15)), ('for', (26, 27)), ('for', (32, 33))]","[('Development set results', (0, 3)), ('addition', (6, 7)), ('word embeddings', (8, 10)), ('model', (12, 13)), ('character LSTM', (16, 18)), ('mixed effect', (20, 22)), ('improves', (24, 25)), ('performance', (25, 26)), ('some languages', (27, 29)), ('hurts', (31, 32)), ('others', (33, 34))]","[['Development set results', 'show', 'addition'], ['addition', 'of', 'word embeddings'], ['word embeddings', 'to', 'model'], ['word embeddings', 'uses', 'character LSTM'], ['model', 'uses', 'character LSTM'], ['performance', 'for', 'some languages'], ['hurts', 'for', 'others']]","[['Development set results', 'has', 'addition'], ['mixed effect', 'has', 'improves'], ['improves', 'has', 'performance']]",[],[],constituency_parsing,8,178
122,experiments,"On 8 of the 9 languages , our test set result exceeds the previous best - published numbers from any system we are aware of .","[('On', (0, 1)), ('exceeds', (11, 12))]","[('8 of the 9 languages', (1, 6)), ('our test set result', (7, 11)), ('previous best - published numbers', (13, 18))]","[['our test set result', 'exceeds', 'previous best - published numbers']]","[['8 of the 9 languages', 'has', 'our test set result']]",[],[],constituency_parsing,8,180
123,research-problem,Improving Coreference Resolution by Learning Entity - Level Distributed Representations,[],"[('Coreference Resolution', (1, 3))]",[],[],[],[],coreference_resolution,0,2
124,model,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .","[('train', (6, 7)), ('to build', (11, 13)), ('of', (15, 16)), ('of', (17, 18))]","[('deep neural network', (8, 11)), ('distributed representations', (13, 15)), ('pairs', (16, 17)), ('coreference clusters', (18, 20))]","[['deep neural network', 'to build', 'distributed representations'], ['distributed representations', 'of', 'pairs'], ['pairs', 'of', 'coreference clusters'], ['pairs', 'of', 'coreference clusters']]",[],"[['Model', 'train', 'deep neural network']]",[],coreference_resolution,0,16
125,model,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .","[('captures', (1, 2)), ('with', (6, 7)), ('of', (10, 11)), ('instead of', (15, 17)), ('of', (20, 21))]","[('entity - level information', (2, 6)), ('large number', (8, 10)), ('learned , continuous features', (11, 15)), ('small number', (18, 20)), ('hand - crafted categorical ones', (21, 26))]","[['entity - level information', 'with', 'large number'], ['large number', 'of', 'learned , continuous features'], ['small number', 'of', 'hand - crafted categorical ones'], ['learned , continuous features', 'instead of', 'small number'], ['small number', 'instead of', 'hand - crafted categorical ones'], ['small number', 'of', 'hand - crafted categorical ones']]",[],"[['Model', 'captures', 'entity - level information']]",[],coreference_resolution,0,17
126,model,"Using the cluster - pair representations , our network learns when combining two coreference clusters is desirable .","[('Using', (0, 1)), ('learns', (9, 10))]","[('cluster - pair representations', (2, 6)), ('our network', (7, 9)), ('when combining two coreference clusters', (10, 15)), ('desirable', (16, 17))]","[['our network', 'learns', 'when combining two coreference clusters']]","[['cluster - pair representations', 'has', 'our network']]","[['Model', 'Using', 'cluster - pair representations']]",[],coreference_resolution,0,18
127,model,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .","[('At', (0, 1)), ('builds up', (4, 6)), ('starting with', (10, 12)), ('in', (14, 15)), ('merging', (20, 21)), ('pair of', (22, 24))]","[('test time', (1, 3)), ('coreference clusters', (6, 8)), ('incrementally', (8, 9)), ('each mention', (12, 14)), ('its own cluster', (15, 18)), ('clusters', (24, 25)), ('each step', (25, 27))]","[['test time', 'builds up', 'coreference clusters'], ['coreference clusters', 'starting with', 'each mention'], ['each mention', 'in', 'its own cluster']]","[['coreference clusters', 'has', 'incrementally'], ['clusters', 'has', 'each step']]","[['Model', 'At', 'test time']]",[],coreference_resolution,0,19
128,model,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,"[('makes', (1, 2)), ('with', (4, 5)), ('combines', (15, 16)), ('of', (18, 19))]","[('these decisions', (2, 4)), ('novel easy - first cluster - ranking procedure', (6, 14)), ('strengths', (17, 18)), ('cluster - ranking ( Rahman and and easy - first coreference algorithms', (19, 31))]","[['these decisions', 'with', 'novel easy - first cluster - ranking procedure'], ['novel easy - first cluster - ranking procedure', 'combines', 'strengths'], ['strengths', 'of', 'cluster - ranking ( Rahman and and easy - first coreference algorithms']]","[['these decisions', 'has', 'novel easy - first cluster - ranking procedure']]","[['Model', 'makes', 'these decisions']]",[],coreference_resolution,0,20
129,model,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,"[('using', (4, 5)), ('inspired by', (12, 14)), ('to train', (15, 17))]","[('learning - to - search algorithm', (6, 12)), ('SEARN', (14, 15)), ('our neural network', (17, 20))]","[['learning - to - search algorithm', 'inspired by', 'SEARN'], ['SEARN', 'to train', 'our neural network']]",[],"[['Model', 'using', 'learning - to - search algorithm']]",[],coreference_resolution,0,22
130,model,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,"[('learn', (6, 7)), ('available from', (14, 16)), ('lead to', (28, 30))]","[('which action', (7, 9)), ('current state', (17, 19)), ('partially completed coreference clustering', (21, 25)), ('high - scoring coreference partition', (31, 36))]","[['which action', 'available from', 'current state'], ['which action', 'lead to', 'high - scoring coreference partition']]","[['current state', 'name', 'partially completed coreference clustering']]","[['Model', 'learn', 'which action']]",[],coreference_resolution,0,23
131,results,Our mention - ranking model surpasses all previous systems .,"[('surpasses', (5, 6))]","[('Our mention - ranking model', (0, 5)), ('all previous systems', (6, 9))]","[['Our mention - ranking model', 'surpasses', 'all previous systems']]",[],[],[],coreference_resolution,0,229
132,results,"The cluster - ranking model improves results further across both languages and all evaluation metrics , demonstrating the utility of incorporating entity - level information .","[('improves', (5, 6)), ('across', (8, 9))]","[('cluster - ranking model', (1, 5)), ('results further', (6, 8)), ('both languages and all evaluation metrics', (9, 15))]","[['cluster - ranking model', 'improves', 'results further'], ['results further', 'across', 'both languages and all evaluation metrics']]","[['cluster - ranking model', 'has', 'results further']]",[],"[['Results', 'has', 'cluster - ranking model']]",coreference_resolution,0,231
133,research-problem,End - to - end Deep Reinforcement Learning Based Coreference Resolution,[],"[('Coreference Resolution', (9, 11))]",[],[],[],[],coreference_resolution,1,2
134,model,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .","[('propose', (5, 6)), ('to resolve', (17, 19))]","[('goal - directed endto - end deep reinforcement learning framework', (7, 17)), ('coreference', (19, 20))]","[['goal - directed endto - end deep reinforcement learning framework', 'to resolve', 'coreference']]",[],"[['Model', 'propose', 'goal - directed endto - end deep reinforcement learning framework']]",[],coreference_resolution,1,23
135,model,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .","[('leverage', (3, 4)), ('as', (8, 9)), ('includes', (14, 15)), ('scoring', (19, 20)), ('generating', (25, 26)), ('from', (35, 36)), ('to', (39, 40))]","[('neural architecture', (5, 7)), ('our policy network', (9, 12)), ('learning', (15, 16)), ('span representation', (16, 18)), ('potential entity mentions', (20, 23)), ('probability distribution', (27, 29)), ('over all possible coreference linking actions', (29, 35)), ('current mention', (37, 39)), ('antecedents', (41, 42))]","[['neural architecture', 'as', 'our policy network'], ['our policy network', 'includes', 'learning'], ['span representation', 'scoring', 'potential entity mentions'], ['our policy network', 'generating', 'probability distribution'], ['learning', 'generating', 'probability distribution'], ['over all possible coreference linking actions', 'from', 'current mention'], ['current mention', 'to', 'antecedents']]","[['neural architecture', 'name', 'our policy network'], ['learning', 'has', 'span representation'], ['probability distribution', 'has', 'over all possible coreference linking actions']]","[['Model', 'leverage', 'neural architecture']]",[],coreference_resolution,1,24
136,model,"Once a sequence of linking actions are made , our reward function is used to measure how good the generated coreference clusters are , which is directly related to coreference evaluation metrics .","[('of', (3, 4)), ('used to', (13, 15)), ('measure', (15, 16)), ('directly related to', (26, 29))]","[('sequence', (2, 3)), ('linking actions', (4, 6)), ('our reward function', (9, 12)), ('how good', (16, 18)), ('generated coreference clusters', (19, 22)), ('coreference evaluation metrics', (29, 32))]","[['sequence', 'of', 'linking actions'], ['our reward function', 'measure', 'how good'], ['how good', 'directly related to', 'coreference evaluation metrics'], ['generated coreference clusters', 'directly related to', 'coreference evaluation metrics']]","[['sequence', 'has', 'linking actions'], ['how good', 'has', 'generated coreference clusters']]",[],"[['Model', 'has', 'sequence']]",coreference_resolution,1,25
137,model,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .","[('introduce', (3, 4)), ('to encourage', (8, 10)), ('prevent', (12, 13)), ('from', (15, 16)), ('to', (18, 19))]","[('entropy regularization term', (5, 8)), ('exploration', (10, 11)), ('policy', (14, 15)), ('prematurely converging', (16, 18)), ('bad local optimum', (20, 23))]","[['entropy regularization term', 'to encourage', 'exploration'], ['entropy regularization term', 'prevent', 'policy'], ['policy', 'from', 'prematurely converging'], ['prematurely converging', 'to', 'bad local optimum']]",[],"[['Model', 'introduce', 'entropy regularization term']]",[],coreference_resolution,1,26
138,model,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .","[('update', (3, 4)), ('based on', (9, 11)), ('associated with', (13, 15)), ('of', (16, 17)), ('computed on', (22, 24))]","[('regularized policy network parameters', (5, 9)), ('rewards', (12, 13)), ('sequences', (15, 16)), ('sampled actions', (17, 19)), ('whole input document', (25, 28))]","[['regularized policy network parameters', 'based on', 'rewards'], ['rewards', 'associated with', 'sequences'], ['sequences', 'of', 'sampled actions'], ['sampled actions', 'computed on', 'whole input document']]",[],"[['Model', 'update', 'regularized policy network parameters']]",[],coreference_resolution,1,27
139,experiments,Experiments,[],"[('Experiments', (0, 1))]",[],[],[],[],coreference_resolution,1,101
140,hyperparameters,"First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .","[('pretrain', (3, 4)), ('for', (11, 12)), ('use', (17, 18)), ('for', (21, 22))]","[('our model', (4, 6)), ('around 200 K steps', (12, 16)), ('learned parameters', (19, 21)), ('initialization', (22, 23))]","[['learned parameters', 'for', 'initialization'], ['our model', 'use', 'learned parameters'], ['learned parameters', 'for', 'initialization']]",[],"[['Hyperparameters', 'pretrain', 'our model']]",[],coreference_resolution,1,104
141,hyperparameters,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .","[('set', (3, 4)), ('of', (6, 7)), ('tune', (14, 15)), ('in', (20, 21)), ('set', (37, 38))]","[('number', (5, 6)), ('sampled trajectories', (7, 9)), ('regularization parameter', (16, 18)), ('{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }', (21, 36)), ('10 ? 4', (40, 43))]","[['number', 'of', 'sampled trajectories'], ['sampled trajectories', 'tune', 'regularization parameter'], ['regularization parameter', 'in', '{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }'], ['regularization parameter', 'set', '10 ? 4']]","[['number', 'has', 'sampled trajectories'], ['regularization parameter', 'has', '{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }']]","[['Hyperparameters', 'set', 'number']]",[],coreference_resolution,1,105
142,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],coreference_resolution,1,109
143,results,"Built on top of the model in but excluding ELMo , our base reinforced model improves the average F 1 score around 2 points ( statistical significant t- test with p < 0.05 ) compared with .","[('improves', (15, 16)), ('around', (21, 22)), ('with', (29, 30))]","[('our base reinforced model', (11, 15)), ('average F 1 score', (17, 21)), ('2 points', (22, 24)), ('statistical significant t- test', (25, 29)), ('p < 0.05', (30, 33))]","[['our base reinforced model', 'improves', 'average F 1 score'], ['average F 1 score', 'around', '2 points'], ['statistical significant t- test', 'with', 'p < 0.05']]",[],[],"[['Results', 'has', 'our base reinforced model']]",coreference_resolution,1,115
144,results,"Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .","[('using', (4, 5)), ('to encourage', (7, 9)), ('improve', (11, 12)), ('by', (14, 15))]","[('entropy regularization', (5, 7)), ('exploration', (9, 10)), ('result', (13, 14)), ('1 point', (15, 17))]","[['entropy regularization', 'to encourage', 'exploration'], ['entropy regularization', 'improve', 'result'], ['exploration', 'improve', 'result'], ['result', 'by', '1 point']]",[],[],[],coreference_resolution,1,117
145,results,"Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .","[('introducing', (2, 3)), ('to', (9, 10)), ('boosts', (15, 16))]","[('context - dependent ELMo embedding', (4, 9)), ('our base model', (10, 13)), ('performance', (17, 18))]","[['context - dependent ELMo embedding', 'to', 'our base model']]",[],"[['Results', 'introducing', 'context - dependent ELMo embedding']]",[],coreference_resolution,1,118
146,results,"Overall , our full model achieves the state - of the - art performance of 73.8 % F1 - score when using ELMo and entropy regularization ( compared to models marked with * in , and our approach simultaneously obtains the best F1 -score of 70.5 % when using fixed word embedding only .","[('achieves', (5, 6)), ('of', (14, 15)), ('using', (21, 22))]","[('our full model', (2, 5)), ('state - of the - art performance', (7, 14)), ('73.8 % F1 - score', (15, 20)), ('ELMo and entropy regularization', (22, 26))]","[['our full model', 'achieves', 'state - of the - art performance'], ['state - of the - art performance', 'of', '73.8 % F1 - score'], ['73.8 % F1 - score', 'using', 'ELMo and entropy regularization']]",[],[],"[['Results', 'has', 'our full model']]",coreference_resolution,1,120
147,research-problem,Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning .,[],"[('Coreference resolution', (0, 2))]",[],[],[],[],coreference_resolution,2,4
148,approach,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .","[('explore using', (5, 7)), ('to directly optimize', (12, 15)), ('for', (18, 19))]","[('two variants of reinforcement learning', (7, 12)), ('coreference system', (16, 18)), ('coreference evaluation metrics', (19, 22))]","[['two variants of reinforcement learning', 'to directly optimize', 'coreference system'], ['coreference system', 'for', 'coreference evaluation metrics']]",[],"[['Approach', 'explore using', 'two variants of reinforcement learning']]",[],coreference_resolution,2,15
149,approach,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .","[('modify', (4, 5)), ('by incorporating', (11, 13)), ('associated with', (15, 17)), ('into', (20, 21))]","[('max-margin coreference objective', (6, 9)), ('reward', (14, 15)), ('each coreference decision', (17, 20)), (""loss 's slack rescaling"", (22, 26))]","[['max-margin coreference objective', 'by incorporating', 'reward'], ['reward', 'associated with', 'each coreference decision'], ['reward', 'into', ""loss 's slack rescaling""]]",[],"[['Approach', 'modify', 'max-margin coreference objective']]",[],coreference_resolution,2,16
150,experiments,We also test the REINFORCE policy gradient algorithm .,"[('test', (2, 3))]","[('REINFORCE policy gradient algorithm', (4, 8))]",[],[],[],[],coreference_resolution,2,17
151,approach,Our model is a neural mention - ranking model .,[],"[('Our model', (0, 2)), ('neural mention - ranking model', (4, 9))]",[],"[['Our model', 'has', 'neural mention - ranking model']]",[],"[['Approach', 'has', 'Our model']]",coreference_resolution,2,18
152,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],coreference_resolution,2,108
153,results,"We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .","[('find that', (1, 3)), ('does', (4, 5)), ('than', (7, 8)), ('performs', (15, 16)), ('on', (20, 21))]","[('REINFORCE', (3, 4)), ('slightly better', (5, 7)), ('heuristic loss', (9, 11)), ('reward rescaling', (13, 15)), ('significantly better', (16, 18)), ('both languages', (21, 23))]","[['REINFORCE', 'does', 'slightly better'], ['slightly better', 'than', 'heuristic loss'], ['reward rescaling', 'performs', 'significantly better'], ['significantly better', 'on', 'both languages']]","[['REINFORCE', 'has', 'slightly better']]","[['Results', 'find that', 'REINFORCE']]",[],coreference_resolution,2,110
154,results,"The reward - rescaled max - margin loss combines the best of both worlds , resulting in superior performance .","[('combines', (8, 9)), ('resulting in', (15, 17))]","[('reward - rescaled max - margin loss', (1, 8)), ('best of both worlds', (10, 14)), ('superior performance', (17, 19))]","[['reward - rescaled max - margin loss', 'combines', 'best of both worlds'], ['best of both worlds', 'resulting in', 'superior performance']]",[],[],"[['Results', 'has', 'reward - rescaled max - margin loss']]",coreference_resolution,2,115
155,research-problem,Higher - order Coreference Resolution with Coarse - to - fine Inference,[],"[('Coreference Resolution', (3, 5))]",[],[],[],[],coreference_resolution,3,2
156,approach,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,"[('introduce', (1, 2)), ('of', (4, 5)), ('uses', (10, 11)), ('in', (17, 18))]","[('approximation', (3, 4)), ('higher - order inference', (5, 9)), ('span - ranking architecture', (12, 16)), ('an iterative manner', (18, 21))]","[['approximation', 'of', 'higher - order inference'], ['higher - order inference', 'uses', 'span - ranking architecture'], ['span - ranking architecture', 'in', 'an iterative manner']]",[],"[['Approach', 'introduce', 'approximation']]",[],coreference_resolution,3,15
157,approach,"At each iteration , the antecedent distribution is used as an attention mechanism to optionally update existing span representations , enabling later corefer - Speaker 1 : U m and think that is what 's - Go ahead Linda .","[('At', (0, 1)), ('used as', (8, 10)), ('to optionally update', (13, 16)), ('enabling', (20, 21))]","[('each iteration', (1, 3)), ('antecedent distribution', (5, 7)), ('attention mechanism', (11, 13)), ('existing span representations', (16, 19)), ('later corefer', (21, 23))]","[['antecedent distribution', 'used as', 'attention mechanism'], ['attention mechanism', 'to optionally update', 'existing span representations'], ['existing span representations', 'enabling', 'later corefer']]","[['each iteration', 'has', 'antecedent distribution']]","[['Approach', 'At', 'each iteration']]",[],coreference_resolution,3,16
158,approach,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .","[('To alleviate', (0, 2)), ('from', (4, 5)), ('propose', (13, 14)), ('learned with', (21, 23))]","[('computational challenges', (2, 4)), ('higher - order inference', (6, 10)), ('coarseto - fine approach', (15, 19)), ('single endto - end objective', (24, 29))]","[['computational challenges', 'from', 'higher - order inference'], ['higher - order inference', 'propose', 'coarseto - fine approach'], ['coarseto - fine approach', 'learned with', 'single endto - end objective']]","[['computational challenges', 'has', 'higher - order inference']]","[['Approach', 'To alleviate', 'computational challenges']]",[],coreference_resolution,3,19
159,approach,We introduce a less accurate but more efficient coarse factor in the pairwise scoring function .,"[('in', (10, 11))]","[('less accurate', (3, 5)), ('more efficient coarse factor', (6, 10)), ('pairwise scoring function', (12, 15))]","[['more efficient coarse factor', 'in', 'pairwise scoring function']]",[],[],[],coreference_resolution,3,20
160,approach,This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor .,"[('enables', (3, 4)), ('during', (8, 9)), ('reduces', (11, 12))]","[('additional factor', (1, 3)), ('extra pruning step', (5, 8)), ('inference', (9, 10)), ('number of antecedents', (13, 16))]","[['additional factor', 'enables', 'extra pruning step'], ['extra pruning step', 'during', 'inference'], ['additional factor', 'reduces', 'number of antecedents']]",[],[],"[['Approach', 'has', 'additional factor']]",coreference_resolution,3,21
161,approach,"Intuitively , the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function .","[('cheaply computes', (4, 6)), ('of', (9, 10)), ('before applying', (12, 14))]","[('rough sketch', (7, 9)), ('likely antecedents', (10, 12)), ('more expensive scoring function', (15, 19))]","[['rough sketch', 'of', 'likely antecedents'], ['rough sketch', 'before applying', 'more expensive scoring function']]",[],"[['Approach', 'cheaply computes', 'rough sketch']]",[],coreference_resolution,3,22
162,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],coreference_resolution,3,112
163,results,"The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .","[('augmented with', (13, 15)), ('achieves', (22, 23))]","[('baseline', (1, 2)), ('span - ranking model', (8, 12)), ('both ELMo and hyperparameter tuning', (15, 20)), ('72.3 F1', (23, 25))]","[['span - ranking model', 'augmented with', 'both ELMo and hyperparameter tuning'], ['span - ranking model', 'achieves', '72.3 F1']]","[['baseline', 'has', 'span - ranking model']]",[],"[['Results', 'has', 'baseline']]",coreference_resolution,3,118
164,results,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .","[('achieves', (3, 4)), ('setting', (7, 8)), ('for', (14, 15))]","[('Our full approach', (0, 3)), ('73.0 F1', (4, 6)), ('new state of the art', (9, 14)), ('coreference resolution', (15, 17))]","[['Our full approach', 'achieves', '73.0 F1'], ['73.0 F1', 'setting', 'new state of the art'], ['new state of the art', 'for', 'coreference resolution']]",[],[],"[['Results', 'has', 'Our full approach']]",coreference_resolution,3,119
165,results,"Despite using far less computation , it outperforms the baseline because the coarse scores s c ( i , j ) can be computed for all antecedents , enabling the model to potentially predict a coreference link between any two spans in the document .","[('Despite using', (0, 2)), ('outperforms', (7, 8))]","[('far less computation', (2, 5)), ('baseline', (9, 10))]","[['far less computation', 'outperforms', 'baseline']]","[['far less computation', 'has', 'baseline']]","[['Results', 'Despite using', 'far less computation']]",[],coreference_resolution,3,121
166,results,"As a result , we observe a much higher recall when adopting the coarse - to - fine approach .","[('observe', (5, 6)), ('when adopting', (10, 12))]","[('much higher recall', (7, 10)), ('coarse - to - fine approach', (13, 19))]","[['much higher recall', 'when adopting', 'coarse - to - fine approach']]",[],[],[],coreference_resolution,3,122
167,results,We also observe further improvement by including the second - order inference ( Section 3 ) .,"[('by including', (5, 7))]","[('further improvement', (3, 5)), ('second - order inference', (8, 12))]","[['further improvement', 'by including', 'second - order inference']]",[],[],[],coreference_resolution,3,123
168,research-problem,A Mention - Ranking Model for Abstract Anaphora Resolution,[],"[('Abstract Anaphora Resolution', (6, 9))]",[],[],[],[],coreference_resolution,4,2
169,research-problem,"Current research in anaphora ( or coreference ) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real Leo Born , Juri Opitz and Anette Frank contributed equally to this work .",[],"[('anaphora ( or coreference ) resolution', (3, 9))]",[],[],[],[],coreference_resolution,4,15
170,model,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .","[('inspired by', (3, 5)), ('for', (10, 11)), ('combines', (14, 15)), ('for learning', (21, 23)), ('between', (24, 25))]","[('mention - ranking model', (6, 10)), ('coreference resolution', (11, 13)), ('Siamese Net', (18, 20)), ('similarity', (23, 24)), ('sentences', (25, 26))]","[['mention - ranking model', 'for', 'coreference resolution'], ['Siamese Net', 'for learning', 'similarity'], ['similarity', 'between', 'sentences']]",[],"[['Model', 'inspired by', 'mention - ranking model']]",[],coreference_resolution,4,28
171,model,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .","[('Given', (0, 1)), ('in', (6, 7)), ('learns', (42, 43)), ('for', (44, 45))]","[('anaphoric sentence', (2, 4)), ('candidate antecedent', (13, 15)), ('LSTM - Siamese Net', (38, 42)), ('representations', (43, 44)), ('the candidate and the anaphoric sentence', (45, 51)), ('shared space', (53, 55))]","[['LSTM - Siamese Net', 'learns', 'representations'], ['representations', 'for', 'the candidate and the anaphoric sentence']]",[],"[['Model', 'Given', 'anaphoric sentence']]",[],coreference_resolution,4,29
172,model,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,"[('combined into', (3, 5)), ('to calculate', (9, 11)), ('characterizes', (14, 15))]","[('representations', (1, 2)), ('joint representation', (6, 8)), ('score', (12, 13)), ('relation', (16, 17))]","[['representations', 'combined into', 'joint representation'], ['joint representation', 'to calculate', 'score'], ['score', 'characterizes', 'relation']]",[],[],"[['Model', 'has', 'representations']]",coreference_resolution,4,30
173,model,The learned score is used to select the highest - scoring antecedent candidate for the given anaphoric sentence and hence its anaphor .,"[('to select', (5, 7)), ('for', (13, 14)), ('hence', (19, 20))]","[('learned score', (1, 3)), ('highest - scoring antecedent candidate', (8, 13)), ('given anaphoric sentence', (15, 18)), ('anaphor', (21, 22))]","[['learned score', 'to select', 'highest - scoring antecedent candidate'], ['highest - scoring antecedent candidate', 'for', 'given anaphoric sentence'], ['highest - scoring antecedent candidate', 'hence', 'anaphor']]",[],[],"[['Model', 'has', 'learned score']]",coreference_resolution,4,31
174,model,We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL .,"[('consider', (1, 2)), ('at', (4, 5)), ('provide', (8, 9)), ('of', (11, 12)), ('of', (14, 15)), ('of', (20, 21)), ('to characterize', (30, 32)), ('for', (40, 41)), ('in', (45, 46))]","[('one anaphor', (2, 4)), ('a time', (5, 7)), ('embedding', (10, 11)), ('context', (13, 14)), ('anaphor', (16, 17)), ('head', (22, 23)), ('anaphoric phrase', (25, 27)), ('each individual anaphorsimilar', (32, 35)), ('individuating', (41, 42)), ('multiply occurring predicates', (42, 45)), ('SRL', (46, 47))]","[['one anaphor', 'at', 'a time'], ['embedding', 'of', 'context'], ['context', 'of', 'anaphor'], ['context', 'of', 'anaphor'], ['head', 'of', 'anaphoric phrase'], ['multiply occurring predicates', 'in', 'SRL']]","[['individuating', 'has', 'multiply occurring predicates']]","[['Model', 'consider', 'one anaphor']]",[],coreference_resolution,4,32
175,model,With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent .,"[('show', (4, 5)), ('learns', (8, 9)), ('between', (11, 12)), ('in', (14, 15)), ('and', (18, 19))]","[('model', (7, 8)), ('relation', (10, 11)), ('anaphor', (13, 14)), ('anaphoric sentence', (16, 18)), ('antecedent', (20, 21))]","[['model', 'learns', 'relation'], ['relation', 'between', 'anaphor'], ['anaphor', 'in', 'anaphoric sentence'], ['anaphor', 'in', 'antecedent'], ['relation', 'and', 'antecedent']]",[],"[['Model', 'show', 'model']]",[],coreference_resolution,4,33
176,model,It produces large amounts of instances and is easily adaptable to other languages .,"[('produces', (1, 2)), ('of', (4, 5)), ('to', (10, 11))]","[('large amounts', (2, 4)), ('instances', (5, 6)), ('easily adaptable', (8, 10)), ('other languages', (11, 13))]","[['large amounts', 'of', 'instances'], ['easily adaptable', 'to', 'other languages']]",[],"[['Model', 'produces', 'large amounts']]",[],coreference_resolution,4,36
177,code,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora .,[],"[('https://github.com/amarasovic / neural-abstract-anaphora', (17, 20))]",[],[],[],[],coreference_resolution,4,41
178,experiments,Baselines and evaluation metrics,[],"[('Baselines', (0, 1))]",[],[],[],[],coreference_resolution,4,156
179,baselines,"Additionally , we report the preceding sentence baseline ( PS BL ) that chooses the previous sentence for the antecedent and TAGbaseline ( TAG BL ) that randomly chooses a candidate with the constituent tag label in {S , VP , ROOT , SBAR } .","[('report', (3, 4)), ('chooses', (13, 14)), ('for', (17, 18)), ('randomly chooses', (27, 29)), ('with', (31, 32)), ('in', (36, 37))]","[('preceding sentence baseline ( PS BL )', (5, 12)), ('previous sentence', (15, 17)), ('antecedent', (19, 20)), ('TAGbaseline ( TAG BL )', (21, 26)), ('candidate', (30, 31)), ('constituent tag label', (33, 36)), ('{S , VP , ROOT , SBAR }', (37, 45))]","[['preceding sentence baseline ( PS BL )', 'chooses', 'previous sentence'], ['preceding sentence baseline ( PS BL )', 'chooses', 'TAGbaseline ( TAG BL )'], ['previous sentence', 'for', 'antecedent'], ['previous sentence', 'for', 'TAGbaseline ( TAG BL )'], ['TAGbaseline ( TAG BL )', 'randomly chooses', 'candidate'], ['candidate', 'with', 'constituent tag label'], ['constituent tag label', 'in', '{S , VP , ROOT , SBAR }']]",[],"[['Baselines', 'report', 'preceding sentence baseline ( PS BL )']]",[],coreference_resolution,4,158
180,hyperparameters,"Glo Ve word embeddings pre-trained on the Gigaword and Wikipedia , and did not fine - tune them .","[('pre-trained on', (4, 6)), ('not fine - tune', (13, 17))]","[('Glo Ve word embeddings', (0, 4)), ('Gigaword and Wikipedia', (7, 10))]","[['Glo Ve word embeddings', 'pre-trained on', 'Gigaword and Wikipedia']]",[],[],"[['Hyperparameters', 'has', 'Glo Ve word embeddings']]",coreference_resolution,4,172
181,hyperparameters,"Vocabulary was built from the words in the training data with frequency in { 3 , U ( 1 , 10 ) } , and OOV words were replaced with an UNK token .","[('built from', (2, 4)), ('in', (6, 7)), ('with', (10, 11)), ('in', (12, 13)), ('replaced with', (28, 30))]","[('Vocabulary', (0, 1)), ('words', (5, 6)), ('training data', (8, 10)), ('frequency', (11, 12)), ('{ 3 , U ( 1 , 10 ) }', (13, 23)), ('OOV words', (25, 27)), ('UNK token', (31, 33))]","[['Vocabulary', 'built from', 'words'], ['words', 'in', 'training data'], ['frequency', 'in', '{ 3 , U ( 1 , 10 ) }'], ['words', 'with', 'frequency'], ['training data', 'with', 'frequency'], ['frequency', 'in', '{ 3 , U ( 1 , 10 ) }'], ['OOV words', 'replaced with', 'UNK token']]",[],[],"[['Hyperparameters', 'has', 'Vocabulary']]",coreference_resolution,4,173
182,hyperparameters,"The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .","[('of', (2, 3)), ('set to', (8, 10))]","[('size', (1, 2)), ('LSTMs hidden states', (4, 7)), ('{ 100 , qlog - U ( 30 , 150 ) }', (10, 22))]","[['size', 'of', 'LSTMs hidden states'], ['LSTMs hidden states', 'set to', '{ 100 , qlog - U ( 30 , 150 ) }']]",[],[],"[['Hyperparameters', 'has', 'size']]",coreference_resolution,4,177
183,hyperparameters,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .","[('initialized', (1, 2)), ('of', (5, 6)), ('with', (8, 9))]","[('weight matrices', (3, 5)), ('LSTMs', (7, 8)), ('random orthogonal matrices', (9, 12))]","[['weight matrices', 'of', 'LSTMs'], ['LSTMs', 'with', 'random orthogonal matrices']]",[],"[['Hyperparameters', 'initialized', 'weight matrices']]",[],coreference_resolution,4,178
184,hyperparameters,The first feed - forward layer size is set to a value in Optimization .,"[('set to', (8, 10)), ('in', (12, 13))]","[('first feed - forward layer size', (1, 7)), ('value', (11, 12)), ('Optimization', (13, 14))]","[['first feed - forward layer size', 'set to', 'value'], ['value', 'in', 'Optimization']]","[['first feed - forward layer size', 'has', 'value']]",[],"[['Hyperparameters', 'has', 'first feed - forward layer size']]",coreference_resolution,4,179
185,hyperparameters,"We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .","[('trained', (1, 2)), ('in', (4, 5)), ('using', (6, 7)), ('with', (15, 16)), ('of', (19, 20))]","[('model', (3, 4)), ('minibatches', (5, 6)), ('Adam', (7, 8)), ('learning rate', (17, 19)), ('10 ? 4', (20, 23)), ('maximal batch size', (24, 27)), ('64', (27, 28))]","[['model', 'in', 'minibatches'], ['minibatches', 'using', 'Adam'], ['Adam', 'with', 'learning rate'], ['Adam', 'with', 'maximal batch size'], ['learning rate', 'of', '10 ? 4']]","[['learning rate', 'has', '10 ? 4'], ['maximal batch size', 'has', '64']]","[['Hyperparameters', 'trained', 'model']]",[],coreference_resolution,4,180
186,hyperparameters,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .","[('clip', (1, 2)), ('by', (3, 4)), ('with', (7, 8)), ('in', (11, 12))]","[('gradients', (2, 3)), ('global norm', (4, 6)), ('clipping value', (9, 11)), ('{ 1.0 , U ( 1 , 100 ) }', (12, 22))]","[['gradients', 'by', 'global norm'], ['global norm', 'with', 'clipping value'], ['clipping value', 'in', '{ 1.0 , U ( 1 , 100 ) }']]",[],"[['Hyperparameters', 'clip', 'gradients']]",[],coreference_resolution,4,181
187,hyperparameters,We train for 10 epochs and choose the model that performs best on the devset .,"[('train', (1, 2)), ('for', (2, 3))]","[('10 epochs', (3, 5))]",[],[],"[['Hyperparameters', 'train', '10 epochs']]",[],coreference_resolution,4,182
188,hyperparameters,"We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) } .","[('used', (1, 2)), ('with', (7, 8))]","[('l 2 - regularization', (3, 7)), ('{ 10 ?5 , log - U (10 ?7 , 10 ?2 ) }', (10, 24))]","[['l 2 - regularization', 'with', '{ 10 ?5 , log - U (10 ?7 , 10 ?2 ) }']]","[['l 2 - regularization', 'has', '{ 10 ?5 , log - U (10 ?7 , 10 ?2 ) }']]","[['Hyperparameters', 'used', 'l 2 - regularization']]",[],coreference_resolution,4,184
189,hyperparameters,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .","[('with', (1, 2)), ('applied to', (18, 20)), ('of', (22, 23)), ('optionally to', (32, 34)), ('with', (36, 37))]","[('Dropout', (0, 1)), ('keep probability k p', (3, 7)), ('{ 0.8 , U( 0.5 , 1.0 ) }', (8, 17)), ('outputs', (21, 22)), ('LSTMs', (24, 25)), ('feed - forward layers', (27, 31)), ('input', (35, 36)), ('k p ? U (0.8 , 1.0 )', (37, 45))]","[['Dropout', 'with', 'keep probability k p'], ['input', 'with', 'k p ? U (0.8 , 1.0 )'], ['keep probability k p', 'applied to', 'outputs'], ['{ 0.8 , U( 0.5 , 1.0 ) }', 'applied to', 'outputs'], ['outputs', 'of', 'LSTMs'], ['keep probability k p', 'optionally to', 'input'], ['input', 'with', 'k p ? U (0.8 , 1.0 )']]","[['keep probability k p', 'has', '{ 0.8 , U( 0.5 , 1.0 ) }']]",[],"[['Hyperparameters', 'has', 'Dropout']]",coreference_resolution,4,185
190,results,"In terms of s@1 score , MR - LSTM outperforms both KZH13 's results and TAG BL without even necessitating HP tuning .","[('In terms of', (0, 3)), ('both', (10, 11)), ('without even necessitating', (17, 20))]","[('s@1 score', (3, 5)), ('MR - LSTM', (6, 9)), ('outperforms', (9, 10)), (""KZH13 's results"", (11, 14)), ('TAG BL', (15, 17)), ('HP tuning', (20, 22))]","[['MR - LSTM', 'both', ""KZH13 's results""], ['MR - LSTM', 'both', 'TAG BL'], ['outperforms', 'both', ""KZH13 's results""], ['outperforms', 'both', 'TAG BL'], ['TAG BL', 'without even necessitating', 'HP tuning']]","[['s@1 score', 'has', 'MR - LSTM'], ['MR - LSTM', 'has', 'outperforms'], ['outperforms', 'has', ""KZH13 's results""]]","[['Results', 'In terms of', 's@1 score']]",[],coreference_resolution,4,188
191,results,"From we observe : ( 1 ) with HPs tuned on ARRAU - AA , we obtain results well beyond KZH13 , ( 2 ) all ablated model variants perform worse than the full model , ( 3 ) a large performance drop when omitting syntactic information ( tag , cut ) suggests that the model makes good use of it .","[('observe', (2, 3)), ('tuned on', (9, 11)), ('obtain', (16, 17)), ('well beyond', (18, 20)), ('perform', (29, 30)), ('than', (31, 32)), ('when', (43, 44)), ('omitting', (44, 45))]","[('with HPs', (7, 9)), ('ARRAU - AA', (11, 14)), ('results', (17, 18)), ('KZH13', (20, 21)), ('all ablated model variants', (25, 29)), ('worse', (30, 31)), ('full model', (33, 35)), ('large performance drop', (40, 43)), ('syntactic information', (45, 47))]","[['with HPs', 'tuned on', 'ARRAU - AA'], ['with HPs', 'obtain', 'results'], ['results', 'well beyond', 'KZH13'], ['all ablated model variants', 'perform', 'worse'], ['worse', 'than', 'full model'], ['large performance drop', 'omitting', 'syntactic information']]",[],"[['Results', 'observe', 'with HPs']]",[],coreference_resolution,4,190
192,results,"Performance of 68.10 s@1 score indicates that the model is able to learn without syntactic guidance , contrary to the 19.68 s@1 score before tuning .","[('Performance of', (0, 2)), ('indicates', (5, 6))]","[('68.10 s@1 score', (2, 5)), ('model', (8, 9)), ('learn', (12, 13)), ('without syntactic guidance', (13, 16))]","[['68.10 s@1 score', 'indicates', 'model']]","[['learn', 'has', 'without syntactic guidance']]","[['Results', 'Performance of', '68.10 s@1 score']]",[],coreference_resolution,4,195
193,results,Results on the ARRAU corpus,"[('on', (1, 2))]","[('ARRAU corpus', (3, 5))]",[],[],"[['Results', 'on', 'ARRAU corpus']]",[],coreference_resolution,4,199
194,results,"The MR - LSTM is more successful in resolving nominal than pronominal anaphors , although the training data provides only pronominal ones .","[('than', (10, 11))]","[('MR - LSTM', (1, 4)), ('more successful', (5, 7)), ('resolving', (8, 9)), ('nominal', (9, 10)), ('pronominal anaphors', (11, 13))]","[['nominal', 'than', 'pronominal anaphors']]","[['MR - LSTM', 'has', 'more successful'], ['resolving', 'has', 'nominal']]",[],"[['Results', 'has', 'MR - LSTM']]",coreference_resolution,4,200
195,results,"Moreover , for shell noun resolution in KZH13 's dataset , the MR - LSTM achieved s@1 scores in the range 76.09-93.14 , while the best variant of the model achieves 51.89 s@1 score for nominal anaphors in ARRAU - AA .","[('for', (2, 3)), ('in', (6, 7)), ('achieved', (15, 16)), ('in the range', (18, 21)), ('of', (27, 28)), ('achieves', (30, 31)), ('for', (34, 35)), ('in', (37, 38))]","[('shell noun resolution', (3, 6)), (""KZH13 's dataset"", (7, 10)), ('MR - LSTM', (12, 15)), ('s@1 scores', (16, 18)), ('76.09-93.14', (21, 22)), ('best variant', (25, 27)), ('model', (29, 30)), ('51.89 s@1 score', (31, 34)), ('nominal anaphors', (35, 37)), ('ARRAU - AA', (38, 41))]","[['shell noun resolution', 'in', ""KZH13 's dataset""], ['nominal anaphors', 'in', 'ARRAU - AA'], ['MR - LSTM', 'achieved', 's@1 scores'], ['s@1 scores', 'in the range', '76.09-93.14'], ['best variant', 'of', 'model'], ['best variant', 'achieves', '51.89 s@1 score'], ['model', 'achieves', '51.89 s@1 score'], ['51.89 s@1 score', 'for', 'nominal anaphors'], ['nominal anaphors', 'in', 'ARRAU - AA']]","[['shell noun resolution', 'has', ""KZH13 's dataset""]]","[['Results', 'for', 'shell noun resolution']]",[],coreference_resolution,4,202
196,results,"This is what we can observe from row 2 vs. row 6 in Table 5 : the MR - LSTM without context embedding ( ctx ) achieves a comparable s@ 2 score with the variant that omits syntactic information , but better s@3 - 4 scores .","[('achieves', (26, 27)), ('with', (32, 33)), ('omits', (36, 37))]","[('MR - LSTM without context embedding ( ctx )', (17, 26)), ('comparable s@ 2 score', (28, 32)), ('variant', (34, 35)), ('syntactic information', (37, 39)), ('better s@3 - 4 scores', (41, 46))]","[['MR - LSTM without context embedding ( ctx )', 'achieves', 'comparable s@ 2 score'], ['comparable s@ 2 score', 'with', 'variant'], ['variant', 'omits', 'syntactic information']]",[],[],[],coreference_resolution,4,209
197,research-problem,Learning Global Features for Coreference Resolution,[],"[('Coreference Resolution', (4, 6))]",[],[],[],[],coreference_resolution,5,2
198,research-problem,There is compelling evidence that coreference prediction would benefit from modeling global information about entity - clusters .,[],"[('coreference prediction', (5, 7))]",[],[],[],[],coreference_resolution,5,4
199,approach,"In this work , we posit that global context is indeed necessary for further improvements in coreference resolution , but argue that informative cluster , rather than mention , level features are very difficult to devise , limiting their effectiveness .","[('posit', (5, 6)), ('for', (12, 13)), ('in', (15, 16))]","[('global context', (7, 9)), ('indeed necessary', (10, 12)), ('further improvements', (13, 15)), ('coreference resolution', (16, 18))]","[['indeed necessary', 'for', 'further improvements'], ['further improvements', 'in', 'coreference resolution']]","[['global context', 'has', 'indeed necessary']]","[['Approach', 'posit', 'global context']]",[],coreference_resolution,5,12
200,approach,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .","[('learn', (6, 7)), ('of', (8, 9)), ('by embedding', (11, 13)), ('using', (15, 16))]","[('representations', (7, 8)), ('mention clusters', (9, 11)), ('sequentially', (14, 15)), ('recurrent neural network', (17, 20))]","[['representations', 'of', 'mention clusters'], ['representations', 'by embedding', 'sequentially'], ['sequentially', 'using', 'recurrent neural network']]",[],"[['Approach', 'learn', 'representations']]",[],coreference_resolution,5,13
201,approach,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .","[('learns', (11, 12)), ('from', (15, 16)), ('present in', (19, 21))]","[('no manually defined cluster features', (3, 8)), ('global representation', (13, 15)), ('individual mentions', (17, 19)), ('each cluster', (21, 23))]","[['global representation', 'from', 'individual mentions'], ['individual mentions', 'present in', 'each cluster']]",[],[],[],coreference_resolution,5,14
202,approach,We incorporate these representations into a mention - ranking style coreference system .,"[('into', (4, 5))]","[('mention - ranking style coreference system', (6, 12))]",[],[],"[['Approach', 'into', 'mention - ranking style coreference system']]",[],coreference_resolution,5,15
203,approach,"The entire model , including the recurrent neural network and the mention - ranking sub-system , is trained end - to - end on the coreference task .","[('including', (4, 5)), ('trained', (17, 18)), ('on', (23, 24))]","[('the recurrent neural network and the mention - ranking sub-system', (5, 15)), ('end - to - end', (18, 23)), ('coreference task', (25, 27))]","[['the recurrent neural network and the mention - ranking sub-system', 'trained', 'end - to - end'], ['end - to - end', 'on', 'coreference task']]",[],"[['Approach', 'including', 'the recurrent neural network and the mention - ranking sub-system']]",[],coreference_resolution,5,16
204,approach,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .","[('train', (1, 2)), ('as', (4, 5)), ('with', (8, 9))]","[('model', (3, 4)), ('local classifier', (6, 8)), ('fixed context', (9, 11))]","[['model', 'as', 'local classifier'], ['local classifier', 'with', 'fixed context']]","[['model', 'has', 'local classifier']]","[['Approach', 'train', 'model']]",[],coreference_resolution,5,17
205,experimental-setup,"For training , we use document - size minibatches , which allows for efficient pre-computation of RNN states , and we minimize the loss described in Section 5 with AdaGrad ( after clipping LSTM gradients to lie ( elementwise ) in ( ?10 , 10 ) ) .","[('use', (4, 5)), ('minimize', (21, 22)), ('with', (28, 29))]","[('training', (1, 2)), ('document - size minibatches', (5, 9)), ('loss', (23, 24)), ('AdaGrad', (29, 30))]","[['training', 'use', 'document - size minibatches'], ['document - size minibatches', 'minimize', 'loss'], ['loss', 'with', 'AdaGrad']]",[],[],[],coreference_resolution,5,181
206,experimental-setup,"We find that the initial learning rate chosen for AdaGrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .","[('for', (8, 9)), ('choose', (19, 20)), ('out of', (25, 27))]","[('learning rates', (20, 22)), ('each layer', (23, 25)), ('{ 0.1 , 0.02 , 0.01 , 0.002 , 0.001 }', (27, 38))]","[['learning rates', 'for', 'each layer'], ['each layer', 'out of', '{ 0.1 , 0.02 , 0.01 , 0.002 , 0.001 }']]",[],"[['Experimental setup', 'for', 'learning rates']]",[],coreference_resolution,5,182
207,experimental-setup,"In experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? R 200 , and hp ( x n , y) ? R 700 .","[('set', (4, 5)), ('to be', (23, 25))]","[('ha ( x n ) , h c ( x n ) , and h ( m )', (5, 23)), ('R 200', (26, 28)), ('hp ( x n , y)', (30, 36)), ('R 700', (37, 39))]",[],"[['hp ( x n , y)', 'has', 'R 700']]","[['Experimental setup', 'set', 'ha ( x n ) , h c ( x n ) , and h ( m )']]",[],coreference_resolution,5,183
208,experimental-setup,"We use a single - layer LSTM ( without "" peep - hole "" connections ) , as implemented in the element - rnn library .","[('use', (1, 2)), ('without', (8, 9)), ('implemented in', (18, 20))]","[('single - layer LSTM', (3, 7)), ('"" peep - hole "" connections', (9, 15)), ('element - rnn library', (21, 25))]","[['single - layer LSTM', 'without', '"" peep - hole "" connections'], ['single - layer LSTM', 'implemented in', 'element - rnn library']]",[],"[['Experimental setup', 'use', 'single - layer LSTM']]",[],coreference_resolution,5,184
209,experimental-setup,"For regularization , we apply Dropout with a rate of 0.4 before applying the linear weights u , and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot -product scores .","[('For', (0, 1)), ('apply', (4, 5)), ('with', (6, 7)), ('of', (9, 10)), ('before applying', (11, 13)), ('to', (28, 29)), ('before forming', (32, 34))]","[('regularization', (1, 2)), ('Dropout', (5, 6)), ('rate', (8, 9)), ('0.4', (10, 11)), ('linear weights u', (14, 17)), ('0.3', (27, 28)), ('LSTM states', (30, 32)), ('dot -product scores', (35, 38))]","[['regularization', 'apply', 'Dropout'], ['Dropout', 'with', 'rate'], ['rate', 'of', '0.4'], ['rate', 'before applying', 'linear weights u'], ['0.4', 'before applying', 'linear weights u'], ['0.3', 'to', 'LSTM states'], ['LSTM states', 'before forming', 'dot -product scores']]",[],"[['Experimental setup', 'For', 'regularization']]",[],coreference_resolution,5,185
210,code,Code for our system is available at https : //github.com/swiseman/nn_coref .,[],"[('https : //github.com/swiseman/nn_coref', (7, 10))]",[],[],[],[],coreference_resolution,5,189
211,experimental-setup,"The system makes use of a GPU for training , and trains in about two hours .","[('makes use of', (2, 5)), ('for', (7, 8))]","[('system', (1, 2)), ('GPU', (6, 7)), ('training', (8, 9)), ('trains', (11, 12)), ('about two hours', (13, 16))]","[['system', 'makes use of', 'GPU'], ['system', 'makes use of', 'trains'], ['GPU', 'for', 'training']]",[],[],"[['Experimental setup', 'has', 'system']]",coreference_resolution,5,190
212,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],coreference_resolution,5,191
213,results,"We see a statistically significant improvement of over 0.8 Co NLL points over the previous state of the art , and the highest F 1 scores to date on all three CoNLL metrics .","[('see', (1, 2)), ('of', (6, 7)), ('over', (12, 13)), ('on', (28, 29))]","[('statistically significant improvement', (3, 6)), ('over 0.8 Co NLL points', (7, 12)), ('previous state of the art', (14, 19)), ('highest F 1 scores', (22, 26)), ('all three CoNLL metrics', (29, 33))]","[['statistically significant improvement', 'of', 'over 0.8 Co NLL points'], ['over 0.8 Co NLL points', 'over', 'previous state of the art']]",[],"[['Results', 'see', 'statistically significant improvement']]",[],coreference_resolution,5,193
214,results,"In we see that the RNN improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .","[('with', (11, 12)), ('on', (18, 19)), ('for', (28, 29))]","[('RNN', (5, 6)), ('improves', (6, 7)), ('performance', (7, 8)), ('most dramatic improve - ments', (13, 18)), ('non-anaphoric pronouns', (19, 21)), ('errors', (23, 24)), ('decreased significantly', (26, 28)), ('non-anaphoric nominal and proper mentions', (29, 34))]","[['performance', 'with', 'most dramatic improve - ments'], ['most dramatic improve - ments', 'on', 'non-anaphoric pronouns'], ['decreased significantly', 'for', 'non-anaphoric nominal and proper mentions']]","[['RNN', 'has', 'improves'], ['improves', 'has', 'performance'], ['errors', 'has', 'decreased significantly']]",[],"[['Results', 'has', 'RNN']]",coreference_resolution,5,203
215,results,"Importantly , the RNN performance is significantly better than that of the Avg baseline , which barely improves over mention - ranking , even with oracle history .","[('than', (8, 9)), ('over', (18, 19))]","[('RNN performance', (3, 5)), ('significantly better', (6, 8)), ('Avg baseline', (12, 14)), ('barely improves', (16, 18)), ('mention - ranking', (19, 22))]","[['significantly better', 'than', 'Avg baseline'], ['barely improves', 'over', 'mention - ranking']]","[['RNN performance', 'has', 'significantly better']]",[],"[['Results', 'has', 'RNN performance']]",coreference_resolution,5,205
216,research-problem,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution,[],"[('End - to - End Co -reference Resolution', (9, 17))]",[],[],[],[],coreference_resolution,6,2
217,research-problem,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .",[],"[('end - to - end co-reference resolution ( E2E - CR )', (18, 30))]",[],[],[],[],coreference_resolution,6,4
218,research-problem,"While the traditional E2E - CR model generates word representations by running long short - term memory ( LSTM ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .",[],"[('E2E - CR', (3, 6))]",[],[],[],[],coreference_resolution,6,5
219,model,Co-reference resolution requires models to cluster mentions that refer to the same physical entities .,[],"[('Co-reference resolution', (0, 2))]",[],[],[],"[['Model', 'has', 'Co-reference resolution']]",coreference_resolution,6,10
220,model,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .","[('propose', (46, 47)), ('for', (52, 53))]","[('cross - sentence encoder', (48, 52)), ('end - to - end co-reference ( E2E - CR )', (53, 64))]","[['cross - sentence encoder', 'for', 'end - to - end co-reference ( E2E - CR )']]",[],"[['Model', 'propose', 'cross - sentence encoder']]",[],coreference_resolution,6,24
221,model,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .","[('from', (8, 9)), ('containing', (14, 15)), ('added to', (23, 25))]","[('external memory block', (11, 14)), ('syntactic and semantic information', (15, 19)), ('context sentences', (20, 22)), ('standard LSTM model', (26, 29))]","[['syntactic and semantic information', 'from', 'context sentences'], ['external memory block', 'containing', 'syntactic and semantic information'], ['syntactic and semantic information', 'added to', 'standard LSTM model']]",[],"[['Model', 'from', 'external memory block']]",[],coreference_resolution,6,25
222,model,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .","[('With', (0, 1)), ('encode', (12, 13)), ('as', (15, 16)), ('calculate', (21, 22)), ('of', (24, 25)), ('by taking', (27, 29))]","[('context memory block', (2, 5)), ('proposed model', (7, 9)), ('input sentences', (13, 15)), ('batch', (17, 18)), ('representations', (23, 24)), ('input words', (25, 27)), ('target sentences and context sentences', (30, 35))]","[['proposed model', 'encode', 'input sentences'], ['input sentences', 'as', 'batch'], ['proposed model', 'calculate', 'representations'], ['representations', 'of', 'input words'], ['input words', 'by taking', 'target sentences and context sentences']]","[['context memory block', 'has', 'proposed model']]","[['Model', 'With', 'context memory block']]",[],coreference_resolution,6,26
223,hyperparameters,"In practice , the LSTM modules applied in our model have 200 output units .","[('applied in', (6, 8))]","[('LSTM modules', (4, 6)), ('our model', (8, 10)), ('200 output units', (11, 14))]","[['LSTM modules', 'applied in', 'our model']]",[],[],"[['Hyperparameters', 'has', 'LSTM modules']]",coreference_resolution,6,87
224,hyperparameters,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .","[('In', (0, 1)), ('calculate', (4, 5)), ('using', (9, 10)), ('with', (13, 14)), ('consisting of', (17, 19))]","[('ASL', (1, 2)), ('cross - sentence dependency', (5, 9)), ('multilayer perceptron', (11, 13)), ('one hidden layer', (14, 17)), ('150 hidden units', (19, 22))]","[['ASL', 'calculate', 'cross - sentence dependency'], ['cross - sentence dependency', 'using', 'multilayer perceptron'], ['multilayer perceptron', 'with', 'one hidden layer'], ['one hidden layer', 'consisting of', '150 hidden units']]",[],"[['Hyperparameters', 'In', 'ASL']]",[],coreference_resolution,6,88
225,hyperparameters,The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,"[('set as', (5, 7)), ('decays', (9, 10))]","[('initial learning rate', (1, 4)), ('0.001', (7, 8)), ('0.001 %', (10, 12)), ('every 100 steps', (12, 15))]","[['initial learning rate', 'set as', '0.001'], ['initial learning rate', 'decays', '0.001 %']]","[['initial learning rate', 'has', '0.001'], ['0.001 %', 'has', 'every 100 steps']]",[],"[['Hyperparameters', 'has', 'initial learning rate']]",coreference_resolution,6,89
226,hyperparameters,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .","[('optimized with', (3, 5))]","[('model', (1, 2)), ('Adam algorithm', (6, 8))]","[['model', 'optimized with', 'Adam algorithm']]",[],[],"[['Hyperparameters', 'has', 'model']]",coreference_resolution,6,90
227,hyperparameters,We randomly select up to 40 continuous sentences for training if the input is too long .,"[('select', (2, 3)), ('for', (8, 9))]","[('up to 40 continuous sentences', (3, 8)), ('training', (9, 10))]","[['up to 40 continuous sentences', 'for', 'training']]",[],"[['Hyperparameters', 'select', 'up to 40 continuous sentences']]",[],coreference_resolution,6,91
228,results,Experiment Results and Discussion,[],"[('Results', (1, 2))]",[],[],[],[],coreference_resolution,6,93
229,results,"Comparing with the baseline model that achieved 67.2 % F1 score , the ASL model improved the performance by 0.6 % and achieved 67.8 % average F1 .","[('Comparing with', (0, 2)), ('achieved', (6, 7)), ('improved', (15, 16)), ('by', (18, 19)), ('achieved', (22, 23))]","[('baseline model', (3, 5)), ('67.2 % F1 score', (7, 11)), ('ASL model', (13, 15)), ('performance', (17, 18)), ('0.6 %', (19, 21)), ('67.8 % average F1', (23, 27))]","[['baseline model', 'achieved', '67.2 % F1 score'], ['ASL model', 'improved', 'performance'], ['performance', 'by', '0.6 %'], ['ASL model', 'achieved', '67.8 % average F1']]","[['baseline model', 'has', '67.2 % F1 score']]","[['Results', 'Comparing with', 'baseline model']]",[],coreference_resolution,6,97
230,results,"show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .","[('show', (0, 1)), ('consider', (5, 6)), ('significantly outperform', (10, 12)), ('encodes', (17, 18)), ('from', (20, 21))]","[('models', (3, 4)), ('cross - sentence dependency', (6, 10)), ('baseline model', (13, 15)), ('each sentence', (18, 20)), ('input document', (22, 24))]","[['models', 'consider', 'cross - sentence dependency'], ['cross - sentence dependency', 'significantly outperform', 'baseline model'], ['baseline model', 'encodes', 'each sentence'], ['each sentence', 'from', 'input document']]","[['models', 'has', 'cross - sentence dependency']]","[['Results', 'show', 'models']]",[],coreference_resolution,6,108
231,results,"Experiments also indicated that the ASL model has better performance than the LSL model , since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence - level embeddings .","[('indicated', (2, 3)), ('than', (10, 11)), ('extracts', (18, 19)), ('with', (21, 22)), ('instead of', (25, 27))]","[('ASL model', (5, 7)), ('better performance', (8, 10)), ('LSL model', (12, 14)), ('context information', (19, 21)), ('attention mechanism', (23, 25)), ('simply viewing', (27, 29)), ('sentence - level embeddings', (29, 33))]","[['better performance', 'than', 'LSL model'], ['better performance', 'extracts', 'context information'], ['context information', 'with', 'attention mechanism'], ['attention mechanism', 'instead of', 'simply viewing']]","[['ASL model', 'has', 'better performance'], ['simply viewing', 'has', 'sentence - level embeddings']]","[['Results', 'indicated', 'ASL model']]",[],coreference_resolution,6,109
232,research-problem,Coreference Resolution with Entity Equalization,[],"[('Coreference Resolution', (0, 2))]",[],[],[],[],coreference_resolution,7,2
233,approach,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .","[('provides', (6, 7)), ('facilitates', (21, 22))]","[('entity - level representation', (8, 12)), ('simple and intuitive manner', (14, 18)), ('end - to - end optimization', (22, 28))]","[['entity - level representation', 'facilitates', 'end - to - end optimization']]",[],"[['Approach', 'provides', 'entity - level representation']]",[],coreference_resolution,7,18
234,approach,"Our "" Entity Equalization "" approach posits that each entity should be represented via the sum of its corresponding mention representations .","[('represented via', (12, 14)), ('of', (16, 17))]","[('Our "" Entity Equalization "" approach', (0, 6)), ('posits', (6, 7)), ('each entity', (8, 10)), ('sum', (15, 16)), ('corresponding mention representations', (18, 21))]","[['each entity', 'represented via', 'sum'], ['sum', 'of', 'corresponding mention representations']]","[['Our "" Entity Equalization "" approach', 'has', 'posits']]",[],"[['Approach', 'has', 'Our "" Entity Equalization "" approach']]",coreference_resolution,7,19
235,approach,"Similar to recent coreference models , our approach uses contextual embeddings as input mention representations .","[('uses', (8, 9)), ('as', (11, 12))]","[('contextual embeddings', (9, 11)), ('input mention representations', (12, 15))]","[['contextual embeddings', 'as', 'input mention representations']]",[],"[['Approach', 'uses', 'contextual embeddings']]",[],coreference_resolution,7,22
236,approach,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .","[('use', (11, 12))]","[('BERT embeddings', (12, 14))]",[],[],"[['Approach', 'use', 'BERT embeddings']]",[],coreference_resolution,7,23
237,approach,We show that this can be done by using BERT in a fully convolutional manner .,"[('using', (8, 9)), ('in', (10, 11))]","[('BERT', (9, 10)), ('fully convolutional manner', (12, 15))]","[['BERT', 'in', 'fully convolutional manner']]",[],"[['Approach', 'using', 'BERT']]",[],coreference_resolution,7,25
238,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],coreference_resolution,7,115
239,results,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg .","[('with', (9, 10)), ('achieves', (21, 22))]","[('baseline', (1, 2)), ('span - ranking model', (4, 8)), ('ELMo input features', (10, 13)), ('second - order span representations', (14, 19)), ('73.0 % Avg', (22, 25))]","[['span - ranking model', 'with', 'ELMo input features'], ['span - ranking model', 'with', 'second - order span representations'], ['second - order span representations', 'achieves', '73.0 % Avg']]","[['baseline', 'has', 'span - ranking model']]",[],"[['Results', 'has', 'baseline']]",coreference_resolution,7,119
240,results,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,"[('Replacing', (2, 3)), ('with', (6, 7)), ('achieves', (9, 10))]","[('ELMo features', (4, 6)), ('BERT features', (7, 9)), ('76. 25 % average F1', (10, 15))]","[['ELMo features', 'with', 'BERT features'], ['ELMo features', 'achieves', '76. 25 % average F1'], ['BERT features', 'achieves', '76. 25 % average F1']]",[],[],[],coreference_resolution,7,120
241,results,"Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .","[('Removing', (0, 1)), ('while using', (8, 10)), ('achieves', (12, 13))]","[('second - order span - representations', (2, 8)), ('BERT features', (10, 12)), ('76.37 % F1', (13, 16))]","[['second - order span - representations', 'while using', 'BERT features'], ['BERT features', 'achieves', '76.37 % F1']]",[],"[['Results', 'Removing', 'second - order span - representations']]",[],coreference_resolution,7,121
242,results,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .","[('with', (4, 5)), ('achieves', (7, 8))]","[('secondorder span representations', (1, 4)), ('Entity Equalization', (5, 7)), ('76. 64 % average F1', (8, 13))]","[['secondorder span representations', 'with', 'Entity Equalization'], ['secondorder span representations', 'achieves', '76. 64 % average F1'], ['Entity Equalization', 'achieves', '76. 64 % average F1']]",[],[],[],coreference_resolution,7,122
243,results,"Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .","[('set', (2, 3)), ('for', (9, 10)), ('improving', (13, 14)), ('by', (20, 21))]","[('new state of the art', (4, 9)), ('coreference resolution', (10, 12)), ('previous state of the art', (15, 20)), ('3.6 % average F1', (21, 25))]","[['new state of the art', 'for', 'coreference resolution'], ['new state of the art', 'improving', 'previous state of the art'], ['previous state of the art', 'by', '3.6 % average F1']]",[],"[['Results', 'set', 'new state of the art']]",[],coreference_resolution,7,123
244,research-problem,End - to - end Neural Coreference Resolution,[],"[('End - to - end Neural Coreference Resolution', (0, 8))]",[],[],[],[],coreference_resolution,8,2
245,research-problem,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .,[],"[('end - to - end coreference resolution', (4, 11))]",[],[],[],[],coreference_resolution,8,4
246,model,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .,"[('present', (1, 2)), ('learned', (17, 18)), ('given', (21, 22))]","[('neural coreference resolution model', (11, 15)), ('end - toend', (18, 21)), ('only gold mention clusters', (22, 26))]","[['neural coreference resolution model', 'learned', 'end - toend'], ['end - toend', 'given', 'only gold mention clusters']]",[],"[['Model', 'present', 'neural coreference resolution model']]",[],coreference_resolution,8,10
247,model,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .","[('how to', (43, 45))]","[('training', (25, 26)), ('end - to - end neural model', (27, 34)), ('jointly learns', (35, 37)), ('spans', (38, 39)), ('entity mentions', (40, 42)), ('cluster', (46, 47))]",[],"[['training', 'has', 'end - to - end neural model'], ['jointly learns', 'has', 'spans']]",[],[],coreference_resolution,8,12
248,model,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,"[('reasons over', (2, 4)), ('of', (6, 7)), ('up to', (9, 11)), ('directly optimizes', (15, 17)), ('of', (20, 21)), ('from', (23, 24))]","[('space', (5, 6)), ('all spans', (7, 9)), ('maximum length', (12, 14)), ('marginal likelihood', (18, 20)), ('antecedent spans', (21, 23)), ('gold coreference clusters', (24, 27))]","[['space', 'of', 'all spans'], ['marginal likelihood', 'of', 'antecedent spans'], ['all spans', 'up to', 'maximum length'], ['marginal likelihood', 'of', 'antecedent spans'], ['antecedent spans', 'from', 'gold coreference clusters']]",[],"[['Model', 'reasons over', 'space']]",[],coreference_resolution,8,13
249,model,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .","[('includes', (1, 2)), ('decides', (8, 9))]","[('span - ranking model', (3, 7)), ('which of the previous spans', (14, 19)), ('good antecedent', (25, 27))]",[],[],"[['Model', 'includes', 'span - ranking model']]",[],coreference_resolution,8,14
250,model,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .","[('of', (3, 4)), ('representing', (9, 10)), ('in', (13, 14)), ('combine', (18, 19)), ('with', (24, 25)), ('over', (31, 32))]","[('At the core', (0, 3)), ('vector embeddings', (7, 9)), ('spans', (10, 11)), ('text', (12, 13)), ('document', (15, 16)), ('context - dependent boundary representations', (19, 24)), ('head - finding attention mechanism', (26, 31)), ('span', (33, 34))]","[['spans', 'of', 'text'], ['vector embeddings', 'representing', 'spans'], ['vector embeddings', 'combine', 'context - dependent boundary representations'], ['context - dependent boundary representations', 'with', 'head - finding attention mechanism'], ['head - finding attention mechanism', 'over', 'span']]",[],[],"[['Model', 'has', 'At the core']]",coreference_resolution,8,15
251,model,"The attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .","[('inspired by', (4, 6)), ('from', (14, 15))]","[('attention component', (1, 3)), ('parser - derived head - word matching features', (6, 14)), ('previous systems', (15, 17)), ('less susceptible', (20, 22)), ('cascading errors', (23, 25))]","[['attention component', 'inspired by', 'parser - derived head - word matching features'], ['parser - derived head - word matching features', 'from', 'previous systems']]",[],[],"[['Model', 'has', 'attention component']]",coreference_resolution,8,16
252,experiments,Hyperparameters,[],"[('Hyperparameters', (0, 1))]",[],[],[],[],coreference_resolution,8,116
253,hyperparameters,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .","[('of', (6, 7)), ('to be', (21, 23))]","[('word embeddings', (1, 3)), ('fixed concatenation', (4, 6)), ('300 - dimensional GloVe embeddings and 50 - dimensional embeddings', (7, 17)), ('normalized', (20, 21)), ('unit vectors', (23, 25))]","[['fixed concatenation', 'of', '300 - dimensional GloVe embeddings and 50 - dimensional embeddings'], ['normalized', 'to be', 'unit vectors']]","[['word embeddings', 'has', 'fixed concatenation']]",[],"[['Hyperparameters', 'has', 'word embeddings']]",coreference_resolution,8,118
254,hyperparameters,Outof - vocabulary words are represented by a vector of zeros .,"[('represented by', (5, 7)), ('of', (9, 10))]","[('Outof - vocabulary words', (0, 4)), ('vector', (8, 9)), ('zeros', (10, 11))]","[['Outof - vocabulary words', 'represented by', 'vector'], ['vector', 'of', 'zeros']]",[],[],"[['Hyperparameters', 'has', 'Outof - vocabulary words']]",coreference_resolution,8,119
255,hyperparameters,"In the character CNN , characters are represented as learned 8 - dimensional embeddings .","[('In', (0, 1)), ('represented as', (7, 9))]","[('character CNN', (2, 4)), ('characters', (5, 6)), ('learned 8 - dimensional embeddings', (9, 14))]","[['characters', 'represented as', 'learned 8 - dimensional embeddings']]","[['character CNN', 'has', 'characters']]","[['Hyperparameters', 'In', 'character CNN']]",[],coreference_resolution,8,120
256,hyperparameters,"The convolutions have window sizes of 3 , 4 , and 5 characters , each consisting of 50 filters .","[('have', (2, 3)), ('of', (5, 6)), ('consisting of', (15, 17))]","[('convolutions', (1, 2)), ('window sizes', (3, 5)), ('3 , 4 , and 5 characters', (6, 13)), ('50 filters', (17, 19))]","[['convolutions', 'have', 'window sizes'], ['window sizes', 'of', '3 , 4 , and 5 characters'], ['3 , 4 , and 5 characters', 'consisting of', '50 filters']]","[['convolutions', 'has', 'window sizes']]",[],"[['Hyperparameters', 'has', 'convolutions']]",coreference_resolution,8,121
257,hyperparameters,The hidden states in the LSTMs have 200 dimensions .,"[('in', (3, 4))]","[('hidden states', (1, 3)), ('LSTMs', (5, 6)), ('200 dimensions', (7, 9))]","[['hidden states', 'in', 'LSTMs']]",[],[],"[['Hyperparameters', 'has', 'hidden states']]",coreference_resolution,8,123
258,hyperparameters,Each feedforward neural network consists of two hidden layers with 150 dimensions and rectified linear units .,"[('consists of', (4, 6)), ('with', (9, 10))]","[('Each feedforward neural network', (0, 4)), ('two hidden layers', (6, 9)), ('150 dimensions', (10, 12)), ('rectified linear units', (13, 16))]","[['Each feedforward neural network', 'consists of', 'two hidden layers'], ['Each feedforward neural network', 'consists of', 'rectified linear units'], ['two hidden layers', 'with', '150 dimensions']]",[],[],"[['Hyperparameters', 'has', 'Each feedforward neural network']]",coreference_resolution,8,124
259,hyperparameters,We use ADAM for learning with a minibatch size of 1 .,"[('use', (1, 2)), ('for', (3, 4)), ('with', (5, 6)), ('of', (9, 10))]","[('ADAM', (2, 3)), ('learning', (4, 5)), ('minibatch size', (7, 9)), ('1', (10, 11))]","[['ADAM', 'for', 'learning'], ['learning', 'with', 'minibatch size'], ['minibatch size', 'of', '1']]",[],"[['Hyperparameters', 'use', 'ADAM']]",[],coreference_resolution,8,133
260,hyperparameters,The LSTM weights are initialized with random orthonormal matrices as described in .,"[('initialized with', (4, 6))]","[('LSTM weights', (1, 3)), ('random orthonormal matrices', (6, 9))]","[['LSTM weights', 'initialized with', 'random orthonormal matrices']]",[],[],"[['Hyperparameters', 'has', 'LSTM weights']]",coreference_resolution,8,134
261,hyperparameters,We apply 0.5 dropout to the word embeddings and character CNN outputs .,"[('apply', (1, 2)), ('to', (4, 5))]","[('0.5 dropout', (2, 4)), ('word embeddings', (6, 8)), ('character CNN outputs', (9, 12))]","[['0.5 dropout', 'to', 'word embeddings'], ['0.5 dropout', 'to', 'character CNN outputs']]",[],"[['Hyperparameters', 'apply', '0.5 dropout']]",[],coreference_resolution,8,135
262,hyperparameters,We apply 0.2 dropout to all hidden layers and feature embeddings .,"[('to', (4, 5))]","[('0.2 dropout', (2, 4)), ('all hidden layers', (5, 8)), ('feature embeddings', (9, 11))]","[['0.2 dropout', 'to', 'all hidden layers'], ['0.2 dropout', 'to', 'feature embeddings']]",[],[],[],coreference_resolution,8,136
263,hyperparameters,Dropout masks are shared across timesteps to preserve long - distance information as described in .,"[('shared across', (3, 5)), ('to preserve', (6, 8))]","[('Dropout masks', (0, 2)), ('timesteps', (5, 6)), ('long - distance information', (8, 12))]","[['Dropout masks', 'shared across', 'timesteps'], ['Dropout masks', 'to preserve', 'long - distance information']]",[],[],"[['Hyperparameters', 'has', 'Dropout masks']]",coreference_resolution,8,137
264,hyperparameters,The learning rate is decayed by 0.1 % every 100 steps .,"[('by', (5, 6))]","[('learning rate', (1, 3)), ('decayed', (4, 5)), ('0.1 %', (6, 8)), ('every 100 steps', (8, 11))]","[['decayed', 'by', '0.1 %']]","[['learning rate', 'has', 'decayed'], ['0.1 %', 'has', 'every 100 steps']]",[],"[['Hyperparameters', 'has', 'learning rate']]",coreference_resolution,8,138
265,hyperparameters,"The model is trained for up to 150 epochs , with early stopping based on the development set .","[('trained for', (3, 5)), ('with', (10, 11))]","[('up to 150 epochs', (5, 9)), ('early stopping', (11, 13))]","[['up to 150 epochs', 'with', 'early stopping']]",[],"[['Hyperparameters', 'trained for', 'up to 150 epochs']]",[],coreference_resolution,8,139
266,hyperparameters,All code is implemented in Tensor - Flow and is publicly available .,"[('implemented in', (3, 5))]","[('All code', (0, 2)), ('Tensor - Flow', (5, 8))]","[['All code', 'implemented in', 'Tensor - Flow']]",[],[],"[['Hyperparameters', 'has', 'All code']]",coreference_resolution,8,140
267,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],coreference_resolution,8,147
268,results,Coreference Results,[],"[('Coreference Results', (0, 2))]",[],[],[],"[['Results', 'has', 'Coreference Results']]",coreference_resolution,8,151
269,results,We outperform previous systems in all metrics .,"[('outperform', (1, 2)), ('in', (4, 5))]","[('previous systems', (2, 4)), ('all metrics', (5, 7))]","[['previous systems', 'in', 'all metrics']]",[],[],[],coreference_resolution,8,153
270,results,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .","[('improves', (6, 7)), ('by', (17, 18)), ('by', (28, 29))]","[('our single model', (3, 6)), ('state - of - the - art average F1', (8, 17)), ('1.5', (18, 19)), ('our 5 - model ensemble', (21, 26)), ('improves', (26, 27)), ('3.1', (29, 30))]","[['our single model', 'improves', 'state - of - the - art average F1'], ['our single model', 'improves', 'our 5 - model ensemble'], ['state - of - the - art average F1', 'by', '1.5'], ['improves', 'by', '3.1']]","[['our 5 - model ensemble', 'has', 'improves']]",[],[],coreference_resolution,8,154
271,results,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .","[('from', (5, 6)), ('in', (7, 8))]","[('most significant gains', (1, 4)), ('improvements', (6, 7)), ('recall', (8, 9))]","[['most significant gains', 'from', 'improvements'], ['improvements', 'in', 'recall']]",[],[],"[['Results', 'has', 'most significant gains']]",coreference_resolution,8,155
272,ablation-analysis,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .","[('between', (2, 3)), ('for', (12, 13))]","[('distance', (1, 2)), ('spans and the width of spans', (3, 9)), ('crucial signals', (10, 12)), ('coreference resolution', (13, 15))]","[['distance', 'between', 'spans and the width of spans'], ['crucial signals', 'for', 'coreference resolution']]",[],[],"[['Ablation analysis', 'has', 'distance']]",coreference_resolution,8,162
273,ablation-analysis,They contribute 3.8 F1 to the final result .,"[('contribute', (1, 2)), ('to', (4, 5))]","[('3.8 F1', (2, 4)), ('final result', (6, 8))]","[['3.8 F1', 'to', 'final result']]",[],"[['Ablation analysis', 'contribute', '3.8 F1']]",[],coreference_resolution,8,163
274,ablation-analysis,"Since coreference decisions often involve rare named entities , we see a contribution of 0.9 F1 from character - level modeling .","[('see', (10, 11)), ('of', (13, 14)), ('from', (16, 17))]","[('contribution', (12, 13)), ('0.9 F1', (14, 16)), ('character - level modeling', (17, 21))]","[['contribution', 'of', '0.9 F1'], ['0.9 F1', 'from', 'character - level modeling']]",[],"[['Ablation analysis', 'see', 'contribution']]",[],coreference_resolution,8,169
275,ablation-analysis,Ablations also show a 1.3 F1 degradation in performance without the attention mechanism for finding task - specific heads .,"[('show', (2, 3)), ('without', (9, 10)), ('for finding', (13, 15))]","[('1.3 F1', (4, 6)), ('degradation', (6, 7)), ('attention mechanism', (11, 13)), ('task - specific heads', (15, 19))]","[['degradation', 'without', 'attention mechanism'], ['attention mechanism', 'for finding', 'task - specific heads']]","[['1.3 F1', 'has', 'degradation']]","[['Ablation analysis', 'show', '1.3 F1']]",[],coreference_resolution,8,173
276,ablation-analysis,"As shown in , keeping mention candidates detected by the rule - based system over predicted parse trees ( Raghunathan et al. , 2010 ) degrades performance by 1 F1 .","[('keeping', (4, 5)), ('detected by', (7, 9)), ('over', (14, 15)), ('degrades', (25, 26)), ('by', (27, 28))]","[('mention candidates', (5, 7)), ('rule - based system', (10, 14)), ('predicted parse trees', (15, 18)), ('performance', (26, 27)), ('1 F1', (28, 30))]","[['mention candidates', 'detected by', 'rule - based system'], ['mention candidates', 'over', 'predicted parse trees'], ['rule - based system', 'over', 'predicted parse trees'], ['mention candidates', 'degrades', 'performance'], ['rule - based system', 'degrades', 'performance'], ['predicted parse trees', 'degrades', 'performance'], ['performance', 'by', '1 F1']]","[['mention candidates', 'has', 'rule - based system']]","[['Ablation analysis', 'keeping', 'mention candidates']]",[],coreference_resolution,8,181
277,ablation-analysis,"With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .","[('With', (0, 1)), ('see', (5, 6)), ('of', (8, 9))]","[('oracle mentions', (1, 3)), ('improvement', (7, 8)), ('17.5 F1', (9, 11))]","[['oracle mentions', 'see', 'improvement'], ['improvement', 'of', '17.5 F1']]","[['oracle mentions', 'has', 'improvement']]","[['Ablation analysis', 'With', 'oracle mentions']]",[],coreference_resolution,8,183
278,research-problem,BERT for Coreference Resolution : Baselines and Analysis,[],"[('Coreference Resolution', (2, 4))]",[],[],[],[],coreference_resolution,9,2
279,model,"We fine - tune BERT to coreference resolution , achieving strong improvements on the GAP and benchmarks .","[('fine - tune', (1, 4)), ('to', (5, 6))]","[('BERT', (4, 5)), ('coreference resolution', (6, 8))]","[['BERT', 'to', 'coreference resolution']]",[],"[['Model', 'fine - tune', 'BERT']]",[],coreference_resolution,9,11
280,model,We present two ways of extending the c 2f - coref model in .,"[('present', (1, 2)), ('of', (4, 5)), ('extending', (5, 6))]","[('two ways', (2, 4)), ('c 2f - coref model', (7, 12))]","[['two ways', 'extending', 'c 2f - coref model']]","[['two ways', 'name', 'c 2f - coref model']]","[['Model', 'present', 'two ways']]",[],coreference_resolution,9,12
281,code,1 https://github.com/mandarjoshi90/coref,[],"[('https://github.com/mandarjoshi90/coref', (1, 2))]",[],[],[],[],coreference_resolution,9,16
282,hyperparameters,We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,"[('extend', (1, 2)), ('of', (6, 7))]","[('original Tensorflow implementations', (3, 6)), ('c 2f - coref 3 and BERT', (7, 14))]","[['original Tensorflow implementations', 'of', 'c 2f - coref 3 and BERT']]",[],"[['Hyperparameters', 'extend', 'original Tensorflow implementations']]",[],coreference_resolution,9,63
283,hyperparameters,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .","[('fine tune', (1, 3)), ('on', (5, 6)), ('for', (10, 11)), ('using', (13, 14)), ('of', (16, 17)), ('of', (22, 23)), ('with', (31, 32)), ('for', (34, 35))]","[('all models', (3, 5)), ('OntoNotes English data', (7, 10)), ('20 epochs', (11, 13)), ('dropout', (15, 16)), ('0.3', (17, 18)), ('learning rates', (20, 22)), ('1 10 ?5 and 2 10 ? 4', (23, 31)), ('linear decay', (32, 34)), ('BERT parameters', (36, 38)), ('task parameters', (40, 42))]","[['all models', 'on', 'OntoNotes English data'], ['OntoNotes English data', 'for', '20 epochs'], ['20 epochs', 'using', 'dropout'], ['20 epochs', 'using', 'learning rates'], ['dropout', 'of', '0.3'], ['learning rates', 'of', '1 10 ?5 and 2 10 ? 4'], ['1 10 ?5 and 2 10 ? 4', 'with', 'linear decay'], ['1 10 ?5 and 2 10 ? 4', 'with', 'task parameters'], ['linear decay', 'for', 'BERT parameters']]","[['dropout', 'has', '0.3']]","[['Hyperparameters', 'fine tune', 'all models']]",[],coreference_resolution,9,64
284,hyperparameters,"We trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for BERT - base and BERT - large respectively .","[('trained', (1, 2)), ('with', (4, 5)), ('of', (8, 9))]","[('separate models', (2, 4)), ('max segment len', (5, 8)), ('128', (9, 10)), ('256', (11, 12)), ('384', (13, 14)), ('512', (16, 17))]","[['separate models', 'with', 'max segment len'], ['max segment len', 'of', '128']]",[],"[['Hyperparameters', 'trained', 'separate models']]",[],coreference_resolution,9,66
285,results,Paragraph Level : GAP,[],"[('Paragraph Level : GAP', (0, 4))]",[],[],[],"[['Results', 'has', 'Paragraph Level : GAP']]",coreference_resolution,9,70
286,results,Table 2 shows that BERT improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .,"[('shows', (2, 3)), ('improves', (5, 6)), ('by', (11, 12)), ('for', (17, 18))]","[('BERT', (4, 5)), ('c 2 f - coref', (6, 11)), ('9 % and 11.5 %', (12, 17)), ('base and large models', (19, 23))]","[['BERT', 'improves', 'c 2 f - coref'], ['c 2 f - coref', 'by', '9 % and 11.5 %'], ['9 % and 11.5 %', 'for', 'base and large models']]",[],"[['Results', 'shows', 'BERT']]",[],coreference_resolution,9,76
287,experiments,Document Level : OntoNotes,[],"[('Document Level : OntoNotes', (0, 4))]",[],[],[],[],coreference_resolution,9,78
288,results,shows that BERT - base offers an improvement of 0.9 % over the ELMo - based c2 fcoref model .,"[('shows', (0, 1)), ('offers', (5, 6)), ('of', (8, 9)), ('over', (11, 12))]","[('BERT - base', (2, 5)), ('improvement', (7, 8)), ('0.9 %', (9, 11)), ('ELMo - based c2 fcoref model', (13, 19))]","[['BERT - base', 'offers', 'improvement'], ['improvement', 'of', '0.9 %'], ['0.9 %', 'over', 'ELMo - based c2 fcoref model']]","[['BERT - base', 'has', 'improvement']]","[['Results', 'shows', 'BERT - base']]",[],coreference_resolution,9,83
289,results,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .","[('improves', (6, 7)), ('by', (12, 13)), ('of', (17, 18))]","[('BERT - large', (0, 3)), ('c 2 f - coref', (7, 12)), ('much larger margin', (14, 17)), ('3.9 %', (18, 20))]","[['BERT - large', 'improves', 'c 2 f - coref'], ['c 2 f - coref', 'by', 'much larger margin'], ['much larger margin', 'of', '3.9 %']]",[],[],[],coreference_resolution,9,87
290,results,We also observe that the overlap variant offers no improvement over independent .,"[('observe', (2, 3)), ('offers', (7, 8)), ('over', (10, 11))]","[('overlap variant', (5, 7)), ('no improvement', (8, 10)), ('independent', (11, 12))]","[['overlap variant', 'offers', 'no improvement'], ['no improvement', 'over', 'independent']]","[['overlap variant', 'has', 'no improvement']]","[['Results', 'observe', 'overlap variant']]",[],coreference_resolution,9,88
291,research-problem,A Hierarchical Model for Data - to - Text Generation,[],"[('Data - to - Text Generation', (4, 10))]",[],[],[],[],data-to-text_generation,0,2
292,research-problem,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .",[],"[('Transcribing structured data into natural language descriptions', (0, 7)), ('data - to - text', (18, 23))]",[],[],[],[],data-to-text_generation,0,4
293,model,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .","[('propose', (6, 7)), ('assuming', (13, 14)), ('should be', (16, 18))]","[('new structured - data encoder', (8, 13)), ('structures', (15, 16)), ('hierarchically captured', (18, 20))]","[['new structured - data encoder', 'assuming', 'structures'], ['structures', 'should be', 'hierarchically captured']]",[],"[['Model', 'propose', 'new structured - data encoder']]",[],data-to-text_generation,0,36
294,model,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .","[('focuses on', (2, 4)), ('of', (6, 7))]","[('encoding', (5, 6)), ('data - structure', (8, 11))]","[['encoding', 'of', 'data - structure']]",[],"[['Model', 'focuses on', 'encoding']]",[],data-to-text_generation,0,37
295,model,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .","[('model', (2, 3)), ('of', (6, 7)), ('using', (9, 10)), ('first encoding', (16, 18)), ('on the basis of', (20, 24)), ('then encoding', (27, 29)), ('on the basis of', (32, 36)), ('introduce', (41, 42)), ('in', (45, 46)), ('to ensure', (52, 54)), ('of', (56, 57)), ('comparison to', (62, 64)), ('integrate', (75, 76)), ('to compute', (80, 82)), ('fed into', (85, 87))]","[('general structure', (4, 6)), ('data', (8, 9)), ('two - level architecture', (11, 15)), ('all entities', (18, 20)), ('elements', (25, 26)), ('data structure', (30, 32)), ('entities', (37, 38)), ('Transformer encoder', (43, 45)), ('data - to - text models', (46, 52)), ('robust encoding', (54, 56)), ('each element / entities', (57, 61)), ('all others', (64, 66)), ('hierarchical attention mechanism', (77, 80)), ('hierarchical context', (83, 85)), ('decoder', (88, 89))]","[['general structure', 'of', 'data'], ['data', 'using', 'two - level architecture'], ['two - level architecture', 'first encoding', 'all entities'], ['all entities', 'on the basis of', 'elements'], ['data structure', 'on the basis of', 'entities'], ['Transformer encoder', 'in', 'data - to - text models'], ['data - to - text models', 'to ensure', 'robust encoding'], ['robust encoding', 'of', 'each element / entities'], ['each element / entities', 'comparison to', 'all others'], ['two - level architecture', 'integrate', 'hierarchical attention mechanism'], ['hierarchical attention mechanism', 'to compute', 'hierarchical context'], ['hierarchical context', 'fed into', 'decoder']]",[],"[['Model', 'model', 'general structure']]",[],data-to-text_generation,0,39
296,experiments,Baselines,[],"[('Baselines', (0, 1))]",[],[],[],[],data-to-text_generation,0,188
297,baselines,Wiseman is a standard encoder - decoder system with copy mechanism .,"[('with', (8, 9))]","[('Wiseman', (0, 1)), ('standard encoder - decoder system', (3, 8)), ('copy mechanism', (9, 11))]","[['standard encoder - decoder system', 'with', 'copy mechanism']]","[['Wiseman', 'has', 'standard encoder - decoder system']]",[],"[['Baselines', 'has', 'Wiseman']]",data-to-text_generation,0,191
298,baselines,"Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .","[('with', (7, 8)), ('first generated with', (15, 18)), ('replaced by', (22, 24)), ('extracted from', (26, 28)), ('by', (30, 31))]","[('Li', (0, 1)), ('standard encoder - decoder', (3, 7)), ('delayed copy mechanism', (9, 12)), ('text', (13, 14)), ('placeholders', (18, 19)), ('salient records', (24, 26)), ('table', (29, 30)), ('pointer network', (32, 34))]","[['standard encoder - decoder', 'with', 'delayed copy mechanism'], ['placeholders', 'replaced by', 'salient records'], ['salient records', 'extracted from', 'table'], ['table', 'by', 'pointer network']]","[['Li', 'has', 'standard encoder - decoder'], ['standard encoder - decoder', 'has', 'delayed copy mechanism'], ['delayed copy mechanism', 'has', 'text']]",[],"[['Baselines', 'has', 'Li']]",data-to-text_generation,0,192
299,baselines,"Puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .","[('acts in', (3, 5)), ('generates', (14, 15)), ('from', (24, 25)), ('generates', (34, 35))]","[('Puduppully - plan', (0, 3)), ('two steps', (5, 7)), ('first standard encoder - decoder', (9, 14)), ('plan', (16, 17)), ('second standard encoder - decoder', (29, 34)), ('text', (35, 36)), ('plan', (38, 39))]","[['Puduppully - plan', 'acts in', 'two steps'], ['first standard encoder - decoder', 'generates', 'plan'], ['second standard encoder - decoder', 'generates', 'text']]","[['Puduppully - plan', 'has', 'two steps'], ['two steps', 'has', 'first standard encoder - decoder']]",[],"[['Baselines', 'has', 'Puduppully - plan']]",data-to-text_generation,0,193
300,baselines,Puduppully - updt .,[],"[('Puduppully - updt', (0, 3))]",[],[],[],"[['Baselines', 'has', 'Puduppully - updt']]",data-to-text_generation,0,194
301,baselines,"It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .","[('consists in', (1, 3)), ('with', (9, 10)), ('aimed at', (13, 15)), ('during', (18, 19))]","[('standard encoder - decoder', (4, 8)), ('added module', (11, 13)), ('updating', (15, 16)), ('record representations', (16, 18)), ('generation process', (20, 22))]","[['standard encoder - decoder', 'with', 'added module'], ['added module', 'aimed at', 'updating'], ['record representations', 'during', 'generation process']]","[['updating', 'has', 'record representations']]",[],[],data-to-text_generation,0,195
302,baselines,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .","[('At', (0, 1)), ('should be', (12, 14))]","[('each decoding step', (1, 4)), ('a gated recurrent network', (5, 9)), ('computes', (9, 10)), ('records', (11, 12)), ('updated', (14, 15))]","[['records', 'should be', 'updated']]","[['each decoding step', 'has', 'a gated recurrent network'], ['a gated recurrent network', 'has', 'computes'], ['computes', 'has', 'records']]",[],[],data-to-text_generation,0,196
303,hyperparameters,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,"[('of', (2, 3)), ('of', (10, 11)), ('set to', (16, 18))]","[('size', (1, 2)), ('record value embeddings', (4, 7)), ('hidden layers', (8, 10)), ('Transformer encoders', (12, 14)), ('300', (18, 19))]","[['size', 'of', 'record value embeddings'], ['hidden layers', 'of', 'Transformer encoders'], ['hidden layers', 'of', 'Transformer encoders'], ['Transformer encoders', 'set to', '300']]",[],[],"[['Hyperparameters', 'has', 'size']]",data-to-text_generation,0,208
304,hyperparameters,We use dropout at rate 0.5 .,"[('use', (1, 2)), ('at', (3, 4))]","[('dropout', (2, 3)), ('rate', (4, 5)), ('0.5', (5, 6))]","[['dropout', 'at', 'rate']]","[['rate', 'has', '0.5']]","[['Hyperparameters', 'use', 'dropout']]",[],data-to-text_generation,0,209
305,hyperparameters,The models are trained with a batch size of 64 .,"[('trained with', (3, 5)), ('of', (8, 9))]","[('batch size', (6, 8)), ('64', (9, 10))]","[['batch size', 'of', '64']]",[],"[['Hyperparameters', 'trained with', 'batch size']]",[],data-to-text_generation,0,210
306,hyperparameters,"We follow the training procedure in and train the model for a fixed number of 25 K updates , and average the weights of the last 5 checkpoints ( at every 1 K updates ) to ensure more stability across runs .","[('train', (7, 8)), ('for', (10, 11)), ('of', (14, 15)), ('average', (20, 21)), ('of', (23, 24)), ('at', (29, 30)), ('to ensure', (35, 37)), ('across', (39, 40))]","[('model', (9, 10)), ('fixed number', (12, 14)), ('25 K updates', (15, 18)), ('weights', (22, 23)), ('last 5 checkpoints', (25, 28)), ('every 1 K updates', (30, 34)), ('more stability', (37, 39)), ('runs', (40, 41))]","[['model', 'for', 'fixed number'], ['fixed number', 'of', '25 K updates'], ['weights', 'of', 'last 5 checkpoints'], ['weights', 'of', 'last 5 checkpoints'], ['last 5 checkpoints', 'at', 'every 1 K updates'], ['last 5 checkpoints', 'to ensure', 'more stability'], ['more stability', 'across', 'runs']]",[],"[['Hyperparameters', 'train', 'model']]",[],data-to-text_generation,0,211
307,hyperparameters,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .","[('reduced by', (18, 20)), ('every', (21, 22))]","[('Adam optimizer', (6, 8)), ('initial learning rate', (10, 13)), ('0.001', (14, 15)), ('half', (20, 21)), ('10 K steps', (22, 25))]","[['initial learning rate', 'reduced by', 'half'], ['half', 'every', '10 K steps']]","[['initial learning rate', 'has', '0.001']]",[],"[['Hyperparameters', 'has', 'Adam optimizer']]",data-to-text_generation,0,212
308,hyperparameters,We used beam search with beam size of 5 during inference .,"[('used', (1, 2)), ('with', (4, 5)), ('of', (7, 8)), ('during', (9, 10))]","[('beam search', (2, 4)), ('beam size', (5, 7)), ('5', (8, 9)), ('inference', (10, 11))]","[['beam search', 'with', 'beam size'], ['beam size', 'of', '5'], ['beam size', 'during', 'inference'], ['5', 'during', 'inference']]",[],"[['Hyperparameters', 'used', 'beam search']]",[],data-to-text_generation,0,213
309,hyperparameters,All the models are implemented in Open NMT - py .,"[('implemented in', (4, 6))]","[('All the models', (0, 3)), ('Open NMT - py', (6, 10))]","[['All the models', 'implemented in', 'Open NMT - py']]",[],[],"[['Hyperparameters', 'has', 'All the models']]",data-to-text_generation,0,214
310,code,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,[],"[('https://github.com/KaijuML/data-to-text-hierarchical', (5, 6))]",[],[],[],[],data-to-text_generation,0,215
311,ablation-analysis,"As shown in , we can see the lower results obtained by the Flat scenario compared to the other scenarios ( e.g. BLEU 16.7 vs. 17.5 for resp .","[('see', (6, 7)), ('obtained by', (10, 12)), ('compared to', (15, 17))]","[('lower results', (8, 10)), ('Flat scenario', (13, 15)), ('other scenarios', (18, 20))]","[['lower results', 'obtained by', 'Flat scenario'], ['Flat scenario', 'compared to', 'other scenarios']]","[['lower results', 'has', 'Flat scenario']]",[],[],data-to-text_generation,0,226
312,ablation-analysis,"Second , the comparison between scenario Hierarchical - kv and Hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted CS - R% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .","[('between', (4, 5)), ('shows', (12, 13)), ('omitting entirely', (14, 16)), ('in', (22, 23))]","[('comparison', (3, 4)), ('scenario Hierarchical - kv and Hierarchical -k', (5, 12)), ('record values', (20, 22)), ('attention mechanism', (24, 26)), ('more effective', (27, 29))]","[['comparison', 'between', 'scenario Hierarchical - kv and Hierarchical -k'], ['record values', 'in', 'attention mechanism']]","[['comparison', 'has', 'scenario Hierarchical - kv and Hierarchical -k']]",[],"[['Ablation analysis', 'has', 'comparison']]",data-to-text_generation,0,229
313,ablation-analysis,"Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman , reinforcing the crucial role of structure in data semantics and saliency .","[('achieve', (3, 4)), ('on', (7, 8)), ('compared to', (11, 13))]","[('Our hierarchical models', (0, 3)), ('significantly better scores', (4, 7)), ('all metrics', (8, 10)), ('flat architecture', (14, 16))]","[['Our hierarchical models', 'achieve', 'significantly better scores'], ['significantly better scores', 'on', 'all metrics'], ['significantly better scores', 'compared to', 'flat architecture']]",[],[],"[['Ablation analysis', 'has', 'Our hierarchical models']]",data-to-text_generation,0,244
314,ablation-analysis,Results shows that our Flat scenario obtains a significant higher BLEU score ( 16.7 vs. 14.5 ) and generates fluent descriptions with accurate mentions ( RG - P % ) thatare also included in the gold descriptions ( CS - R% ) .,"[('shows', (1, 2)), ('obtains', (6, 7)), ('generates', (18, 19)), ('with', (21, 22)), ('included in', (32, 34))]","[('our Flat scenario', (3, 6)), ('significant higher BLEU score', (8, 12)), ('fluent descriptions', (19, 21)), ('accurate mentions', (22, 24)), ('gold descriptions', (35, 37))]","[['our Flat scenario', 'obtains', 'significant higher BLEU score'], ['our Flat scenario', 'generates', 'fluent descriptions'], ['fluent descriptions', 'with', 'accurate mentions'], ['accurate mentions', 'included in', 'gold descriptions']]",[],"[['Ablation analysis', 'shows', 'our Flat scenario']]",[],data-to-text_generation,0,251
315,ablation-analysis,"Our hierarchical models outperform the two - step decoders of Li and Puduppully - plan on both BLEU and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .","[('outperform', (3, 4)), ('of', (9, 10)), ('on both', (15, 17))]","[('two - step decoders', (5, 9)), ('Li and Puduppully - plan', (10, 15)), ('BLEU and all qualitative metrics', (17, 22))]","[['two - step decoders', 'of', 'Li and Puduppully - plan'], ['Li and Puduppully - plan', 'on both', 'BLEU and all qualitative metrics']]",[],"[['Ablation analysis', 'outperform', 'two - step decoders']]",[],data-to-text_generation,0,253
316,research-problem,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation,[],"[('Sequence - to - Sequence Natural Language Generation', (8, 16))]",[],[],[],[],data-to-text_generation,1,2
317,research-problem,Natural language generation lies at the core of generative dialogue systems and conversational agents .,[],"[('Natural language generation', (0, 3))]",[],[],[],[],data-to-text_generation,1,4
318,model,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .","[('present', (2, 3)), ('train and test', (12, 15)), ('on', (15, 16)), ('in', (20, 21))]","[('neural ensemble natural language generator', (4, 9)), ('three large unaligned datasets', (16, 20)), ('restaurant , television , and laptop domains', (22, 29))]","[['neural ensemble natural language generator', 'train and test', 'three large unaligned datasets'], ['neural ensemble natural language generator', 'on', 'three large unaligned datasets'], ['three large unaligned datasets', 'in', 'restaurant , television , and laptop domains']]",[],"[['Model', 'present', 'neural ensemble natural language generator']]",[],data-to-text_generation,1,28
319,model,"We explore novel ways to represent the MR inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .","[('explore', (1, 2)), ('to represent', (4, 6)), ('including', (10, 11)), ('for', (13, 14))]","[('novel ways', (2, 4)), ('MR inputs', (7, 9)), ('novel methods', (11, 13)), ('delexicalizing', (14, 15)), ('slots and their values', (15, 19)), ('automatic slot alignment', (20, 23)), ('semantic reranker', (31, 33))]","[['novel ways', 'to represent', 'MR inputs'], ['MR inputs', 'including', 'novel methods'], ['novel methods', 'for', 'delexicalizing']]","[['delexicalizing', 'has', 'slots and their values']]","[['Model', 'explore', 'novel ways']]",[],data-to-text_generation,1,29
320,experimental-setup,We built our ensemble model using the seq2seq framework for TensorFlow .,"[('built', (1, 2)), ('using', (5, 6)), ('for', (9, 10))]","[('ensemble model', (3, 5)), ('seq2seq framework', (7, 9)), ('TensorFlow', (10, 11))]","[['ensemble model', 'using', 'seq2seq framework'], ['seq2seq framework', 'for', 'TensorFlow']]",[],"[['Experimental setup', 'built', 'ensemble model']]",[],data-to-text_generation,1,158
321,experimental-setup,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .","[('use', (4, 5)), ('with', (9, 10)), ('per', (12, 13)), ('use', (19, 20))]","[('Our individual LSTM models', (0, 4)), ('bidirectional LSTM encoder', (6, 9)), ('512 cells', (10, 12)), ('layer', (13, 14)), ('CNN models', (17, 19)), ('pooling encoder', (21, 23))]","[['Our individual LSTM models', 'use', 'bidirectional LSTM encoder'], ['bidirectional LSTM encoder', 'with', '512 cells'], ['512 cells', 'per', 'layer'], ['CNN models', 'use', 'pooling encoder']]",[],[],"[['Experimental setup', 'has', 'Our individual LSTM models']]",data-to-text_generation,1,159
322,experimental-setup,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,"[('in', (2, 3)), ('with', (12, 13))]","[('decoder', (1, 2)), ('all models', (3, 5)), ('4 - layer RNN decoder', (7, 12)), ('512 LSTM cells per layer', (13, 18)), ('attention', (20, 21))]","[['decoder', 'in', 'all models'], ['4 - layer RNN decoder', 'with', '512 LSTM cells per layer'], ['4 - layer RNN decoder', 'with', 'attention']]","[['decoder', 'has', 'all models'], ['all models', 'has', '4 - layer RNN decoder']]",[],"[['Experimental setup', 'has', 'decoder']]",data-to-text_generation,1,160
323,experiments,Experiments on the E2E Dataset,[],"[('E2E Dataset', (3, 5))]",[],[],[],[],data-to-text_generation,1,165
324,experiments,Automatic Metric Evaluation,[],"[('Automatic Metric Evaluation', (0, 3))]",[],[],[],[],data-to-text_generation,1,170
325,results,The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo - samples in the training set .,"[('in', (2, 3)), ('clearly benefit from', (12, 15))]","[('LSTM and the CNN models', (7, 12)), ('additional pseudo - samples', (15, 19)), ('training set', (21, 23))]","[['additional pseudo - samples', 'in', 'training set'], ['LSTM and the CNN models', 'clearly benefit from', 'additional pseudo - samples']]",[],"[['Results', 'in', 'LSTM and the CNN models']]",[],data-to-text_generation,1,172
326,results,"On the official E2E test set , our ensemble model performs comparably to the baseline model , TGen , in terms of automatic metrics ) .","[('On', (0, 1)), ('performs', (10, 11)), ('to', (12, 13))]","[('official E2E test set', (2, 6)), ('our ensemble model', (7, 10)), ('comparably', (11, 12)), ('baseline model', (14, 16))]","[['our ensemble model', 'performs', 'comparably'], ['comparably', 'to', 'baseline model']]","[['official E2E test set', 'has', 'our ensemble model']]","[['Results', 'On', 'official E2E test set']]",[],data-to-text_generation,1,180
327,experiments,Experiments on TV and Laptop Datasets,"[('on', (1, 2))]","[('TV and Laptop Datasets', (2, 6))]",[],[],[],[],data-to-text_generation,1,210
328,results,"As shows , our ensemble model performs competitively with the baseline on the TV dataset , and it outperforms it on the Laptop dataset by a wide margin .","[('performs', (6, 7)), ('with', (8, 9)), ('on', (11, 12)), ('by', (24, 25))]","[('our ensemble model', (3, 6)), ('competitively', (7, 8)), ('baseline', (10, 11)), ('TV dataset', (13, 15)), ('outperforms', (18, 19)), ('Laptop dataset', (22, 24)), ('wide margin', (26, 28))]","[['our ensemble model', 'performs', 'competitively'], ['competitively', 'with', 'baseline'], ['baseline', 'on', 'TV dataset'], ['outperforms', 'on', 'Laptop dataset'], ['outperforms', 'by', 'wide margin'], ['Laptop dataset', 'by', 'wide margin']]","[['outperforms', 'has', 'Laptop dataset']]",[],"[['Results', 'has', 'our ensemble model']]",data-to-text_generation,1,212
329,research-problem,Deep Graph Convolutional Encoders for Structured Data to Text Generation,[],"[('Structured Data to Text Generation', (5, 10))]",[],[],[],[],data-to-text_generation,2,2
330,research-problem,Most previous work on neural text generation from graph - structured data relies on standard sequence - to - sequence methods .,[],"[('neural text generation from graph - structured data', (4, 12))]",[],[],[],[],data-to-text_generation,2,4
331,research-problem,Most previous work casts the graph structured data to text generation task as a sequenceto - sequence problem .,[],"[('graph structured data to text generation', (5, 11))]",[],[],[],[],data-to-text_generation,2,22
332,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],data-to-text_generation,2,131
333,results,WebNLG task,[],"[('WebNLG task', (0, 2))]",[],[],[],"[['Results', 'has', 'WebNLG task']]",data-to-text_generation,2,132
334,results,"In this setting , the model with GCN encoder outperforms a strong baseline that employs the LSTM encoder , with .009 BLEU points .","[('with', (6, 7)), ('outperforms', (9, 10)), ('employs', (14, 15)), ('with', (19, 20))]","[('the model', (4, 6)), ('GCN encoder', (7, 9)), ('strong baseline', (11, 13)), ('LSTM encoder', (16, 18)), ('.009 BLEU points', (20, 23))]","[['the model', 'with', 'GCN encoder'], ['LSTM encoder', 'with', '.009 BLEU points'], ['GCN encoder', 'outperforms', 'strong baseline'], ['strong baseline', 'employs', 'LSTM encoder'], ['LSTM encoder', 'with', '.009 BLEU points']]","[['the model', 'has', 'GCN encoder']]",[],[],data-to-text_generation,2,134
335,results,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,"[('than', (7, 8)), ('with', (10, 11)), ('of', (14, 15))]","[('GCN model', (1, 3)), ('more stable', (5, 7)), ('baseline', (9, 10)), ('standard deviation', (12, 14)), ('.004 vs . 010', (15, 19))]","[['more stable', 'than', 'baseline'], ['baseline', 'with', 'standard deviation'], ['standard deviation', 'of', '.004 vs . 010']]","[['GCN model', 'has', 'more stable']]",[],"[['Results', 'has', 'GCN model']]",data-to-text_generation,2,135
336,results,The GCN EC model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points ; and MELBOURNE by .014 BLEU points .,"[('outperforms', (4, 5)), ('by', (19, 20)), ('by', (26, 27))]","[('GCN EC model', (1, 4)), ('PKUWRITER', (5, 6)), ('.047 BLEU points', (20, 23)), ('MELBOURNE', (25, 26)), ('.014 BLEU points', (27, 30))]","[['GCN EC model', 'outperforms', 'PKUWRITER'], ['GCN EC model', 'outperforms', 'MELBOURNE'], ['MELBOURNE', 'by', '.014 BLEU points']]",[],[],"[['Results', 'has', 'GCN EC model']]",data-to-text_generation,2,137
337,results,SR11 Deep task,[],"[('SR11 Deep task', (0, 3))]",[],[],[],"[['Results', 'has', 'SR11 Deep task']]",data-to-text_generation,2,139
338,results,"We also compare the neural models with upper bound results on the same dataset by the pipeline model of The STUMBA - D and TBDIL model obtains respectively .794 and . 805 BLUE , outperforming the GCN - based model .","[('compare', (2, 3)), ('with', (6, 7)), ('by', (14, 15)), ('of', (18, 19)), ('obtains', (26, 27)), ('outperforming', (34, 35))]","[('neural models', (4, 6)), ('upper bound results', (7, 10)), ('pipeline model', (16, 18)), ('The STUMBA - D and TBDIL model', (19, 26)), ('.794 and . 805 BLUE', (28, 33)), ('GCN - based model', (36, 40))]","[['neural models', 'with', 'upper bound results'], ['upper bound results', 'by', 'pipeline model'], ['pipeline model', 'of', 'The STUMBA - D and TBDIL model'], ['The STUMBA - D and TBDIL model', 'obtains', '.794 and . 805 BLUE'], ['The STUMBA - D and TBDIL model', 'outperforming', 'GCN - based model']]","[['neural models', 'has', 'upper bound results']]",[],[],data-to-text_generation,2,150
339,ablation-analysis,The first thing we notice is the importance of skip connections between GCN layers .,"[('notice', (4, 5)), ('of', (8, 9)), ('between', (11, 12))]","[('importance', (7, 8)), ('skip connections', (9, 11)), ('GCN layers', (12, 14))]","[['importance', 'of', 'skip connections'], ['skip connections', 'between', 'GCN layers']]",[],"[['Ablation analysis', 'notice', 'importance']]",[],data-to-text_generation,2,163
340,ablation-analysis,Residual and dense connections lead to similar results .,"[('lead to', (4, 6))]","[('Residual and dense connections', (0, 4)), ('similar results', (6, 8))]","[['Residual and dense connections', 'lead to', 'similar results']]",[],[],"[['Ablation analysis', 'has', 'Residual and dense connections']]",data-to-text_generation,2,164
341,ablation-analysis,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .","[('produce', (9, 10)), ('than', (18, 19))]","[('Dense connections', (0, 2)), ('models', (10, 11)), ('bigger', (11, 12)), ('slightly less accurate', (14, 17)), ('residual connections', (19, 21))]","[['Dense connections', 'produce', 'models'], ['slightly less accurate', 'than', 'residual connections']]","[['models', 'has', 'bigger']]",[],"[['Ablation analysis', 'has', 'Dense connections']]",data-to-text_generation,2,165
342,research-problem,Pragmatically Informative Text Generation,[],"[('Text Generation', (2, 4))]",[],[],[],[],data-to-text_generation,3,2
343,research-problem,Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures .,[],"[('language generation and interpretation', (5, 9))]",[],[],[],[],data-to-text_generation,3,10
344,approach,"Our work builds on a line of learned Rational Speech Acts ( RSA ) models , in which generated strings are selected to optimize the behav - Human - written A cheap coffee shop in riverside with a 5 out of 5 customer rating is Fitzbillies .","[('builds on', (2, 4))]","[('learned Rational Speech Acts ( RSA ) models', (7, 15))]",[],[],"[['Approach', 'builds on', 'learned Rational Speech Acts ( RSA ) models']]",[],data-to-text_generation,3,13
345,approach,"The canonical presentation of the RSA framework ( Frank and Goodman , 2012 ) is grounded in reference resolution : models of speakers attempt to describe referents in the presence of distractors , and models of listeners attempt to resolve descriptors to referents .","[('of', (3, 4)), ('grounded in', (15, 17)), ('to', (24, 25)), ('describe', (25, 26)), ('in the presence of', (27, 31)), ('resolve', (39, 40))]","[('canonical presentation', (1, 3)), ('RSA framework', (5, 7)), ('reference resolution', (17, 19)), ('models of speakers', (20, 23)), ('referents', (26, 27)), ('distractors', (31, 32)), ('models of listeners', (34, 37)), ('descriptors', (40, 41)), ('referents', (42, 43))]","[['canonical presentation', 'of', 'RSA framework'], ['RSA framework', 'grounded in', 'reference resolution'], ['descriptors', 'to', 'referents'], ['models of speakers', 'describe', 'referents'], ['referents', 'in the presence of', 'distractors'], ['models of listeners', 'resolve', 'descriptors']]","[['canonical presentation', 'name', 'RSA framework'], ['reference resolution', 'has', 'models of speakers']]",[],"[['Approach', 'has', 'canonical presentation']]",data-to-text_generation,3,26
346,experiments,Abstractive Summarization,[],"[('Abstractive Summarization', (0, 2))]",[],[],[],[],data-to-text_generation,3,89
347,experiments,"The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .","[('obtain', (3, 4)), ('in', (7, 8)), ('0.2-1.8', (11, 12)), ('over', (13, 14)), ('with', (20, 21)), ('outperforming', (28, 29))]","[('pragmatic methods', (1, 3)), ('improvements', (4, 5)), ('0.2-0.5', (6, 7)), ('ROUGE scores', (8, 10)), ('METEOR', (12, 13)), ('base S 0 model', (15, 19)), ('distractor - based approach SD', (22, 27)), ('reconstructorbased approach S R', (30, 34))]","[['pragmatic methods', 'obtain', 'improvements'], ['0.2-0.5', 'in', 'ROUGE scores'], ['pragmatic methods', '0.2-1.8', 'METEOR'], ['METEOR', 'over', 'base S 0 model'], ['distractor - based approach SD', 'outperforming', 'reconstructorbased approach S R']]","[['pragmatic methods', 'has', 'improvements']]",[],[],data-to-text_generation,3,94
348,experiments,"SD 1 is strong across all metrics , obtaining results competitive to the best previous abstractive systems . ( b ) Coverage ratios by attribute type ( columns ) for the base model S0 , and for the pragmatic system SD 1 when constructing the distractor by masking the specified attribute ( rows ) .","[('across', (4, 5))]","[('SD', (0, 1)), ('strong', (3, 4)), ('all metrics', (5, 7))]","[['strong', 'across', 'all metrics']]","[['SD', 'has', 'strong']]",[],[],data-to-text_generation,3,95
349,research-problem,Data - to - Text Generation with Content Selection and Planning,[],"[('Data - to - Text Generation', (0, 6))]",[],[],[],[],data-to-text_generation,4,2
350,model,Our model learns a content plan from the input and conditions on the content plan in order to generate the output document ( see for an illustration ) .,"[('learns', (2, 3)), ('from', (6, 7)), ('conditions on', (10, 12)), ('to generate', (17, 19))]","[('content plan', (4, 6)), ('input', (8, 9)), ('content plan', (13, 15)), ('output document', (20, 22))]","[['content plan', 'from', 'input'], ['content plan', 'to generate', 'output document']]",[],"[['Model', 'learns', 'content plan']]",[],data-to-text_generation,4,20
351,model,"We train our model end - to - end using neural networks and evaluate its performance on ROTOWIRE , a recently released dataset which contains statistics of NBA basketball games paired with human - written summaries ( see ) .","[('train', (1, 2)), ('using', (9, 10))]","[('end - to - end', (4, 9)), ('neural networks', (10, 12))]","[['end - to - end', 'using', 'neural networks']]",[],"[['Model', 'train', 'end - to - end']]",[],data-to-text_generation,4,22
352,hyperparameters,"We used one - layer pointer networks during content planning , and two - layer LSTMs during text generation .","[('used', (1, 2)), ('during', (7, 8)), ('during', (16, 17))]","[('one - layer pointer networks', (2, 7)), ('content planning', (8, 10)), ('two - layer LSTMs', (12, 16)), ('text generation', (17, 19))]","[['one - layer pointer networks', 'during', 'content planning'], ['two - layer LSTMs', 'during', 'text generation'], ['two - layer LSTMs', 'during', 'text generation']]",[],"[['Hyperparameters', 'used', 'one - layer pointer networks']]",[],data-to-text_generation,4,158
353,hyperparameters,Input feeding was employed for the text decoder .,"[('employed for', (3, 5))]","[('Input feeding', (0, 2)), ('text decoder', (6, 8))]","[['Input feeding', 'employed for', 'text decoder']]",[],[],"[['Hyperparameters', 'has', 'Input feeding']]",data-to-text_generation,4,159
354,hyperparameters,We applied dropout ) at a rate of 0.3 .,"[('applied', (1, 2)), ('at', (4, 5)), ('of', (7, 8))]","[('dropout', (2, 3)), ('rate', (6, 7)), ('0.3', (8, 9))]","[['dropout', 'at', 'rate'], ['rate', 'of', '0.3']]",[],"[['Hyperparameters', 'applied', 'dropout']]",[],data-to-text_generation,4,160
355,hyperparameters,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .","[('trained for', (2, 4)), ('with', (6, 7)), ('selected from', (22, 24))]","[('Models', (0, 1)), ('25 epochs', (4, 6)), ('Adagrad optimizer', (8, 10)), ('initial learning rate', (12, 15)), ('0.15', (16, 17)), ('learning rate decay', (18, 21)), ('{ 0.5 , 0.97 }', (24, 29)), ('batch size', (31, 33)), ('5', (34, 35))]","[['Models', 'trained for', '25 epochs'], ['25 epochs', 'with', 'Adagrad optimizer'], ['learning rate decay', 'selected from', '{ 0.5 , 0.97 }']]","[['initial learning rate', 'has', '0.15'], ['batch size', 'has', '5']]",[],"[['Hyperparameters', 'has', 'Models']]",data-to-text_generation,4,161
356,hyperparameters,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .","[('For', (0, 1)), ('made use of', (5, 8)), ('set', (11, 12)), ('to', (15, 16))]","[('text decoding', (1, 3)), ('BPTT', (8, 9)), ('truncation size', (13, 15)), ('100', (16, 17))]","[['text decoding', 'made use of', 'BPTT'], ['truncation size', 'to', '100']]","[['truncation size', 'has', '100']]","[['Hyperparameters', 'For', 'text decoding']]",[],data-to-text_generation,4,162
357,hyperparameters,We set the beam size to 5 during inference .,"[('set', (1, 2)), ('to', (5, 6))]","[('beam size', (3, 5)), ('5', (6, 7))]","[['beam size', 'to', '5']]","[['beam size', 'has', '5']]","[['Hyperparameters', 'set', 'beam size']]",[],data-to-text_generation,4,163
358,hyperparameters,All models are implemented in Open NMT - py .,"[('implemented in', (3, 5))]","[('All models', (0, 2)), ('Open NMT - py', (5, 9))]","[['All models', 'implemented in', 'Open NMT - py']]",[],[],"[['Hyperparameters', 'has', 'All models']]",data-to-text_generation,4,164
359,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],data-to-text_generation,4,165
360,results,"As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .","[('upon', (7, 8)), ('irrespective of', (21, 23))]","[('NCP', (5, 6)), ('improves', (6, 7)), ('vanilla encoderdecoder models ( ED + JC , ED + CC )', (8, 20)), ('copy mechanism', (24, 26))]","[['improves', 'upon', 'vanilla encoderdecoder models ( ED + JC , ED + CC )'], ['improves', 'irrespective of', 'copy mechanism'], ['vanilla encoderdecoder models ( ED + JC , ED + CC )', 'irrespective of', 'copy mechanism']]","[['NCP', 'has', 'improves']]",[],"[['Results', 'has', 'NCP']]",data-to-text_generation,4,179
361,results,"In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .","[('achieves', (4, 5)), ('with', (7, 8)), ('indicates', (15, 16)), ('brings', (23, 24))]","[('comparable scores', (5, 7)), ('either joint or conditional copy mechanism', (8, 14)), ('content planner', (20, 22)), ('performance improvements', (24, 26))]","[['comparable scores', 'with', 'either joint or conditional copy mechanism'], ['content planner', 'brings', 'performance improvements']]",[],[],[],data-to-text_generation,4,180
362,results,"Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .","[('achieves', (5, 6)), ('in terms of', (13, 16))]","[('NCP + CC', (2, 5)), ('best content selection and content ordering scores', (6, 13)), ('BLEU', (16, 17))]","[['NCP + CC', 'achieves', 'best content selection and content ordering scores'], ['best content selection and content ordering scores', 'in terms of', 'BLEU']]",[],[],"[['Results', 'has', 'NCP + CC']]",data-to-text_generation,4,181
363,results,"Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .","[('Compared to', (0, 2)), ('achieve', (12, 13)), ('of', (16, 17)), ('in terms of', (20, 23)), ('by', (31, 32)), ('by', (36, 37)), ('by', (43, 44)), ('by', (49, 50))]","[('best reported system', (3, 6)), ('absolute improvement', (14, 16)), ('approximately 12 %', (17, 20)), ('relation generation', (23, 25)), ('content selection', (26, 28)), ('precision', (28, 29)), ('improves', (30, 31)), ('5 %', (32, 34)), ('recall', (35, 36)), ('15 %', (37, 39)), ('content ordering', (40, 42)), ('increases', (42, 43)), ('3 %', (44, 46)), ('BLEU', (48, 49)), ('1.5 points', (50, 52))]","[['best reported system', 'achieve', 'absolute improvement'], ['absolute improvement', 'of', 'approximately 12 %'], ['approximately 12 %', 'in terms of', 'relation generation'], ['improves', 'by', '5 %'], ['recall', 'by', '15 %'], ['increases', 'by', '3 %'], ['BLEU', 'by', '1.5 points']]","[['best reported system', 'has', 'absolute improvement'], ['content selection', 'has', 'precision'], ['precision', 'has', 'improves'], ['content ordering', 'has', 'increases']]","[['Results', 'Compared to', 'best reported system']]",[],data-to-text_generation,4,182
364,results,"As far as the template - based system is concerned , we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics .","[('obtains', (15, 16)), ('on', (24, 25))]","[('template - based system', (4, 8)), ('low', (16, 17)), ('BLEU and CS precision', (17, 21)), ('scores', (22, 23)), ('high', (23, 24)), ('CS recall and RG metrics', (25, 30))]","[['template - based system', 'obtains', 'low'], ['high', 'on', 'CS recall and RG metrics']]","[['low', 'has', 'BLEU and CS precision'], ['scores', 'has', 'high']]",[],[],data-to-text_generation,4,184
365,research-problem,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation,[],"[('Neural Data - to - Text Generation', (11, 18))]",[],[],[],[],data-to-text_generation,5,2
366,research-problem,"Data - to - text generation can be conceptually divided into two parts : ordering and structuring the information ( planning ) , and generating fluent language describing the information ( realization ) .",[],"[('Data - to - text generation', (0, 6))]",[],[],[],[],data-to-text_generation,5,5
367,model,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .","[('propose', (2, 3)), ('fed into', (15, 17))]","[('explicit , symbolic , text planning stage', (4, 11)), ('output', (13, 14)), ('neural generation system', (18, 21))]","[['output', 'fed into', 'neural generation system']]","[['explicit , symbolic , text planning stage', 'has', 'output']]","[['Model', 'propose', 'explicit , symbolic , text planning stage']]",[],data-to-text_generation,5,49
368,model,The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,"[('determines', (3, 4)), ('expresses', (8, 9)), ('of', (18, 19))]","[('text planner', (1, 3)), ('information structure', (5, 7)), ('unambiguously', (10, 11)), ('sequence', (17, 18)), ('ordered trees', (19, 21))]","[['text planner', 'determines', 'information structure'], ['text planner', 'expresses', 'unambiguously'], ['sequence', 'of', 'ordered trees']]",[],[],"[['Model', 'has', 'text planner']]",data-to-text_generation,5,50
369,model,"Once the plan is determined , 2 a neural generation system is used to transform it into fluent , natural language text .",[],"[('plan', (2, 3)), ('neural generation system', (8, 11)), ('transform', (14, 15)), ('fluent , natural language text', (17, 22))]",[],"[['plan', 'has', 'neural generation system']]",[],"[['Model', 'has', 'plan']]",data-to-text_generation,5,52
370,code,We release our code and the corpus extended with matching plans in https://github.com/AmitMY/ chimera .,[],"[('https://github.com/AmitMY/ chimera', (12, 14))]",[],[],[],[],data-to-text_generation,5,57
371,baselines,"We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .","[('in', (6, 7)), ('on', (23, 24)), ('in', (26, 27)), ('scored', (44, 45))]","[('best', (4, 5)), ('Melbourne', (11, 12)), ('end - to - end system', (14, 20)), ('scored', (21, 22)), ('best', (22, 23)), ('all categories', (24, 26)), ('automatic evaluation', (28, 30)), ('UPF - FORGe', (32, 35)), ('classic grammar - based NLG system', (37, 43)), ('human evaluation', (48, 50))]","[['all categories', 'in', 'automatic evaluation'], ['best', 'on', 'all categories'], ['all categories', 'in', 'automatic evaluation'], ['all categories', 'in', 'UPF - FORGe'], ['all categories', 'in', 'classic grammar - based NLG system']]","[['best', 'has', 'Melbourne'], ['Melbourne', 'has', 'end - to - end system'], ['scored', 'has', 'best'], ['UPF - FORGe', 'has', 'classic grammar - based NLG system']]",[],[],data-to-text_generation,5,208
372,baselines,"Additionally , we developed an end - to - end neural baseline which outperforms the WebNLG neural systems .","[('developed', (3, 4))]","[('end - to - end neural baseline', (5, 12))]",[],[],"[['Baselines', 'developed', 'end - to - end neural baseline']]",[],data-to-text_generation,5,209
373,baselines,"It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .","[('uses', (1, 2)), ('with', (9, 10))]","[('set encoder', (3, 5)), ('LSTM decoder', (7, 9)), ('attention', (10, 11)), ('copy - attention mechanism', (13, 17)), ('neural checklist model', (19, 22))]","[['LSTM decoder', 'with', 'attention'], ['LSTM decoder', 'with', 'neural checklist model']]",[],[],[],data-to-text_generation,5,210
374,results,6 Experiments and Results,[],"[('Results', (3, 4))]",[],[],[],[],data-to-text_generation,5,213
375,results,Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics,"[('outperform', (6, 7)), ('on', (12, 13))]","[('StrongNeural and BestPlan systems', (2, 6)), ('all the WebNLG participating systems', (7, 12)), ('all automatic metrics', (13, 16))]","[['StrongNeural and BestPlan systems', 'outperform', 'all the WebNLG participating systems'], ['all the WebNLG participating systems', 'on', 'all automatic metrics']]",[],[],"[['Results', 'has', 'StrongNeural and BestPlan systems']]",data-to-text_generation,5,217
376,research-problem,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation,[],"[('Character - based Data - to - text Generation', (6, 15))]",[],[],[],[],data-to-text_generation,6,2
377,research-problem,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .",[],"[('natural language generation', (19, 22))]",[],[],[],[],data-to-text_generation,6,4
378,model,"Moreover , since characters constitute the common "" building blocks "" of every text , it also allows a more general approach to text generation , enabling the possibility to exploit transfer learning for training .",[],"[('text generation', (23, 25))]",[],[],[],[],data-to-text_generation,6,8
379,model,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .","[('present', (15, 16)), ('with', (26, 27)), ('results in', (30, 32))]","[('character - level sequence - to - sequence model', (17, 26)), ('attention mechanism', (27, 29)), ('completely neural end - to - end architecture', (33, 41))]","[['character - level sequence - to - sequence model', 'with', 'attention mechanism'], ['attention mechanism', 'results in', 'completely neural end - to - end architecture']]",[],"[['Model', 'present', 'character - level sequence - to - sequence model']]",[],data-to-text_generation,6,23
380,model,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .","[('does not require', (10, 13))]","[('delexicalization', (13, 14)), ('tokenization', (15, 16)), ('lowercasing', (17, 18))]",[],[],[],[],data-to-text_generation,6,24
381,model,"As we will see , such an approach achieves rather interesting performance results and produces a vocabulary - free model that is inherently more general , as it does not depend on a specific domain 's set of terms , but rather on a general alphabet .","[('produces', (14, 15)), ('does not depend on', (28, 32)), ('on', (42, 43))]","[('vocabulary - free model', (16, 20)), ('inherently more general', (22, 25)), (""specific domain 's set of terms"", (33, 39)), ('general alphabet', (44, 46))]","[['vocabulary - free model', 'does not depend on', ""specific domain 's set of terms""], ['vocabulary - free model', 'on', 'general alphabet']]",[],"[['Model', 'produces', 'vocabulary - free model']]",[],data-to-text_generation,6,25
382,model,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .","[('of', (16, 17)), ('consisting in', (33, 35)), ('between', (38, 39)), ('improves', (67, 68)), ('enhancing', (73, 74)), ('consists in', (77, 79))]","[('character - wise copy mechanism', (27, 32)), ('soft switch', (36, 38)), ('generation and copy mode', (39, 43)), ('peculiar training procedure', (62, 65)), ('internal representation capabilities', (69, 72)), ('recall', (74, 75)), ('exchange', (80, 81)), ('encoder and decoder RNNs', (82, 86))]","[['exchange', 'of', 'encoder and decoder RNNs'], ['character - wise copy mechanism', 'consisting in', 'soft switch'], ['soft switch', 'between', 'generation and copy mode'], ['internal representation capabilities', 'enhancing', 'recall']]",[],"[['Model', 'of', 'character - wise copy mechanism']]",[],data-to-text_generation,6,27
383,experimental-setup,"We developed our system using the PyTorch framework 2 , release 0.4.1 3 .","[('developed', (1, 2)), ('using', (4, 5))]","[('system', (3, 4)), ('PyTorch framework 2 , release 0.4.1', (6, 12))]","[['system', 'using', 'PyTorch framework 2 , release 0.4.1']]",[],"[['Experimental setup', 'developed', 'system']]",[],data-to-text_generation,6,105
384,experimental-setup,"We minimize the negative log - likelihood loss using teacher forcing and Adam , the latter being an optimizer that computes individual adaptive learning rates .","[('minimize', (1, 2)), ('using', (8, 9))]","[('negative log - likelihood loss', (3, 8)), ('teacher forcing', (9, 11)), ('Adam', (12, 13))]","[['negative log - likelihood loss', 'using', 'teacher forcing'], ['negative log - likelihood loss', 'using', 'Adam']]",[],"[['Experimental setup', 'minimize', 'negative log - likelihood loss']]",[],data-to-text_generation,6,108
385,experiments,Results and Discussion,[],"[('Results', (0, 1))]",[],[],[],[],data-to-text_generation,6,118
386,results,"A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .","[('always obtains', (9, 11)), ('with respect to', (14, 17)), ('on', (18, 19))]","[('first interesting result', (1, 4)), ('EDA_CS', (8, 9)), ('higher metric values', (11, 14)), ('TGen', (17, 18)), ('Hotel and Restaurant datasets', (20, 24))]","[['EDA_CS', 'always obtains', 'higher metric values'], ['higher metric values', 'with respect to', 'TGen'], ['TGen', 'on', 'Hotel and Restaurant datasets']]","[['first interesting result', 'has', 'EDA_CS']]",[],"[['Results', 'has', 'first interesting result']]",data-to-text_generation,6,137
387,results,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .","[('achieves', (10, 11)), ('out of', (12, 14))]","[('E2E +', (6, 8)), ('TGen', (9, 10)), ('three', (11, 12)), ('five', (14, 15)), ('metrics values', (16, 18))]","[['TGen', 'achieves', 'three'], ['three', 'out of', 'five']]","[['E2E +', 'has', 'TGen']]",[],[],data-to-text_generation,6,138
388,results,"A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .","[('obtain', (12, 13)), ('with respect to', (15, 18)), ('in', (20, 21)), ('on', (24, 25))]","[('approach EDA_CS TL', (7, 10)), ('better performance', (13, 15)), ('training', (18, 19)), ('EDA_CS', (19, 20)), ('standard way', (22, 24)), ('Hotel and Restaurant datasets', (26, 30))]","[['approach EDA_CS TL', 'obtain', 'better performance'], ['better performance', 'with respect to', 'training'], ['training', 'in', 'standard way'], ['EDA_CS', 'in', 'standard way'], ['standard way', 'on', 'Hotel and Restaurant datasets']]","[['training', 'has', 'EDA_CS']]",[],"[['Results', 'has', 'approach EDA_CS TL']]",data-to-text_generation,6,140
389,results,"Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .","[('shows', (4, 5)), ('of', (8, 9)), ('with respect to', (13, 16)), ('compared to', (20, 22))]","[('EDA_CS TL', (2, 4)), ('bleu increment', (6, 8)), ('at least 14 %', (9, 13)), (""TGen 's score"", (16, 19)), ('both Hotel and Restaurant datasets', (22, 27))]","[['EDA_CS TL', 'shows', 'bleu increment'], ['bleu increment', 'of', 'at least 14 %'], ['at least 14 %', 'with respect to', ""TGen 's score""], [""TGen 's score"", 'compared to', 'both Hotel and Restaurant datasets']]","[['EDA_CS TL', 'has', 'bleu increment']]",[],"[['Results', 'has', 'EDA_CS TL']]",data-to-text_generation,6,141
390,results,"Finally , the baseline model , EDA , is largely outperformed by all other examined methods .","[('by', (11, 12))]","[('baseline model', (3, 5)), ('EDA', (6, 7)), ('largely outperformed', (9, 11)), ('all other examined methods', (12, 16))]","[['largely outperformed', 'by', 'all other examined methods']]","[['baseline model', 'name', 'EDA']]",[],"[['Results', 'has', 'baseline model']]",data-to-text_generation,6,142
391,research-problem,An improved neural network model for joint POS tagging and dependency parsing,[],"[('joint POS tagging and dependency parsing', (6, 12))]",[],[],[],[],dependency_parsing,0,2
392,research-problem,We propose a novel neural network model for joint part - of - speech ( POS ) tagging and dependency parsing .,[],"[('joint part - of - speech ( POS ) tagging and dependency parsing', (8, 21))]",[],[],[],[],dependency_parsing,0,4
393,code,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,[],"[('https://github.com/datquocnguyen/jPTDP', (11, 12))]",[],[],[],[],dependency_parsing,0,9
394,research-problem,"Dependency parsing - a key research topic in natural language processing ( NLP ) in the last decade ) - has also been demonstrated to be extremely useful in many applications such as relation extraction , semantic parsing and machine translation ) .",[],"[('Dependency parsing', (0, 2))]",[],[],[],[],dependency_parsing,0,11
395,model,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .","[('present', (5, 6)), ('for', (13, 14))]","[('novel neural network - based model', (7, 13)), ('jointly learning', (14, 16)), ('POS tagging and dependency paring', (16, 21))]","[['novel neural network - based model', 'for', 'jointly learning']]","[['jointly learning', 'has', 'POS tagging and dependency paring']]","[['Model', 'present', 'novel neural network - based model']]",[],dependency_parsing,0,18
396,model,Our joint model extends the well - known BIST graph - based dependency parser with an additional lower - level BiLSTM - based tagging component .,"[('extends', (3, 4)), ('with', (14, 15))]","[('joint model', (1, 3)), ('well - known BIST graph - based dependency parser', (5, 14)), ('additional lower - level BiLSTM - based tagging component', (16, 25))]","[['joint model', 'extends', 'well - known BIST graph - based dependency parser'], ['well - known BIST graph - based dependency parser', 'with', 'additional lower - level BiLSTM - based tagging component']]",[],[],"[['Model', 'has', 'joint model']]",dependency_parsing,0,19
397,experimental-setup,Our jPTDP v 2.0 is implemented using DYNET v2.0 with a fixed random seed .,"[('implemented using', (5, 7)), ('with', (9, 10))]","[('jPTDP v 2.0', (1, 4)), ('DYNET v2.0', (7, 9)), ('fixed random seed', (11, 14))]","[['jPTDP v 2.0', 'implemented using', 'DYNET v2.0'], ['DYNET v2.0', 'with', 'fixed random seed']]",[],[],"[['Experimental setup', 'has', 'jPTDP v 2.0']]",dependency_parsing,0,75
398,experimental-setup,"Word embeddings are initialized either randomly or by pre-trained word vectors , while character and POS tag embeddings are randomly initialized .",[],"[('Word embeddings', (0, 2)), ('initialized', (3, 4)), ('randomly', (5, 6)), ('pre-trained word vectors', (8, 11)), ('character and POS tag embeddings', (13, 18)), ('randomly initialized', (19, 21))]",[],"[['Word embeddings', 'has', 'initialized'], ['initialized', 'has', 'randomly'], ['character and POS tag embeddings', 'has', 'randomly initialized']]",[],"[['Experimental setup', 'has', 'Word embeddings']]",dependency_parsing,0,77
399,experimental-setup,We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,"[('apply', (1, 2)), ('with', (3, 4)), ('to', (9, 10)), ('of', (12, 13))]","[('dropout', (2, 3)), ('67 % keep probability', (5, 9)), ('inputs', (11, 12)), ('BiLSTMs', (13, 14)), ('MLPs', (15, 16))]","[['dropout', 'with', '67 % keep probability'], ['67 % keep probability', 'to', 'inputs'], ['inputs', 'of', 'BiLSTMs']]","[['dropout', 'has', '67 % keep probability']]","[['Experimental setup', 'apply', 'dropout']]",[],dependency_parsing,0,79
400,experimental-setup,"Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .","[('to learn', (8, 10)), ('for', (12, 13))]","[('word dropout', (6, 8)), ('embedding', (11, 12)), ('unknown words', (13, 15))]","[['word dropout', 'to learn', 'embedding'], ['embedding', 'for', 'unknown words']]",[],[],[],dependency_parsing,0,80
401,experimental-setup,"We optimize the objective loss using Adam ( Kingma and Ba , 2014 ) with an initial learning rate at 0.001 and no mini-batches .","[('optimize', (1, 2)), ('using', (5, 6)), ('with', (14, 15)), ('at', (19, 20))]","[('objective loss', (3, 5)), ('Adam', (6, 7)), ('initial learning rate', (16, 19)), ('0.001', (20, 21)), ('no mini-batches', (22, 24))]","[['objective loss', 'using', 'Adam'], ['Adam', 'with', 'initial learning rate'], ['Adam', 'with', 'no mini-batches'], ['initial learning rate', 'at', '0.001'], ['initial learning rate', 'at', 'no mini-batches']]","[['initial learning rate', 'has', '0.001']]","[['Experimental setup', 'optimize', 'objective loss']]",[],dependency_parsing,0,82
402,experimental-setup,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .","[('use', (9, 10))]","[('100 - dimensional word embeddings', (10, 15)), ('50 - dimensional character embeddings', (16, 21)), ('100 dimensional POS tag embeddings', (22, 27))]",[],[],"[['Experimental setup', 'use', '100 - dimensional word embeddings']]",[],dependency_parsing,0,86
403,experimental-setup,We also fix the number of hidden nodes in MLPs at 100 .,"[('fix', (2, 3)), ('in', (8, 9)), ('at', (10, 11))]","[('number of hidden nodes', (4, 8)), ('MLPs', (9, 10)), ('100', (11, 12))]","[['number of hidden nodes', 'in', 'MLPs'], ['number of hidden nodes', 'at', '100'], ['MLPs', 'at', '100']]",[],"[['Experimental setup', 'fix', 'number of hidden nodes']]",[],dependency_parsing,0,87
404,experiments,"Clearly , our model produces very competitive parsing results .","[('produces', (4, 5))]","[('our model', (2, 4)), ('very competitive parsing results', (5, 9))]","[['our model', 'produces', 'very competitive parsing results']]",[],[],[],dependency_parsing,0,99
405,experiments,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .","[('obtains', (5, 6)), ('at', (9, 10)), ('at', (16, 17))]","[('UAS score', (7, 9)), ('94.51 %', (10, 12)), ('LAS score', (14, 16)), ('92.87 %', (17, 19))]","[['UAS score', 'at', '94.51 %'], ['LAS score', 'at', '92.87 %'], ['LAS score', 'at', '92.87 %']]",[],[],[],dependency_parsing,0,100
406,experiments,We achieve 0.9 % lower parsing scores than the state - of - the - art dependency parser of .,"[('achieve', (1, 2)), ('than', (7, 8))]","[('0.9 % lower parsing scores', (2, 7)), ('state - of - the - art dependency parser', (9, 18))]","[['0.9 % lower parsing scores', 'than', 'state - of - the - art dependency parser']]",[],[],[],dependency_parsing,0,102
407,experiments,"We also obtain a state - of - the - art POS tagging accuracy at 97.97 % on the test Section 23 , which is about 0.4 + % higher than those by , and .","[('obtain', (2, 3)), ('at', (14, 15)), ('on', (17, 18))]","[('state - of - the - art POS tagging accuracy', (4, 14)), ('97.97 %', (15, 17)), ('test', (19, 20))]","[['state - of - the - art POS tagging accuracy', 'at', '97.97 %'], ['state - of - the - art POS tagging accuracy', 'on', 'test'], ['97.97 %', 'on', 'test']]",[],[],[],dependency_parsing,0,106
408,research-problem,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,[],"[('Dependency Parsing', (3, 5))]",[],[],[],[],dependency_parsing,1,2
409,approach,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .","[('centered around', (7, 9))]","[('BiRNNs', (9, 10))]",[],[],"[['Approach', 'centered around', 'BiRNNs']]",[],dependency_parsing,1,24
410,approach,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .","[('represent', (1, 2)), ('by', (4, 5)), ('use', (10, 11)), ('of', (13, 14)), ('of', (17, 18)), ('as', (21, 22)), ('passed to', (29, 31))]","[('each word', (2, 4)), ('BiLSTM encoding', (6, 8)), ('concatenation', (12, 13)), ('minimal set', (15, 17)), ('BiLSTM encodings', (19, 21)), ('feature function', (23, 25)), ('non-linear scoring function ( multi - layer perceptron )', (32, 41))]","[['each word', 'by', 'BiLSTM encoding'], ['concatenation', 'of', 'minimal set'], ['minimal set', 'of', 'BiLSTM encodings'], ['minimal set', 'of', 'BiLSTM encodings'], ['BiLSTM encodings', 'as', 'feature function'], ['feature function', 'passed to', 'non-linear scoring function ( multi - layer perceptron )']]","[['concatenation', 'has', 'minimal set']]","[['Approach', 'represent', 'each word']]",[],dependency_parsing,1,26
411,approach,"Crucially , the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem .","[('trained with', (5, 7)), ('to learn', (14, 16)), ('for', (20, 21))]","[('BiLSTM', (3, 4)), ('rest of the parser', (8, 12)), ('good feature representation', (17, 20)), ('parsing problem', (22, 24))]","[['BiLSTM', 'trained with', 'rest of the parser'], ['rest of the parser', 'to learn', 'good feature representation'], ['good feature representation', 'for', 'parsing problem']]",[],[],"[['Approach', 'has', 'BiLSTM']]",dependency_parsing,1,27
412,approach,"We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures , transition - based ( Section 4 ) as well as a graph - based ( Section 5 ) .","[('using', (8, 9)), ('in', (13, 14))]","[('BiLSTM feature extractor', (10, 13)), ('two parsing architectures', (14, 17)), ('transition - based', (18, 21)), ('graph - based', (29, 32))]","[['BiLSTM feature extractor', 'in', 'two parsing architectures']]","[['two parsing architectures', 'name', 'transition - based']]","[['Approach', 'using', 'BiLSTM feature extractor']]",[],dependency_parsing,1,29
413,approach,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .","[('jointly train', (6, 8)), ('on top of', (13, 16))]","[('graphbased parser', (2, 4)), ('structured - prediction model', (9, 13)), ('BiLSTM', (17, 18))]","[['graphbased parser', 'jointly train', 'structured - prediction model'], ['structured - prediction model', 'on top of', 'BiLSTM']]",[],[],[],dependency_parsing,1,30
414,hyperparameters,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .","[('implemented in', (3, 5)), ('using', (7, 8)), ('for', (12, 13))]","[('parsers', (1, 2)), ('python', (5, 6)), ('PyCNN toolkit', (9, 11)), ('neural network training', (13, 16))]","[['parsers', 'implemented in', 'python'], ['parsers', 'using', 'PyCNN toolkit'], ['python', 'using', 'PyCNN toolkit'], ['PyCNN toolkit', 'for', 'neural network training']]",[],[],"[['Hyperparameters', 'has', 'parsers']]",dependency_parsing,1,281
415,code,The code is available at the github repository https://github.com/elikip / bist -parser .,[],"[('https://github.com/elikip / bist -parser', (8, 12))]",[],[],[],[],dependency_parsing,1,282
416,hyperparameters,"We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .","[('use', (1, 2)), ('implemented in', (5, 7)), ('using', (11, 12))]","[('LSTM variant', (3, 5)), ('PyCNN', (7, 8)), ('optimize', (10, 11)), ('Adam optimizer', (13, 15))]","[['LSTM variant', 'implemented in', 'PyCNN'], ['LSTM variant', 'using', 'Adam optimizer'], ['optimize', 'using', 'Adam optimizer']]",[],"[['Hyperparameters', 'use', 'LSTM variant']]",[],dependency_parsing,1,283
417,results,"It is clear that our parsers are very competitive , despite using very simple parsing architectures and minimal feature extractors .",[],"[('our parsers', (4, 6)), ('very competitive', (7, 9))]",[],"[['our parsers', 'has', 'very competitive']]",[],"[['Results', 'has', 'our parsers']]",dependency_parsing,1,299
418,results,"When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .","[('When', (0, 1)), ('with', (14, 15)), ('including', (27, 28))]","[('not using external embeddings', (1, 5)), ('first - order graph - based parser', (7, 14)), ('2 features', (15, 17)), ('outperforms', (17, 18)), ('all other systems', (18, 21)), ('not using external resources', (22, 26)), ('third - order TurboParser', (29, 33))]","[['first - order graph - based parser', 'with', '2 features'], ['not using external resources', 'including', 'third - order TurboParser']]","[['not using external embeddings', 'has', 'first - order graph - based parser'], ['2 features', 'has', 'outperforms'], ['outperforms', 'has', 'all other systems']]","[['Results', 'When', 'not using external embeddings']]",[],dependency_parsing,1,300
419,results,"The greedy transition based parser with 4 features also matches or outperforms most other parsers , including the beam - based transition parser with heavily engineered features of and the Stack - LSTM parser of , as well as the same parser when trained using a dynamic oracle .","[('with', (5, 6)), ('matches or outperforms', (9, 12)), ('including', (16, 17)), ('with', (23, 24))]","[('greedy transition based parser', (1, 5)), ('4 features', (6, 8)), ('most other parsers', (12, 15)), ('beam - based transition parser', (18, 23)), ('heavily engineered features', (24, 27))]","[['greedy transition based parser', 'with', '4 features'], ['beam - based transition parser', 'with', 'heavily engineered features'], ['4 features', 'matches or outperforms', 'most other parsers'], ['most other parsers', 'including', 'beam - based transition parser'], ['beam - based transition parser', 'with', 'heavily engineered features']]","[['greedy transition based parser', 'has', '4 features']]",[],"[['Results', 'has', 'greedy transition based parser']]",dependency_parsing,1,301
420,results,Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,"[('Moving from', (0, 2)), ('to', (8, 9)), ('leads to', (17, 19)), ('in', (21, 22)), ('for', (23, 24))]","[('simple ( 4 features )', (3, 8)), ('extended ( 11 features ) feature set', (10, 17)), ('some gains', (19, 21)), ('accuracy', (22, 23)), ('English', (25, 26)), ('Chinese', (27, 28))]","[['simple ( 4 features )', 'to', 'extended ( 11 features ) feature set'], ['simple ( 4 features )', 'leads to', 'some gains'], ['extended ( 11 features ) feature set', 'leads to', 'some gains'], ['some gains', 'in', 'accuracy'], ['accuracy', 'for', 'English'], ['accuracy', 'for', 'Chinese']]",[],"[['Results', 'Moving from', 'simple ( 4 features )']]",[],dependency_parsing,1,302
421,results,"Interestingly , when adding external word embeddings the accuracy of the graph - based parser degrades .","[('when adding', (2, 4)), ('of', (9, 10))]","[('external word embeddings', (4, 7)), ('accuracy', (8, 9)), ('graph - based parser', (11, 15)), ('degrades', (15, 16))]","[['accuracy', 'of', 'graph - based parser']]","[['external word embeddings', 'has', 'accuracy']]","[['Results', 'when adding', 'external word embeddings']]",[],dependency_parsing,1,303
422,model,"We give a probabilistic interpretation to the ensemble parser ( with a minor modification ) , viewing it as an instance of minimum Bayes risk inference .","[('give', (1, 2)), ('to', (5, 6)), ('of', (21, 22))]","[('probabilistic interpretation', (3, 5)), ('ensemble parser', (7, 9)), ('viewing', (16, 17)), ('instance', (20, 21)), ('minimum Bayes risk inference', (22, 26))]","[['probabilistic interpretation', 'to', 'ensemble parser'], ['instance', 'of', 'minimum Bayes risk inference']]","[['viewing', 'has', 'instance']]","[['Model', 'give', 'probabilistic interpretation']]",[],dependency_parsing,2,14
423,model,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .","[('distilling', (7, 8)), ('into', (10, 11)), ('with', (15, 16)), ('defining', (19, 20))]","[('ensemble', (9, 10)), ('single FOG parser', (12, 15)), ('discriminative training', (16, 18)), ('new cost function', (21, 24))]","[['ensemble', 'into', 'single FOG parser'], ['single FOG parser', 'with', 'discriminative training'], ['discriminative training', 'defining', 'new cost function']]",[],"[['Model', 'distilling', 'ensemble']]",[],dependency_parsing,2,18
424,model,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .","[('derive', (5, 6)), ('of', (8, 9)), ('from', (12, 13)), ('use', (21, 22))]","[('cost', (7, 8)), ('each possible attachment', (9, 12)), (""ensemble 's division of votes"", (14, 19)), ('discriminative learning', (25, 27))]","[['cost', 'of', 'each possible attachment'], ['each possible attachment', 'from', ""ensemble 's division of votes""]]",[],"[['Model', 'derive', 'cost']]",[],dependency_parsing,2,19
425,research-problem,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .",[],"[('graphbased dependency parsing', (9, 12))]",[],[],[],[],dependency_parsing,2,22
426,ablation-analysis,"First , consider the neural FOG parser trained with Hamming cost ( C H in the second - to - last row ) .","[('consider', (2, 3)), ('trained with', (7, 9))]","[('neural FOG parser', (4, 7)), ('Hamming cost', (9, 11))]","[['neural FOG parser', 'trained with', 'Hamming cost']]",[],"[['Ablation analysis', 'consider', 'neural FOG parser']]",[],dependency_parsing,2,176
427,results,"This is a very strong benchmark , outperforming many higherorder graph - based and neural network models on all three datasets .","[('is', (1, 2)), ('outperforming', (7, 8)), ('on', (17, 18))]","[('very strong benchmark', (3, 6)), ('many higherorder graph - based and neural network models', (8, 17)), ('all three datasets', (18, 21))]","[['very strong benchmark', 'outperforming', 'many higherorder graph - based and neural network models'], ['many higherorder graph - based and neural network models', 'on', 'all three datasets']]",[],[],[],dependency_parsing,2,177
428,results,"Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .","[('training', (2, 3)), ('gives', (9, 10)), ('for', (12, 13))]","[('distillation cost', (7, 9)), ('consistent improvements', (10, 12)), ('all languages', (13, 15))]","[['distillation cost', 'gives', 'consistent improvements'], ['consistent improvements', 'for', 'all languages']]",[],"[['Results', 'training', 'distillation cost']]",[],dependency_parsing,2,178
429,results,"For English , we see that this model comes close to the slower ensemble it was trained to simulate .","[('For', (0, 1)), ('to', (10, 11))]","[('English', (1, 2)), ('comes close', (8, 10)), ('slower ensemble', (12, 14))]","[['comes close', 'to', 'slower ensemble']]","[['English', 'has', 'comes close']]","[['Results', 'For', 'English']]",[],dependency_parsing,2,179
430,results,"For Chinese , it achieves the best published scores , for German the best published UAS scores , and just after Bohnet and Nivre ( 2012 ) for LAS .","[('achieves', (4, 5)), ('for', (10, 11))]","[('Chinese', (1, 2)), ('best published scores', (6, 9)), ('German', (11, 12)), ('best published UAS scores', (13, 17))]","[['Chinese', 'achieves', 'best published scores'], ['best published scores', 'for', 'German']]","[['German', 'has', 'best published UAS scores']]",[],[],dependency_parsing,2,180
431,research-problem,From POS tagging to dependency parsing for biomedical event extraction,[],"[('POS tagging', (1, 3)), ('dependency parsing', (4, 6))]",[],"[['POS tagging', 'has', 'dependency parsing']]",[],[],dependency_parsing,3,2
432,code,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep .,[],"[('https://github.com/datquocnguyen/BioPosDep', (7, 8))]",[],[],[],[],dependency_parsing,3,12
433,research-problem,"In this paper , we therefore investigate current stateof - the - art ( SOTA ) approaches to dependency parsing as applied to biomedical texts .","[('investigate', (6, 7)), ('to', (17, 18)), ('applied to', (21, 23))]","[('current stateof - the - art ( SOTA ) approaches', (7, 17)), ('dependency parsing', (18, 20)), ('biomedical texts', (23, 25))]","[['current stateof - the - art ( SOTA ) approaches', 'to', 'dependency parsing'], ['dependency parsing', 'applied to', 'biomedical texts']]",[],"[['Research problem', 'investigate', 'current stateof - the - art ( SOTA ) approaches']]",[],dependency_parsing,3,31
434,model,"Finally , we study the impact of parser choice on biomedical event extraction , following the structure of the extrinsic parser evaluation shared task ( EPE 2017 ) for biomedical event extraction .","[('study', (3, 4)), ('of', (6, 7)), ('on', (9, 10))]","[('impact', (5, 6)), ('parser choice', (7, 9)), ('biomedical event extraction', (10, 13))]","[['impact', 'of', 'parser choice'], ['parser choice', 'on', 'biomedical event extraction']]","[['impact', 'has', 'parser choice']]","[['Model', 'study', 'impact']]",[],dependency_parsing,3,33
435,experimental-setup,"For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .","[('For', (0, 1)), ('employ', (26, 27))]","[('three BiLSTM - CRF - based models', (2, 9)), ('Stanford - NNdep', (10, 13)), ('jPTDP', (14, 15)), ('Stanford - Biaffine', (16, 19)), ('200 dimensional pre-trained word vectors', (27, 32))]",[],"[['three BiLSTM - CRF - based models', 'name', 'Stanford - NNdep']]","[['Experimental setup', 'For', 'three BiLSTM - CRF - based models']]",[],dependency_parsing,3,97
436,experimental-setup,"For the traditional feature - based models MarMoT , NLP4J - POS and NLP4J - dep , we use their original pure Java implementations with default hyperparameter settings .","[('use', (18, 19)), ('with', (24, 25))]","[('traditional feature - based models', (2, 7)), ('MarMoT', (7, 8)), ('NLP4J - POS', (9, 12)), ('NLP4J - dep', (13, 16)), ('original pure Java implementations', (20, 24)), ('default hyperparameter settings', (25, 28))]","[['traditional feature - based models', 'use', 'original pure Java implementations'], ['NLP4J - dep', 'use', 'original pure Java implementations'], ['original pure Java implementations', 'with', 'default hyperparameter settings']]","[['traditional feature - based models', 'name', 'MarMoT']]",[],[],dependency_parsing,3,99
437,experimental-setup,"For the BiLSTM - CRF - based models , we use default hyper - parameters provided in with the following exceptions : for training , we use Nadam and run for 50 epochs .","[('use', (10, 11)), ('run for', (29, 31))]","[('BiLSTM - CRF - based models', (2, 8)), ('Nadam', (27, 28)), ('50 epochs', (31, 33))]",[],[],[],[],dependency_parsing,3,100
438,experimental-setup,"For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .","[('select', (6, 7)), ('from', (10, 11)), ('of', (19, 20)), ('from', (23, 24))]","[('Stanford - NNdep', (1, 4)), ('word CutOff', (8, 10)), ('{ 1 , 2 }', (11, 16)), ('size', (18, 19)), ('hidden layer', (21, 23)), ('{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }', (24, 39))]","[['Stanford - NNdep', 'select', 'word CutOff'], ['word CutOff', 'from', '{ 1 , 2 }'], ['hidden layer', 'from', '{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }'], ['size', 'of', 'hidden layer'], ['hidden layer', 'from', '{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }']]",[],[],[],dependency_parsing,3,103
439,experimental-setup,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .","[('use', (4, 5)), ('fix', (11, 12)), ('at', (16, 17))]","[('jPTDP', (1, 2)), ('50 - dimensional character embeddings', (5, 10)), ('initial learning rate', (13, 16)), ('0.0005', (17, 18))]","[['jPTDP', 'use', '50 - dimensional character embeddings'], ['50 - dimensional character embeddings', 'fix', 'initial learning rate'], ['initial learning rate', 'at', '0.0005']]",[],[],[],dependency_parsing,3,104
440,experimental-setup,"We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .","[('of', (5, 6)), ('at', (8, 9)), ('select', (11, 12)), ('of', (14, 15)), ('in', (17, 18)), ('from', (20, 21))]","[('number', (4, 5)), ('BiLSTM layers', (6, 8)), ('2', (9, 10)), ('number', (13, 14)), ('LSTM units', (15, 17)), ('each layer', (18, 20)), ('{ 100 , 150 , 200 , 250 , 300 }', (21, 32))]","[['number', 'of', 'BiLSTM layers'], ['number', 'of', 'LSTM units'], ['BiLSTM layers', 'at', '2'], ['number', 'of', 'LSTM units'], ['LSTM units', 'in', 'each layer'], ['each layer', 'from', '{ 100 , 150 , 200 , 250 , 300 }']]",[],[],[],dependency_parsing,3,105
441,results,POS tagging results,[],"[('POS tagging results', (0, 3))]",[],[],[],"[['Results', 'has', 'POS tagging results']]",dependency_parsing,3,118
442,results,"BiLSTM - CRF and Mar - MoT obtain the lowest scores on GENIA and CRAFT , respectively .","[('obtain', (7, 8)), ('on', (11, 12))]","[('BiLSTM - CRF and Mar - MoT', (0, 7)), ('lowest scores', (9, 11)), ('GENIA', (12, 13)), ('CRAFT', (14, 15))]","[['BiLSTM - CRF and Mar - MoT', 'obtain', 'lowest scores'], ['lowest scores', 'on', 'GENIA'], ['lowest scores', 'on', 'CRAFT']]",[],[],"[['Results', 'has', 'BiLSTM - CRF and Mar - MoT']]",dependency_parsing,3,120
443,results,jPTDP obtains a similar score to Mar - MoT on GENIA and similar score to BiLSTM - CRF on CRAFT .,"[('obtains', (1, 2)), ('to', (5, 6)), ('on', (9, 10)), ('on', (18, 19))]","[('jPTDP', (0, 1)), ('similar score', (3, 5)), ('Mar - MoT', (6, 9)), ('GENIA', (10, 11)), ('BiLSTM - CRF', (15, 18)), ('CRAFT', (19, 20))]","[['jPTDP', 'obtains', 'similar score'], ['similar score', 'to', 'Mar - MoT'], ['Mar - MoT', 'on', 'GENIA'], ['BiLSTM - CRF', 'on', 'CRAFT'], ['BiLSTM - CRF', 'on', 'CRAFT']]",[],[],"[['Results', 'has', 'jPTDP']]",dependency_parsing,3,121
444,results,"In particular , MarMoT obtains accuracy results at 98.61 % and 97.07 % on GENIA and CRAFT , which are about 0.2 % and 0.4 % absolute lower than NLP4J - POS , respectively .","[('obtains', (4, 5)), ('at', (7, 8)), ('on', (13, 14))]","[('MarMoT', (3, 4)), ('accuracy results', (5, 7)), ('98.61 % and 97.07 %', (8, 13)), ('GENIA and CRAFT', (14, 17))]","[['MarMoT', 'obtains', 'accuracy results'], ['accuracy results', 'at', '98.61 % and 97.07 %'], ['98.61 % and 97.07 %', 'on', 'GENIA and CRAFT']]","[['MarMoT', 'has', 'accuracy results']]",[],"[['Results', 'has', 'MarMoT']]",dependency_parsing,3,122
445,results,BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .,"[('obtains', (3, 4)), ('of', (5, 6)), ('on', (8, 9)), ('on', (15, 16))]","[('BiLSTM - CRF', (0, 3)), ('accuracies', (4, 5)), ('98.44 %', (6, 8)), ('GE - NIA', (9, 12)), ('97.25 %', (13, 15)), ('CRAFT', (16, 17))]","[['BiLSTM - CRF', 'obtains', 'accuracies'], ['accuracies', 'of', '98.44 %'], ['98.44 %', 'on', 'GE - NIA'], ['98.44 %', 'on', 'CRAFT'], ['97.25 %', 'on', 'CRAFT'], ['97.25 %', 'on', 'CRAFT']]","[['BiLSTM - CRF', 'has', 'accuracies']]",[],"[['Results', 'has', 'BiLSTM - CRF']]",dependency_parsing,3,124
446,results,"Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .","[('for', (2, 3)), ('provided', (14, 15)), ('to', (19, 20))]","[('PTB', (3, 4)), ('CNN - based character - level word embeddings', (5, 13)), ('0.1 % improvement', (16, 19)), ('BiLSTM - CRF', (20, 23))]","[['CNN - based character - level word embeddings', 'provided', '0.1 % improvement'], ['0.1 % improvement', 'to', 'BiLSTM - CRF']]","[['PTB', 'has', 'CNN - based character - level word embeddings']]","[['Results', 'for', 'PTB']]",[],dependency_parsing,3,130
447,results,"On both GENIA and CRAFT , BiLSTM - CRF with character - level word embeddings obtains the highest accuracy scores .","[('On both', (0, 2)), ('with', (9, 10)), ('obtains', (15, 16))]","[('GENIA and CRAFT', (2, 5)), ('BiLSTM - CRF', (6, 9)), ('character - level word embeddings', (10, 15)), ('highest accuracy scores', (17, 20))]","[['BiLSTM - CRF', 'with', 'character - level word embeddings'], ['character - level word embeddings', 'obtains', 'highest accuracy scores']]","[['GENIA and CRAFT', 'has', 'BiLSTM - CRF']]","[['Results', 'On both', 'GENIA and CRAFT']]",[],dependency_parsing,3,135
448,results,Overall dependency parsing results,[],"[('Overall dependency parsing results', (0, 4))]",[],[],[],"[['Results', 'has', 'Overall dependency parsing results']]",dependency_parsing,3,140
449,results,"On GENIA , among pre-trained models , BLLIP obtains highest results .","[('On', (0, 1)), ('among', (3, 4)), ('obtains', (8, 9))]","[('GENIA', (1, 2)), ('pre-trained models', (4, 6)), ('BLLIP', (7, 8)), ('highest results', (9, 11))]","[['GENIA', 'among', 'pre-trained models'], ['BLLIP', 'obtains', 'highest results']]","[['GENIA', 'has', 'pre-trained models'], ['pre-trained models', 'has', 'BLLIP']]","[['Results', 'On', 'GENIA']]",[],dependency_parsing,3,148
450,results,The pre-trained Stanford - Biaffine ( v1 ) model produces lower scores than the pre-trained Stanford - NNdep model on GENIA .,"[('produces', (9, 10)), ('than', (12, 13)), ('on', (19, 20))]","[('pre-trained Stanford - Biaffine ( v1 ) model', (1, 9)), ('lower scores', (10, 12)), ('pre-trained Stanford - NNdep model', (14, 19)), ('GENIA', (20, 21))]","[['pre-trained Stanford - Biaffine ( v1 ) model', 'produces', 'lower scores'], ['lower scores', 'than', 'pre-trained Stanford - NNdep model'], ['pre-trained Stanford - NNdep model', 'on', 'GENIA']]",[],[],[],dependency_parsing,3,150
451,results,Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .,"[('result in', (8, 10)), ('at', (27, 28)), ('vs.', (30, 31)), ('at', (37, 38))]","[('pre-trained NNdep and Biaffine models', (3, 8)), ('no significant performance differences', (10, 14)), ('pre-trained Stanford tagger', (24, 27)), ('98.37 %', (28, 30)), ('retrained NLP4J - POS model', (32, 37)), ('98.80 %', (38, 40))]","[['pre-trained NNdep and Biaffine models', 'result in', 'no significant performance differences'], ['pre-trained Stanford tagger', 'at', '98.37 %'], ['98.37 %', 'vs.', 'retrained NLP4J - POS model'], ['retrained NLP4J - POS model', 'at', '98.80 %']]",[],[],"[['Results', 'has', 'pre-trained NNdep and Biaffine models']]",dependency_parsing,3,152
452,research-problem,Stack - Pointer Networks for Dependency Parsing,[],"[('Dependency Parsing', (5, 7))]",[],[],[],[],dependency_parsing,4,2
453,model,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .","[('propose', (5, 6)), ('for', (11, 12))]","[('novel neural network architecture', (7, 11)), ('dependency parsing', (12, 14)), ('stackpointer networks ( STACKPTR )', (15, 20))]","[['novel neural network architecture', 'for', 'dependency parsing']]",[],"[['Model', 'propose', 'novel neural network architecture']]",[],dependency_parsing,4,25
454,model,"STACKPTR is a transition - based architecture , with the corresponding asymptotic efficiency , but still maintains a global view of the sentence that proves essential for achieving competitive accuracy .","[('is', (1, 2)), ('with', (8, 9)), ('maintains', (16, 17)), ('of', (20, 21)), ('proves', (24, 25)), ('for achieving', (26, 28))]","[('STACKPTR', (0, 1)), ('transition - based architecture', (3, 7)), ('corresponding asymptotic efficiency', (10, 13)), ('global view', (18, 20)), ('sentence', (22, 23)), ('essential', (25, 26)), ('competitive accuracy', (28, 30))]","[['STACKPTR', 'is', 'transition - based architecture'], ['transition - based architecture', 'with', 'corresponding asymptotic efficiency'], ['STACKPTR', 'maintains', 'global view'], ['global view', 'of', 'sentence'], ['sentence', 'proves', 'essential'], ['essential', 'for achieving', 'competitive accuracy']]","[['STACKPTR', 'has', 'transition - based architecture'], ['transition - based architecture', 'has', 'corresponding asymptotic efficiency']]",[],"[['Model', 'has', 'STACKPTR']]",dependency_parsing,4,26
455,model,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .","[('as', (7, 8)), ('equipped with', (13, 15)), ('to maintain', (18, 20)), ('of', (22, 23)), ('in', (25, 26))]","[('Our STACKPTR parser', (0, 3)), ('pointer network', (5, 7)), ('backbone', (9, 10)), ('internal stack', (16, 18)), ('order', (21, 22)), ('head words', (23, 25)), ('tree structures', (26, 28))]","[['pointer network', 'as', 'backbone'], ['Our STACKPTR parser', 'equipped with', 'internal stack'], ['internal stack', 'to maintain', 'order'], ['order', 'of', 'head words'], ['head words', 'in', 'tree structures']]","[['Our STACKPTR parser', 'has', 'pointer network'], ['pointer network', 'has', 'backbone']]",[],"[['Model', 'has', 'Our STACKPTR parser']]",dependency_parsing,4,27
456,model,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .","[('performs', (3, 4)), ('in', (5, 6)), ('at', (16, 17)), ('generates', (21, 22)), ('by assigning', (24, 26)), ('for', (28, 29)), ('of', (34, 35))]","[('STACKPTR parser', (1, 3)), ('parsing', (4, 5)), ('incremental , topdown , depth - first fashion', (7, 15)), ('arc', (23, 24)), ('child', (27, 28)), ('headword', (30, 31)), ('top', (33, 34)), ('internal stack', (36, 38))]","[['STACKPTR parser', 'performs', 'parsing'], ['parsing', 'in', 'incremental , topdown , depth - first fashion'], ['headword', 'at', 'top'], ['arc', 'by assigning', 'child'], ['child', 'for', 'headword'], ['top', 'of', 'internal stack']]",[],[],"[['Model', 'has', 'STACKPTR parser']]",dependency_parsing,4,28
457,baselines,"For fair comparison of the parsing performance , we re-implemented the graph - based Deep Biaffine ( BIAF ) parser , which achieved state - of - the - art results on a wide range of languages .","[('re-implemented', (9, 10))]","[('graph - based Deep Biaffine ( BIAF ) parser', (11, 20))]",[],[],"[['Baselines', 're-implemented', 'graph - based Deep Biaffine ( BIAF ) parser']]",[],dependency_parsing,4,158
458,experiments,Main Results,[],"[('Results', (1, 2))]",[],[],[],[],dependency_parsing,4,160
459,results,"On UAS and LAS , the Full variation of STACKPTR with decoding beam size 10 outperforms BIAF on Chinese , and obtains competitive performance on English and German .","[('On', (0, 1)), ('with', (10, 11)), ('outperforms', (15, 16)), ('on', (17, 18)), ('obtains', (21, 22)), ('on', (24, 25))]","[('UAS and LAS', (1, 4)), ('Full variation of STACKPTR', (6, 10)), ('decoding beam size 10', (11, 15)), ('BIAF', (16, 17)), ('Chinese', (18, 19)), ('competitive performance', (22, 24)), ('English', (25, 26)), ('German', (27, 28))]","[['BIAF', 'On', 'Chinese'], ['Full variation of STACKPTR', 'with', 'decoding beam size 10'], ['decoding beam size 10', 'outperforms', 'BIAF'], ['BIAF', 'on', 'Chinese'], ['competitive performance', 'on', 'English'], ['competitive performance', 'on', 'German'], ['Full variation of STACKPTR', 'obtains', 'competitive performance'], ['competitive performance', 'on', 'English'], ['competitive performance', 'on', 'German']]","[['UAS and LAS', 'has', 'Full variation of STACKPTR']]","[['Results', 'On', 'UAS and LAS']]",[],dependency_parsing,4,165
460,results,"An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .","[('achieves', (8, 9)), ('on', (12, 13)), ('performs', (18, 19)), ('than', (21, 22)), ('on', (24, 25))]","[('Full model', (6, 8)), ('best accuracy', (10, 12)), ('English and Chinese', (13, 16)), ('slightly worse', (19, 21)), ('+ sib', (22, 24)), ('German', (25, 26))]","[['Full model', 'achieves', 'best accuracy'], ['best accuracy', 'on', 'English and Chinese'], ['+ sib', 'on', 'German'], ['slightly worse', 'than', '+ sib'], ['+ sib', 'on', 'German']]","[['Full model', 'has', 'best accuracy']]",[],[],dependency_parsing,4,166
461,results,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .","[('significantly outperforms', (6, 8)), ('on', (9, 10))]","[('LCM and UCM', (1, 4)), ('STACKPTR', (5, 6)), ('BIAF', (8, 9)), ('all languages', (10, 12))]","[['STACKPTR', 'significantly outperforms', 'BIAF'], ['BIAF', 'on', 'all languages']]","[['LCM and UCM', 'has', 'STACKPTR']]",[],[],dependency_parsing,4,168
462,results,The results of our parser on RA are slightly worse than BIAF .,"[('on', (5, 6)), ('than', (10, 11))]","[('RA', (6, 7)), ('slightly worse', (8, 10)), ('BIAF', (11, 12))]","[['slightly worse', 'than', 'BIAF']]",[],"[['Results', 'on', 'RA']]",[],dependency_parsing,4,169
463,results,"re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .","[('obtains', (3, 4)), ('than', (6, 7))]","[('re-implementation of BIAF', (0, 3)), ('better performance', (4, 6)), ('original one', (8, 10))]","[['re-implementation of BIAF', 'obtains', 'better performance'], ['better performance', 'than', 'original one']]",[],[],"[['Results', 'has', 're-implementation of BIAF']]",dependency_parsing,4,175
464,results,"Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .","[('achieves', (2, 3)), ('on both', (11, 13)), ('on', (16, 17)), ('on', (22, 23))]","[('Our model', (0, 2)), ('state - of - the - art performance', (3, 11)), ('UAS', (13, 14)), ('LAS', (15, 16)), ('Chinese', (17, 18)), ('best UAS', (20, 22)), ('English', (23, 24))]","[['Our model', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on both', 'UAS'], ['state - of - the - art performance', 'on both', 'LAS'], ['state - of - the - art performance', 'on both', 'best UAS'], ['LAS', 'on', 'Chinese'], ['best UAS', 'on', 'English'], ['best UAS', 'on', 'English']]",[],[],"[['Results', 'has', 'Our model']]",dependency_parsing,4,176
465,results,"On German , the performance is competitive with BIAF , and significantly better than other models .","[('On', (0, 1)), ('with', (7, 8)), ('than', (13, 14))]","[('German', (1, 2)), ('performance', (4, 5)), ('competitive', (6, 7)), ('BIAF', (8, 9)), ('significantly better', (11, 13)), ('other models', (14, 16))]","[['competitive', 'with', 'BIAF'], ['significantly better', 'than', 'other models']]","[['German', 'has', 'performance'], ['performance', 'has', 'competitive']]","[['Results', 'On', 'German']]",[],dependency_parsing,4,177
466,research-problem,Structured Training for Neural Network Transition - Based Parsing,[],"[('Neural Network Transition - Based Parsing', (3, 9))]",[],[],[],[],dependency_parsing,5,2
467,research-problem,We present structured perceptron training for neural network transition - based dependency parsing .,[],"[('neural network transition - based dependency parsing', (6, 13))]",[],[],[],[],dependency_parsing,5,4
468,research-problem,"Lately , dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages and the efficiency of dependency parsers .",[],"[('dependency parsing', (2, 4))]",[],[],[],[],dependency_parsing,5,11
469,model,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .",[],"[('transition - based parsing', (1, 5))]",[],[],[],[],dependency_parsing,5,13
470,model,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .","[('combine', (5, 6)), ('of', (9, 10)), ('with', (12, 13)), ('enabled by', (16, 18))]","[('representational power', (7, 9)), ('neural networks', (10, 12)), ('superior search', (14, 16)), ('structured training and inference', (18, 22))]","[['representational power', 'of', 'neural networks'], ['neural networks', 'with', 'superior search'], ['superior search', 'enabled by', 'structured training and inference']]",[],"[['Model', 'combine', 'representational power']]",[],dependency_parsing,5,24
471,model,"As in prior work , we train the neural network to model the probability of individual parse actions .","[('train', (6, 7)), ('to model', (10, 12)), ('of', (14, 15))]","[('neural network', (8, 10)), ('probability', (13, 14)), ('individual parse actions', (15, 18))]","[['neural network', 'to model', 'probability'], ['probability', 'of', 'individual parse actions']]",[],"[['Model', 'train', 'neural network']]",[],dependency_parsing,5,29
472,model,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .","[('use', (3, 4)), ('from', (6, 7)), ('of', (9, 10)), ('as', (13, 14)), ('in', (16, 17)), ('trained with', (23, 25))]","[('activations', (5, 6)), ('all layers', (7, 9)), ('neural network', (11, 13)), ('representation', (15, 16)), ('structured perceptron model', (18, 21)), ('beam search and early updates', (25, 30))]","[['activations', 'from', 'all layers'], ['all layers', 'of', 'neural network'], ['activations', 'as', 'representation'], ['neural network', 'as', 'representation'], ['representation', 'in', 'structured perceptron model'], ['structured perceptron model', 'trained with', 'beam search and early updates']]",[],"[['Model', 'use', 'activations']]",[],dependency_parsing,5,31
473,model,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .","[('generate', (5, 6)), ('by parsing', (14, 16)), ('with', (18, 19)), ('selecting', (23, 24)), ('produced', (32, 33))]","[('large quantities of high - confidence parse trees', (6, 14)), ('unlabeled data', (16, 18)), ('two different parsers', (19, 22)), ('only the sentences', (24, 27)), ('two parsers', (30, 32)), ('same trees', (34, 36))]","[['large quantities of high - confidence parse trees', 'by parsing', 'unlabeled data'], ['unlabeled data', 'with', 'two different parsers'], ['large quantities of high - confidence parse trees', 'selecting', 'only the sentences'], ['two parsers', 'produced', 'same trees']]",[],"[['Model', 'generate', 'large quantities of high - confidence parse trees']]",[],dependency_parsing,5,35
474,model,"This approach is known as "" tri-training "" and we show that it benefits our neural network parser significantly more than other approaches .","[('known as', (3, 5))]","[('tri-training', (6, 7))]",[],[],"[['Model', 'known as', 'tri-training']]",[],dependency_parsing,5,36
475,hyperparameters,We used the publicly available word2vec 2 tool to learn CBOW embeddings following the sample configuration provided with the tool .,"[('used', (1, 2)), ('to learn', (8, 10))]","[('publicly available word2vec 2 tool', (3, 8)), ('CBOW embeddings', (10, 12))]","[['publicly available word2vec 2 tool', 'to learn', 'CBOW embeddings']]",[],"[['Hyperparameters', 'used', 'publicly available word2vec 2 tool']]",[],dependency_parsing,5,200
476,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],dependency_parsing,5,219
477,results,"The highest of these is , with a reported accuracy of 94.22 % UAS .","[('of', (2, 3))]","[('reported accuracy', (8, 10)), ('94.22 % UAS', (11, 14))]","[['reported accuracy', 'of', '94.22 % UAS']]",[],[],[],dependency_parsing,5,222
478,results,"Even though the UAS is not directly comparable , it is typically similar , and this suggests that our model is competitive with some of the highest reported accuries for dependencies on WSJ .","[('with', (22, 23)), ('for', (29, 30)), ('on', (31, 32))]","[('our model', (18, 20)), ('competitive', (21, 22)), ('some of the highest reported accuries', (23, 29)), ('dependencies', (30, 31)), ('WSJ', (32, 33))]","[['competitive', 'with', 'some of the highest reported accuries'], ['some of the highest reported accuries', 'for', 'dependencies'], ['dependencies', 'on', 'WSJ']]","[['our model', 'has', 'competitive']]",[],[],dependency_parsing,5,223
479,research-problem,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING,[],"[('NEURAL DEPENDENCY PARSING', (4, 7))]",[],[],[],[],dependency_parsing,6,2
480,model,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .","[('modify', (1, 2)), ('build', (19, 20)), ('uses', (26, 27)), ('replace', (31, 32)), ('with', (43, 44))]","[('neural graphbased approach', (3, 6)), ('network', (21, 22)), ('larger', (24, 25)), ('more regularization', (27, 29)), ('traditional MLP - based attention mechanism and affine label classifier', (33, 43)), ('biaffine ones', (44, 46))]","[['network', 'uses', 'more regularization'], ['traditional MLP - based attention mechanism and affine label classifier', 'with', 'biaffine ones']]","[['network', 'has', 'larger']]","[['Model', 'modify', 'neural graphbased approach']]",[],dependency_parsing,6,13
481,hyperparameters,"We choose to optimize with Adam , which ( among other things ) keeps a moving average of the L 2 norm of the gradient for each parameter throughout training and divides the gradient for each parameter by this moving average , ensuring that the magnitude of the gradients will on average be close to one .","[('choose', (1, 2)), ('with', (4, 5))]","[('optimize', (3, 4)), ('Adam', (5, 6))]","[['optimize', 'with', 'Adam']]","[['optimize', 'has', 'Adam']]","[['Hyperparameters', 'choose', 'optimize']]",[],dependency_parsing,6,96
482,results,"Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .","[('gets', (2, 3)), ('on', (8, 9)), ('as', (13, 14)), ('on', (32, 33)), ('on', (41, 42))]","[('Our model', (0, 2)), ('nearly the same UAS performance', (3, 8)), ('PTB - SD 3.3.0', (9, 13)), ('current SOTA model', (15, 18)), ('SOTA UAS performance', (29, 32)), ('CTB 5.1 7', (33, 36)), ('SOTA performance', (39, 41)), ('all CoNLL 09 languages', (42, 46))]","[['Our model', 'gets', 'nearly the same UAS performance'], ['Our model', 'gets', 'SOTA UAS performance'], ['Our model', 'gets', 'SOTA performance'], ['nearly the same UAS performance', 'on', 'PTB - SD 3.3.0'], ['PTB - SD 3.3.0', 'as', 'current SOTA model'], ['SOTA UAS performance', 'on', 'CTB 5.1 7'], ['SOTA performance', 'on', 'all CoNLL 09 languages']]","[['Our model', 'has', 'nearly the same UAS performance']]",[],"[['Results', 'has', 'Our model']]",dependency_parsing,6,102
483,research-problem,"This form of training , which accounts for model predictions at training time , improves parsing accuracies .","[('parsing', (15, 16))]",[],[],[],[],[],dependency_parsing,7,5
484,research-problem,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures ; this formalization is known as transitionbased parsing , and is often coupled with a greedy search procedure .",[],"[('Natural language parsing', (0, 3))]",[],[],[],[],dependency_parsing,7,8
485,research-problem,"The literature on transition - based parsing is vast , but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state .",[],"[('transition - based parsing', (3, 7))]",[],[],[],[],dependency_parsing,7,9
486,model,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .","[('adapt', (5, 6)), ('as', (10, 11)), ('to explore', (11, 13)), ('from', (18, 19))]","[('training criterion', (7, 9)), ('parser states', (13, 15)), ('training data', (20, 22)), ('model', (27, 28)), ('being learned', (31, 33))]","[['training criterion', 'to explore', 'parser states']]",[],"[['Model', 'adapt', 'training criterion']]",[],dependency_parsing,7,17
487,model,"To do so , we use the method of to dynamically chose an optimal ( relative to the final attachment accuracy ) action given an imperfect history .","[('use', (5, 6)), ('to', (9, 10)), ('dynamically chose', (10, 12)), ('given', (23, 24))]","[('method', (7, 8)), ('optimal ( relative to the final attachment accuracy ) action', (13, 23)), ('imperfect history', (25, 27))]","[['method', 'dynamically chose', 'optimal ( relative to the final attachment accuracy ) action'], ['optimal ( relative to the final attachment accuracy ) action', 'given', 'imperfect history']]",[],"[['Model', 'use', 'method']]",[],dependency_parsing,7,18
488,model,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .","[('By', (0, 1)), ('between', (2, 3)), ('sampled from', (5, 7)), ('at', (20, 21))]","[('interpolating', (1, 2)), ('algorithm states', (3, 5)), ('model', (8, 9)), ('training data', (14, 16)), ('more robust predictions', (17, 20)), ('test time', (21, 23))]","[['interpolating', 'between', 'algorithm states'], ['algorithm states', 'sampled from', 'model'], ['more robust predictions', 'at', 'test time']]",[],"[['Model', 'By', 'interpolating']]",[],dependency_parsing,7,19
489,experiments,Experiments,[],"[('Experiments', (0, 1))]",[],[],[],[],dependency_parsing,7,76
490,results,The score achieved by the dynamic oracle for English is 93.56 UAS .,"[('achieved by', (2, 4)), ('for', (7, 8))]","[('dynamic oracle', (5, 7)), ('English', (8, 9)), ('93.56 UAS', (10, 12))]","[['dynamic oracle', 'for', 'English']]",[],[],[],dependency_parsing,7,78
491,results,"Moreover , the Chinese score establishes the state - of - the - art , using the same settings as Chen and Manning ( 2014 ) .","[('establishes', (5, 6))]","[('Chinese score', (3, 5)), ('state - of - the - art', (7, 14))]","[['Chinese score', 'establishes', 'state - of - the - art']]","[['Chinese score', 'has', 'state - of - the - art']]",[],"[['Results', 'has', 'Chinese score']]",dependency_parsing,7,80
492,research-problem,Globally Normalized Transition - Based Neural Networks,[],"[('Globally Normalized Transition - Based Neural Networks', (0, 7))]",[],[],[],[],dependency_parsing,8,2
493,research-problem,"We introduce a globally normalized transition - based neural network model that achieves state - of - the - art part - ofspeech tagging , dependency parsing and sentence compression results .",[],"[('globally normalized transition - based neural network', (3, 10))]",[],[],[],[],dependency_parsing,8,4
494,model,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .","[('demonstrate', (4, 5)), ('without', (11, 12)), ('achieve', (15, 16)), ('than', (20, 21))]","[('simple feed - forward networks', (6, 11)), ('any recurrence', (12, 14)), ('comparable or better accuracies', (16, 20)), ('LSTMs', (21, 22)), ('globally normalized', (28, 30))]","[['simple feed - forward networks', 'without', 'any recurrence'], ['simple feed - forward networks', 'achieve', 'comparable or better accuracies'], ['comparable or better accuracies', 'than', 'LSTMs']]",[],"[['Model', 'demonstrate', 'simple feed - forward networks']]",[],dependency_parsing,8,11
495,model,"Our model , described in detail in Section 2 , uses a transition system and feature embeddings as introduced by * On leave from Columbia University ..","[('uses', (10, 11))]","[('transition system and feature embeddings', (12, 17))]",[],[],"[['Model', 'uses', 'transition system and feature embeddings']]",[],dependency_parsing,8,12
496,model,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .","[('for maintaining', (11, 13)), ('introduce', (16, 17)), ('with', (19, 20)), ('to overcome', (28, 30))]","[('beam search', (9, 11)), ('multiple hypotheses', (13, 15)), ('global normalization', (17, 19)), ('conditional random field ( CRF ) objective', (21, 28)), ('label bias problem', (31, 34))]","[['beam search', 'for maintaining', 'multiple hypotheses'], ['global normalization', 'with', 'conditional random field ( CRF ) objective'], ['conditional random field ( CRF ) objective', 'to overcome', 'label bias problem']]",[],[],[],dependency_parsing,8,13
497,model,"Since we use beam inference , we approximate the partition function by summing over the elements in the beam , and use early updates .","[('use', (2, 3)), ('approximate', (7, 8)), ('by', (11, 12)), ('summing over', (12, 14)), ('in', (16, 17)), ('use', (21, 22))]","[('beam inference', (3, 5)), ('partition function', (9, 11)), ('elements', (15, 16)), ('beam', (18, 19)), ('early updates', (22, 24))]","[['beam inference', 'approximate', 'partition function'], ['partition function', 'summing over', 'elements'], ['elements', 'in', 'beam'], ['beam inference', 'use', 'early updates'], ['partition function', 'use', 'early updates']]",[],"[['Model', 'use', 'beam inference']]",[],dependency_parsing,8,14
498,model,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,"[('compute', (1, 2)), ('based on', (3, 5)), ('perform', (10, 11)), ('of', (14, 15)), ('based on', (19, 21))]","[('gradients', (2, 3)), ('approximate global normalization', (6, 9)), ('full backpropagation training', (11, 14)), ('all neural network parameters', (15, 19)), ('CRF loss', (22, 24))]","[['gradients', 'based on', 'approximate global normalization'], ['gradients', 'perform', 'full backpropagation training'], ['full backpropagation training', 'of', 'all neural network parameters'], ['all neural network parameters', 'based on', 'CRF loss']]","[['gradients', 'has', 'approximate global normalization']]",[],[],dependency_parsing,8,15
499,experiments,Experiments,[],"[('Experiments', (0, 1))]",[],[],[],[],dependency_parsing,8,149
500,experiments,Part of Speech Tagging,[],"[('Part of Speech Tagging', (0, 4))]",[],[],[],[],dependency_parsing,8,158
501,experiments,Results .,[],"[('Results', (0, 1))]",[],[],[],[],dependency_parsing,8,169
502,results,Our globally normalized model again significantly outperforms the local model .,"[('significantly outperforms', (5, 7))]","[('Our globally normalized model', (0, 4)), ('local model', (8, 10))]","[['Our globally normalized model', 'significantly outperforms', 'local model']]",[],[],[],dependency_parsing,8,171
503,results,Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5 .,"[('with', (2, 3)), ('suffers from', (7, 9))]","[('Beam search', (0, 2)), ('locally normalized model', (4, 7)), ('severe label bias issues', (9, 13))]","[['Beam search', 'with', 'locally normalized model'], ['locally normalized model', 'suffers from', 'severe label bias issues']]",[],[],"[['Results', 'has', 'Beam search']]",dependency_parsing,8,172
504,results,"Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .","[('Using', (0, 1)), ('with', (3, 4)), ('with', (13, 14)), ('leads to', (17, 19)), ('in', (23, 24))]","[('beam search', (1, 3)), ('locally normalized model', (5, 8)), ('does not help', (8, 11)), ('global normalization', (14, 16)), ('7 % reduction', (20, 23)), ('relative error', (24, 26))]","[['beam search', 'with', 'locally normalized model'], ['beam search', 'with', 'global normalization'], ['beam search', 'with', 'global normalization'], ['global normalization', 'leads to', '7 % reduction'], ['7 % reduction', 'in', 'relative error']]","[['beam search', 'has', 'locally normalized model'], ['locally normalized model', 'has', 'does not help']]","[['Results', 'Using', 'beam search']]",[],dependency_parsing,8,186
505,results,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .","[('of', (2, 3)), ('is', (6, 7)), ('increasing', (10, 11)), ('on', (13, 14)), ('by', (18, 19))]","[('set', (1, 2)), ('character ngrams feature', (3, 6)), ('very important', (7, 9)), ('average accuracy', (11, 13)), (""CoNLL '09 datasets"", (15, 18)), ('about 0.5 % absolute', (19, 23))]","[['set', 'of', 'character ngrams feature'], ['character ngrams feature', 'is', 'very important'], ['set', 'increasing', 'average accuracy'], ['character ngrams feature', 'increasing', 'average accuracy'], ['average accuracy', 'on', ""CoNLL '09 datasets""], ['average accuracy', 'by', 'about 0.5 % absolute'], [""CoNLL '09 datasets"", 'by', 'about 0.5 % absolute']]","[['set', 'has', 'character ngrams feature'], ['character ngrams feature', 'has', 'very important']]",[],"[['Results', 'has', 'set']]",dependency_parsing,8,187
506,experiments,Dependency Parsing,[],"[('Dependency Parsing', (0, 2))]",[],[],[],[],dependency_parsing,8,189
507,experiments,Results .,[],"[('Results', (0, 1))]",[],[],[],[],dependency_parsing,8,191
508,results,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .","[('compares', (10, 11)), ('to', (12, 13))]","[('our model', (8, 10)), ('favorably', (11, 12)), ('94.26 % LAS', (14, 17)), ('92.41 % UAS', (18, 21))]","[['our model', 'compares', 'favorably'], ['favorably', 'to', '94.26 % LAS'], ['favorably', 'to', '92.41 % UAS']]",[],[],"[['Results', 'has', 'our model']]",dependency_parsing,8,192
509,results,Our results also significantly outperform the LSTM - based approaches of .,"[('significantly outperform', (3, 5))]","[('Our results', (0, 2)), ('LSTM - based approaches', (6, 10))]","[['Our results', 'significantly outperform', 'LSTM - based approaches']]",[],[],[],dependency_parsing,8,194
510,research-problem,Bag of Tricks for Efficient Text Classification,[],"[('Text Classification', (5, 7))]",[],[],[],[],document_classification,0,2
511,model,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .","[('to', (7, 8)), ('scale', (8, 9)), ('with', (15, 16))]","[('baselines', (10, 11)), ('very large corpus', (12, 15)), ('large output space', (17, 20))]","[['baselines', 'to', 'very large corpus'], ['very large corpus', 'with', 'large output space']]",[],"[['Model', 'to', 'baselines']]",[],document_classification,0,14
512,research-problem,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .","[('show that', (12, 14)), ('with', (16, 17)), ('train on', (26, 28)), ('within', (31, 32))]","[('linear models', (14, 16)), ('rank constraint', (18, 20)), ('fast loss approximation', (22, 25)), ('billion words', (29, 31)), ('ten minutes', (32, 34))]","[['linear models', 'with', 'rank constraint'], ['billion words', 'within', 'ten minutes']]",[],"[['Research problem', 'show that', 'linear models']]",[],document_classification,0,15
513,experiments,Sentiment analysis,[],"[('Sentiment analysis', (0, 2))]",[],[],[],[],document_classification,0,53
514,results,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .","[('use', (1, 2)), ('run', (6, 7)), ('for', (8, 9)), ('with', (11, 12)), ('selected on', (15, 17)), ('from', (20, 21))]","[('10 hidden units', (2, 5)), ('fastText', (7, 8)), ('5 epochs', (9, 11)), ('learning rate', (13, 15)), ('validation set', (18, 20)), ('0.05 , 0.1 , 0.25 , 0.5', (22, 29))]","[['fastText', 'for', '5 epochs'], ['5 epochs', 'with', 'learning rate'], ['learning rate', 'selected on', 'validation set'], ['validation set', 'from', '0.05 , 0.1 , 0.25 , 0.5']]",[],"[['Results', 'use', '10 hidden units']]",[],document_classification,0,60
515,results,"On this task , adding bigram information improves the performance by 1 - 4 % .","[('adding', (4, 5)), ('improves', (7, 8)), ('by', (10, 11))]","[('bigram information', (5, 7)), ('performance', (9, 10)), ('1 - 4 %', (11, 15))]","[['bigram information', 'improves', 'performance'], ['performance', 'by', '1 - 4 %']]",[],"[['Results', 'adding', 'bigram information']]",[],document_classification,0,61
516,results,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .","[('than', (6, 7)), ('than', (19, 20))]","[('our accuracy', (1, 3)), ('slightly better', (4, 6)), ('char - CNN', (7, 10)), ('char - CRNN', (11, 14)), ('bit worse', (17, 19)), ('VDCNN', (20, 21))]","[['slightly better', 'than', 'char - CNN'], ['slightly better', 'than', 'char - CRNN'], ['bit worse', 'than', 'VDCNN']]","[['our accuracy', 'has', 'slightly better'], ['bit worse', 'has', 'VDCNN']]",[],"[['Results', 'has', 'our accuracy']]",document_classification,0,62
517,results,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .","[('with', (15, 16)), ('on', (20, 21)), ('goes', (22, 23))]","[('trigrams', (16, 17)), ('performance', (19, 20)), ('Sogou', (21, 22)), ('up to 97.1 %', (23, 27))]","[['performance', 'on', 'Sogou'], ['performance', 'goes', 'up to 97.1 %'], ['Sogou', 'goes', 'up to 97.1 %']]","[['trigrams', 'has', 'performance']]",[],[],document_classification,0,63
518,results,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,"[('using', (11, 12)), ('up to', (13, 15)), ('leads to', (16, 18))]","[('n-grams', (12, 13)), ('5', (15, 16)), ('best performance', (19, 21))]","[['n-grams', 'up to', '5'], ['n-grams', 'leads to', 'best performance'], ['5', 'leads to', 'best performance']]",[],"[['Results', 'using', 'n-grams']]",[],document_classification,0,65
519,experiments,Tag prediction,[],"[('Tag prediction', (0, 2))]",[],[],[],[],document_classification,0,69
520,baselines,We consider a frequency - based baseline which predicts the most frequent tag .,"[('consider', (1, 2)), ('predicts', (8, 9))]","[('frequency - based baseline', (3, 7)), ('most frequent tag', (10, 13))]","[['frequency - based baseline', 'predicts', 'most frequent tag']]",[],[],[],document_classification,0,79
521,baselines,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .","[('compare with', (2, 4))]","[('Tagspace ( Weston et al. , 2014 )', (4, 12)), ('tag prediction model', (16, 19))]",[],"[['Tagspace ( Weston et al. , 2014 )', 'has', 'tag prediction model']]",[],[],document_classification,0,80
522,experiments,Results and training time . and 200 .,[],"[('Results', (0, 1))]",[],[],[],[],document_classification,0,82
523,experiments,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .","[('adding', (13, 14)), ('gives', (15, 16)), ('in', (20, 21))]","[('bigrams', (14, 15)), ('significant boost', (18, 20)), ('accuracy', (21, 22))]","[['bigrams', 'gives', 'significant boost'], ['significant boost', 'in', 'accuracy']]",[],[],[],document_classification,0,83
524,research-problem,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION,[],"[('CROSS - LINGUAL DOCUMENT CLASSIFICATION', (5, 10))]",[],[],[],[],document_classification,1,2
525,research-problem,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",[],"[('cross - lingual understanding ( XLU )', (3, 10))]",[],[],[],[],document_classification,1,5
526,baselines,We combine state - of - the - art cross - lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU .,[],"[('XLU', (41, 42))]",[],[],[],[],document_classification,1,8
527,model,"In particular , we focus on two approaches for domain adaptation .","[('focus on', (4, 6)), ('for', (8, 9))]","[('two approaches', (6, 8)), ('domain adaptation', (9, 11))]","[['two approaches', 'for', 'domain adaptation']]",[],"[['Model', 'focus on', 'two approaches']]",[],document_classification,1,38
528,model,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,"[('based on', (4, 6)), ('using', (17, 18))]","[('first method', (1, 3)), ('masked language model ( MLM ) pre-training', (6, 13)), ('unlabeled target language corpora', (18, 22))]","[['first method', 'based on', 'masked language model ( MLM ) pre-training'], ['masked language model ( MLM ) pre-training', 'using', 'unlabeled target language corpora']]",[],[],"[['Model', 'has', 'first method']]",document_classification,1,39
529,model,"The second method is unsupervised data augmentation ( UDA ) ) , where synthetic paraphrases are generated from the unlabeled corpus , and the model is trained on a label consistency loss .","[('where', (12, 13)), ('generated from', (16, 18)), ('trained on', (26, 28))]","[('second method', (1, 3)), ('unsupervised data augmentation ( UDA )', (4, 10)), ('synthetic paraphrases', (13, 15)), ('unlabeled corpus', (19, 21)), ('model', (24, 25)), ('label consistency loss', (29, 32))]","[['unsupervised data augmentation ( UDA )', 'where', 'synthetic paraphrases'], ['synthetic paraphrases', 'generated from', 'unlabeled corpus'], ['model', 'trained on', 'label consistency loss']]","[['second method', 'name', 'unsupervised data augmentation ( UDA )']]",[],"[['Model', 'has', 'second method']]",document_classification,1,41
530,baselines,Fine-tune ( Ft ) : Fine - tuning the pre-trained model with the source - domain training set .,"[('with', (11, 12))]","[('Fine-tune ( Ft )', (0, 4)), ('Fine - tuning', (5, 8)), ('pre-trained model', (9, 11)), ('source - domain training set', (13, 18))]","[['pre-trained model', 'with', 'source - domain training set']]","[['Fine-tune ( Ft )', 'has', 'Fine - tuning'], ['Fine - tuning', 'has', 'pre-trained model']]",[],"[['Baselines', 'has', 'Fine-tune ( Ft )']]",document_classification,1,152
531,baselines,Fine - tune with UDA ( UDA ) :,[],"[('Fine - tune with UDA ( UDA )', (0, 8))]",[],[],[],"[['Baselines', 'has', 'Fine - tune with UDA ( UDA )']]",document_classification,1,154
532,baselines,This method utilizes the unlabeled data from the target domain by optimizing the UDA loss function ( Eq. ) .,"[('utilizes', (2, 3)), ('from', (6, 7)), ('by optimizing', (10, 12))]","[('unlabeled data', (4, 6)), ('target domain', (8, 10)), ('UDA loss function', (13, 16))]","[['unlabeled data', 'from', 'target domain'], ['unlabeled data', 'by optimizing', 'UDA loss function']]",[],[],[],document_classification,1,155
533,baselines,Self - training based on the UDA model ( UDA + Self ) :,[],"[('Self - training based on the UDA model ( UDA + Self )', (0, 13))]",[],[],[],"[['Baselines', 'has', 'Self - training based on the UDA model ( UDA + Self )']]",document_classification,1,156
534,baselines,"We first train the Ft model and UDA model , and choose the better one as the teacher model .","[('first train', (1, 3)), ('choose', (11, 12))]","[('Ft model and UDA model', (4, 9)), ('teacher model', (17, 19))]",[],[],[],[],document_classification,1,157
535,baselines,"The teacher model is used to train a new XLM student using only unlabeled data U tgt in the target domain , as described above .","[('used to train', (4, 7)), ('using', (11, 12)), ('in', (17, 18))]","[('new XLM student', (8, 11)), ('only unlabeled data U tgt', (12, 17)), ('target domain', (19, 21))]","[['new XLM student', 'using', 'only unlabeled data U tgt'], ['only unlabeled data U tgt', 'in', 'target domain']]",[],[],[],document_classification,1,158
536,results,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .","[('Looking at', (0, 2)), ('of', (15, 16)), ('between', (29, 30))]","[('Ft ( XLM ) results', (2, 7)), ('substantial gap', (27, 29)), ('model performance', (31, 33)), ('the cross -lingual settings and the monolingual baselines', (34, 42))]","[['substantial gap', 'between', 'model performance']]","[['Ft ( XLM ) results', 'has', 'substantial gap']]","[['Results', 'Looking at', 'Ft ( XLM ) results']]",[],document_classification,1,171
537,results,Both the UDA algorithm and MLM pre-training can offer significant improvements by utilizing the unlabeled data .,"[('offer', (8, 9)), ('by utilizing', (11, 13))]","[('UDA algorithm and MLM pre-training', (2, 7)), ('significant improvements', (9, 11)), ('unlabeled data', (14, 16))]","[['UDA algorithm and MLM pre-training', 'offer', 'significant improvements'], ['significant improvements', 'by utilizing', 'unlabeled data']]","[['UDA algorithm and MLM pre-training', 'has', 'significant improvements']]",[],"[['Results', 'has', 'UDA algorithm and MLM pre-training']]",document_classification,1,172
538,results,"In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .","[('In', (0, 1)), ('usnig', (20, 21)), ('provides', (24, 25)), ('compared with', (27, 29))]","[('sentiment classification task', (2, 5)), ('Ft ( XLM ft ) model', (14, 20)), ('MLM pre-training', (21, 23)), ('larger improvements', (25, 27)), ('UDA method', (30, 32))]","[['Ft ( XLM ft ) model', 'usnig', 'MLM pre-training'], ['Ft ( XLM ft ) model', 'provides', 'larger improvements'], ['MLM pre-training', 'provides', 'larger improvements'], ['larger improvements', 'compared with', 'UDA method']]","[['sentiment classification task', 'has', 'Ft ( XLM ft ) model'], ['Ft ( XLM ft ) model', 'has', 'MLM pre-training']]","[['Results', 'In', 'sentiment classification task']]",[],document_classification,1,173
539,results,"On the other hand , the MLM method is relatively more resource intensive and takes longer to converge ( see Appendix A.5 ) .","[('is', (8, 9)), ('takes', (14, 15)), ('to', (16, 17))]","[('MLM method', (6, 8)), ('relatively more resource intensive', (9, 13)), ('longer', (15, 16)), ('converge', (17, 18))]","[['MLM method', 'is', 'relatively more resource intensive'], ['MLM method', 'takes', 'longer'], ['longer', 'to', 'converge']]","[['MLM method', 'has', 'relatively more resource intensive']]",[],"[['Results', 'has', 'MLM method']]",document_classification,1,174
540,results,"In contrast , in the MLdoc dataset , when the size of the unlabeled samples is limited , the UDA method is more helpful .","[('in', (3, 4)), ('of', (11, 12)), ('is', (15, 16))]","[('MLdoc dataset', (5, 7)), ('size', (10, 11)), ('unlabeled samples', (13, 15)), ('limited', (16, 17)), ('UDA method', (19, 21)), ('more helpful', (22, 24))]","[['size', 'of', 'unlabeled samples'], ['size', 'is', 'limited'], ['unlabeled samples', 'is', 'limited']]","[['MLdoc dataset', 'has', 'size'], ['UDA method', 'has', 'more helpful']]","[['Results', 'in', 'MLdoc dataset']]",[],document_classification,1,175
541,results,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .","[('observe', (7, 8)), ('over', (15, 16))]","[('self - training technique', (9, 13)), ('consistently improves', (13, 15)), ('teacher model', (17, 19))]","[['consistently improves', 'over', 'teacher model']]","[['self - training technique', 'has', 'consistently improves']]","[['Results', 'observe', 'self - training technique']]",[],document_classification,1,178
542,results,It offers best results in both XLM and XLM ft based classifiers .,"[('offers', (1, 2)), ('in', (4, 5))]","[('best results', (2, 4)), ('both XLM and XLM ft based classifiers', (5, 12))]","[['best results', 'in', 'both XLM and XLM ft based classifiers']]",[],[],[],document_classification,1,179
543,results,"In the MLdoc dataset , self - training also achieves the best results over all , however the gains are less clear .","[('achieves', (9, 10))]","[('MLdoc dataset', (2, 4)), ('self - training', (5, 8)), ('best results over all', (11, 15))]","[['self - training', 'achieves', 'best results over all']]","[['MLdoc dataset', 'has', 'self - training']]",[],[],document_classification,1,181
544,results,"Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .","[('comparing with', (2, 4)), ('completely close', (21, 23)), ('by utilizing', (26, 28)), ('in', (30, 31))]","[('best cross - lingual results and monolingual fine - tune baseline', (5, 16)), ('performance gap', (24, 26)), ('unlabeled data', (28, 30)), ('target language', (32, 34))]","[['best cross - lingual results and monolingual fine - tune baseline', 'completely close', 'performance gap'], ['performance gap', 'by utilizing', 'unlabeled data'], ['unlabeled data', 'in', 'target language']]","[['best cross - lingual results and monolingual fine - tune baseline', 'has', 'performance gap']]","[['Results', 'comparing with', 'best cross - lingual results and monolingual fine - tune baseline']]",[],document_classification,1,183
545,results,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .","[('reaches', (4, 5)), ('improving over', (15, 17)), ('by', (20, 21))]","[('our framework', (2, 4)), ('new state - of - the - art results', (5, 14)), ('vanilla XLM baselines', (17, 20)), ('44 % on average', (21, 25))]","[['our framework', 'reaches', 'new state - of - the - art results'], ['our framework', 'improving over', 'vanilla XLM baselines'], ['new state - of - the - art results', 'improving over', 'vanilla XLM baselines'], ['vanilla XLM baselines', 'by', '44 % on average']]",[],[],"[['Results', 'has', 'our framework']]",document_classification,1,184
546,ablation-analysis,"Leveraging the unlabeled data from other domains does not offer consistent improvement , however can provide additional value in isolated cases .","[('Leveraging', (0, 1)), ('from', (4, 5))]","[('unlabeled data', (2, 4)), ('other domains', (5, 7)), ('not offer consistent improvement', (8, 12))]","[['unlabeled data', 'from', 'other domains']]",[],"[['Ablation analysis', 'Leveraging', 'unlabeled data']]",[],document_classification,1,214
547,research-problem,Neural Attentive Bag - of - Entities Model for Text Classification,[],"[('Text Classification', (9, 11))]",[],[],[],[],document_classification,10,2
548,code,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec .,[],"[('https://github.com/wikipedia2vec/wikipedia2vec', (11, 12))]",[],[],[],[],document_classification,10,9
549,model,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .","[('proposes', (2, 3)), ('addresses', (21, 22)), ('by modeling', (26, 28)), ('in', (30, 31)), ('using', (34, 35)), ('in', (36, 37))]","[('Neural Attentive Bagof - Entities ( NABoE ) model', (4, 13)), ('neural network model', (17, 20)), ('text classification problem', (23, 26)), ('semantics', (29, 30)), ('target documents', (32, 34)), ('entities', (35, 36)), ('KB', (38, 39))]","[['neural network model', 'addresses', 'text classification problem'], ['text classification problem', 'by modeling', 'semantics'], ['semantics', 'in', 'target documents'], ['semantics', 'using', 'entities'], ['entities', 'in', 'KB']]","[['Neural Attentive Bagof - Entities ( NABoE ) model', 'has', 'neural network model']]","[['Model', 'proposes', 'Neural Attentive Bagof - Entities ( NABoE ) model']]",[],document_classification,10,21
550,model,"For each entity name in a document ( e.g. , "" Apple "" ) , our model first detects entities that maybe referred to by this name ( e.g. , Apple Inc. , Apple ( food ) ) , and then represents the document using the weighted average of the embeddings of these entities .","[('For each', (0, 2)), ('in', (4, 5)), ('detects', (18, 19)), ('referred to by', (22, 25)), ('represents', (41, 42)), ('using', (44, 45)), ('of', (48, 49)), ('of', (51, 52))]","[('entity name', (2, 4)), ('document', (6, 7)), ('our model', (15, 17)), ('entities', (19, 20)), ('document', (43, 44)), ('weighted average', (46, 48)), ('embeddings', (50, 51)), ('entities', (53, 54))]","[['entity name', 'in', 'document'], ['our model', 'detects', 'entities'], ['entities', 'represents', 'document'], ['document', 'using', 'weighted average'], ['weighted average', 'of', 'embeddings'], ['embeddings', 'of', 'entities']]","[['entity name', 'has', 'document']]","[['Model', 'For each', 'entity name']]",[],document_classification,10,22
551,model,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,"[('computed using', (3, 5)), ('to', (14, 15)), ('that are', (23, 25)), ('in', (27, 28))]","[('weights', (1, 2)), ('novel neural attention mechanism', (6, 10)), ('less ambiguous', (25, 27)), ('meaning', (28, 29)), ('more relevant', (30, 32)), ('document', (34, 35))]","[['weights', 'computed using', 'novel neural attention mechanism'], ['more relevant', 'to', 'document'], ['less ambiguous', 'in', 'meaning']]",[],[],"[['Model', 'has', 'weights']]",document_classification,10,23
552,model,"In other words , the attention mechanism is designed to compute weights by jointly addressing entity linking and entity salience detection tasks .","[('to compute', (9, 11)), ('jointly addressing', (13, 15))]","[('attention mechanism', (5, 7)), ('weights', (11, 12)), ('entity linking and entity salience detection tasks', (15, 22))]","[['attention mechanism', 'to compute', 'weights'], ['weights', 'jointly addressing', 'entity linking and entity salience detection tasks']]",[],[],"[['Model', 'has', 'attention mechanism']]",document_classification,10,24
553,hyperparameters,The model was trained using mini-batch SGD with its learning rate controlled by Adam and its mini-batch size set to 32 .,"[('trained using', (3, 5)), ('with', (7, 8)), ('controlled by', (11, 13)), ('set to', (18, 20))]","[('model', (1, 2)), ('mini-batch SGD', (5, 7)), ('learning rate', (9, 11)), ('Adam', (13, 14)), ('mini-batch size', (16, 18)), ('32', (20, 21))]","[['model', 'trained using', 'mini-batch SGD'], ['mini-batch SGD', 'with', 'learning rate'], ['learning rate', 'controlled by', 'Adam'], ['mini-batch size', 'set to', '32']]","[['mini-batch size', 'has', '32']]",[],"[['Hyperparameters', 'has', 'model']]",document_classification,10,81
554,hyperparameters,The size of the embeddings of words and entities was set to d = 300 .,"[('of', (2, 3)), ('of', (5, 6)), ('set to', (10, 12))]","[('size', (1, 2)), ('embeddings', (4, 5)), ('words and entities', (6, 9)), ('d = 300', (12, 15))]","[['size', 'of', 'embeddings'], ['size', 'of', 'embeddings'], ['embeddings', 'of', 'words and entities'], ['size', 'set to', 'd = 300'], ['words and entities', 'set to', 'd = 300']]",[],[],"[['Hyperparameters', 'has', 'size']]",document_classification,10,83
555,experiments,Baselines,[],"[('Baselines', (0, 1))]",[],[],[],[],document_classification,10,86
556,baselines,BoW,[],"[('BoW', (0, 1))]",[],[],[],"[['Baselines', 'has', 'BoW']]",document_classification,10,88
557,baselines,This model is based on a logistic regression classifier with conventional binary BoW features .,"[('based on', (3, 5)), ('with', (9, 10))]","[('logistic regression classifier', (6, 9)), ('conventional binary BoW features', (10, 14))]","[['logistic regression classifier', 'with', 'conventional binary BoW features']]",[],[],[],document_classification,10,89
558,baselines,FTS- BRNN,[],"[('FTS- BRNN', (0, 2))]",[],[],[],"[['Baselines', 'has', 'FTS- BRNN']]",document_classification,10,90
559,baselines,This model is based on a bidirectional RNN with gated recurrent units ( GRU ) .,"[('based on', (3, 5)), ('with', (8, 9))]","[('bidirectional RNN', (6, 8)), ('gated recurrent units ( GRU )', (9, 15))]","[['bidirectional RNN', 'with', 'gated recurrent units ( GRU )']]",[],[],[],document_classification,10,91
560,baselines,NTEE This model is a state - of - the - art model that uses a multi - layer perceptron classifier with the features computed using the embeddings of words and entities trained on Wikipedia using the neural network model proposed in their paper .,"[('is', (3, 4)), ('uses', (14, 15)), ('with', (21, 22)), ('computed using', (24, 26)), ('of', (28, 29)), ('trained on', (32, 34))]","[('NTEE', (0, 1)), ('state - of - the - art model', (5, 13)), ('multi - layer perceptron classifier', (16, 21)), ('features', (23, 24)), ('embeddings', (27, 28)), ('words and entities', (29, 32)), ('Wikipedia', (34, 35))]","[['NTEE', 'is', 'state - of - the - art model'], ['state - of - the - art model', 'uses', 'multi - layer perceptron classifier'], ['multi - layer perceptron classifier', 'with', 'features'], ['features', 'computed using', 'embeddings'], ['embeddings', 'of', 'words and entities'], ['multi - layer perceptron classifier', 'trained on', 'Wikipedia'], ['words and entities', 'trained on', 'Wikipedia']]","[['NTEE', 'has', 'state - of - the - art model']]",[],"[['Baselines', 'has', 'NTEE']]",document_classification,10,93
561,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],document_classification,10,100
562,results,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .","[('Relative to', (0, 2)), ('yielded', (7, 8)), ('on', (12, 13))]","[('baselines', (3, 4)), ('our models', (5, 7)), ('enhanced over all performance', (8, 12)), ('both datasets', (13, 15))]","[['our models', 'yielded', 'enhanced over all performance'], ['enhanced over all performance', 'on', 'both datasets']]","[['baselines', 'has', 'our models']]","[['Results', 'Relative to', 'baselines']]",[],document_classification,10,101
563,results,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,"[('outperformed', (5, 6))]","[('NABoE - full model', (1, 5)), ('all baseline models', (6, 9))]","[['NABoE - full model', 'outperformed', 'all baseline models']]",[],[],"[['Results', 'has', 'NABoE - full model']]",document_classification,10,102
564,results,"Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .","[('outperformed', (5, 6)), ('in terms of', (10, 13)), ('on', (15, 16)), ('on', (25, 26))]","[('NABoE-entity model', (3, 5)), ('all the baseline models', (6, 10)), ('both measures', (13, 15)), ('20NG dataset', (17, 19)), ('F 1 score', (22, 25)), ('R8 dataset', (27, 29))]","[['NABoE-entity model', 'outperformed', 'all the baseline models'], ['all the baseline models', 'in terms of', 'both measures'], ['both measures', 'on', '20NG dataset'], ['F 1 score', 'on', 'R8 dataset']]",[],[],[],document_classification,10,103
565,research-problem,Task - oriented Word Embedding for Text Classification,[],"[('Text Classification', (6, 8))]",[],[],[],[],document_classification,11,2
566,model,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .","[('propose', (5, 6)), ('denoted as', (14, 16))]","[('task - oriented word embedding method', (7, 13)), ('ToWE', (16, 17))]","[['task - oriented word embedding method', 'denoted as', 'ToWE']]","[['task - oriented word embedding method', 'name', 'ToWE']]","[['Model', 'propose', 'task - oriented word embedding method']]",[],document_classification,11,30
567,model,"In our method , the words ' contextual information and task information are inherently jointed to construct the word embeddings .","[('are', (12, 13))]","[(""words ' contextual information and task information"", (5, 12)), ('inherently jointed', (13, 15))]","[[""words ' contextual information and task information"", 'are', 'inherently jointed']]","[[""words ' contextual information and task information"", 'has', 'inherently jointed']]",[],"[['Model', 'has', ""words ' contextual information and task information""]]",document_classification,11,38
568,model,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .","[('To model', (0, 2)), ('regularize', (7, 8)), ('of', (10, 11)), ('adjust', (23, 24)), ('of', (26, 27)), ('in', (30, 31))]","[('task information', (3, 5)), ('distribution', (9, 10)), ('salient words', (12, 14)), ('distribution', (25, 26)), ('other words', (28, 30)), ('embedding space', (32, 34))]","[['task information', 'regularize', 'distribution'], ['distribution', 'of', 'salient words'], ['task information', 'adjust', 'distribution'], ['distribution', 'of', 'other words'], ['other words', 'in', 'embedding space']]",[],"[['Model', 'To model', 'task information']]",[],document_classification,11,40
569,baselines,"To evaluate our method , we consider the following baselines : ( 1 ) the BOW method is employed as a basic baseline .","[('as', (19, 20))]","[('BOW method', (15, 17))]",[],[],[],"[['Baselines', 'has', 'BOW method']]",document_classification,11,146
570,baselines,It represents each document as a bag of words and the weighting scheme is TFIDF .,"[('represents', (1, 2))]","[('each document', (2, 4)), ('bag of words', (6, 9)), ('weighting scheme', (11, 13)), ('TFIDF', (14, 15))]",[],"[['weighting scheme', 'has', 'TFIDF']]",[],[],document_classification,11,147
571,baselines,( 2 ) the Word2 Vec method is a neural network language method which learns word embeddings by maximizing the conditional probability leveraging contextual information .,"[('learns', (14, 15)), ('by maximizing', (17, 19)), ('leveraging', (22, 23))]","[('Word2 Vec method', (4, 7)), ('neural network language method', (9, 13)), ('word embeddings', (15, 17)), ('conditional probability', (20, 22)), ('contextual information', (23, 25))]","[['neural network language method', 'learns', 'word embeddings'], ['word embeddings', 'by maximizing', 'conditional probability'], ['conditional probability', 'leveraging', 'contextual information']]","[['Word2 Vec method', 'has', 'neural network language method']]",[],"[['Baselines', 'has', 'Word2 Vec method']]",document_classification,11,149
572,results,"( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .","[('performs', (5, 6)), ('than', (7, 8))]","[('Our method', (3, 5)), ('better', (6, 7)), ('other methods', (9, 11))]","[['Our method', 'performs', 'better'], ['better', 'than', 'other methods']]",[],[],"[['Results', 'has', 'Our method']]",document_classification,11,183
573,results,"In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .","[('significantly outperforms', (8, 10)), ('on', (13, 14))]","[('ToWE - SG method', (4, 8)), ('other baselines', (11, 13)), ('20 New s Group', (15, 19)), ('5 Abstract s Group', (20, 24)), ('MR', (26, 27))]","[['ToWE - SG method', 'significantly outperforms', 'other baselines'], ['other baselines', 'on', '20 New s Group'], ['other baselines', 'on', '5 Abstract s Group'], ['other baselines', 'on', 'MR']]",[],[],[],document_classification,11,184
574,results,"( 2 ) The word embedding methods outperform the basic bag - of - words methods in most cases , indicating the superiority of distributed word representation over the one - hot representation .","[('outperform', (7, 8))]","[('word embedding methods', (4, 7)), ('basic bag - of - words methods', (9, 16))]","[['word embedding methods', 'outperform', 'basic bag - of - words methods']]",[],[],"[['Results', 'has', 'word embedding methods']]",document_classification,11,186
575,results,"Our method achieves better performance over Retrofit method , indicating that the task - specific features could be more effective compared with general semantic relations constructed by humans in the knowledge bases .","[('achieves', (2, 3)), ('over', (5, 6))]","[('better performance', (3, 5)), ('Retrofit method', (6, 8))]","[['better performance', 'over', 'Retrofit method']]",[],"[['Results', 'achieves', 'better performance']]",[],document_classification,11,190
576,results,"Our method outperforms the TWE method on both the document - level and sentence - level tasks , which shows the stability and reliability of modeling taskspecific features in real - world applications .","[('outperforms', (2, 3)), ('on', (6, 7))]","[('TWE method', (4, 6)), ('document - level and sentence - level tasks', (9, 17))]","[['TWE method', 'on', 'document - level and sentence - level tasks']]",[],"[['Results', 'outperforms', 'TWE method']]",[],document_classification,11,193
577,research-problem,Graph Convolutional Networks for Text Classification,[],"[('Text Classification', (4, 6))]",[],[],[],[],document_classification,12,2
578,model,"In this work , we propose a new graph neural networkbased method for text classification .","[('propose', (5, 6)), ('for', (12, 13))]","[('new graph neural networkbased method', (7, 12)), ('text classification', (13, 15))]","[['new graph neural networkbased method', 'for', 'text classification']]",[],"[['Model', 'propose', 'new graph neural networkbased method']]",[],document_classification,12,22
579,model,"We construct a single large graph from an entire corpus , which contains words and documents as nodes .","[('construct', (1, 2)), ('from', (6, 7)), ('contains', (12, 13)), ('as', (16, 17))]","[('single large graph', (3, 6)), ('entire corpus', (8, 10)), ('words and documents', (13, 16)), ('nodes', (17, 18))]","[['single large graph', 'from', 'entire corpus'], ['entire corpus', 'contains', 'words and documents'], ['words and documents', 'as', 'nodes']]",[],"[['Model', 'construct', 'single large graph']]",[],document_classification,12,23
580,model,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .","[('model', (1, 2)), ('with', (4, 5)), ('captures', (21, 22))]","[('graph', (3, 4)), ('Graph Convolutional Network ( GCN )', (6, 12)), ('simple and effective graph neural network', (14, 20)), ('high order neighborhoods information', (22, 26))]","[['graph', 'with', 'Graph Convolutional Network ( GCN )'], ['graph', 'with', 'simple and effective graph neural network'], ['simple and effective graph neural network', 'captures', 'high order neighborhoods information']]","[['Graph Convolutional Network ( GCN )', 'has', 'simple and effective graph neural network']]","[['Model', 'model', 'graph']]",[],document_classification,12,24
581,model,The edge between two word nodes is built byword co-occurrence information and the edge between a word node and document node is built using word frequency and word 's document frequency .,"[('between', (2, 3)), ('built', (7, 8)), ('between', (14, 15)), ('built using', (22, 24))]","[('edge', (1, 2)), ('two word nodes', (3, 6)), ('byword co-occurrence information', (8, 11)), ('word node and document node', (16, 21)), (""word frequency and word 's document frequency"", (24, 31))]","[['edge', 'between', 'two word nodes'], ['two word nodes', 'built', 'byword co-occurrence information'], ['word node and document node', 'built using', ""word frequency and word 's document frequency""]]",[],[],"[['Model', 'has', 'edge']]",document_classification,12,25
582,model,We then turn text classification problem into anode classification problem .,"[('turn', (2, 3)), ('into', (6, 7))]","[('text classification problem', (3, 6)), ('anode classification problem', (7, 10))]","[['text classification problem', 'into', 'anode classification problem']]",[],"[['Model', 'turn', 'text classification problem']]",[],document_classification,12,26
583,code,Our source code is available at https://github. com/yao8839836/text_gcn .,[],"[('https://github. com/yao8839836/text_gcn', (6, 8))]",[],[],[],[],document_classification,12,28
584,experiments,Baselines .,[],"[('Baselines', (0, 1))]",[],[],[],[],document_classification,12,114
585,baselines,We compare our Text GCN with multiple stateof - the - art text classification and embedding methods as follows :,"[('compare', (1, 2)), ('with', (5, 6))]","[('our Text GCN', (2, 5)), ('multiple stateof - the - art text classification and embedding methods', (6, 17))]","[['our Text GCN', 'with', 'multiple stateof - the - art text classification and embedding methods']]",[],"[['Baselines', 'compare', 'our Text GCN']]",[],document_classification,12,115
586,baselines,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,"[('with', (12, 13))]","[('TF - IDF + LR', (0, 5)), ('bag - of - words model', (6, 12)), ('term frequencyinverse document frequency weighting', (13, 18))]","[['bag - of - words model', 'with', 'term frequencyinverse document frequency weighting']]","[['TF - IDF + LR', 'has', 'bag - of - words model']]",[],"[['Baselines', 'has', 'TF - IDF + LR']]",document_classification,12,116
587,baselines,Logistic Regression is used as the classifier .,"[('used as', (3, 5))]","[('Logistic Regression', (0, 2)), ('classifier', (6, 7))]","[['Logistic Regression', 'used as', 'classifier']]",[],[],"[['Baselines', 'has', 'Logistic Regression']]",document_classification,12,117
588,baselines,CNN : Convolutional Neural Network ( Kim 2014 ) .,[],"[('CNN : Convolutional Neural Network', (0, 5))]",[],[],[],"[['Baselines', 'has', 'CNN : Convolutional Neural Network']]",document_classification,12,118
589,baselines,We explored CNN -rand which uses randomly initialized word embeddings and CNN - non- static which uses pre-trained word embeddings .,"[('explored', (1, 2)), ('uses', (5, 6)), ('uses', (16, 17))]","[('CNN -rand', (2, 4)), ('randomly initialized word embeddings', (6, 10)), ('CNN - non- static', (11, 15)), ('pre-trained word embeddings', (17, 20))]","[['CNN -rand', 'uses', 'randomly initialized word embeddings'], ['CNN -rand', 'uses', 'CNN - non- static'], ['CNN - non- static', 'uses', 'pre-trained word embeddings'], ['CNN - non- static', 'uses', 'pre-trained word embeddings']]",[],"[['Baselines', 'explored', 'CNN -rand']]",[],document_classification,12,119
590,baselines,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,"[('uses', (8, 9)), ('as', (13, 14)), ('of', (16, 17))]","[('LSTM', (0, 1)), ('last hidden state', (10, 13)), ('representation', (15, 16)), ('whole text', (18, 20))]","[['last hidden state', 'as', 'representation'], ['representation', 'of', 'whole text']]",[],[],"[['Baselines', 'has', 'LSTM']]",document_classification,12,120
591,baselines,"Bi- LSTM : a bi-directional LSTM , commonly used in text classification .","[('used in', (8, 10))]","[('Bi- LSTM', (0, 2)), ('bi-directional LSTM', (4, 6)), ('text classification', (10, 12))]","[['bi-directional LSTM', 'used in', 'text classification']]","[['Bi- LSTM', 'has', 'bi-directional LSTM']]",[],"[['Baselines', 'has', 'Bi- LSTM']]",document_classification,12,122
592,baselines,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .","[('of', (13, 14)), ('in', (15, 16)), ('are', (17, 18))]","[('PV - DBOW', (0, 3)), ('paragraph vector model', (5, 8)), ('orders', (12, 13)), ('words', (14, 15)), ('text', (16, 17)), ('ignored', (18, 19))]","[['orders', 'of', 'words'], ['words', 'in', 'text'], ['words', 'are', 'ignored']]","[['PV - DBOW', 'has', 'paragraph vector model'], ['paragraph vector model', 'has', 'orders']]",[],"[['Baselines', 'has', 'PV - DBOW']]",document_classification,12,124
593,baselines,We used Logistic Regression as the classifier .,"[('used', (1, 2)), ('as', (4, 5))]","[('Logistic Regression', (2, 4)), ('classifier', (6, 7))]","[['Logistic Regression', 'as', 'classifier']]",[],"[['Baselines', 'used', 'Logistic Regression']]",[],document_classification,12,125
594,baselines,"PV - DM : a paragraph vector model proposed by , which considers the word order .","[('considers', (12, 13))]","[('PV - DM', (0, 3)), ('paragraph vector model', (5, 8)), ('word order', (14, 16))]","[['paragraph vector model', 'considers', 'word order']]","[['PV - DM', 'has', 'paragraph vector model']]",[],"[['Baselines', 'has', 'PV - DM']]",document_classification,12,126
595,baselines,We used Logistic Regression as the classifier .,"[('used', (1, 2)), ('as', (4, 5))]","[('Logistic Regression', (2, 4)), ('classifier', (6, 7))]","[['Logistic Regression', 'as', 'classifier']]",[],"[['Baselines', 'used', 'Logistic Regression']]",[],document_classification,12,127
596,baselines,"PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .","[('firstly learns', (7, 9)), ('based on', (11, 13)), ('containing', (16, 17)), ('as', (22, 23)), ('averages', (26, 27)), ('as', (29, 30)), ('for', (32, 33))]","[('PTE', (0, 1)), ('predictive text embedding', (2, 5)), ('word embedding', (9, 11)), ('heterogeneous text network', (13, 16)), ('words , documents and labels', (17, 22)), ('nodes', (23, 24)), ('word embeddings', (27, 29)), ('document embeddings', (30, 32)), ('text classification', (33, 35))]","[['predictive text embedding', 'firstly learns', 'word embedding'], ['word embedding', 'based on', 'heterogeneous text network'], ['heterogeneous text network', 'containing', 'words , documents and labels'], ['words , documents and labels', 'as', 'nodes'], ['predictive text embedding', 'averages', 'word embeddings'], ['word embeddings', 'as', 'document embeddings'], ['document embeddings', 'for', 'text classification']]","[['PTE', 'has', 'predictive text embedding']]",[],"[['Baselines', 'has', 'PTE']]",document_classification,12,128
597,baselines,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .","[('treats', (12, 13)), ('as', (21, 22)), ('feeds', (26, 27)), ('into', (29, 30))]","[('fast Text', (0, 2)), ('average of word / n- grams embeddings', (14, 21)), ('document embeddings', (22, 24)), ('document embeddings', (27, 29)), ('linear classifier', (31, 33))]","[['average of word / n- grams embeddings', 'as', 'document embeddings'], ['document embeddings', 'into', 'linear classifier']]",[],[],"[['Baselines', 'has', 'fast Text']]",document_classification,12,129
598,baselines,"SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .","[('employs', (8, 9)), ('operated over', (12, 14))]","[('SWEM', (0, 1)), ('simple word embedding models', (2, 6)), ('simple pooling strategies', (9, 12)), ('word embeddings', (14, 16))]","[['SWEM', 'employs', 'simple pooling strategies'], ['simple word embedding models', 'employs', 'simple pooling strategies'], ['simple pooling strategies', 'operated over', 'word embeddings']]","[['SWEM', 'has', 'simple word embedding models']]",[],"[['Baselines', 'has', 'SWEM']]",document_classification,12,131
599,baselines,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .","[('embeds', (9, 10)), ('in', (14, 15)), ('for', (19, 20))]","[('LEAM', (0, 1)), ('label - embedding attentive models', (2, 7)), ('words and labels', (11, 14)), ('same joint space', (16, 19)), ('text classification', (20, 22))]","[['label - embedding attentive models', 'embeds', 'words and labels'], ['label - embedding attentive models', 'in', 'same joint space'], ['words and labels', 'in', 'same joint space'], ['same joint space', 'for', 'text classification']]","[['LEAM', 'has', 'label - embedding attentive models']]",[],"[['Baselines', 'has', 'LEAM']]",document_classification,12,132
600,baselines,It utilizes label descriptions .,"[('utilizes', (1, 2))]","[('label descriptions', (2, 4))]",[],[],[],[],document_classification,12,133
601,baselines,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .","[('operates', (11, 12)), ('over', (13, 14)), ('used', (33, 34))]","[('Graph - CNN - C', (0, 5)), ('graph CNN model', (7, 10)), ('convolutions', (12, 13)), ('word embedding similarity graphs', (14, 18)), ('Chebyshev filter', (30, 32))]","[['graph CNN model', 'operates', 'convolutions'], ['convolutions', 'over', 'word embedding similarity graphs']]","[['Graph - CNN - C', 'has', 'graph CNN model']]",[],"[['Baselines', 'has', 'Graph - CNN - C']]",document_classification,12,134
602,baselines,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) . ,"[('same as', (7, 9)), ('using', (15, 16))]","[('Graph - CNN - S', (0, 5)), ('Graph - CNN - C', (9, 14)), ('Spline filter', (16, 18))]","[['Graph - CNN - S', 'same as', 'Graph - CNN - C'], ['Graph - CNN - S', 'using', 'Spline filter']]","[['Graph - CNN - S', 'has', 'Graph - CNN - C']]",[],"[['Baselines', 'has', 'Graph - CNN - S']]",document_classification,12,135
603,baselines,Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,"[('same as', (7, 9)), ('using', (15, 16))]","[('Graph - CNN - F', (0, 5)), ('Graph - CNN - C', (9, 14)), ('Fourier filter', (16, 18))]","[['Graph - CNN - F', 'same as', 'Graph - CNN - C'], ['Graph - CNN - F', 'using', 'Fourier filter']]","[['Graph - CNN - F', 'has', 'Graph - CNN - C']]",[],"[['Baselines', 'has', 'Graph - CNN - F']]",document_classification,12,136
604,hyperparameters,"For Text GCN , we set the embedding size of the first convolution layer as 200 and set the window size as 20 .","[('For', (0, 1)), ('set', (5, 6)), ('of', (9, 10)), ('as', (14, 15)), ('as', (21, 22))]","[('Text GCN', (1, 3)), ('embedding size', (7, 9)), ('first convolution layer', (11, 14)), ('200', (15, 16)), ('window size', (19, 21)), ('20', (22, 23))]","[['Text GCN', 'set', 'embedding size'], ['embedding size', 'of', 'first convolution layer'], ['first convolution layer', 'as', '200'], ['window size', 'as', '20']]","[['window size', 'has', '20']]","[['Hyperparameters', 'For', 'Text GCN']]",[],document_classification,12,155
605,hyperparameters,"We tuned other parameters and set the learning rate as 0.02 , dropout For baseline models , we used default parameter settings as in their original papers or implementations .","[('set', (5, 6)), ('as', (9, 10))]","[('learning rate', (7, 9)), ('0.02', (10, 11))]","[['learning rate', 'as', '0.02']]","[['learning rate', 'has', '0.02']]","[['Hyperparameters', 'set', 'learning rate']]",[],document_classification,12,157
606,hyperparameters,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )","[('using', (3, 4)), ('used', (9, 10))]","[('baseline models', (1, 3)), ('pre-trained word embeddings', (4, 7)), ('300 dimensional Glo Ve word embeddings', (10, 16))]","[['baseline models', 'using', 'pre-trained word embeddings'], ['pre-trained word embeddings', 'used', '300 dimensional Glo Ve word embeddings']]",[],[],[],document_classification,12,158
607,results,"Text GCN performs the best and significantly outperforms all baseline models ( p < 0.05 based on student t- test ) on four datasets , which showcases the effectiveness of the proposed method on long text datasets .","[('performs', (2, 3)), ('based on', (15, 17)), ('on', (21, 22))]","[('Text GCN', (0, 2)), ('best', (4, 5)), ('significantly outperforms', (6, 8)), ('all baseline models', (8, 11)), ('p < 0.05', (12, 15)), ('student t- test', (17, 20)), ('four datasets', (22, 24))]","[['Text GCN', 'performs', 'best'], ['p < 0.05', 'based on', 'student t- test']]","[['significantly outperforms', 'has', 'all baseline models'], ['all baseline models', 'has', 'p < 0.05']]",[],"[['Results', 'has', 'Text GCN']]",document_classification,12,161
608,results,"When pre-trained Glo Ve word embeddings are provided , CNN performs much better , especially on Ohsumed and 20 NG .","[('provided', (7, 8)), ('performs', (10, 11)), ('on', (15, 16))]","[('pre-trained Glo Ve word embeddings', (1, 6)), ('CNN', (9, 10)), ('much better', (11, 13)), ('Ohsumed', (16, 17)), ('20 NG', (18, 20))]","[['pre-trained Glo Ve word embeddings', 'provided', 'CNN'], ['CNN', 'performs', 'much better'], ['much better', 'on', 'Ohsumed'], ['much better', 'on', '20 NG']]","[['pre-trained Glo Ve word embeddings', 'has', 'CNN']]",[],[],document_classification,12,163
609,results,"Similarly , LSTM - based models also rely on pre-trained word embeddings and tend to perform better when documents are shorter .","[('rely on', (7, 9)), ('perform', (15, 16)), ('when', (17, 18)), ('are', (19, 20))]","[('LSTM - based models', (2, 6)), ('pre-trained word embeddings', (9, 12)), ('better', (16, 17)), ('documents', (18, 19)), ('shorter', (20, 21))]","[['LSTM - based models', 'rely on', 'pre-trained word embeddings'], ['LSTM - based models', 'perform', 'better'], ['better', 'when', 'documents'], ['documents', 'are', 'shorter']]",[],[],"[['Results', 'has', 'LSTM - based models']]",document_classification,12,165
610,results,"PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .","[('achieves', (3, 4)), ('to', (6, 7)), ('on', (9, 10))]","[('PV - DBOW', (0, 3)), ('comparable results', (4, 6)), ('strong baselines', (7, 9)), ('20 NG', (10, 12)), ('Ohsumed', (13, 14))]","[['PV - DBOW', 'achieves', 'comparable results'], ['comparable results', 'to', 'strong baselines'], ['strong baselines', 'on', '20 NG']]","[['PV - DBOW', 'has', 'comparable results']]",[],"[['Results', 'has', 'PV - DBOW']]",document_classification,12,166
611,results,"PV - DM performs worse than PV - DBOW , the only comparable results are on MR , where word orders are more essential .","[('performs', (3, 4)), ('than', (5, 6))]","[('PV - DM', (0, 3)), ('worse', (4, 5)), ('PV - DBOW', (6, 9))]","[['PV - DM', 'performs', 'worse'], ['worse', 'than', 'PV - DBOW']]",[],[],"[['Results', 'has', 'PV - DM']]",document_classification,12,168
612,results,Graph - CNN models also show competitive performances .,"[('show', (5, 6))]","[('Graph - CNN models', (0, 4)), ('competitive performances', (6, 8))]","[['Graph - CNN models', 'show', 'competitive performances']]","[['Graph - CNN models', 'has', 'competitive performances']]",[],[],document_classification,12,172
613,research-problem,Deep Pyramid Convolutional Neural Networks for Text Categorization,[],"[('Text Categorization', (6, 8))]",[],[],[],[],document_classification,13,2
614,model,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .","[('call', (1, 2)), ('per', (14, 15)), ('in', (18, 19))]","[('deep pyramid CNN ( DPCNN )', (3, 9)), ('computation time', (12, 14)), ('layer', (15, 16)), ('decreases exponentially', (16, 18)), ('pyramid shape', (21, 23))]","[['computation time', 'per', 'layer'], ['decreases exponentially', 'in', 'pyramid shape']]","[['layer', 'has', 'decreases exponentially']]","[['Model', 'call', 'deep pyramid CNN ( DPCNN )']]",[],document_classification,13,29
615,model,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .","[('After converting', (0, 2)), ('to', (4, 5)), ('alternates', (12, 13)), ('over and over', (20, 23))]","[('discrete text', (2, 4)), ('continuous representation', (5, 7)), ('DPCNN architecture', (9, 11)), ('a convolution block and a downsampling layer', (13, 20))]","[['discrete text', 'to', 'continuous representation'], ['DPCNN architecture', 'alternates', 'a convolution block and a downsampling layer']]","[['discrete text', 'has', 'continuous representation']]","[['Model', 'After converting', 'discrete text']]",[],document_classification,13,30
616,model,The network depth can be treated as a meta-parameter .,"[('treated as', (5, 7))]","[('network depth', (1, 3)), ('meta-parameter', (8, 9))]","[['network depth', 'treated as', 'meta-parameter']]","[['network depth', 'has', 'meta-parameter']]",[],"[['Model', 'has', 'network depth']]",document_classification,13,31
617,model,The computational complexity of this network is bounded to be no more than twice that of one convolution block .,"[('of', (3, 4)), ('bounded to be', (7, 10))]","[('computational complexity', (1, 3)), ('no more than twice', (10, 14)), ('one convolution block', (16, 19))]","[['no more than twice', 'of', 'one convolution block'], ['computational complexity', 'bounded to be', 'no more than twice']]",[],[],"[['Model', 'has', 'computational complexity']]",document_classification,13,32
618,model,We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification ..,"[('with', (4, 5))]","[('DPCNN', (3, 4)), ('15 weight layers', (5, 8))]","[['DPCNN', 'with', '15 weight layers']]",[],[],"[['Model', 'has', 'DPCNN']]",document_classification,13,36
619,model,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .","[('performs', (3, 4))]","[('first layer', (1, 3)), ('text region embedding', (4, 7))]","[['first layer', 'performs', 'text region embedding']]",[],[],"[['Model', 'has', 'first layer']]",document_classification,13,37
620,model,It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .,"[('of', (5, 6)), ('interleaved with', (16, 18)), ('with', (20, 21)), ('for', (23, 24))]","[('stacking', (4, 5)), ('convolution blocks', (6, 8)), ('pooling layers', (18, 20)), ('stride 2', (21, 23)), ('downsampling', (24, 25))]","[['stacking', 'of', 'convolution blocks'], ['convolution blocks', 'interleaved with', 'pooling layers'], ['pooling layers', 'with', 'stride 2'], ['stride 2', 'for', 'downsampling']]","[['stacking', 'has', 'convolution blocks']]",[],"[['Model', 'has', 'stacking']]",document_classification,13,38
621,model,The final pooling layer aggregates internal data for each document into one vector .,"[('for', (7, 8)), ('into', (10, 11))]","[('final pooling layer', (1, 4)), ('aggregates', (4, 5)), ('internal data', (5, 7)), ('each document', (8, 10)), ('one vector', (11, 13))]","[['internal data', 'for', 'each document'], ['each document', 'into', 'one vector']]","[['final pooling layer', 'has', 'aggregates'], ['aggregates', 'has', 'internal data']]",[],"[['Model', 'has', 'final pooling layer']]",document_classification,13,39
622,hyperparameters,We use max pooling for all pooling layers .,"[('use', (1, 2)), ('for', (4, 5))]","[('max pooling', (2, 4)), ('all pooling layers', (5, 8))]","[['max pooling', 'for', 'all pooling layers']]",[],"[['Hyperparameters', 'use', 'max pooling']]",[],document_classification,13,40
623,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],document_classification,13,150
624,results,Large data results,[],"[('Large data results', (0, 3))]",[],[],[],"[['Results', 'has', 'Large data results']]",document_classification,13,155
625,results,"On all the five datasets , DPCNN outperforms all of the previous results , which validates the effectiveness of our approach .","[('On', (0, 1)), ('outperforms', (7, 8))]","[('all the five datasets', (1, 5)), ('DPCNN', (6, 7)), ('all of the previous results', (8, 13))]","[['DPCNN', 'outperforms', 'all of the previous results']]","[['all the five datasets', 'has', 'DPCNN']]","[['Results', 'On', 'all the five datasets']]",[],document_classification,13,159
626,results,Small data results,[],"[('Small data results', (0, 3))]",[],[],[],"[['Results', 'has', 'Small data results']]",document_classification,13,191
627,results,"For these small datasets , the DPCNN performances with 100 - dim unsupervised embed - dings are shown , which turned out to be as good as those with 300 - dim unsupervised embeddings .","[('with', (8, 9)), ('turned out to be', (20, 24)), ('as good as', (24, 27)), ('with', (28, 29))]","[('DPCNN performances', (6, 8)), ('100 - dim unsupervised embed - dings', (9, 16)), ('300 - dim unsupervised embeddings', (29, 34))]","[['DPCNN performances', 'with', '100 - dim unsupervised embed - dings'], ['100 - dim unsupervised embed - dings', 'as good as', '300 - dim unsupervised embeddings']]",[],[],[],document_classification,13,194
628,results,"ShallowCNN ( row 2 ) rivals DPCNN ( row 1 ) , and Zhang et al. 's best linear model ( row 3 ) moved up from the worst performer to the third best performer .","[('rivals', (5, 6))]","[('ShallowCNN', (0, 1)), ('DPCNN', (6, 7))]","[['ShallowCNN', 'rivals', 'DPCNN']]",[],[],"[['Results', 'has', 'ShallowCNN']]",document_classification,13,196
629,research-problem,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,[],"[('Supervised and Semi- Supervised Text Categorization', (0, 6))]",[],[],[],[],document_classification,14,2
630,research-problem,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",[],"[('text categorization', (16, 18))]",[],[],[],[],document_classification,14,4
631,model,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .","[('build on', (5, 7)), ('of', (10, 11)), ('explore', (18, 19)), ('via', (24, 25)), ('in', (41, 42))]","[('general framework', (8, 10)), ('region embedding + pooling', (12, 16)), ('more sophisticated region embedding', (20, 24)), ('Long Short - Term Memory ( LSTM )', (25, 33)), ('supervised and semi-supervised settings', (43, 47))]","[['general framework', 'of', 'region embedding + pooling'], ['more sophisticated region embedding', 'via', 'Long Short - Term Memory ( LSTM )']]","[['general framework', 'name', 'region embedding + pooling']]","[['Model', 'build on', 'general framework']]",[],document_classification,14,31
632,model,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,"[('enable', (4, 5)), ('of', (6, 7)), ('over', (8, 9)), ('than feasible with', (12, 15))]","[('learning', (5, 6)), ('dependencies', (7, 8)), ('larger time lags', (9, 12)), ('traditional recurrent networks', (15, 18))]","[['learning', 'of', 'dependencies'], ['dependencies', 'over', 'larger time lags'], ['larger time lags', 'than feasible with', 'traditional recurrent networks']]",[],"[['Model', 'enable', 'learning']]",[],document_classification,14,35
633,model,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .","[('to embed', (8, 10)), ('of', (12, 13))]","[('LSTM', (4, 5)), ('text regions', (10, 12)), ('variable ( and possibly large ) sizes', (13, 20))]","[['LSTM', 'to embed', 'text regions'], ['text regions', 'of', 'variable ( and possibly large ) sizes']]",[],[],"[['Model', 'has', 'LSTM']]",document_classification,14,36
634,model,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .","[('to', (3, 4)), ('including', (12, 13)), ('of', (14, 15)), ('used to produce', (20, 23))]","[('strategy', (1, 2)), ('simplify the model', (4, 7)), ('elimination', (13, 14)), ('word embedding layer', (16, 19)), ('input', (23, 24)), ('LSTM', (25, 26))]","[['strategy', 'to', 'simplify the model'], ['input', 'to', 'LSTM'], ['elimination', 'of', 'word embedding layer'], ['word embedding layer', 'used to produce', 'input']]","[['strategy', 'has', 'simplify the model'], ['elimination', 'has', 'word embedding layer']]",[],"[['Model', 'has', 'strategy']]",document_classification,14,38
635,code,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,[],"[('http://riejohnson.com/cnn download.html', (8, 10))]",[],[],[],[],document_classification,14,46
636,experiments,Experiments ( supervised ),[],"[('Experiments ( supervised )', (0, 4))]",[],[],[],"[['Experiments', 'has', 'Experiments ( supervised )']]",document_classification,14,124
637,experiments,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,"[('done with', (2, 4)), ('with', (5, 6)), ('with', (11, 12)), ('for', (16, 17))]","[('Optimization', (0, 1)), ('SGD', (4, 5)), ('mini-batch size 50 or 100', (6, 11)), ('momentum', (12, 13)), ('optionally rmsprop', (14, 16)), ('acceleration', (17, 18))]","[['Optimization', 'done with', 'SGD'], ['Optimization', 'done with', 'optionally rmsprop'], ['SGD', 'with', 'mini-batch size 50 or 100'], ['SGD', 'with', 'optionally rmsprop'], ['mini-batch size 50 or 100', 'with', 'momentum'], ['optionally rmsprop', 'for', 'acceleration']]",[],[],[],document_classification,14,135
638,experiments,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .","[('see that', (9, 11)), ('with', (17, 18)), ('outperforms', (25, 26)), ('on', (35, 36))]","[('one - hot bidirectional LSTM', (12, 17)), ('pooling', (18, 19)), ('word - vector LSTM ( wv - LSTM )', (26, 35)), ('all the datasets', (36, 39))]","[['one - hot bidirectional LSTM', 'with', 'pooling'], ['one - hot bidirectional LSTM', 'outperforms', 'word - vector LSTM ( wv - LSTM )'], ['word - vector LSTM ( wv - LSTM )', 'on', 'all the datasets']]","[['pooling', 'name', 'word - vector LSTM ( wv - LSTM )']]",[],[],document_classification,14,139
639,experiments,Now we review the non -LSTM baseline methods .,"[('review', (2, 3))]","[('non -LSTM baseline methods', (4, 8))]",[],[],[],[],document_classification,14,140
640,results,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .","[('on', (2, 3)), ('out of', (4, 6)), ('outperforms', (14, 15))]","[('three', (3, 4)), ('four datasets', (7, 9)), ('oh - 2 LSTMp', (10, 14)), ('SVM', (15, 16)), ('CNN', (18, 19))]","[['three', 'out of', 'four datasets'], ['oh - 2 LSTMp', 'outperforms', 'SVM'], ['oh - 2 LSTMp', 'outperforms', 'CNN']]","[['three', 'has', 'four datasets'], ['four datasets', 'has', 'oh - 2 LSTMp']]",[],[],document_classification,14,143
641,results,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .","[('than', (9, 10)), ('outperforms', (25, 26))]","[('RCV1', (2, 3)), ('n-gram SVM', (4, 6)), ('no better', (7, 9)), ('bag - of - word SVM', (10, 16)), ('bow - CNN', (22, 25)), ('seq-CNN', (26, 27))]","[['no better', 'than', 'bag - of - word SVM'], ['bow - CNN', 'outperforms', 'seq-CNN']]","[['RCV1', 'has', 'n-gram SVM'], ['n-gram SVM', 'has', 'no better']]",[],[],document_classification,14,147
642,results,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .","[('works', (6, 7))]","[('one - hot CNN', (2, 6)), ('surprising well', (7, 9))]","[['one - hot CNN', 'works', 'surprising well']]","[['one - hot CNN', 'has', 'surprising well']]",[],"[['Results', 'has', 'one - hot CNN']]",document_classification,14,158
643,results,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .","[('on', (4, 5)), ('is', (6, 7)), ('of', (15, 16))]","[('previous best performance', (1, 4)), ('20NG', (5, 6)), ('15.3', (7, 8)), ('DL15', (16, 17))]","[['previous best performance', 'on', '20NG'], ['20NG', 'is', '15.3']]",[],[],"[['Results', 'has', 'previous best performance']]",document_classification,14,160
644,results,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .","[('achieved', (5, 6))]","[('Our oh - 2 LSTMp', (0, 5)), ('13.32', (6, 7))]","[['Our oh - 2 LSTMp', 'achieved', '13.32']]",[],[],"[['Results', 'has', 'Our oh - 2 LSTMp']]",document_classification,14,161
645,experiments,Semi-supervised experiments,[],"[('Semi-supervised experiments', (0, 2))]",[],[],[],[],document_classification,14,195
646,results,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .","[('clearly outperformed', (6, 8)), ('underperformed', (15, 16)), ('with', (18, 19))]","[('pre-trained wv - LSTM', (2, 6)), ('supervised wv - LSTM', (9, 13)), ('models', (17, 18)), ('region tv-embeddings', (19, 21))]","[['pre-trained wv - LSTM', 'clearly outperformed', 'supervised wv - LSTM'], ['supervised wv - LSTM', 'underperformed', 'models'], ['models', 'with', 'region tv-embeddings']]",[],[],"[['Results', 'has', 'pre-trained wv - LSTM']]",document_classification,14,210
647,results,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .","[('performed', (18, 19))]","[('wv - 2 LSTMp using the Google News vectors', (4, 13)), ('relatively poorly', (19, 21))]","[['wv - 2 LSTMp using the Google News vectors', 'performed', 'relatively poorly']]",[],[],[],document_classification,14,216
648,results,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .","[('review', (2, 3)), ('of', (5, 6)), ('with', (10, 11)), ('with', (25, 26)), ('with', (28, 29))]","[('performance', (4, 5)), ('one - hot CNN', (6, 10)), ('one 200 - dim CNN tv-embedding', (11, 17)), ('comparable', (24, 25)), ('our LSTM', (26, 28)), ('two 100 - dim LSTM tv-embeddings', (29, 35))]","[['performance', 'of', 'one - hot CNN'], ['one - hot CNN', 'with', 'one 200 - dim CNN tv-embedding'], ['comparable', 'with', 'our LSTM'], ['our LSTM', 'with', 'two 100 - dim LSTM tv-embeddings'], ['comparable', 'with', 'our LSTM'], ['our LSTM', 'with', 'two 100 - dim LSTM tv-embeddings']]","[['performance', 'has', 'one - hot CNN']]","[['Results', 'review', 'performance']]",[],document_classification,14,220
649,results,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,"[('rivals or outperforms', (7, 10)), ('on', (17, 18)), ('on', (24, 25))]","[('LSTM', (1, 2)), ('CNN', (11, 12)), ('IMDB / Elec', (18, 21)), ('underperforms', (22, 23)), ('RCV1', (25, 26))]","[['LSTM', 'rivals or outperforms', 'CNN'], ['CNN', 'on', 'IMDB / Elec'], ['underperforms', 'on', 'RCV1']]",[],[],"[['Results', 'has', 'LSTM']]",document_classification,14,221
650,results,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .","[('Increasing', (0, 1)), ('of', (3, 4)), ('from', (6, 7)), ('to', (8, 9)), ('on', (10, 11)), ('obtain', (14, 15))]","[('dimensionality', (2, 3)), ('LSTM tvembeddings', (4, 6)), ('100', (7, 8)), ('300', (9, 10)), ('RCV1', (11, 12)), ('8.62', (15, 16))]","[['dimensionality', 'of', 'LSTM tvembeddings'], ['LSTM tvembeddings', 'from', '100'], ['LSTM tvembeddings', 'to', '300'], ['300', 'on', 'RCV1']]",[],"[['Results', 'Increasing', 'dimensionality']]",[],document_classification,14,222
651,research-problem,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,[],"[('SEMI - SUPERVISED TEXT CLASSIFICATION', (4, 9))]",[],[],[],[],document_classification,15,2
652,research-problem,Previous work has primarily applied adversarial and virtual adversarial training to image classification tasks .,"[('to', (10, 11))]","[('adversarial and virtual adversarial training', (5, 10))]",[],[],[],[],document_classification,15,18
653,model,"In this work , we extend these techniques to text classification tasks and sequence models .",[],"[('text classification tasks', (9, 12)), ('sequence models', (13, 15))]",[],[],[],[],document_classification,15,19
654,model,Adversarial perturbations typically consist of making small modifications to very many real - valued inputs .,"[('consist of', (3, 5)), ('to', (8, 9))]","[('Adversarial perturbations', (0, 2)), ('small modifications', (6, 8)), ('very many real - valued inputs', (9, 15))]","[['small modifications', 'to', 'very many real - valued inputs']]",[],[],"[['Model', 'has', 'Adversarial perturbations']]",document_classification,15,20
655,research-problem,"For text classification , the input is discrete , and usually represented as a series of highdimensional one - hot vectors .","[('For', (0, 1)), ('is', (6, 7)), ('represented as', (11, 13)), ('of', (15, 16))]","[('text classification', (1, 3)), ('input', (5, 6)), ('discrete', (7, 8)), ('series', (14, 15)), ('highdimensional one - hot vectors', (16, 21))]","[['input', 'is', 'discrete'], ['input', 'represented as', 'series'], ['series', 'of', 'highdimensional one - hot vectors']]","[['text classification', 'has', 'input']]","[['Research problem', 'For', 'text classification']]",[],document_classification,15,21
656,model,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .","[('define', (18, 19)), ('on', (21, 22)), ('instead of', (25, 27))]","[('perturbation', (15, 16)), ('continuous word embeddings', (22, 25)), ('discrete word inputs', (27, 30))]","[['continuous word embeddings', 'instead of', 'discrete word inputs']]",[],[],[],document_classification,15,22
657,model,We thus propose this approach exclusively as a means of regularizing a text classifier by stabilizing the classification function .,"[('as a means of', (6, 10)), ('by', (14, 15)), ('stabilizing', (15, 16))]","[('regularizing', (10, 11)), ('text classifier', (12, 14)), ('classification function', (17, 19))]","[['text classifier', 'stabilizing', 'classification function']]","[['regularizing', 'has', 'text classifier']]","[['Model', 'as a means of', 'regularizing']]",[],document_classification,15,25
658,experimental-setup,All experiments used TensorFlow on GPUs .,"[('used', (2, 3)), ('on', (4, 5))]","[('TensorFlow', (3, 4)), ('GPUs', (5, 6))]","[['TensorFlow', 'on', 'GPUs']]",[],"[['Experimental setup', 'used', 'TensorFlow']]",[],document_classification,15,100
659,code,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text .,[],"[('https://github.com/tensorflow/models/tree/master/adversarial_text', (5, 6))]",[],[],[],[],document_classification,15,101
660,experimental-setup,"We trained for 100,000 steps .","[('trained for', (1, 3))]","[('100,000 steps', (3, 5))]",[],[],"[['Experimental setup', 'trained for', '100,000 steps']]",[],document_classification,15,112
661,experimental-setup,We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,"[('applied', (1, 2)), ('with', (4, 5)), ('set to', (6, 8)), ('on', (9, 10)), ('except', (13, 14))]","[('gradient clipping', (2, 4)), ('norm', (5, 6)), ('1.0', (8, 9)), ('all the parameters', (10, 13)), ('word embeddings', (14, 16))]","[['gradient clipping', 'with', 'norm'], ['norm', 'set to', '1.0'], ['1.0', 'on', 'all the parameters'], ['all the parameters', 'except', 'word embeddings']]",[],"[['Experimental setup', 'applied', 'gradient clipping']]",[],document_classification,15,113
662,experimental-setup,"For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .","[('For', (0, 1)), ('of', (2, 3)), ('applied', (9, 10)), ('on', (11, 12)), ('with', (16, 17))]","[('regularization', (1, 2)), ('recurrent language model', (4, 7)), ('dropout', (10, 11)), ('word embedding layer', (13, 16)), ('0.5 dropout rate', (17, 20))]","[['regularization', 'of', 'recurrent language model'], ['regularization', 'applied', 'dropout'], ['recurrent language model', 'applied', 'dropout'], ['dropout', 'on', 'word embedding layer'], ['word embedding layer', 'with', '0.5 dropout rate']]",[],"[['Experimental setup', 'For', 'regularization']]",[],document_classification,15,115
663,experimental-setup,"For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .","[('used', (7, 8)), ('for', (12, 13)), ('shared with', (31, 33))]","[('bidirectional LSTM model', (2, 5)), ('512 hidden units LSTM', (8, 12)), ('standard order and reversed order sequences', (15, 21)), ('256 dimensional word embeddings', (25, 29)), ('both of the LSTMs', (33, 37))]","[['bidirectional LSTM model', 'used', '512 hidden units LSTM'], ['512 hidden units LSTM', 'for', 'standard order and reversed order sequences'], ['256 dimensional word embeddings', 'shared with', 'both of the LSTMs']]",[],[],[],document_classification,15,116
664,results,We saw that cosine distance on adversarial and virtual adversarial training ( 0.159-0.331 ) were much smaller than ones on the baseline and random perturbation method ( 0.244-0.399 ) .,"[('saw that', (1, 3)), ('on', (5, 6)), ('than ones on', (17, 20))]","[('cosine distance', (3, 5)), ('adversarial and virtual adversarial training', (6, 11)), ('much smaller', (15, 17)), ('baseline and random perturbation method', (21, 26))]","[['cosine distance', 'on', 'adversarial and virtual adversarial training'], ['much smaller', 'than ones on', 'baseline and random perturbation method']]",[],"[['Results', 'saw that', 'cosine distance']]",[],document_classification,15,163
665,results,shows the test performance on the Elec and RCV1 datasets .,"[('shows', (0, 1)), ('on', (4, 5))]","[('test performance', (2, 4)), ('Elec and RCV1 datasets', (6, 10))]","[['test performance', 'on', 'Elec and RCV1 datasets']]","[['test performance', 'has', 'Elec and RCV1 datasets']]","[['Results', 'shows', 'test performance']]",[],document_classification,15,165
666,results,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .","[('see', (2, 3)), ('on', (9, 10)), ('achieved', (14, 15))]","[('our proposed method', (3, 6)), ('improved', (6, 7)), ('test performance', (7, 9)), ('baseline method', (11, 13)), ('state of the art performance', (15, 20))]","[['test performance', 'on', 'baseline method'], ['our proposed method', 'achieved', 'state of the art performance']]","[['our proposed method', 'has', 'improved'], ['improved', 'has', 'test performance']]","[['Results', 'see', 'our proposed method']]",[],document_classification,15,166
667,results,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,"[('improves', (4, 5)), ('on', (5, 6))]","[('Our unidirectional LSTM model', (0, 4)), ('state of the art method', (7, 12)), ('our method with a bidirectional LSTM', (13, 19)), ('further improves', (19, 21)), ('results', (21, 22)), ('RCV1', (23, 24))]","[['results', 'on', 'RCV1']]","[['our method with a bidirectional LSTM', 'has', 'further improves'], ['further improves', 'has', 'results']]",[],"[['Results', 'has', 'Our unidirectional LSTM model']]",document_classification,15,167
668,results,shows test performance on the Rotten Tomatoes dataset .,[],"[('Rotten Tomatoes dataset', (5, 8))]",[],[],[],"[['Results', 'has', 'Rotten Tomatoes dataset']]",document_classification,15,169
669,results,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .","[('able to', (3, 5)), ('over', (6, 7)), ('with', (12, 13))]","[('Adversarial training', (0, 2)), ('improve', (5, 6)), ('baseline method', (8, 10)), ('adversarial and virtual adversarial cost', (14, 19))]","[['Adversarial training', 'able to', 'improve'], ['improve', 'over', 'baseline method'], ['Adversarial training', 'with', 'adversarial and virtual adversarial cost']]","[['Adversarial training', 'has', 'improve']]",[],"[['Results', 'has', 'Adversarial training']]",document_classification,15,170
670,results,However the test performance of only virtual adversarial training was worse than the baseline .,"[('of', (4, 5)), ('than', (11, 12))]","[('test performance', (2, 4)), ('only virtual adversarial training', (5, 9)), ('worse', (10, 11)), ('baseline', (13, 14))]","[['test performance', 'of', 'only virtual adversarial training'], ['worse', 'than', 'baseline']]",[],[],[],document_classification,15,171
671,research-problem,A C - LSTM Neural Network for Text Classification,[],"[('Text Classification', (7, 9))]",[],[],[],[],document_classification,16,2
672,model,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .","[('introduce', (5, 6)), ('short for', (9, 11)), ('by combining', (14, 16)), ('to model', (19, 21))]","[('new architecture', (7, 9)), ('C - LSTM', (11, 14)), ('CNN and LSTM', (16, 19)), ('sentences', (21, 22))]","[['new architecture', 'short for', 'C - LSTM'], ['new architecture', 'by combining', 'CNN and LSTM'], ['CNN and LSTM', 'to model', 'sentences']]",[],"[['Model', 'introduce', 'new architecture']]",[],document_classification,16,29
673,model,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .","[('of', (5, 6)), ('design', (12, 13)), ('by feeding', (23, 25)), ('into', (33, 34))]","[('simple end - to - end , unified architecture', (14, 23)), ('output', (26, 27)), ('one - layer CNN', (29, 33)), ('LSTM', (34, 35))]","[['output', 'of', 'one - layer CNN'], ['simple end - to - end , unified architecture', 'by feeding', 'output'], ['one - layer CNN', 'into', 'LSTM']]",[],[],[],document_classification,16,30
674,model,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,"[('constructed on top of', (3, 7)), ('from', (11, 12)), ('to learn', (16, 18)), ('of', (22, 23))]","[('CNN', (1, 2)), ('pre-trained word vectors', (8, 11)), ('massive unlabeled text data', (12, 16)), ('higher - level representions', (18, 22)), ('n-grams', (23, 24))]","[['CNN', 'constructed on top of', 'pre-trained word vectors'], ['pre-trained word vectors', 'from', 'massive unlabeled text data'], ['massive unlabeled text data', 'to learn', 'higher - level representions'], ['higher - level representions', 'of', 'n-grams']]",[],[],"[['Model', 'has', 'CNN']]",document_classification,16,31
675,model,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .","[('to learn', (1, 3)), ('from', (5, 6)), ('of', (15, 16)), ('organized as', (18, 20)), ('to serve', (23, 25)), ('of', (28, 29))]","[('sequential correlations', (3, 5)), ('higher - level suqence representations', (6, 11)), ('feature maps', (13, 15)), ('CNN', (16, 17)), ('sequential window features', (20, 23)), ('input', (27, 28)), ('LSTM', (29, 30))]","[['sequential correlations', 'from', 'higher - level suqence representations'], ['feature maps', 'of', 'CNN'], ['feature maps', 'organized as', 'sequential window features'], ['CNN', 'organized as', 'sequential window features'], ['sequential window features', 'to serve', 'input'], ['input', 'of', 'LSTM']]",[],"[['Model', 'to learn', 'sequential correlations']]",[],document_classification,16,32
676,model,"We choose sequence - based input other than relying on the syntactic parse trees before feeding in the neural network , thus our model does n't rely on any external language knowledge and complicated pre-processing .","[('choose', (1, 2)), ('before', (14, 15)), ('in', (16, 17))]","[('sequence - based input', (2, 6)), ('feeding', (15, 16)), ('neural network', (18, 20))]","[['feeding', 'in', 'neural network']]",[],"[['Model', 'choose', 'sequence - based input']]",[],document_classification,16,34
677,experimental-setup,"We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .","[('implement', (1, 2)), ('based on', (4, 6))]","[('our model', (2, 4)), ('Theano', (6, 7))]","[['our model', 'based on', 'Theano']]",[],"[['Experimental setup', 'implement', 'our model']]",[],document_classification,16,142
678,experimental-setup,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .","[('train', (13, 14)), ('on', (16, 17))]","[('model', (15, 16)), ('GPU', (18, 19))]","[['model', 'on', 'GPU']]",[],"[['Experimental setup', 'train', 'model']]",[],document_classification,16,143
679,experimental-setup,"In our final settings , we only use one convolutional layer and one LSTM layer for both tasks .","[('use', (7, 8))]","[('one convolutional layer', (8, 11)), ('one LSTM layer', (12, 15))]",[],[],"[['Experimental setup', 'use', 'one convolutional layer']]",[],document_classification,16,147
680,experimental-setup,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .","[('For', (0, 1)), ('set to be', (8, 11)), ('set to be', (17, 20))]","[('TREC', (1, 2)), ('number of filters', (4, 7)), ('300', (11, 12)), ('memory dimension', (14, 16)), ('300', (20, 21))]","[['number of filters', 'set to be', '300'], ['memory dimension', 'set to be', '300']]","[['TREC', 'has', 'number of filters'], ['number of filters', 'has', '300'], ['memory dimension', 'has', '300']]","[['Experimental setup', 'For', 'TREC']]",[],document_classification,16,167
681,experimental-setup,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,"[('dropped', (9, 10)), ('outwith', (10, 11)), ('of', (13, 14))]","[('word vector layer and the LSTM layer', (1, 8)), ('probability', (12, 13)), ('0.5', (14, 15))]","[['probability', 'of', '0.5']]","[['word vector layer and the LSTM layer', 'has', 'probability']]",[],"[['Experimental setup', 'has', 'word vector layer and the LSTM layer']]",document_classification,16,168
682,experimental-setup,We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,"[('add', (2, 3)), ('with', (5, 6)), ('of', (8, 9)), ('to', (10, 11)), ('in', (13, 14)), ('for', (17, 18))]","[('L2 regularization', (3, 5)), ('factor', (7, 8)), ('0.001', (9, 10)), ('weights', (12, 13)), ('softmax layer', (15, 17)), ('both tasks', (18, 20))]","[['L2 regularization', 'with', 'factor'], ['factor', 'of', '0.001'], ['0.001', 'to', 'weights'], ['weights', 'in', 'softmax layer'], ['softmax layer', 'for', 'both tasks']]","[['L2 regularization', 'has', 'factor']]","[['Experimental setup', 'add', 'L2 regularization']]",[],document_classification,16,169
683,experimental-setup,Results and Model Analysis,[],"[('Results', (0, 1))]",[],[],[],[],document_classification,16,170
684,results,Sentiment Classification,[],"[('Sentiment Classification', (0, 2))]",[],[],[],"[['Results', 'has', 'Sentiment Classification']]",document_classification,16,173
685,results,"To the best of our knowledge , we achieve the fourth best published result for the 5 - class classification task on this dataset .","[('achieve', (8, 9)), ('for', (14, 15))]","[('fourth best published result', (10, 14)), ('5 - class classification task', (16, 21))]","[['fourth best published result', 'for', '5 - class classification task']]",[],"[['Results', 'achieve', 'fourth best published result']]",[],document_classification,16,184
686,results,"For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .","[('For', (0, 1)), ('achieve', (7, 8)), ('with respect to', (10, 13))]","[('binary classification task', (2, 5)), ('comparable results', (8, 10)), ('state - of - the - art ones', (14, 22))]","[['binary classification task', 'achieve', 'comparable results'], ['comparable results', 'with respect to', 'state - of - the - art ones']]",[],"[['Results', 'For', 'binary classification task']]",[],document_classification,16,185
687,results,Question Type Classification,[],"[('Question Type Classification', (0, 3))]",[],[],[],"[['Results', 'has', 'Question Type Classification']]",document_classification,16,191
688,results,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .",[],"[('Our result', (3, 5)), ('consistently outperforms', (5, 7)), ('all published neural baseline models', (7, 12))]",[],"[['Our result', 'has', 'consistently outperforms'], ['consistently outperforms', 'has', 'all published neural baseline models']]",[],"[['Results', 'has', 'Our result']]",document_classification,16,198
689,results,( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,"[('to', (7, 8))]","[('close', (6, 7)), ('state - of - the - art SVM', (11, 19))]","[['close', 'to', 'state - of - the - art SVM']]",[],[],[],document_classification,16,199
690,ablation-analysis,"However , we found in our experiments that single convolutional layer with filter length 3 always outperforms the other cases .","[('found', (3, 4)), ('with', (11, 12)), ('always outperforms', (15, 17))]","[('single convolutional layer', (8, 11)), ('filter length 3', (12, 15)), ('other cases', (18, 20))]","[['single convolutional layer', 'with', 'filter length 3'], ['filter length 3', 'always outperforms', 'other cases']]",[],"[['Ablation analysis', 'found', 'single convolutional layer']]",[],document_classification,16,206
691,ablation-analysis,It it shown that single convolutional layer with filter length 3 performs best among all filter configurations .,"[('shown', (2, 3)), ('with', (7, 8)), ('performs', (11, 12)), ('among', (13, 14))]","[('single convolutional layer', (4, 7)), ('filter length 3', (8, 11)), ('best', (12, 13)), ('all filter configurations', (14, 17))]","[['single convolutional layer', 'with', 'filter length 3'], ['filter length 3', 'performs', 'best'], ['best', 'among', 'all filter configurations']]",[],"[['Ablation analysis', 'shown', 'single convolutional layer']]",[],document_classification,16,210
692,ablation-analysis,"For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .","[('For', (0, 1)), ('in', (7, 8)), ('shown that', (12, 14)), ('with', (16, 17)), ('that', (22, 23))]","[('multiple convolutional layers', (4, 7)), ('parallel', (8, 9)), ('filter configurations', (14, 16)), ('filter length 3', (17, 20)), ('performs better', (20, 22)), ('without tri-gram filters', (24, 27))]","[['multiple convolutional layers', 'in', 'parallel'], ['multiple convolutional layers', 'shown that', 'filter configurations'], ['performs better', 'shown that', 'without tri-gram filters'], ['filter configurations', 'with', 'filter length 3'], ['performs better', 'that', 'without tri-gram filters']]","[['multiple convolutional layers', 'has', 'parallel'], ['filter length 3', 'has', 'performs better'], ['performs better', 'has', 'without tri-gram filters']]","[['Ablation analysis', 'For', 'multiple convolutional layers']]",[],document_classification,16,211
693,research-problem,Very Deep Convolutional Networks for Text Classification,[],"[('Text Classification', (5, 7))]",[],[],[],[],document_classification,17,2
694,research-problem,"We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences , jointly with the task .","[('to develop', (8, 10)), ('able to learn', (14, 17)), ('of', (19, 20)), ('with', (24, 25))]","[('deep architectures', (10, 12)), ('hierarchical representations', (17, 19)), ('whole sentences', (20, 22)), ('jointly', (23, 24)), ('task', (26, 27))]","[['deep architectures', 'able to learn', 'hierarchical representations'], ['hierarchical representations', 'of', 'whole sentences'], ['jointly', 'with', 'task']]",[],"[['Research problem', 'to develop', 'deep architectures']]",[],document_classification,17,34
695,model,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .","[('use', (7, 8)), ('of', (10, 11)), ('using', (19, 20))]","[('deep architectures', (8, 10)), ('many convolutional layers', (11, 14)), ('up to 29 layers', (20, 24))]","[['deep architectures', 'of', 'many convolutional layers']]",[],"[['Model', 'use', 'deep architectures']]",[],document_classification,17,35
696,experimental-setup,"The dictionary consists of the following characters "" abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:'"" / | # $ % & *' +=<>( ) [ ]{} "" plus a special padding , space and unknown token which add up to a total of 69 tokens .","[('consists of', (2, 4))]","[('dictionary', (1, 2)), ('abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:\'"" / | # $ % & *\' +=<>( ) [ ]{}', (8, 21)), ('special padding', (24, 26)), ('space', (27, 28)), ('unknown token', (29, 31))]","[['dictionary', 'consists of', 'abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:\'"" / | # $ % & *\' +=<>( ) [ ]{}'], ['dictionary', 'consists of', 'unknown token']]","[['dictionary', 'has', 'abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:\'"" / | # $ % & *\' +=<>( ) [ ]{}']]",[],"[['Experimental setup', 'has', 'dictionary']]",document_classification,17,196
697,experimental-setup,"The input text is padded to a fixed size of 1014 , larger text are truncated .","[('padded to', (4, 6)), ('of', (9, 10))]","[('input text', (1, 3)), ('fixed size', (7, 9)), ('1014', (10, 11))]","[['input text', 'padded to', 'fixed size'], ['fixed size', 'of', '1014']]",[],[],"[['Experimental setup', 'has', 'input text']]",document_classification,17,197
698,experimental-setup,The character embedding is of size 16 .,"[('of', (4, 5))]","[('character embedding', (1, 3)), ('size 16', (5, 7))]","[['character embedding', 'of', 'size 16']]","[['character embedding', 'has', 'size 16']]",[],"[['Experimental setup', 'has', 'character embedding']]",document_classification,17,198
699,experimental-setup,"Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .","[('performed with', (2, 4)), ('using', (6, 7)), ('of', (9, 10)), ('of', (17, 18)), ('of', (21, 22))]","[('Training', (0, 1)), ('SGD', (4, 5)), ('mini-batch', (8, 9)), ('size', (10, 11)), ('128', (11, 12)), ('initial learning rate', (14, 17)), ('0.01', (18, 19)), ('momentum', (20, 21)), ('0.9', (22, 23))]","[['Training', 'performed with', 'SGD'], ['SGD', 'using', 'mini-batch'], ['SGD', 'using', 'momentum'], ['mini-batch', 'of', 'size'], ['initial learning rate', 'of', '0.01'], ['momentum', 'of', '0.9'], ['initial learning rate', 'of', '0.01'], ['momentum', 'of', '0.9'], ['momentum', 'of', '0.9']]","[['size', 'has', '128'], ['initial learning rate', 'has', '0.01'], ['momentum', 'has', '0.9']]",[],"[['Experimental setup', 'has', 'Training']]",document_classification,17,199
700,experimental-setup,The implementation is done using Torch 7 .,"[('done using', (3, 5))]","[('implementation', (1, 2)), ('Torch 7', (5, 7))]","[['implementation', 'done using', 'Torch 7']]",[],[],"[['Experimental setup', 'has', 'implementation']]",document_classification,17,204
701,experimental-setup,All experiments are performed on a single NVidia K40 GPU .,"[('performed on', (3, 5))]","[('single NVidia K40 GPU', (6, 10))]",[],[],"[['Experimental setup', 'performed on', 'single NVidia K40 GPU']]",[],document_classification,17,205
702,experimental-setup,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .","[('without', (17, 18))]","[('use', (5, 6)), ('temporal batch norm', (14, 17)), ('dropout', (18, 19))]","[['temporal batch norm', 'without', 'dropout']]",[],[],[],document_classification,17,206
703,results,"Our deep architecture works well on big data sets in particular , even for small depths .","[('works', (3, 4)), ('on', (5, 6))]","[('Our deep architecture', (0, 3)), ('well', (4, 5)), ('big data sets', (6, 9))]","[['Our deep architecture', 'works', 'well'], ['well', 'on', 'big data sets']]","[['Our deep architecture', 'has', 'well']]",[],"[['Results', 'has', 'Our deep architecture']]",document_classification,17,210
704,results,"For the smallest depth we use ( 9 convolutional layers ) , we see that our model already performs better than Zhang 's convolutional baselines ( which includes 6 convolutional layers and has a different architecture ) on the biggest data sets :","[('For', (0, 1)), ('use', (5, 6)), ('see that', (13, 15)), ('performs', (18, 19)), ('than', (20, 21)), ('includes', (27, 28))]","[('smallest depth', (2, 4)), ('9 convolutional layers', (7, 10)), ('our model', (15, 17)), ('better', (19, 20)), (""Zhang 's convolutional baselines"", (21, 25)), ('6 convolutional layers', (28, 31)), ('different architecture', (34, 36))]","[['smallest depth', 'use', '9 convolutional layers'], ['9 convolutional layers', 'see that', 'our model'], ['our model', 'performs', 'better'], ['better', 'than', ""Zhang 's convolutional baselines""], [""Zhang 's convolutional baselines"", 'includes', '6 convolutional layers']]",[],[],[],document_classification,17,212
705,results,The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples . :,"[('in', (4, 5)), ('observed on', (9, 11))]","[('most important decrease', (1, 4)), ('classification error', (5, 7)), ('largest data set Amazon Full', (12, 17)), ('more than 3 Million training samples', (19, 25))]","[['most important decrease', 'in', 'classification error'], ['most important decrease', 'observed on', 'largest data set Amazon Full'], ['classification error', 'observed on', 'largest data set Amazon Full']]",[],[],"[['Results', 'has', 'most important decrease']]",document_classification,17,214
706,results,"We also observe that for a small depth , temporal max - pooling works best on all data sets .","[('for', (4, 5)), ('works', (13, 14)), ('on', (15, 16))]","[('small depth', (6, 8)), ('temporal max - pooling', (9, 13)), ('best', (14, 15)), ('all data sets', (16, 19))]","[['temporal max - pooling', 'works', 'best'], ['best', 'on', 'all data sets']]","[['small depth', 'has', 'temporal max - pooling']]","[['Results', 'for', 'small depth']]",[],document_classification,17,217
707,research-problem,Character - level Convolutional Networks for Text Classification,[],"[('Text Classification', (6, 8))]",[],[],[],[],document_classification,18,2
708,model,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .","[('treating', (5, 6)), ('as a kind of', (7, 11)), ('at', (13, 14)), ('applying', (18, 19))]","[('text', (6, 7)), ('raw signal', (11, 13)), ('character level', (14, 16)), ('temporal ( one-dimensional ) ConvNets', (19, 24))]","[['text', 'as a kind of', 'raw signal'], ['raw signal', 'at', 'character level']]",[],"[['Model', 'treating', 'text']]",[],document_classification,18,18
709,experiments,Traditional Methods,[],"[('Traditional Methods', (0, 2))]",[],[],[],[],document_classification,18,103
710,baselines,Bag - of - words and its TFIDF .,[],"[('Bag - of - words and its TFIDF', (0, 8))]",[],[],[],"[['Baselines', 'has', 'Bag - of - words and its TFIDF']]",document_classification,18,106
711,hyperparameters,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .","[('by selecting', (13, 15)), ('from', (19, 20))]","[('bag - of - words model', (5, 11)), ('constructed', (12, 13)), ('50,000 most frequent words', (15, 19)), ('training subset', (21, 23))]","[['bag - of - words model', 'by selecting', '50,000 most frequent words'], ['constructed', 'by selecting', '50,000 most frequent words'], ['50,000 most frequent words', 'from', 'training subset']]","[['bag - of - words model', 'has', 'constructed']]",[],"[['Hyperparameters', 'has', 'bag - of - words model']]",document_classification,18,107
712,baselines,Bag - of - ngrams and its TFIDF .,[],"[('Bag - of - ngrams and its TFIDF', (0, 8))]",[],[],[],"[['Baselines', 'has', 'Bag - of - ngrams and its TFIDF']]",document_classification,18,112
713,hyperparameters,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .","[('by selecting', (9, 11)), ('from', (23, 24)), ('for', (27, 28))]","[('bag - of - ngrams models', (1, 7)), ('constructed', (8, 9)), ('500,000 most frequent n-grams ( up to 5 - grams )', (12, 23)), ('training subset', (25, 27)), ('each dataset', (28, 30))]","[['bag - of - ngrams models', 'by selecting', '500,000 most frequent n-grams ( up to 5 - grams )'], ['constructed', 'by selecting', '500,000 most frequent n-grams ( up to 5 - grams )'], ['500,000 most frequent n-grams ( up to 5 - grams )', 'from', 'training subset'], ['training subset', 'for', 'each dataset']]","[['bag - of - ngrams models', 'has', 'constructed']]",[],"[['Hyperparameters', 'has', 'bag - of - ngrams models']]",document_classification,18,113
714,baselines,Bag - of - means on word embedding .,[],"[('Bag - of - means on word embedding', (0, 8))]",[],[],[],"[['Baselines', 'has', 'Bag - of - means on word embedding']]",document_classification,18,115
715,baselines,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .","[('uses', (7, 8)), ('on', (9, 10)), ('learnt from', (11, 13)), ('of', (16, 17))]","[('k-means', (8, 9)), ('word2vec', (10, 11)), ('training subset', (14, 16)), ('each dataset', (17, 19))]","[['k-means', 'on', 'word2vec'], ['word2vec', 'learnt from', 'training subset'], ['training subset', 'of', 'each dataset']]",[],[],[],document_classification,18,116
716,experiments,Deep Learning Methods,[],"[('Deep Learning Methods', (0, 3))]",[],[],[],[],document_classification,18,121
717,baselines,Word - based ConvNets .,[],"[('Word - based ConvNets', (0, 4))]",[],[],[],"[['Baselines', 'has', 'Word - based ConvNets']]",document_classification,18,124
718,experiments,Long - short term memory .,[],"[('Long - short term memory', (0, 5))]",[],[],[],[],document_classification,18,131
719,results,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,"[('work for', (14, 16)), ('without', (18, 19)), ('for', (21, 22))]","[('character - level ConvNets', (9, 13)), ('text classification', (16, 18)), ('need', (20, 21)), ('words', (22, 23))]","[['character - level ConvNets', 'work for', 'text classification'], ['text classification', 'without', 'need'], ['need', 'for', 'words']]",[],[],"[['Results', 'has', 'character - level ConvNets']]",document_classification,18,194
720,results,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .","[('like', (2, 3)), ('remain', (5, 6)), ('for', (8, 9)), ('of', (10, 11)), ('up to', (12, 14)), ('goes to', (24, 26)), ('of', (28, 29)), ('do', (31, 32)), ('observe', (33, 34))]","[('Traditional methods', (0, 2)), ('n-grams TFIDF', (3, 5)), ('strong candidates', (6, 8)), ('dataset', (9, 10)), ('size', (11, 12)), ('several hundreds of thousands', (14, 18)), ('dataset', (23, 24)), ('scale', (27, 28)), ('several millions', (29, 31)), ('character - level ConvNets', (35, 39)), ('better', (42, 43))]","[['Traditional methods', 'like', 'n-grams TFIDF'], ['n-grams TFIDF', 'remain', 'strong candidates'], ['strong candidates', 'for', 'dataset'], ['dataset', 'of', 'size'], ['size', 'up to', 'several hundreds of thousands'], ['dataset', 'goes to', 'scale'], ['scale', 'of', 'several millions'], ['several millions', 'observe', 'character - level ConvNets']]",[],[],"[['Results', 'has', 'Traditional methods']]",document_classification,18,199
721,results,Conv Nets may work well for user - generated data .,"[('may work', (2, 4)), ('for', (5, 6))]","[('Conv Nets', (0, 2)), ('well', (4, 5)), ('user - generated data', (6, 10))]","[['Conv Nets', 'may work', 'well'], ['well', 'for', 'user - generated data']]","[['Conv Nets', 'has', 'well']]",[],"[['Results', 'has', 'Conv Nets']]",document_classification,18,200
722,experiments,Choice of alphabet makes a difference .,"[('makes', (3, 4))]","[('Choice of alphabet', (0, 3)), ('difference', (5, 6))]","[['Choice of alphabet', 'makes', 'difference']]","[['Choice of alphabet', 'has', 'difference']]",[],[],document_classification,18,207
723,experiments,Semantics of tasks may not matter .,"[('of', (1, 2)), ('may not', (3, 5))]","[('Semantics', (0, 1)), ('tasks', (2, 3)), ('matter', (5, 6))]","[['Semantics', 'of', 'tasks'], ['tasks', 'may not', 'matter']]",[],[],[],document_classification,18,211
724,research-problem,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling,[],"[('Text Classification', (0, 2))]",[],[],[],[],document_classification,19,2
725,model,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .","[('proposes', (5, 6)), ('with', (13, 14)), ('to capture', (24, 26)), ('on', (27, 28))]","[('Bidirectional Long Short - Term Memory Networks', (6, 13)), ('Two - Dimensional Max Pooling', (14, 19)), ('BLSTM - 2DPooling', (20, 23)), ('features', (26, 27)), ('time - step dimension and the feature vector dimension', (30, 39))]","[['Bidirectional Long Short - Term Memory Networks', 'with', 'Two - Dimensional Max Pooling'], ['Two - Dimensional Max Pooling', 'to capture', 'features'], ['features', 'on', 'time - step dimension and the feature vector dimension']]","[['Two - Dimensional Max Pooling', 'name', 'BLSTM - 2DPooling']]","[['Model', 'proposes', 'Bidirectional Long Short - Term Memory Networks']]",[],document_classification,19,31
726,model,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,"[('first utilizes', (1, 3)), ('to transform', (13, 15)), ('into', (17, 18))]","[('Bidirectional Long Short - Term Memory Networks ( BLSTM )', (3, 13)), ('text', (16, 17)), ('vectors', (18, 19))]","[['Bidirectional Long Short - Term Memory Networks ( BLSTM )', 'to transform', 'text'], ['text', 'into', 'vectors']]",[],"[['Model', 'first utilizes', 'Bidirectional Long Short - Term Memory Networks ( BLSTM )']]",[],document_classification,19,32
727,model,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,"[('obtain', (9, 10))]","[('2D max pooling operation', (2, 6)), ('fixed - length vector', (11, 15))]","[['2D max pooling operation', 'obtain', 'fixed - length vector']]",[],[],[],document_classification,19,33
728,model,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,"[('applies', (3, 4)), ('to capture', (11, 13)), ('to represent', (16, 18))]","[('2D convolution ( BLSTM - 2DCNN )', (4, 11)), ('more meaningful features', (13, 16)), ('input text', (19, 21))]","[['2D convolution ( BLSTM - 2DCNN )', 'to capture', 'more meaningful features'], ['more meaningful features', 'to represent', 'input text']]",[],"[['Model', 'applies', '2D convolution ( BLSTM - 2DCNN )']]",[],document_classification,19,34
729,hyperparameters,"The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .","[('of', (2, 3)), ('is', (5, 6)), ('of', (11, 12)), ('is', (13, 14))]","[('dimension', (1, 2)), ('word embeddings', (3, 5)), ('300', (6, 7)), ('hidden units', (9, 11)), ('LSTM', (12, 13)), ('300', (14, 15))]","[['dimension', 'of', 'word embeddings'], ['hidden units', 'of', 'LSTM'], ['dimension', 'is', '300'], ['word embeddings', 'is', '300'], ['LSTM', 'is', '300'], ['hidden units', 'of', 'LSTM'], ['hidden units', 'is', '300'], ['LSTM', 'is', '300']]",[],[],"[['Hyperparameters', 'has', 'dimension']]",document_classification,19,170
730,hyperparameters,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .","[('use', (1, 2)), ('for', (6, 7)), ('of', (9, 10)), ('of', (19, 20))]","[('100 convolutional filters each', (2, 6)), ('window sizes', (7, 9)), ('( 3 , 3 )', (10, 15)), ('2D pooling size', (16, 19)), ('( 2 , 2 )', (20, 25))]","[['100 convolutional filters each', 'for', 'window sizes'], ['window sizes', 'of', '( 3 , 3 )'], ['2D pooling size', 'of', '( 2 , 2 )'], ['2D pooling size', 'of', '( 2 , 2 )']]",[],"[['Hyperparameters', 'use', '100 convolutional filters each']]",[],document_classification,19,171
731,hyperparameters,We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,"[('set', (1, 2)), ('as', (5, 6)), ('of', (11, 12)), ('as', (13, 14))]","[('mini-batch size', (3, 5)), ('10', (6, 7)), ('learning rate', (9, 11)), ('AdaDelta', (12, 13)), ('default value 1.0', (15, 18))]","[['mini-batch size', 'as', '10'], ['learning rate', 'of', 'AdaDelta'], ['learning rate', 'as', 'default value 1.0'], ['AdaDelta', 'as', 'default value 1.0']]","[['mini-batch size', 'has', '10']]","[['Hyperparameters', 'set', 'mini-batch size']]",[],document_classification,19,172
732,hyperparameters,"For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .","[('For', (0, 1)), ('employ', (4, 5)), ('with', (7, 8)), ('of', (10, 11)), ('for', (12, 13)), ('for', (18, 19)), ('for', (24, 25)), ('use', (31, 32)), ('with', (35, 36)), ('over', (40, 41))]","[('regularization', (1, 2)), ('Dropout operation', (5, 7)), ('dropout rate', (8, 10)), ('0.5', (11, 12)), ('word embeddings', (14, 16)), ('0.2', (17, 18)), ('BLSTM layer', (20, 22)), ('0.4', (23, 24)), ('penultimate layer', (26, 28)), ('l 2 penalty', (32, 35)), ('coefficient 10 ? 5', (36, 40)), ('parameters', (42, 43))]","[['0.5', 'For', 'word embeddings'], ['0.2', 'For', 'BLSTM layer'], ['regularization', 'employ', 'Dropout operation'], ['Dropout operation', 'with', 'dropout rate'], ['dropout rate', 'of', '0.5'], ['0.5', 'for', 'word embeddings'], ['0.2', 'for', 'BLSTM layer'], ['0.2', 'for', '0.4'], ['0.4', 'for', 'penultimate layer'], ['l 2 penalty', 'with', 'coefficient 10 ? 5'], ['coefficient 10 ? 5', 'over', 'parameters']]",[],"[['Hyperparameters', 'For', 'regularization']]",[],document_classification,19,173
733,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],document_classification,19,176
734,results,The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,"[('achieves', (5, 6)), ('on', (8, 9))]","[('BLSTM - 2DCNN model', (1, 5)), ('excellent performance', (6, 8)), ('4 out of 6 tasks', (9, 14))]","[['BLSTM - 2DCNN model', 'achieves', 'excellent performance'], ['excellent performance', 'on', '4 out of 6 tasks']]",[],[],"[['Results', 'has', 'BLSTM - 2DCNN model']]",document_classification,19,179
735,results,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .","[('on', (11, 12))]","[('52.4 % and 89.5 %', (4, 9)), ('test accuracies', (9, 11)), ('SST - 1', (12, 15)), ('SST - 2', (16, 19))]","[['test accuracies', 'on', 'SST - 1'], ['test accuracies', 'on', 'SST - 2']]","[['52.4 % and 89.5 %', 'has', 'test accuracies']]",[],[],document_classification,19,180
736,results,BLSTM - 2DPooling performs worse than the state - of - the - art models .,"[('performs', (3, 4)), ('than', (5, 6))]","[('BLSTM - 2DPooling', (0, 3)), ('worse', (4, 5)), ('state - of - the - art models', (7, 15))]","[['BLSTM - 2DPooling', 'performs', 'worse'], ['worse', 'than', 'state - of - the - art models']]",[],[],"[['Results', 'has', 'BLSTM - 2DPooling']]",document_classification,19,181
737,results,"BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .","[('beats', (3, 4)), ('on', (6, 7))]","[('BLSTM - CNN', (0, 3)), ('all baselines', (4, 6)), ('SST - 1', (7, 10)), ('SST - 2', (11, 14)), ('TREC datasets', (16, 18))]","[['BLSTM - CNN', 'beats', 'all baselines'], ['all baselines', 'on', 'SST - 1'], ['all baselines', 'on', 'TREC datasets']]",[],[],"[['Results', 'has', 'BLSTM - CNN']]",document_classification,19,183
738,results,"As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .","[('for', (1, 2)), ('gets', (10, 11))]","[('Subj and MR datasets', (2, 6)), ('BLSTM - 2DCNN', (7, 10)), ('second higher accuracies', (12, 15))]","[['BLSTM - 2DCNN', 'gets', 'second higher accuracies']]","[['Subj and MR datasets', 'has', 'BLSTM - 2DCNN'], ['BLSTM - 2DCNN', 'has', 'second higher accuracies']]","[['Results', 'for', 'Subj and MR datasets']]",[],document_classification,19,184
739,results,"Compared with RCNN , BLSTM - 2DCNN achieves a comparable result .","[('Compared with', (0, 2)), ('achieves', (7, 8))]","[('RCNN', (2, 3)), ('BLSTM - 2DCNN', (4, 7)), ('comparable result', (9, 11))]","[['BLSTM - 2DCNN', 'achieves', 'comparable result']]","[['RCNN', 'has', 'BLSTM - 2DCNN']]","[['Results', 'Compared with', 'RCNN']]",[],document_classification,19,188
740,results,"Compared with ReNN , the proposed two models do not depend on external language - specific features such as dependency parse trees .","[('on', (11, 12)), ('such as', (17, 19))]","[('ReNN', (2, 3)), ('proposed two models', (5, 8)), ('do not depend', (8, 11)), ('external language - specific features', (12, 17)), ('dependency parse trees', (19, 22))]","[['do not depend', 'on', 'external language - specific features'], ['external language - specific features', 'such as', 'dependency parse trees']]","[['ReNN', 'has', 'proposed two models'], ['proposed two models', 'has', 'do not depend']]",[],[],document_classification,19,190
741,results,"Compared with DSCNN , BLSTM - 2DCNN outperforms it on five datasets .","[('on', (9, 10))]","[('DSCNN', (2, 3)), ('BLSTM - 2DCNN', (4, 7)), ('outperforms', (7, 8)), ('five datasets', (10, 12))]","[['BLSTM - 2DCNN', 'on', 'five datasets'], ['outperforms', 'on', 'five datasets']]","[['DSCNN', 'has', 'BLSTM - 2DCNN'], ['BLSTM - 2DCNN', 'has', 'outperforms'], ['outperforms', 'has', 'five datasets']]",[],[],document_classification,19,194
742,research-problem,Rethinking Complex Neural Network Architectures for Document Classification,[],"[('Document Classification', (6, 8))]",[],[],[],[],document_classification,2,2
743,approach,"Like the papers cited above , we question the need for overly complex neural architectures , focusing on the problem of document classification .","[('question', (7, 8)), ('focusing on', (16, 18))]","[('overly complex neural architectures', (11, 15)), ('document classification', (21, 23))]",[],[],"[['Approach', 'question', 'overly complex neural architectures']]",[],document_classification,2,21
744,approach,"Starting with a large - scale reproducibility study of several recent neural models , we find that a simple bi-directional LSTM ( BiLSTM ) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .","[('Starting with', (0, 2)), ('of', (8, 9)), ('find that', (15, 17)), ('with', (25, 26)), ('yields', (28, 29)), ('competitive or exceed', (36, 39)), ('on', (44, 45))]","[('large - scale reproducibility study', (3, 8)), ('several recent neural models', (9, 13)), ('simple bi-directional LSTM ( BiLSTM ) architecture', (18, 25)), ('appropriate regularization', (26, 28)), ('accuracy and F 1', (29, 33)), ('state of the art', (40, 44)), ('four standard benchmark datasets', (45, 49))]","[['large - scale reproducibility study', 'of', 'several recent neural models'], ['several recent neural models', 'find that', 'simple bi-directional LSTM ( BiLSTM ) architecture'], ['simple bi-directional LSTM ( BiLSTM ) architecture', 'with', 'appropriate regularization'], ['appropriate regularization', 'yields', 'accuracy and F 1'], ['accuracy and F 1', 'competitive or exceed', 'state of the art'], ['state of the art', 'on', 'four standard benchmark datasets']]",[],"[['Approach', 'Starting with', 'large - scale reproducibility study']]",[],document_classification,2,22
745,baselines,"We conduct a large - scale reproducibility study involving HAN , XML - CNN , KimCNN , and SGM .","[('conduct', (1, 2)), ('involving', (8, 9))]","[('large - scale reproducibility study', (3, 8)), ('HAN , XML - CNN , KimCNN , and SGM', (9, 19))]","[['large - scale reproducibility study', 'involving', 'HAN , XML - CNN , KimCNN , and SGM']]",[],"[['Baselines', 'conduct', 'large - scale reproducibility study']]",[],document_classification,2,60
746,experimental-setup,"In addition , we compare the neural approaches to logistic regression ( LR ) and support vector machines ( SVMs ) .","[('compare', (4, 5)), ('to', (8, 9))]","[('neural approaches', (6, 8)), ('logistic regression ( LR )', (9, 14)), ('support vector machines ( SVMs )', (15, 21))]","[['neural approaches', 'to', 'logistic regression ( LR )'], ['neural approaches', 'to', 'support vector machines ( SVMs )']]",[],"[['Experimental setup', 'compare', 'neural approaches']]",[],document_classification,2,64
747,experimental-setup,"The LR model is trained using a one - vs - rest multi-label objective , while the SVM is trained with a linear kernel .","[('trained using', (4, 6)), ('trained with', (19, 21))]","[('LR model', (1, 3)), ('one - vs - rest multi-label objective', (7, 14)), ('SVM', (17, 18)), ('linear kernel', (22, 24))]","[['LR model', 'trained using', 'one - vs - rest multi-label objective'], ['SVM', 'trained with', 'linear kernel']]",[],[],"[['Experimental setup', 'has', 'LR model']]",document_classification,2,65
748,experimental-setup,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .","[('performed on', (5, 7)), ('with', (16, 17)), ('as', (19, 20))]","[('Nvidia GTX 1080 and RTX 2080 Ti GPUs', (7, 15)), ('PyTorch 0.4.1', (17, 19)), ('backend framework', (21, 23))]","[['Nvidia GTX 1080 and RTX 2080 Ti GPUs', 'with', 'PyTorch 0.4.1'], ['PyTorch 0.4.1', 'as', 'backend framework']]",[],"[['Experimental setup', 'performed on', 'Nvidia GTX 1080 and RTX 2080 Ti GPUs']]",[],document_classification,2,67
749,experimental-setup,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,"[('use', (1, 2)), ('for computing', (4, 6)), ('implementing', (12, 13))]","[('Scikitlearn 0.19.2', (2, 4)), ('tf - idf vectors', (7, 11)), ('LR and SVMs', (13, 16))]","[['Scikitlearn 0.19.2', 'for computing', 'tf - idf vectors'], ['Scikitlearn 0.19.2', 'implementing', 'LR and SVMs']]",[],"[['Experimental setup', 'use', 'Scikitlearn 0.19.2']]",[],document_classification,2,68
750,results,"We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .","[('see that', (1, 3)), ('achieves', (8, 9)), ('on', (13, 14)), ('establishing', (26, 27)), ('of', (29, 30)), ('for', (33, 34)), ('on', (39, 40)), ('of', (43, 44))]","[('our simple LSTM reg model', (3, 8)), ('state of the art', (9, 13)), ('Reuters and IMDB', (14, 17)), ('mean scores', (27, 29)), ('87.0 and 52.8', (30, 33)), ('F 1 score and accuracy', (34, 39)), ('test sets', (41, 43)), ('Reuters and IMDB', (44, 47))]","[['our simple LSTM reg model', 'achieves', 'state of the art'], ['state of the art', 'on', 'Reuters and IMDB'], ['state of the art', 'establishing', 'mean scores'], ['mean scores', 'of', '87.0 and 52.8'], ['87.0 and 52.8', 'for', 'F 1 score and accuracy'], ['F 1 score and accuracy', 'on', 'test sets'], ['test sets', 'of', 'Reuters and IMDB']]",[],"[['Results', 'see that', 'our simple LSTM reg model']]",[],document_classification,2,111
751,results,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .","[('observe', (1, 2)), ('upon', (7, 8)), ('of', (10, 11)), ('across', (13, 14))]","[('LSTM reg', (3, 5)), ('consistently improves', (5, 7)), ('performance', (9, 10)), ('LSTM base', (11, 13)), ('all of the tasks', (14, 18))]","[['consistently improves', 'upon', 'performance'], ['performance', 'of', 'LSTM base'], ['LSTM base', 'across', 'all of the tasks']]","[['LSTM reg', 'has', 'consistently improves']]","[['Results', 'observe', 'LSTM reg']]",[],document_classification,2,113
752,results,A few of our LSTM reg runs attain state - of - theart test F 1 scores on AAPD .,"[('attain', (7, 8)), ('on', (17, 18))]","[('few of our LSTM reg runs', (1, 7)), ('state - of - theart test F 1 scores', (8, 17)), ('AAPD', (18, 19))]","[['few of our LSTM reg runs', 'attain', 'state - of - theart test F 1 scores'], ['state - of - theart test F 1 scores', 'on', 'AAPD']]",[],[],[],document_classification,2,114
753,results,"Interestingly , the non-neural LR and SVM baselines perform remarkably well .","[('perform', (8, 9))]","[('non-neural LR and SVM baselines', (3, 8)), ('remarkably well', (9, 11))]","[['non-neural LR and SVM baselines', 'perform', 'remarkably well']]",[],[],"[['Results', 'has', 'non-neural LR and SVM baselines']]",document_classification,2,119
754,results,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .","[('On', (0, 1)), ('beats', (8, 9)), ('including', (13, 14))]","[('Reuters', (1, 2)), ('SVM', (7, 8)), ('many neural baselines', (9, 12)), ('our non-regularized LSTM base', (14, 18))]","[['SVM', 'beats', 'many neural baselines'], ['many neural baselines', 'including', 'our non-regularized LSTM base']]","[['Reuters', 'has', 'SVM']]",[],[],document_classification,2,120
755,results,"On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .","[('ties or beats', (6, 9)), ('losing only to', (13, 16))]","[('AAPD', (1, 2)), ('SVM', (4, 5)), ('other models', (10, 12)), ('SGM', (16, 17))]","[['SVM', 'ties or beats', 'other models']]","[['AAPD', 'has', 'SVM']]",[],[],document_classification,2,121
756,results,"Compared to the SVM , the LR baseline appears better suited for the single - label datasets IMDB and Yelp 2014 , where it achieves better accuracy than the SVM does .","[('Compared to', (0, 2)), ('appears', (8, 9)), ('for', (11, 12)), ('achieves', (24, 25)), ('than', (27, 28))]","[('SVM', (3, 4)), ('LR baseline', (6, 8)), ('better suited', (9, 11)), ('single - label datasets IMDB and Yelp 2014', (13, 21)), ('better accuracy', (25, 27)), ('SVM', (29, 30))]","[['LR baseline', 'appears', 'better suited'], ['better suited', 'for', 'single - label datasets IMDB and Yelp 2014'], ['LR baseline', 'achieves', 'better accuracy'], ['better accuracy', 'than', 'SVM']]","[['SVM', 'has', 'LR baseline'], ['LR baseline', 'has', 'better suited']]","[['Results', 'Compared to', 'SVM']]",[],document_classification,2,122
757,research-problem,Practical Text Classification With Large Pre-Trained Language Models,[],"[('Practical Text Classification', (0, 3))]",[],[],[],[],document_classification,20,2
758,approach,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .","[('train', (5, 6)), ('on', (12, 13)), ('transfer', (21, 22)), ('to', (24, 25)), ('based on', (42, 44))]","[('both mLSTM and Transformer language models', (6, 12)), ('large 40 GB text dataset', (14, 19)), ('those models', (22, 24)), ('two text classification problems', (25, 29)), ('binary sentiment', (30, 32)), ('multidimensional emotion classification', (39, 42)), ('Plutchik wheel of emotions', (45, 49))]","[['both mLSTM and Transformer language models', 'on', 'large 40 GB text dataset'], ['both mLSTM and Transformer language models', 'transfer', 'those models'], ['those models', 'to', 'two text classification problems'], ['multidimensional emotion classification', 'based on', 'Plutchik wheel of emotions']]","[['two text classification problems', 'name', 'binary sentiment']]","[['Approach', 'train', 'both mLSTM and Transformer language models']]",[],document_classification,20,17
759,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],document_classification,20,139
760,results,Binary Sentiment Tweets,[],"[('Binary Sentiment Tweets', (0, 3))]",[],[],[],"[['Results', 'has', 'Binary Sentiment Tweets']]",document_classification,20,140
761,results,"While the Transformer gets close but does not exceed the state of the art on the SST dataset , it exceeds both the mL - STM and ELMo baseline as well as both Watson and Google Sentiment APIs on the company tweets .","[('gets', (3, 4)), ('close but does not exceed', (4, 9)), ('on', (14, 15)), ('exceeds', (20, 21)), ('on', (38, 39))]","[('Transformer', (2, 3)), ('state of the art', (10, 14)), ('SST dataset', (16, 18)), ('mL - STM and ELMo baseline', (23, 29)), ('Watson and Google Sentiment APIs', (33, 38)), ('company tweets', (40, 42))]","[['Transformer', 'close but does not exceed', 'state of the art'], ['state of the art', 'on', 'SST dataset'], ['Watson and Google Sentiment APIs', 'on', 'company tweets'], ['state of the art', 'exceeds', 'mL - STM and ELMo baseline'], ['Watson and Google Sentiment APIs', 'on', 'company tweets']]","[['Transformer', 'has', 'state of the art']]",[],"[['Results', 'has', 'Transformer']]",document_classification,20,143
762,results,Multi - Label Emotion Tweets,[],"[('Multi - Label Emotion Tweets', (0, 5))]",[],[],[],"[['Results', 'has', 'Multi - Label Emotion Tweets']]",document_classification,20,145
763,results,We find that our models outperform Watson on every emotion category .,"[('on', (7, 8))]","[('our models', (3, 5)), ('outperform', (5, 6)), ('Watson', (6, 7)), ('every emotion category', (8, 11))]","[['outperform', 'on', 'every emotion category'], ['Watson', 'on', 'every emotion category']]","[['our models', 'has', 'outperform'], ['outperform', 'has', 'Watson']]",[],[],document_classification,20,148
764,results,Sem Eval Tweets,[],"[('Sem Eval Tweets', (0, 3))]",[],[],[],"[['Results', 'has', 'Sem Eval Tweets']]",document_classification,20,149
765,results,"Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .","[('achieved', (2, 3)), ('among', (8, 9)), ('with', (12, 13)), ('for', (17, 18))]","[('Our model', (0, 2)), ('top macro-averaged F1 score', (4, 8)), ('all submission', (9, 11)), ('lower scores', (15, 17)), ('micro -average F1', (19, 22))]","[['Our model', 'achieved', 'top macro-averaged F1 score'], ['top macro-averaged F1 score', 'among', 'all submission'], ['lower scores', 'for', 'micro -average F1']]",[],[],"[['Results', 'has', 'Our model']]",document_classification,20,152
766,results,We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .,"[('find', (18, 19)), ('outperforms', (22, 23)), ('across', (26, 27))]","[('Transformer', (9, 10)), ('m LSTM', (11, 13)), ('Plutchik categories', (27, 29))]",[],[],[],[],document_classification,20,154
767,results,Our models gets lower F 1 scores on the company tweets dataset than on equivalent Se -m Eval categories .,"[('gets', (2, 3)), ('on', (7, 8)), ('than on', (12, 14))]","[('Our models', (0, 2)), ('lower', (3, 4)), ('F 1 scores', (4, 7)), ('company tweets dataset', (9, 12)), ('equivalent Se -m Eval categories', (14, 19))]","[['Our models', 'gets', 'lower'], ['F 1 scores', 'on', 'company tweets dataset'], ['F 1 scores', 'than on', 'equivalent Se -m Eval categories']]","[['Our models', 'has', 'lower'], ['lower', 'has', 'F 1 scores']]",[],"[['Results', 'has', 'Our models']]",document_classification,20,162
768,research-problem,Squeezed Very Deep Convolutional Neural Networks for Text Classification,[],"[('Text Classification', (7, 9))]",[],[],[],[],document_classification,3,2
769,model,"In this paper , we investigate modifications on the network proposed by Conneau et al. with the aim of reducing its number of parameters , storage size and latency with minimal performance degradation .","[('investigate', (5, 6)), ('on', (7, 8))]","[('modifications', (6, 7)), ('network proposed by Conneau et al.', (9, 15)), ('aim of reducing', (17, 20)), ('number of parameters , storage size and latency', (21, 29))]","[['modifications', 'on', 'network proposed by Conneau et al.']]","[['aim of reducing', 'has', 'number of parameters , storage size and latency']]","[['Model', 'investigate', 'modifications']]",[],document_classification,3,30
770,model,To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques .,"[('used', (5, 6))]","[('Temporal Depthwise Separable Convolution', (6, 10)), ('Global Average Pooling techniques', (11, 15))]",[],[],"[['Model', 'used', 'Temporal Depthwise Separable Convolution']]",[],document_classification,3,31
771,model,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .","[('propose', (7, 8)), ('requires', (24, 25)), ('compared to', (28, 30))]","[('Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )', (9, 18)), ('significantly fewer parameters', (25, 28)), ('stateof - the - art CNNs', (31, 37))]","[['significantly fewer parameters', 'compared to', 'stateof - the - art CNNs']]",[],"[['Model', 'propose', 'Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )']]",[],document_classification,3,32
772,experimental-setup,"For SVDCNN and Char - CNN , we calculated the abovementioned number from the network architecture implemented in PyTorch .","[('For', (0, 1)), ('implemented in', (16, 18))]","[('SVDCNN and Char - CNN', (1, 6)), ('network architecture', (14, 16)), ('PyTorch', (18, 19))]","[['network architecture', 'implemented in', 'PyTorch']]",[],"[['Experimental setup', 'For', 'SVDCNN and Char - CNN']]",[],document_classification,3,141
773,experimental-setup,"The SVDCNN experimental settings are similar to the original VDCNN paper , using the same dictionary and the same embedding size of 16 .","[('similar to', (5, 7)), ('using', (12, 13)), ('of', (21, 22))]","[('SVDCNN experimental settings', (1, 4)), ('original VDCNN paper', (8, 11)), ('same dictionary', (14, 16)), ('same embedding size', (18, 21)), ('16', (22, 23))]","[['SVDCNN experimental settings', 'similar to', 'original VDCNN paper'], ['SVDCNN experimental settings', 'using', 'same dictionary'], ['SVDCNN experimental settings', 'using', 'same embedding size'], ['original VDCNN paper', 'using', 'same dictionary'], ['same embedding size', 'of', '16']]","[['SVDCNN experimental settings', 'has', 'original VDCNN paper']]",[],"[['Experimental setup', 'has', 'SVDCNN experimental settings']]",document_classification,3,146
774,experimental-setup,"The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .","[('performed with', (4, 6)), ('utilizing', (8, 9)), ('of', (11, 12)), ('with', (14, 15)), ('of', (17, 18))]","[('training', (1, 2)), ('SGD', (6, 7)), ('size batch', (9, 11)), ('64', (12, 13)), ('maximum', (16, 17)), ('100 epochs', (18, 20))]","[['training', 'performed with', 'SGD'], ['SGD', 'utilizing', 'size batch'], ['size batch', 'of', '64'], ['maximum', 'of', '100 epochs'], ['size batch', 'with', 'maximum'], ['64', 'with', 'maximum'], ['maximum', 'of', '100 epochs']]","[['size batch', 'has', '64']]",[],"[['Experimental setup', 'has', 'training']]",document_classification,3,147
775,experimental-setup,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .","[('use', (1, 2)), ('of', (6, 7)), ('of', (11, 12)), ('of', (17, 18))]","[('initial learning rate', (3, 6)), ('0.01', (7, 8)), ('momentum', (10, 11)), ('0.9', (12, 13)), ('weight decay', (15, 17)), ('0.001', (18, 19))]","[['initial learning rate', 'of', '0.01'], ['momentum', 'of', '0.9'], ['weight decay', 'of', '0.001'], ['momentum', 'of', '0.9'], ['weight decay', 'of', '0.001'], ['weight decay', 'of', '0.001']]",[],"[['Experimental setup', 'use', 'initial learning rate']]",[],document_classification,3,148
776,experimental-setup,All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,"[('performed on', (4, 6))]","[('NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU', (7, 18))]",[],[],"[['Experimental setup', 'performed on', 'NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU']]",[],document_classification,3,149
777,results,The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks .,"[('obtained by', (3, 5))]","[('network reduction', (1, 3)), ('GAP', (6, 7)), ('FC layers', (17, 19))]","[['network reduction', 'obtained by', 'GAP']]","[['network reduction', 'has', 'GAP']]",[],"[['Results', 'has', 'network reduction']]",document_classification,3,158
778,results,"Considering a dataset with four target classes , and comparing SVDCNN with VDCNN , the number of parameters of the FC layers has passed from 12.59 to 0.02 million parameters , representing a reduction of 99.84 % .","[('Considering', (0, 1)), ('with', (3, 4)), ('comparing', (9, 10)), ('of', (18, 19)), ('passed from', (23, 25)), ('to', (26, 27)), ('representing', (31, 32)), ('of', (34, 35))]","[('dataset', (2, 3)), ('four target classes', (4, 7)), ('SVDCNN with VDCNN', (10, 13)), ('number of parameters', (15, 18)), ('12.59', (25, 26)), ('0.02 million parameters', (27, 30)), ('reduction', (33, 34)), ('99.84 %', (35, 37))]","[['dataset', 'with', 'four target classes'], ['number of parameters', 'passed from', '12.59'], ['12.59', 'to', '0.02 million parameters'], ['0.02 million parameters', 'representing', 'reduction'], ['reduction', 'of', '99.84 %']]","[['dataset', 'has', 'four target classes'], ['SVDCNN with VDCNN', 'has', 'number of parameters']]","[['Results', 'Considering', 'dataset']]",[],document_classification,3,159
779,results,"Following with the same comparison , but to Char - CNN , the proposed model is 99.82 % smaller , 0.02 against 11.36 million of FC parameters .","[('is', (15, 16))]","[('Char - CNN', (8, 11)), ('proposed model', (13, 15)), ('99.82 % smaller', (16, 19))]","[['proposed model', 'is', '99.82 % smaller']]","[['Char - CNN', 'has', 'proposed model'], ['proposed model', 'has', '99.82 % smaller']]",[],[],document_classification,3,160
780,results,"While our most in - depth model ( 29 ) occupies only 6 MB , VDCNN with the same depth occupies 64. 16 MB of storage .","[('occupies only', (10, 12))]","[('most in - depth model', (2, 7)), ('6 MB', (12, 14))]","[['most in - depth model', 'occupies only', '6 MB']]",[],[],"[['Results', 'has', 'most in - depth model']]",document_classification,3,162
781,results,"Regarding accuracy results , usually , a model with such parameter reduction should present some loss of accuracy in comparison to the original model .","[('Regarding', (0, 1))]","[('accuracy results', (1, 3))]",[],[],"[['Results', 'Regarding', 'accuracy results']]",[],document_classification,3,166
782,results,"Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .","[('between', (5, 6)), ('varies between', (10, 12))]","[('performance difference', (3, 5)), ('VDCNN and SVDCNN models', (6, 10)), ('0.4 and 1.3 %', (12, 16))]","[['performance difference', 'between', 'VDCNN and SVDCNN models'], ['performance difference', 'varies between', '0.4 and 1.3 %'], ['VDCNN and SVDCNN models', 'varies between', '0.4 and 1.3 %']]",[],[],"[['Results', 'has', 'performance difference']]",document_classification,3,167
783,research-problem,Joint Embedding of Words and Labels for Text Classification,[],"[('Text Classification', (7, 9))]",[],[],[],[],document_classification,4,2
784,model,"Our primary contribution is therefore to propose such a solution by making use of the label embedding framework , and propose the Label - Embedding Attentive Model ( LEAM ) to improve text classification .","[('propose', (6, 7)), ('making use of', (11, 14)), ('to improve', (30, 32))]","[('label embedding framework', (15, 18)), ('Label - Embedding Attentive Model ( LEAM )', (22, 30)), ('text classification', (32, 34))]","[['Label - Embedding Attentive Model ( LEAM )', 'to improve', 'text classification']]",[],"[['Model', 'propose', 'label embedding framework']]",[],document_classification,4,35
785,model,"The proposed LEAM is implemented by jointly embedding the word and label in the same latent space , and the text representations are constructed directly using the text - label compatibility .","[('implemented by', (4, 6)), ('in', (12, 13)), ('constructed directly using', (23, 26))]","[('proposed LEAM', (1, 3)), ('jointly embedding', (6, 8)), ('word and label', (9, 12)), ('same latent space', (14, 17)), ('text representations', (20, 22)), ('text - label compatibility', (27, 31))]","[['proposed LEAM', 'implemented by', 'jointly embedding'], ['word and label', 'in', 'same latent space'], ['text representations', 'constructed directly using', 'text - label compatibility']]","[['jointly embedding', 'has', 'word and label']]",[],"[['Model', 'has', 'proposed LEAM']]",document_classification,4,37
786,model,"Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .","[('informative for', (19, 21))]","[('label embedding framework', (1, 4)), ('Label - attentive text representation', (13, 18)), ('downstream classification task', (22, 25))]","[['Label - attentive text representation', 'informative for', 'downstream classification task']]","[['label embedding framework', 'has', 'Label - attentive text representation']]",[],"[['Model', 'has', 'label embedding framework']]",document_classification,4,38
787,model,"( ii ) The LEAM learning procedure only involves a series of basic algebraic operations , and hence it retains the interpretability of simple models , especially when the label description is available .","[('involves', (8, 9))]","[('LEAM learning procedure', (4, 7)), ('series of basic algebraic operations', (10, 15))]","[['LEAM learning procedure', 'involves', 'series of basic algebraic operations']]",[],[],"[['Model', 'has', 'LEAM learning procedure']]",document_classification,4,39
788,experimental-setup,Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model .,"[('use', (2, 3)), ('for', (12, 13))]","[('300 - dimensional Glo Ve word embeddings', (3, 10)), ('word embeddings and label embeddings', (13, 18))]",[],[],"[['Experimental setup', 'use', '300 - dimensional Glo Ve word embeddings']]",[],document_classification,4,199
789,experimental-setup,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .","[('initialized from', (10, 12)), ('with', (15, 16))]","[('Out - Of - Vocabulary ( OOV ) words', (0, 9)), ('uniform distribution', (13, 15)), ('range [ ? 0.01 , 0.01 ]', (16, 23))]","[['Out - Of - Vocabulary ( OOV ) words', 'initialized from', 'uniform distribution'], ['uniform distribution', 'with', 'range [ ? 0.01 , 0.01 ]']]",[],[],"[['Experimental setup', 'has', 'Out - Of - Vocabulary ( OOV ) words']]",document_classification,4,200
790,baselines,The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task .,"[('implemented as', (4, 6)), ('followed by', (9, 11))]","[('final classifier', (1, 3)), ('MLP layer', (7, 9)), ('sigmoid or softmax function', (12, 16))]","[['final classifier', 'implemented as', 'MLP layer'], ['MLP layer', 'followed by', 'sigmoid or softmax function']]",[],[],"[['Baselines', 'has', 'final classifier']]",document_classification,4,201
791,experimental-setup,"We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .","[('train', (1, 2)), ('with', (6, 7)), ('with', (18, 19)), ('of', (23, 24)), ('of', (30, 31))]","[(""our model 's parameters"", (2, 6)), ('Adam Optimizer ( Kingma and Ba , 2014 )', (8, 17)), ('initial learning rate', (20, 23)), ('0.001', (24, 25)), ('minibatch size', (28, 30)), ('100', (31, 32))]","[[""our model 's parameters"", 'with', 'Adam Optimizer ( Kingma and Ba , 2014 )'], [""our model 's parameters"", 'with', 'initial learning rate'], [""our model 's parameters"", 'with', 'minibatch size'], [""our model 's parameters"", 'with', 'initial learning rate'], [""our model 's parameters"", 'with', 'minibatch size'], ['initial learning rate', 'of', '0.001'], ['minibatch size', 'of', '100']]","[['initial learning rate', 'has', '0.001'], ['minibatch size', 'has', '100']]","[['Experimental setup', 'train', ""our model 's parameters""]]",[],document_classification,4,202
792,experimental-setup,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .","[('employed on', (3, 5)), ('with', (10, 11))]","[('Dropout regularization', (0, 2)), ('final MLP layer', (6, 9)), ('dropout rate', (11, 13)), ('0.5', (13, 14))]","[['Dropout regularization', 'employed on', 'final MLP layer'], ['Dropout regularization', 'with', 'dropout rate'], ['final MLP layer', 'with', 'dropout rate']]","[['dropout rate', 'has', '0.5']]",[],"[['Experimental setup', 'has', 'Dropout regularization']]",document_classification,4,203
793,experimental-setup,The model is implemented using Tensorflow and is trained on GPU Titan X .,"[('implemented using', (3, 5)), ('trained on', (8, 10))]","[('model', (1, 2)), ('Tensorflow', (5, 6)), ('GPU Titan X', (10, 13))]","[['model', 'implemented using', 'Tensorflow'], ['model', 'trained on', 'GPU Titan X']]",[],[],"[['Experimental setup', 'has', 'model']]",document_classification,4,204
794,code,The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,[],"[('https://github.com/guoyinwang/LEAM', (9, 10))]",[],[],[],[],document_classification,4,205
795,baselines,"We compare against the three baselines : a logistic regression model with bag - ofwords , a bidirectional gated recurrent unit ( Bi - GRU ) and a single - layer 1 D convolutional network .","[('compare against', (1, 3)), ('with', (11, 12))]","[('logistic regression model', (8, 11)), ('bag - ofwords', (12, 15)), ('bidirectional gated recurrent unit ( Bi - GRU )', (17, 26)), ('single - layer 1 D convolutional network', (28, 35))]","[['logistic regression model', 'with', 'bag - ofwords'], ['logistic regression model', 'with', 'single - layer 1 D convolutional network']]",[],"[['Baselines', 'compare against', 'logistic regression model']]",[],document_classification,4,249
796,baselines,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .","[('compare with', (2, 4)), ('for', (7, 8))]","[('multi-label classification of clinical text', (8, 13)), ('Condensed Memory Networks ( C - MemNN )', (15, 23)), ('Attentive LSTM', (24, 26)), ('Convolutional Attention ( CAML )', (27, 32))]",[],[],"[['Baselines', 'compare with', 'multi-label classification of clinical text']]",[],document_classification,4,250
797,results,"LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .","[('provides', (1, 2)), ('than', (13, 14)), ('except', (16, 17))]","[('LEAM', (0, 1)), ('best AUC score', (3, 6)), ('better F1 and P@5 values', (8, 13)), ('all methods', (14, 16)), ('CNN', (17, 18))]","[['LEAM', 'provides', 'best AUC score'], ['better F1 and P@5 values', 'than', 'all methods'], ['all methods', 'except', 'CNN']]","[['LEAM', 'has', 'best AUC score']]",[],"[['Results', 'has', 'LEAM']]",document_classification,4,256
798,results,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .","[('consistently outperforms', (1, 3)), ('performs', (15, 16)), ('than', (17, 18))]","[('CNN', (0, 1)), ('basic Bi - GRU architecture', (4, 9)), ('logistic regression baseline', (12, 15)), ('worse', (16, 17)), ('all deep learning architectures', (18, 22))]","[['CNN', 'consistently outperforms', 'basic Bi - GRU architecture'], ['logistic regression baseline', 'performs', 'worse'], ['worse', 'than', 'all deep learning architectures']]",[],[],[],document_classification,4,257
799,research-problem,HDLTex : Hierarchical Deep Learning for Text Classification,[],"[('Text Classification', (6, 8))]",[],[],[],[],document_classification,5,2
800,research-problem,"Central to these information processing methods is document classification , which has become an important application for supervised learning .",[],"[('document classification', (7, 9))]",[],[],[],[],document_classification,5,5
801,model,Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,[],"[('hierarchical classification', (3, 5))]",[],[],[],[],document_classification,5,9
802,model,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,"[('presents', (2, 3)), ('to', (6, 7)), ('call', (12, 13))]","[('new approach', (4, 6)), ('hierarchical document classification', (7, 10)), ('Hierarchical Deep Learning for Text classification ( HDLTex )', (13, 22))]","[['new approach', 'to', 'hierarchical document classification'], ['hierarchical document classification', 'call', 'Hierarchical Deep Learning for Text classification ( HDLTex )']]",[],"[['Model', 'presents', 'new approach']]",[],document_classification,5,24
803,model,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,"[('combines', (1, 2)), ('to allow', (5, 7)), ('by', (13, 14)), ('of', (15, 16))]","[('HDLTex', (0, 1)), ('deep learning architectures', (2, 5)), ('over all and specialized learning', (8, 13)), ('level', (14, 15)), ('document hierarchy', (17, 19))]","[['HDLTex', 'combines', 'deep learning architectures'], ['deep learning architectures', 'to allow', 'over all and specialized learning'], ['over all and specialized learning', 'by', 'level'], ['level', 'of', 'document hierarchy']]","[['HDLTex', 'name', 'deep learning architectures']]",[],"[['Model', 'has', 'HDLTex']]",document_classification,5,25
804,results,The following results were obtained using a combination of central processing units ( CPUs ) and graphical processing units ( GPUs ) .,"[('using', (5, 6))]","[('central processing units ( CPUs )', (9, 15)), ('graphical processing units ( GPUs )', (16, 22))]",[],[],"[['Results', 'using', 'central processing units ( CPUs )']]",[],document_classification,5,231
805,experimental-setup,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .","[('done on', (3, 5)), ('with', (14, 15)), ('were', (25, 26))]","[('Xeon E5 ? 2640 ( 2.6 GHz )', (6, 14)), ('32 cores', (15, 17)), ('64GB memory', (18, 20)), ('GPU cards', (23, 25)), ('N vidia Quadro K620', (26, 30)), ('N vidia Tesla K20c', (31, 35))]","[['Xeon E5 ? 2640 ( 2.6 GHz )', 'with', '32 cores'], ['Xeon E5 ? 2640 ( 2.6 GHz )', 'with', '64GB memory'], ['GPU cards', 'were', 'N vidia Quadro K620'], ['GPU cards', 'were', 'N vidia Tesla K20c']]","[['GPU cards', 'has', 'N vidia Quadro K620']]","[['Experimental setup', 'done on', 'Xeon E5 ? 2640 ( 2.6 GHz )']]",[],document_classification,5,232
806,experimental-setup,"We implemented our approaches in Python using the Compute Unified Device Architecture ( CUDA ) , which is a parallel computing platform and Application Programming Interface ( API ) model created by N vidia .","[('implemented', (1, 2)), ('in', (4, 5)), ('using', (6, 7)), ('is', (17, 18)), ('created by', (30, 32))]","[('our approaches', (2, 4)), ('Python', (5, 6)), ('Compute Unified Device Architecture ( CUDA )', (8, 15)), ('parallel computing platform and Application Programming Interface ( API ) model', (19, 30)), ('N vidia', (32, 34))]","[['our approaches', 'in', 'Python'], ['our approaches', 'using', 'Compute Unified Device Architecture ( CUDA )'], ['Python', 'using', 'Compute Unified Device Architecture ( CUDA )'], ['Compute Unified Device Architecture ( CUDA )', 'is', 'parallel computing platform and Application Programming Interface ( API ) model'], ['parallel computing platform and Application Programming Interface ( API ) model', 'created by', 'N vidia']]",[],"[['Experimental setup', 'implemented', 'our approaches']]",[],document_classification,5,233
807,experimental-setup,"We also used Keras and Tensor Flow libraries for creating the neural networks , .","[('used', (2, 3)), ('for creating', (8, 10))]","[('Keras and Tensor Flow libraries', (3, 8)), ('neural networks', (11, 13))]","[['Keras and Tensor Flow libraries', 'for creating', 'neural networks']]",[],"[['Experimental setup', 'used', 'Keras and Tensor Flow libraries']]",[],document_classification,5,234
808,baselines,"The baseline tests compare three conventional document classification approaches ( nave Bayes and two versions of SVM ) and stacking SVM with three deep learning approaches ( DNN , RNN , and CNN ) .","[('compare', (3, 4)), ('with', (21, 22))]","[('three conventional document classification approaches', (4, 9)), ('nave Bayes', (10, 12)), ('two versions of SVM', (13, 17)), ('stacking SVM', (19, 21)), ('three deep learning approaches', (22, 26)), ('DNN', (27, 28)), ('RNN', (29, 30)), ('CNN', (32, 33))]","[['stacking SVM', 'with', 'three deep learning approaches']]","[['three conventional document classification approaches', 'name', 'nave Bayes'], ['three deep learning approaches', 'name', 'DNN']]",[],[],document_classification,5,236
809,results,In this set of tests the RNN outperforms the others for all three W OS data sets .,"[('outperforms', (7, 8)), ('for', (10, 11))]","[('RNN', (6, 7)), ('others', (9, 10)), ('all three W OS data sets', (11, 17))]","[['RNN', 'outperforms', 'others'], ['others', 'for', 'all three W OS data sets']]",[],[],[],document_classification,5,237
810,results,CNN performs secondbest for three data sets .,"[('performs', (1, 2)), ('for', (3, 4))]","[('CNN', (0, 1)), ('secondbest', (2, 3)), ('three data sets', (4, 7))]","[['CNN', 'performs', 'secondbest'], ['secondbest', 'for', 'three data sets']]",[],[],"[['Results', 'has', 'CNN']]",document_classification,5,238
811,results,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,"[('is', (4, 5)), ('for', (6, 7))]","[('SVM with term weighting', (0, 4)), ('third', (5, 6)), ('first two sets', (8, 11))]","[['SVM with term weighting', 'is', 'third'], ['third', 'for', 'first two sets']]","[['SVM with term weighting', 'has', 'third']]",[],"[['Results', 'has', 'SVM with term weighting']]",document_classification,5,239
812,results,"Overall , nave Bayes does much worse than the other methods throughout these tests .","[('does', (4, 5)), ('than', (7, 8))]","[('nave Bayes', (2, 4)), ('much worse', (5, 7)), ('other methods', (9, 11))]","[['nave Bayes', 'does', 'much worse'], ['much worse', 'than', 'other methods']]","[['nave Bayes', 'has', 'much worse']]",[],"[['Results', 'has', 'nave Bayes']]",document_classification,5,242
813,results,"As for the tests of classifying these documents within a hierarchy , the HDLTex approaches with stacked , deep learning architectures clearly provide superior performance .","[('with', (15, 16)), ('provide', (22, 23))]","[('HDLTex approaches', (13, 15)), ('stacked , deep learning architectures', (16, 21)), ('superior performance', (23, 25))]","[['HDLTex approaches', 'with', 'stacked , deep learning architectures'], ['stacked , deep learning architectures', 'provide', 'superior performance']]",[],[],[],document_classification,5,243
814,results,"For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .","[('For', (0, 1)), ('obtained by', (12, 14)), ('for', (17, 18)), ('for', (25, 26))]","[('data set W OS ? 11967', (1, 7)), ('best accuracy', (9, 11)), ('RNN', (16, 17)), ('first level of classification', (19, 23)), ('DNN', (24, 25)), ('second level', (27, 29))]","[['RNN', 'For', 'first level of classification'], ['DNN', 'For', 'second level'], ['RNN', 'for', 'first level of classification'], ['RNN', 'for', 'DNN'], ['DNN', 'for', 'second level'], ['DNN', 'for', 'second level']]","[['data set W OS ? 11967', 'has', 'best accuracy']]","[['Results', 'For', 'data set W OS ? 11967']]",[],document_classification,5,244
815,results,"This gives accuracies of 94 % for the first level , 92 % for the second level and 86 % over all .","[('gives', (1, 2)), ('of', (3, 4)), ('for', (6, 7)), ('for', (13, 14))]","[('accuracies', (2, 3)), ('94 %', (4, 6)), ('first level', (8, 10)), ('92 %', (11, 13)), ('second level', (15, 17)), ('86 %', (18, 20)), ('over all', (20, 22))]","[['accuracies', 'of', '94 %'], ['94 %', 'for', 'first level'], ['94 %', 'for', '86 %'], ['92 %', 'for', 'second level'], ['92 %', 'for', 'second level']]","[['86 %', 'has', 'over all']]",[],[],document_classification,5,245
816,results,For data set W OS ? 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2 .,"[('achieved by', (12, 14)), ('for', (15, 16))]","[('data set W OS ? 46985', (1, 7)), ('best scores', (8, 10)), ('RNN', (14, 15)), ('level one', (16, 18)), ('level 2', (24, 26))]","[['best scores', 'achieved by', 'RNN'], ['RNN', 'for', 'level one']]","[['data set W OS ? 46985', 'has', 'best scores']]",[],[],document_classification,5,247
817,research-problem,Explicit Interaction Model towards Text Classification,[],"[('Text Classification', (4, 6))]",[],[],[],[],document_classification,6,2
818,model,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .","[('introduce', (7, 8)), ('capable of incorporating', (21, 24)), ('for', (30, 31))]","[('interaction mechanism ( Wang and Jiang 2016 b )', (9, 18)), ('word - level matching signals', (25, 30)), ('text classification', (31, 33))]","[['interaction mechanism ( Wang and Jiang 2016 b )', 'capable of incorporating', 'word - level matching signals'], ['word - level matching signals', 'for', 'text classification']]",[],"[['Model', 'introduce', 'interaction mechanism ( Wang and Jiang 2016 b )']]",[],document_classification,6,31
819,model,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .","[('Based upon', (0, 2)), ('devise', (7, 8)), ('dubbed as', (13, 15))]","[('interaction mechanism', (3, 5)), ('EXplicit interAction Model', (9, 12)), ('EXAM', (15, 16))]","[['interaction mechanism', 'devise', 'EXplicit interAction Model'], ['EXplicit interAction Model', 'dubbed as', 'EXAM']]","[['interaction mechanism', 'name', 'EXplicit interAction Model'], ['EXplicit interAction Model', 'name', 'EXAM']]","[['Model', 'Based upon', 'interaction mechanism']]",[],document_classification,6,35
820,model,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .","[('consists of', (5, 7))]","[('three main components', (7, 10)), ('word - level encoder', (11, 15)), ('interaction layer', (16, 18)), ('aggregation layer', (20, 22))]",[],"[['three main components', 'name', 'word - level encoder']]","[['Model', 'consists of', 'three main components']]",[],document_classification,6,36
821,model,The word - level encoder projects the textual contents into the word - level representations .,"[('projects', (5, 6)), ('into', (9, 10))]","[('word - level encoder', (1, 5)), ('textual contents', (7, 9)), ('word - level representations', (11, 15))]","[['word - level encoder', 'projects', 'textual contents'], ['textual contents', 'into', 'word - level representations']]",[],[],"[['Model', 'has', 'word - level encoder']]",document_classification,6,37
822,model,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .","[('calculates', (5, 6)), ('between', (9, 10))]","[('interaction layer', (3, 5)), ('matching scores', (7, 9)), ('words and classes', (11, 14))]","[['interaction layer', 'calculates', 'matching scores'], ['matching scores', 'between', 'words and classes']]",[],[],"[['Model', 'has', 'interaction layer']]",document_classification,6,38
823,model,"Then , the last layer aggregates those matching scores into predictions over each class , respectively .","[('into', (9, 10)), ('over', (11, 12))]","[('last layer', (3, 5)), ('aggregates', (5, 6)), ('matching scores', (7, 9)), ('predictions', (10, 11)), ('each class', (12, 14))]","[['matching scores', 'into', 'predictions'], ['predictions', 'over', 'each class']]","[['last layer', 'has', 'aggregates'], ['aggregates', 'has', 'matching scores']]",[],"[['Model', 'has', 'last layer']]",document_classification,6,39
824,experiments,Experiments,[],"[('Experiments', (0, 1))]",[],[],[],[],document_classification,6,129
825,results,Multi - Class Classification,[],"[('Multi - Class Classification', (0, 4))]",[],[],[],"[['Results', 'has', 'Multi - Class Classification']]",document_classification,6,130
826,experimental-setup,"For the multi -class task , we chose region embedding as the Encoder in EXAM .","[('chose', (7, 8)), ('as', (10, 11)), ('in', (13, 14))]","[('region embedding', (8, 10)), ('Encoder', (12, 13)), ('EXAM', (14, 15))]","[['region embedding', 'as', 'Encoder'], ['Encoder', 'in', 'EXAM']]",[],"[['Experimental setup', 'chose', 'region embedding']]",[],document_classification,6,141
827,experimental-setup,The region size is 7 and embedding size is 128 .,"[('is', (3, 4)), ('is', (8, 9))]","[('region size', (1, 3)), ('7', (4, 5)), ('embedding size', (6, 8)), ('128', (9, 10))]","[['region size', 'is', '7'], ['embedding size', 'is', '128'], ['embedding size', 'is', '128']]","[['region size', 'has', '7'], ['embedding size', 'has', '128']]",[],"[['Experimental setup', 'has', 'region size']]",document_classification,6,142
828,experimental-setup,We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,"[('used', (1, 2)), ('as', (9, 10)), ('with', (12, 13)), ('set to', (23, 25))]","[('adam ( Kingma and Ba 2014 )', (2, 9)), ('optimizer', (11, 12)), ('initial learning rate', (14, 17)), ('0.0001', (17, 18)), ('batch size', (20, 22)), ('16', (25, 26))]","[['adam ( Kingma and Ba 2014 )', 'as', 'optimizer'], ['adam ( Kingma and Ba 2014 )', 'with', 'initial learning rate'], ['optimizer', 'with', 'initial learning rate'], ['batch size', 'set to', '16']]","[['initial learning rate', 'has', '0.0001'], ['batch size', 'has', '16']]","[['Experimental setup', 'used', 'adam ( Kingma and Ba 2014 )']]",[],document_classification,6,143
829,experimental-setup,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .","[('for', (1, 2)), ('set', (7, 8)), ('of', (10, 11)), ('as', (14, 15)), ('times', (16, 17))]","[('aggregation MLP', (3, 5)), ('size', (9, 10)), ('hidden layer', (12, 14)), ('2', (15, 16)), ('interaction feature length', (17, 20))]","[['aggregation MLP', 'set', 'size'], ['size', 'of', 'hidden layer'], ['size', 'as', '2'], ['hidden layer', 'as', '2'], ['2', 'times', 'interaction feature length']]",[],"[['Experimental setup', 'for', 'aggregation MLP']]",[],document_classification,6,144
830,experimental-setup,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,"[('implemented and trained by', (3, 7)), ('with', (13, 14))]","[('Our models', (0, 2)), ('MXNet', (7, 8)), ('single NVIDIA TITAN Xp', (15, 19))]","[['Our models', 'implemented and trained by', 'MXNet'], ['MXNet', 'with', 'single NVIDIA TITAN Xp']]",[],[],"[['Experimental setup', 'has', 'Our models']]",document_classification,6,145
831,experiments,Baselines,[],"[('Baselines', (0, 1))]",[],[],[],[],document_classification,6,146
832,baselines,The baselines are mainly in three variants :,"[('mainly in', (3, 5))]","[('three variants', (5, 7))]",[],[],[],[],document_classification,6,148
833,baselines,1 ) models based on feature engineering ;,"[('based on', (3, 5))]","[('models', (2, 3)), ('feature engineering', (5, 7))]","[['models', 'based on', 'feature engineering']]",[],[],"[['Baselines', 'has', 'models']]",document_classification,6,149
834,baselines,"2 ) Char - based deep models , and 3 ) Word - based deep models .",[],"[('Char - based deep models', (2, 7)), ('Word - based deep models', (11, 16))]",[],[],[],[],document_classification,6,150
835,results,Models based on feature engineering get the worst results on all the five datasets compared to the other methods .,"[('based on', (1, 3)), ('get', (5, 6)), ('on', (9, 10))]","[('Models', (0, 1)), ('feature engineering', (3, 5)), ('worst results', (7, 9)), ('all the five datasets', (10, 14))]","[['Models', 'based on', 'feature engineering'], ['Models', 'get', 'worst results'], ['feature engineering', 'get', 'worst results'], ['worst results', 'on', 'all the five datasets']]",[],[],"[['Results', 'has', 'Models']]",document_classification,6,154
836,results,Char - based models get the highest over all scores on the two Amazon datasets .,"[('get', (4, 5)), ('on', (10, 11))]","[('Char - based models', (0, 4)), ('highest over all scores', (6, 10)), ('two Amazon datasets', (12, 15))]","[['Char - based models', 'get', 'highest over all scores'], ['highest over all scores', 'on', 'two Amazon datasets']]","[['Char - based models', 'has', 'highest over all scores']]",[],"[['Results', 'has', 'Char - based models']]",document_classification,6,156
837,results,Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .,"[('exceed', (4, 5)), ('on', (8, 9)), ('lose on', (12, 14))]","[('Word - based baselines', (0, 4)), ('other variants', (6, 8)), ('three datasets', (9, 11)), ('two Amazon datasets', (15, 18))]","[['Word - based baselines', 'exceed', 'other variants'], ['other variants', 'on', 'three datasets'], ['Word - based baselines', 'lose on', 'two Amazon datasets']]","[['Word - based baselines', 'has', 'other variants']]",[],[],document_classification,6,160
838,results,"For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .","[('For', (0, 1)), ('performs', (8, 9))]","[('five baselines', (2, 4)), ('W.C Region Emb', (5, 8)), ('best', (10, 11))]","[['W.C Region Emb', 'performs', 'best']]","[['five baselines', 'has', 'W.C Region Emb'], ['W.C Region Emb', 'has', 'best']]","[['Results', 'For', 'five baselines']]",[],document_classification,6,162
839,results,"It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .","[('see that', (4, 6)), ('achieves', (7, 8)), ('over', (11, 12))]","[('EXAM', (6, 7)), ('best performance', (9, 11)), ('three datasets', (13, 15)), ('AG', (16, 17)), ('Yah. A.', (18, 20)), ('DBP', (21, 22))]","[['EXAM', 'achieves', 'best performance'], ['best performance', 'over', 'three datasets']]","[['three datasets', 'name', 'AG']]","[['Results', 'see that', 'EXAM']]",[],document_classification,6,163
840,results,"For the Yah.A. , EXAM improves the best performance by 1.1 % .","[('improves', (5, 6)), ('by', (9, 10))]","[('Yah.A.', (2, 3)), ('EXAM', (4, 5)), ('best performance', (7, 9)), ('1.1 %', (10, 12))]","[['EXAM', 'improves', 'best performance'], ['best performance', 'by', '1.1 %']]","[['Yah.A.', 'has', 'EXAM']]",[],[],document_classification,6,164
841,baselines,Multi - Label Classification,[],"[('Multi - Label Classification', (0, 4))]",[],[],[],"[['Baselines', 'has', 'Multi - Label Classification']]",document_classification,6,176
842,experimental-setup,We implemented the baseline models and EXAM by MXNet .,"[('implemented', (1, 2)), ('by', (7, 8))]","[('baseline models and EXAM', (3, 7)), ('MXNet', (8, 9))]","[['baseline models and EXAM', 'by', 'MXNet']]",[],"[['Experimental setup', 'implemented', 'baseline models and EXAM']]",[],document_classification,6,194
843,experimental-setup,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .","[('used', (1, 2)), ('trained by', (4, 6)), ('to initialize', (7, 9)), ('is', (17, 18))]","[('matrix', (3, 4)), ('word2vec', (6, 7)), ('embedding layer', (10, 12)), ('embedding size', (15, 17)), ('256', (18, 19))]","[['matrix', 'trained by', 'word2vec'], ['word2vec', 'to initialize', 'embedding layer'], ['embedding size', 'is', '256']]","[['embedding size', 'has', '256']]","[['Experimental setup', 'used', 'matrix']]",[],document_classification,6,195
844,experimental-setup,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .","[('adopted', (1, 2)), ('as', (3, 4))]","[('GRU', (2, 3)), ('Encoder', (5, 6)), ('each GRU Cell', (8, 11)), ('1,024 hidden states', (12, 15))]","[['GRU', 'as', 'Encoder']]","[['each GRU Cell', 'has', '1,024 hidden states']]","[['Experimental setup', 'adopted', 'GRU']]",[],document_classification,6,196
845,experimental-setup,The accumulated MLP has 60 hidden units .,[],"[('accumulated MLP', (1, 3)), ('60 hidden units', (4, 7))]",[],"[['accumulated MLP', 'has', '60 hidden units']]",[],"[['Experimental setup', 'has', 'accumulated MLP']]",document_classification,6,197
846,experimental-setup,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,"[('applied', (1, 2)), ('to optimize', (3, 5)), ('on', (6, 7)), ('with', (11, 12)), ('of', (15, 16)), ('is', (22, 23))]","[('Adam', (2, 3)), ('models', (5, 6)), ('NVIDIA TITAN Xp', (8, 11)), ('batch size', (13, 15)), ('1000', (16, 17)), ('initial learning rate', (19, 22)), ('0.001', (23, 24))]","[['Adam', 'to optimize', 'models'], ['NVIDIA TITAN Xp', 'with', 'batch size'], ['batch size', 'of', '1000'], ['initial learning rate', 'is', '0.001']]","[['batch size', 'has', '1000'], ['initial learning rate', 'has', '0.001']]","[['Experimental setup', 'applied', 'Adam']]",[],document_classification,6,198
847,experimental-setup,The validation set is applied for early - stopping to avoid overfitting .,"[('applied for', (4, 6)), ('to avoid', (9, 11))]","[('validation set', (1, 3)), ('early - stopping', (6, 9)), ('overfitting', (11, 12))]","[['validation set', 'applied for', 'early - stopping'], ['early - stopping', 'to avoid', 'overfitting']]",[],[],"[['Experimental setup', 'has', 'validation set']]",document_classification,6,199
848,results,Word - based models are better than char - based models in Kanshan - Cup dataset .,"[('better than', (5, 7)), ('in', (11, 12))]","[('Word - based models', (0, 4)), ('char - based models', (7, 11)), ('Kanshan - Cup dataset', (12, 16))]","[['Word - based models', 'better than', 'char - based models'], ['char - based models', 'in', 'Kanshan - Cup dataset']]",[],[],"[['Results', 'has', 'Word - based models']]",document_classification,6,210
849,results,Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .,"[('achieve', (2, 3)), ('over', (12, 13))]","[('Our models', (0, 2)), ('state - of - the - art performance', (4, 12)), ('two different datasets', (13, 16))]","[['Our models', 'achieve', 'state - of - the - art performance'], ['state - of - the - art performance', 'over', 'two different datasets']]",[],[],"[['Results', 'has', 'Our models']]",document_classification,6,213
850,research-problem,A Corpus for Multilingual Document Classification in Eight Languages,[],"[('Multilingual Document Classification', (3, 6))]",[],[],[],[],document_classification,7,2
851,research-problem,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .,[],"[('Cross - lingual document classification', (0, 5))]",[],[],[],[],document_classification,7,4
852,research-problem,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .","[('use', (5, 6)), ('in', (8, 9)), ('to define', (14, 16)), ('for', (23, 24)), ('namely', (29, 30))]","[('data', (7, 8)), ('Reuters Corpus Volume 2', (10, 14)), ('new cross - lingual document classification tasks', (16, 23)), ('eight very different languages', (24, 28)), ('English', (30, 31)), ('French', (32, 33)), ('Spanish', (34, 35)), ('Italian', (36, 37)), ('German', (38, 39)), ('Russian', (40, 41)), ('Chinese', (42, 43)), ('Japanese', (44, 45))]","[['data', 'in', 'Reuters Corpus Volume 2'], ['Reuters Corpus Volume 2', 'to define', 'new cross - lingual document classification tasks'], ['new cross - lingual document classification tasks', 'for', 'eight very different languages'], ['eight very different languages', 'namely', 'English'], ['eight very different languages', 'namely', 'French'], ['eight very different languages', 'namely', 'Spanish'], ['eight very different languages', 'namely', 'Italian'], ['eight very different languages', 'namely', 'German'], ['eight very different languages', 'namely', 'Russian'], ['eight very different languages', 'namely', 'Chinese'], ['eight very different languages', 'namely', 'Japanese']]",[],"[['Research problem', 'use', 'data']]",[],document_classification,7,31
853,model,"For each language , we define a train , development and test corpus .","[('For', (0, 1)), ('define', (5, 6))]","[('each language', (1, 3)), ('train , development and test corpus', (7, 13))]","[['each language', 'define', 'train , development and test corpus']]",[],"[['Model', 'For', 'each language']]",[],document_classification,7,32
854,results,Zero - shot cross - lingual document classification,[],"[('Zero - shot cross - lingual document classification', (0, 8))]",[],[],[],"[['Results', 'has', 'Zero - shot cross - lingual document classification']]",document_classification,7,114
855,results,The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,"[('based on', (2, 4)), ('perform', (7, 8)), ('on', (10, 11))]","[('classifiers', (1, 2)), ('MultiCCA embeddings', (5, 7)), ('very well', (8, 10)), ('development corpus', (12, 14)), ('accuracies', (15, 16)), ('close or exceeding 90 %', (16, 21))]","[['classifiers', 'based on', 'MultiCCA embeddings'], ['classifiers', 'perform', 'very well'], ['MultiCCA embeddings', 'perform', 'very well'], ['very well', 'on', 'development corpus']]","[['accuracies', 'has', 'close or exceeding 90 %']]",[],"[['Results', 'has', 'classifiers']]",document_classification,7,116
856,results,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .","[('trained on', (2, 4)), ('transfered to', (10, 12)), ('for', (19, 20)), ('out of', (21, 23))]","[('system', (1, 2)), ('English', (4, 5)), ('different languages', (13, 15)), ('scores', (17, 18)), ('best', (18, 19)), ('three', (20, 21)), ('seven languages', (23, 25)), ('DE', (26, 27)), ('IT', (28, 29)), ('ZH', (30, 31))]","[['system', 'trained on', 'English'], ['best', 'for', 'three'], ['best', 'for', 'ZH'], ['three', 'out of', 'seven languages'], ['three', 'out of', 'ZH']]","[['scores', 'has', 'best'], ['seven languages', 'name', 'DE']]",[],"[['Results', 'has', 'system']]",document_classification,7,117
857,results,"However , the transfer accuracies are quite low when training the classifiers on other languages than English , in particular for Russian , Chinese and Japanese .","[('when training', (8, 10)), ('on', (12, 13)), ('than', (15, 16)), ('in particular for', (18, 21))]","[('transfer accuracies', (3, 5)), ('quite low', (6, 8)), ('classifiers', (11, 12)), ('other languages', (13, 15)), ('English', (16, 17)), ('Russian', (21, 22)), ('Chinese', (23, 24)), ('Japanese', (25, 26))]","[['quite low', 'when training', 'classifiers'], ['classifiers', 'on', 'other languages'], ['other languages', 'than', 'English'], ['other languages', 'in particular for', 'Russian'], ['other languages', 'in particular for', 'Chinese'], ['other languages', 'in particular for', 'Japanese']]","[['transfer accuracies', 'has', 'quite low']]",[],[],document_classification,7,119
858,results,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,"[('using', (2, 3)), ('seem to be', (6, 9))]","[('systems', (1, 2)), ('multilingual sentence embeddings', (3, 6)), ('more robust', (11, 13)), ('less language specific', (14, 17))]","[['systems', 'using', 'multilingual sentence embeddings'], ['multilingual sentence embeddings', 'seem to be', 'less language specific']]",[],[],[],document_classification,7,120
859,results,Training on German or French actually leads to better transfer performance than training on English .,"[('on', (1, 2)), ('leads to', (6, 8)), ('than', (11, 12)), ('on', (13, 14))]","[('Training', (0, 1)), ('German or French', (2, 5)), ('better transfer performance', (8, 11)), ('training', (12, 13)), ('English', (14, 15))]","[['Training', 'on', 'German or French'], ['training', 'on', 'English'], ['Training', 'leads to', 'better transfer performance'], ['German or French', 'leads to', 'better transfer performance'], ['better transfer performance', 'than', 'training'], ['better transfer performance', 'on', 'English'], ['training', 'on', 'English']]",[],[],"[['Results', 'has', 'Training']]",document_classification,7,122
860,results,Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,"[('between', (2, 3)), ('like', (6, 7)), ('achieves', (11, 12))]","[('Crosslingual transfer', (0, 2)), ('very different languages', (3, 6)), ('Chinese and Russian', (7, 10)), ('remarkable results', (12, 14))]","[['Crosslingual transfer', 'between', 'very different languages'], ['very different languages', 'like', 'Chinese and Russian'], ['Crosslingual transfer', 'achieves', 'remarkable results']]",[],[],[],document_classification,7,123
861,results,Joint multilingual document classification,[],"[('Joint multilingual document classification', (0, 4))]",[],[],[],"[['Results', 'has', 'Joint multilingual document classification']]",document_classification,7,130
862,results,"This leads to important improvement for all languages , in comparison to zero - shot or targeted transfer learning .","[('leads to', (1, 3)), ('for', (5, 6)), ('in comparison to', (9, 12))]","[('important improvement', (3, 5)), ('all languages', (6, 8)), ('zero - shot or targeted transfer learning', (12, 19))]","[['important improvement', 'for', 'all languages'], ['important improvement', 'in comparison to', 'zero - shot or targeted transfer learning'], ['all languages', 'in comparison to', 'zero - shot or targeted transfer learning']]",[],"[['Results', 'leads to', 'important improvement']]",[],document_classification,7,134
863,research-problem,Disconnected Recurrent Neural Networks for Text Categorization,[],"[('Text Categorization', (5, 7))]",[],[],[],[],document_classification,8,2
864,model,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .","[('incorporate', (5, 6)), ('into', (7, 8)), ('propose', (10, 11)), ('named', (14, 15))]","[('positioninvariance', (6, 7)), ('RNN', (8, 9)), ('novel model', (12, 14)), ('Disconnected Recurrent Neural Network ( DRNN )', (15, 22))]","[['positioninvariance', 'into', 'RNN'], ['novel model', 'named', 'Disconnected Recurrent Neural Network ( DRNN )']]","[['novel model', 'name', 'Disconnected Recurrent Neural Network ( DRNN )']]","[['Model', 'incorporate', 'positioninvariance']]",[],document_classification,8,23
865,model,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .","[('maintain', (1, 2)), ('utilize', (8, 9)), ('to extract', (11, 13))]","[('position - invariance', (3, 6)), ('max pooling', (9, 11)), ('important information', (14, 16))]","[['position - invariance', 'utilize', 'max pooling'], ['max pooling', 'to extract', 'important information']]",[],"[['Model', 'maintain', 'position - invariance']]",[],document_classification,8,27
866,model,Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units .,"[('regarded as', (6, 8)), ('where', (12, 13)), ('replaced with', (16, 18))]","[('Our proposed model', (0, 3)), ('special 1D CNN', (9, 12)), ('convolution kernels', (13, 15)), ('recurrent units', (18, 20))]","[['Our proposed model', 'regarded as', 'special 1D CNN'], ['special 1D CNN', 'where', 'convolution kernels'], ['convolution kernels', 'replaced with', 'recurrent units']]",[],[],"[['Model', 'has', 'Our proposed model']]",document_classification,8,28
867,hyperparameters,"We utilize the 300D Glo Ve 840B vectors ( Pennington et al. , 2014 ) as our pre-trained word embeddings .","[('utilize', (1, 2)), ('as', (15, 16))]","[('300D Glo Ve 840B vectors ( Pennington et al. , 2014 )', (3, 15)), ('our pre-trained word embeddings', (16, 20))]","[['300D Glo Ve 840B vectors ( Pennington et al. , 2014 )', 'as', 'our pre-trained word embeddings']]",[],"[['Hyperparameters', 'utilize', '300D Glo Ve 840B vectors ( Pennington et al. , 2014 )']]",[],document_classification,8,156
868,hyperparameters,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .","[('use', (1, 2)), ('to optimize', (8, 10))]","[('Adadelta ( Zeiler , 2012 )', (2, 8)), ('all the trainable parameters', (10, 14))]","[['Adadelta ( Zeiler , 2012 )', 'to optimize', 'all the trainable parameters']]",[],"[['Hyperparameters', 'use', 'Adadelta ( Zeiler , 2012 )']]",[],document_classification,8,159
869,hyperparameters,The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,"[('of', (2, 3)), ('set as', (5, 7))]","[('Adadelta', (3, 4)), ('1 e ? 6', (14, 18))]",[],[],"[['Hyperparameters', 'of', 'Adadelta']]",[],document_classification,8,160
870,hyperparameters,"To avoid the gradient explosion problem , we apply gradient norm clipping .","[('To avoid', (0, 2)), ('apply', (8, 9))]","[('gradient explosion problem', (3, 6)), ('gradient norm clipping', (9, 12))]","[['gradient explosion problem', 'apply', 'gradient norm clipping']]",[],"[['Hyperparameters', 'To avoid', 'gradient explosion problem']]",[],document_classification,8,161
871,experiments,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('128', (6, 7))]","[['batch size', 'set to', '128']]","[['batch size', 'has', '128']]",[],[],document_classification,8,162
872,results,We can see that very deep CNN ( VDCNN ) performs well in large datasets .,"[('see that', (2, 4)), ('performs', (10, 11)), ('in', (12, 13))]","[('very deep CNN ( VDCNN )', (4, 10)), ('well', (11, 12)), ('large datasets', (13, 15))]","[['very deep CNN ( VDCNN )', 'performs', 'well'], ['well', 'in', 'large datasets']]",[],"[['Results', 'see that', 'very deep CNN ( VDCNN )']]",[],document_classification,8,172
873,results,shows that our model achieves 10 - 50 % relative error reduction compared with char - CRNN in these datasets .,"[('shows', (0, 1)), ('achieves', (4, 5)), ('compared with', (12, 14))]","[('our model', (2, 4)), ('10 - 50 % relative error reduction', (5, 12)), ('char - CRNN', (14, 17))]","[['our model', 'achieves', '10 - 50 % relative error reduction'], ['10 - 50 % relative error reduction', 'compared with', 'char - CRNN']]",[],"[['Results', 'shows', 'our model']]",[],document_classification,8,180
874,results,shows that DRNN performs far better than CNN .,"[('performs', (3, 4)), ('than', (6, 7))]","[('DRNN', (2, 3)), ('far better', (4, 6)), ('CNN', (7, 8))]","[['DRNN', 'performs', 'far better'], ['far better', 'than', 'CNN']]",[],[],"[['Results', 'has', 'DRNN']]",document_classification,8,186
875,results,Our model DRNN achieves much better performance than GRU and LSTM .,"[('achieves', (3, 4)), ('than', (7, 8))]","[('Our model DRNN', (0, 3)), ('much better performance', (4, 7)), ('GRU and LSTM', (8, 11))]","[['Our model DRNN', 'achieves', 'much better performance'], ['much better performance', 'than', 'GRU and LSTM']]",[],[],"[['Results', 'has', 'Our model DRNN']]",document_classification,8,193
876,research-problem,Investigating Capsule Networks with Dynamic Routing for Text Classification,[],"[('Text Classification', (7, 9))]",[],[],[],[],document_classification,9,2
877,code,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,[],"[('https : //github.com/andyweizhao/capsule_text_ classification', (7, 11))]",[],[],[],[],document_classification,9,10
878,model,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,"[('called', (3, 4))]","[('A recent method', (0, 3)), ('capsule network', (4, 6))]","[['A recent method', 'called', 'capsule network']]","[['A recent method', 'name', 'capsule network']]",[],[],document_classification,9,31
879,model,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,"[('to decide', (6, 8)), ('between', (11, 12)), ('from', (13, 14))]","[('an iterative routing process', (2, 6)), ('credit attribution', (9, 11)), ('nodes', (12, 13)), ('lower and higher layers', (14, 18))]","[['an iterative routing process', 'to decide', 'credit attribution'], ['credit attribution', 'between', 'nodes'], ['credit attribution', 'from', 'lower and higher layers']]",[],[],"[['Model', 'has', 'an iterative routing process']]",document_classification,9,32
880,model,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words thatare unrelated to specific categories .","[('to stabilize', (4, 6)), ('to alleviate', (10, 12)), ('of', (14, 15)), ('may contain', (19, 21)), ('such as', (25, 27)), ('unrelated to', (33, 35))]","[('Three strategies', (0, 2)), ('dynamic routing process', (7, 10)), ('disturbance', (13, 14)), ('some noise capsules', (15, 18)), ('"" background "" information', (21, 25)), ('stop words', (27, 29)), ('words', (31, 32)), ('specific categories', (35, 37))]","[['Three strategies', 'to stabilize', 'dynamic routing process'], ['dynamic routing process', 'to alleviate', 'disturbance'], ['disturbance', 'of', 'some noise capsules'], ['some noise capsules', 'may contain', '"" background "" information'], ['"" background "" information', 'such as', 'stop words'], ['words', 'unrelated to', 'specific categories']]",[],[],"[['Model', 'has', 'Three strategies']]",document_classification,9,36
881,hyperparameters,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .","[('use', (5, 6)), ('to initialize', (11, 13))]","[('300 - dimensional word2vec vectors', (6, 11)), ('embedding vectors', (13, 15))]","[['300 - dimensional word2vec vectors', 'to initialize', 'embedding vectors']]",[],"[['Hyperparameters', 'use', '300 - dimensional word2vec vectors']]",[],document_classification,9,139
882,hyperparameters,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,"[('conduct', (1, 2)), ('with', (3, 4)), ('for', (6, 7)), ('for', (13, 14))]","[('mini-batch', (2, 3)), ('size 50', (4, 6)), (""AG 's news"", (7, 10)), ('size 25', (11, 13)), ('other datasets', (14, 16))]","[['mini-batch', 'with', 'size 50'], ['size 50', 'for', ""AG 's news""], ['size 25', 'for', 'other datasets']]",[],"[['Hyperparameters', 'conduct', 'mini-batch']]",[],document_classification,9,140
883,hyperparameters,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,"[('with', (5, 6))]","[('Adam optimization algorithm', (2, 5)), ('1e - 3 learning rate', (6, 11))]","[['Adam optimization algorithm', 'with', '1e - 3 learning rate']]",[],[],[],document_classification,9,141
884,baselines,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .","[('including', (15, 16))]","[('several strong baseline methods', (11, 15)), ('LSTM / Bi - LSTM', (17, 22)), ('tree - structured LSTM ( Tree - LSTM )', (23, 32)), ('LSTM regularized by linguistic knowledge ( LR - LSTM )', (33, 43)), ('CNNrand / CNN - static / CNN - non-static ( Kim , 2014 )', (44, 58)), ('very deep convolutional network ( VD - CNN )', (59, 68)), ('character - level convolutional network ( CL - CNN )', (70, 80))]","[['several strong baseline methods', 'including', 'LSTM / Bi - LSTM'], ['several strong baseline methods', 'including', 'tree - structured LSTM ( Tree - LSTM )'], ['several strong baseline methods', 'including', 'LSTM regularized by linguistic knowledge ( LR - LSTM )'], ['several strong baseline methods', 'including', 'very deep convolutional network ( VD - CNN )'], ['several strong baseline methods', 'including', 'character - level convolutional network ( CL - CNN )']]","[['several strong baseline methods', 'name', 'LSTM / Bi - LSTM']]",[],"[['Baselines', 'has', 'several strong baseline methods']]",document_classification,9,144
885,experiments,Experimental Results,[],"[('Results', (1, 2))]",[],[],[],[],document_classification,9,145
886,results,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .","[('observe', (5, 6)), ('achieve', (10, 11)), ('on', (13, 14))]","[('capsule networks', (8, 10)), ('best results', (11, 13)), ('4 out of 6 benchmarks', (14, 19))]","[['capsule networks', 'achieve', 'best results'], ['best results', 'on', '4 out of 6 benchmarks']]",[],"[['Results', 'observe', 'capsule networks']]",[],document_classification,9,149
887,ablation-analysis,Single - Label to Multi - Label Text Classification,[],"[('Single - Label to Multi - Label Text Classification', (0, 9))]",[],[],[],"[['Ablation analysis', 'has', 'Single - Label to Multi - Label Text Classification']]",document_classification,9,156
888,ablation-analysis,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .","[('investigate', (5, 6)), ('of', (8, 9)), ('on', (11, 12)), ('using only', (16, 18)), ('as', (23, 24))]","[('capability', (7, 8)), ('capsule network', (9, 11)), ('multi-label text classification', (12, 15)), ('single - label samples', (19, 23)), ('training data', (24, 26))]","[['capability', 'of', 'capsule network'], ['capsule network', 'on', 'multi-label text classification'], ['multi-label text classification', 'using only', 'single - label samples'], ['single - label samples', 'as', 'training data']]","[['capability', 'has', 'capsule network']]","[['Ablation analysis', 'investigate', 'capability']]",[],document_classification,9,163
889,ablation-analysis,"From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .","[('observe that', (6, 8)), ('have', (11, 12)), ('in terms of', (16, 19)), ('over', (23, 24)), ('on', (28, 29)), ('in', (32, 33))]","[('capsule networks', (9, 11)), ('substantial and significant improvement', (12, 16)), ('all four evaluation metrics', (19, 23)), ('strong baseline methods', (25, 28)), ('test sets', (30, 32)), ('Reuters - Multi-label and Reuters - Full datasets', (34, 42))]","[['capsule networks', 'have', 'substantial and significant improvement'], ['substantial and significant improvement', 'in terms of', 'all four evaluation metrics'], ['all four evaluation metrics', 'over', 'strong baseline methods'], ['strong baseline methods', 'on', 'test sets'], ['test sets', 'in', 'Reuters - Multi-label and Reuters - Full datasets']]","[['capsule networks', 'has', 'substantial and significant improvement']]","[['Ablation analysis', 'observe that', 'capsule networks']]",[],document_classification,9,175
890,ablation-analysis,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .","[('achieved on', (6, 8)), ('contains', (16, 17)), ('in', (20, 21))]","[('larger improvement', (3, 5)), ('Reuters - Multi - label dataset', (8, 14)), ('multi-label documents', (18, 20)), ('test set', (22, 24))]","[['larger improvement', 'achieved on', 'Reuters - Multi - label dataset'], ['Reuters - Multi - label dataset', 'contains', 'multi-label documents'], ['multi-label documents', 'in', 'test set']]",[],[],"[['Ablation analysis', 'has', 'larger improvement']]",document_classification,9,176
891,ablation-analysis,The capsule network has much stronger transferring capability than the conventional deep neural networks .,"[('than', (8, 9))]","[('capsule network', (1, 3)), ('much stronger transferring capability', (4, 8)), ('conventional deep neural networks', (10, 14))]","[['much stronger transferring capability', 'than', 'conventional deep neural networks']]","[['capsule network', 'has', 'much stronger transferring capability']]",[],"[['Ablation analysis', 'has', 'capsule network']]",document_classification,9,178
892,ablation-analysis,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .","[('on', (6, 7)), ('indicate that', (11, 13)), ('over', (19, 20)), ('on', (21, 22))]","[('good results', (4, 6)), ('Reuters - Full', (7, 10)), ('capsule network', (14, 16)), ('robust superiority', (17, 19)), ('competitors', (20, 21)), ('single - label documents', (22, 26))]","[['good results', 'on', 'Reuters - Full'], ['competitors', 'on', 'single - label documents'], ['good results', 'indicate that', 'capsule network'], ['Reuters - Full', 'indicate that', 'capsule network'], ['robust superiority', 'over', 'competitors'], ['competitors', 'on', 'single - label documents']]","[['Reuters - Full', 'has', 'capsule network'], ['capsule network', 'has', 'robust superiority']]",[],[],document_classification,9,179
893,ablation-analysis,Connection Strength Visualization,[],"[('Connection Strength Visualization', (0, 3))]",[],[],[],"[['Ablation analysis', 'has', 'Connection Strength Visualization']]",document_classification,9,180
894,ablation-analysis,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .","[('observe', (5, 6)), ('correctly recognize and cluster', (10, 14)), ('with respect to', (17, 20))]","[('capsule networks', (7, 9)), ('important phrases', (15, 17)), ('text categories', (21, 23))]","[['capsule networks', 'correctly recognize and cluster', 'important phrases'], ['important phrases', 'with respect to', 'text categories']]",[],"[['Ablation analysis', 'observe', 'capsule networks']]",[],document_classification,9,188
895,research-problem,Deep Joint Entity Disambiguation with Local Neural Attention,[],"[('Joint Entity Disambiguation', (1, 4))]",[],[],[],[],entity_linking,0,2
896,research-problem,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .,[],"[('Entity disambiguation ( ED )', (0, 5))]",[],[],[],[],entity_linking,0,9
897,research-problem,"ED research has largely focused on two types of contextual information for disambiguation : local information based on words that occur in a context window around an entity mention , and , global information , exploiting document - level coherence of the referenced entities .",[],"[('ED', (0, 1))]",[],[],[],[],entity_linking,0,12
898,model,The explicit goal of our work is to use deep learning in order to learn basic features and their combinations from scratch .,"[('use', (8, 9)), ('to learn', (13, 15)), ('from', (20, 21))]","[('deep learning', (9, 11)), ('basic features', (15, 17)), ('combinations', (19, 20)), ('scratch', (21, 22))]","[['deep learning', 'to learn', 'basic features'], ['combinations', 'from', 'scratch']]",[],"[['Model', 'use', 'deep learning']]",[],entity_linking,0,18
899,experimental-setup,All models are implemented in the Torch framework .,"[('implemented in', (3, 5))]","[('Torch framework', (6, 8))]",[],[],"[['Experimental setup', 'implemented in', 'Torch framework']]",[],entity_linking,0,179
900,experimental-setup,"For entity embeddings only , we use Wikipedia ( Feb 2014 ) corpus for training .","[('For', (0, 1)), ('use', (6, 7)), ('for', (13, 14))]","[('entity embeddings only', (1, 4)), ('Wikipedia ( Feb 2014 ) corpus', (7, 13)), ('training', (14, 15))]","[['Wikipedia ( Feb 2014 ) corpus', 'For', 'training'], ['entity embeddings only', 'use', 'Wikipedia ( Feb 2014 ) corpus'], ['Wikipedia ( Feb 2014 ) corpus', 'for', 'training']]",[],"[['Experimental setup', 'For', 'entity embeddings only']]",[],entity_linking,0,181
901,experiments,Entity vectors are initialized randomly from a 0 mean normal distribution with standard deviation 1 .,"[('from', (5, 6)), ('with', (11, 12))]","[('Entity vectors', (0, 2)), ('initialized randomly', (3, 5)), ('0 mean normal distribution', (7, 11)), ('standard deviation 1', (12, 15))]","[['initialized randomly', 'from', '0 mean normal distribution'], ['0 mean normal distribution', 'with', 'standard deviation 1']]","[['Entity vectors', 'has', 'initialized randomly']]",[],[],entity_linking,0,182
902,experiments,We first train each entity vector on the entity 's Wikipedia canonical description page ( title words included ) for 400 iterations .,"[('train', (2, 3)), ('on', (6, 7)), ('for', (19, 20))]","[('each entity vector', (3, 6)), (""entity 's Wikipedia canonical description page"", (8, 14)), ('400 iterations', (20, 22))]","[['each entity vector', 'on', ""entity 's Wikipedia canonical description page""], [""entity 's Wikipedia canonical description page"", 'for', '400 iterations']]",[],[],[],entity_linking,0,183
903,experiments,We use Adagrad with a learning rate of 0.3 .,"[('use', (1, 2)), ('with', (3, 4)), ('of', (7, 8))]","[('Adagrad', (2, 3)), ('learning rate', (5, 7)), ('0.3', (8, 9))]","[['Adagrad', 'with', 'learning rate'], ['learning rate', 'of', '0.3']]",[],[],[],entity_linking,0,186
904,experiments,"We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .","[('choose', (1, 2)), ('pre-trained ( fixed ) Word2 Vec word vectors', (8, 16)), ('of', (28, 29)), ('for', (30, 31))]","[('embedding size', (2, 4)), ('d = 300', (4, 7)), ('8 , ? = 0.6 , ? = 0.1', (16, 25)), ('window size', (26, 28)), ('20', (29, 30)), ('hyperlinks', (32, 33))]","[['d = 300', 'pre-trained ( fixed ) Word2 Vec word vectors', '8 , ? = 0.6 , ? = 0.1'], ['window size', 'of', '20'], ['20', 'for', 'hyperlinks']]","[['embedding size', 'has', 'd = 300'], ['window size', 'has', '20']]",[],[],entity_linking,0,187
905,experimental-setup,Training of those takes 20 hours on a single TitanX GPU with 12 GB of memory .,"[('of', (1, 2)), ('takes', (3, 4)), ('on', (6, 7)), ('with', (11, 12))]","[('Training', (0, 1)), ('20 hours', (4, 6)), ('single TitanX GPU', (8, 11)), ('12 GB', (12, 14)), ('memory', (15, 16))]","[['12 GB', 'of', 'memory'], ['Training', 'takes', '20 hours'], ['20 hours', 'on', 'single TitanX GPU'], ['single TitanX GPU', 'with', '12 GB']]",[],[],"[['Experimental setup', 'has', 'Training']]",entity_linking,0,190
906,experimental-setup,"Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .","[('trained on', (7, 9)), ('validated on', (17, 19)), ('tested on', (23, 25))]","[('Our local and global ED models', (0, 6)), ('AIDA - train ( multiple epochs )', (9, 16)), ('AIDA - A', (19, 22)), ('AIDA - B', (25, 28))]","[['Our local and global ED models', 'trained on', 'AIDA - train ( multiple epochs )'], ['AIDA - train ( multiple epochs )', 'validated on', 'AIDA - A'], ['Our local and global ED models', 'tested on', 'AIDA - B']]",[],[],"[['Experimental setup', 'has', 'Our local and global ED models']]",entity_linking,0,199
907,experimental-setup,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .","[('use', (1, 2)), ('with', (3, 4)), ('of', (6, 7)), ('until', (10, 11)), ('exceeds', (13, 14)), ('setting it to', (18, 21))]","[('Adam', (2, 3)), ('learning rate', (4, 6)), ('1e - 4', (7, 10)), ('validation accuracy', (11, 13)), ('90 %', (14, 16)), ('1e - 5', (21, 24))]","[['Adam', 'with', 'learning rate'], ['learning rate', 'of', '1e - 4'], ['1e - 4', 'until', 'validation accuracy'], ['validation accuracy', 'exceeds', '90 %'], ['validation accuracy', 'setting it to', '1e - 5'], ['90 %', 'setting it to', '1e - 5']]",[],"[['Experimental setup', 'use', 'Adam']]",[],entity_linking,0,200
908,experimental-setup,"To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .","[('use', (4, 5)), ('stop', (10, 11)), ('if', (12, 13)), ('after', (19, 20))]","[('regularize', (1, 2)), ('early stopping', (5, 7)), ('learning', (11, 12)), ('validation accuracy', (14, 16)), ('does not increase', (16, 19)), ('500 epochs', (20, 22))]","[['regularize', 'use', 'early stopping'], ['early stopping', 'stop', 'learning'], ['learning', 'if', 'validation accuracy'], ['does not increase', 'after', '500 epochs']]","[['validation accuracy', 'has', 'does not increase']]",[],[],entity_linking,0,206
909,experimental-setup,"Training on a single GPU takes , on average , 2 ms per mention , or 16 hours for 1250 epochs over AIDA - train .","[('on', (1, 2)), ('takes', (5, 6)), ('on average', (7, 9)), ('for', (18, 19)), ('over', (21, 22))]","[('Training', (0, 1)), ('single GPU', (3, 5)), ('2 ms per mention', (10, 14)), ('16 hours', (16, 18)), ('1250 epochs', (19, 21)), ('AIDA - train', (22, 25))]","[['Training', 'on', 'single GPU'], ['single GPU', 'takes', '2 ms per mention'], ['single GPU', 'takes', '16 hours'], ['single GPU', 'on average', '2 ms per mention'], ['16 hours', 'for', '1250 epochs'], ['1250 epochs', 'over', 'AIDA - train']]","[['Training', 'has', 'single GPU']]",[],"[['Experimental setup', 'has', 'Training']]",entity_linking,0,207
910,results,We obtain state of the art accuracy on AIDA which is the largest and hardest ( by the accuracy of thep ( e |m ) baseline ) manually created ED dataset .,"[('obtain', (1, 2)), ('on', (7, 8))]","[('state of the art accuracy', (2, 7)), ('AIDA', (8, 9))]","[['state of the art accuracy', 'on', 'AIDA']]",[],"[['Results', 'obtain', 'state of the art accuracy']]",[],entity_linking,0,228
911,results,"To gain further insight , we analyzed the accuracy on the AIDA - B dataset for situations where gold entities have low frequency or mention prior .","[('analyzed', (6, 7)), ('on', (9, 10))]","[('accuracy', (8, 9)), ('AIDA - B dataset', (11, 15))]","[['accuracy', 'on', 'AIDA - B dataset']]",[],"[['Results', 'analyzed', 'accuracy']]",[],entity_linking,0,232
912,results,shows that our method performs well in these harder cases . :,"[('shows that', (0, 2)), ('performs', (4, 5)), ('in', (6, 7))]","[('our method', (2, 4)), ('well', (5, 6)), ('these harder cases', (7, 10))]","[['our method', 'performs', 'well'], ['well', 'in', 'these harder cases']]",[],"[['Results', 'shows that', 'our method']]",[],entity_linking,0,233
913,code,Our code and data are publicly available : http://github.com/dalab/deep-ed,[],"[('http://github.com/dalab/deep-ed', (8, 9))]",[],[],[],[],entity_linking,0,257
914,research-problem,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation,[],"[('Named Entity Disambiguation', (10, 13))]",[],[],[],[],entity_linking,1,2
915,research-problem,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .",[],"[('named entity disambiguation ( NED )', (16, 22))]",[],[],[],[],entity_linking,1,5
916,model,We evaluated our model by addressing NED using a simple NED model based on the trained contextualized embeddings .,[],"[('NED', (6, 7))]",[],[],[],[],entity_linking,1,9
917,model,"In this paper , we describe a new contextualized embedding model for words and entities for NED .","[('describe', (5, 6)), ('for', (11, 12)), ('for', (15, 16))]","[('new contextualized embedding model', (7, 11)), ('words and entities', (12, 15)), ('NED', (16, 17))]","[['new contextualized embedding model', 'for', 'words and entities'], ['words and entities', 'for', 'NED'], ['new contextualized embedding model', 'for', 'NED'], ['words and entities', 'for', 'NED']]",[],"[['Model', 'describe', 'new contextualized embedding model']]",[],entity_linking,1,18
918,model,"Following , the proposed model is based on the bidirectional transformer encoder .","[('based on', (6, 8))]","[('proposed model', (3, 5)), ('bidirectional transformer encoder', (9, 12))]","[['proposed model', 'based on', 'bidirectional transformer encoder']]",[],[],"[['Model', 'has', 'proposed model']]",entity_linking,1,19
919,model,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .","[('takes', (1, 2)), ('in', (8, 9)), ('produces', (14, 15)), ('for', (18, 19))]","[('sequence of words and entities', (3, 8)), ('input text', (10, 12)), ('contextualized embedding', (16, 18)), ('each word and entity', (19, 23))]","[['sequence of words and entities', 'in', 'input text'], ['contextualized embedding', 'for', 'each word and entity']]",[],"[['Model', 'takes', 'sequence of words and entities']]",[],entity_linking,1,20
920,model,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .","[('propose', (5, 6)), ('aims to train', (14, 17)), ('by predicting', (20, 22)), ('based on', (25, 27)), ('in', (31, 32))]","[('masked entity prediction', (6, 9)), ('new task', (11, 13)), ('embedding model', (18, 20)), ('randomly masked entities', (22, 25)), ('words and non-masked entities', (27, 31)), ('input text', (33, 35))]","[['new task', 'aims to train', 'embedding model'], ['embedding model', 'by predicting', 'randomly masked entities'], ['randomly masked entities', 'based on', 'words and non-masked entities'], ['words and non-masked entities', 'in', 'input text']]","[['masked entity prediction', 'has', 'new task']]","[['Model', 'propose', 'masked entity prediction']]",[],entity_linking,1,21
921,model,We trained the model using texts and their entity annotations retrieved from Wikipedia .,"[('trained', (1, 2)), ('using', (4, 5)), ('retrieved from', (10, 12))]","[('model', (3, 4)), ('texts', (5, 6)), ('entity annotations', (8, 10)), ('Wikipedia', (12, 13))]","[['model', 'using', 'texts'], ['entity annotations', 'retrieved from', 'Wikipedia']]",[],"[['Model', 'trained', 'model']]",[],entity_linking,1,22
922,results,"As shown , our models outperformed all previously proposed models .","[('outperformed', (5, 6))]","[('our models', (3, 5)), ('all previously proposed models', (6, 10))]","[['our models', 'outperformed', 'all previously proposed models']]",[],[],"[['Results', 'has', 'our models']]",entity_linking,1,113
923,results,"Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .","[('using', (2, 3)), ('boosted', (6, 7)), ('by', (9, 10))]","[('pseudo entity annotations', (3, 6)), ('accuracy', (8, 9)), ('0.3 %', (10, 12))]","[['pseudo entity annotations', 'boosted', 'accuracy'], ['accuracy', 'by', '0.3 %']]",[],"[['Results', 'using', 'pseudo entity annotations']]",[],entity_linking,1,114
924,results,"Our models achieved new state - of - the - art results on four of the five datasets , namely MSNBC , AQUAINT , ACE2004 , and WNED - WIKI , and performed competitive on the WNED - CLUEWEB dataset .","[('achieved', (2, 3)), ('on', (12, 13)), ('of', (14, 15)), ('namely', (19, 20)), ('performed', (32, 33)), ('on', (34, 35))]","[('Our models', (0, 2)), ('new state - of - the - art results', (3, 12)), ('four', (13, 14)), ('five datasets', (16, 18)), ('MSNBC', (20, 21)), ('AQUAINT', (22, 23)), ('ACE2004', (24, 25)), ('WNED - WIKI', (27, 30)), ('competitive', (33, 34)), ('WNED - CLUEWEB dataset', (36, 40))]","[['Our models', 'achieved', 'new state - of - the - art results'], ['new state - of - the - art results', 'on', 'four'], ['four', 'of', 'five datasets'], ['five datasets', 'namely', 'MSNBC'], ['five datasets', 'namely', 'AQUAINT'], ['five datasets', 'namely', 'ACE2004'], ['five datasets', 'namely', 'WNED - WIKI'], ['five datasets', 'performed', 'competitive'], ['competitive', 'on', 'WNED - CLUEWEB dataset']]","[['five datasets', 'name', 'MSNBC']]",[],"[['Results', 'has', 'Our models']]",entity_linking,1,118
925,results,"Furthermore , using pseudo entity annotations improved the performance on the AQUAINT and ACE2004 datasets .","[('improved', (6, 7)), ('on', (9, 10))]","[('performance', (8, 9)), ('AQUAINT and ACE2004 datasets', (11, 15))]","[['performance', 'on', 'AQUAINT and ACE2004 datasets']]",[],"[['Results', 'improved', 'performance']]",[],entity_linking,1,119
926,research-problem,Deep contextualized word representations,[],"[('Deep contextualized word representations', (0, 4))]",[],[],[],[],entity_linking,10,2
927,research-problem,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",[],"[('deep contextualized word representation', (6, 10))]",[],[],[],[],entity_linking,10,4
928,research-problem,Pre-trained word representations are a key component in many neural language understanding models .,[],"[('Pre-trained word representations', (0, 3))]",[],[],[],[],entity_linking,10,9
929,model,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .","[('introduce', (5, 6)), ('of', (9, 10))]","[('new type', (7, 9)), ('deep contextualized word representation', (10, 14))]","[['new type', 'of', 'deep contextualized word representation']]","[['new type', 'name', 'deep contextualized word representation']]","[['Model', 'introduce', 'new type']]",[],entity_linking,10,12
930,model,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,"[('differ from', (2, 4)), ('assigned', (13, 14)), ('function of', (19, 21))]","[('Our representations', (0, 2)), ('traditional word type embeddings', (4, 8)), ('each token', (10, 12)), ('representation', (15, 16)), ('entire input sentence', (22, 25))]","[['Our representations', 'differ from', 'traditional word type embeddings'], ['each token', 'assigned', 'representation'], ['representation', 'function of', 'entire input sentence']]",[],[],"[['Model', 'has', 'Our representations']]",entity_linking,10,13
931,model,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,"[('use', (1, 2)), ('derived from', (3, 5)), ('trained with', (10, 12)), ('on', (22, 23))]","[('vectors', (2, 3)), ('bidirectional LSTM', (6, 8)), ('coupled lan - guage model ( LM ) objective', (13, 22)), ('large text corpus', (24, 27))]","[['vectors', 'derived from', 'bidirectional LSTM'], ['bidirectional LSTM', 'trained with', 'coupled lan - guage model ( LM ) objective'], ['coupled lan - guage model ( LM ) objective', 'on', 'large text corpus']]",[],"[['Model', 'use', 'vectors']]",[],entity_linking,10,14
932,model,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","[('call them', (5, 7))]","[('ELMo ( Embeddings from Language Models ) representations', (7, 15))]",[],[],"[['Model', 'call them', 'ELMo ( Embeddings from Language Models ) representations']]",[],entity_linking,10,15
933,model,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .","[('are', (11, 12)), ('of', (22, 23)), ('of', (24, 25))]","[('ELMo representations', (9, 11)), ('deep', (12, 13)), ('function', (21, 22)), ('internal layers', (26, 28)), ('biLM', (30, 31))]","[['ELMo representations', 'are', 'deep'], ['function', 'of', 'internal layers'], ['function', 'of', 'internal layers']]","[['ELMo representations', 'has', 'deep']]",[],"[['Model', 'has', 'ELMo representations']]",entity_linking,10,16
934,model,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .","[('learn', (4, 5)), ('of', (8, 9)), ('stacked above', (11, 13)), ('for', (16, 17)), ('markedly improves', (22, 24)), ('using', (27, 28))]","[('linear combination', (6, 8)), ('vectors', (10, 11)), ('each input word', (13, 16)), ('each end task', (17, 20)), ('performance', (24, 25)), ('top LSTM layer', (29, 32))]","[['linear combination', 'of', 'vectors'], ['vectors', 'stacked above', 'each input word'], ['each input word', 'for', 'each end task'], ['each input word', 'markedly improves', 'performance'], ['performance', 'using', 'top LSTM layer']]",[],"[['Model', 'learn', 'linear combination']]",[],entity_linking,10,17
935,model,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense disambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .","[('Using', (0, 1)), ('show', (5, 6)), ('capture', (13, 14)), ('of', (18, 19)), ('model', (43, 44)), ('of', (45, 46))]","[('intrinsic evaluations', (1, 3)), ('higher - level LSTM states', (8, 13)), ('context - dependent aspects', (14, 18)), ('word meaning', (19, 21)), ('lowerlevel states', (41, 43)), ('aspects', (44, 45)), ('syntax', (46, 47))]","[['intrinsic evaluations', 'show', 'higher - level LSTM states'], ['higher - level LSTM states', 'capture', 'context - dependent aspects'], ['context - dependent aspects', 'of', 'word meaning'], ['lowerlevel states', 'model', 'aspects'], ['aspects', 'of', 'syntax']]","[['intrinsic evaluations', 'has', 'higher - level LSTM states']]","[['Model', 'Using', 'intrinsic evaluations']]",[],entity_linking,10,19
936,experiments,Question answering,[],"[('Question answering', (0, 2))]",[],[],[],[],entity_linking,10,103
937,experiments,"After adding ELMo to the baseline model , test set F 1 improved by 4.7 % from 81.1 % to 85.8 % , a 24.9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1.4 % .","[('adding', (1, 2)), ('to', (3, 4)), ('improved by', (12, 14)), ('from', (16, 17)), ('to', (19, 20)), ('over', (29, 30)), ('improving', (34, 35)), ('by', (46, 47))]","[('ELMo', (2, 3)), ('baseline model', (5, 7)), ('test set F 1', (8, 12)), ('4.7 %', (14, 16)), ('81.1 %', (17, 19)), ('85.8 %', (20, 22)), ('24.9 % relative error reduction', (24, 29)), ('baseline', (31, 32)), ('overall single model state - of - the - art', (36, 46)), ('1.4 %', (47, 49))]","[['ELMo', 'to', 'baseline model'], ['81.1 %', 'to', '85.8 %'], ['test set F 1', 'improved by', '4.7 %'], ['4.7 %', 'from', '81.1 %'], ['81.1 %', 'to', '85.8 %'], ['24.9 % relative error reduction', 'over', 'baseline'], ['test set F 1', 'improving', 'overall single model state - of - the - art'], ['overall single model state - of - the - art', 'by', '1.4 %']]","[['ELMo', 'has', 'baseline model'], ['baseline model', 'has', 'test set F 1']]",[],[],entity_linking,10,107
938,experiments,The increase of 4.7 % with ELMo is also significantly larger then the 1.8 % improvement from adding CoVe to a baseline model .,"[('of', (2, 3)), ('with', (5, 6)), ('then', (11, 12)), ('from adding', (16, 18)), ('to', (19, 20))]","[('increase', (1, 2)), ('4.7 %', (3, 5)), ('ELMo', (6, 7)), ('significantly larger', (9, 11)), ('1.8 % improvement', (13, 16)), ('CoVe', (18, 19)), ('baseline model', (21, 23))]","[['increase', 'of', '4.7 %'], ['4.7 %', 'with', 'ELMo'], ['significantly larger', 'then', '1.8 % improvement'], ['1.8 % improvement', 'from adding', 'CoVe'], ['1.8 % improvement', 'to', 'baseline model'], ['CoVe', 'to', 'baseline model']]","[['significantly larger', 'has', '1.8 % improvement']]",[],[],entity_linking,10,109
939,experiments,Textual entailment,[],"[('Textual entailment', (0, 2))]",[],[],[],[],entity_linking,10,112
940,experiments,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .","[('adding', (2, 3)), ('to', (4, 5)), ('improves', (8, 9)), ('by', (10, 11)), ('of', (13, 14)), ('across', (16, 17))]","[('ELMo', (3, 4)), ('ESIM model', (6, 8)), ('accuracy', (9, 10)), ('an average', (11, 13)), ('0.7 %', (14, 16)), ('five random seeds', (17, 20))]","[['ELMo', 'to', 'ESIM model'], ['ELMo', 'improves', 'accuracy'], ['ESIM model', 'improves', 'accuracy'], ['accuracy', 'by', 'an average'], ['an average', 'of', '0.7 %'], ['0.7 %', 'across', 'five random seeds']]",[],[],[],entity_linking,10,116
941,experiments,Semantic role labeling,[],"[('Semantic role labeling', (0, 3))]",[],[],[],[],entity_linking,10,118
942,experiments,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .","[('from', (8, 9)), ('adding', (15, 16)), ('improved', (17, 18)), ('by', (22, 23)), ('to', (27, 28)), ('over', (40, 41)), ('by', (46, 47))]","[('ELMo', (16, 17)), ('average F 1', (19, 22)), ('3.2 %', (23, 25)), ('67.2', (26, 27)), ('70.4', (28, 29)), ('improving', (39, 40)), ('previous best ensemble result', (42, 46)), ('1.6 % F 1', (47, 51))]","[['67.2', 'from', '70.4'], ['ELMo', 'improved', 'average F 1'], ['average F 1', 'by', '3.2 %'], ['67.2', 'to', '70.4'], ['improving', 'over', 'previous best ensemble result'], ['previous best ensemble result', 'by', '1.6 % F 1']]",[],[],[],entity_linking,10,124
943,experiments,Named entity extraction,[],"[('Named entity extraction', (0, 3))]",[],[],[],[],entity_linking,10,125
944,experiments,"As shown in , our ELMo enhanced biLSTM - CRF achieves 92. 22 % F 1 averaged over five runs .","[('achieves', (10, 11)), ('averaged over', (16, 18))]","[('our ELMo enhanced biLSTM - CRF', (4, 10)), ('92. 22 % F 1', (11, 16)), ('five runs', (18, 20))]","[['our ELMo enhanced biLSTM - CRF', 'achieves', '92. 22 % F 1'], ['92. 22 % F 1', 'averaged over', 'five runs']]",[],[],[],entity_linking,10,128
945,research-problem,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,[],"[('Neural Word Sense Disambiguation', (10, 14))]",[],[],[],[],entity_linking,11,2
946,approach,"We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision .",[],"[('neural WSD', (11, 13))]",[],[],[],[],entity_linking,11,5
947,approach,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .",[],"[('WSD', (9, 10))]",[],[],[],[],entity_linking,11,6
948,research-problem,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,11,8
949,approach,"In this work , the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet , such as the hypernymy , the hyponymy , the meronymy , the antonymy , etc .","[('taking', (12, 13)), ('of', (14, 15)), ('between', (18, 19)), ('such as', (24, 26))]","[('advantage', (13, 14)), ('semantic relationships', (16, 18)), ('senses', (19, 20)), ('included', (20, 21)), ('WordNet', (22, 23)), ('hypernymy', (27, 28)), ('hyponymy', (30, 31)), ('meronymy', (33, 34)), ('antonymy', (36, 37))]","[['advantage', 'of', 'semantic relationships'], ['semantic relationships', 'between', 'senses'], ['senses', 'such as', 'hypernymy'], ['senses', 'such as', 'hyponymy'], ['senses', 'such as', 'meronymy'], ['senses', 'such as', 'antonymy'], ['WordNet', 'such as', 'hypernymy']]","[['advantage', 'has', 'semantic relationships'], ['senses', 'has', 'included']]","[['Approach', 'taking', 'advantage']]",[],entity_linking,11,17
950,approach,"Our method is based on the observation that a sense and its closest related senses ( it s hypernym or it s hyponyms for instance ) all share a common idea or concept , and so a word can sometimes be disambiguated using only related concepts .","[('based on', (3, 5)), ('share', (27, 28))]","[('observation', (6, 7)), ('sense and its closest related senses', (9, 15)), ('common idea or concept', (29, 33))]",[],"[['observation', 'has', 'sense and its closest related senses']]","[['Approach', 'based on', 'observation']]",[],entity_linking,11,18
951,experimental-setup,"For BERT , we used the model named "" bert - largecased "" of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .","[('For', (0, 1)), ('used', (4, 5)), ('named', (7, 8)), ('of', (13, 14)), ('consists of', (20, 22)), ('of', (23, 24)), ('trained on', (27, 29))]","[('BERT', (1, 2)), ('model', (6, 7)), ('bert - largecased', (9, 12)), ('PyTorch implementation', (15, 17)), ('vectors', (22, 23)), ('dimension 1024', (24, 26)), ('Book s Corpus and English Wikipedia', (29, 35))]","[['BERT', 'used', 'model'], ['model', 'named', 'bert - largecased'], ['bert - largecased', 'of', 'PyTorch implementation'], ['vectors', 'of', 'dimension 1024'], ['bert - largecased', 'consists of', 'vectors'], ['PyTorch implementation', 'consists of', 'vectors'], ['vectors', 'of', 'dimension 1024']]","[['model', 'name', 'bert - largecased']]","[['Experimental setup', 'For', 'BERT']]",[],entity_linking,11,132
952,experimental-setup,"For the Transformer encoder layers , we used the same parameters as the "" base "" model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .","[('of', (17, 18)), ('with', (23, 24)), ('of', (31, 32))]","[('Transformer encoder layers', (2, 5)), ('6 layers', (21, 23)), ('8 attention heads', (24, 27)), ('hidden size', (29, 31)), ('2048', (32, 33)), ('dropout', (36, 37)), ('0.1', (38, 39))]","[['hidden size', 'of', '2048'], ['dropout', 'of', '0.1'], ['6 layers', 'with', '8 attention heads'], ['6 layers', 'with', 'dropout'], ['hidden size', 'of', '2048']]",[],[],[],entity_linking,11,134
953,results,"In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .","[('observe', (7, 8)), ('use', (12, 13)), ('through', (17, 18)), ('obtain', (23, 24)), ('not', (34, 35)), ('use', (35, 36))]","[('our systems', (9, 11)), ('sense vocabulary compression', (14, 17)), ('hypernyms', (18, 19)), ('all relations', (21, 23)), ('scores', (24, 25)), ('overall equivalent', (27, 29)), ('systems', (31, 32))]","[['our systems', 'use', 'sense vocabulary compression'], ['sense vocabulary compression', 'through', 'hypernyms'], ['sense vocabulary compression', 'through', 'all relations'], ['all relations', 'obtain', 'scores']]",[],"[['Results', 'observe', 'our systems']]",[],entity_linking,11,164
954,results,"In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .","[('In comparison to', (0, 3)), ('outperform systematically', (29, 31)), ('on', (36, 37))]","[('other works', (4, 6)), ('state of the art', (32, 36)), ('every task', (37, 39))]","[['state of the art', 'on', 'every task']]","[['other works', 'has', 'state of the art']]","[['Results', 'In comparison to', 'other works']]",[],entity_linking,11,169
955,ablation-analysis,"As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .","[('as', (20, 21)), ('on', (28, 29)), ('lead to', (32, 34)), ('above', (35, 36))]","[('additional training corpus ( WNGC )', (7, 13)), ('use', (17, 18)), ('BERT', (19, 20)), ('input embeddings', (21, 23)), ('major impact', (26, 28)), ('our results', (29, 31)), ('scores', (34, 35)), ('state of the art', (37, 41))]","[['BERT', 'as', 'input embeddings'], ['major impact', 'on', 'our results'], ['scores', 'above', 'state of the art']]","[['scores', 'has', 'state of the art']]",[],[],entity_linking,11,180
956,ablation-analysis,"Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .","[('Using', (0, 1)), ('instead of', (2, 4)), ('improves', (8, 9)), ('by approximately', (12, 14)), ('in', (18, 19)), ('adding', (23, 24)), ('to', (26, 27))]","[('BERT', (1, 2)), ('ELMo or Glo Ve', (4, 8)), ('score', (11, 12)), ('3 and 5 points', (14, 18)), ('every experiment', (19, 21)), ('WNGC', (25, 26)), ('training data', (28, 30)), ('improves', (30, 31)), ('approximately 2 points', (33, 36))]","[['BERT', 'instead of', 'ELMo or Glo Ve'], ['BERT', 'improves', 'score'], ['ELMo or Glo Ve', 'improves', 'score'], ['score', 'by approximately', '3 and 5 points'], ['3 and 5 points', 'in', 'every experiment'], ['BERT', 'adding', 'WNGC'], ['ELMo or Glo Ve', 'adding', 'WNGC'], ['WNGC', 'to', 'training data']]",[],"[['Ablation analysis', 'Using', 'BERT']]",[],entity_linking,11,181
957,ablation-analysis,"Finally , using ensembles adds roughly another 1 point to the final F1 score .","[('using', (2, 3)), ('adds', (4, 5)), ('to', (9, 10))]","[('ensembles', (3, 4)), ('roughly another 1 point', (5, 9)), ('final F1 score', (11, 14))]","[['ensembles', 'adds', 'roughly another 1 point'], ['roughly another 1 point', 'to', 'final F1 score']]",[],"[['Ablation analysis', 'using', 'ensembles']]",[],entity_linking,11,182
958,ablation-analysis,"However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .","[('through', (5, 6)), ('seems to', (8, 10)), ('negatively impact', (10, 12)), ('results in', (13, 15))]","[('compression method', (3, 5)), ('all relations', (6, 8)), ('some cases', (15, 17))]","[['compression method', 'through', 'all relations']]",[],[],[],entity_linking,11,186
959,research-problem,Incorporating Glosses into Neural Word Sense Disambiguation,[],"[('Neural Word Sense Disambiguation', (3, 7))]",[],[],[],[],entity_linking,12,2
960,research-problem,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,12,4
961,research-problem,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,[],"[('WSD', (13, 14))]",[],[],[],[],entity_linking,12,5
962,model,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","[('propose', (5, 6)), ('of', (22, 23))]","[('novel model GAS', (7, 10)), ('gloss - augmented WSD neural network', (12, 18)), ('variant', (21, 22)), ('memory network', (24, 26))]","[['variant', 'of', 'memory network']]","[['novel model GAS', 'has', 'gloss - augmented WSD neural network']]","[['Model', 'propose', 'novel model GAS']]",[],entity_linking,12,30
963,model,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"[('jointly encodes', (1, 3)), ('of', (7, 8)), ('models', (12, 13)), ('between', (16, 17)), ('in', (21, 22))]","[('GAS', (0, 1)), ('context and glosses', (4, 7)), ('target word', (9, 11)), ('semantic relationship', (14, 16)), ('context and glosses', (18, 21)), ('memory module', (23, 25))]","[['GAS', 'jointly encodes', 'context and glosses'], ['context and glosses', 'of', 'target word'], ['GAS', 'models', 'semantic relationship'], ['semantic relationship', 'between', 'context and glosses'], ['context and glosses', 'in', 'memory module']]",[],[],"[['Model', 'has', 'GAS']]",entity_linking,12,31
964,model,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .","[('between', (7, 8)), ('employ', (15, 16)), ('within', (19, 20)), ('as', (22, 23)), ('adopt', (27, 28))]","[('inner relationship', (5, 7)), ('glosses and context', (8, 11)), ('more accurately', (11, 13)), ('multiple passes operation', (16, 19)), ('memory', (21, 22)), ('re-reading process', (24, 26)), ('two memory updating mechanisms', (28, 32))]","[['inner relationship', 'between', 'glosses and context'], ['inner relationship', 'employ', 'multiple passes operation'], ['multiple passes operation', 'within', 'memory'], ['memory', 'as', 're-reading process'], ['multiple passes operation', 'adopt', 'two memory updating mechanisms']]","[['glosses and context', 'has', 'more accurately']]",[],[],entity_linking,12,32
965,experiments,Incorporating Glosses into Neural Word Sense Disambiguation,[],"[('Neural Word Sense Disambiguation', (3, 7))]",[],[],[],[],entity_linking,12,65
966,hyperparameters,"We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .","[('use', (1, 2)), ('with', (5, 6)), ('keep', (11, 12)), ('during', (14, 15))]","[('pre-trained word embeddings', (2, 5)), ('300 dimensions', (6, 8)), ('fixed', (13, 14)), ('training process', (16, 18))]","[['pre-trained word embeddings', 'with', '300 dimensions'], ['fixed', 'during', 'training process']]",[],"[['Hyperparameters', 'use', 'pre-trained word embeddings']]",[],entity_linking,12,195
967,hyperparameters,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","[('employ', (1, 2)), ('in', (5, 6)), ('means', (16, 17))]","[('256 hidden units', (2, 5)), ('gloss module', (8, 10)), ('context module', (12, 14)), ('n = 256', (17, 20))]","[['256 hidden units', 'in', 'gloss module'], ['256 hidden units', 'in', 'context module'], ['256 hidden units', 'means', 'n = 256'], ['context module', 'means', 'n = 256']]",[],"[['Hyperparameters', 'employ', '256 hidden units']]",[],entity_linking,12,196
968,hyperparameters,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","[('used for', (3, 5)), ('in', (6, 7)), ('with', (12, 13))]","[('Orthogonal initialization', (0, 2)), ('weights', (5, 6)), ('LSTM', (7, 8)), ('random uniform initialization', (9, 12)), ('range [ - 0.1 , 0.1 ]', (13, 20)), ('others', (23, 24))]","[['Orthogonal initialization', 'used for', 'weights'], ['weights', 'in', 'LSTM'], ['random uniform initialization', 'with', 'range [ - 0.1 , 0.1 ]']]",[],[],"[['Hyperparameters', 'has', 'Orthogonal initialization']]",entity_linking,12,197
969,hyperparameters,We assign gloss expansion depth K the value of 4 .,"[('assign', (1, 2)), ('of', (8, 9))]","[('gloss expansion depth K', (2, 6)), ('value', (7, 8)), ('4', (9, 10))]","[['value', 'of', '4']]","[['gloss expansion depth K', 'has', 'value']]","[['Hyperparameters', 'assign', 'gloss expansion depth K']]",[],entity_linking,12,198
970,hyperparameters,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","[('experiment with', (2, 4)), ('from', (12, 13)), ('to', (14, 15)), ('in', (16, 17))]","[('number of passes | T M |', (5, 12)), ('1', (13, 14)), ('5', (15, 16)), ('our framework', (17, 19)), ('finding', (20, 21)), ('| T M | = 3', (21, 27)), ('performs best', (27, 29))]","[['number of passes | T M |', 'from', '1'], ['1', 'to', '5'], ['5', 'in', 'our framework']]","[['number of passes | T M |', 'has', '1'], ['finding', 'has', '| T M | = 3'], ['| T M | = 3', 'has', 'performs best']]","[['Hyperparameters', 'experiment with', 'number of passes | T M |']]",[],entity_linking,12,199
971,hyperparameters,We use Adam optimizer in the training process with 0.001 initial learning rate .,"[('in', (4, 5)), ('with', (8, 9))]","[('Adam optimizer', (2, 4)), ('training process', (6, 8)), ('0.001 initial learning rate', (9, 13))]","[['Adam optimizer', 'in', 'training process'], ['Adam optimizer', 'with', '0.001 initial learning rate'], ['training process', 'with', '0.001 initial learning rate']]",[],[],[],entity_linking,12,200
972,hyperparameters,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .","[('to avoid', (2, 4)), ('use', (7, 8)), ('set', (11, 12)), ('to', (14, 15))]","[('overfitting', (4, 5)), ('dropout regularization', (8, 10)), ('drop rate', (12, 14)), ('0.5', (15, 16))]","[['overfitting', 'use', 'dropout regularization'], ['drop rate', 'to', '0.5']]","[['drop rate', 'has', '0.5']]","[['Hyperparameters', 'to avoid', 'overfitting']]",[],entity_linking,12,201
973,hyperparameters,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,"[('for up to', (2, 5)), ('with', (7, 8)), ('if', (10, 11)), ('within', (17, 18))]","[('Training', (0, 1)), ('100 epochs', (5, 7)), ('early stopping', (8, 10)), ('validation loss', (12, 14)), (""does n't improve"", (14, 17)), ('last 10 epochs', (19, 22))]","[['100 epochs', 'with', 'early stopping'], ['early stopping', 'if', 'validation loss'], [""does n't improve"", 'within', 'last 10 epochs']]","[['validation loss', 'has', ""does n't improve""]]",[],"[['Hyperparameters', 'has', 'Training']]",entity_linking,12,202
974,experiments,Knowledge - based Systems,[],"[('Knowledge - based Systems', (0, 4))]",[],[],[],[],entity_linking,12,210
975,baselines,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,"[('exploits', (2, 3)), ('from', (7, 8)), ('builds', (10, 11)), ('for', (17, 18))]","[('Babelfy', (0, 1)), ('semantic network structure', (4, 7)), ('BabelNet', (8, 9)), ('unified graph - based architecture', (12, 17)), ('WSD and Entity Linking', (18, 22))]","[['Babelfy', 'exploits', 'semantic network structure'], ['semantic network structure', 'from', 'BabelNet'], ['Babelfy', 'builds', 'unified graph - based architecture'], ['unified graph - based architecture', 'for', 'WSD and Entity Linking']]","[['Babelfy', 'has', 'semantic network structure']]",[],"[['Baselines', 'has', 'Babelfy']]",entity_linking,12,212
976,experiments,Supervised Systems,[],"[('Supervised Systems', (0, 2))]",[],[],[],[],entity_linking,12,213
977,baselines,"IMS : Zhi and Ng ( 2010 ) selects a linear Support Vector Machine ( SVM ) as its classifier and makes use of a set of features surrounding the target word within a limited window , such as POS tags , local words and local collocations .","[('selects', (8, 9)), ('as', (17, 18)), ('use', (22, 23)), ('surrounding', (28, 29)), ('within', (32, 33)), ('such as', (37, 39))]","[('IMS', (0, 1)), ('linear Support Vector Machine ( SVM )', (10, 17)), ('classifier', (19, 20)), ('set of features', (25, 28)), ('target word', (30, 32)), ('limited window', (34, 36)), ('POS tags', (39, 41)), ('local words', (42, 44)), ('local collocations', (45, 47))]","[['IMS', 'selects', 'linear Support Vector Machine ( SVM )'], ['linear Support Vector Machine ( SVM )', 'as', 'classifier'], ['linear Support Vector Machine ( SVM )', 'use', 'set of features'], ['set of features', 'surrounding', 'target word'], ['target word', 'within', 'limited window'], ['limited window', 'such as', 'POS tags'], ['limited window', 'such as', 'local words'], ['limited window', 'such as', 'local collocations']]",[],[],"[['Baselines', 'has', 'IMS']]",entity_linking,12,215
978,baselines,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,"[('selects', (3, 4)), ('as', (5, 6)), ('makes use of', (10, 13)), ('as', (15, 16)), ('makes', (18, 19)), ('hard to beat', (20, 23)), ('inmost of', (23, 25))]","[('IMS +emb', (0, 2)), ('IMS', (4, 5)), ('underlying framework', (7, 9)), ('word embeddings', (13, 15)), ('features', (16, 17)), ('WSD datasets', (25, 27))]","[['IMS +emb', 'selects', 'IMS'], ['IMS', 'as', 'underlying framework'], ['IMS +emb', 'makes use of', 'word embeddings'], ['word embeddings', 'as', 'features']]","[['IMS +emb', 'has', 'IMS']]",[],"[['Baselines', 'has', 'IMS +emb']]",entity_linking,12,216
979,experiments,Neural - based Systems,[],"[('Neural - based Systems', (0, 4))]",[],[],[],[],entity_linking,12,217
980,baselines,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,"[('leverages', (3, 4)), ('shares', (9, 10)), ('among', (12, 13))]","[('Bi- LSTM', (0, 2)), ('bidirectional LSTM network', (5, 8)), ('model parameters', (10, 12)), ('all words', (13, 15))]","[['Bi- LSTM', 'leverages', 'bidirectional LSTM network'], ['bidirectional LSTM network', 'shares', 'model parameters'], ['model parameters', 'among', 'all words']]",[],[],"[['Baselines', 'has', 'Bi- LSTM']]",entity_linking,12,219
981,baselines,"Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS : transfers WSD into a sequence learning task and propose a multi - task learning framework for WSD , POS tagging and coarse - grained semantic labels ( LEX ) .","[('transfers', (13, 14)), ('into', (15, 16)), ('propose', (21, 22)), ('for', (28, 29))]","[('Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS', (0, 12)), ('WSD', (14, 15)), ('sequence learning task', (17, 20)), ('multi - task learning framework', (23, 28)), ('WSD', (29, 30)), ('POS tagging', (31, 33)), ('coarse - grained semantic labels ( LEX )', (34, 42))]","[['Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS', 'transfers', 'WSD'], ['WSD', 'into', 'sequence learning task'], ['Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS', 'propose', 'multi - task learning framework'], ['multi - task learning framework', 'for', 'WSD'], ['multi - task learning framework', 'for', 'coarse - grained semantic labels ( LEX )']]",[],[],"[['Baselines', 'has', 'Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS']]",entity_linking,12,221
982,results,English all - words results,[],"[('English all - words results', (0, 5))]",[],[],[],"[['Results', 'has', 'English all - words results']]",entity_linking,12,225
983,results,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,"[('achieves', (4, 5)), ('on', (12, 13)), ('of', (15, 16))]","[('GAS and GAS ext', (0, 4)), ('state - of - theart performance', (6, 12)), ('concatenation', (14, 15)), ('all test datasets', (16, 19))]","[['GAS and GAS ext', 'achieves', 'state - of - theart performance'], ['state - of - theart performance', 'on', 'concatenation'], ['concatenation', 'of', 'all test datasets']]",[],[],"[['Results', 'has', 'GAS and GAS ext']]",entity_linking,12,230
984,results,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .","[('on', (9, 10)), ('find', (18, 19)), ('with', (22, 23)), ('achieves', (27, 28)), ('of', (35, 36))]","[('GAS ext', (20, 22)), ('concatenation memory updating strategy', (23, 27)), ('best results', (29, 31)), ('70.6', (31, 32)), ('concatenation', (34, 35)), ('four test datasets', (37, 40))]","[['best results', 'on', 'concatenation'], ['70.6', 'on', 'concatenation'], ['GAS ext', 'with', 'concatenation memory updating strategy'], ['concatenation memory updating strategy', 'achieves', 'best results'], ['concatenation', 'of', 'four test datasets']]","[['best results', 'has', '70.6']]",[],[],entity_linking,12,231
985,results,It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,"[('boost', (8, 9)), ('avoid', (14, 15)), ('of', (18, 19))]","[('appropriate number of passes', (3, 7)), ('performance', (10, 11)), ('over - fitting', (15, 18)), ('model', (20, 21))]","[['appropriate number of passes', 'boost', 'performance'], ['appropriate number of passes', 'avoid', 'over - fitting'], ['over - fitting', 'of', 'model']]",[],[],[],entity_linking,12,234
986,results,Multiple Passes Analysis,[],"[('Multiple Passes Analysis', (0, 3))]",[],[],[],"[['Results', 'has', 'Multiple Passes Analysis']]",entity_linking,12,242
987,results,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .","[('shows', (1, 2)), ('performs', (6, 7)), ('than', (8, 9))]","[('multiple passes operation', (3, 6)), ('better', (7, 8)), ('one pass', (9, 11))]","[['multiple passes operation', 'performs', 'better'], ['better', 'than', 'one pass']]",[],"[['Results', 'shows', 'multiple passes operation']]",[],entity_linking,12,252
988,results,"In Table 3 , with the increasing number of passes , the F1 - score increases .","[('with', (4, 5))]","[('increasing', (6, 7)), ('number of passes', (7, 10)), ('F1 - score', (12, 15)), ('increases', (15, 16))]",[],"[['increasing', 'has', 'number of passes'], ['number of passes', 'has', 'F1 - score'], ['F1 - score', 'has', 'increases']]","[['Results', 'with', 'increasing']]",[],entity_linking,12,256
989,results,"However , when the number of passes is larger than 3 , the F1- score stops increasing or even decreases due to over-fitting .","[('when', (2, 3)), ('stops', (15, 16)), ('due to', (20, 22))]","[('number of passes', (4, 7)), ('larger than 3', (8, 11)), ('F1- score', (13, 15)), ('increasing or even decreases', (16, 20)), ('over-fitting', (22, 23))]","[['F1- score', 'stops', 'increasing or even decreases'], ['increasing or even decreases', 'due to', 'over-fitting']]","[['number of passes', 'has', 'larger than 3'], ['larger than 3', 'has', 'F1- score']]","[['Results', 'when', 'number of passes']]",[],entity_linking,12,257
990,research-problem,Word Sense Disambiguation using a Bidirectional LSTM,[],"[('Word Sense Disambiguation', (0, 3))]",[],[],[],[],entity_linking,13,2
991,research-problem,"The task of assigning a word token in a text , e.g. rock , to a well defined word sense in a lexicon is called word sense disambiguation ( WSD ) .",[],"[('word sense disambiguation ( WSD )', (25, 31))]",[],[],[],[],entity_linking,13,14
992,research-problem,"Improved WSD would be beneficial to many natural language processing ( NLP ) problems , e.g. machine translation , information Retrieval , information Extraction , and sense aware word representations .",[],"[('WSD', (1, 2))]",[],[],[],[],entity_linking,13,19
993,model,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .","[('modeling', (10, 11)), ('surrounding', (15, 16)), ('using', (26, 27)), ('represent', (36, 37)), ('i.e.', (45, 46))]","[('sequence of words', (12, 15)), ('target word', (17, 19)), ('words', (38, 39)), ('real valued vector representation', (40, 44)), ('word embeddings', (46, 48))]","[['sequence of words', 'surrounding', 'target word'], ['real valued vector representation', 'i.e.', 'word embeddings']]",[],"[['Model', 'modeling', 'sequence of words']]",[],entity_linking,13,23
994,experimental-setup,"The source code , implemented using TensorFlow , has been released as open source 1 .","[('implemented using', (4, 6))]","[('source code', (1, 3)), ('TensorFlow', (6, 7))]","[['source code', 'implemented using', 'TensorFlow']]","[['source code', 'has', 'TensorFlow']]",[],[],entity_linking,13,83
995,experimental-setup,The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,"[('initialized using', (3, 5)), ('of', (7, 8)), ('trained on', (14, 16))]","[('embeddings', (1, 2)), ('set', (6, 7)), ('freely available 2 Glo Ve vectors', (8, 14)), ('Wikipedia and Gigaword', (16, 19))]","[['embeddings', 'initialized using', 'set'], ['set', 'of', 'freely available 2 Glo Ve vectors'], ['freely available 2 Glo Ve vectors', 'trained on', 'Wikipedia and Gigaword']]",[],[],"[['Experimental setup', 'has', 'embeddings']]",entity_linking,13,87
996,experimental-setup,"Words not included in this set are initialized from N ( 0 , 0.1 ) .","[('not included', (1, 3)), ('from', (8, 9))]","[('Words', (0, 1)), ('set', (5, 6)), ('initialized', (7, 8)), ('N ( 0 , 0.1 )', (9, 15))]","[['Words', 'from', 'N ( 0 , 0.1 )'], ['initialized', 'from', 'N ( 0 , 0.1 )']]","[['Words', 'has', 'set']]",[],"[['Experimental setup', 'has', 'Words']]",entity_linking,13,88
997,results,Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,"[('achieves', (3, 4)), ('on', (7, 8)), ('tied with', (11, 13)), ('on', (17, 18))]","[('Our proposed model', (0, 3)), ('top score', (5, 7)), ('SE2', (8, 9)), ('IMS + adapted CW', (13, 17)), ('SE3', (18, 19))]","[['Our proposed model', 'achieves', 'top score'], ['top score', 'on', 'SE2'], ['IMS + adapted CW', 'on', 'SE3'], ['Our proposed model', 'tied with', 'IMS + adapted CW'], ['IMS + adapted CW', 'on', 'SE3']]","[['Our proposed model', 'has', 'top score']]",[],"[['Results', 'has', 'Our proposed model']]",entity_linking,13,106
998,results,"Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .","[('see that', (3, 5)), ('consistently improves', (6, 8)), ('on', (10, 11))]","[('dropword', (5, 6)), ('results', (9, 10)), ('both SE2 and SE3', (11, 15))]","[['dropword', 'consistently improves', 'results'], ['results', 'on', 'both SE2 and SE3']]",[],"[['Results', 'see that', 'dropword']]",[],entity_linking,13,107
999,results,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .","[('Randomizing', (0, 1)), ('of', (3, 4)), ('yields', (7, 8))]","[('order', (2, 3)), ('input words', (5, 7)), ('substantially worse result', (9, 12))]","[['order', 'of', 'input words'], ['input words', 'yields', 'substantially worse result']]",[],"[['Results', 'Randomizing', 'order']]",[],entity_linking,13,108
1000,results,We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these datasets .,"[('makes use of', (7, 10)), ('in', (12, 13))]","[('system', (5, 6)), ('information', (11, 12)), ('pre-trained word embeddings', (14, 17))]","[['system', 'makes use of', 'information'], ['information', 'in', 'pre-trained word embeddings']]",[],[],[],entity_linking,13,109
1001,research-problem,Knowledge - based Word Sense Disambiguation using Topic Models,[],"[('Knowledge - based Word Sense Disambiguation', (0, 6))]",[],[],[],[],entity_linking,14,2
1002,research-problem,Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data .,[],"[('Disambiguation', (0, 1))]",[],[],[],[],entity_linking,14,4
1003,research-problem,Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context .,[],"[('WSD', (1, 2))]",[],[],[],[],entity_linking,14,5
1004,research-problem,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,14,12
1005,model,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .","[('propose', (5, 6)), ('for', (13, 14)), ('utilizes', (22, 23)), ('as', (26, 27)), ('for', (29, 30))]","[('novel knowledge - based WSD algorithm', (7, 13)), ('all - word WSD task', (15, 20)), ('whole document', (24, 26)), ('context', (28, 29)), ('word', (31, 32))]","[['novel knowledge - based WSD algorithm', 'for', 'all - word WSD task'], ['all - word WSD task', 'utilizes', 'whole document'], ['whole document', 'as', 'context'], ['context', 'for', 'word']]",[],"[['Model', 'propose', 'novel knowledge - based WSD algorithm']]",[],entity_linking,14,22
1006,model,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .","[('to model', (2, 4)), ('for', (7, 8)), ('leverage', (11, 12)), ('of', (14, 15)), ('especially', (18, 19))]","[('whole document', (5, 7)), ('WSD', (8, 9)), ('formalism', (13, 14)), ('topic models', (15, 17)), ('Latent Dirichlet Allocation ( LDA )', (19, 25))]","[['whole document', 'for', 'WSD'], ['formalism', 'of', 'topic models'], ['topic models', 'especially', 'Latent Dirichlet Allocation ( LDA )']]",[],"[['Model', 'to model', 'whole document']]",[],entity_linking,14,23
1007,model,Our method is a variant of LDA in which the topic proportions for a document are replaced by synset proportions for a document .,"[('of', (5, 6)), ('in which', (7, 9)), ('for', (12, 13)), ('replaced by', (16, 18)), ('for', (20, 21))]","[('Our method', (0, 2)), ('variant', (4, 5)), ('LDA', (6, 7)), ('topic proportions', (10, 12)), ('document', (14, 15)), ('synset proportions', (18, 20)), ('document', (22, 23))]","[['variant', 'of', 'LDA'], ['LDA', 'in which', 'topic proportions'], ['topic proportions', 'for', 'document'], ['synset proportions', 'for', 'document'], ['topic proportions', 'replaced by', 'synset proportions'], ['synset proportions', 'for', 'document']]","[['Our method', 'has', 'variant']]",[],"[['Model', 'has', 'Our method']]",entity_linking,14,24
1008,model,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,"[('use', (1, 2)), ('for', (5, 6)), ('over', (9, 10)), ('to model', (11, 13)), ('of', (15, 16)), ('within', (17, 18))]","[('non-uniform prior', (3, 5)), ('synset distribution', (7, 9)), ('words', (10, 11)), ('frequency', (14, 15)), ('words', (16, 17)), ('synset', (19, 20))]","[['non-uniform prior', 'for', 'synset distribution'], ['synset distribution', 'over', 'words'], ['words', 'to model', 'frequency'], ['frequency', 'of', 'words'], ['words', 'within', 'synset']]",[],"[['Model', 'use', 'non-uniform prior']]",[],entity_linking,14,25
1009,model,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .","[('model', (4, 5)), ('between', (7, 8)), ('using', (10, 11)), ('for drawing', (14, 16)), ('of', (19, 20))]","[('relationships', (6, 7)), ('synsets', (8, 9)), ('logisticnormal prior', (12, 14)), ('synset proportions', (17, 19)), ('document', (21, 22))]","[['relationships', 'between', 'synsets'], ['relationships', 'using', 'logisticnormal prior'], ['synsets', 'using', 'logisticnormal prior'], ['logisticnormal prior', 'for drawing', 'synset proportions'], ['synset proportions', 'of', 'document']]",[],"[['Model', 'model', 'relationships']]",[],entity_linking,14,26
1010,results,"The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .","[('denoted by', (4, 6)), ('outperforms', (19, 20)), ('by', (30, 31)), ('achieving', (40, 41)), ('of', (46, 47)), ('compared to', (49, 51)), ('of', (54, 55))]","[('proposed method', (1, 3)), ('WSD - TM', (6, 9)), ('state - of - the - art WSD system', (21, 30)), ('significant margin', (32, 34)), ('overall F1 - score', (42, 46)), ('66.9', (47, 48)), (""Moro14 's score"", (51, 54)), ('65.5', (55, 56))]","[['proposed method', 'denoted by', 'WSD - TM'], ['proposed method', 'outperforms', 'state - of - the - art WSD system'], ['state - of - the - art WSD system', 'by', 'significant margin'], ['state - of - the - art WSD system', 'achieving', 'overall F1 - score'], ['overall F1 - score', 'of', '66.9'], [""Moro14 's score"", 'of', '65.5'], [""Moro14 's score"", 'of', '65.5']]","[['proposed method', 'name', 'WSD - TM']]",[],"[['Results', 'has', 'proposed method']]",entity_linking,14,169
1011,results,"We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .","[('observe that', (2, 4)), ('of', (6, 7)), ('than', (14, 15))]","[('performance', (5, 6)), ('proposed model', (8, 10)), ('not much worse', (11, 14)), ('best supervised system', (16, 19))]","[['performance', 'of', 'proposed model'], ['not much worse', 'than', 'best supervised system']]","[['performance', 'has', 'proposed model']]",[],[],entity_linking,14,170
1012,results,The proposed system outperforms all previous knowledgebased systems overall parts of speech .,"[('outperforms', (3, 4)), ('overall', (8, 9))]","[('proposed system', (1, 3)), ('all previous knowledgebased systems', (4, 8)), ('parts of speech', (9, 12))]","[['proposed system', 'outperforms', 'all previous knowledgebased systems'], ['all previous knowledgebased systems', 'overall', 'parts of speech']]",[],[],"[['Results', 'has', 'proposed system']]",entity_linking,14,172
1013,research-problem,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,[],"[('Entity Linking', (5, 7))]",[],[],[],[],entity_linking,15,2
1014,research-problem,"The first stage for every QA approach is entity linking ( EL ) , that is the identification of entity mentions in the question and linking them to entities in KB .",[],"[('entity linking ( EL )', (8, 13))]",[],[],[],[],entity_linking,15,11
1015,research-problem,The state - of - the - art QA systems usually rely on off - the - shelf EL systems to extract entities from the question .,[],"[('EL', (18, 19))]",[],[],[],[],entity_linking,15,14
1016,model,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .","[('perform', (16, 17)), ('in', (24, 25)), ('makes', (30, 31))]","[('entity mention detection and entity disambiguation', (17, 23)), ('jointly', (23, 24)), ('single neural model', (26, 29)), ('whole process', (32, 34)), ('end - to - end differentiable', (34, 40))]","[['jointly', 'in', 'single neural model'], ['single neural model', 'makes', 'whole process']]","[['entity mention detection and entity disambiguation', 'has', 'jointly'], ['whole process', 'has', 'end - to - end differentiable']]","[['Model', 'perform', 'entity mention detection and entity disambiguation']]",[],entity_linking,15,27
1017,model,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .","[('To overcome', (0, 2)), ('in', (4, 5)), ('automatically learn', (9, 11)), ('over', (12, 13)), ('of', (15, 16)), ('of', (17, 18))]","[('noise', (3, 4)), ('data', (6, 7)), ('features', (11, 12)), ('set', (14, 15)), ('contexts', (16, 17)), ('different granularity levels', (18, 21))]","[['noise', 'in', 'data'], ['noise', 'automatically learn', 'features'], ['features', 'over', 'set'], ['set', 'of', 'contexts'], ['contexts', 'of', 'different granularity levels'], ['contexts', 'of', 'different granularity levels']]","[['noise', 'has', 'data']]","[['Model', 'To overcome', 'noise']]",[],entity_linking,15,29
1018,model,Each level of granularity is handled by a separate component of the model .,"[('handled by', (5, 7)), ('of', (10, 11))]","[('Each level of granularity', (0, 4)), ('separate component', (8, 10)), ('model', (12, 13))]","[['Each level of granularity', 'handled by', 'separate component'], ['separate component', 'of', 'model']]",[],[],"[['Model', 'has', 'Each level of granularity']]",entity_linking,15,30
1019,model,"A token - level component extracts higher - level features from the whole question context , whereas a character - level component builds lower - level features for the candidate n-gram .","[('extracts', (5, 6)), ('from', (10, 11)), ('builds', (22, 23)), ('for', (27, 28))]","[('token - level component', (1, 5)), ('higher - level features', (6, 10)), ('whole question context', (12, 15)), ('character - level component', (18, 22)), ('lower - level features', (23, 27)), ('candidate n-gram', (29, 31))]","[['token - level component', 'extracts', 'higher - level features'], ['higher - level features', 'from', 'whole question context'], ['character - level component', 'builds', 'lower - level features'], ['lower - level features', 'for', 'candidate n-gram']]",[],[],"[['Model', 'has', 'token - level component']]",entity_linking,15,31
1020,model,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .","[('extract', (3, 4)), ('from', (5, 6)), ('of', (10, 11)), ('extracted for', (20, 22)), ('based on', (32, 34)), ('surrounding', (36, 37)), ('in', (40, 41))]","[('features', (4, 5)), ('knowledge base context', (7, 10)), ('candidate entity', (12, 14)), ('character - level features', (15, 19)), ('entity label', (23, 25)), ('higher - level features', (26, 30)), ('entities', (35, 36)), ('candidate entity', (38, 40)), ('knowledge graph', (42, 44))]","[['features', 'from', 'knowledge base context'], ['knowledge base context', 'of', 'candidate entity'], ['character - level features', 'extracted for', 'entity label'], ['entities', 'surrounding', 'candidate entity'], ['candidate entity', 'in', 'knowledge graph']]",[],"[['Model', 'extract', 'features']]",[],entity_linking,15,32
1021,model,This information is aggregated and used to predict whether the n-gram is an entity mention and to what entity it should be linked .,"[('is', (2, 3)), ('used', (5, 6)), ('to predict', (6, 8)), ('should be', (20, 22))]","[('aggregated', (3, 4)), ('n-gram', (10, 11)), ('entity mention', (13, 15)), ('to what entity', (16, 19)), ('linked', (22, 23))]","[['n-gram', 'is', 'entity mention'], ['to what entity', 'should be', 'linked']]","[['to what entity', 'has', 'linked']]",[],[],entity_linking,15,33
1022,code,The complete code as well as the scripts that produce the evaluation data can be found here : https://github.com/UKPLab/ starsem2018-entity-linking .,[],"[('https://github.com/UKPLab/ starsem2018-entity-linking', (18, 20))]",[],[],[],[],entity_linking,15,42
1023,experiments,Existing systems,[],"[('Existing systems', (0, 2))]",[],[],[],[],entity_linking,15,212
1024,baselines,"In our experiments , we compare to DBPedia Spotlight that was used in several QA systems and represents a strong baseline for entity linking 4 .","[('compare', (5, 6)), ('used in', (11, 13)), ('represents', (17, 18)), ('for', (21, 22))]","[('DBPedia Spotlight', (7, 9)), ('several QA systems', (13, 16)), ('strong baseline', (19, 21)), ('entity linking', (22, 24))]","[['DBPedia Spotlight', 'used in', 'several QA systems'], ['DBPedia Spotlight', 'represents', 'strong baseline'], ['strong baseline', 'for', 'entity linking']]",[],"[['Baselines', 'compare', 'DBPedia Spotlight']]",[],entity_linking,15,213
1025,results,"In addition , we are able to compare to the state - of - the - art S - MART system , since their output on the WebQSP datasets was publicly released 5 .",[],"[('S - MART system', (17, 21))]",[],[],[],[],entity_linking,15,214
1026,baselines,We also include a heuristics baseline that ranks candidate entities according to their frequency in Wikipedia .,"[('include', (2, 3)), ('ranks', (7, 8))]","[('heuristics baseline', (4, 6)), ('candidate entities', (8, 10))]","[['heuristics baseline', 'ranks', 'candidate entities']]",[],"[['Baselines', 'include', 'heuristics baseline']]",[],entity_linking,15,216
1027,baselines,Simplified VCG,[],"[('Simplified VCG', (0, 2))]",[],[],[],"[['Baselines', 'has', 'Simplified VCG']]",entity_linking,15,218
1028,hyperparameters,"In particular , we employ features that cover ( 1 ) frequency of the entity in Wikipedia , ( 2 ) edit distance between the label of the entity and the token n-gram , ( 3 ) number of entities and relations immediately connected to the entity in the KB , ( 4 ) word overlap between the input question and the labels of the connected entities and relations , ( 5 ) length of the n-gram .","[('employ', (4, 5)), ('of', (12, 13)), ('in', (15, 16)), ('between', (23, 24)), ('of', (26, 27)), ('connected to', (43, 45)), ('in', (47, 48)), ('between', (56, 57)), ('of', (63, 64)), ('of', (74, 75))]","[('features', (5, 6)), ('that cover', (6, 8)), ('frequency', (11, 12)), ('entity', (14, 15)), ('Wikipedia', (16, 17)), ('edit distance', (21, 23)), ('label', (25, 26)), ('entity', (28, 29)), ('token n-gram', (31, 33)), ('number of entities and relations', (37, 42)), ('entity', (46, 47)), ('KB', (49, 50)), ('word overlap', (54, 56)), ('input question', (58, 60)), ('labels', (62, 63)), ('connected entities and relations', (65, 69)), ('length', (73, 74)), ('n-gram', (76, 77))]","[['frequency', 'of', 'entity'], ['label', 'of', 'entity'], ['labels', 'of', 'connected entities and relations'], ['entity', 'in', 'Wikipedia'], ['edit distance', 'between', 'label'], ['edit distance', 'between', 'token n-gram'], ['label', 'of', 'entity'], ['label', 'of', 'token n-gram'], ['number of entities and relations', 'connected to', 'entity'], ['entity', 'in', 'KB'], ['word overlap', 'between', 'input question'], ['word overlap', 'between', 'length'], ['labels', 'of', 'connected entities and relations'], ['length', 'of', 'n-gram']]","[['features', 'has', 'that cover'], ['that cover', 'has', 'frequency']]","[['Hyperparameters', 'employ', 'features']]",[],entity_linking,15,220
1029,results,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,"[('shows', (3, 4)), ('than', (12, 13)), ('by', (17, 18))]","[('VCG model', (1, 3)), ('overall F- score result', (5, 9)), ('better', (11, 12)), ('DBPedia Spotlight baseline', (14, 17)), ('wide margin', (19, 21))]","[['VCG model', 'shows', 'overall F- score result'], ['better', 'than', 'DBPedia Spotlight baseline'], ['better', 'by', 'wide margin'], ['DBPedia Spotlight baseline', 'by', 'wide margin']]","[['VCG model', 'has', 'overall F- score result'], ['overall F- score result', 'has', 'better']]",[],"[['Results', 'has', 'VCG model']]",entity_linking,15,238
1030,results,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,"[('achieves', (7, 8)), ('compared to', (12, 14)), ('keep', (19, 20)), ('of', (23, 24))]","[('our model', (5, 7)), ('higher precision values', (8, 11)), ('other approaches', (14, 16)), ('satisfactory level', (21, 23)), ('recall', (24, 25))]","[['our model', 'achieves', 'higher precision values'], ['higher precision values', 'compared to', 'other approaches'], ['higher precision values', 'keep', 'satisfactory level'], ['satisfactory level', 'of', 'recall']]",[],[],"[['Results', 'has', 'our model']]",entity_linking,15,239
1031,research-problem,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,[],"[('Word Sense Disambiguation', (7, 10))]",[],[],[],[],entity_linking,16,2
1032,research-problem,"To mine these data properly , attributable to their innate ambiguity , a Word Sense Disambiguation ( WSD ) algorithm can avoid numbers of difficulties in Natural Language Processing ( NLP ) pipeline .",[],"[('Word Sense Disambiguation ( WSD )', (13, 19))]",[],[],[],[],entity_linking,16,5
1033,research-problem,"However , considering a large number of ambiguous words in one language or technical domain , we may encounter limiting constraints for proper deployment of existing WSD models .",[],"[('WSD', (26, 27))]",[],[],[],[],entity_linking,16,6
1034,model,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .","[('develop', (5, 6)), ('leverages', (11, 12))]","[('our supervised WSD model', (6, 10)), ('Bidirectional Long Short - Term Memory ( BLSTM ) network', (13, 23))]","[['our supervised WSD model', 'leverages', 'Bidirectional Long Short - Term Memory ( BLSTM ) network']]",[],"[['Model', 'develop', 'our supervised WSD model']]",[],entity_linking,16,16
1035,model,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .","[('works with', (2, 4)), ('i.e.', (8, 9)), ('are', (14, 15)), ('during', (16, 17)), ('employs', (21, 22)), ('i.e.', (26, 27)), ('are', (32, 33)), ('through', (34, 35)), ('called', (40, 41)), ('for', (45, 46))]","[('neural sense vectors', (4, 7)), ('sense embeddings', (9, 11)), ('learned', (15, 16)), ('model training', (17, 19)), ('neural word vectors', (22, 25)), ('word embeddings', (27, 29)), ('learned', (33, 34)), ('unsupervised deep learning approach', (36, 40)), ('GloVe', (41, 42)), ('context words', (51, 53))]","[['neural sense vectors', 'i.e.', 'sense embeddings'], ['neural sense vectors', 'are', 'learned'], ['neural word vectors', 'are', 'learned'], ['learned', 'during', 'model training'], ['neural word vectors', 'i.e.', 'word embeddings'], ['neural word vectors', 'are', 'learned'], ['learned', 'through', 'unsupervised deep learning approach'], ['unsupervised deep learning approach', 'called', 'GloVe']]","[['neural word vectors', 'name', 'word embeddings']]","[['Model', 'works with', 'neural sense vectors']]",[],entity_linking,16,17
1036,results,Between - all - models comparisons,[],"[('Between - all - models comparisons', (0, 6))]",[],[],[],"[['Results', 'has', 'Between - all - models comparisons']]",entity_linking,16,110
1037,results,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .","[('show', (1, 2)), ('sits among', (5, 7))]","[('our single model', (2, 5)), ('5 top - performing algorithms', (8, 13))]","[['our single model', 'sits among', '5 top - performing algorithms']]",[],"[['Results', 'show', 'our single model']]",[],entity_linking,16,114
1038,results,shows the results of the top - performing and low - performing supervised algorithms .,"[('shows', (0, 1)), ('of', (3, 4))]","[('results', (2, 3)), ('top - performing and low - performing supervised algorithms', (5, 14))]","[['results', 'of', 'top - performing and low - performing supervised algorithms']]","[['results', 'has', 'top - performing and low - performing supervised algorithms']]","[['Results', 'shows', 'results']]",[],entity_linking,16,115
1039,results,Within - our - model comparisons,[],"[('Within - our - model comparisons', (0, 6))]",[],[],[],"[['Results', 'has', 'Within - our - model comparisons']]",entity_linking,16,128
1040,results,"We observe if reverse the sequential follow of information into our Bidirectional LSTM , we shuffle the order of the context words , or even replace our Bidirectional LSTMs with two different fully - connected networks of the same size 50 ( the size of the LSTMs outputs ) , the achieved results were notably less than 72.5 % .","[('into', (9, 10)), ('shuffle', (15, 16)), ('of', (18, 19)), ('replace', (25, 26)), ('with', (29, 30)), ('of', (36, 37)), ('achieved', (51, 52))]","[('reverse', (3, 4)), ('sequential follow of information', (5, 9)), ('our Bidirectional LSTM', (10, 13)), ('order', (17, 18)), ('context words', (20, 22)), ('our Bidirectional LSTMs', (26, 29)), ('two different fully - connected networks', (30, 36)), ('same size 50', (38, 41)), ('results', (52, 53)), ('notably less than 72.5 %', (54, 59))]","[['sequential follow of information', 'into', 'our Bidirectional LSTM'], ['our Bidirectional LSTM', 'shuffle', 'order'], ['order', 'of', 'context words'], ['our Bidirectional LSTMs', 'with', 'two different fully - connected networks'], ['two different fully - connected networks', 'of', 'same size 50']]","[['reverse', 'has', 'sequential follow of information']]",[],[],entity_linking,16,136
1041,research-problem,Neural Sequence Learning Models for Word Sense Disambiguation,[],"[('Word Sense Disambiguation', (5, 8))]",[],[],[],[],entity_linking,2,2
1042,research-problem,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .",[],"[('WSD', (20, 21))]",[],[],[],[],entity_linking,2,9
1043,model,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .","[('of', (26, 27)), ('aim at', (38, 40)), ('modeling', (40, 41)), ('as', (48, 49)), ('in terms of', (51, 54))]","[('joint disambiguation', (42, 44)), ('target text', (46, 48)), ('whole', (50, 51)), ('sequence labeling problem', (55, 58))]","[['whole', 'in terms of', 'sequence labeling problem']]",[],"[['Model', 'of', 'joint disambiguation']]",[],entity_linking,2,18
1044,model,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .","[('design , analyze and compare', (6, 11)), ('of', (15, 16)), ('ranging from', (19, 21)), ('to', (29, 30))]","[('experimentally', (11, 12)), ('various neural architectures', (12, 15)), ('different complexities', (16, 18)), ('single bidirectional Long Short - Term Memory', (22, 29)), ('sequence - tosequence approach', (31, 35))]","[['various neural architectures', 'of', 'different complexities'], ['different complexities', 'ranging from', 'single bidirectional Long Short - Term Memory'], ['single bidirectional Long Short - Term Memory', 'to', 'sequence - tosequence approach']]","[['experimentally', 'has', 'various neural architectures']]","[['Model', 'design , analyze and compare', 'experimentally']]",[],entity_linking,2,20
1045,model,"Each architecture reflects a particular way of modeling the disambiguation problem , but they all share some key features that set them apart from previous supervised approaches to WSD : they are trained end - to - end from sense - annotated text to sense labels , and learn a single all - words model from the training data , without fine tuning or explicit engineering of local features .","[('reflects', (2, 3)), ('of', (6, 7)), ('from', (23, 24)), ('to', (27, 28)), ('trained', (32, 33)), ('from', (38, 39)), ('learn', (48, 49)), ('without', (60, 61)), ('of', (66, 67))]","[('Each architecture', (0, 2)), ('particular way', (4, 6)), ('modeling', (7, 8)), ('disambiguation problem', (9, 11)), ('end - to - end', (33, 38)), ('sense - annotated text', (39, 43)), ('sense labels', (44, 46)), ('single all - words model', (50, 55)), ('training data', (57, 59)), ('fine tuning', (61, 63)), ('explicit engineering', (64, 66)), ('local features', (67, 69))]","[['Each architecture', 'reflects', 'particular way'], ['particular way', 'of', 'modeling'], ['explicit engineering', 'of', 'local features'], ['end - to - end', 'from', 'sense - annotated text'], ['Each architecture', 'learn', 'single all - words model'], ['Each architecture', 'without', 'fine tuning'], ['explicit engineering', 'of', 'local features']]","[['Each architecture', 'has', 'particular way'], ['modeling', 'has', 'disambiguation problem']]",[],"[['Model', 'has', 'Each architecture']]",entity_linking,2,21
1046,hyperparameters,"To set a level playing field with comparison systems on English all - words WSD , we followed and , for all our models , we used a layer of word embeddings pre-trained 8 on the English uk WaC corpus as initialization , and kept them fixed during the training process .","[('used', (26, 27)), ('pre-trained 8 on', (32, 35)), ('as', (40, 41)), ('kept', (44, 45)), ('during', (47, 48))]","[('layer of word embeddings', (28, 32)), ('English uk WaC corpus', (36, 40)), ('initialization', (41, 42)), ('fixed', (46, 47)), ('training process', (49, 51))]","[['layer of word embeddings', 'pre-trained 8 on', 'English uk WaC corpus'], ['English uk WaC corpus', 'as', 'initialization'], ['layer of word embeddings', 'kept', 'fixed']]",[],"[['Hyperparameters', 'used', 'layer of word embeddings']]",[],entity_linking,2,144
1047,hyperparameters,For all architectures we then employed 2 layers of bidirectional LSTM with 2048 hidden units ( 1024 units per direction ) .,"[('For', (0, 1)), ('employed', (5, 6)), ('of', (8, 9)), ('with', (11, 12))]","[('all architectures', (1, 3)), ('2 layers', (6, 8)), ('bidirectional LSTM', (9, 11)), ('2048 hidden units', (12, 15)), ('1024 units', (16, 18)), ('per direction', (18, 20))]","[['all architectures', 'employed', '2 layers'], ['2 layers', 'of', 'bidirectional LSTM'], ['bidirectional LSTM', 'with', '2048 hidden units']]","[['2048 hidden units', 'has', '1024 units'], ['1024 units', 'has', 'per direction']]","[['Hyperparameters', 'For', 'all architectures']]",[],entity_linking,2,145
1048,results,"We report the F1 - score on each in - dividual test set , as well as the F1- score obtained on the concatenation of all four test sets , divided by part - of - speech tag .","[('report', (1, 2)), ('on', (6, 7)), ('obtained on', (20, 22)), ('of', (24, 25)), ('divided by', (30, 32))]","[('F1 - score', (3, 6)), ('each in - dividual test set', (7, 13)), ('concatenation', (23, 24)), ('all four test sets', (25, 29)), ('part - of - speech tag', (32, 38))]","[['F1 - score', 'on', 'each in - dividual test set'], ['concatenation', 'of', 'all four test sets']]",[],"[['Results', 'report', 'F1 - score']]",[],entity_linking,2,155
1049,hyperparameters,"As supervised systems , we considered Context2 Vec and It Makes Sense , both the original implementation and the best configuration reported by , which also integrates word embeddings using exponential decay .","[('considered', (5, 6)), ('integrates', (26, 27)), ('using', (29, 30))]","[('Context2 Vec and It Makes Sense', (6, 12)), ('original implementation', (15, 17)), ('best configuration', (19, 21)), ('word embeddings', (27, 29)), ('exponential decay', (30, 32))]","[['best configuration', 'integrates', 'word embeddings'], ['word embeddings', 'using', 'exponential decay']]","[['Context2 Vec and It Makes Sense', 'has', 'original implementation']]","[['Hyperparameters', 'considered', 'Context2 Vec and It Makes Sense']]",[],entity_linking,2,157
1050,results,"11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .","[('achieved', (7, 8)), ('to', (31, 32)), ('in', (36, 37)), ('with', (43, 44)), ('tuned over', (46, 48))]","[('both BLSTM and Seq2Seq', (3, 7)), ('results', (8, 9)), ('state - of - the - art', (12, 19)), ('statistically equivalent', (20, 22)), ('best supervised system', (33, 36)), ('each benchmark', (37, 39)), ('performing', (40, 41)), ('on par', (41, 43)), ('word experts', (44, 46)), ('explicitly engineered features', (48, 51))]","[['both BLSTM and Seq2Seq', 'achieved', 'results'], ['best supervised system', 'in', 'each benchmark'], ['on par', 'with', 'word experts'], ['word experts', 'tuned over', 'explicitly engineered features']]","[['performing', 'has', 'on par']]",[],"[['Results', 'has', 'both BLSTM and Seq2Seq']]",entity_linking,2,161
1051,results,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .","[('tended consistently', (5, 7))]","[('BLSTM models', (3, 5)), ('outperform', (8, 9)), ('Seq2Seq counterparts', (10, 12))]","[['BLSTM models', 'tended consistently', 'outperform']]","[['BLSTM models', 'has', 'outperform'], ['outperform', 'has', 'Seq2Seq counterparts']]",[],[],entity_linking,2,162
1052,results,English All - words WSD,[],"[('English All - words WSD', (0, 5))]",[],[],[],"[['Results', 'has', 'English All - words WSD']]",entity_linking,2,164
1053,results,"It is worth noting that RNN - based architectures outperformed classical supervised approaches when dealing with verbs , which are shown to be highly ambiguous .","[('worth noting', (2, 4)), ('outperformed', (9, 10)), ('dealing with', (14, 16))]","[('RNN - based architectures', (5, 9)), ('classical supervised approaches', (10, 13)), ('verbs', (16, 17))]","[['RNN - based architectures', 'outperformed', 'classical supervised approaches'], ['classical supervised approaches', 'dealing with', 'verbs']]",[],"[['Results', 'worth noting', 'RNN - based architectures']]",[],entity_linking,2,169
1054,results,"Both BLSTM and Seq2Seq outperformed UKB and IMS trained on SemCor , as well as recent supervised approaches based on distributional semantics and neural architectures .","[('outperformed', (4, 5)), ('trained on', (8, 10)), ('as well as', (12, 15)), ('based on', (18, 20))]","[('Both BLSTM and Seq2Seq', (0, 4)), ('UKB and IMS', (5, 8)), ('SemCor', (10, 11)), ('recent supervised approaches', (15, 18)), ('distributional semantics', (20, 22)), ('neural architectures', (23, 25))]","[['Both BLSTM and Seq2Seq', 'outperformed', 'UKB and IMS'], ['UKB and IMS', 'trained on', 'SemCor'], ['recent supervised approaches', 'based on', 'distributional semantics'], ['recent supervised approaches', 'based on', 'neural architectures']]",[],[],[],entity_linking,2,171
1055,results,Multilingual All - words WSD,[],"[('Multilingual All - words WSD', (0, 5))]",[],[],[],"[['Results', 'has', 'Multilingual All - words WSD']]",entity_linking,2,172
1056,results,"F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .","[('show', (4, 5)), ('consistently outperformed', (19, 21)), ('achieved', (25, 26)), ('with', (30, 31)), ('in', (35, 36))]","[('F - score figures', (0, 4)), ('bilingual and multilingual models', (6, 10)), ('MFS baseline', (22, 24)), ('results', (26, 27)), ('competitive', (29, 30)), ('best participating systems', (32, 35)), ('task', (37, 38))]","[['F - score figures', 'show', 'bilingual and multilingual models'], ['bilingual and multilingual models', 'consistently outperformed', 'MFS baseline'], ['bilingual and multilingual models', 'achieved', 'results'], ['competitive', 'with', 'best participating systems'], ['best participating systems', 'in', 'task']]",[],[],"[['Results', 'has', 'F - score figures']]",entity_linking,2,179
1057,results,"We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .","[('note', (2, 3)), ('when', (18, 19)), ('from', (20, 21))]","[('overall F- score performance', (5, 9)), ('did not change substantially', (9, 13)), ('moving', (19, 20)), ('bilingual to multilingual models', (21, 25))]","[['moving', 'from', 'bilingual to multilingual models']]","[['overall F- score performance', 'has', 'did not change substantially']]","[['Results', 'note', 'overall F- score performance']]",[],entity_linking,2,180
1058,research-problem,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings,[],"[('Word Sense Disambiguation', (7, 10))]",[],[],[],[],entity_linking,3,2
1059,research-problem,"Since vectors of the same word type can vary depending on the respective context , they implicitly provide a model for word sense disambiguation ( WSD ) .",[],"[('word sense disambiguation ( WSD )', (21, 27))]",[],[],[],[],entity_linking,3,7
1060,research-problem,We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs .,[],"[('WSD', (8, 9))]",[],[],[],[],entity_linking,3,8
1061,research-problem,We show that CWEs can be utilized directly to approach the WSD task due to their nature of providing distinct vector representations for the same token depending on its context .,"[('show that', (1, 3)), ('utilized', (6, 7)), ('to approach', (8, 10))]","[('CWEs', (3, 4)), ('directly', (7, 8)), ('WSD task', (11, 13))]","[['CWEs', 'utilized', 'directly'], ['CWEs', 'to approach', 'WSD task'], ['directly', 'to approach', 'WSD task']]","[['CWEs', 'has', 'directly']]","[['Research problem', 'show that', 'CWEs']]",[],entity_linking,3,49
1062,model,"To learn the semantic capabilities of CWEs , we employ a simple , yet interpretable approach to WSD using a k -nearest neighbor classification ( kNN ) approach .","[('To learn', (0, 2)), ('of', (5, 6)), ('employ', (9, 10)), ('to', (16, 17)), ('using', (18, 19))]","[('semantic capabilities', (3, 5)), ('CWEs', (6, 7)), ('simple , yet interpretable approach', (11, 16)), ('WSD', (17, 18)), ('k -nearest neighbor classification ( kNN ) approach', (20, 28))]","[['semantic capabilities', 'of', 'CWEs'], ['semantic capabilities', 'employ', 'simple , yet interpretable approach'], ['simple , yet interpretable approach', 'to', 'WSD'], ['WSD', 'using', 'k -nearest neighbor classification ( kNN ) approach']]","[['semantic capabilities', 'has', 'CWEs']]","[['Model', 'To learn', 'semantic capabilities']]",[],entity_linking,3,50
1063,results,Contextualized Embeddings,[],"[('Contextualized Embeddings', (0, 2))]",[],[],[],"[['Results', 'has', 'Contextualized Embeddings']]",entity_linking,3,134
1064,results,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,"[('with', (3, 4)), ('as well as', (5, 8)), ('beats', (10, 11)), ('of', (16, 17))]","[('Simple k NN', (0, 3)), ('ELMo', (4, 5)), ('BERT embeddings', (8, 10)), ('state of the art', (12, 16)), ('lexical sample task', (18, 21))]","[['Simple k NN', 'with', 'ELMo'], ['ELMo', 'as well as', 'BERT embeddings'], ['BERT embeddings', 'beats', 'state of the art'], ['state of the art', 'of', 'lexical sample task']]",[],[],[],entity_linking,3,137
1065,results,BERT also outperforms all others on the SE - 3 task .,"[('on', (5, 6))]","[('BERT', (0, 1)), ('outperforms', (2, 3)), ('all others', (3, 5)), ('SE - 3 task', (7, 11))]","[['outperforms', 'on', 'SE - 3 task'], ['all others', 'on', 'SE - 3 task']]","[['BERT', 'has', 'outperforms'], ['outperforms', 'has', 'all others']]",[],[],entity_linking,3,138
1066,results,"However , we observe a major performance drop of our approach for the two all - words WSD tasks in which no training data is provided along with the test set .","[('observe', (3, 4)), ('of', (8, 9)), ('for', (11, 12)), ('provided', (25, 26)), ('along with', (26, 28))]","[('major performance drop', (5, 8)), ('our approach', (9, 11)), ('two all - words WSD tasks', (13, 19)), ('no training data', (21, 24)), ('test set', (29, 31))]","[['major performance drop', 'of', 'our approach'], ['major performance drop', 'for', 'two all - words WSD tasks'], ['our approach', 'for', 'two all - words WSD tasks'], ['no training data', 'along with', 'test set']]",[],"[['Results', 'observe', 'major performance drop']]",[],entity_linking,3,139
1067,results,"In fact , similarity of contextualized embeddings largely relies on semantically and structurally similar sentence contexts of polysemic target words .","[('of', (4, 5)), ('on', (9, 10)), ('of', (16, 17))]","[('similarity', (3, 4)), ('contextualized embeddings', (5, 7)), ('largely relies', (7, 9)), ('semantically and structurally similar sentence contexts', (10, 16)), ('polysemic target words', (17, 20))]","[['similarity', 'of', 'contextualized embeddings'], ['semantically and structurally similar sentence contexts', 'of', 'polysemic target words'], ['contextualized embeddings', 'on', 'semantically and structurally similar sentence contexts'], ['largely relies', 'on', 'semantically and structurally similar sentence contexts'], ['semantically and structurally similar sentence contexts', 'of', 'polysemic target words']]","[['contextualized embeddings', 'has', 'largely relies']]",[],"[['Results', 'has', 'similarity']]",entity_linking,3,141
1068,results,"As can be seen in , the SE - 2 and SE - 3 training datasets provide more CWEs for each word and sense , and our approach performs better with a growing number of CWEs , even with a higher average number of senses per word as is the casein SE - 3 .","[('provide', (16, 17)), ('for', (19, 20)), ('performs', (28, 29)), ('with', (30, 31)), ('of', (34, 35))]","[('SE - 2 and SE - 3 training datasets', (7, 16)), ('more CWEs', (17, 19)), ('each word and sense', (20, 24)), ('our approach', (26, 28)), ('better', (29, 30)), ('growing number', (32, 34)), ('CWEs', (35, 36))]","[['SE - 2 and SE - 3 training datasets', 'provide', 'more CWEs'], ['more CWEs', 'for', 'each word and sense'], ['our approach', 'performs', 'better'], ['better', 'with', 'growing number'], ['growing number', 'of', 'CWEs']]",[],[],[],entity_linking,3,146
1069,results,"Moreover , CWEs actually do not organize well in spherically shaped form in the embedding space .","[('in', (8, 9)), ('in', (12, 13))]","[('CWEs', (2, 3)), ('not organize well', (5, 8)), ('spherically shaped form', (9, 12)), ('embedding space', (14, 16))]","[['not organize well', 'in', 'spherically shaped form'], ['spherically shaped form', 'in', 'embedding space'], ['spherically shaped form', 'in', 'embedding space']]","[['CWEs', 'has', 'not organize well']]",[],"[['Results', 'has', 'CWEs']]",entity_linking,3,149
1070,results,Nearest Neighbors,[],"[('Nearest Neighbors', (0, 2))]",[],[],[],"[['Results', 'has', 'Nearest Neighbors']]",entity_linking,3,151
1071,results,K- Optimization :,[],"[('K- Optimization', (0, 2))]",[],[],[],"[['Results', 'has', 'K- Optimization']]",entity_linking,3,152
1072,results,"For SensEval - 2 and SensEval - 3 , we achieve a new state - of - the - art result .","[('For', (0, 1)), ('achieve', (10, 11))]","[('SensEval - 2 and SensEval - 3', (1, 8)), ('new state - of - the - art result', (12, 21))]","[['SensEval - 2 and SensEval - 3', 'achieve', 'new state - of - the - art result']]",[],"[['Results', 'For', 'SensEval - 2 and SensEval - 3']]",[],entity_linking,3,155
1073,results,"For the S7 - T * , we also achieve minor improvements with a higher k , but still drastically lack behind the state of the art .","[('achieve', (9, 10)), ('with', (12, 13)), ('drastically lack behind', (19, 22))]","[('S7 - T *', (2, 6)), ('minor improvements', (10, 12)), ('higher k', (14, 16)), ('state of the art', (23, 27))]","[['S7 - T *', 'achieve', 'minor improvements'], ['minor improvements', 'with', 'higher k'], ['minor improvements', 'drastically lack behind', 'state of the art']]",[],[],[],entity_linking,3,157
1074,results,Senses in CWE space :,[],"[('Senses in CWE space', (0, 4))]",[],[],[],"[['Results', 'has', 'Senses in CWE space']]",entity_linking,3,158
1075,results,We investigate how well different CWE models encode information such as distinguishable senses in their vector space .,"[('investigate', (1, 2)), ('encode', (7, 8)), ('such as', (9, 11)), ('in', (13, 14))]","[('different CWE models', (4, 7)), ('information', (8, 9)), ('distinguishable senses', (11, 13)), ('vector space', (15, 17))]","[['different CWE models', 'encode', 'information'], ['information', 'such as', 'distinguishable senses'], ['distinguishable senses', 'in', 'vector space']]",[],"[['Results', 'investigate', 'different CWE models']]",[],entity_linking,3,159
1076,results,The Flair embeddings hardly allow to distinguish any clusters as most senses are scattered across the entire plot .,"[('hardly allow', (3, 5)), ('to distinguish', (5, 7)), ('scattered across', (13, 15))]","[('Flair embeddings', (1, 3)), ('any clusters', (7, 9)), ('most senses', (10, 12)), ('entire plot', (16, 18))]","[['Flair embeddings', 'to distinguish', 'any clusters'], ['most senses', 'scattered across', 'entire plot']]",[],[],"[['Results', 'has', 'Flair embeddings']]",entity_linking,3,162
1077,results,"In the ELMo embedding space , the major senses are slightly more separated in different regions of the point cloud .","[('In', (0, 1)), ('in', (13, 14)), ('of', (16, 17))]","[('ELMo embedding space', (2, 5)), ('major senses', (7, 9)), ('slightly more separated', (10, 13)), ('different regions', (14, 16)), ('point cloud', (18, 20))]","[['slightly more separated', 'In', 'different regions'], ['slightly more separated', 'in', 'different regions'], ['different regions', 'of', 'point cloud']]","[['ELMo embedding space', 'has', 'major senses'], ['major senses', 'has', 'slightly more separated']]","[['Results', 'In', 'ELMo embedding space']]",[],entity_linking,3,163
1078,research-problem,Learning Distributed Representations of Texts and Entities from Knowledge Base,[],"[('Learning Distributed Representations of Texts and Entities from Knowledge Base', (0, 10))]",[],[],[],[],entity_linking,4,2
1079,model,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .","[('propose', (4, 5)), ('to', (18, 19))]","[('Neural Text - Entity Encoder ( NTEE )', (5, 13)), ('neural network model', (15, 18)), ('jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities', (19, 35))]","[['neural network model', 'to', 'jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities']]","[['Neural Text - Entity Encoder ( NTEE )', 'has', 'neural network model']]","[['Model', 'propose', 'Neural Text - Entity Encoder ( NTEE )']]",[],entity_linking,4,26
1080,model,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .","[('For', (0, 1)), ('in', (3, 4)), ('to', (10, 11)), ('predict', (11, 12)), ('places', (17, 18)), ('in', (28, 29))]","[('every text', (1, 3)), ('KB', (5, 6)), ('our model', (7, 9)), ('relevant entities', (13, 15)), ('text and the relevant entities', (19, 24)), ('close', (24, 25)), ('each other', (26, 28)), ('continuous vector space', (30, 33))]","[['every text', 'in', 'KB'], ['close', 'in', 'each other'], ['each other', 'in', 'continuous vector space'], ['close', 'to', 'each other'], ['our model', 'predict', 'relevant entities'], ['our model', 'places', 'text and the relevant entities'], ['each other', 'in', 'continuous vector space']]","[['every text', 'has', 'KB'], ['text and the relevant entities', 'has', 'close']]","[['Model', 'For', 'every text']]",[],entity_linking,4,27
1081,model,We use humanedited entity annotations obtained from Wikipedia ( see ) as supervised data of relevant entities to the texts containing these annotations .,"[('use', (1, 2)), ('obtained from', (5, 7)), ('as', (11, 12)), ('of', (14, 15)), ('to', (17, 18)), ('containing', (20, 21))]","[('humanedited entity annotations', (2, 5)), ('Wikipedia', (7, 8)), ('supervised data', (12, 14)), ('relevant entities', (15, 17)), ('texts', (19, 20)), ('these annotations', (21, 23))]","[['humanedited entity annotations', 'obtained from', 'Wikipedia'], ['humanedited entity annotations', 'as', 'supervised data'], ['supervised data', 'of', 'relevant entities'], ['relevant entities', 'to', 'texts'], ['texts', 'containing', 'these annotations']]",[],"[['Model', 'use', 'humanedited entity annotations']]",[],entity_linking,4,28
1082,model,"Note that , KB entities have been conventionally used to model semantics of texts .","[('conventionally used', (7, 9)), ('to model', (9, 11)), ('of', (12, 13))]","[('KB entities', (3, 5)), ('semantics', (11, 12)), ('texts', (13, 14))]","[['KB entities', 'to model', 'semantics'], ['semantics', 'of', 'texts']]",[],[],"[['Model', 'has', 'KB entities']]",entity_linking,4,29
1083,research-problem,"A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .","[('represents', (12, 13)), ('of', (15, 16)), ('using', (18, 19)), ('corresponds to', (27, 29)), ('of', (32, 33)), ('to', (35, 36))]","[('representative example', (1, 3)), ('Explicit Semantic Analysis ( ESA )', (4, 10)), ('semantics', (14, 15)), ('text', (17, 18)), ('sparse vector space', (20, 23)), ('each dimension', (25, 27)), ('relevance score', (30, 32)), ('text', (34, 35)), ('each entity', (36, 38))]","[['Explicit Semantic Analysis ( ESA )', 'represents', 'semantics'], ['semantics', 'of', 'text'], ['relevance score', 'of', 'text'], ['text', 'using', 'sparse vector space'], ['each dimension', 'corresponds to', 'relevance score'], ['relevance score', 'of', 'text'], ['text', 'to', 'each entity']]","[['representative example', 'has', 'Explicit Semantic Analysis ( ESA )']]",[],[],entity_linking,4,30
1084,research-problem,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .","[('shows', (3, 4)), ('accurately represented', (8, 10)), ('using', (10, 11)), ('of', (14, 15))]","[('ESA', (2, 3)), ('text', (5, 6)), ('small set', (12, 14)), ('relevant entities', (16, 18))]","[['ESA', 'shows', 'text'], ['small set', 'of', 'relevant entities']]",[],[],[],entity_linking,4,31
1085,model,"Furthermore , we also consider that placing texts and entities into the same vector space enables us to easily compute the similarity between texts and entities , which can be beneficial for various KB - related tasks .","[('into', (10, 11)), ('enables', (15, 16)), ('easily compute', (18, 20)), ('between', (22, 23))]","[('placing', (6, 7)), ('texts and entities', (7, 10)), ('same vector space', (12, 15)), ('similarity', (21, 22)), ('texts and entities', (23, 26))]","[['placing', 'into', 'same vector space'], ['texts and entities', 'into', 'same vector space'], ['placing', 'easily compute', 'similarity'], ['texts and entities', 'easily compute', 'similarity'], ['similarity', 'between', 'texts and entities']]","[['placing', 'has', 'texts and entities']]",[],[],entity_linking,4,35
1086,code,We release our code and trained models to the community at https://github.com/ studio-ousia /ntee to facilitate further academic research .,[],"[('https://github.com/ studio-ousia /ntee', (11, 14))]",[],[],[],[],entity_linking,4,54
1087,baselines,BOW is a conventional approach using a logistic regression ( LR ) classifier trained with binary BOW features to predict the correct answer .,"[('using', (5, 6)), ('trained with', (13, 15))]","[('BOW', (0, 1)), ('logistic regression ( LR ) classifier', (7, 13)), ('binary BOW features', (15, 18))]","[['logistic regression ( LR ) classifier', 'trained with', 'binary BOW features']]",[],[],"[['Baselines', 'has', 'BOW']]",entity_linking,4,125
1088,baselines,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,"[('based on', (4, 6)), ('augmented with', (9, 11)), ('with', (14, 15))]","[('BOW - DT', (0, 3)), ('BOW baseline', (7, 9)), ('feature set', (12, 14)), ('dependency relation indicators', (15, 18))]","[['BOW - DT', 'based on', 'BOW baseline'], ['BOW baseline', 'augmented with', 'feature set'], ['feature set', 'with', 'dependency relation indicators']]",[],[],"[['Baselines', 'has', 'BOW - DT']]",entity_linking,4,126
1089,baselines,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,"[('based on', (4, 6)), ('to derive', (10, 12)), ('of', (15, 16))]","[('QANTA', (0, 1)), ('recursive neural network', (7, 10)), ('distributed representations', (13, 15)), ('questions', (16, 17))]","[['QANTA', 'based on', 'recursive neural network'], ['recursive neural network', 'to derive', 'distributed representations'], ['distributed representations', 'of', 'questions']]",[],[],"[['Baselines', 'has', 'QANTA']]",entity_linking,4,127
1090,baselines,FTS - BRNN is based on the bidirectional recurrent neural network ( RNN ) with gated recurrent units ( GRU ) .,"[('based on', (4, 6)), ('with', (14, 15))]","[('FTS - BRNN', (0, 3)), ('bidirectional recurrent neural network ( RNN )', (7, 14)), ('gated recurrent units ( GRU )', (15, 21))]","[['FTS - BRNN', 'based on', 'bidirectional recurrent neural network ( RNN )'], ['bidirectional recurrent neural network ( RNN )', 'with', 'gated recurrent units ( GRU )']]",[],[],"[['Baselines', 'has', 'FTS - BRNN']]",entity_linking,4,129
1091,results,"In particular , despite the simplicity of the neural network architecture of our method compared to the state - of - the - art methods ( i.e. , QANTA and FTS - BRNN ) , our method clearly outperformed these methods .","[('compared to', (14, 16))]","[('our method', (12, 14)), ('state - of - the - art methods', (17, 25)), ('QANTA', (28, 29)), ('FTS - BRNN', (30, 33)), ('clearly outperformed', (37, 39))]","[['our method', 'compared to', 'state - of - the - art methods']]","[['state - of - the - art methods', 'name', 'QANTA']]",[],[],entity_linking,4,137
1092,results,"Our observations indicated that our method mostly performed perfect in terms of predicting the types of target answers ( e.g. , locations , events , and people ) .","[('in terms of', (9, 12)), ('of', (15, 16))]","[('our method', (4, 6)), ('mostly performed perfect', (6, 9)), ('predicting', (12, 13)), ('types', (14, 15)), ('target answers', (16, 18)), ('locations', (21, 22)), ('events', (23, 24)), ('people', (26, 27))]","[['mostly performed perfect', 'in terms of', 'predicting'], ['types', 'of', 'target answers']]","[['our method', 'has', 'mostly performed perfect'], ['predicting', 'has', 'types']]",[],"[['Results', 'has', 'our method']]",entity_linking,4,140
1093,research-problem,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,[],"[('Neural Word Sense Disambiguation', (8, 12))]",[],[],[],[],entity_linking,5,2
1094,research-problem,"In Word Sense Disambiguation ( WSD ) , the predominant approach generally involves a supervised system trained on sense annotated corpora .",[],"[('Word Sense Disambiguation ( WSD )', (1, 7))]",[],[],[],[],entity_linking,5,4
1095,research-problem,"Our method leads to state of the art results on most WSD evaluation tasks , while improving the coverage of supervised systems , reducing the training time and the size of the models , without additional training data .",[],"[('WSD', (11, 12))]",[],[],[],[],entity_linking,5,7
1096,approach,We propose a method for reducing the vocabulary of senses of Word Net by selecting the minimal set of senses required for differentiating the meaning of every word .,"[('propose', (1, 2)), ('for reducing', (4, 6)), ('of', (8, 9)), ('of', (10, 11)), ('by selecting', (13, 15)), ('of', (18, 19)), ('required for', (20, 22)), ('of', (25, 26))]","[('method', (3, 4)), ('vocabulary', (7, 8)), ('senses', (9, 10)), ('Word Net', (11, 13)), ('minimal set', (16, 18)), ('senses', (19, 20)), ('differentiating', (22, 23)), ('meaning', (24, 25)), ('every word', (26, 28))]","[['method', 'for reducing', 'vocabulary'], ['vocabulary', 'of', 'senses'], ['minimal set', 'of', 'senses'], ['meaning', 'of', 'every word'], ['senses', 'of', 'Word Net'], ['minimal set', 'of', 'senses'], ['meaning', 'of', 'every word'], ['minimal set', 'of', 'senses'], ['senses', 'required for', 'differentiating'], ['meaning', 'of', 'every word']]","[['differentiating', 'has', 'meaning']]","[['Approach', 'propose', 'method']]",[],entity_linking,5,27
1097,code,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,[],"[('https://github.com/getalp/disambiguate', (17, 18))]",[],[],[],[],entity_linking,5,31
1098,results,"In subsection 3.2 , we showed that our vocabulary reduction method improves the coverage of supervised systems overall WordNet vocabulary .","[('showed', (5, 6)), ('improves', (11, 12)), ('of', (14, 15)), ('overall', (17, 18))]","[('our vocabulary reduction method', (7, 11)), ('coverage', (13, 14)), ('supervised systems', (15, 17)), ('WordNet vocabulary', (18, 20))]","[['our vocabulary reduction method', 'improves', 'coverage'], ['coverage', 'of', 'supervised systems'], ['coverage', 'overall', 'WordNet vocabulary'], ['supervised systems', 'overall', 'WordNet vocabulary']]",[],"[['Results', 'showed', 'our vocabulary reduction method']]",[],entity_linking,5,181
1099,results,"In , we can see that this coverage improvement holds true on the evaluation tasks , for both training sets .","[('see that', (4, 6)), ('holds', (9, 10)), ('on', (11, 12)), ('for', (16, 17))]","[('coverage improvement', (7, 9)), ('true', (10, 11)), ('evaluation tasks', (13, 15)), ('both training sets', (17, 20))]","[['coverage improvement', 'holds', 'true'], ['coverage improvement', 'on', 'evaluation tasks'], ['true', 'on', 'evaluation tasks'], ['evaluation tasks', 'for', 'both training sets']]","[['coverage improvement', 'has', 'true']]","[['Results', 'see that', 'coverage improvement']]",[],entity_linking,5,182
1100,results,"Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .","[('obtained by', (13, 15)), ('using', (17, 18))]","[('difference of scores', (10, 13)), ('our system', (15, 17)), ('sense vocabulary reduction or not', (19, 24)), ('not significant', (26, 28))]","[['difference of scores', 'obtained by', 'our system'], ['difference of scores', 'using', 'sense vocabulary reduction or not'], ['our system', 'using', 'sense vocabulary reduction or not']]","[['difference of scores', 'has', 'our system']]",[],[],entity_linking,5,185
1101,results,"However we can notice a very large gap on the SemEval 2013 task , especially when the SemCor is used alone for training .","[('notice', (3, 4)), ('on', (8, 9)), ('when', (15, 16)), ('used', (19, 20)), ('for', (21, 22))]","[('very large gap', (5, 8)), ('SemEval 2013 task', (10, 13)), ('SemCor', (17, 18)), ('alone', (20, 21)), ('training', (22, 23))]","[['very large gap', 'on', 'SemEval 2013 task'], ['very large gap', 'when', 'SemCor'], ['SemCor', 'used', 'alone'], ['alone', 'for', 'training']]",[],[],[],entity_linking,5,186
1102,results,"When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .","[('add', (2, 3)), ('to', (7, 8)), ('obtain', (14, 15)), ('on', (21, 22)), ('except', (24, 25))]","[('WordNet Gloss Tagged', (4, 7)), ('training data', (9, 11)), ('systematically state of the art results', (15, 21)), ('all tasks', (22, 24)), ('SensEval 3', (26, 28))]","[['WordNet Gloss Tagged', 'to', 'training data'], ['WordNet Gloss Tagged', 'obtain', 'systematically state of the art results'], ['training data', 'obtain', 'systematically state of the art results'], ['systematically state of the art results', 'on', 'all tasks']]","[['WordNet Gloss Tagged', 'has', 'training data']]","[['Results', 'add', 'WordNet Gloss Tagged']]",[],entity_linking,5,190
1103,results,"Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .","[('does not consistently improves or decreases', (7, 13)), ('on', (15, 16)), ('as', (35, 36))]","[('sense reduction method', (4, 7)), ('score', (14, 15)), ('every task', (16, 18)), ('result', (30, 31)), ('roughly the same', (32, 35)), ('without sense reduction', (36, 39))]","[['sense reduction method', 'does not consistently improves or decreases', 'score'], ['score', 'on', 'every task'], ['result', 'as', 'without sense reduction'], ['roughly the same', 'as', 'without sense reduction']]","[['result', 'has', 'roughly the same']]",[],"[['Results', 'has', 'sense reduction method']]",entity_linking,5,191
1104,results,"As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .","[('in', (11, 12)), ('improves systematically', (15, 17))]","[('ensembling', (5, 6)), ('very efficient method', (8, 11)), ('WSD', (12, 13)), ('all our results', (17, 20))]","[['very efficient method', 'in', 'WSD'], ['very efficient method', 'improves systematically', 'all our results']]","[['ensembling', 'has', 'very efficient method']]",[],"[['Results', 'has', 'ensembling']]",entity_linking,5,193
1105,results,"Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .","[('with', (2, 3)), ('when applying', (10, 12))]","[('ensembles', (3, 4)), ('scores', (6, 7)), ('significantly higher', (8, 10)), ('vocabulary reduction algorithm', (13, 16))]","[['significantly higher', 'when applying', 'vocabulary reduction algorithm']]","[['ensembles', 'has', 'scores'], ['scores', 'has', 'significantly higher']]","[['Results', 'with', 'ensembles']]",[],entity_linking,5,194
1106,research-problem,Incorporating Glosses into Neural Word Sense Disambiguation,[],[],[],[],[],[],entity_linking,6,2
1107,research-problem,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,[],"[('Word Sense Disambiguation ( WSD )', (0, 6))]",[],[],[],[],entity_linking,6,4
1108,research-problem,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,[],"[('WSD', (13, 14))]",[],[],[],[],entity_linking,6,5
1109,model,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .","[('propose', (5, 6)), ('is', (19, 20)), ('of', (22, 23))]","[('novel model GAS', (7, 10)), ('gloss - augmented WSD neural network', (12, 18)), ('variant', (21, 22)), ('memory network', (24, 26))]","[['gloss - augmented WSD neural network', 'is', 'variant'], ['variant', 'of', 'memory network']]","[['novel model GAS', 'has', 'gloss - augmented WSD neural network']]","[['Model', 'propose', 'novel model GAS']]",[],entity_linking,6,31
1110,model,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,"[('jointly encodes', (1, 3)), ('of', (7, 8)), ('models', (12, 13)), ('between', (16, 17)), ('in', (21, 22))]","[('GAS', (0, 1)), ('context and glosses', (4, 7)), ('target word', (9, 11)), ('semantic relationship', (14, 16)), ('context and glosses', (18, 21)), ('memory module', (23, 25))]","[['GAS', 'jointly encodes', 'context and glosses'], ['context and glosses', 'of', 'target word'], ['GAS', 'models', 'semantic relationship'], ['semantic relationship', 'between', 'context and glosses'], ['context and glosses', 'in', 'memory module']]",[],[],"[['Model', 'has', 'GAS']]",entity_linking,6,32
1111,model,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .","[('measure', (3, 4)), ('between', (7, 8)), ('employ', (15, 16)), ('within', (19, 20)), ('as', (22, 23)), ('adopt', (27, 28))]","[('inner relationship', (5, 7)), ('glosses and context', (8, 11)), ('more accurately', (11, 13)), ('multiple passes operation', (16, 19)), ('memory', (21, 22)), ('re-reading process', (24, 26)), ('two memory updating mechanisms', (28, 32))]","[['inner relationship', 'between', 'glosses and context'], ['inner relationship', 'employ', 'multiple passes operation'], ['multiple passes operation', 'within', 'memory'], ['memory', 'as', 're-reading process'], ['multiple passes operation', 'adopt', 'two memory updating mechanisms']]","[['glosses and context', 'has', 'more accurately']]","[['Model', 'measure', 'inner relationship']]",[],entity_linking,6,33
1112,hyperparameters,"We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .","[('use', (1, 2)), ('with', (5, 6)), ('keep', (11, 12)), ('during', (14, 15))]","[('pre-trained word embeddings', (2, 5)), ('300 dimensions', (6, 8)), ('fixed', (13, 14)), ('training process', (16, 18))]","[['pre-trained word embeddings', 'with', '300 dimensions'], ['pre-trained word embeddings', 'keep', 'fixed'], ['fixed', 'during', 'training process']]",[],"[['Hyperparameters', 'use', 'pre-trained word embeddings']]",[],entity_linking,6,196
1113,hyperparameters,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .","[('employ', (1, 2)), ('in', (5, 6)), ('means', (16, 17))]","[('256 hidden units', (2, 5)), ('gloss module', (8, 10)), ('context module', (12, 14)), ('n = 256', (17, 20))]","[['256 hidden units', 'in', 'gloss module'], ['256 hidden units', 'in', 'context module'], ['256 hidden units', 'means', 'n = 256'], ['context module', 'means', 'n = 256']]",[],"[['Hyperparameters', 'employ', '256 hidden units']]",[],entity_linking,6,197
1114,hyperparameters,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .","[('used for', (3, 5)), ('in', (6, 7)), ('with', (12, 13))]","[('Orthogonal initialization', (0, 2)), ('weights', (5, 6)), ('LSTM', (7, 8)), ('random uniform initialization', (9, 12)), ('range [ - 0.1 , 0.1 ]', (13, 20))]","[['Orthogonal initialization', 'used for', 'weights'], ['weights', 'in', 'LSTM'], ['random uniform initialization', 'with', 'range [ - 0.1 , 0.1 ]']]",[],[],"[['Hyperparameters', 'has', 'Orthogonal initialization']]",entity_linking,6,198
1115,hyperparameters,We assign gloss expansion depth K the value of 4 .,"[('assign', (1, 2)), ('of', (8, 9))]","[('gloss expansion depth K', (2, 6)), ('value', (7, 8)), ('4', (9, 10))]","[['value', 'of', '4']]","[['gloss expansion depth K', 'has', 'value']]","[['Hyperparameters', 'assign', 'gloss expansion depth K']]",[],entity_linking,6,199
1116,hyperparameters,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .","[('experiment with', (2, 4)), ('from', (12, 13)), ('in', (16, 17))]","[('number of passes | T M |', (5, 12)), ('1 to 5', (13, 16)), ('our framework', (17, 19))]","[['number of passes | T M |', 'from', '1 to 5'], ['1 to 5', 'in', 'our framework']]","[['number of passes | T M |', 'has', '1 to 5']]","[['Hyperparameters', 'experiment with', 'number of passes | T M |']]",[],entity_linking,6,200
1117,hyperparameters,We use Adam optimizer in the training process with 0.001 initial learning rate .,"[('in', (4, 5)), ('with', (8, 9))]","[('Adam optimizer', (2, 4)), ('training process', (6, 8)), ('0.001', (9, 10)), ('initial learning rate', (10, 13))]","[['Adam optimizer', 'in', 'training process'], ['Adam optimizer', 'with', '0.001'], ['training process', 'with', '0.001']]","[['0.001', 'has', 'initial learning rate']]",[],[],entity_linking,6,201
1118,hyperparameters,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .","[('to', (2, 3)), ('set', (11, 12))]","[('dropout regularization', (8, 10)), ('drop rate', (12, 14)), ('0.5', (15, 16))]","[['drop rate', 'to', '0.5']]","[['drop rate', 'has', '0.5']]","[['Hyperparameters', 'to', 'dropout regularization']]",[],entity_linking,6,202
1119,hyperparameters,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,"[('runs for', (1, 3)), ('with', (7, 8)), ('if', (10, 11)), ('within', (17, 18))]","[('Training', (0, 1)), ('100 epochs', (5, 7)), ('early stopping', (8, 10)), ('validation loss', (12, 14)), (""does n't improve"", (14, 17)), ('10 epochs', (20, 22))]","[['100 epochs', 'with', 'early stopping'], ['early stopping', 'if', 'validation loss'], [""does n't improve"", 'within', '10 epochs']]","[['validation loss', 'has', ""does n't improve""]]",[],"[['Hyperparameters', 'has', 'Training']]",entity_linking,6,203
1120,results,English all - words results,[],"[('English all - words results', (0, 5))]",[],[],[],"[['Results', 'has', 'English all - words results']]",entity_linking,6,228
1121,results,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,"[('achieves', (4, 5)), ('on', (12, 13)), ('of', (15, 16))]","[('GAS and GAS ext', (0, 4)), ('state - of - theart performance', (6, 12)), ('concatenation', (14, 15)), ('all test datasets', (16, 19))]","[['GAS and GAS ext', 'achieves', 'state - of - theart performance'], ['state - of - theart performance', 'on', 'concatenation'], ['concatenation', 'of', 'all test datasets']]",[],[],"[['Results', 'has', 'GAS and GAS ext']]",entity_linking,6,233
1122,results,". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .","[('performs', (2, 3)), ('on', (4, 5)), ('find', (13, 14)), ('with', (17, 18)), ('achieves', (22, 23)), ('on', (27, 28)), ('of', (30, 31))]","[('best', (3, 4)), ('all the test sets', (5, 9)), ('GAS ext', (15, 17)), ('concatenation memory updating strategy', (18, 22)), ('best results', (24, 26)), ('70.6', (26, 27)), ('concatenation', (29, 30)), ('four test datasets', (32, 35))]","[['best', 'on', 'all the test sets'], ['best results', 'on', 'concatenation'], ['70.6', 'on', 'concatenation'], ['best', 'find', 'GAS ext'], ['GAS ext', 'with', 'concatenation memory updating strategy'], ['concatenation memory updating strategy', 'achieves', 'best results'], ['70.6', 'on', 'concatenation'], ['concatenation', 'of', 'four test datasets']]","[['best results', 'has', '70.6']]","[['Results', 'performs', 'best']]",[],entity_linking,6,237
1123,results,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .","[('find that', (15, 17)), ('outperforms', (20, 21)), ('on', (27, 28))]","[('our best model', (17, 20)), ('previous best neural network models', (22, 27)), ('every individual test set', (28, 32))]","[['our best model', 'outperforms', 'previous best neural network models'], ['previous best neural network models', 'on', 'every individual test set']]",[],"[['Results', 'find that', 'our best model']]",[],entity_linking,6,238
1124,results,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .","[('beat', (7, 8)), ('on', (11, 12))]","[('our best model', (2, 5)), ('IMS + emb', (8, 11)), ('SE3 , SE13 and SE15 test sets', (13, 20))]","[['our best model', 'beat', 'IMS + emb'], ['IMS + emb', 'on', 'SE3 , SE13 and SE15 test sets']]",[],[],[],entity_linking,6,240
1125,results,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,"[('Incorporating', (0, 1)), ('into', (2, 3)), ('greatly improve', (6, 8)), ('extending', (11, 12)), ('further boost', (16, 18))]","[('glosses', (1, 2)), ('neural WSD', (3, 5)), ('performance', (9, 10)), ('original gloss', (13, 15)), ('results', (19, 20))]","[['glosses', 'into', 'neural WSD'], ['glosses', 'greatly improve', 'performance'], ['neural WSD', 'greatly improve', 'performance'], ['glosses', 'extending', 'original gloss'], ['original gloss', 'further boost', 'results']]",[],"[['Results', 'Incorporating', 'glosses']]",[],entity_linking,6,241
1126,results,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .","[('Compared with', (0, 2)), ('greatly improves', (16, 18)), ('by', (21, 22)), ('with', (27, 28))]","[('Bi - LSTM baseline', (3, 7)), ('our proposed model', (13, 16)), ('WSD task', (19, 21)), ('2.2 % F1 - score', (22, 27)), ('gloss knowledge', (31, 33))]","[['our proposed model', 'greatly improves', 'WSD task'], ['WSD task', 'by', '2.2 % F1 - score']]","[['Bi - LSTM baseline', 'has', 'our proposed model']]","[['Results', 'Compared with', 'Bi - LSTM baseline']]",[],entity_linking,6,242
1127,results,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .","[('compared with', (2, 4)), ('further improve', (19, 21)), ('with the help of', (23, 27)), ('through', (30, 31))]","[('GAS', (5, 6)), ('GAS ext', (16, 18)), ('performance', (22, 23)), ('extended glosses', (28, 30)), ('semantic relations', (32, 34))]","[['GAS ext', 'further improve', 'performance'], ['performance', 'with the help of', 'extended glosses'], ['extended glosses', 'through', 'semantic relations']]","[['GAS', 'has', 'GAS ext']]","[['Results', 'compared with', 'GAS']]",[],entity_linking,6,243
1128,research-problem,Semi-supervised Word Sense Disambiguation with Neural Models,[],"[('Semi-supervised Word Sense Disambiguation', (0, 4))]",[],[],[],[],entity_linking,7,2
1129,research-problem,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,[],"[('word sense disambiguation ( WSD )', (9, 15))]",[],[],[],[],entity_linking,7,4
1130,research-problem,"Recently , researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms .",[],"[('WSD', (20, 21))]",[],[],[],[],entity_linking,7,5
1131,model,"In this paper , we describe two novel WSD algorithms .","[('describe', (5, 6))]","[('two novel WSD algorithms', (6, 10))]",[],[],"[['Model', 'describe', 'two novel WSD algorithms']]",[],entity_linking,7,20
1132,model,The first is based on a Long Short Term Memory ( LSTM ) ) .,"[('based on', (3, 5))]","[('first', (1, 2)), ('Long Short Term Memory ( LSTM )', (6, 13))]","[['first', 'based on', 'Long Short Term Memory ( LSTM )']]",[],[],"[['Model', 'has', 'first']]",entity_linking,7,21
1133,experiments,"Since this model is able to take into account word order when classifying , it performs significantly better than an algorithm based on a continuous bag of words model ( Word2 vec ) , especially on verbs .","[('take into account', (6, 9)), ('when', (11, 12))]","[('word order', (9, 11)), ('classifying', (12, 13))]","[['word order', 'when', 'classifying']]",[],[],[],entity_linking,7,22
1134,model,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,"[('present', (2, 3)), ('uses', (7, 8)), ('to label', (10, 12)), ('based on', (14, 16)), ('to', (18, 19))]","[('semi-supervised algorithm', (4, 6)), ('label propagation', (8, 10)), ('unlabeled sentences', (12, 14)), ('similarity', (17, 18)), ('labeled ones', (19, 21))]","[['semi-supervised algorithm', 'uses', 'label propagation'], ['label propagation', 'to label', 'unlabeled sentences'], ['unlabeled sentences', 'based on', 'similarity'], ['similarity', 'to', 'labeled ones']]",[],"[['Model', 'present', 'semi-supervised algorithm']]",[],entity_linking,7,23
1135,experiments,Sem Eval Tasks,[],"[('Sem Eval Tasks', (0, 3))]",[],[],[],[],entity_linking,7,130
1136,results,Our proposed algorithms achieve the highest all - words F 1 scores except for Sem - Eval 2013 .,"[('achieve', (3, 4)), ('except for', (12, 14))]","[('Our proposed algorithms', (0, 3)), ('highest all - words F 1 scores', (5, 12)), ('Sem - Eval 2013', (14, 18))]","[['Our proposed algorithms', 'achieve', 'highest all - words F 1 scores'], ['highest all - words F 1 scores', 'except for', 'Sem - Eval 2013']]",[],[],"[['Results', 'has', 'Our proposed algorithms']]",entity_linking,7,136
1137,results,"Unified WSD has the highest F 1 score on Nouns ( Sem - Eval - 7 Coarse ) , but our algorithms outperform Unified WSD on other part - of - speech tags .","[('on', (8, 9)), ('outperform', (22, 23)), ('on', (25, 26))]","[('Unified WSD', (0, 2)), ('highest F 1 score', (4, 8)), ('Nouns ( Sem - Eval - 7 Coarse )', (9, 18)), ('our algorithms', (20, 22)), ('Unified WSD', (23, 25)), ('other part - of - speech tags', (26, 33))]","[['highest F 1 score', 'on', 'Nouns ( Sem - Eval - 7 Coarse )'], ['Unified WSD', 'on', 'other part - of - speech tags'], ['our algorithms', 'outperform', 'Unified WSD'], ['Unified WSD', 'on', 'other part - of - speech tags']]","[['Unified WSD', 'has', 'highest F 1 score']]",[],"[['Results', 'has', 'Unified WSD']]",entity_linking,7,138
1138,results,Word2 Vec vectors Vs. LSTM,[],"[('Word2 Vec vectors Vs. LSTM', (0, 5))]",[],[],[],"[['Results', 'has', 'Word2 Vec vectors Vs. LSTM']]",entity_linking,7,148
1139,results,"It performs similar to IMS + Word2 Vec ( T: SemCor ) , a SVM - based classifier studied in .","[('performs', (1, 2))]","[('similar', (2, 3)), ('IMS + Word2 Vec ( T: SemCor )', (4, 12))]",[],[],"[['Results', 'performs', 'similar']]",[],entity_linking,7,150
1140,results,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,"[('shows', (0, 1)), ('outperforms', (5, 6)), ('across', (10, 11))]","[('LSTM classifier', (3, 5)), ('Word2 Vec classifier', (7, 10)), ('board', (12, 13))]","[['LSTM classifier', 'outperforms', 'Word2 Vec classifier'], ['Word2 Vec classifier', 'across', 'board']]",[],"[['Results', 'shows', 'LSTM classifier']]",[],entity_linking,7,151
1141,results,Sem Cor Vs. OMSTI,[],"[('Sem Cor Vs. OMSTI', (0, 4))]",[],[],[],"[['Results', 'has', 'Sem Cor Vs. OMSTI']]",entity_linking,7,152
1142,results,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .","[('trained with', (10, 12)), ('performs', (13, 14)), ('than', (15, 16)), ('with', (18, 19))]","[('LSTM classifier', (8, 10)), ('OMSTI', (12, 13)), ('worse', (14, 15)), ('trained', (17, 18)), ('SemCor', (19, 20))]","[['LSTM classifier', 'trained with', 'OMSTI'], ['LSTM classifier', 'performs', 'worse'], ['OMSTI', 'performs', 'worse'], ['worse', 'than', 'trained'], ['worse', 'with', 'SemCor'], ['trained', 'with', 'SemCor']]",[],[],[],entity_linking,7,153
1143,baselines,"While the SVM classifier studied in maybe able to learn a model which copes with this noise , our naive nearest neighbor classifiers do not have a learned model and deal less well with noisy labels .","[('able to learn', (7, 10)), ('copes with', (13, 15)), ('do not have', (23, 26)), ('deal', (30, 31)), ('with', (33, 34))]","[('SVM classifier', (2, 4)), ('model', (11, 12)), ('noise', (16, 17)), ('our naive nearest neighbor classifiers', (18, 23)), ('learned model', (27, 29)), ('less well', (31, 33)), ('noisy labels', (34, 36))]","[['SVM classifier', 'able to learn', 'model'], ['model', 'copes with', 'noise'], ['our naive nearest neighbor classifiers', 'do not have', 'learned model'], ['less well', 'with', 'noisy labels']]",[],[],"[['Baselines', 'has', 'SVM classifier']]",entity_linking,7,155
1144,experiments,NOAD Eval,[],"[('NOAD Eval', (0, 2))]",[],[],[],[],entity_linking,7,160
1145,experiments,LSTM classifier,[],"[('LSTM classifier', (0, 2))]",[],[],[],[],entity_linking,7,178
1146,baselines,Most frequent sense :,[],"[('Most frequent sense', (0, 3))]",[],[],[],"[['Baselines', 'has', 'Most frequent sense']]",entity_linking,7,180
1147,results,"LSTM outperforms Word2Vec by more than 10 % overall words , where most of the gains are from verbs and adverbs .","[('outperforms', (1, 2)), ('by', (3, 4)), ('from', (17, 18))]","[('LSTM', (0, 1)), ('Word2Vec', (2, 3)), ('more than 10 % overall words', (4, 10)), ('most of the gains', (12, 16)), ('verbs and adverbs', (18, 21))]","[['LSTM', 'outperforms', 'Word2Vec'], ['Word2Vec', 'by', 'more than 10 % overall words'], ['most of the gains', 'from', 'verbs and adverbs']]",[],[],"[['Results', 'has', 'LSTM']]",entity_linking,7,184
1148,experiments,Change of training data,[],"[('Change of training data', (0, 4))]",[],[],[],[],entity_linking,7,186
1149,results,The SemCor ( or MASC ) trained classifier is on a par with the NOAD trained classifier on F1 score .,"[('on', (9, 10)), ('with', (12, 13)), ('on', (17, 18))]","[('SemCor ( or MASC ) trained classifier', (1, 8)), ('par', (11, 12)), ('NOAD trained classifier', (14, 17)), ('F1 score', (18, 20))]","[['SemCor ( or MASC ) trained classifier', 'on', 'par'], ['NOAD trained classifier', 'on', 'F1 score'], ['par', 'with', 'NOAD trained classifier'], ['NOAD trained classifier', 'on', 'F1 score']]","[['SemCor ( or MASC ) trained classifier', 'has', 'par']]",[],"[['Results', 'has', 'SemCor ( or MASC ) trained classifier']]",entity_linking,7,191
1150,ablation-analysis,Change of language model capacity,[],"[('Change of language model capacity', (0, 5))]",[],[],[],"[['Ablation analysis', 'has', 'Change of language model capacity']]",entity_linking,7,193
1151,hyperparameters,"To balance the accuracy and resource usage , we use the second best LSTM model ( h = 2048 and p = 512 ) by default .","[('balance', (1, 2)), ('use', (9, 10)), ('by', (24, 25))]","[('accuracy and resource usage', (3, 7)), ('second best LSTM model ( h = 2048 and p = 512 )', (11, 24)), ('default', (25, 26))]","[['accuracy and resource usage', 'use', 'second best LSTM model ( h = 2048 and p = 512 )'], ['second best LSTM model ( h = 2048 and p = 512 )', 'by', 'default']]",[],"[['Hyperparameters', 'balance', 'accuracy and resource usage']]",[],entity_linking,7,196
1152,ablation-analysis,Semi-supervised WSD,[],"[('Semi-supervised WSD', (0, 2))]",[],[],[],"[['Ablation analysis', 'has', 'Semi-supervised WSD']]",entity_linking,7,197
1153,results,"As can be observed from , LP did not yield clear benefits when using the Word2 Vec language model .","[('did not yield', (7, 10)), ('using', (13, 14))]","[('LP', (6, 7)), ('clear benefits', (10, 12)), ('Word2 Vec language model', (15, 19))]","[['LP', 'did not yield', 'clear benefits'], ['clear benefits', 'using', 'Word2 Vec language model']]","[['LP', 'has', 'clear benefits']]",[],"[['Results', 'has', 'LP']]",entity_linking,7,201
1154,results,"We did see significant improvements , 6.3 % increase on SemCor and 7.3 % increase on MASC , using LP with the LSTM language model .","[('see', (2, 3)), ('on', (9, 10)), ('on', (15, 16)), ('using', (18, 19)), ('with', (20, 21))]","[('significant improvements', (3, 5)), ('6.3 % increase', (6, 9)), ('SemCor', (10, 11)), ('7.3 % increase', (12, 15)), ('MASC', (16, 17)), ('LP', (19, 20)), ('LSTM language model', (22, 25))]","[['6.3 % increase', 'on', 'SemCor'], ['7.3 % increase', 'on', 'MASC'], ['7.3 % increase', 'on', 'MASC'], ['7.3 % increase', 'using', 'LP'], ['MASC', 'using', 'LP'], ['LP', 'with', 'LSTM language model']]","[['significant improvements', 'has', '6.3 % increase']]","[['Results', 'see', 'significant improvements']]",[],entity_linking,7,202
1155,ablation-analysis,Change of seed data :,[],"[('Change of seed data', (0, 4))]",[],[],[],"[['Ablation analysis', 'has', 'Change of seed data']]",entity_linking,7,204
1156,ablation-analysis,"As can be seen in , LP substantially improves classifier F1 when the training datasets are SemCor + NOAD or MASC + NOAD .","[('when', (11, 12)), ('are', (15, 16))]","[('LP', (6, 7)), ('substantially improves', (7, 9)), ('classifier F1', (9, 11)), ('training datasets', (13, 15)), ('SemCor + NOAD', (16, 19)), ('MASC + NOAD', (20, 23))]","[['substantially improves', 'when', 'training datasets'], ['classifier F1', 'when', 'training datasets'], ['training datasets', 'are', 'SemCor + NOAD'], ['training datasets', 'are', 'MASC + NOAD']]","[['LP', 'has', 'substantially improves'], ['substantially improves', 'has', 'classifier F1']]",[],"[['Ablation analysis', 'has', 'LP']]",entity_linking,7,205
1157,ablation-analysis,Change of graph density :,[],"[('Change of graph density', (0, 4))]",[],[],[],"[['Ablation analysis', 'has', 'Change of graph density']]",entity_linking,7,208
1158,hyperparameters,"By default , we construct the LP graph by connecting two nodes if their affinity is above 95 % percentile .","[('construct', (4, 5)), ('by connecting', (8, 10)), ('if', (12, 13))]","[('LP graph', (6, 8)), ('two nodes', (10, 12)), ('affinity', (14, 15)), ('above 95 % percentile', (16, 20))]","[['LP graph', 'by connecting', 'two nodes'], ['two nodes', 'if', 'affinity']]","[['affinity', 'has', 'above 95 % percentile']]","[['Hyperparameters', 'construct', 'LP graph']]",[],entity_linking,7,209
1159,ablation-analysis,"The F1 scores are relatively stable when the percentile ranges between 85 to 98 , but decrease when the percentile drops to 80 .","[('when', (6, 7)), ('ranges between', (9, 11)), ('when', (17, 18)), ('drops to', (20, 22))]","[('F1 scores', (1, 3)), ('relatively stable', (4, 6)), ('percentile', (8, 9)), ('85 to 98', (11, 14)), ('decrease', (16, 17)), ('percentile', (19, 20)), ('80', (22, 23))]","[['relatively stable', 'when', 'percentile'], ['percentile', 'ranges between', '85 to 98'], ['decrease', 'when', 'percentile'], ['percentile', 'drops to', '80']]","[['F1 scores', 'has', 'relatively stable']]",[],"[['Ablation analysis', 'has', 'F1 scores']]",entity_linking,7,212
1160,research-problem,"Language modeling tasks , in which words , or word - pieces , are predicted on the basis of a local context , have been very effective for learning word embeddings and context dependent representations of phrases .",[],"[('learning word embeddings', (28, 31)), ('context dependent representations of phrases', (32, 37))]",[],[],[],[],entity_linking,8,5
1161,approach,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .","[('present', (1, 2)), ('of', (14, 15)), ('have', (19, 20)), ('to match', (22, 24)), ('of', (28, 29)), ('in which', (32, 34))]","[('RELIC ( Representations of Entities Learned in Context )', (2, 11)), ('table', (13, 14)), ('independent entity embeddings', (15, 18)), ('trained', (21, 22)), ('fixed length vector representations', (24, 28)), ('textual context', (30, 32)), ('entities', (35, 36)), ('been seen', (37, 39))]","[['table', 'of', 'independent entity embeddings'], ['fixed length vector representations', 'of', 'textual context'], ['independent entity embeddings', 'have', 'trained'], ['independent entity embeddings', 'to match', 'fixed length vector representations'], ['trained', 'to match', 'fixed length vector representations'], ['fixed length vector representations', 'of', 'textual context'], ['textual context', 'in which', 'entities']]","[['RELIC ( Representations of Entities Learned in Context )', 'has', 'table']]","[['Approach', 'present', 'RELIC ( Representations of Entities Learned in Context )']]",[],entity_linking,8,21
1162,approach,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .","[('apply', (1, 2)), ('to', (3, 4))]","[('RELIC', (2, 3)), ('entity typing', (4, 6)), ('entity linking', (22, 24)), ('trivia question answering', (38, 41))]","[['RELIC', 'to', 'entity typing']]",[],"[['Approach', 'apply', 'RELIC']]",[],entity_linking,8,22
1163,experiments,"To train RELIC , we obtain data from the 2018 - 10 - 22 dump of English Wikipedia .","[('train', (1, 2)), ('obtain', (5, 6)), ('from', (7, 8))]","[('RELIC', (2, 3)), ('data', (6, 7)), ('2018 - 10 - 22 dump of English Wikipedia', (9, 18))]","[['RELIC', 'obtain', 'data'], ['data', 'from', '2018 - 10 - 22 dump of English Wikipedia']]",[],[],[],entity_linking,8,98
1164,hyperparameters,We limit each context sentence to 128 tokens .,"[('limit', (1, 2)), ('to', (5, 6))]","[('each context sentence', (2, 5)), ('128 tokens', (6, 8))]","[['each context sentence', 'to', '128 tokens']]",[],"[['Hyperparameters', 'limit', 'each context sentence']]",[],entity_linking,8,101
1165,hyperparameters,We set the entity embedding size to d = 300 .,"[('set', (1, 2)), ('to', (6, 7))]","[('entity embedding size', (3, 6)), ('d = 300', (7, 10))]","[['entity embedding size', 'to', 'd = 300']]","[['entity embedding size', 'has', 'd = 300']]","[['Hyperparameters', 'set', 'entity embedding size']]",[],entity_linking,8,104
1166,approach,We train the model using Tensor Flow,"[('train', (1, 2)), ('using', (4, 5))]","[('model', (3, 4)), ('Tensor Flow', (5, 7))]","[['model', 'using', 'Tensor Flow']]",[],"[['Approach', 'train', 'model']]",[],entity_linking,8,105
1167,experiments,ENTITY LINKING,[],"[('ENTITY LINKING', (0, 2))]",[],[],[],[],entity_linking,8,113
1168,results,"However , when we do adopt the standard CoNLL - Aida training set and alias table , RELIC matches the state of the art on this benchmark , despite using far fewer hand engineered resources use the large Wikidata knowledge base to create entity representations ) .","[('adopt', (5, 6)), ('matches', (18, 19)), ('on', (24, 25))]","[('standard CoNLL - Aida training set and alias table', (7, 16)), ('RELIC', (17, 18)), ('state of the art', (20, 24)), ('benchmark', (26, 27))]","[['RELIC', 'matches', 'state of the art'], ['state of the art', 'on', 'benchmark']]","[['standard CoNLL - Aida training set and alias table', 'has', 'RELIC']]",[],[],entity_linking,8,124
1169,results,"Finally , we believe that RELIC 's entity linking performance could be boosted even higher through the adoption of commonly used entity linking features .","[('boosted', (12, 13)), ('through', (15, 16)), ('of', (18, 19))]","[(""RELIC 's entity linking performance"", (5, 10)), ('even higher', (13, 15)), ('adoption', (17, 18)), ('commonly used entity linking features', (19, 24))]","[[""RELIC 's entity linking performance"", 'boosted', 'even higher'], ['even higher', 'through', 'adoption'], ['adoption', 'of', 'commonly used entity linking features']]",[],[],[],entity_linking,8,134
1170,results,"RELIC outperforms prior work , even with only 5 % of the training data .","[('outperforms', (1, 2)), ('with', (6, 7)), ('of', (10, 11))]","[('RELIC', (0, 1)), ('prior work', (2, 4)), ('only 5 %', (7, 10)), ('training data', (12, 14))]","[['RELIC', 'outperforms', 'prior work'], ['prior work', 'with', 'only 5 %'], ['only 5 %', 'of', 'training data']]",[],[],[],entity_linking,8,141
1171,results,ENTITY - LEVEL FINE TYPING,[],"[('ENTITY - LEVEL FINE TYPING', (0, 5))]",[],[],[],"[['Results', 'has', 'ENTITY - LEVEL FINE TYPING']]",entity_linking,8,142
1172,results,show that RELIC significantly outperforms prior results on both datasets .,"[('show', (0, 1)), ('on', (7, 8))]","[('RELIC', (2, 3)), ('significantly outperforms', (3, 5)), ('prior results', (5, 7)), ('both datasets', (8, 10))]","[['significantly outperforms', 'on', 'both datasets'], ['prior results', 'on', 'both datasets']]","[['RELIC', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'prior results']]","[['Results', 'show', 'RELIC']]",[],entity_linking,8,146
1173,results,"For TypeNet , aggregate mention - level types and train with a structured loss based on the TypeNet hierarchy , but is still outperformed by our flat classifier of binary labels .","[('For', (0, 1)), ('train with', (9, 11)), ('based on', (14, 16)), ('by', (24, 25)), ('of', (28, 29))]","[('TypeNet', (1, 2)), ('aggregate', (3, 4)), ('mention - level types', (4, 8)), ('structured loss', (12, 14)), ('TypeNet hierarchy', (17, 19)), ('still outperformed', (22, 24)), ('our flat classifier', (25, 28)), ('binary labels', (29, 31))]","[['TypeNet', 'train with', 'structured loss'], ['structured loss', 'based on', 'TypeNet hierarchy'], ['still outperformed', 'by', 'our flat classifier'], ['our flat classifier', 'of', 'binary labels']]","[['TypeNet', 'has', 'aggregate'], ['aggregate', 'has', 'mention - level types']]","[['Results', 'For', 'TypeNet']]",[],entity_linking,8,148
1174,results,EFFECT OF MASKING,[],"[('EFFECT OF MASKING', (0, 3))]",[],[],[],"[['Results', 'has', 'EFFECT OF MASKING']]",entity_linking,8,151
1175,results,"It is clear that masking mentions during training is beneficial for entity typing tasks , but detrimental for entity linking .","[('clear that', (2, 4)), ('during', (6, 7)), ('for', (10, 11)), ('for', (17, 18))]","[('masking mentions', (4, 6)), ('training', (7, 8)), ('beneficial', (9, 10)), ('entity typing tasks', (11, 14)), ('detrimental', (16, 17)), ('entity linking', (18, 20))]","[['masking mentions', 'during', 'training'], ['beneficial', 'for', 'entity typing tasks'], ['detrimental', 'for', 'entity linking']]",[],"[['Results', 'clear that', 'masking mentions']]",[],entity_linking,8,154
1176,results,"A higher mask rate leads to better performance , both in low and high - data situations .","[('leads to', (4, 6))]","[('A higher mask rate', (0, 4)), ('better performance', (6, 8)), ('low and high - data situations', (11, 17))]","[['A higher mask rate', 'leads to', 'better performance']]",[],[],"[['Results', 'has', 'A higher mask rate']]",entity_linking,8,159
1177,results,"However , we would like to point out that that a mask rate of 10 % , RELIC nears optimum performance on most tasks .","[('nears', (18, 19)), ('on', (21, 22))]","[('mask rate of 10 %', (11, 16)), ('RELIC', (17, 18)), ('optimum performance', (19, 21)), ('most tasks', (22, 24))]","[['RELIC', 'nears', 'optimum performance'], ['optimum performance', 'on', 'most tasks']]","[['mask rate of 10 %', 'has', 'RELIC']]",[],[],entity_linking,8,162
1178,results,FEW - SHOT CATEGORY COMPLETION,[],"[('FEW - SHOT CATEGORY COMPLETION', (0, 5))]",[],[],[],"[['Results', 'has', 'FEW - SHOT CATEGORY COMPLETION']]",entity_linking,8,164
1179,results,"Furthermore , due to the incompleteness of the the FIGMENT and TypeNet type systems , we also believe that RELIC 's performance is approaching the upper bound on both of these supervised tasks .","[('due to', (2, 4)), ('of', (6, 7)), ('approaching', (23, 24)), ('on', (27, 28))]","[('incompleteness', (5, 6)), ('FIGMENT and TypeNet type systems', (9, 14)), (""RELIC 's performance"", (19, 22)), ('upper bound', (25, 27)), ('both of these supervised tasks', (28, 33))]","[['incompleteness', 'of', 'FIGMENT and TypeNet type systems'], [""RELIC 's performance"", 'approaching', 'upper bound'], ['upper bound', 'on', 'both of these supervised tasks']]","[['incompleteness', 'has', 'FIGMENT and TypeNet type systems'], [""RELIC 's performance"", 'has', 'upper bound']]","[['Results', 'due to', 'incompleteness']]",[],entity_linking,8,166
1180,results,TRIVIA QUESTION ANSWERING,[],"[('TRIVIA QUESTION ANSWERING', (0, 3))]",[],[],[],"[['Results', 'has', 'TRIVIA QUESTION ANSWERING']]",entity_linking,8,181
1181,results,We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,"[('observe', (1, 2)), ('taken by', (10, 12)), ('outperforms', (13, 14)), ('taken by', (19, 21))]","[('retrieve - then - read approach', (4, 10)), ('ORQA', (12, 13)), ('direct answer retrieval approach', (15, 19)), ('RELIC', (21, 22))]","[['retrieve - then - read approach', 'taken by', 'ORQA'], ['retrieve - then - read approach', 'outperforms', 'direct answer retrieval approach'], ['ORQA', 'outperforms', 'direct answer retrieval approach'], ['direct answer retrieval approach', 'taken by', 'RELIC']]",[],"[['Results', 'observe', 'retrieve - then - read approach']]",[],entity_linking,8,196
1182,results,"However , ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that RELIC 's much faster nearest neighbor lookup captures 80 % of ORQA 's performance .","[('see', (22, 23)), ('captures', (31, 32)), ('of', (34, 35))]","[(""RELIC 's much faster nearest neighbor lookup"", (24, 31)), ('80 %', (32, 34)), (""ORQA 's performance"", (35, 38))]","[[""RELIC 's much faster nearest neighbor lookup"", 'captures', '80 %'], ['80 %', 'of', ""ORQA 's performance""]]",[],"[['Results', 'see', ""RELIC 's much faster nearest neighbor lookup""]]",[],entity_linking,8,197
1183,results,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .","[(""outperforms 's"", (6, 8)), ('by', (11, 12))]","[('RELIC', (5, 6)), ('reading comprehension baseline', (8, 11)), ('20 points', (12, 14))]","[['RELIC', ""outperforms 's"", 'reading comprehension baseline'], ['reading comprehension baseline', 'by', '20 points']]","[['RELIC', 'has', 'reading comprehension baseline']]",[],[],entity_linking,8,198
1184,research-problem,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,[],"[('Named Entity Disambiguation', (10, 13))]",[],[],[],[],entity_linking,9,2
1185,research-problem,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",[],"[('Named Entity Disambiguation ( NED )', (0, 6))]",[],[],[],[],entity_linking,9,4
1186,research-problem,"In this paper , we propose a novel embedding method specifically designed for NED .",[],"[('NED', (13, 14))]",[],[],[],[],entity_linking,9,5
1187,model,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .","[('propose', (5, 6)), ('to construct', (8, 10)), ('jointly maps', (14, 16)), ('into', (19, 20))]","[('method', (7, 8)), ('novel embedding', (11, 13)), ('words and entities', (16, 19)), ('same continuous vector space', (21, 25))]","[['method', 'to construct', 'novel embedding'], ['novel embedding', 'jointly maps', 'words and entities'], ['words and entities', 'into', 'same continuous vector space']]",[],"[['Model', 'propose', 'method']]",[],entity_linking,9,21
1188,model,"In this model , similar words and entities are placed close to one another in a vector space .","[('placed', (9, 10)), ('to', (11, 12)), ('in', (14, 15))]","[('similar words and entities', (4, 8)), ('close', (10, 11)), ('one another', (12, 14)), ('vector space', (16, 18))]","[['similar words and entities', 'placed', 'close'], ['close', 'to', 'one another'], ['one another', 'in', 'vector space']]",[],[],"[['Model', 'has', 'similar words and entities']]",entity_linking,9,22
1189,model,"Hence , we can measure the similarity between any pair of items ( i.e. , words , entities , and a word and an entity ) by simply computing their cosine similarity .","[('measure', (4, 5)), ('between', (7, 8)), ('by simply computing', (26, 29))]","[('similarity', (6, 7)), ('any pair of items', (8, 12)), ('cosine similarity', (30, 32))]","[['similarity', 'between', 'any pair of items'], ['any pair of items', 'by simply computing', 'cosine similarity']]",[],"[['Model', 'measure', 'similarity']]",[],entity_linking,9,23
1190,model,"Our model is based on the skip - gram model , a recently proposed embedding model that learns to predict each context word given the target word .","[('based on', (3, 5)), ('to predict', (18, 20)), ('given', (23, 24))]","[('Our model', (0, 2)), ('skip - gram model', (6, 10)), ('learns', (17, 18)), ('each context word', (20, 23)), ('target word', (25, 27))]","[['Our model', 'based on', 'skip - gram model'], ['learns', 'to predict', 'each context word'], ['each context word', 'given', 'target word']]",[],[],"[['Model', 'has', 'Our model']]",entity_linking,9,25
1191,model,Our model consists of the following three models based on the skip - gram model :,"[('consists of', (2, 4))]","[('following three models', (5, 8))]",[],[],"[['Model', 'consists of', 'following three models']]",[],entity_linking,9,26
1192,model,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .","[('to predict', (10, 12)), ('given', (14, 15)), ('in', (18, 19)), ('to estimate', (30, 32)), ('given', (34, 35)), ('in', (38, 39)), ('of', (42, 43)), ('to predict', (55, 57)), ('given', (59, 60)), ('using', (63, 64)), ('in', (69, 70))]","[('conventional skip - gram model', (3, 8)), ('learns', (9, 10)), ('neighboring words', (12, 14)), ('target word', (16, 18)), ('text corpora', (19, 21)), ('KB graph model', (25, 28)), ('learns', (29, 30)), ('neighboring entities', (32, 34)), ('target entity', (36, 38)), ('link graph', (40, 42)), ('KB', (44, 45)), ('anchor context model', (50, 53)), ('learns', (54, 55)), ('neighboring words', (57, 59)), ('target entity', (61, 63)), ('anchors and their context words', (64, 69)), ('KB', (71, 72))]","[['learns', 'to predict', 'neighboring words'], ['neighboring words', 'given', 'target word'], ['neighboring entities', 'given', 'target entity'], ['target word', 'in', 'text corpora'], ['KB graph model', 'to estimate', 'neighboring entities'], ['learns', 'to estimate', 'neighboring entities'], ['neighboring entities', 'given', 'target entity'], ['target entity', 'in', 'link graph'], ['link graph', 'of', 'KB'], ['learns', 'to predict', 'neighboring words'], ['neighboring words', 'given', 'target entity'], ['target entity', 'using', 'anchors and their context words'], ['anchors and their context words', 'in', 'KB']]","[['conventional skip - gram model', 'has', 'learns']]",[],"[['Model', 'has', 'conventional skip - gram model']]",entity_linking,9,27
1193,model,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .","[('develop', (8, 9)), ('computes', (14, 15)), ('using', (17, 18))]","[('proposed embedding', (3, 5)), ('straightforward NED method', (10, 13)), ('two contexts', (15, 17)), ('textual context similarity', (22, 25)), ('coherence', (27, 28))]","[['proposed embedding', 'develop', 'straightforward NED method'], ['straightforward NED method', 'computes', 'two contexts']]","[['proposed embedding', 'has', 'straightforward NED method']]",[],[],entity_linking,9,29
1194,model,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .","[('combines', (3, 4)), ('with', (6, 7)), ('using', (16, 17))]","[('Our NED method', (0, 3)), ('contexts', (5, 6)), ('several standard features', (7, 10)), ('supervised machine learning', (17, 20))]","[['Our NED method', 'combines', 'contexts'], ['contexts', 'with', 'several standard features'], ['several standard features', 'using', 'supervised machine learning']]","[['Our NED method', 'has', 'contexts']]",[],"[['Model', 'has', 'Our NED method']]",entity_linking,9,32
1195,results,Our method successfully achieved enhanced performance on both the CoNLL and the TAC 2010 datasets .,"[('achieved', (3, 4)), ('on', (6, 7))]","[('Our method', (0, 2)), ('enhanced performance', (4, 6)), ('CoNLL and the TAC 2010 datasets', (9, 15))]","[['Our method', 'achieved', 'enhanced performance'], ['enhanced performance', 'on', 'CoNLL and the TAC 2010 datasets']]",[],[],"[['Results', 'has', 'Our method']]",entity_linking,9,215
1196,results,"Moreover , we found that the choice of candidate generation method considerably affected performance on the CoNLL dataset .","[('found that', (3, 5)), ('of', (7, 8)), ('on', (14, 15))]","[('choice', (6, 7)), ('candidate generation method', (8, 11)), ('considerably affected', (11, 13)), ('performance', (13, 14)), ('CoNLL dataset', (16, 18))]","[['choice', 'of', 'candidate generation method'], ['performance', 'on', 'CoNLL dataset']]","[['candidate generation method', 'has', 'considerably affected'], ['considerably affected', 'has', 'performance']]","[['Results', 'found that', 'choice']]",[],entity_linking,9,216
1197,results,Our method outperformed all the state - of - the - art methods on both datasets .,"[('outperformed', (2, 3)), ('on', (13, 14))]","[('all the state - of - the - art methods', (3, 13)), ('both datasets', (14, 16))]","[['all the state - of - the - art methods', 'on', 'both datasets']]",[],"[['Results', 'outperformed', 'all the state - of - the - art methods']]",[],entity_linking,9,218
1198,research-problem,"3D Morphable Models ( 3 DMMs ) are powerful statistical models of 3D facial shape and texture , and among the stateof - the - art methods for reconstructing facial shape from single images .",[],"[('reconstructing facial shape from single images', (28, 34))]",[],[],[],[],face_alignment,0,4
1199,research-problem,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,[],"[('3 D facial shape estimation from single images', (0, 8))]",[],[],[],[],face_alignment,0,18
1200,model,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .","[('propose', (1, 2)), ('for', (4, 5)), ('from', (10, 11)), ('with', (25, 26)), ('exhibits', (31, 32))]","[('methodology', (3, 4)), ('learning', (5, 6)), ('statistical texture model', (7, 10)), ('"" in - the -wild "" facial images', (11, 19)), ('full correspondence', (23, 25)), ('statistical shape prior', (27, 30)), ('both identity and expression variations', (32, 37))]","[['methodology', 'for', 'learning'], ['statistical texture model', 'from', '"" in - the -wild "" facial images'], ['full correspondence', 'with', 'statistical shape prior'], ['statistical shape prior', 'exhibits', 'both identity and expression variations']]","[['methodology', 'has', 'learning'], ['learning', 'has', 'statistical texture model']]","[['Model', 'propose', 'methodology']]",[],face_alignment,0,41
1201,model,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .","[('show', (23, 24)), ('how to learn', (24, 27)), ('for', (30, 31))]","[('featurebased texture models', (27, 30)), ('3 DMMs', (31, 33))]","[['featurebased texture models', 'for', '3 DMMs']]",[],"[['Model', 'show', 'featurebased texture models']]",[],face_alignment,0,42
1202,model,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .","[('using', (6, 7)), ('gets', (24, 25))]","[('advantage', (4, 5)), ('"" in - the -wild "" feature - based texture model', (8, 19)), ('fitting strategy', (22, 24)), ('simplified', (25, 26))]","[['advantage', 'using', '"" in - the -wild "" feature - based texture model'], ['fitting strategy', 'gets', 'simplified']]","[['"" in - the -wild "" feature - based texture model', 'has', 'fitting strategy'], ['fitting strategy', 'has', 'simplified']]",[],[],face_alignment,0,43
1203,experiments,3D Shape Recovery,[],"[('3D Shape Recovery', (0, 3))]",[],[],[],[],face_alignment,0,210
1204,results,"The Classic model struggles to fit to the "" in - the -wild "" conditions present in the test set , and performs the worst .","[('to fit', (4, 6)), ('present in', (15, 17)), ('performs', (22, 23))]","[('Classic model', (1, 3)), ('struggles', (3, 4)), ('"" in - the -wild "" conditions', (8, 15)), ('test set', (18, 20)), ('worst', (24, 25))]","[['struggles', 'to fit', '"" in - the -wild "" conditions'], ['"" in - the -wild "" conditions', 'present in', 'test set'], ['Classic model', 'performs', 'worst']]","[['Classic model', 'has', 'struggles']]",[],"[['Results', 'has', 'Classic model']]",face_alignment,0,222
1205,results,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .","[('able to', (15, 17)), ('recover', (17, 18)), ('due to', (21, 23)), ('for', (27, 28))]","[('texture - free Linear model', (1, 6)), ('does better', (6, 8)), ('ITW model', (11, 13)), ('facial shapes', (19, 21)), ('ideal feature basis', (24, 27)), ('"" in - the -wild "" conditions', (29, 36))]","[['ITW model', 'recover', 'facial shapes'], ['facial shapes', 'due to', 'ideal feature basis'], ['ideal feature basis', 'for', '"" in - the -wild "" conditions']]","[['texture - free Linear model', 'has', 'does better'], ['does better', 'has', 'ITW model']]",[],"[['Results', 'has', 'texture - free Linear model']]",face_alignment,0,223
1206,results,"We note that in a wide variety of expression , identity , lighting and occlusion conditions our model is able to robustly reconstruct a realistic 3 D facial shape that stands up to scrutiny .","[('in', (3, 4)), ('of', (7, 8)), ('able to', (19, 21)), ('robustly reconstruct', (21, 23)), ('stands up to', (30, 33))]","[('wide variety', (5, 7)), ('expression , identity , lighting and occlusion conditions', (8, 16)), ('our model', (16, 18)), ('realistic 3 D facial shape', (24, 29)), ('scrutiny', (33, 34))]","[['wide variety', 'of', 'expression , identity , lighting and occlusion conditions'], ['our model', 'robustly reconstruct', 'realistic 3 D facial shape'], ['realistic 3 D facial shape', 'stands up to', 'scrutiny']]","[['wide variety', 'has', 'expression , identity , lighting and occlusion conditions'], ['expression , identity , lighting and occlusion conditions', 'has', 'our model']]","[['Results', 'in', 'wide variety']]",[],face_alignment,0,225
1207,results,Quantitative Normal Recovery,[],"[('Quantitative Normal Recovery', (0, 3))]",[],[],[],"[['Results', 'has', 'Quantitative Normal Recovery']]",face_alignment,0,226
1208,experiments,ITW slightly outperforms IMM even though both IMM and PS - NL use all four available images of each subject .,[],"[('ITW', (0, 1)), ('slightly outperforms', (1, 3)), ('IMM', (3, 4))]",[],"[['ITW', 'has', 'slightly outperforms'], ['slightly outperforms', 'has', 'IMM']]",[],[],face_alignment,0,231
1209,research-problem,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,[],"[('Large Pose 3D Face Reconstruction from a Single Image', (0, 9))]",[],[],[],[],face_alignment,1,2
1210,research-problem,Abstract 3 D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty .,[],"[('3 D face reconstruction', (1, 5))]",[],[],[],[],face_alignment,1,5
1211,model,"We describe a very simple approach which bypasses many of the difficulties encountered in 3D face reconstruction by using a novel volumetric representation of the 3D facial geometry , and an appropriate CNN architecture that is trained to regress directly from a 2 D facial image to the corresponding 3D volume .","[('bypasses', (7, 8)), ('encountered in', (12, 14)), ('by using', (17, 19)), ('of', (23, 24)), ('trained to', (36, 38)), ('directly from', (39, 41)), ('to', (46, 47))]","[('many of the difficulties', (8, 12)), ('3D face reconstruction', (14, 17)), ('novel volumetric representation', (20, 23)), ('3D facial geometry', (25, 28)), ('appropriate CNN architecture', (31, 34)), ('regress', (38, 39)), ('2 D facial image', (42, 46)), ('corresponding 3D volume', (48, 51))]","[['many of the difficulties', 'encountered in', '3D face reconstruction'], ['many of the difficulties', 'by using', 'appropriate CNN architecture'], ['3D face reconstruction', 'by using', 'novel volumetric representation'], ['3D face reconstruction', 'by using', 'appropriate CNN architecture'], ['novel volumetric representation', 'of', '3D facial geometry'], ['appropriate CNN architecture', 'trained to', 'regress'], ['regress', 'directly from', '2 D facial image'], ['2 D facial image', 'to', 'corresponding 3D volume']]",[],"[['Model', 'bypasses', 'many of the difficulties']]",[],face_alignment,1,33
1212,hyperparameters,"Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .","[('trained', (5, 6)), ('using', (11, 12)), ('with', (13, 14)), ('of', (18, 19)), ('after', (26, 27)), ('to', (29, 30))]","[('Each of our architectures', (0, 4)), ('end - to - end', (6, 11)), ('RMSProp', (12, 13)), ('initial learning rate', (15, 18)), ('10 ? 4', (19, 22)), ('lowered', (25, 26)), ('40 epochs', (27, 29)), ('10 ?5', (30, 32))]","[['Each of our architectures', 'trained', 'end - to - end'], ['Each of our architectures', 'using', 'RMSProp'], ['end - to - end', 'using', 'RMSProp'], ['RMSProp', 'with', 'initial learning rate'], ['initial learning rate', 'of', '10 ? 4'], ['lowered', 'after', '40 epochs'], ['40 epochs', 'to', '10 ?5']]",[],[],"[['Hyperparameters', 'has', 'Each of our architectures']]",face_alignment,1,147
1213,hyperparameters,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .","[('During', (0, 1)), ('applied to', (6, 8)), ('applied', (25, 26))]","[('training', (1, 2)), ('random augmentation', (3, 5)), ('each input sample ( face image )', (8, 15)), ('corresponding target ( 3D volume )', (17, 23)), ('in - plane rotation r', (26, 31)), ('translation t z , t y', (40, 46)), ('scale s', (56, 58))]","[['random augmentation', 'applied to', 'each input sample ( face image )']]","[['training', 'has', 'random augmentation']]","[['Hyperparameters', 'During', 'training']]",[],face_alignment,1,148
1214,hyperparameters,"In 20 % of cases , the input and target were flipped horizontally .","[('In', (0, 1))]","[('20 % of cases', (1, 5)), ('input and target', (7, 10)), ('flipped horizontally', (11, 13))]",[],"[['20 % of cases', 'has', 'input and target']]","[['Hyperparameters', 'In', '20 % of cases']]",[],face_alignment,1,149
1215,hyperparameters,"Finally , the input samples were adjusted with some colour scaling on each RGB channel .","[('adjusted with', (6, 8)), ('on', (11, 12))]","[('input samples', (3, 5)), ('colour scaling', (9, 11)), ('each RGB channel', (12, 15))]","[['input samples', 'adjusted with', 'colour scaling'], ['colour scaling', 'on', 'each RGB channel']]",[],[],"[['Hyperparameters', 'has', 'input samples']]",face_alignment,1,150
1216,hyperparameters,"In the case of the VRN - Guided , the landmark detection module was trained to regress Gaussians with standard deviation of approximately 3 pixels (? = 1 ) .","[('of', (3, 4)), ('trained to', (14, 16)), ('regress', (16, 17)), ('with', (18, 19)), ('of', (21, 22))]","[('case', (2, 3)), ('VRN - Guided', (5, 8)), ('landmark detection module', (10, 13)), ('Gaussians', (17, 18)), ('standard deviation', (19, 21)), ('approximately 3 pixels (? = 1 )', (22, 29))]","[['case', 'of', 'VRN - Guided'], ['standard deviation', 'of', 'approximately 3 pixels (? = 1 )'], ['landmark detection module', 'regress', 'Gaussians'], ['Gaussians', 'with', 'standard deviation'], ['standard deviation', 'of', 'approximately 3 pixels (? = 1 )']]","[['case', 'has', 'VRN - Guided'], ['VRN - Guided', 'has', 'landmark detection module']]",[],[],face_alignment,1,151
1217,results,Volumetric Regression Networks largely outperform,[],"[('Volumetric Regression Networks', (0, 3)), ('largely outperform', (3, 5))]",[],"[['Volumetric Regression Networks', 'has', 'largely outperform']]",[],"[['Results', 'has', 'Volumetric Regression Networks']]",face_alignment,1,159
1218,results,"3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .","[('on', (3, 4))]","[('3DDFA and EOS', (0, 3)), ('all datasets', (4, 6))]","[['3DDFA and EOS', 'on', 'all datasets']]","[['3DDFA and EOS', 'has', 'all datasets']]",[],[],face_alignment,1,160
1219,results,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .","[('perform well', (4, 6)), ('across', (6, 7)), ('of', (10, 11))]","[('All VRNs', (2, 4)), ('whole spectrum', (8, 10)), ('facial poses', (11, 13)), ('expressions', (14, 15)), ('occlusions', (16, 17))]","[['All VRNs', 'perform well', 'whole spectrum'], ['whole spectrum', 'of', 'facial poses']]",[],[],"[['Results', 'has', 'All VRNs']]",face_alignment,1,161
1220,results,"3 . The best performing VRN is the one guided by detected landmarks , however at the cost of higher computational complexity :","[('guided by', (9, 11)), ('at', (15, 16)), ('of', (18, 19))]","[('best performing VRN', (3, 6)), ('detected landmarks', (11, 13)), ('cost', (17, 18)), ('higher computational complexity', (19, 22))]","[['detected landmarks', 'at', 'cost'], ['cost', 'of', 'higher computational complexity']]","[['cost', 'has', 'higher computational complexity']]",[],"[['Results', 'has', 'best performing VRN']]",face_alignment,1,163
1221,results,"4 . VRN - Multitask does not always perform particularly better than the plain VRN ( in fact on BU - 4 DFE it performs worse ) , not justifying the increase of network complexity .","[('not always perform', (6, 9)), ('than', (11, 12))]","[('VRN - Multitask', (2, 5)), ('particularly better', (9, 11)), ('plain VRN', (13, 15))]","[['VRN - Multitask', 'not always perform', 'particularly better'], ['particularly better', 'than', 'plain VRN']]",[],[],"[['Results', 'has', 'VRN - Multitask']]",face_alignment,1,165
1222,ablation-analysis,Effect of pose .,[],"[('Effect of pose', (0, 3))]",[],[],[],"[['Ablation analysis', 'has', 'Effect of pose']]",face_alignment,1,200
1223,ablation-analysis,"As shown in , the performance of our method decreases as the pose increases .","[('of', (6, 7)), ('as', (10, 11))]","[('performance', (5, 6)), ('our method', (7, 9)), ('decreases', (9, 10)), ('pose', (12, 13)), ('increases', (13, 14))]","[['performance', 'of', 'our method'], ['decreases', 'as', 'pose']]","[['performance', 'has', 'our method'], ['our method', 'has', 'decreases'], ['pose', 'has', 'increases']]",[],"[['Ablation analysis', 'has', 'performance']]",face_alignment,1,202
1224,ablation-analysis,Effect of expression .,[],"[('Effect of expression', (0, 3))]",[],[],[],"[['Ablation analysis', 'has', 'Effect of expression']]",face_alignment,1,205
1225,ablation-analysis,"This kind of extreme acted facial expressions generally do not occur in the training set , yet as shown in , the performance variation across different expressions is quite minor .","[('across', (24, 25))]","[('performance variation', (22, 24)), ('different expressions', (25, 27)), ('quite minor', (28, 30))]","[['performance variation', 'across', 'different expressions']]",[],[],"[['Ablation analysis', 'has', 'performance variation']]",face_alignment,1,208
1226,ablation-analysis,Effect of Gaussian size for guidance .,[],"[('Effect of Gaussian size for guidance', (0, 6))]",[],[],[],"[['Ablation analysis', 'has', 'Effect of Gaussian size for guidance']]",face_alignment,1,209
1227,ablation-analysis,"The performance of the 3D reconstruction dropped by a negligible amount , suggesting that as long as the Gaussians are of a sensible size , guidance will always help .","[('of', (2, 3)), ('by', (7, 8))]","[('performance', (1, 2)), ('3D reconstruction', (4, 6)), ('dropped', (6, 7)), ('negligible amount', (9, 11))]","[['performance', 'of', '3D reconstruction'], ['dropped', 'by', 'negligible amount']]","[['3D reconstruction', 'has', 'dropped']]",[],"[['Ablation analysis', 'has', 'performance']]",face_alignment,1,211
1228,research-problem,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,[],"[('Robust Facial Landmark Localisation', (3, 7))]",[],[],[],[],face_alignment,10,2
1229,research-problem,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .",[],"[('Facial landmark localisation', (0, 3)), ('face alignment', (5, 7))]",[],[],[],[],face_alignment,10,14
1230,model,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .","[('namely', (5, 6)), ('to improve', (13, 15)), ('for', (21, 22))]","[('novel loss function', (1, 4)), ('Wing loss', (7, 9)), ('deep neural network training capability', (16, 21)), ('small and medium range errors', (22, 27))]","[['novel loss function', 'namely', 'Wing loss'], ['novel loss function', 'to improve', 'deep neural network training capability'], ['deep neural network training capability', 'for', 'small and medium range errors']]","[['novel loss function', 'has', 'Wing loss']]",[],[],face_alignment,10,34
1231,model,"a data augmentation strategy , i.e. pose - based data balancing , that compensates the low frequency of occurrence of samples with large out - of - plane head rotations in the training set .","[('i.e.', (5, 6)), ('compensates', (13, 14)), ('of', (17, 18)), ('of', (19, 20)), ('with', (21, 22)), ('in', (30, 31))]","[('data augmentation strategy', (1, 4)), ('pose - based data balancing', (6, 11)), ('low frequency', (15, 17)), ('occurrence', (18, 19)), ('samples', (20, 21)), ('large out - of - plane head rotations', (22, 30)), ('training set', (32, 34))]","[['data augmentation strategy', 'i.e.', 'pose - based data balancing'], ['data augmentation strategy', 'compensates', 'low frequency'], ['pose - based data balancing', 'compensates', 'low frequency'], ['low frequency', 'of', 'occurrence'], ['occurrence', 'of', 'samples'], ['occurrence', 'of', 'samples'], ['occurrence', 'with', 'large out - of - plane head rotations'], ['samples', 'with', 'large out - of - plane head rotations'], ['large out - of - plane head rotations', 'in', 'training set']]","[['data augmentation strategy', 'name', 'pose - based data balancing'], ['low frequency', 'has', 'occurrence']]",[],[],face_alignment,10,35
1232,research-problem,a two - stage facial landmark localisation framework for performance boosting .,"[('for', (8, 9))]","[('two - stage facial landmark localisation framework', (1, 8)), ('performance boosting', (9, 11))]","[['two - stage facial landmark localisation framework', 'for', 'performance boosting']]",[],[],[],face_alignment,10,36
1233,experimental-setup,"In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .","[('used', (5, 6))]","[('Matlab 2017a', (6, 8)), ('Mat - ConvNet toolbox', (10, 14))]",[],[],"[['Experimental setup', 'used', 'Matlab 2017a']]",[],face_alignment,10,209
1234,experimental-setup,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .","[('of', (4, 5)), ('conducted on', (8, 10)), ('running', (12, 13)), ('with', (15, 16))]","[('training and testing', (1, 4)), ('our networks', (5, 7)), ('server', (11, 12)), ('Ubuntu 16.04', (13, 15)), ('Intel Xeon E5-2667 v4 CPU', (17, 22)), ('256 GB RAM', (23, 26)), ('4 NVIDIA GeForce GTX Titan X ( Pascal ) cards', (27, 37))]","[['training and testing', 'of', 'our networks'], ['training and testing', 'conducted on', 'server'], ['training and testing', 'conducted on', '4 NVIDIA GeForce GTX Titan X ( Pascal ) cards'], ['our networks', 'conducted on', 'server'], ['our networks', 'conducted on', '4 NVIDIA GeForce GTX Titan X ( Pascal ) cards'], ['server', 'running', 'Ubuntu 16.04'], ['server', 'running', '4 NVIDIA GeForce GTX Titan X ( Pascal ) cards'], ['Ubuntu 16.04', 'with', 'Intel Xeon E5-2667 v4 CPU'], ['Ubuntu 16.04', 'with', '4 NVIDIA GeForce GTX Titan X ( Pascal ) cards']]",[],[],"[['Experimental setup', 'has', 'training and testing']]",face_alignment,10,210
1235,experimental-setup,Note that we only use one GPU card for measuring the run time .,"[('for measuring', (8, 10))]","[('only use one GPU card', (3, 8)), ('run time', (11, 13))]","[['only use one GPU card', 'for measuring', 'run time']]",[],[],[],face_alignment,10,211
1236,experimental-setup,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .","[('set', (1, 2)), ('to', (5, 6)), ('to', (12, 13)), ('to', (17, 18)), ('for', (19, 20))]","[('weight decay', (3, 5)), ('5 10 ? 4', (6, 10)), ('momentum', (11, 12)), ('0.9', (13, 14)), ('batch size', (15, 17)), ('8', (18, 19)), ('network training', (20, 22))]","[['weight decay', 'to', '5 10 ? 4'], ['momentum', 'to', '0.9'], ['batch size', 'to', '8'], ['momentum', 'to', '0.9'], ['batch size', 'to', '8'], ['batch size', 'to', '8'], ['8', 'for', 'network training']]","[['weight decay', 'has', '5 10 ? 4'], ['momentum', 'has', '0.9'], ['batch size', 'has', '8']]","[['Experimental setup', 'set', 'weight decay']]",[],face_alignment,10,212
1237,experimental-setup,Each model was trained for 120 k iterations .,"[('trained for', (3, 5))]","[('Each model', (0, 2)), ('120 k iterations', (5, 8))]","[['Each model', 'trained for', '120 k iterations']]",[],[],"[['Experimental setup', 'has', 'Each model']]",face_alignment,10,213
1238,experimental-setup,"The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .","[('used for', (5, 7)), ('with', (13, 14)), ('of', (16, 17)), ('used to', (19, 21))]","[('standard ReLu function', (1, 4)), ('nonlinear activation', (7, 9)), ('Max pooling', (11, 13)), ('stride', (15, 16)), ('2', (17, 18)), ('downsize', (21, 22)), ('feature maps', (22, 24))]","[['standard ReLu function', 'used for', 'nonlinear activation'], ['Max pooling', 'with', 'stride'], ['stride', 'of', '2'], ['Max pooling', 'used to', 'downsize']]","[['downsize', 'has', 'feature maps']]",[],"[['Experimental setup', 'has', 'standard ReLu function']]",face_alignment,10,215
1239,experimental-setup,"For the convolutional layer , we used 3 3 kernels with the stride of 1 .","[('For', (0, 1)), ('used', (6, 7)), ('with', (10, 11)), ('of', (13, 14))]","[('convolutional layer', (2, 4)), ('3 3 kernels', (7, 10)), ('stride', (12, 13)), ('1', (14, 15))]","[['convolutional layer', 'used', '3 3 kernels'], ['3 3 kernels', 'with', 'stride'], ['stride', 'of', '1']]",[],"[['Experimental setup', 'For', 'convolutional layer']]",[],face_alignment,10,216
1240,experimental-setup,"For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .","[('set to', (12, 14)), ('for', (15, 16)), ('for', (19, 20))]","[('proposed PDB strategy', (2, 5)), ('number of bins K', (7, 11)), ('17', (14, 15)), ('AFLW', (16, 17)), ('9', (18, 19)), ('300W', (20, 21))]","[['number of bins K', 'set to', '17'], ['17', 'for', 'AFLW'], ['17', 'for', '9'], ['9', 'for', '300W'], ['9', 'for', '300W']]","[['proposed PDB strategy', 'has', 'number of bins K'], ['number of bins K', 'has', '17']]",[],[],face_alignment,10,218
1241,experimental-setup,"For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .","[('reduced', (15, 16)), ('from', (19, 20)), ('to', (24, 25)), ('for', (28, 29)), ('to', (38, 39)), ('for', (43, 44))]","[('CNN - 6', (1, 4)), ('input image size', (6, 9)), ('64 64 3', (10, 13)), ('learning rate', (17, 19)), ('3 10 ? 6', (20, 24)), ('3 10 ?8', (25, 28)), ('L2 loss', (30, 32)), ('3 10 ?5', (35, 38)), ('3 10 ? 7', (39, 43)), ('other loss functions', (45, 48))]","[['input image size', 'reduced', 'learning rate'], ['learning rate', 'from', '3 10 ? 6'], ['learning rate', 'from', '3 10 ?5'], ['3 10 ? 6', 'to', '3 10 ?8'], ['3 10 ?8', 'for', 'L2 loss'], ['3 10 ?5', 'to', '3 10 ? 7'], ['3 10 ? 7', 'for', 'other loss functions']]","[['CNN - 6', 'has', 'input image size'], ['input image size', 'has', '64 64 3'], ['learning rate', 'has', '3 10 ? 6']]",[],[],face_alignment,10,219
1242,experimental-setup,"The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .","[('of', (2, 3)), ('set', (7, 8)), ('is', (24, 25)), ('reduced', (30, 31)), ('from', (34, 35)), ('to', (39, 40)), ('for', (43, 44)), ('to', (54, 55)), ('for', (59, 60))]","[('parameters', (1, 2)), ('Wing loss', (4, 6)), ('tow = 10 and = 2', (8, 14)), ('CNN - 7', (16, 19)), ('input image size', (21, 24)), ('128 128 3', (25, 28)), ('learning rate', (32, 34)), ('1 10 ? 6', (35, 39)), ('1 10 ?8', (40, 43)), ('L2 loss', (45, 47)), ('1 10 ? 5', (50, 54)), ('1 10 ? 7', (55, 59)), ('other loss functions', (61, 64))]","[['parameters', 'of', 'Wing loss'], ['parameters', 'set', 'tow = 10 and = 2'], ['Wing loss', 'set', 'tow = 10 and = 2'], ['input image size', 'is', '128 128 3'], ['learning rate', 'from', '1 10 ? 6'], ['1 10 ? 6', 'to', '1 10 ?8'], ['1 10 ?8', 'for', 'L2 loss'], ['1 10 ? 5', 'to', '1 10 ? 7'], ['1 10 ? 7', 'for', 'other loss functions']]","[['parameters', 'has', 'Wing loss'], ['CNN - 7', 'has', 'input image size'], ['input image size', 'has', '128 128 3'], ['learning rate', 'has', '1 10 ? 6']]",[],"[['Experimental setup', 'has', 'parameters']]",face_alignment,10,220
1243,experimental-setup,"To perform data augmentation , we randomly rotated each training image between [ ? 30 , 30 ] degrees for CNN - 6 and between [ ? 10 , 10 ] degrees for CNN - 7 .","[('To perform', (0, 2)), ('randomly rotated', (6, 8)), ('between', (11, 12)), ('for', (19, 20)), ('for', (32, 33))]","[('data augmentation', (2, 4)), ('training image', (9, 11)), ('[ ? 30 , 30 ] degrees', (12, 19)), ('CNN - 6', (20, 23)), ('[ ? 10 , 10 ] degrees', (25, 32)), ('CNN - 7', (33, 36))]","[['data augmentation', 'randomly rotated', 'training image'], ['training image', 'between', '[ ? 30 , 30 ] degrees'], ['training image', 'between', '[ ? 10 , 10 ] degrees'], ['[ ? 30 , 30 ] degrees', 'for', 'CNN - 6'], ['[ ? 10 , 10 ] degrees', 'for', 'CNN - 7']]",[],"[['Experimental setup', 'To perform', 'data augmentation']]",[],face_alignment,10,222
1244,experimental-setup,"For bounding box perturbation , we applied random translations to the upper-left and bottom - right corners of the face bounding box within 5 % of the bounding .","[('applied', (6, 7)), ('to', (9, 10)), ('of', (17, 18)), ('within', (22, 23)), ('of', (25, 26))]","[('bounding box perturbation', (1, 4)), ('random translations', (7, 9)), ('upper-left and bottom - right corners', (11, 17)), ('face bounding box', (19, 22)), ('5 %', (23, 25)), ('bounding', (27, 28))]","[['bounding box perturbation', 'applied', 'random translations'], ['random translations', 'to', 'upper-left and bottom - right corners'], ['upper-left and bottom - right corners', 'of', 'face bounding box'], ['face bounding box', 'within', '5 %'], ['upper-left and bottom - right corners', 'of', 'face bounding box'], ['5 %', 'of', 'bounding']]",[],[],[],face_alignment,10,224
1245,experimental-setup,"We compare our method with a set of state - of - the - art approaches , including SDM , ERT , RCPR , CFSS , LBF , GRF , CCL , DAC - CSR and TR - DRN . box size .","[('including', (17, 18))]","[('set of state - of - the - art approaches', (6, 16)), ('SDM', (18, 19)), ('ERT', (20, 21)), ('RCPR', (22, 23)), ('CFSS', (24, 25)), ('LBF', (26, 27)), ('GRF', (28, 29)), ('CCL', (30, 31)), ('DAC - CSR', (32, 35)), ('TR - DRN', (36, 39))]","[['set of state - of - the - art approaches', 'including', 'SDM'], ['set of state - of - the - art approaches', 'including', 'ERT'], ['set of state - of - the - art approaches', 'including', 'RCPR'], ['set of state - of - the - art approaches', 'including', 'CFSS'], ['set of state - of - the - art approaches', 'including', 'LBF'], ['set of state - of - the - art approaches', 'including', 'GRF'], ['set of state - of - the - art approaches', 'including', 'CCL'], ['set of state - of - the - art approaches', 'including', 'DAC - CSR'], ['set of state - of - the - art approaches', 'including', 'TR - DRN']]","[['set of state - of - the - art approaches', 'name', 'SDM']]",[],[],face_alignment,10,226
1246,experimental-setup,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .","[('randomly injected', (3, 5)), ('to', (11, 12)), ('with', (15, 16)), ('of', (18, 19))]","[('Gaussian blur (? = 1 )', (5, 11)), ('each training image', (12, 15)), ('probability', (17, 18)), ('50 %', (19, 21))]","[['Gaussian blur (? = 1 )', 'to', 'each training image'], ['each training image', 'with', 'probability'], ['probability', 'of', '50 %']]","[['probability', 'has', '50 %']]","[['Experimental setup', 'randomly injected', 'Gaussian blur (? = 1 )']]",[],face_alignment,10,227
1247,results,Comparison with state of the art 7.2.1 AFLW,[],"[('AFLW', (7, 8))]",[],[],[],[],face_alignment,10,234
1248,results,"As shown in , our CNN - 6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function ( magenta solid line ) .","[('outperforms', (9, 10))]","[('our CNN - 6/7 network', (4, 9)), ('all the other approaches', (10, 14))]","[['our CNN - 6/7 network', 'outperforms', 'all the other approaches']]",[],[],"[['Results', 'has', 'our CNN - 6/7 network']]",face_alignment,10,246
1249,results,"Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .","[('by', (2, 3)), ('simply switching', (3, 5)), ('from', (8, 9)), ('to', (10, 11)), ('of', (18, 19))]","[('loss function', (6, 8)), ('L2', (9, 10)), ('L1', (11, 12)), ('smooth L1', (13, 15)), ('performance', (17, 18)), ('our method', (19, 21)), ('improved significantly', (23, 25))]","[['loss function', 'from', 'L2'], ['L2', 'to', 'L1'], ['performance', 'of', 'our method']]",[],[],[],face_alignment,10,248
1250,results,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .","[('use of', (3, 5)), ('further improves', (11, 13))]","[('newly proposed Wing loss function', (6, 11)), ('accuracy', (14, 15))]","[['newly proposed Wing loss function', 'further improves', 'accuracy']]",[],[],[],face_alignment,10,249
1251,experimental-setup,300 W,[],"[('300 W', (0, 2))]",[],[],[],"[['Experimental setup', 'has', '300 W']]",face_alignment,10,251
1252,results,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .","[('with', (11, 12)), ('outperforms', (22, 23)), ('on', (32, 33))]","[('our two - stage landmark localisation framework', (4, 11)), ('PDB strategy', (13, 15)), ('newly proposed Wing loss function', (17, 22)), ('other stateof - the - art algorithms', (25, 32)), ('300 W dataset', (34, 37))]","[['our two - stage landmark localisation framework', 'with', 'PDB strategy'], ['newly proposed Wing loss function', 'outperforms', 'other stateof - the - art algorithms'], ['other stateof - the - art algorithms', 'on', '300 W dataset']]","[['our two - stage landmark localisation framework', 'has', 'PDB strategy']]",[],"[['Results', 'has', 'our two - stage landmark localisation framework']]",face_alignment,10,262
1253,results,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,"[('reduced by', (4, 6)), ('compared to', (10, 12)), ('reported by', (16, 18))]","[('error', (1, 2)), ('almost 20 %', (6, 9)), ('current best result', (13, 16)), ('RAR algorithm', (19, 21))]","[['error', 'reduced by', 'almost 20 %'], ['almost 20 %', 'compared to', 'current best result'], ['current best result', 'reported by', 'RAR algorithm']]",[],[],"[['Results', 'has', 'error']]",face_alignment,10,263
1254,research-problem,Unsupervised Training for 3D Morphable Model Regression,[],"[('3D Morphable Model Regression', (3, 7))]",[],[],[],[],face_alignment,11,2
1255,model,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,"[('presents', (2, 3)), ('for', (5, 6)), ('removes', (11, 12)), ('for', (15, 16)), ('on', (22, 23)), ('to reproduce', (25, 27))]","[('method', (4, 5)), ('training', (6, 7)), ('regression network', (8, 10)), ('need', (14, 15)), ('supervised training data', (16, 19)), ('reliance', (21, 22)), ('inverse rendering', (23, 25)), ('image pixels', (27, 29))]","[['method', 'for', 'training'], ['need', 'for', 'supervised training data'], ['regression network', 'removes', 'need'], ['regression network', 'removes', 'reliance'], ['need', 'for', 'supervised training data'], ['reliance', 'on', 'inverse rendering'], ['inverse rendering', 'to reproduce', 'image pixels']]","[['method', 'has', 'training'], ['training', 'has', 'regression network']]","[['Model', 'presents', 'method']]",[],face_alignment,11,20
1256,model,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .","[('learns to', (4, 6)), ('minimize', (6, 7)), ('based on', (9, 11)), ('produced by', (15, 17)), ('such as', (21, 23))]","[('network', (3, 4)), ('loss', (8, 9)), ('facial identity features', (12, 15)), ('face recognition network', (18, 21)), ('VGG - Face', (23, 26)), (""Google 's FaceNet"", (27, 30))]","[['network', 'minimize', 'loss'], ['loss', 'based on', 'facial identity features'], ['facial identity features', 'produced by', 'face recognition network'], ['face recognition network', 'such as', 'VGG - Face'], ['face recognition network', 'such as', ""Google 's FaceNet""]]",[],[],"[['Model', 'has', 'network']]",face_alignment,11,21
1257,model,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,"[('exploit', (1, 2)), ('to apply', (4, 6)), ('matches', (9, 10)), ('between', (13, 14)), ('of', (21, 22))]","[('invariance', (3, 4)), ('loss', (7, 8)), ('identity features', (11, 13)), ('input photograph', (15, 17)), ('synthetic rendering', (19, 21)), ('predicted face', (23, 25))]","[['invariance', 'to apply', 'loss'], ['loss', 'matches', 'identity features'], ['identity features', 'between', 'input photograph'], ['identity features', 'between', 'synthetic rendering'], ['synthetic rendering', 'of', 'predicted face']]","[['invariance', 'has', 'loss']]","[['Model', 'exploit', 'invariance']]",[],face_alignment,11,23
1258,model,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .","[('alleviate', (1, 2)), ('by applying', (5, 7)), ('to match', (15, 17)), ('of', (19, 20)), ('to', (23, 24)), ('of', (26, 27)), ('to ensure', (34, 36)), ('correctly reinterpret', (40, 42)), ('combines', (52, 53)), ('from', (54, 55))]","[('fooling problem', (3, 5)), ('three novel losses', (7, 10)), ('batch distribution loss', (12, 15)), ('statistics', (18, 19)), ('each training batch', (20, 23)), ('morphable model', (28, 30)), ('loopback loss', (32, 34)), ('regression network', (37, 39)), ('own output', (43, 45)), ('multi-view identity loss', (48, 51)), ('features', (53, 54)), ('multiple , independent views', (55, 59)), ('predicted shape', (61, 63))]","[['fooling problem', 'by applying', 'three novel losses'], ['batch distribution loss', 'to match', 'statistics'], ['statistics', 'of', 'each training batch'], ['multiple , independent views', 'of', 'predicted shape'], ['loopback loss', 'to ensure', 'regression network'], ['regression network', 'correctly reinterpret', 'own output'], ['multi-view identity loss', 'combines', 'features'], ['features', 'from', 'multiple , independent views']]","[['three novel losses', 'name', 'batch distribution loss']]","[['Model', 'alleviate', 'fooling problem']]",[],face_alignment,11,26
1259,model,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .","[('train', (5, 6)), ('using only', (13, 15)), ('of', (28, 29))]","[('3D shape and texture regression network', (7, 13)), ('face recognition network', (16, 19)), ('morphable face model', (21, 24)), ('dataset', (27, 28)), ('unlabeled face images', (29, 32))]","[['3D shape and texture regression network', 'using only', 'face recognition network'], ['3D shape and texture regression network', 'using only', 'morphable face model'], ['3D shape and texture regression network', 'using only', 'dataset'], ['dataset', 'of', 'unlabeled face images']]",[],"[['Model', 'train', '3D shape and texture regression network']]",[],face_alignment,11,27
1260,experiments,"We show that despite learning from unlabeled photographs , the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects .","[('show', (1, 2)), ('on', (14, 15)), ('of', (17, 18))]","[('3D face results', (10, 13)), ('improve', (13, 14)), ('accuracy', (16, 17)), ('previous work', (18, 20))]","[['improve', 'on', 'accuracy'], ['accuracy', 'of', 'previous work']]","[['3D face results', 'has', 'improve']]",[],[],face_alignment,11,28
1261,results,Neutral Pose Reconstruction on MICC,[],"[('Neutral Pose Reconstruction on MICC', (0, 5))]",[],[],[],"[['Results', 'has', 'Neutral Pose Reconstruction on MICC']]",face_alignment,11,184
1262,results,"Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .","[('to', (9, 10)), ('by', (13, 14))]","[('Our results', (0, 2)), ('improved absolute error', (6, 9)), ('ground truth', (11, 13)), ('20 - 25 %', (14, 18))]","[['improved absolute error', 'to', 'ground truth'], ['ground truth', 'by', '20 - 25 %']]",[],[],"[['Results', 'has', 'Our results']]",face_alignment,11,194
1263,results,"We are also more stable across changing environments , with similar results for all three test sets .","[('across', (5, 6))]","[('more stable', (3, 5)), ('changing environments', (6, 8))]","[['more stable', 'across', 'changing environments']]",[],[],[],face_alignment,11,195
1264,results,Face Recognition Results,[],"[('Face Recognition Results', (0, 3))]",[],[],[],"[['Results', 'has', 'Face Recognition Results']]",face_alignment,11,196
1265,results,Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .,"[('achieves', (2, 3)), ('between', (6, 7)), ('of', (10, 11)), ('on', (12, 13))]","[('Our method', (0, 2)), ('average similarity', (4, 6)), ('rendering and photo', (7, 10)), ('0.403', (11, 12)), ('MoFA test', (13, 15))]","[['Our method', 'achieves', 'average similarity'], ['average similarity', 'between', 'rendering and photo'], ['rendering and photo', 'of', '0.403'], ['0.403', 'on', 'MoFA test']]",[],[],"[['Results', 'has', 'Our method']]",face_alignment,11,202
1266,results,"Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .","[('closer to', (5, 7)), ('than', (12, 13))]","[(""Our method 's results"", (0, 4)), ('same - person distribution', (8, 12)), ('differentperson distribution', (14, 16))]","[[""Our method 's results"", 'closer to', 'same - person distribution'], ['same - person distribution', 'than', 'differentperson distribution']]",[],[],"[['Results', 'has', ""Our method 's results""]]",face_alignment,11,205
1267,results,"Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .","[('between', (4, 5)), ('with', (19, 20))]","[('distance', (3, 4)), ('GT distribution and the same - person LFW distribution', (6, 15)), ('very low', (16, 18)), ('same mean', (22, 24))]","[['distance', 'between', 'GT distribution and the same - person LFW distribution'], ['very low', 'with', 'same mean']]",[],[],[],face_alignment,11,211
1268,research-problem,Dense Face Alignment,[],"[('Dense Face Alignment', (0, 3))]",[],[],[],[],face_alignment,12,2
1269,research-problem,Face alignment is a classic problem in the computer vision field .,[],"[('Face alignment', (0, 2))]",[],[],[],[],face_alignment,12,4
1270,model,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .","[('learn', (9, 10)), ('to fit', (12, 14)), ('to', (19, 20))]","[('CNN', (11, 12)), ('3 D face model', (15, 19)), ('face image', (21, 23))]","[['CNN', 'to fit', '3 D face model'], ['3 D face model', 'to', 'face image']]",[],"[['Model', 'learn', 'CNN']]",[],face_alignment,12,38
1271,model,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .","[('To tackle', (0, 2)), ('of', (4, 5)), ('employ', (12, 13))]","[('first challenge', (2, 4)), ('limited landmark labeling', (5, 8)), ('additional constraints', (13, 15))]","[['first challenge', 'of', 'limited landmark labeling'], ['limited landmark labeling', 'employ', 'additional constraints']]",[],"[['Model', 'To tackle', 'first challenge']]",[],face_alignment,12,41
1272,model,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .","[('include', (1, 2)), ('where', (4, 5)), ('of', (7, 8)), ('match', (12, 13)), ('detected on', (28, 30)), ('of', (33, 34)), ('map to', (38, 40)), ('on', (43, 44))]","[('contour constraint', (2, 4)), ('contour', (6, 7)), ('predicted shape', (9, 11)), ('detected 2 D face boundary', (14, 19)), ('SIFT constraint', (21, 23)), ('SIFT key points', (25, 28)), ('two face images', (30, 33)), ('same individual', (35, 37)), ('same vertexes', (41, 43)), ('3D face model', (45, 48))]","[['contour constraint', 'where', 'contour'], ['contour constraint', 'where', 'SIFT constraint'], ['SIFT constraint', 'where', 'SIFT key points'], ['contour', 'of', 'predicted shape'], ['two face images', 'of', 'same individual'], ['predicted shape', 'match', 'detected 2 D face boundary'], ['SIFT key points', 'detected on', 'two face images'], ['two face images', 'of', 'same individual'], ['SIFT key points', 'map to', 'same vertexes'], ['same vertexes', 'on', '3D face model']]","[['contour constraint', 'has', 'contour']]","[['Model', 'include', 'contour constraint']]",[],face_alignment,12,42
1273,model,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .","[('integrated into', (3, 5)), ('as', (8, 9)), ('where', (14, 15)), ('results in', (22, 24)), ('for', (27, 28))]","[('Both constraints', (0, 2)), ('CNN training', (6, 8)), ('additional loss function terms', (9, 13)), ('end - to - end training', (16, 22)), ('enhanced CNN', (25, 27)), ('3 D face model fitting', (28, 33))]","[['Both constraints', 'integrated into', 'CNN training'], ['CNN training', 'as', 'additional loss function terms'], ['additional loss function terms', 'where', 'end - to - end training'], ['end - to - end training', 'results in', 'enhanced CNN'], ['enhanced CNN', 'for', '3 D face model fitting']]",[],[],"[['Model', 'has', 'Both constraints']]",face_alignment,12,43
1274,model,"For the second challenge of leveraging multiple datasets , the 3D face model fitting approach has the inherent advantage in handling multiple training databases .","[('For', (0, 1)), ('leveraging', (5, 6))]","[('second challenge', (2, 4)), ('multiple datasets', (6, 8)), ('3D face model fitting approach', (10, 15)), ('inherent advantage', (17, 19)), ('handling', (20, 21)), ('multiple training databases', (21, 24))]","[['second challenge', 'leveraging', 'multiple datasets']]","[['multiple datasets', 'has', '3D face model fitting approach'], ['3D face model fitting approach', 'has', 'inherent advantage'], ['handling', 'has', 'multiple training databases']]","[['Model', 'For', 'second challenge']]",[],face_alignment,12,44
1275,experiments,Dense Face Alignment,[],"[('Dense Face Alignment', (0, 3))]",[],[],[],[],face_alignment,12,83
1276,hyperparameters,"To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .","[('To train', (0, 2)), ('use', (6, 7)), ('for', (14, 15))]","[('network', (3, 4)), ('20 , 10 , and 10 epochs', (7, 14)), ('stage 1 to 3', (15, 19))]","[['network', 'use', '20 , 10 , and 10 epochs'], ['20 , 10 , and 10 epochs', 'for', 'stage 1 to 3']]",[],"[['Hyperparameters', 'To train', 'network']]",[],face_alignment,12,203
1277,hyperparameters,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .","[('set', (1, 2)), ('as', (7, 8)), ('reduce', (14, 15)), ('by', (18, 19)), ('of', (21, 22)), ('when', (23, 24)), ('approaches', (27, 28))]","[('initial global learning rate', (3, 7)), ('1 e ? 3', (8, 12)), ('learning rate', (16, 18)), ('factor', (20, 21)), ('10', (22, 23)), ('training error', (25, 27)), ('plateau', (29, 30))]","[['initial global learning rate', 'as', '1 e ? 3'], ['learning rate', 'by', 'factor'], ['factor', 'of', '10'], ['10', 'when', 'training error'], ['training error', 'approaches', 'plateau']]","[['initial global learning rate', 'has', '1 e ? 3'], ['factor', 'has', '10']]","[['Hyperparameters', 'set', 'initial global learning rate']]",[],face_alignment,12,204
1278,hyperparameters,"The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .","[('for', (15, 16))]","[('minibatch size', (1, 3)), ('32', (4, 5)), ('weight decay', (6, 8)), ('0.005', (9, 10)), ('leak factor', (13, 15)), ('Leaky ReLU', (16, 18)), ('0.01', (19, 20))]","[['leak factor', 'for', 'Leaky ReLU']]","[['minibatch size', 'has', '32'], ['weight decay', 'has', '0.005']]",[],"[['Hyperparameters', 'has', 'minibatch size']]",face_alignment,12,205
1279,results,"For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .","[('For', (0, 1)), ('outperforms', (7, 8)), ('with', (11, 12)), ('of', (15, 16))]","[('AFLW - LFPA', (1, 4)), ('our method', (5, 7)), ('best methods', (9, 11)), ('large margin', (13, 15)), ('17.8 % improvement', (16, 19))]","[['our method', 'outperforms', 'best methods'], ['best methods', 'with', 'large margin'], ['large margin', 'of', '17.8 % improvement']]","[['AFLW - LFPA', 'has', 'our method']]","[['Results', 'For', 'AFLW - LFPA']]",[],face_alignment,12,218
1280,results,"For AFLW2000 - 3D , our method also shows a large improvement .","[('shows', (8, 9))]","[('AFLW2000 - 3D', (1, 4)), ('our method', (5, 7)), ('large improvement', (10, 12))]","[['our method', 'shows', 'large improvement']]","[['AFLW2000 - 3D', 'has', 'our method'], ['our method', 'has', 'large improvement']]",[],[],face_alignment,12,219
1281,results,"Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .","[('for', (2, 3)), ('with', (4, 5)), ('in', (7, 8)), ('improves', (16, 17)), ('by', (19, 20)), ('from', (23, 24)), ('to', (25, 26))]","[('images', (3, 4)), ('yaw angle', (5, 7)), ('[ 60 , 90 ]', (8, 13)), ('our method', (14, 16)), ('performance', (18, 19)), ('28 %', (20, 22)), ('7.93', (24, 25)), ('5.68', (26, 27))]","[['images', 'with', 'yaw angle'], ['yaw angle', 'in', '[ 60 , 90 ]'], ['yaw angle', 'in', 'our method'], ['our method', 'improves', 'performance'], ['performance', 'by', '28 %'], ['28 %', 'from', '7.93'], ['7.93', 'to', '5.68']]",[],"[['Results', 'for', 'images']]",[],face_alignment,12,220
1282,results,"For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .","[('reaches', (26, 27))]","[('IJB - A dataset', (2, 6)), ('our method', (23, 25)), ('higher accuracy', (28, 30))]","[['our method', 'reaches', 'higher accuracy']]","[['IJB - A dataset', 'has', 'our method']]",[],[],face_alignment,12,221
1283,results,"Even though the proposed method can handle largepose alignment , to show its performance on the near- frontal datasets , we evaluate our method on the 300W dataset .","[('on', (24, 25))]","[('300W dataset', (26, 28))]",[],[],"[['Results', 'on', '300W dataset']]",[],face_alignment,12,225
1284,results,Our method is the second best method on the challenging set .,"[('on', (7, 8))]","[('Our method', (0, 2)), ('second best method', (4, 7)), ('challenging set', (9, 11))]","[['second best method', 'on', 'challenging set']]","[['Our method', 'has', 'second best method']]",[],"[['Results', 'has', 'Our method']]",face_alignment,12,228
1285,results,"In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .","[('of', (5, 6)), ('comparable to', (9, 11)), ('designed for', (15, 17))]","[('performance', (4, 5)), ('our method', (6, 8)), ('other methods', (11, 13)), ('near - frontal datasets', (17, 21))]","[['performance', 'of', 'our method'], ['performance', 'comparable to', 'other methods'], ['our method', 'comparable to', 'other methods'], ['other methods', 'designed for', 'near - frontal datasets']]","[['performance', 'has', 'our method']]",[],"[['Results', 'has', 'performance']]",face_alignment,12,229
1286,ablation-analysis,The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,"[('of', (2, 3)), ('on', (5, 6)), ('by adding', (12, 14))]","[('accuracy', (1, 2)), ('our method', (3, 5)), ('AFLW2000 - 3D', (7, 10)), ('consistently improves', (10, 12)), ('more datasets', (14, 16))]","[['accuracy', 'of', 'our method'], ['accuracy', 'on', 'AFLW2000 - 3D'], ['our method', 'on', 'AFLW2000 - 3D'], ['consistently improves', 'by adding', 'more datasets']]",[],[],"[['Ablation analysis', 'has', 'accuracy']]",face_alignment,12,238
1287,ablation-analysis,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .","[('For', (0, 1)), ('achieves', (9, 10)), ('by utilizing', (17, 19)), ('in', (21, 22)), ('over', (28, 29))]","[('AFLW - PIFA dataset', (2, 6)), ('our method', (7, 9)), ('9.5 % and 20 %', (10, 15)), ('relative improvement', (15, 17)), ('datasets', (20, 21)), ('stage 2 and stage 3', (23, 28)), ('first stage', (30, 32))]","[['our method', 'achieves', '9.5 % and 20 %'], ['relative improvement', 'by utilizing', 'datasets'], ['datasets', 'in', 'stage 2 and stage 3'], ['stage 2 and stage 3', 'over', 'first stage']]","[['AFLW - PIFA dataset', 'has', 'our method'], ['9.5 % and 20 %', 'has', 'relative improvement']]","[['Ablation analysis', 'For', 'AFLW - PIFA dataset']]",[],face_alignment,12,239
1288,ablation-analysis,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .","[('including', (1, 2)), ('from', (4, 5)), ('have', (14, 15)), ('achieve', (20, 21)), ('of', (22, 23))]","[('datasets', (3, 4)), ('second and third stages', (7, 11)), ('26 % relative improvement', (15, 19)), ('NME', (21, 22)), ('3.86 %', (23, 25))]","[['datasets', 'from', 'second and third stages'], ['second and third stages', 'have', '26 % relative improvement'], ['26 % relative improvement', 'achieve', 'NME'], ['NME', 'of', '3.86 %']]","[['datasets', 'has', 'second and third stages']]","[['Ablation analysis', 'including', 'datasets']]",[],face_alignment,12,240
1289,ablation-analysis,Comparing LFC + SPC and LFC + CFC performances shows that the CFC is more helpful than the SPC .,"[('Comparing', (0, 1)), ('shows that', (9, 11)), ('than', (16, 17))]","[('LFC + SPC and LFC + CFC performances', (1, 9)), ('CFC', (12, 13)), ('more helpful', (14, 16)), ('SPC', (18, 19))]","[['LFC + SPC and LFC + CFC performances', 'shows that', 'CFC'], ['more helpful', 'than', 'SPC']]","[['LFC + SPC and LFC + CFC performances', 'has', 'CFC'], ['CFC', 'has', 'more helpful']]","[['Ablation analysis', 'Comparing', 'LFC + SPC and LFC + CFC performances']]",[],face_alignment,12,248
1290,ablation-analysis,Using all constraints achieves the best performance .,"[('Using', (0, 1)), ('achieves', (3, 4))]","[('all constraints', (1, 3)), ('best performance', (5, 7))]","[['all constraints', 'achieves', 'best performance']]","[['all constraints', 'has', 'best performance']]","[['Ablation analysis', 'Using', 'all constraints']]",[],face_alignment,12,250
1291,ablation-analysis,This result shows that for the images with NME - lp between 5 % and 15 % the SPC is helpful .,"[('shows', (2, 3)), ('with', (7, 8)), ('between', (11, 12))]","[('result', (1, 2)), ('images', (6, 7)), ('NME - lp', (8, 11)), ('5 % and 15 %', (12, 17)), ('SPC', (18, 19)), ('helpful', (20, 21))]","[['images', 'with', 'NME - lp'], ['NME - lp', 'between', '5 % and 15 %']]","[['result', 'has', 'images'], ['5 % and 15 %', 'has', 'SPC'], ['SPC', 'has', 'helpful']]",[],"[['Ablation analysis', 'has', 'result']]",face_alignment,12,254
1292,research-problem,"We demonstrate the superior representation power of our nonlinear 3 DMM over its linear counterpart , and its contribution to face alignment and 3D reconstruction .",[],"[('face alignment', (20, 22)), ('3D reconstruction', (23, 25))]",[],[],[],[],face_alignment,13,12
1293,model,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .","[('utilize', (3, 4)), ('instead of', (8, 10)), ('as', (14, 15))]","[('two network decoders', (4, 7)), ('two PCA spaces', (10, 13)), ('shape and texture model components', (16, 21))]","[['two network decoders', 'instead of', 'two PCA spaces'], ['two PCA spaces', 'as', 'shape and texture model components']]",[],"[['Model', 'utilize', 'two network decoders']]",[],face_alignment,13,45
1294,model,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .","[('design', (8, 9)), ('for', (11, 12)), ('for', (24, 25)), ('for', (33, 34))]","[('different networks', (9, 11)), ('shape and texture', (12, 15)), ('multi - layer perceptron ( MLP )', (17, 24)), ('shape', (25, 26)), ('convolutional neural network ( CNN )', (27, 33)), ('texture', (34, 35))]","[['different networks', 'for', 'shape and texture'], ['multi - layer perceptron ( MLP )', 'for', 'shape'], ['multi - layer perceptron ( MLP )', 'for', 'texture'], ['convolutional neural network ( CNN )', 'for', 'texture'], ['multi - layer perceptron ( MLP )', 'for', 'shape'], ['convolutional neural network ( CNN )', 'for', 'texture']]",[],"[['Model', 'design', 'different networks']]",[],face_alignment,13,46
1295,model,Each decoder will take a shape or texture representation as input and output the dense 3 D face or a face texture .,"[('take', (3, 4)), ('as', (9, 10)), ('output', (12, 13))]","[('Each decoder', (0, 2)), ('shape or texture representation', (5, 9)), ('input', (10, 11)), ('dense 3 D face or a face texture', (14, 22))]","[['Each decoder', 'take', 'shape or texture representation'], ['shape or texture representation', 'as', 'input'], ['Each decoder', 'output', 'dense 3 D face or a face texture']]",[],[],"[['Model', 'has', 'Each decoder']]",face_alignment,13,47
1296,model,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .","[('learn', (3, 4)), ('to', (7, 8)), ('formulated as', (15, 17))]","[('fitting algorithm', (5, 7)), ('our nonlinear 3 DMM', (8, 12)), ('CNN encoder', (18, 20))]","[['fitting algorithm', 'to', 'our nonlinear 3 DMM'], ['our nonlinear 3 DMM', 'formulated as', 'CNN encoder']]","[['fitting algorithm', 'has', 'our nonlinear 3 DMM']]","[['Model', 'learn', 'fitting algorithm']]",[],face_alignment,13,49
1297,model,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .","[('takes', (2, 3)), ('as', (8, 9)), ('generates', (11, 12)), ('from which', (18, 20)), ('estimate', (22, 23))]","[('encoder', (1, 2)), ('2 D face image', (4, 8)), ('input', (9, 10)), ('shape and texture parameters', (13, 17)), ('two decoders', (20, 22)), ('3D face and texture', (24, 28))]","[['encoder', 'takes', '2 D face image'], ['2 D face image', 'as', 'input'], ['encoder', 'generates', 'shape and texture parameters'], ['shape and texture parameters', 'from which', 'two decoders'], ['two decoders', 'estimate', '3D face and texture']]",[],[],"[['Model', 'has', 'encoder']]",face_alignment,13,50
1298,model,"The 3 D face and texture would perfectly reconstruct the input face , if the fitting algorithm and 3 DMM are well learnt .","[('perfectly reconstruct', (7, 9)), ('if', (13, 14))]","[('3 D face and texture', (1, 6)), ('input face', (10, 12)), ('fitting algorithm and 3 DMM', (15, 20)), ('well learnt', (21, 23))]","[['3 D face and texture', 'perfectly reconstruct', 'input face'], ['input face', 'if', 'fitting algorithm and 3 DMM']]",[],[],"[['Model', 'has', '3 D face and texture']]",face_alignment,13,51
1299,model,"Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .","[('to generate', (8, 10)), ('by fusing', (13, 15)), ('estimated by', (26, 28))]","[('differentiable rendering layer', (5, 8)), ('reconstructed face', (11, 13)), ('3D face', (16, 18)), ('texture', (19, 20)), ('camera projection parameters', (23, 26)), ('encoder', (29, 30))]","[['differentiable rendering layer', 'to generate', 'reconstructed face'], ['reconstructed face', 'by fusing', '3D face'], ['reconstructed face', 'by fusing', 'texture'], ['reconstructed face', 'by fusing', 'camera projection parameters'], ['camera projection parameters', 'estimated by', 'encoder']]",[],[],[],face_alignment,13,52
1300,model,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .","[('learnt', (17, 18)), ('to minimize', (19, 21)), ('between', (23, 24))]","[('endto - end learning scheme', (3, 8)), ('encoder and two decoders', (12, 16)), ('jointly', (18, 19)), ('difference', (22, 23)), ('reconstructed face', (25, 27)), ('input face', (29, 31))]","[['encoder and two decoders', 'learnt', 'jointly'], ['encoder and two decoders', 'to minimize', 'difference'], ['jointly', 'to minimize', 'difference'], ['difference', 'between', 'reconstructed face'], ['difference', 'between', 'input face']]",[],[],"[['Model', 'has', 'endto - end learning scheme']]",face_alignment,13,53
1301,model,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,"[('Jointly learning', (0, 2)), ('leverage', (13, 14)), ('of', (17, 18)), ('without relying on', (21, 24))]","[('3 DMM and the model fitting encoder', (3, 10)), ('large collection', (15, 17)), ('unconstrained 2D images', (18, 21)), ('3D scans', (24, 26))]","[['3 DMM and the model fitting encoder', 'leverage', 'large collection'], ['large collection', 'of', 'unconstrained 2D images'], ['unconstrained 2D images', 'without relying on', '3D scans']]",[],"[['Model', 'Jointly learning', '3 DMM and the model fitting encoder']]",[],face_alignment,13,54
1302,experiments,We show significantly improved shape and texture representation power over the linear 3 DMM .,"[('show', (1, 2)), ('over', (9, 10))]","[('significantly improved', (2, 4)), ('shape and texture representation power', (4, 9)), ('linear 3 DMM', (11, 14))]","[['shape and texture representation power', 'over', 'linear 3 DMM']]","[['significantly improved', 'has', 'shape and texture representation power']]",[],[],face_alignment,13,55
1303,hyperparameters,"The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.","[('optimized using', (3, 5)), ('with', (7, 8)), ('of', (12, 13)), ('when minimizing', (14, 16)), ('when minimizing', (21, 23))]","[('model', (1, 2)), ('Adam optimizer', (5, 7)), ('initial learning rate', (9, 12)), ('0.001', (13, 14)), ('L 0', (16, 18)), ('0.0002', (20, 21))]","[['model', 'optimized using', 'Adam optimizer'], ['model', 'with', '0.0002'], ['Adam optimizer', 'with', 'initial learning rate'], ['Adam optimizer', 'with', '0.0002'], ['initial learning rate', 'of', '0.001'], ['initial learning rate', 'of', '0.0002'], ['0.001', 'when minimizing', 'L 0']]",[],[],"[['Hyperparameters', 'has', 'model']]",face_alignment,13,208
1304,hyperparameters,"We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .","[('set', (1, 2))]","[('following parameters', (3, 5)), ('Q', (6, 7)), ('53 , 215', (8, 11)), ('U = V', (12, 15)), ('128', (16, 17)), ('l S = l T', (18, 23)), ('160', (24, 25))]",[],"[['following parameters', 'has', 'Q'], ['Q', 'has', '53 , 215'], ['U = V', 'has', '128'], ['l S = l T', 'has', '160']]","[['Hyperparameters', 'set', 'following parameters']]",[],face_alignment,13,209
1305,results,Representation Power,[],"[('Representation Power', (0, 2))]",[],[],[],"[['Results', 'has', 'Representation Power']]",face_alignment,13,226
1306,baselines,"Alternatively , we can minimize the reconstruction error in the image space , through the rendering layer with the groundtruth S and m .","[('minimize', (4, 5)), ('in', (8, 9)), ('through', (13, 14)), ('with', (17, 18))]","[('reconstruction error', (6, 8)), ('image space', (10, 12)), ('rendering layer', (15, 17)), ('groundtruth S and m', (19, 23))]","[['reconstruction error', 'in', 'image space'], ['reconstruction error', 'through', 'rendering layer'], ['rendering layer', 'with', 'groundtruth S and m']]",[],[],[],face_alignment,13,230
1307,results,"As in , our nonlinear texture is closer to the groundtruth than the linear model , especially for in - the - wild images ( the first two rows ) .","[('closer to', (7, 9)), ('than', (11, 12)), ('especially for', (16, 18))]","[('our nonlinear texture', (3, 6)), ('groundtruth', (10, 11)), ('linear model', (13, 15)), ('in - the - wild images', (18, 24))]","[['our nonlinear texture', 'closer to', 'groundtruth'], ['groundtruth', 'than', 'linear model'], ['linear model', 'especially for', 'in - the - wild images']]",[],[],"[['Results', 'has', 'our nonlinear texture']]",face_alignment,13,233
1308,results,"Quantitatively , our nonlinear model has significantly lower L 1 reconstruction error than the lin - We also compare the power of nonlinear and linear 3 DMM in representing real - world 3D scans .","[('significantly lower', (6, 8)), ('than', (12, 13))]","[('our nonlinear model', (2, 5)), ('L 1 reconstruction error', (8, 12)), ('lin', (14, 15))]","[['our nonlinear model', 'significantly lower', 'L 1 reconstruction error'], ['L 1 reconstruction error', 'than', 'lin']]","[['our nonlinear model', 'has', 'L 1 reconstruction error']]",[],"[['Results', 'has', 'our nonlinear model']]",face_alignment,13,235
1309,results,"Our nonlinear model has a significantly smaller reconstruction error than the linear model , 0.0196 vs. 0.0241 ( Tab. 3 ) .","[('than', (9, 10))]","[('significantly smaller reconstruction error', (5, 9)), ('linear model', (11, 13)), ('0.0196 vs. 0.0241', (14, 17))]","[['significantly smaller reconstruction error', 'than', 'linear model']]",[],[],[],face_alignment,13,247
1310,results,Quantitative evaluation of 3D reconstruction .,[],"[('3D reconstruction', (3, 5))]",[],[],[],[],face_alignment,13,264
1311,results,We obtain a low error that is comparable to optimization - based methods .,"[('obtain', (1, 2)), ('comparable to', (7, 9))]","[('low error', (3, 5)), ('optimization - based methods', (9, 13))]","[['low error', 'comparable to', 'optimization - based methods']]",[],"[['Results', 'obtain', 'low error']]",[],face_alignment,13,265
1312,experiments,3D Face Reconstruction .,[],"[('3D Face Reconstruction', (0, 3))]",[],[],[],[],face_alignment,13,271
1313,results,"We achieve on - par results with Garrido et al. , an offline optimization method , while surpassing all other regression methods ] .","[('achieve', (1, 2)), ('with', (6, 7)), ('surpassing', (17, 18))]","[('on - par results', (2, 6)), ('Garrido et al.', (7, 10)), ('other regression methods', (19, 22))]","[['on - par results', 'with', 'Garrido et al.'], ['on - par results', 'surpassing', 'other regression methods']]",[],"[['Results', 'achieve', 'on - par results']]",[],face_alignment,13,280
1314,ablation-analysis,Using a global image - based discriminator is redundant as the global structure is guaranteed by the rendering layer .,"[('Using', (0, 1)), ('guaranteed by', (14, 16))]","[('global image - based discriminator', (2, 7)), ('redundant', (8, 9)), ('global structure', (11, 13)), ('rendering layer', (17, 19))]","[['global structure', 'guaranteed by', 'rendering layer']]","[['global image - based discriminator', 'has', 'redundant']]","[['Ablation analysis', 'Using', 'global image - based discriminator']]",[],face_alignment,13,284
1315,ablation-analysis,"Also , we empirically find that using global image - based discriminator can cause severe artifacts in the resultant texture .","[('using', (6, 7)), ('cause', (13, 14)), ('in', (16, 17))]","[('global image - based discriminator', (7, 12)), ('severe artifacts', (14, 16)), ('resultant texture', (18, 20))]","[['global image - based discriminator', 'cause', 'severe artifacts'], ['severe artifacts', 'in', 'resultant texture']]",[],"[['Ablation analysis', 'using', 'global image - based discriminator']]",[],face_alignment,13,285
1316,ablation-analysis,"Clearly , patchGAN offers higher realism and fewer artifacts .","[('offers', (3, 4))]","[('patchGAN', (2, 3)), ('higher realism and fewer artifacts', (4, 9))]","[['patchGAN', 'offers', 'higher realism and fewer artifacts']]","[['patchGAN', 'has', 'higher realism and fewer artifacts']]",[],"[['Ablation analysis', 'has', 'patchGAN']]",face_alignment,13,287
1317,research-problem,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses,[],"[('Facial Alignment', (5, 7))]",[],[],[],[],face_alignment,14,2
1318,research-problem,Robust face recognition and analysis are contingent upon accurate localization of facial features .,[],"[('Robust face recognition and analysis', (0, 5)), ('localization of facial features', (9, 13))]",[],[],[],[],face_alignment,14,14
1319,model,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .","[('observe', (9, 10)), ('by using', (19, 21)), ('deformed to', (25, 27)), ('at', (30, 31)), ('compared to', (36, 38))]","[('fairly accurate 3 D models', (11, 16)), ('generated', (18, 19)), ('simple mean shape', (22, 25)), ('input image', (28, 30)), ('relatively low computational cost', (32, 36)), ('other approaches', (38, 40))]","[['generated', 'by using', 'simple mean shape'], ['simple mean shape', 'deformed to', 'input image'], ['input image', 'at', 'relatively low computational cost'], ['relatively low computational cost', 'compared to', 'other approaches']]","[['fairly accurate 3 D models', 'has', 'generated']]","[['Model', 'observe', 'fairly accurate 3 D models']]",[],face_alignment,14,33
1320,experimental-setup,Our network is implemented in the Caffe framework .,"[('implemented in', (3, 5))]","[('Our network', (0, 2)), ('Caffe framework', (6, 8))]","[['Our network', 'implemented in', 'Caffe framework']]",[],[],"[['Experimental setup', 'has', 'Our network']]",face_alignment,14,203
1321,experimental-setup,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .","[('consisting of', (5, 7))]","[('new layer', (1, 3)), ('3D TPS transformation module', (8, 12)), ('camera projection module', (14, 17)), ('bilinear sampler module', (19, 22))]","[['new layer', 'consisting of', '3D TPS transformation module'], ['new layer', 'consisting of', 'bilinear sampler module']]",[],[],[],face_alignment,14,204
1322,experimental-setup,"We adopt two architectures , AlexNet and VGG - 16 , as the pre-trained models for our shared feature extraction networks in , i.e. we use the convolution layers from the pre-trained models to initialize ours .","[('adopt', (1, 2)), ('for', (15, 16)), ('use', (25, 26)), ('from', (29, 30)), ('to initialize', (33, 35))]","[('two architectures', (2, 4)), ('AlexNet and VGG - 16', (5, 10)), ('pre-trained models', (13, 15)), ('our shared feature extraction networks', (16, 21)), ('convolution layers', (27, 29)), ('pre-trained models', (31, 33)), ('ours', (35, 36))]","[['pre-trained models', 'for', 'our shared feature extraction networks'], ['our shared feature extraction networks', 'use', 'convolution layers'], ['convolution layers', 'from', 'pre-trained models'], ['convolution layers', 'to initialize', 'ours'], ['pre-trained models', 'to initialize', 'ours']]","[['two architectures', 'name', 'AlexNet and VGG - 16']]","[['Experimental setup', 'adopt', 'two architectures']]",[],face_alignment,14,206
1323,experimental-setup,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .","[('For', (0, 1)), ('freeze', (6, 7)), ('for', (11, 12))]","[('AlexNet architecture', (2, 4)), ('first layer', (8, 10)), ('VGG - 16 architecture', (13, 17)), ('first 4 layers', (19, 22)), ('frozen', (23, 24))]","[['AlexNet architecture', 'freeze', 'first layer']]","[['VGG - 16 architecture', 'has', 'first 4 layers'], ['first 4 layers', 'has', 'frozen']]","[['Experimental setup', 'For', 'AlexNet architecture']]",[],face_alignment,14,208
1324,experimental-setup,The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,"[('implemented by', (5, 7)), ('on top of', (10, 13))]","[('2D landmark regression', (1, 4)), ('attaching', (7, 8)), ('additional layers', (8, 10)), ('last convolution layer', (14, 17))]","[['2D landmark regression', 'implemented by', 'attaching'], ['additional layers', 'on top of', 'last convolution layer']]","[['attaching', 'has', 'additional layers']]",[],"[['Experimental setup', 'has', '2D landmark regression']]",face_alignment,14,209
1325,research-problem,Face Alignment Across Large Poses : A 3D Solution,[],"[('Face Alignment Across Large Poses', (0, 5))]",[],[],[],[],face_alignment,15,2
1326,research-problem,"Face alignment , which fits a face model to an image and extracts the semantic meanings of facial pixels , has been an important topic in CV community .",[],"[('Face alignment', (0, 2))]",[],[],[],[],face_alignment,15,4
1327,model,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .","[('to', (4, 5)), ('fit', (5, 6))]","[('3D dense face model', (7, 11)), ('image', (20, 21))]",[],[],"[['Model', 'to', '3D dense face model']]",[],face_alignment,15,45
1328,model,We call this method 3D Dense Face Alignment ( 3DDFA ) .,"[('call', (1, 2))]","[('3D Dense Face Alignment ( 3DDFA )', (4, 11))]",[],[],"[['Model', 'call', '3D Dense Face Alignment ( 3DDFA )']]",[],face_alignment,15,47
1329,model,"To resolve the fitting process in 3 DDFA , we propose a cascaded convolutional neutral network ( CNN ) based regression method .","[('To resolve', (0, 2)), ('in', (5, 6)), ('propose', (10, 11))]","[('fitting process', (3, 5)), ('DDFA', (7, 8)), ('cascaded convolutional neutral network ( CNN ) based regression method', (12, 22))]","[['DDFA', 'propose', 'cascaded convolutional neutral network ( CNN ) based regression method']]","[['fitting process', 'has', 'DDFA']]","[['Model', 'To resolve', 'fitting process']]",[],face_alignment,15,50
1330,model,"In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .","[('adopt', (5, 6)), ('to fit', (7, 9)), ('with', (13, 14)), ('namely', (19, 20))]","[('CNN', (6, 7)), ('3D face model', (10, 13)), ('specifically designed feature', (15, 18)), ('Projected Normalized Coordinate Code ( PNCC )', (20, 27))]","[['CNN', 'to fit', '3D face model'], ['3D face model', 'with', 'specifically designed feature'], ['specifically designed feature', 'namely', 'Projected Normalized Coordinate Code ( PNCC )']]","[['specifically designed feature', 'name', 'Projected Normalized Coordinate Code ( PNCC )']]","[['Model', 'adopt', 'CNN']]",[],face_alignment,15,52
1331,model,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .","[('proposed as', (10, 12))]","[('Weighted Parameter Distance Cost ( WPDC )', (2, 9)), ('cost function', (13, 15))]","[['Weighted Parameter Distance Cost ( WPDC )', 'proposed as', 'cost function']]","[['Weighted Parameter Distance Cost ( WPDC )', 'has', 'cost function']]",[],"[['Model', 'has', 'Weighted Parameter Distance Cost ( WPDC )']]",face_alignment,15,53
1332,model,"To the best of our knowledge , this is the first attempt to solve the 3D face alignment with CNN .","[('to solve', (12, 14)), ('with', (18, 19))]","[('first attempt', (10, 12)), ('3D face alignment', (15, 18)), ('CNN', (19, 20))]","[['first attempt', 'to solve', '3D face alignment'], ['3D face alignment', 'with', 'CNN']]",[],[],[],face_alignment,15,54
1333,model,"To enable the training of the 3DDFA , we construct a face database containing pairs of 2D face images and 3D face models .","[('To enable', (0, 2)), ('of', (4, 5)), ('construct', (9, 10)), ('containing', (13, 14)), ('of', (15, 16))]","[('training', (3, 4)), ('3DDFA', (6, 7)), ('face database', (11, 13)), ('pairs', (14, 15)), ('2D face images and 3D face models', (16, 23))]","[['training', 'of', '3DDFA'], ['pairs', 'of', '2D face images and 3D face models'], ['3DDFA', 'construct', 'face database'], ['face database', 'containing', 'pairs'], ['pairs', 'of', '2D face images and 3D face models']]","[['training', 'has', '3DDFA']]","[['Model', 'To enable', 'training']]",[],face_alignment,15,56
1334,model,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,"[('propose', (2, 3)), ('to synthesize', (7, 9)), ('across', (14, 15))]","[('face profiling algorithm', (4, 7)), ('60 k + training samples', (9, 14)), ('large poses', (15, 17))]","[['face profiling algorithm', 'to synthesize', '60 k + training samples'], ['60 k + training samples', 'across', 'large poses']]",[],"[['Model', 'propose', 'face profiling algorithm']]",[],face_alignment,15,57
1335,experiments,The synthesized samples well simulate the face appearances in large poses and boost the performance of both prior and our proposed face alignment algorithms .,"[('simulate', (4, 5)), ('in', (8, 9)), ('boost', (12, 13)), ('of', (15, 16))]","[('synthesized samples', (1, 3)), ('face appearances', (6, 8)), ('large poses', (9, 11)), ('performance', (14, 15)), ('prior and our proposed face alignment algorithms', (17, 24))]","[['face appearances', 'in', 'large poses'], ['synthesized samples', 'boost', 'performance'], ['performance', 'of', 'prior and our proposed face alignment algorithms']]",[],[],[],face_alignment,15,58
1336,code,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/ .",[],"[('http://www.cbsr.ia.ac.cn/users / xiangyuzhu/', (13, 16))]",[],[],[],[],face_alignment,15,59
1337,ablation-analysis,Error Reduction in Cascade :,[],"[('Error Reduction in Cascade', (0, 4))]",[],[],[],"[['Ablation analysis', 'has', 'Error Reduction in Cascade']]",face_alignment,15,221
1338,ablation-analysis,"As observed , the testing error is reduced due to initialization regeneration .","[('due to', (8, 10))]","[('testing error', (4, 6)), ('reduced', (7, 8)), ('initialization regeneration', (10, 12))]","[['reduced', 'due to', 'initialization regeneration']]","[['testing error', 'has', 'reduced']]",[],"[['Ablation analysis', 'has', 'testing error']]",face_alignment,15,225
1339,ablation-analysis,In the generic cascade process the training and testing errors converge fast after 2 iterations .,"[('In', (0, 1)), ('after', (12, 13))]","[('generic cascade process', (2, 5)), ('training and testing errors', (6, 10)), ('converge fast', (10, 12)), ('2 iterations', (13, 15))]","[['converge fast', 'after', '2 iterations']]","[['generic cascade process', 'has', 'training and testing errors'], ['training and testing errors', 'has', 'converge fast']]","[['Ablation analysis', 'In', 'generic cascade process']]",[],face_alignment,15,226
1340,hyperparameters,"While with initialization regeneration , the training error is updated at the beginning of each iteration and the testing error continues to descend .","[('with', (1, 2)), ('at', (10, 11)), ('of', (13, 14))]","[('initialization regeneration', (2, 4)), ('training error', (6, 8)), ('updated', (9, 10)), ('beginning', (12, 13)), ('each iteration', (14, 16)), ('testing error', (18, 20)), ('continues to descend', (20, 23))]","[['updated', 'at', 'beginning'], ['beginning', 'of', 'each iteration']]","[['initialization regeneration', 'has', 'training error'], ['training error', 'has', 'updated'], ['testing error', 'has', 'continues to descend']]","[['Hyperparameters', 'with', 'initialization regeneration']]",[],face_alignment,15,227
1341,results,Performance with Different Costs :,[],"[('Performance with Different Costs', (0, 4))]",[],[],[],"[['Results', 'has', 'Performance with Different Costs']]",face_alignment,15,231
1342,ablation-analysis,It is shown that PDC can not well model the fitting error and converges to an unsatisfied result .,"[('can not well model', (5, 9)), ('to', (14, 15))]","[('PDC', (4, 5)), ('fitting error', (10, 12)), ('converges', (13, 14)), ('unsatisfied result', (16, 18))]","[['PDC', 'can not well model', 'fitting error'], ['converges', 'to', 'unsatisfied result']]","[['converges', 'has', 'unsatisfied result']]",[],"[['Ablation analysis', 'has', 'PDC']]",face_alignment,15,235
1343,baselines,"WPDC explicitly models the priority of each parameter and adaptively optimizes them with the parameter weights , leading to the best result .","[('explicitly models', (1, 3)), ('of', (5, 6)), ('with', (12, 13)), ('leading to', (17, 19))]","[('WPDC', (0, 1)), ('priority', (4, 5)), ('each parameter', (6, 8)), ('adaptively optimizes', (9, 11)), ('parameter weights', (14, 16)), ('best result', (20, 22))]","[['WPDC', 'explicitly models', 'priority'], ['WPDC', 'explicitly models', 'adaptively optimizes'], ['priority', 'of', 'each parameter'], ['adaptively optimizes', 'with', 'parameter weights'], ['WPDC', 'leading to', 'best result'], ['adaptively optimizes', 'leading to', 'best result'], ['parameter weights', 'leading to', 'best result']]","[['adaptively optimizes', 'has', 'parameter weights']]",[],"[['Baselines', 'has', 'WPDC']]",face_alignment,15,237
1344,baselines,Large Pose Face Alignment in AFLW Protocol :,[],"[('Large Pose Face Alignment in AFLW Protocol', (0, 7))]",[],[],[],"[['Baselines', 'has', 'Large Pose Face Alignment in AFLW Protocol']]",face_alignment,15,240
1345,results,"Firstly , the results indicate that all the methods benefits substantially from face profiling when dealing with large poses .","[('from', (11, 12)), ('dealing with', (15, 17))]","[('all the methods', (6, 9)), ('benefits substantially', (9, 11)), ('face profiling', (12, 14)), ('large poses', (17, 19))]","[['benefits substantially', 'from', 'face profiling']]","[['all the methods', 'has', 'benefits substantially']]",[],[],face_alignment,15,258
1346,results,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .","[('reaches', (3, 4)), ('beyond', (15, 16))]","[('3DDFA', (2, 3)), ('state of the art', (5, 9)), ('above all the 2D methods', (9, 14)), ('medium poses', (16, 18))]","[['3DDFA', 'reaches', 'state of the art'], ['above all the 2D methods', 'beyond', 'medium poses']]","[['3DDFA', 'has', 'state of the art'], ['state of the art', 'has', 'above all the 2D methods']]",[],"[['Results', 'has', '3DDFA']]",face_alignment,15,262
1347,results,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,"[('of', (4, 5)), ('demonstrates', (7, 8)), ('to', (10, 11))]","[('minimum standard deviation', (1, 4)), ('3DDFA', (5, 6)), ('robustness', (9, 10)), ('pose variations', (11, 13))]","[['minimum standard deviation', 'of', '3DDFA'], ['minimum standard deviation', 'demonstrates', 'robustness'], ['3DDFA', 'demonstrates', 'robustness'], ['robustness', 'to', 'pose variations']]",[],[],"[['Results', 'has', 'minimum standard deviation']]",face_alignment,15,263
1348,results,"Finally , the performance of 3DDFA can be further improved with the SDM landmark refinement in Section 5.2 .","[('of', (4, 5)), ('further improved', (8, 10)), ('with', (10, 11))]","[('performance', (3, 4)), ('3DDFA', (5, 6)), ('SDM landmark refinement', (12, 15))]","[['performance', 'of', '3DDFA']]","[['performance', 'has', '3DDFA']]",[],[],face_alignment,15,264
1349,baselines,3D Face Alignment in AFLW2000-3D,[],"[('3D Face Alignment in AFLW2000-3D', (0, 5))]",[],[],[],"[['Baselines', 'has', '3D Face Alignment in AFLW2000-3D']]",face_alignment,15,265
1350,results,"For all the methods , despite with ground truth bounding boxes the performance in [ 60 , 90 ] and the standard deviation are obviously reduced when considering all the landmarks .","[('despite with', (5, 7)), ('considering', (27, 28))]","[('all the methods', (1, 4)), ('ground truth bounding boxes', (7, 11)), ('performance', (12, 13)), ('standard deviation', (21, 23)), ('obviously reduced', (24, 26)), ('all the landmarks', (28, 31))]","[['all the methods', 'despite with', 'ground truth bounding boxes']]","[['all the methods', 'has', 'ground truth bounding boxes'], ['ground truth bounding boxes', 'has', 'performance']]",[],[],face_alignment,15,270
1351,results,Common Challenging Full TSPM,[],"[('Common Challenging Full TSPM', (0, 4))]",[],[],[],"[['Results', 'has', 'Common Challenging Full TSPM']]",face_alignment,15,279
1352,results,"8 that even as a generic face alignment algorithm , 3 DDFA still demonstrates competitive performance on the common set and state - of - the - art performance on the challenging set .","[('demonstrates', (13, 14)), ('on', (16, 17)), ('on', (29, 30))]","[('DDFA', (11, 12)), ('competitive performance', (14, 16)), ('common set and state - of - the - art performance', (18, 29)), ('challenging set', (31, 33))]","[['DDFA', 'demonstrates', 'competitive performance'], ['competitive performance', 'on', 'common set and state - of - the - art performance'], ['common set and state - of - the - art performance', 'on', 'challenging set']]",[],[],[],face_alignment,15,280
1353,research-problem,Deep Multi- Center Learning for Face Alignment,[],"[('Face Alignment', (5, 7))]",[],[],[],[],face_alignment,16,2
1354,code,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,[],"[('https://github.com/ZhiwenShao/MCNet-Extension', (8, 9))]",[],[],[],[],face_alignment,16,11
1355,model,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .","[('propose', (6, 7)), ('named', (12, 13)), ('to exploit', (20, 22)), ('among', (25, 26))]","[('novel deep learning framework', (8, 12)), ('Multi - Center Learning ( MCL )', (13, 20)), ('strong correlations', (23, 25)), ('landmarks', (26, 27))]","[['novel deep learning framework', 'named', 'Multi - Center Learning ( MCL )'], ['novel deep learning framework', 'to exploit', 'strong correlations'], ['strong correlations', 'among', 'landmarks']]","[['novel deep learning framework', 'name', 'Multi - Center Learning ( MCL )']]","[['Model', 'propose', 'novel deep learning framework']]",[],face_alignment,16,35
1356,model,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .","[('uses', (5, 6)), ('to predict', (10, 12)), ('emphasizes on', (22, 24)), ('of', (26, 27)), ('of', (30, 31))]","[('our network', (3, 5)), ('multiple shape prediction layers', (6, 10)), ('locations of landmarks', (13, 16)), ('each shape prediction layer', (18, 22)), ('detection', (25, 26)), ('cluster', (29, 30)), ('landmarks', (31, 32))]","[['our network', 'uses', 'multiple shape prediction layers'], ['multiple shape prediction layers', 'to predict', 'locations of landmarks'], ['each shape prediction layer', 'emphasizes on', 'detection'], ['detection', 'of', 'cluster'], ['cluster', 'of', 'landmarks']]",[],[],"[['Model', 'has', 'our network']]",face_alignment,16,36
1357,model,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .","[('By weighting', (0, 2)), ('of', (4, 5))]","[('loss', (3, 4)), ('each landmark', (5, 7)), ('challenging landmarks', (8, 10)), ('focused firstly', (11, 13)), ('cluster of landmarks', (16, 19)), ('further optimized', (20, 22))]","[['loss', 'of', 'each landmark']]",[],"[['Model', 'By weighting', 'loss']]",[],face_alignment,16,37
1358,model,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .","[('to integrate', (14, 16)), ('into', (20, 21))]","[('model assembling method', (11, 14)), ('multiple shape prediction layers', (16, 20)), ('one shape prediction layer', (21, 25))]","[['model assembling method', 'to integrate', 'multiple shape prediction layers'], ['multiple shape prediction layers', 'into', 'one shape prediction layer']]",[],[],[],face_alignment,16,38
1359,model,The entire framework reinforces the learning process of each landmark with a low model complexity .,"[('reinforces', (3, 4)), ('of', (7, 8)), ('with', (10, 11))]","[('entire framework', (1, 3)), ('learning process', (5, 7)), ('each landmark', (8, 10)), ('low model complexity', (12, 15))]","[['entire framework', 'reinforces', 'learning process'], ['learning process', 'of', 'each landmark'], ['each landmark', 'with', 'low model complexity']]",[],[],"[['Model', 'has', 'entire framework']]",face_alignment,16,39
1360,experimental-setup,"Uniform scaling and translation with different extents on face bounding boxes are further conducted , in which each newly generated face bounding box is used to crop the face .","[('with', (4, 5)), ('on', (7, 8))]","[('Uniform scaling and translation', (0, 4)), ('different extents', (5, 7)), ('face bounding boxes', (8, 11)), ('conducted', (13, 14))]","[['Uniform scaling and translation', 'with', 'different extents'], ['different extents', 'on', 'face bounding boxes']]",[],[],"[['Experimental setup', 'has', 'Uniform scaling and translation']]",face_alignment,16,210
1361,experimental-setup,Finally training samples are augmented through horizontal flip and JPEG compression .,"[('augmented through', (4, 6))]","[('training samples', (1, 3)), ('horizontal flip', (6, 8)), ('JPEG compression', (9, 11))]","[['training samples', 'augmented through', 'horizontal flip'], ['training samples', 'augmented through', 'JPEG compression']]",[],[],"[['Experimental setup', 'has', 'training samples']]",face_alignment,16,211
1362,experimental-setup,We train our MCL using an open source deep learning framework Caffe .,"[('train', (1, 2)), ('using', (4, 5))]","[('our MCL', (2, 4)), ('open source deep learning framework Caffe', (6, 12))]","[['our MCL', 'using', 'open source deep learning framework Caffe']]",[],"[['Experimental setup', 'train', 'our MCL']]",[],face_alignment,16,213
1363,experimental-setup,"The input face patch is a 50 50 grayscale image , and each pixel value is normalized to [ ?1 , 1 ) by subtracting 128 and multiplying 0.0078125 .","[('normalized to', (16, 18)), ('by subtracting', (23, 25))]","[('input face patch', (1, 4)), ('50 50 grayscale image', (6, 10)), ('each pixel value', (12, 15)), ('[ ?1 , 1 )', (18, 23)), ('128', (25, 26)), ('multiplying', (27, 28)), ('0.0078125', (28, 29))]","[['each pixel value', 'normalized to', '[ ?1 , 1 )'], ['each pixel value', 'normalized to', 'multiplying'], ['[ ?1 , 1 )', 'by subtracting', '128']]","[['input face patch', 'has', '50 50 grayscale image'], ['multiplying', 'has', '0.0078125']]",[],"[['Experimental setup', 'has', 'input face patch']]",face_alignment,16,214
1364,experimental-setup,"A more complex model is needed for a labeling pattern with more facial landmarks , so Dis set to be 512/512/1 , 024 for 5/29/68 facial landmarks .","[('for', (6, 7)), ('with', (10, 11))]","[('more complex model', (1, 4)), ('labeling pattern', (8, 10)), ('more facial landmarks', (11, 14))]","[['more complex model', 'for', 'labeling pattern'], ['labeling pattern', 'with', 'more facial landmarks']]",[],[],"[['Experimental setup', 'has', 'more complex model']]",face_alignment,16,215
1365,experimental-setup,"The type of solver is SGD with a mini-batch size of 64 , a momentum of 0.9 , and a weight decay of 0.0005 .","[('is', (4, 5)), ('with', (6, 7)), ('of', (10, 11)), ('of', (15, 16)), ('of', (22, 23))]","[('type of solver', (1, 4)), ('SGD', (5, 6)), ('mini-batch size', (8, 10)), ('64', (11, 12)), ('momentum', (14, 15)), ('0.9', (16, 17)), ('weight decay', (20, 22)), ('0.0005', (23, 24))]","[['type of solver', 'is', 'SGD'], ['SGD', 'with', 'mini-batch size'], ['mini-batch size', 'of', '64'], ['momentum', 'of', '0.9'], ['weight decay', 'of', '0.0005'], ['momentum', 'of', '0.9'], ['weight decay', 'of', '0.0005']]","[['type of solver', 'has', 'SGD']]",[],"[['Experimental setup', 'has', 'type of solver']]",face_alignment,16,216
1366,experimental-setup,"The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .","[('of', (4, 5)), ('of', (23, 24))]","[('maximum learning iterations', (1, 4)), ('pre-training', (5, 6)), ('each finetuning step', (7, 10)), ('1810 4 and 610 4', (11, 16)), ('initial learning rates', (20, 23)), ('pre-training', (24, 25)), ('each fine - tuning step', (26, 31)), ('0.02 and 0.001', (32, 35))]","[['maximum learning iterations', 'of', 'pre-training'], ['initial learning rates', 'of', 'pre-training'], ['initial learning rates', 'of', 'pre-training']]",[],[],"[['Experimental setup', 'has', 'maximum learning iterations']]",face_alignment,16,217
1367,experimental-setup,"The learning rate is multiplied by a factor of 0.3 at every 3 10 4 iterations , and the remaining parameter ?","[('multiplied by', (4, 6)), ('of', (8, 9)), ('at', (10, 11))]","[('learning rate', (1, 3)), ('factor', (7, 8)), ('0.3', (9, 10)), ('every 3 10 4 iterations', (11, 16))]","[['learning rate', 'multiplied by', 'factor'], ['factor', 'of', '0.3'], ['0.3', 'at', 'every 3 10 4 iterations']]","[['learning rate', 'has', 'factor']]",[],"[['Experimental setup', 'has', 'learning rate']]",face_alignment,16,219
1368,baselines,"We compare our work MCL against state - of - the - art methods including ESR , SDM , Cascaded CNN , RCPR , CFAN , LBF , c GPRT , CFSS , TCDCN , , ALR , CFT , RFLD , RecNet , RAR , and FLD + PDE .","[('including', (14, 15))]","[('state - of - the - art methods', (6, 14)), ('ESR', (15, 16)), ('SDM', (17, 18)), ('Cascaded CNN', (19, 21)), ('RCPR', (22, 23)), ('CFAN', (24, 25)), ('LBF', (26, 27)), ('c GPRT', (28, 30)), ('CFSS', (31, 32)), ('TCDCN', (33, 34)), ('ALR', (36, 37)), ('CFT', (38, 39)), ('RFLD', (40, 41)), ('RecNet', (42, 43)), ('RAR', (44, 45)), ('FLD + PDE', (47, 50))]","[['state - of - the - art methods', 'including', 'ESR'], ['state - of - the - art methods', 'including', 'SDM'], ['state - of - the - art methods', 'including', 'Cascaded CNN'], ['state - of - the - art methods', 'including', 'RCPR'], ['state - of - the - art methods', 'including', 'CFAN'], ['state - of - the - art methods', 'including', 'LBF'], ['state - of - the - art methods', 'including', 'c GPRT'], ['state - of - the - art methods', 'including', 'CFSS'], ['state - of - the - art methods', 'including', 'TCDCN'], ['state - of - the - art methods', 'including', 'ALR'], ['state - of - the - art methods', 'including', 'CFT'], ['state - of - the - art methods', 'including', 'RFLD'], ['state - of - the - art methods', 'including', 'RecNet'], ['state - of - the - art methods', 'including', 'RAR'], ['state - of - the - art methods', 'including', 'FLD + PDE']]","[['state - of - the - art methods', 'name', 'ESR']]",[],[],face_alignment,16,230
1369,results,"Our method MCL outperforms most of the state - of - the - art methods , especially on AFLW dataset where a relative error reduction of 3.93 % is achieved compared to RecNet .","[('outperforms', (3, 4)), ('on', (17, 18)), ('where', (20, 21)), ('achieved', (29, 30)), ('compared to', (30, 32))]","[('Our method MCL', (0, 3)), ('most of the state - of - the - art methods', (4, 15)), ('AFLW dataset', (18, 20)), ('relative error reduction', (22, 25)), ('3.93 %', (26, 28)), ('RecNet', (32, 33))]","[['Our method MCL', 'outperforms', 'most of the state - of - the - art methods'], ['most of the state - of - the - art methods', 'on', 'AFLW dataset'], ['AFLW dataset', 'where', 'relative error reduction'], ['3.93 %', 'compared to', 'RecNet']]",[],[],"[['Results', 'has', 'Our method MCL']]",face_alignment,16,237
1370,results,We compare with other methods on several challenging images from AFLW and COFW respectively in .,"[('from', (9, 10))]","[('challenging images', (7, 9)), ('AFLW and COFW', (10, 13))]","[['challenging images', 'from', 'AFLW and COFW']]",[],[],[],face_alignment,16,239
1371,results,"MCL demonstrates a superior capability of handling severe occlusions and complex variations of pose , expression , illumination .","[('demonstrates', (1, 2)), ('of handling', (5, 7)), ('of', (12, 13))]","[('MCL', (0, 1)), ('superior capability', (3, 5)), ('severe occlusions', (7, 9)), ('complex variations', (10, 12)), ('pose , expression , illumination', (13, 18))]","[['MCL', 'demonstrates', 'superior capability'], ['superior capability', 'of handling', 'severe occlusions'], ['superior capability', 'of handling', 'complex variations'], ['complex variations', 'of', 'pose , expression , illumination']]","[['MCL', 'has', 'superior capability']]",[],"[['Results', 'has', 'MCL']]",face_alignment,16,242
1372,results,It is observed that MCL achieves competitive performance on all three benchmarks .,"[('observed', (2, 3)), ('achieves', (5, 6)), ('on', (8, 9))]","[('MCL', (4, 5)), ('competitive performance', (6, 8)), ('all three benchmarks', (9, 12))]","[['MCL', 'achieves', 'competitive performance'], ['competitive performance', 'on', 'all three benchmarks']]",[],"[['Results', 'observed', 'MCL']]",[],face_alignment,16,244
1373,results,The average running speed of deep learning methods for detecting 68 facial landmarks are presented in .,"[('of', (4, 5)), ('for detecting', (8, 10))]","[('average running speed', (1, 4)), ('deep learning methods', (5, 8)), ('68 facial landmarks', (10, 13))]","[['average running speed', 'of', 'deep learning methods'], ['deep learning methods', 'for detecting', '68 facial landmarks']]",[],[],"[['Results', 'has', 'average running speed']]",face_alignment,16,245
1374,ablation-analysis,1 ) Global Average Pooling vs. Full Connection :,[],"[('Global Average Pooling vs. Full Connection', (2, 8))]",[],[],[],"[['Ablation analysis', 'has', 'Global Average Pooling vs. Full Connection']]",face_alignment,16,256
1375,ablation-analysis,It can be seen that BM performs better on IBUG and COFW but worse on AFLW than pre-BM .,"[('seen that', (3, 5)), ('performs', (6, 7)), ('on', (8, 9)), ('on', (14, 15)), ('than', (16, 17))]","[('BM', (5, 6)), ('better', (7, 8)), ('IBUG and COFW', (9, 12)), ('worse', (13, 14)), ('AFLW', (15, 16)), ('pre-BM', (17, 18))]","[['BM', 'performs', 'better'], ['better', 'on', 'IBUG and COFW'], ['worse', 'on', 'AFLW'], ['worse', 'on', 'AFLW'], ['AFLW', 'than', 'pre-BM']]",[],"[['Ablation analysis', 'seen that', 'BM']]",[],face_alignment,16,259
1376,ablation-analysis,It demonstrates that Global Average Pooling is more advantageous for more complex problems with more facial landmarks .,"[('demonstrates', (1, 2)), ('for', (9, 10)), ('with', (13, 14))]","[('Global Average Pooling', (3, 6)), ('more advantageous', (7, 9)), ('more complex problems', (10, 13)), ('more facial landmarks', (14, 17))]","[['more advantageous', 'for', 'more complex problems'], ['more complex problems', 'with', 'more facial landmarks']]","[['Global Average Pooling', 'has', 'more advantageous']]","[['Ablation analysis', 'demonstrates', 'Global Average Pooling']]",[],face_alignment,16,260
1377,ablation-analysis,2 ) Robustness of Weighting :,[],"[('Robustness of Weighting', (2, 5))]",[],[],[],"[['Ablation analysis', 'has', 'Robustness of Weighting']]",face_alignment,16,266
1378,ablation-analysis,"When ? is 0.4 , WM can still achieves good performance .","[('achieves', (8, 9))]","[('0.4', (3, 4)), ('good performance', (9, 11))]",[],[],[],[],face_alignment,16,273
1379,ablation-analysis,3 ) Analysis of Shape Prediction Layers :,[],"[('Analysis of Shape Prediction Layers', (2, 7))]",[],[],[],"[['Ablation analysis', 'has', 'Analysis of Shape Prediction Layers']]",face_alignment,16,276
1380,ablation-analysis,"Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .","[('Compared to', (0, 2)), ('reduce', (14, 15)), ('of', (18, 19))]","[('WM', (2, 3)), ('left eye model and the right eye model', (5, 13)), ('alignment errors', (16, 18)), ('corresponding clusters', (20, 22))]","[['left eye model and the right eye model', 'reduce', 'alignment errors'], ['alignment errors', 'of', 'corresponding clusters']]","[['WM', 'has', 'left eye model and the right eye model']]","[['Ablation analysis', 'Compared to', 'WM']]",[],face_alignment,16,279
1381,ablation-analysis,"As a result , the assembled AM can improve the detection accuracy of landmarks of the left eye and the right eye on the basis of WM .","[('improve', (8, 9)), ('of', (12, 13)), ('of', (14, 15)), ('on', (22, 23))]","[('assembled AM', (5, 7)), ('detection accuracy', (10, 12)), ('landmarks', (13, 14)), ('left eye and the right eye', (16, 22)), ('basis of WM', (24, 27))]","[['assembled AM', 'improve', 'detection accuracy'], ['detection accuracy', 'of', 'landmarks'], ['landmarks', 'of', 'left eye and the right eye'], ['detection accuracy', 'of', 'landmarks'], ['landmarks', 'of', 'left eye and the right eye'], ['left eye and the right eye', 'on', 'basis of WM']]",[],[],"[['Ablation analysis', 'has', 'assembled AM']]",face_alignment,16,280
1382,ablation-analysis,Note that the two models also improve the localization precision of other clusters .,"[('improve', (6, 7)), ('of', (10, 11))]","[('localization precision', (8, 10)), ('other clusters', (11, 13))]","[['localization precision', 'of', 'other clusters']]",[],"[['Ablation analysis', 'improve', 'localization precision']]",[],face_alignment,16,281
1383,ablation-analysis,4 ) Integration of Weighting Fine - Tuning and Multi - Center Fine - Tuning :,[],"[('Integration of Weighting Fine - Tuning and Multi - Center Fine - Tuning', (2, 15))]",[],[],[],"[['Ablation analysis', 'has', 'Integration of Weighting Fine - Tuning and Multi - Center Fine - Tuning']]",face_alignment,16,285
1384,ablation-analysis,"The accuracy of AM is superior to that of Simplified AM especially on challenging IBUG , which is attributed to the integration of two stages .","[('of', (2, 3)), ('to', (6, 7)), ('especially on', (11, 13))]","[('accuracy', (1, 2)), ('AM', (3, 4)), ('superior', (5, 6)), ('Simplified AM', (9, 11)), ('challenging IBUG', (13, 15))]","[['accuracy', 'of', 'AM'], ['superior', 'to', 'Simplified AM'], ['Simplified AM', 'especially on', 'challenging IBUG']]","[['AM', 'has', 'superior']]",[],"[['Ablation analysis', 'has', 'accuracy']]",face_alignment,16,288
1385,ablation-analysis,It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG .,"[('seen that', (3, 5)), ('on', (10, 11)), ('to search', (14, 16)), ('on', (19, 20))]","[('Weighting Simplified AM', (5, 8)), ('improves slightly', (8, 10)), ('COFW', (11, 12)), ('fails', (13, 14)), ('better solution', (17, 19)), ('IBUG', (20, 21))]","[['improves slightly', 'on', 'COFW'], ['better solution', 'on', 'IBUG'], ['fails', 'to search', 'better solution'], ['better solution', 'on', 'IBUG']]","[['Weighting Simplified AM', 'has', 'improves slightly']]","[['Ablation analysis', 'seen that', 'Weighting Simplified AM']]",[],face_alignment,16,290
1386,ablation-analysis,It can be observed that AM has higher accuracy and stronger robustness than BM and WM .,"[('observed', (3, 4)), ('than', (12, 13))]","[('AM', (5, 6)), ('higher accuracy and stronger robustness', (7, 12)), ('BM and WM', (13, 16))]","[['higher accuracy and stronger robustness', 'than', 'BM and WM']]","[['AM', 'has', 'higher accuracy and stronger robustness']]","[['Ablation analysis', 'observed', 'AM']]",[],face_alignment,16,293
1387,ablation-analysis,The localization accuracy of facial landmarks from each cluster is improved in the details .,"[('of', (3, 4)), ('from', (6, 7))]","[('localization accuracy', (1, 3)), ('facial landmarks', (4, 6)), ('each cluster', (7, 9)), ('improved', (10, 11))]","[['localization accuracy', 'of', 'facial landmarks'], ['facial landmarks', 'from', 'each cluster']]",[],[],"[['Ablation analysis', 'has', 'localization accuracy']]",face_alignment,16,295
1388,ablation-analysis,D. MCL for Partially Occluded Faces,[],"[('D. MCL for Partially Occluded Faces', (0, 6))]",[],[],[],"[['Ablation analysis', 'has', 'D. MCL for Partially Occluded Faces']]",face_alignment,16,297
1389,ablation-analysis,The correlations among different facial parts are very useful for face alignment especially for partially occluded faces .,"[('among', (2, 3)), ('for', (9, 10)), ('for', (13, 14))]","[('correlations', (1, 2)), ('different facial parts', (3, 6)), ('very useful', (7, 9)), ('face alignment', (10, 12)), ('partially occluded faces', (14, 17))]","[['correlations', 'among', 'different facial parts'], ['very useful', 'for', 'face alignment'], ['very useful', 'for', 'partially occluded faces'], ['face alignment', 'for', 'partially occluded faces']]",[],[],"[['Ablation analysis', 'has', 'correlations']]",face_alignment,16,298
1390,ablation-analysis,"After processing testing faces with occlusions , the mean error results of both WM and AM increase .","[('with', (4, 5)), ('of', (11, 12))]","[('processing', (1, 2)), ('testing faces', (2, 4)), ('occlusions', (5, 6)), ('mean error results', (8, 11)), ('both WM and AM', (12, 16)), ('increase', (16, 17))]","[['testing faces', 'with', 'occlusions'], ['mean error results', 'of', 'both WM and AM']]","[['processing', 'has', 'testing faces'], ['both WM and AM', 'has', 'increase']]",[],[],face_alignment,16,303
1391,ablation-analysis,"Besides the results of landmarks from the left eye cluster , the results of remaining landmarks from other clusters become worse slightly .","[('of', (3, 4)), ('from', (5, 6)), ('from', (16, 17)), ('become', (19, 20))]","[('results', (2, 3)), ('landmarks', (4, 5)), ('left eye cluster', (7, 10)), ('remaining landmarks', (14, 16)), ('other clusters', (17, 19)), ('worse slightly', (20, 22))]","[['results', 'of', 'landmarks'], ['landmarks', 'from', 'left eye cluster'], ['remaining landmarks', 'from', 'other clusters'], ['remaining landmarks', 'from', 'other clusters'], ['remaining landmarks', 'become', 'worse slightly'], ['other clusters', 'become', 'worse slightly']]","[['results', 'has', 'landmarks']]",[],"[['Ablation analysis', 'has', 'results']]",face_alignment,16,304
1392,ablation-analysis,"Note that WM and AM still perform well on occluded left eyes with the mean error of 6.60 and 6.50 respectively , due to the following reasons .","[('on', (8, 9)), ('with', (12, 13)), ('of', (16, 17))]","[('WM and AM', (2, 5)), ('perform well', (6, 8)), ('occluded left eyes', (9, 12)), ('mean error', (14, 16)), ('6.60 and 6.50', (17, 20))]","[['perform well', 'on', 'occluded left eyes'], ['occluded left eyes', 'with', 'mean error'], ['mean error', 'of', '6.60 and 6.50']]","[['WM and AM', 'has', 'perform well']]",[],"[['Ablation analysis', 'has', 'WM and AM']]",face_alignment,16,306
1393,ablation-analysis,E. Weighting Fine - Tuning for State - of - the - Art Frameworks,[],"[('E. Weighting Fine - Tuning for State - of - the - Art Frameworks', (0, 14))]",[],[],[],"[['Ablation analysis', 'has', 'E. Weighting Fine - Tuning for State - of - the - Art Frameworks']]",face_alignment,16,310
1394,ablation-analysis,It can be seen that the mean error of re -DAN is reduced from 7.97 to 7.81 after using our proposed weighting fine - tuning .,"[('seen that', (3, 5)), ('of', (8, 9)), ('from', (13, 14)), ('to', (15, 16)), ('after using', (17, 19))]","[('mean error', (6, 8)), ('re -DAN', (9, 11)), ('reduced', (12, 13)), ('7.97', (14, 15)), ('7.81', (16, 17)), ('our proposed weighting fine - tuning', (19, 25))]","[['mean error', 'of', 're -DAN'], ['reduced', 'from', '7.97'], ['7.97', 'to', '7.81'], ['reduced', 'after using', 'our proposed weighting fine - tuning'], ['7.81', 'after using', 'our proposed weighting fine - tuning']]",[],"[['Ablation analysis', 'seen that', 'mean error']]",[],face_alignment,16,317
1395,research-problem,"Facial landmark detection , or face alignment , is a fundamental task that has been extensively studied .",[],"[('Facial landmark detection', (0, 3)), ('face alignment', (5, 7))]",[],[],[],[],face_alignment,17,5
1396,code,The code is made publicly available at https://github.com/thesouthfrog/stylealign .,[],"[('https://github.com/thesouthfrog/stylealign', (7, 8))]",[],[],[],[],face_alignment,17,11
1397,model,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .","[('propose', (5, 6)), ('to augment', (9, 11)), ('for', (12, 13)), ('without using', (16, 18))]","[('new framework', (7, 9)), ('training', (11, 12)), ('facial landmark detection', (13, 16)), ('extra knowledge', (18, 20))]","[['new framework', 'to augment', 'training'], ['training', 'for', 'facial landmark detection'], ['facial landmark detection', 'without using', 'extra knowledge']]",[],"[['Model', 'propose', 'new framework']]",[],face_alignment,17,30
1398,model,"Instead of directly generating images , we first map face images into the space of structure and style .","[('of', (1, 2)), ('map', (8, 9)), ('into', (11, 12))]","[('face images', (9, 11)), ('space', (13, 14)), ('structure and style', (15, 18))]","[['space', 'of', 'structure and style'], ['face images', 'into', 'space']]",[],[],[],face_alignment,17,31
1399,model,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .","[('To guarantee', (0, 2)), ('of', (4, 5)), ('design', (10, 11)), ('incorporated for', (33, 35)), ('of', (37, 38))]","[('disentanglement', (3, 4)), ('these two spaces', (5, 8)), ('conditional variational auto - encoder model', (12, 18)), ('Kullback - Leiber ( KL ) divergence loss and skip connections', (21, 32)), ('compact representation', (35, 37)), ('style and structure', (38, 41))]","[['disentanglement', 'of', 'these two spaces'], ['compact representation', 'of', 'style and structure'], ['disentanglement', 'design', 'conditional variational auto - encoder model'], ['these two spaces', 'design', 'conditional variational auto - encoder model'], ['Kullback - Leiber ( KL ) divergence loss and skip connections', 'incorporated for', 'compact representation'], ['compact representation', 'of', 'style and structure']]",[],"[['Model', 'To guarantee', 'disentanglement']]",[],face_alignment,17,32
1400,model,"By factoring these features , we perform visual style translation between existing facial geometry .","[('perform', (6, 7)), ('between', (10, 11))]","[('visual style translation', (7, 10)), ('existing facial geometry', (11, 14))]","[['visual style translation', 'between', 'existing facial geometry']]",[],"[['Model', 'perform', 'visual style translation']]",[],face_alignment,17,33
1401,model,"Given existing facial structure , faces with glasses , of poor quality , under blur or strong lighting are rerendered from corresponding style , which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry .","[('Given', (0, 1)), ('rerendered from', (19, 21)), ('train', (29, 30)), ('for', (34, 35)), ('to recognize', (41, 43))]","[('existing facial structure , faces with glasses , of poor quality , under blur or strong lighting', (1, 18)), ('corresponding style', (21, 23)), ('facial landmark detectors', (31, 34)), ('general and robust system', (37, 41)), ('facial geometry', (43, 45))]","[['existing facial structure , faces with glasses , of poor quality , under blur or strong lighting', 'rerendered from', 'corresponding style'], ['existing facial structure , faces with glasses , of poor quality , under blur or strong lighting', 'train', 'facial landmark detectors'], ['facial landmark detectors', 'for', 'general and robust system'], ['general and robust system', 'to recognize', 'facial geometry']]",[],"[['Model', 'Given', 'existing facial structure , faces with glasses , of poor quality , under blur or strong lighting']]",[],face_alignment,17,34
1402,hyperparameters,"Implementation Details Before training , all images are cropped and resized to 256 256 using provided bounding boxes .","[('Before', (2, 3)), ('to', (11, 12)), ('using', (14, 15))]","[('training', (3, 4)), ('all images', (5, 7)), ('cropped and resized', (8, 11)), ('256 256', (12, 14)), ('provided bounding boxes', (15, 18))]","[['cropped and resized', 'to', '256 256'], ['256 256', 'using', 'provided bounding boxes']]","[['training', 'has', 'all images']]","[['Hyperparameters', 'Before', 'training']]",[],face_alignment,17,154
1403,hyperparameters,"We use 6 residual encoder blocks for downsampling the input feature maps , where batch normalization is removed for better synthetic results .","[('use', (1, 2)), ('for', (6, 7)), ('removed for', (17, 19))]","[('6 residual encoder blocks', (2, 6)), ('downsampling', (7, 8)), ('input feature maps', (9, 12)), ('batch normalization', (14, 16)), ('better synthetic results', (19, 22))]","[['6 residual encoder blocks', 'for', 'downsampling'], ['batch normalization', 'removed for', 'better synthetic results']]","[['downsampling', 'has', 'input feature maps']]","[['Hyperparameters', 'use', '6 residual encoder blocks']]",[],face_alignment,17,156
1404,hyperparameters,"For training of the disentangling step , we use Adam with an initial learning rate of 0.01 , which descends linearly to 0.0001 with no augmentation .","[('For', (0, 1)), ('of', (2, 3)), ('use', (8, 9)), ('with', (10, 11)), ('of', (15, 16)), ('to', (21, 22)), ('with', (23, 24))]","[('training', (1, 2)), ('disentangling step', (4, 6)), ('Adam', (9, 10)), ('initial learning rate', (12, 15)), ('0.01', (16, 17)), ('descends linearly', (19, 21)), ('0.0001', (22, 23)), ('no augmentation', (24, 26))]","[['training', 'of', 'disentangling step'], ['initial learning rate', 'of', '0.01'], ['disentangling step', 'use', 'Adam'], ['Adam', 'with', 'initial learning rate'], ['0.0001', 'with', 'no augmentation'], ['initial learning rate', 'of', '0.01'], ['descends linearly', 'to', '0.0001'], ['0.0001', 'with', 'no augmentation']]","[['training', 'has', 'disentangling step'], ['initial learning rate', 'has', '0.01'], ['descends linearly', 'has', '0.0001']]","[['Hyperparameters', 'For', 'training']]",[],face_alignment,17,158
1405,hyperparameters,"For training of detectors , we first augment each landmark map with k random styles sampled from other face images .","[('first augment', (6, 8)), ('with', (11, 12)), ('sampled from', (15, 17))]","[('detectors', (3, 4)), ('each landmark map', (8, 11)), ('k random styles', (12, 15)), ('other face images', (17, 20))]","[['detectors', 'first augment', 'each landmark map'], ['each landmark map', 'with', 'k random styles'], ['k random styles', 'sampled from', 'other face images']]",[],[],[],face_alignment,17,159
1406,hyperparameters,"For the detector architecture , a simple baseline network based on ResNet - 18 is chosen by changing the output dimension of the last FC layers to landmark 2 to demonstrate the increase brought by style translation .","[('based on', (9, 11)), ('by changing', (16, 18)), ('of', (21, 22)), ('to', (26, 27))]","[('detector architecture', (2, 4)), ('simple baseline network', (6, 9)), ('ResNet - 18', (11, 14)), ('output dimension', (19, 21)), ('last FC layers', (23, 26)), ('landmark', (27, 28))]","[['simple baseline network', 'based on', 'ResNet - 18'], ['simple baseline network', 'by changing', 'output dimension'], ['ResNet - 18', 'by changing', 'output dimension'], ['output dimension', 'of', 'last FC layers'], ['last FC layers', 'to', 'landmark']]","[['detector architecture', 'has', 'simple baseline network']]",[],[],face_alignment,17,161
1407,baselines,WFLW,[],"[('WFLW', (0, 1))]",[],[],[],"[['Baselines', 'has', 'WFLW']]",face_alignment,17,164
1408,results,The light - weight Res - 18 is improved by 13.8 % .,"[('improved', (8, 9)), ('by', (9, 10))]","[('light - weight Res - 18', (1, 7)), ('13.8 %', (10, 12))]",[],[],[],[],face_alignment,17,170
1409,results,"By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .","[('utilizing', (1, 2)), ('achieves', (8, 9)), ('under', (12, 13)), ('by', (26, 27))]","[('stronger baseline', (3, 5)), ('our model', (6, 8)), ('4.39 % NME', (9, 12)), ('style - augmented training', (13, 17)), ('outperforms', (18, 19)), ('state - of the - art entries', (19, 26)), ('large margin', (28, 30))]","[['stronger baseline', 'achieves', '4.39 % NME'], ['our model', 'achieves', '4.39 % NME'], ['4.39 % NME', 'under', 'style - augmented training'], ['state - of the - art entries', 'by', 'large margin']]","[['stronger baseline', 'has', 'our model'], ['outperforms', 'has', 'state - of the - art entries']]","[['Results', 'utilizing', 'stronger baseline']]",[],face_alignment,17,171
1410,results,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .","[('brings', (11, 12)), ('to', (15, 16)), ('to', (23, 24)), ('from', (25, 26)), ('to', (29, 30))]","[('our method', (8, 10)), ('15.9 % improvement', (12, 15)), ('SAN model', (16, 18)), ('9 % boost', (20, 23)), ('LAB', (24, 25)), ('5.27 % NME', (26, 29)), ('4.76 %', (30, 32))]","[['our method', 'brings', '15.9 % improvement'], ['our method', 'brings', '9 % boost'], ['15.9 % improvement', 'to', 'SAN model'], ['9 % boost', 'to', 'LAB'], ['5.27 % NME', 'to', '4.76 %'], ['9 % boost', 'to', 'LAB'], ['9 % boost', 'from', '5.27 % NME'], ['LAB', 'from', '5.27 % NME'], ['5.27 % NME', 'to', '4.76 %']]",[],[],[],face_alignment,17,172
1411,results,"With additional "" style- augmented "" synthetic training samples , our model based on a simple backbone outperforms previous state - of - the - art methods .","[('With', (0, 1)), ('based on', (12, 14))]","[('additional "" style- augmented "" synthetic training samples', (1, 9)), ('our model', (10, 12)), ('simple backbone', (15, 17)), ('outperforms', (17, 18)), ('previous state - of - the - art methods', (18, 27))]","[['our model', 'based on', 'simple backbone']]","[['additional "" style- augmented "" synthetic training samples', 'has', 'our model'], ['simple backbone', 'has', 'outperforms'], ['outperforms', 'has', 'previous state - of - the - art methods']]","[['Results', 'With', 'additional "" style- augmented "" synthetic training samples']]",[],face_alignment,17,175
1412,baselines,300W,[],"[('300W', (0, 1))]",[],[],[],"[['Baselines', 'has', '300W']]",face_alignment,17,177
1413,results,"However , our model still yields 1.8 % and 3.1 % improvement on LAB and SAN respectively , which manifest the consistent benefit when using the "" style - augmented "" strategy .","[('yields', (5, 6)), ('on', (12, 13)), ('manifest', (19, 20)), ('using', (24, 25))]","[('1.8 % and 3.1 % improvement', (6, 12)), ('LAB and SAN', (13, 16)), ('consistent benefit', (21, 23)), ('"" style - augmented "" strategy', (26, 32))]","[['1.8 % and 3.1 % improvement', 'on', 'LAB and SAN'], ['1.8 % and 3.1 % improvement', 'manifest', 'consistent benefit'], ['consistent benefit', 'using', '"" style - augmented "" strategy']]",[],[],[],face_alignment,17,183
1414,results,Common Cross - dataset Evaluation on COFW,[],"[('Common Cross - dataset Evaluation on COFW', (0, 7))]",[],[],[],"[['Results', 'has', 'Common Cross - dataset Evaluation on COFW']]",face_alignment,17,185
1415,results,"Our model performs the best with 4.43 % mean error and 2.82 % failure rate , which indicates high robustness to occlusion due to our proper utilization of style translation .","[('performs', (2, 3)), ('with', (5, 6))]","[('Our model', (0, 2)), ('best', (4, 5)), ('4.43 %', (6, 8)), ('mean error', (8, 10)), ('2.82 %', (11, 13)), ('failure rate', (13, 15))]","[['Our model', 'performs', 'best'], ['Our model', 'performs', '2.82 %'], ['best', 'with', '4.43 %'], ['best', 'with', '2.82 %']]","[['4.43 %', 'has', 'mean error'], ['2.82 %', 'has', 'failure rate']]",[],"[['Results', 'has', 'Our model']]",face_alignment,17,188
1416,baselines,AFLW,[],"[('AFLW', (0, 1))]",[],[],[],"[['Baselines', 'has', 'AFLW']]",face_alignment,17,189
1417,experiments,"Exploiting style information also boosts landmark detectors with a large - scale training set ( 25 , 000 images in AFLW ) .","[('Exploiting', (0, 1)), ('boosts', (4, 5)), ('with', (7, 8))]","[('style information', (1, 3)), ('landmark detectors', (5, 7)), ('large - scale training set', (9, 14))]","[['style information', 'boosts', 'landmark detectors'], ['landmark detectors', 'with', 'large - scale training set']]",[],[],[],face_alignment,17,196
1418,results,"Interestingly , our method improves SAN baseline in terms of NME on Full set from 6.94 % to 6.01 % , which indicates that augmenting in style level brings promising improvement on solving large pose variation .","[('improves', (4, 5)), ('in terms of', (7, 10)), ('on', (11, 12)), ('from', (14, 15)), ('to', (17, 18))]","[('our method', (2, 4)), ('SAN baseline', (5, 7)), ('NME', (10, 11)), ('Full set', (12, 14)), ('6.94 %', (15, 17)), ('6.01 %', (18, 20))]","[['our method', 'improves', 'SAN baseline'], ['SAN baseline', 'in terms of', 'NME'], ['NME', 'on', 'Full set'], ['Full set', 'from', '6.94 %'], ['6.94 %', 'to', '6.01 %']]",[],[],"[['Results', 'has', 'our method']]",face_alignment,17,197
1419,results,The visual comparison in shows hidden face part is better modeled with our strategy .,"[('shows', (4, 5)), ('with', (11, 12))]","[('visual comparison', (1, 3)), ('hidden face part', (5, 8)), ('better modeled', (9, 11)), ('our strategy', (12, 14))]","[['visual comparison', 'shows', 'hidden face part'], ['better modeled', 'with', 'our strategy']]","[['visual comparison', 'has', 'hidden face part'], ['hidden face part', 'has', 'better modeled']]",[],"[['Results', 'has', 'visual comparison']]",face_alignment,17,198
1420,ablation-analysis,Disentanglement of style and structure is the key that influences quality of style - augmented samples .,"[('of', (1, 2)), ('influences', (9, 10)), ('of', (11, 12))]","[('Disentanglement', (0, 1)), ('style and structure', (2, 5)), ('key', (7, 8)), ('quality', (10, 11)), ('style - augmented samples', (12, 16))]","[['Disentanglement', 'of', 'style and structure'], ['quality', 'of', 'style - augmented samples'], ['Disentanglement', 'influences', 'quality'], ['style and structure', 'influences', 'quality'], ['key', 'influences', 'quality'], ['quality', 'of', 'style - augmented samples']]",[],[],"[['Ablation analysis', 'has', 'Disentanglement']]",face_alignment,17,201
1421,ablation-analysis,"Style - augmented synthetic images improve detectors ' performance by a large margin , while the improvement is even larger when the number of training images is quite small .","[('improve', (5, 6)), ('by', (9, 10)), ('when', (20, 21)), ('of', (23, 24))]","[('Style - augmented synthetic images', (0, 5)), (""detectors ' performance"", (6, 9)), ('large margin', (11, 13)), ('improvement', (16, 17)), ('even larger', (18, 20)), ('number', (22, 23)), ('training images', (24, 26)), ('quite small', (27, 29))]","[['Style - augmented synthetic images', 'improve', ""detectors ' performance""], [""detectors ' performance"", 'by', 'large margin'], ['even larger', 'when', 'number'], ['number', 'of', 'training images']]","[['improvement', 'has', 'even larger']]",[],"[['Ablation analysis', 'has', 'Style - augmented synthetic images']]",face_alignment,17,210
1422,ablation-analysis,We evaluate our method by adding the number of random sampled styles k of each annotated landmarks on a ResNet - 50 baseline .,"[('evaluate', (1, 2)), ('by adding', (4, 6)), ('of', (8, 9)), ('of', (13, 14)), ('on', (17, 18))]","[('our method', (2, 4)), ('number', (7, 8)), ('random sampled styles k', (9, 13)), ('each annotated landmarks', (14, 17)), ('ResNet - 50 baseline', (19, 23))]","[['our method', 'by adding', 'number'], ['number', 'of', 'random sampled styles k'], ['random sampled styles k', 'of', 'each annotated landmarks'], ['random sampled styles k', 'of', 'each annotated landmarks'], ['each annotated landmarks', 'on', 'ResNet - 50 baseline']]",[],"[['Ablation analysis', 'evaluate', 'our method']]",[],face_alignment,17,225
1423,ablation-analysis,"By adding a number of augmented styles , the model continue gaining improvement .","[('By adding', (0, 2)), ('of', (4, 5))]","[('number', (3, 4)), ('augmented styles', (5, 7)), ('model', (9, 10)), ('continue gaining improvement', (10, 13))]","[['number', 'of', 'augmented styles']]","[['number', 'has', 'augmented styles'], ['augmented styles', 'has', 'model'], ['model', 'has', 'continue gaining improvement']]","[['Ablation analysis', 'By adding', 'number']]",[],face_alignment,17,227
1424,research-problem,Deep Alignment Network : A convolutional neural network for robust face alignment,[],"[('robust face alignment', (9, 12))]",[],[],[],[],face_alignment,18,2
1425,research-problem,"The goal of face alignment is to localize a set of predefined facial landmarks ( eye corners , mouth corners etc. ) in an image of a face .",[],"[('face alignment', (3, 5))]",[],[],[],[],face_alignment,18,12
1426,model,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .","[('proposing', (10, 11)), ('dub', (18, 19))]","[('novel face alignment method', (12, 16)), ('Deep Alignment Network ( DAN )', (19, 25))]","[['novel face alignment method', 'dub', 'Deep Alignment Network ( DAN )']]","[['novel face alignment method', 'name', 'Deep Alignment Network ( DAN )']]","[['Model', 'proposing', 'novel face alignment method']]",[],face_alignment,18,19
1427,model,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .","[('based on', (2, 4)), ('where', (8, 9)), ('refines', (11, 12)), ('estimated at', (15, 17))]","[('multistage neural network', (5, 8)), ('each stage', (9, 11)), ('landmark positions', (13, 15)), ('previous stage', (18, 20))]","[['multistage neural network', 'where', 'each stage'], ['each stage', 'refines', 'landmark positions'], ['landmark positions', 'estimated at', 'previous stage']]","[['multistage neural network', 'has', 'each stage']]","[['Model', 'based on', 'multistage neural network']]",[],face_alignment,18,20
1428,model,The input to each stage of our algorithm ( except the first stage ) area face image normalized to a canonical pose and an image learned from the dense layer of the previous stage .,"[('to', (2, 3)), ('of', (5, 6)), ('normalized to', (17, 19)), ('learned from', (25, 27)), ('of', (30, 31))]","[('The input', (0, 2)), ('each stage', (3, 5)), ('our algorithm', (6, 8)), ('face image', (15, 17)), ('canonical pose', (20, 22)), ('image', (24, 25)), ('dense layer', (28, 30)), ('previous stage', (32, 34))]","[['The input', 'to', 'each stage'], ['each stage', 'of', 'our algorithm'], ['dense layer', 'of', 'previous stage'], ['face image', 'normalized to', 'canonical pose'], ['face image', 'normalized to', 'image'], ['image', 'learned from', 'dense layer'], ['dense layer', 'of', 'previous stage']]","[['The input', 'has', 'each stage']]",[],"[['Model', 'has', 'The input']]",face_alignment,18,21
1429,model,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .","[('make use of', (1, 4)), ('during', (8, 9)), ('of', (11, 12)), ('additionally input at', (16, 19))]","[('entire face image', (5, 8)), ('process', (10, 11)), ('face alignment', (12, 14)), ('each stage', (19, 21)), ('landmark heatmap', (22, 24))]","[['entire face image', 'during', 'process'], ['process', 'of', 'face alignment']]","[['each stage', 'has', 'landmark heatmap']]","[['Model', 'make use of', 'entire face image']]",[],face_alignment,18,22
1430,experimental-setup,During data augmentation a total of 10 images are created from each input image in the training set .,"[('During', (0, 1)), ('created from', (9, 11)), ('in', (14, 15))]","[('data augmentation', (1, 3)), ('total of 10 images', (4, 8)), ('each input image', (11, 14)), ('training set', (16, 18))]","[['total of 10 images', 'created from', 'each input image'], ['each input image', 'in', 'training set']]","[['data augmentation', 'has', 'total of 10 images']]","[['Experimental setup', 'During', 'data augmentation']]",[],face_alignment,18,193
1431,experimental-setup,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,"[('performed using', (2, 4))]","[('Training', (0, 1)), ('Theano 0.9.0', (4, 6)), ('Lasagne 0.2', (7, 9))]","[['Training', 'performed using', 'Theano 0.9.0'], ['Training', 'performed using', 'Lasagne 0.2']]",[],[],"[['Experimental setup', 'has', 'Training']]",face_alignment,18,195
1432,experimental-setup,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,"[('For', (0, 1)), ('use', (3, 4)), ('with', (7, 8)), ('of', (12, 13)), ('of', (18, 19))]","[('optimization', (1, 2)), ('Adam stochastic optimization', (4, 7)), ('initial step size', (9, 12)), ('0.001', (13, 14)), ('mini batch size', (15, 18)), ('64', (19, 20))]","[['optimization', 'use', 'Adam stochastic optimization'], ['Adam stochastic optimization', 'with', 'initial step size'], ['Adam stochastic optimization', 'with', 'mini batch size'], ['initial step size', 'of', '0.001'], ['mini batch size', 'of', '64'], ['mini batch size', 'of', '64']]","[['mini batch size', 'has', '64']]","[['Experimental setup', 'For', 'optimization']]",[],face_alignment,18,196
1433,experimental-setup,For validation we use a random subset of 100 images from the training set .,"[('use', (3, 4)), ('of', (7, 8)), ('from', (10, 11))]","[('validation', (1, 2)), ('random subset', (5, 7)), ('100 images', (8, 10)), ('training set', (12, 14))]","[['validation', 'use', 'random subset'], ['random subset', 'of', '100 images'], ['100 images', 'from', 'training set']]",[],[],[],face_alignment,18,197
1434,experimental-setup,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,"[('runs at', (3, 5)), ('for', (7, 8)), ('processed in', (9, 11)), ('for', (16, 17)), ('processed', (18, 19)), ('on', (20, 21))]","[('Python implementation', (1, 3)), ('73 fps', (5, 7)), ('images', (8, 9)), ('parallel', (11, 12)), ('45 fps', (14, 16)), ('images', (17, 18)), ('sequentially', (19, 20)), ('GeForce GTX 1070 GPU', (22, 26))]","[['Python implementation', 'runs at', '73 fps'], ['Python implementation', 'runs at', '45 fps'], ['73 fps', 'for', 'images'], ['45 fps', 'for', 'images'], ['images', 'processed in', 'parallel'], ['45 fps', 'for', 'images'], ['45 fps', 'processed', 'sequentially'], ['images', 'processed', 'sequentially'], ['sequentially', 'on', 'GeForce GTX 1070 GPU']]",[],[],"[['Experimental setup', 'has', 'Python implementation']]",face_alignment,18,198
1435,results,"a failure rate reduction of 60 % on the 300 W private test set ,","[('of', (4, 5)), ('on', (7, 8))]","[('failure rate reduction', (1, 4)), ('60 %', (5, 7)), ('300 W private test set', (9, 14))]","[['failure rate reduction', 'of', '60 %'], ['60 %', 'on', '300 W private test set']]","[['failure rate reduction', 'has', '60 %']]",[],[],face_alignment,18,210
1436,results,"a failure rate reduction of 72 % on the 300W public test set ,","[('of', (4, 5)), ('on', (7, 8))]","[('72 %', (5, 7)), ('300W public test set', (9, 13))]","[['72 %', 'on', '300W public test set']]",[],[],[],face_alignment,18,211
1437,results,a 9 % improvement of the mean error on the challenging subset .,"[('on', (8, 9))]","[('9 % improvement', (1, 4)), ('mean error', (6, 8)), ('challenging subset', (10, 12))]","[['mean error', 'on', 'challenging subset']]",[],[],[],face_alignment,18,212
1438,research-problem,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild,[],"[('Face Alignment In The Wild', (6, 11))]",[],[],[],[],face_alignment,2,2
1439,research-problem,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .",[],"[('Face Alignment', (0, 2))]",[],[],[],[],face_alignment,2,4
1440,model,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .","[('introduce', (5, 6))]","[('Deep convolutional Cascade for Face Alignment ( DeCaFA )', (7, 16))]",[],[],"[['Model', 'introduce', 'Deep convolutional Cascade for Face Alignment ( DeCaFA )']]",[],face_alignment,2,22
1441,model,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .","[('composed of', (2, 4)), ('produce', (8, 9)), ('relatively to', (15, 17))]","[('DeCaFA', (0, 1)), ('several stages', (4, 6)), ('landmark - wise attention maps', (9, 14)), ('heterogeneous annotation markups', (17, 20))]","[['DeCaFA', 'composed of', 'several stages'], ['several stages', 'produce', 'landmark - wise attention maps'], ['landmark - wise attention maps', 'relatively to', 'heterogeneous annotation markups']]",[],[],"[['Model', 'has', 'DeCaFA']]",face_alignment,2,23
1442,model,"It illustrates how these attention maps are refined through the successive stages , and how the different prediction tasks can benefit from each other .","[('refined through', (7, 9))]","[('attention maps', (4, 6)), ('successive stages', (10, 12))]","[['attention maps', 'refined through', 'successive stages']]",[],[],"[['Model', 'has', 'attention maps']]",face_alignment,2,25
1443,hyperparameters,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .","[('use', (8, 9)), ('contains', (15, 16)), ('with', (21, 22)), ('for', (34, 35))]","[('1 to 4 stages', (9, 13)), ('12 3 3 convolutional layers', (16, 21)), ('64 ? 64 ? 128 ? 128 ? 256 ? 256 channels', (22, 34)), ('downsampling portion', (36, 38))]","[['1 to 4 stages', 'contains', '12 3 3 convolutional layers'], ['12 3 3 convolutional layers', 'with', '64 ? 64 ? 128 ? 128 ? 256 ? 256 channels'], ['64 ? 64 ? 128 ? 128 ? 256 ? 256 channels', 'for', 'downsampling portion']]",[],"[['Hyperparameters', 'use', '1 to 4 stages']]",[],face_alignment,2,130
1444,hyperparameters,The input images are resized to 128 128 grayscale images prior to being processed by the network .,"[('resized to', (4, 6)), ('prior to', (10, 12)), ('by', (14, 15))]","[('input images', (1, 3)), ('128 128 grayscale images', (6, 10)), ('being processed', (12, 14)), ('network', (16, 17))]","[['input images', 'resized to', '128 128 grayscale images'], ['128 128 grayscale images', 'prior to', 'being processed'], ['being processed', 'by', 'network']]",[],[],"[['Hyperparameters', 'has', 'input images']]",face_alignment,2,131
1445,hyperparameters,Each convolution is followed by a batch normalization layer with ReLU activation .,"[('followed by', (3, 5)), ('with', (9, 10))]","[('Each convolution', (0, 2)), ('batch normalization layer', (6, 9)), ('ReLU activation', (10, 12))]","[['Each convolution', 'followed by', 'batch normalization layer'], ['batch normalization layer', 'with', 'ReLU activation']]",[],[],"[['Hyperparameters', 'has', 'Each convolution']]",face_alignment,2,132
1446,hyperparameters,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,"[('followed with', (17, 19))]","[('bilinear image upsampling', (14, 17)), ('3 3 convolutional layers', (19, 23))]","[['bilinear image upsampling', 'followed with', '3 3 convolutional layers']]",[],[],[],face_alignment,2,133
1447,hyperparameters,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,"[('trained using', (4, 6)), ('with', (8, 9)), ('with', (15, 16))]","[('whole architecture', (1, 3)), ('ADAM optimizer', (6, 8)), ('5e ? 4', (10, 13)), ('learning rate', (13, 15)), ('momentum', (16, 17)), ('0.9', (17, 18)), ('learning rate annealing', (19, 22)), ('power 0.9', (23, 25))]","[['whole architecture', 'trained using', 'ADAM optimizer'], ['ADAM optimizer', 'with', '5e ? 4'], ['ADAM optimizer', 'with', 'learning rate annealing'], ['learning rate', 'with', 'momentum'], ['learning rate annealing', 'with', 'power 0.9'], ['learning rate', 'with', 'momentum']]","[['5e ? 4', 'has', 'learning rate'], ['momentum', 'has', '0.9']]",[],"[['Hyperparameters', 'has', 'whole architecture']]",face_alignment,2,134
1448,hyperparameters,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .","[('apply', (1, 2)), ('with', (4, 5)), ('for', (8, 9))]","[('400000 updates', (2, 4)), ('batch size', (5, 7)), ('8', (7, 8)), ('each database', (9, 11))]","[['400000 updates', 'with', 'batch size'], ['batch size', 'for', 'each database'], ['8', 'for', 'each database']]","[['batch size', 'has', '8']]","[['Hyperparameters', 'apply', '400000 updates']]",[],face_alignment,2,135
1449,ablation-analysis,"The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .","[('add', (6, 7)), ('after', (12, 13)), ('on', (15, 16))]","[('accuracy', (1, 2)), ('steadily increases', (2, 4)), ('more stages', (7, 9)), ('saturates', (11, 12)), ('third', (14, 15)), ('LFPW and HELEN', (16, 19))]","[['accuracy', 'add', 'more stages'], ['steadily increases', 'add', 'more stages'], ['saturates', 'after', 'third'], ['third', 'on', 'LFPW and HELEN']]","[['accuracy', 'has', 'steadily increases'], ['saturates', 'has', 'third']]",[],"[['Ablation analysis', 'has', 'accuracy']]",face_alignment,2,153
1450,ablation-analysis,"Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .","[('significantly helps', (7, 9))]","[('Coarsely annotated data ( 5 landmarks )', (0, 7)), ('fine - grained landmark localization', (10, 15))]","[['Coarsely annotated data ( 5 landmarks )', 'significantly helps', 'fine - grained landmark localization']]",[],[],"[['Ablation analysis', 'has', 'Coarsely annotated data ( 5 landmarks )']]",face_alignment,2,156
1451,ablation-analysis,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .","[('reinjecting', (2, 3)), ('significantly improves', (18, 20)), ('on', (22, 23)), ('such as', (25, 27))]","[('whole input image', (4, 7)), ('accuracy', (21, 22)), ('challenging data', (23, 25)), ('300 W - challenging or WFLW - pose', (27, 35))]","[['whole input image', 'significantly improves', 'accuracy'], ['accuracy', 'on', 'challenging data'], ['challenging data', 'such as', '300 W - challenging or WFLW - pose']]","[['whole input image', 'name', 'accuracy']]","[['Ablation analysis', 'reinjecting', 'whole input image']]",[],face_alignment,2,160
1452,ablation-analysis,F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,"[('using', (15, 16)), ('rivals', (20, 21))]","[('F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models )', (0, 15)), ('local + global information', (16, 20)), ('basic deep approach F 1 - Equation ( 4 )', (22, 32))]","[['F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models )', 'using', 'local + global information'], ['local + global information', 'rivals', 'basic deep approach F 1 - Equation ( 4 )']]",[],[],[],face_alignment,2,161
1453,ablation-analysis,"Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .","[('uses', (9, 10)), ('by', (17, 18))]","[('F 5 - Equation fusion', (2, 7)), ('local and global cues', (10, 14)), ('best', (16, 17)), ('significant margin', (19, 21))]","[['F 5 - Equation fusion', 'uses', 'local and global cues'], ['best', 'by', 'significant margin']]",[],[],"[['Ablation analysis', 'has', 'F 5 - Equation fusion']]",face_alignment,2,162
1454,ablation-analysis,"Furthermore , chaining the transfer layers is better than using independant transfer layers : likewise , in such a case , the first transfer layer benefits from the gradients from the subsequents layer at train time .","[('chaining', (2, 3)), ('better than', (7, 9)), ('using', (9, 10))]","[('transfer layers', (4, 6)), ('independant transfer layers', (10, 13))]",[],[],"[['Ablation analysis', 'chaining', 'transfer layers']]",[],face_alignment,2,163
1455,ablation-analysis,"Finally , shows a comparison of our method and state - of - the - art approaches on Celeb A .","[('on', (17, 18))]","[('Celeb A', (18, 20))]",[],[],"[['Ablation analysis', 'on', 'Celeb A']]",[],face_alignment,2,176
1456,ablation-analysis,"Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .","[('sets', (3, 4)), ('on', (13, 14))]","[('DeCaFA', (2, 3)), ('new state - of - the - art', (5, 13)), ('three databases', (15, 17))]","[['DeCaFA', 'sets', 'new state - of - the - art'], ['new state - of - the - art', 'on', 'three databases']]","[['DeCaFA', 'has', 'new state - of - the - art']]",[],"[['Ablation analysis', 'has', 'DeCaFA']]",face_alignment,2,181
1457,research-problem,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,[],"[('Robust Face Alignment', (4, 7))]",[],[],[],[],face_alignment,3,2
1458,code,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss .,[],"[('https://github.com/protossw512/AdaptiveWingLoss', (7, 8))]",[],[],[],[],face_alignment,3,13
1459,research-problem,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .",[],"[('Face alignment', (0, 2)), ('facial landmark localization', (6, 9))]",[],[],[],[],face_alignment,3,15
1460,model,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .","[('propose', (2, 3)), ('of', (24, 25))]","[('new loss function', (4, 7)), ('Adaptive Wing loss', (10, 13)), ('significantly improve', (20, 22)), ('quality', (23, 24)), ('heatmap regression results', (25, 28))]","[['quality', 'of', 'heatmap regression results']]","[['significantly improve', 'has', 'quality']]","[['Model', 'propose', 'new loss function']]",[],face_alignment,3,30
1461,model,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .","[('of', (5, 6)), ('in', (9, 10)), ('such as', (19, 21)), ('to capture', (31, 33))]","[('translation invariance', (3, 5)), ('convolution operation', (7, 9)), ('bottom - up and top - down CNN structures', (10, 19)), ('stacked Hourglass ( HG )', (21, 26)), ('coordinate information', (33, 35))]","[['translation invariance', 'of', 'convolution operation'], ['convolution operation', 'in', 'bottom - up and top - down CNN structures'], ['bottom - up and top - down CNN structures', 'such as', 'stacked Hourglass ( HG )']]","[['bottom - up and top - down CNN structures', 'name', 'stacked Hourglass ( HG )']]",[],[],face_alignment,3,31
1462,model,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .","[('encode', (14, 15)), ('into', (15, 16)), ('on', (26, 27)), ('predicted from', (28, 30))]","[('our model', (16, 18)), ('full coordinate information', (19, 22)), ('information', (24, 25)), ('boundaries', (27, 28)), ('previous HG module', (31, 34))]","[['information', 'on', 'boundaries'], ['boundaries', 'predicted from', 'previous HG module']]","[['our model', 'has', 'full coordinate information']]","[['Model', 'encode', 'our model']]",[],face_alignment,3,32
1463,model,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .","[('To encode', (0, 2)), ('add', (7, 8)), ('by concatenating', (13, 15)), ('into', (19, 20)), ('with', (28, 29))]","[('boundary coordinates', (2, 4)), ('sub-task of boundary prediction', (9, 13)), ('additional boundary channel', (16, 19)), ('ground truth heatmap', (21, 24)), ('jointly trained', (26, 28)), ('other channels', (29, 31))]","[['boundary coordinates', 'add', 'sub-task of boundary prediction'], ['sub-task of boundary prediction', 'by concatenating', 'additional boundary channel'], ['additional boundary channel', 'into', 'ground truth heatmap'], ['jointly trained', 'with', 'other channels']]",[],"[['Model', 'To encode', 'boundary coordinates']]",[],face_alignment,3,34
1464,hyperparameters,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .","[('use', (4, 5)), ('with', (8, 9)), ('of', (13, 14))]","[('RM - SProp', (5, 8)), ('initial learning rate', (10, 13)), ('1 10 ?4', (14, 17))]","[['RM - SProp', 'with', 'initial learning rate'], ['initial learning rate', 'of', '1 10 ?4']]",[],"[['Hyperparameters', 'use', 'RM - SProp']]",[],face_alignment,3,231
1465,hyperparameters,We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,"[('set', (1, 2)), ('to be', (4, 6)), ('to be', (15, 17))]","[('momentum', (3, 4)), ('0', (6, 7)), ('weight decay', (13, 15)), ('1 10 ?5', (17, 20))]","[['momentum', 'to be', '0'], ['weight decay', 'to be', '1 10 ?5']]","[['momentum', 'has', '0'], ['weight decay', 'has', '1 10 ?5']]","[['Hyperparameters', 'set', 'momentum']]",[],face_alignment,3,232
1466,hyperparameters,"We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .","[('train', (1, 2)), ('for', (2, 3)), ('reduced to', (11, 13)), ('after', (21, 22))]","[('240 epoches', (3, 5)), ('learning rate', (8, 10)), ('1 10 ?5 and 1 10 ? 6', (13, 21)), ('80 and 160 epoches', (22, 26))]","[['learning rate', 'reduced to', '1 10 ?5 and 1 10 ? 6'], ['1 10 ?5 and 1 10 ? 6', 'after', '80 and 160 epoches']]","[['learning rate', 'has', '1 10 ?5 and 1 10 ? 6']]","[['Hyperparameters', 'train', '240 epoches']]",[],face_alignment,3,233
1467,hyperparameters,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .","[('performed with', (3, 5))]","[('Data augmentation', (0, 2)), ('random rotation', (5, 7)), ('50', (8, 9)), ('translation', (11, 12)), ('25 px', (13, 15)), ('flipping', (17, 18)), ('50 %', (19, 21)), ('rescaling', (24, 25)), ('15 %', (26, 28))]","[['Data augmentation', 'performed with', 'random rotation'], ['Data augmentation', 'performed with', 'translation'], ['Data augmentation', 'performed with', 'flipping'], ['Data augmentation', 'performed with', 'rescaling']]","[['random rotation', 'has', '50'], ['translation', 'has', '25 px'], ['flipping', 'has', '50 %'], ['rescaling', 'has', '15 %']]",[],"[['Hyperparameters', 'has', 'Data augmentation']]",face_alignment,3,234
1468,hyperparameters,"Random Gaussian blur , noise and occlusion are also used .","[('used', (9, 10))]","[('Random Gaussian blur', (0, 3)), ('noise', (4, 5)), ('occlusion', (6, 7))]","[['Random Gaussian blur', 'used', 'occlusion']]",[],[],[],face_alignment,3,235
1469,results,Evaluation on 300W,"[('on', (1, 2))]","[('300W', (2, 3))]",[],[],"[['Results', 'on', '300W']]",[],face_alignment,3,244
1470,results,"Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .","[('achieve', (5, 6))]","[('Our method', (0, 2)), ('state - of - the - art performance', (7, 15))]","[['Our method', 'achieve', 'state - of - the - art performance']]",[],[],"[['Results', 'has', 'Our method']]",face_alignment,3,245
1471,results,"For the challenge subset ( iBug dataset ) , we are able to outperform","[('For', (0, 1)), ('able to', (11, 13))]","[('challenge subset ( iBug dataset )', (2, 8)), ('outperform', (13, 14))]","[['challenge subset ( iBug dataset )', 'able to', 'outperform']]","[['challenge subset ( iBug dataset )', 'has', 'outperform']]","[['Results', 'For', 'challenge subset ( iBug dataset )']]",[],face_alignment,3,246
1472,results,"Wing by a significant margin , which also proves the robustness of our approach against occlusion and large pose variation .","[('by', (1, 2))]","[('Wing', (0, 1)), ('significant margin', (3, 5))]","[['Wing', 'by', 'significant margin']]","[['Wing', 'has', 'significant margin']]",[],[],face_alignment,3,247
1473,results,"Furthermore , on the 300 W private test dataset ) , we again outperform the previous state - of - theart on variant metrics including NME , AUC and FR measured with either 8 % NME and 10 % NME .","[('on', (2, 3)), ('outperform', (13, 14)), ('on', (21, 22)), ('including', (24, 25)), ('measured with', (30, 32))]","[('300 W private test dataset', (4, 9)), ('previous state - of - theart', (15, 21)), ('variant metrics', (22, 24)), ('NME , AUC and FR', (25, 30)), ('8 % NME and 10 % NME', (33, 40))]","[['previous state - of - theart', 'on', 'variant metrics'], ['300 W private test dataset', 'outperform', 'previous state - of - theart'], ['previous state - of - theart', 'on', 'variant metrics'], ['variant metrics', 'including', 'NME , AUC and FR'], ['NME , AUC and FR', 'measured with', '8 % NME and 10 % NME']]","[['300 W private test dataset', 'has', 'previous state - of - theart']]","[['Results', 'on', '300 W private test dataset']]",[],face_alignment,3,248
1474,results,Evaluation on WFLW,[],"[('WFLW', (2, 3))]",[],[],[],[],face_alignment,3,250
1475,results,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .","[('achieves', (3, 4))]","[('Our method', (0, 2)), ('best results', (5, 7))]","[['Our method', 'achieves', 'best results']]",[],[],"[['Results', 'has', 'Our method']]",face_alignment,3,251
1476,results,On every subset we outperform the previous state - of - the - art ap - :,"[('On', (0, 1)), ('outperform', (4, 5))]","[('every subset', (1, 3)), ('previous state - of - the - art ap -', (6, 16))]","[['every subset', 'outperform', 'previous state - of - the - art ap -']]",[],"[['Results', 'On', 'every subset']]",[],face_alignment,3,252
1477,results,"All in all , our approach fails on only 2.84 % of all images , more than a two times improvement compared with 7.6 .","[('on', (7, 8)), ('of', (11, 12))]","[('our approach', (4, 6)), ('fails', (6, 7)), ('only 2.84 %', (8, 11)), ('all images', (12, 14))]","[['fails', 'on', 'only 2.84 %'], ['only 2.84 %', 'of', 'all images']]","[['our approach', 'has', 'fails']]",[],[],face_alignment,3,256
1478,research-problem,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network,[],"[('Facial Landmarks Detection', (0, 3))]",[],[],[],[],face_alignment,4,2
1479,model,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .","[('develop', (5, 6))]","[('Self - Iterative Regression ( SIR ) framework', (7, 15))]",[],[],"[['Model', 'develop', 'Self - Iterative Regression ( SIR ) framework']]",[],face_alignment,4,31
1480,model,"By means of the powerful representation of Convolutional Neural Network ( CNN ) , we only train one regressor to learn the descent directions in coarse and fine stages together .","[('By means of', (0, 3)), ('of', (6, 7)), ('train', (16, 17)), ('to learn', (19, 21)), ('in', (24, 25))]","[('powerful representation', (4, 6)), ('Convolutional Neural Network ( CNN )', (7, 13)), ('one regressor', (17, 19)), ('descent directions', (22, 24)), ('coarse and fine stages', (25, 29))]","[['powerful representation', 'of', 'Convolutional Neural Network ( CNN )'], ['one regressor', 'to learn', 'descent directions'], ['descent directions', 'in', 'coarse and fine stages']]",[],"[['Model', 'By means of', 'powerful representation']]",[],face_alignment,4,32
1481,model,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .","[('to obtain', (2, 4)), ('proposed', (9, 10)), ('focuses on', (20, 22)), ('around', (24, 25))]","[('discriminative landmarks features', (4, 7)), ('Landmarks - Attention Network ( LAN )', (11, 18)), ('appearance', (23, 24)), ('landmarks', (25, 26))]","[['discriminative landmarks features', 'proposed', 'Landmarks - Attention Network ( LAN )'], ['Landmarks - Attention Network ( LAN )', 'focuses on', 'appearance'], ['appearance', 'around', 'landmarks']]","[['discriminative landmarks features', 'name', 'Landmarks - Attention Network ( LAN )']]","[['Model', 'to obtain', 'discriminative landmarks features']]",[],face_alignment,4,36
1482,model,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .","[('first concurrently extracts', (1, 4)), ('obtains', (10, 11)), ('significantly reduces', (16, 18)), ('of', (20, 21)), ('of', (28, 29))]","[(""local landmarks ' features"", (4, 8)), ('holistic increment', (12, 14)), ('dimension', (19, 20)), ('final feature layer', (22, 25)), ('number', (27, 28)), ('model parameters', (29, 31))]","[['holistic increment', 'significantly reduces', 'dimension'], ['holistic increment', 'significantly reduces', 'number'], ['dimension', 'of', 'final feature layer'], ['dimension', 'of', 'number'], ['number', 'of', 'model parameters']]",[],"[['Model', 'first concurrently extracts', ""local landmarks ' features""]]",[],face_alignment,4,37
1483,experimental-setup,"We perform the experiments based on a machine with Core i7 - 5930 k CPU , 32 GB memory and GTX 1080 GPU with 8G video memory .","[('perform', (1, 2)), ('based on', (4, 6)), ('with', (8, 9)), ('with', (23, 24))]","[('experiments', (3, 4)), ('machine', (7, 8)), ('Core i7 - 5930 k CPU', (9, 15)), ('32 GB memory', (16, 19)), ('GTX 1080 GPU', (20, 23)), ('8G video memory', (24, 27))]","[['experiments', 'based on', 'machine'], ['machine', 'with', 'Core i7 - 5930 k CPU'], ['machine', 'with', 'GTX 1080 GPU'], ['GTX 1080 GPU', 'with', '8G video memory'], ['GTX 1080 GPU', 'with', '8G video memory']]","[['experiments', 'has', 'machine']]","[['Experimental setup', 'perform', 'experiments']]",[],face_alignment,4,167
1484,experimental-setup,The detected faces are resized into 256 256 and the location patch size is 57 57 .,"[('resized into', (4, 6)), ('is', (13, 14))]","[('detected faces', (1, 3)), ('256 256', (6, 8)), ('location patch size', (10, 13)), ('57 57', (14, 16))]","[['detected faces', 'resized into', '256 256'], ['location patch size', 'is', '57 57']]","[['location patch size', 'has', '57 57']]",[],"[['Experimental setup', 'has', 'detected faces']]",face_alignment,4,168
1485,experimental-setup,"For CNN structure , the Rectified Linear Unit ( ReLU ) is adopted as the activation function , and the optimizer is the Adadelta ( Zeiler 2012 ) approach , learning rate is set to 0.1 and weight decay is set to 1 e ?","[('For', (0, 1)), ('is', (11, 12)), ('adopted as', (12, 14)), ('set to', (33, 35)), ('set to', (40, 42))]","[('CNN structure', (1, 3)), ('Rectified Linear Unit ( ReLU )', (5, 11)), ('activation function', (15, 17)), ('optimizer', (20, 21)), ('Adadelta ( Zeiler 2012 ) approach', (23, 29)), ('learning rate', (30, 32)), ('0.1', (35, 36)), ('weight decay', (37, 39)), ('1 e ?', (42, 45))]","[['optimizer', 'is', 'Adadelta ( Zeiler 2012 ) approach'], ['Rectified Linear Unit ( ReLU )', 'adopted as', 'activation function'], ['learning rate', 'set to', '0.1'], ['weight decay', 'set to', '1 e ?']]","[['CNN structure', 'has', 'Rectified Linear Unit ( ReLU )'], ['optimizer', 'has', 'Adadelta ( Zeiler 2012 ) approach'], ['learning rate', 'has', '0.1']]","[['Experimental setup', 'For', 'CNN structure']]",[],face_alignment,4,169
1486,experimental-setup,4 . Training the CNN requires around 2 days .,"[('Training', (2, 3)), ('requires', (5, 6))]","[('CNN', (4, 5)), ('around 2 days', (6, 9))]","[['CNN', 'requires', 'around 2 days']]",[],"[['Experimental setup', 'Training', 'CNN']]",[],face_alignment,4,170
1487,results,The NME results shows that SIR performs comparatively with RAR ) and outperform other existing methods .,"[('shows', (3, 4)), ('performs', (6, 7)), ('with', (8, 9)), ('outperform', (12, 13))]","[('NME results', (1, 3)), ('SIR', (5, 6)), ('comparatively', (7, 8)), ('RAR', (9, 10)), ('other existing methods', (13, 16))]","[['NME results', 'shows', 'SIR'], ['SIR', 'performs', 'comparatively'], ['comparatively', 'with', 'RAR'], ['SIR', 'outperform', 'other existing methods']]","[['NME results', 'has', 'SIR']]",[],"[['Results', 'has', 'NME results']]",face_alignment,4,175
1488,results,"In the more challenging IBUG subset , our method achieves robust performance in large pose , expression and illumination environment .","[('In', (0, 1)), ('achieves', (9, 10)), ('in', (12, 13))]","[('more challenging IBUG subset', (2, 6)), ('our method', (7, 9)), ('robust performance', (10, 12)), ('large pose , expression and illumination environment', (13, 20))]","[['robust performance', 'In', 'large pose , expression and illumination environment'], ['our method', 'achieves', 'robust performance'], ['robust performance', 'in', 'large pose , expression and illumination environment']]","[['more challenging IBUG subset', 'has', 'our method']]",[],[],face_alignment,4,177
1489,results,"As shown in , the SIR method outperform the state - of - the - art methods according to the CED curve .","[('outperform', (7, 8)), ('according to', (17, 19))]","[('SIR method', (5, 7)), ('state - of - the - art methods', (9, 17)), ('CED curve', (20, 22))]","[['SIR method', 'outperform', 'state - of - the - art methods'], ['state - of - the - art methods', 'according to', 'CED curve']]","[['SIR method', 'has', 'state - of - the - art methods']]",[],"[['Results', 'has', 'SIR method']]",face_alignment,4,179
1490,research-problem,Look at Boundary : A Boundary - Aware Face Alignment Algorithm,[],"[('Face Alignment', (8, 10))]",[],[],[],[],face_alignment,5,2
1491,code,Dataset and model will be publicly available at https://wywu.github.io/projects/LAB/LAB.html,[],"[('https://wywu.github.io/projects/LAB/LAB.html', (8, 9))]",[],[],[],[],face_alignment,5,14
1492,research-problem,"Face alignment , which refers to facial landmark detection in this work , serves as a key step for many face applications , e.g. , face recognition , face verification and face frontalisation .",[],"[('facial landmark detection', (6, 9))]",[],[],[],[],face_alignment,5,16
1493,approach,"To this end , we use well - defined facial boundaries to represent the geometric structure of the human face .","[('use', (5, 6)), ('to represent', (11, 13)), ('of', (16, 17))]","[('well - defined facial boundaries', (6, 11)), ('geometric structure', (14, 16)), ('human face', (18, 20))]","[['well - defined facial boundaries', 'to represent', 'geometric structure'], ['geometric structure', 'of', 'human face']]",[],"[['Approach', 'use', 'well - defined facial boundaries']]",[],face_alignment,5,28
1494,approach,"In this work , we represent facial structure using 13 boundary lines .","[('represent', (5, 6)), ('using', (8, 9))]","[('facial structure', (6, 8)), ('13 boundary lines', (9, 12))]","[['facial structure', 'using', '13 boundary lines']]",[],"[['Approach', 'represent', 'facial structure']]",[],face_alignment,5,30
1495,approach,Our boundary - aware face alignment algorithm contains two stages .,"[('contains', (7, 8))]","[('Our boundary - aware face alignment algorithm', (0, 7)), ('two stages', (8, 10))]","[['Our boundary - aware face alignment algorithm', 'contains', 'two stages']]","[['Our boundary - aware face alignment algorithm', 'has', 'two stages']]",[],"[['Approach', 'has', 'Our boundary - aware face alignment algorithm']]",face_alignment,5,32
1496,approach,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,"[('first estimate', (1, 3)), ('with', (10, 11))]","[('facial boundary heatmaps', (3, 6)), ('regress', (8, 9)), ('landmarks', (9, 10)), ('boundary heatmaps', (14, 16))]",[],"[['regress', 'has', 'landmarks']]","[['Approach', 'first estimate', 'facial boundary heatmaps']]",[],face_alignment,5,33
1497,approach,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .","[('To explore', (0, 2)), ('between', (4, 5)), ('introduce', (11, 12)), ('by using', (15, 17))]","[('relationship', (3, 4)), ('facial boundaries and landmarks', (5, 9)), ('adversarial learning ideas', (12, 15)), ('landmark - based boundary effectiveness discriminator', (18, 24))]","[['relationship', 'between', 'facial boundaries and landmarks'], ['relationship', 'introduce', 'adversarial learning ideas'], ['facial boundaries and landmarks', 'introduce', 'adversarial learning ideas'], ['adversarial learning ideas', 'by using', 'landmark - based boundary effectiveness discriminator']]",[],"[['Approach', 'To explore', 'relationship']]",[],face_alignment,5,35
1498,approach,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .","[('in', (16, 17))]","[('boundary heatmap estimator', (1, 4)), ('landmark regressor', (5, 7)), ('boundary effectiveness discriminator', (9, 12)), ('jointly learned', (14, 16)), ('end - to - end manner', (18, 24))]","[['jointly learned', 'in', 'end - to - end manner']]",[],[],"[['Approach', 'has', 'boundary heatmap estimator']]",face_alignment,5,37
1499,approach,We used stacked hourglass structure to estimate facial boundary heatmap and model the structure between facial boundaries through message passing to increase its robustness to occlusion .,"[('used', (1, 2)), ('to estimate', (5, 7)), ('model', (11, 12)), ('between', (14, 15)), ('through', (17, 18)), ('to increase', (20, 22)), ('to', (24, 25))]","[('stacked hourglass structure', (2, 5)), ('facial boundary heatmap', (7, 10)), ('structure', (13, 14)), ('facial boundaries', (15, 17)), ('message passing', (18, 20)), ('robustness', (23, 24)), ('occlusion', (25, 26))]","[['stacked hourglass structure', 'to estimate', 'facial boundary heatmap'], ['stacked hourglass structure', 'model', 'structure'], ['structure', 'between', 'facial boundaries'], ['facial boundaries', 'through', 'message passing'], ['message passing', 'to increase', 'robustness'], ['robustness', 'to', 'occlusion']]",[],"[['Approach', 'used', 'stacked hourglass structure']]",[],face_alignment,5,38
1500,approach,The boundary heatmaps serve as structure cue to guide feature learning for the landmark regressor .,"[('serve as', (3, 5)), ('to guide', (7, 9)), ('for', (11, 12))]","[('boundary heatmaps', (1, 3)), ('structure cue', (5, 7)), ('feature learning', (9, 11)), ('landmark regressor', (13, 15))]","[['boundary heatmaps', 'serve as', 'structure cue'], ['structure cue', 'to guide', 'feature learning'], ['feature learning', 'for', 'landmark regressor']]",[],[],"[['Approach', 'has', 'boundary heatmaps']]",face_alignment,5,40
1501,experimental-setup,All our models are trained with Caffe [ 24 ] on 4 Titan X GPUs .,"[('trained with', (4, 6)), ('on', (10, 11))]","[('All our models', (0, 3)), ('Caffe', (6, 7)), ('4 Titan X GPUs', (11, 15))]","[['All our models', 'trained with', 'Caffe'], ['Caffe', 'on', '4 Titan X GPUs']]",[],[],"[['Experimental setup', 'has', 'All our models']]",face_alignment,5,224
1502,results,Comparison with existing approaches 4.1.1 Evaluation on 300W,"[('on', (6, 7))]","[('300W', (7, 8))]",[],[],"[['Results', 'on', '300W']]",[],face_alignment,5,227
1503,results,Our method performs best among all of the state - of - the - art methods .,"[('performs', (2, 3)), ('among', (4, 5))]","[('Our method', (0, 2)), ('best', (3, 4)), ('all of the state - of - the - art methods', (5, 16))]","[['Our method', 'performs', 'best'], ['best', 'among', 'all of the state - of - the - art methods']]",[],[],"[['Results', 'has', 'Our method']]",face_alignment,5,231
1504,results,Evaluation on WFLW,[],"[('WFLW', (2, 3))]",[],[],[],[],face_alignment,5,234
1505,results,"Though reasonable performance is obtained , there is illustrated to be still a lot of room for improvement for the extreme diversity of samples on WFLW , e.g. , large pose , exaggerated expressions and heavy occlusion .",[],"[('reasonable performance', (1, 3)), ('obtained', (4, 5))]",[],"[['reasonable performance', 'has', 'obtained']]",[],"[['Results', 'has', 'reasonable performance']]",face_alignment,5,237
1506,results,Cross - dataset evaluation on COFW and AFLW,"[('on', (4, 5))]","[('Cross - dataset evaluation', (0, 4)), ('COFW and AFLW', (5, 8))]","[['Cross - dataset evaluation', 'on', 'COFW and AFLW']]",[],[],"[['Results', 'has', 'Cross - dataset evaluation']]",face_alignment,5,238
1507,results,Our model outperforms previous results with a large margin .,"[('outperforms', (2, 3)), ('with', (5, 6))]","[('Our model', (0, 2)), ('previous results', (3, 5)), ('large margin', (7, 9))]","[['Our model', 'outperforms', 'previous results'], ['previous results', 'with', 'large margin']]",[],[],[],face_alignment,5,241
1508,results,We achieve 4.62 % mean error with 2.17 % failure rate .,"[('achieve', (1, 2)), ('with', (6, 7))]","[('4.62 % mean error', (2, 6)), ('2.17 % failure rate', (7, 11))]","[['4.62 % mean error', 'with', '2.17 % failure rate']]",[],[],[],face_alignment,5,242
1509,results,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .","[('by', (6, 7)), ('indicates', (11, 12)), ('of', (14, 15)), ('to handle', (17, 19))]","[('failure rate', (1, 3)), ('significantly reduced', (4, 6)), ('3.75 %', (7, 9)), ('robustness', (13, 14)), ('our method', (15, 17)), ('occlusions', (19, 20))]","[['significantly reduced', 'by', '3.75 %'], ['failure rate', 'indicates', 'robustness'], ['3.75 %', 'indicates', 'robustness'], ['robustness', 'of', 'our method'], ['robustness', 'to handle', 'occlusions'], ['our method', 'to handle', 'occlusions']]","[['failure rate', 'has', 'significantly reduced']]",[],"[['Results', 'has', 'failure rate']]",face_alignment,5,243
1510,results,"Moreover , our method uses boundary information achieves 29 % , 32 % and 29 % relative performance improve- ment over the baseline method ( "" LAB without boundary "" ) on COFW - 29 , AFLW - Full and AFLW - Frontal respectively .","[('uses', (4, 5)), ('achieves', (7, 8)), ('over', (20, 21)), ('on', (31, 32))]","[('our method', (2, 4)), ('boundary information', (5, 7)), ('29 % , 32 % and 29 %', (8, 16)), ('relative performance improve- ment', (16, 20)), ('baseline method ( "" LAB without boundary "" )', (22, 31)), ('COFW - 29', (32, 35)), ('AFLW - Full', (36, 39)), ('AFLW - Frontal', (40, 43))]","[['our method', 'uses', 'boundary information'], ['boundary information', 'achieves', '29 % , 32 % and 29 %'], ['boundary information', 'achieves', 'relative performance improve- ment'], ['relative performance improve- ment', 'over', 'baseline method ( "" LAB without boundary "" )'], ['relative performance improve- ment', 'over', 'AFLW - Frontal'], ['relative performance improve- ment', 'on', 'COFW - 29'], ['baseline method ( "" LAB without boundary "" )', 'on', 'COFW - 29']]","[['29 % , 32 % and 29 %', 'has', 'relative performance improve- ment']]",[],"[['Results', 'has', 'our method']]",face_alignment,5,249
1511,ablation-analysis,"It can be observed easily that boundary map ( "" BM "" ) is the most effective one .","[('observed', (3, 4))]","[('boundary map ( "" BM "" )', (6, 13)), ('most effective one', (15, 18))]",[],"[['boundary map ( "" BM "" )', 'has', 'most effective one']]","[['Ablation analysis', 'observed', 'boundary map ( "" BM "" )']]",[],face_alignment,5,260
1512,ablation-analysis,Boundary information fusion is one of the key steps in our algorithm .,"[('in', (9, 10))]","[('Boundary information fusion', (0, 3)), ('one of the key steps', (4, 9)), ('our algorithm', (10, 12))]","[['one of the key steps', 'in', 'our algorithm']]","[['Boundary information fusion', 'has', 'one of the key steps']]",[],"[['Ablation analysis', 'has', 'Boundary information fusion']]",face_alignment,5,261
1513,ablation-analysis,"As indicated in , our final model that fuses boundary information in all four levels improves mean error from 7.12 % to 6.13 % .","[('in', (2, 3)), ('fuses', (8, 9)), ('improves', (15, 16)), ('from', (18, 19)), ('to', (21, 22))]","[('our final model', (4, 7)), ('boundary information', (9, 11)), ('all four levels', (12, 15)), ('mean error', (16, 18)), ('7.12 %', (19, 21)), ('6.13 %', (22, 24))]","[['boundary information', 'in', 'all four levels'], ['our final model', 'fuses', 'boundary information'], ['boundary information', 'improves', 'mean error'], ['all four levels', 'improves', 'mean error'], ['mean error', 'from', '7.12 %'], ['7.12 %', 'to', '6.13 %']]",[],[],[],face_alignment,5,263
1514,ablation-analysis,It can be observed that performance is improved consistently by fusing boundary heatmaps at more levels .,"[('by fusing', (9, 11)), ('at', (13, 14))]","[('performance', (5, 6)), ('improved consistently', (7, 9)), ('boundary heatmaps', (11, 13)), ('more levels', (14, 16))]","[['improved consistently', 'by fusing', 'boundary heatmaps'], ['boundary heatmaps', 'at', 'more levels']]","[['performance', 'has', 'improved consistently']]",[],"[['Ablation analysis', 'has', 'performance']]",face_alignment,5,265
1515,results,"The comparison between "" BL + HG "" and "" BL + HG/ B "" indicates the effectiveness of boundary information fusion rather than network structure changes .","[('between', (2, 3)), ('indicates', (15, 16)), ('of', (18, 19)), ('rather than', (22, 24))]","[('comparison', (1, 2)), ('"" BL + HG "" and "" BL + HG/ B ""', (3, 15)), ('effectiveness', (17, 18)), ('boundary information fusion', (19, 22)), ('network structure changes', (24, 27))]","[['comparison', 'between', '"" BL + HG "" and "" BL + HG/ B ""'], ['"" BL + HG "" and "" BL + HG/ B ""', 'indicates', 'effectiveness'], ['effectiveness', 'of', 'boundary information fusion'], ['boundary information fusion', 'rather than', 'network structure changes']]","[['comparison', 'has', '"" BL + HG "" and "" BL + HG/ B ""']]",[],"[['Results', 'has', 'comparison']]",face_alignment,5,268
1516,results,"The comparison between "" BL + HG "" and "" BL + CL "" indicates the effectiveness of the using hourglass structure design .","[('indicates', (14, 15)), ('using', (19, 20))]","[('"" BL + HG "" and "" BL + CL ""', (3, 14)), ('effectiveness', (16, 17)), ('hourglass structure design', (20, 23))]","[['"" BL + HG "" and "" BL + CL ""', 'indicates', 'effectiveness'], ['effectiveness', 'using', 'hourglass structure design']]","[['"" BL + HG "" and "" BL + CL ""', 'has', 'effectiveness']]",[],"[['Results', 'has', '"" BL + HG "" and "" BL + CL ""']]",face_alignment,5,269
1517,research-problem,Message passing plays a vital role for heatmap quality improvement when severe occlusions happen .,"[('plays', (2, 3)), ('for', (6, 7)), ('when', (10, 11))]","[('Message passing', (0, 2)), ('vital role', (4, 6)), ('heatmap quality improvement', (7, 10)), ('severe occlusions', (11, 13)), ('happen', (13, 14))]","[['Message passing', 'plays', 'vital role'], ['vital role', 'for', 'heatmap quality improvement'], ['vital role', 'when', 'severe occlusions'], ['heatmap quality improvement', 'when', 'severe occlusions']]","[['Message passing', 'has', 'vital role'], ['severe occlusions', 'has', 'happen']]",[],[],face_alignment,5,270
1518,results,Adversarial learning further improves the quality and effectiveness of boundary heatmaps .,"[('further improves', (2, 4)), ('of', (8, 9))]","[('Adversarial learning', (0, 2)), ('quality and effectiveness', (5, 8)), ('boundary heatmaps', (9, 11))]","[['Adversarial learning', 'further improves', 'quality and effectiveness'], ['quality and effectiveness', 'of', 'boundary heatmaps']]",[],[],"[['Results', 'has', 'Adversarial learning']]",face_alignment,5,272
1519,research-problem,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees,[],"[('Face Alignment', (0, 2))]",[],[],[],[],face_alignment,6,2
1520,model,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .","[('present', (4, 5)), ('based on', (24, 26)), ('of', (33, 34))]","[('3 DDE ( 3D Deeply - initialized Ensemble ) regressor', (6, 16)), ('robust and efficient face alignment algorithm', (18, 24)), ('coarse - to - fine cascade', (27, 33)), ('ERTs', (34, 35))]","[['robust and efficient face alignment algorithm', 'based on', 'coarse - to - fine cascade'], ['coarse - to - fine cascade', 'of', 'ERTs']]","[['3 DDE ( 3D Deeply - initialized Ensemble ) regressor', 'has', 'robust and efficient face alignment algorithm']]","[['Model', 'present', '3 DDE ( 3D Deeply - initialized Ensemble ) regressor']]",[],face_alignment,6,23
1521,model,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,"[('initialized by', (2, 4)), ('to', (11, 12)), ('produced by', (15, 17))]","[('robustly fitting', (4, 6)), ('3 D face model', (7, 11)), ('probability maps', (13, 15)), ('CNN', (18, 19))]","[['3 D face model', 'to', 'probability maps'], ['probability maps', 'produced by', 'CNN']]","[['robustly fitting', 'has', '3 D face model']]","[['Model', 'initialized by', 'robustly fitting']]",[],face_alignment,6,25
1522,model,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .","[('implicitly imposes', (7, 9)), ('on', (13, 14))]","[('ERT', (6, 7)), ('prior face shape', (10, 13)), ('solution', (15, 16))]","[['ERT', 'implicitly imposes', 'prior face shape'], ['prior face shape', 'on', 'solution']]",[],[],"[['Model', 'has', 'ERT']]",face_alignment,6,27
1523,model,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .","[('tackles', (9, 10)), ('of', (13, 14))]","[('coarse - to - fine structure', (3, 9)), ('combinatorial explosion', (11, 13)), ('parts deformation', (14, 16))]","[['coarse - to - fine structure', 'tackles', 'combinatorial explosion'], ['combinatorial explosion', 'of', 'parts deformation']]",[],[],"[['Model', 'has', 'coarse - to - fine structure']]",face_alignment,6,28
1524,model,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,"[('improve', (2, 3)), ('by using', (5, 7)), ('increases', (13, 14)), ('presence of', (18, 20))]","[('initialization', (4, 5)), ('RANSAC - like procedure', (8, 12)), ('robustness', (15, 16)), ('occlusions', (20, 21))]","[['initialization', 'by using', 'RANSAC - like procedure'], ['RANSAC - like procedure', 'increases', 'robustness'], ['robustness', 'presence of', 'occlusions']]",[],"[['Model', 'improve', 'initialization']]",[],face_alignment,6,31
1525,model,We have also introduced early stopping and better data augmentation techniques for increasing the regularization when training both the ERT and the CNN .,"[('introduced', (3, 4)), ('for', (11, 12)), ('when', (15, 16))]","[('early stopping and better data augmentation techniques', (4, 11)), ('increasing', (12, 13)), ('regularization', (14, 15)), ('training', (16, 17)), ('ERT', (19, 20)), ('CNN', (22, 23))]","[['early stopping and better data augmentation techniques', 'for', 'increasing'], ['regularization', 'when', 'training']]","[['increasing', 'has', 'regularization'], ['training', 'has', 'ERT']]","[['Model', 'introduced', 'early stopping and better data augmentation techniques']]",[],face_alignment,6,32
1526,experimental-setup,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .","[('use', (1, 2)), ('with', (5, 6))]","[('Adam stochastic optimization', (2, 5)), ('? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8', (6, 21))]","[['Adam stochastic optimization', 'with', '? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8']]",[],"[['Experimental setup', 'use', 'Adam stochastic optimization']]",[],face_alignment,6,210
1527,experimental-setup,We train until convergence with an initial learning rate ? = 0.001 .,"[('train', (1, 2)), ('until', (2, 3)), ('with', (4, 5))]","[('convergence', (3, 4)), ('initial learning rate', (6, 9)), ('0.001', (11, 12))]","[['convergence', 'with', 'initial learning rate']]","[['initial learning rate', 'has', '0.001']]","[['Experimental setup', 'train', 'convergence']]",[],face_alignment,6,211
1528,experimental-setup,"When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .","[('for', (5, 6)), ('multiply', (10, 11)), ('by', (14, 15))]","[('validation error', (1, 3)), ('levels out', (3, 5)), ('10 epochs', (6, 8)), ('learning rate', (12, 14)), ('decay = 0.05', (15, 18))]","[['levels out', 'for', '10 epochs'], ['validation error', 'multiply', 'learning rate'], ['10 epochs', 'multiply', 'learning rate'], ['learning rate', 'by', 'decay = 0.05']]","[['validation error', 'has', 'levels out']]",[],"[['Experimental setup', 'has', 'validation error']]",face_alignment,6,212
1529,experimental-setup,We apply batch normalization after each convolution .,"[('apply', (1, 2)), ('after', (4, 5))]","[('batch normalization', (2, 4)), ('each convolution', (5, 7))]","[['batch normalization', 'after', 'each convolution']]",[],"[['Experimental setup', 'apply', 'batch normalization']]",[],face_alignment,6,214
1530,experimental-setup,All layers contain 68 filters to describe the required landmark features .,"[('contain', (2, 3)), ('to describe', (5, 7))]","[('All layers', (0, 2)), ('68 filters', (3, 5)), ('required landmark features', (8, 11))]","[['All layers', 'contain', '68 filters'], ['68 filters', 'to describe', 'required landmark features']]","[['All layers', 'has', '68 filters']]",[],"[['Experimental setup', 'has', 'All layers']]",face_alignment,6,215
1531,experimental-setup,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .","[('with', (5, 6)), ('to', (9, 10)), ('to stabilize', (14, 16))]","[('Gaussian filter', (3, 5)), ('? = 33', (6, 9)), ('output probability maps', (11, 14)), ('initialization , g 0', (17, 21))]","[['Gaussian filter', 'with', '? = 33'], ['Gaussian filter', 'to', 'output probability maps'], ['? = 33', 'to', 'output probability maps'], ['output probability maps', 'to stabilize', 'initialization , g 0']]",[],[],[],face_alignment,6,216
1532,experimental-setup,We train the coarse - to - fine ERT with the Gradient Boosting algorithm ) .,"[('with', (9, 10))]","[('coarse - to - fine ERT', (3, 9)), ('Gradient Boosting algorithm', (11, 14))]","[['coarse - to - fine ERT', 'with', 'Gradient Boosting algorithm']]",[],[],[],face_alignment,6,217
1533,experimental-setup,It requires a maximum of T = 20 stages of K = 50 regression trees per stage .,"[('requires', (1, 2)), ('of', (4, 5)), ('of', (9, 10)), ('per', (15, 16))]","[('maximum', (3, 4)), ('T = 20 stages', (5, 9)), ('K = 50 regression trees', (10, 15)), ('stage', (16, 17))]","[['maximum', 'of', 'T = 20 stages'], ['T = 20 stages', 'of', 'K = 50 regression trees'], ['T = 20 stages', 'of', 'K = 50 regression trees'], ['K = 50 regression trees', 'per', 'stage']]",[],"[['Experimental setup', 'requires', 'maximum']]",[],face_alignment,6,218
1534,experimental-setup,The depth of trees is set to 4 .,"[('of', (2, 3)), ('set to', (5, 7))]","[('depth', (1, 2)), ('trees', (3, 4)), ('4', (7, 8))]","[['depth', 'of', 'trees'], ['depth', 'set to', '4'], ['trees', 'set to', '4']]",[],[],"[['Experimental setup', 'has', 'depth']]",face_alignment,6,219
1535,experimental-setup,"The number of tests to choose the best split parameters , ? , is set to 200 .","[('of', (2, 3)), ('to choose', (4, 6)), ('set to', (14, 16))]","[('number', (1, 2)), ('tests', (3, 4)), ('best split parameters', (7, 10)), ('200', (16, 17))]","[['number', 'of', 'tests'], ['tests', 'to choose', 'best split parameters'], ['best split parameters', 'set to', '200']]","[['number', 'has', 'tests']]",[],"[['Experimental setup', 'has', 'number']]",face_alignment,6,220
1536,experimental-setup,We resize each image to set the face size to 160160 pixels .,"[('resize', (1, 2)), ('to set', (4, 6)), ('to', (9, 10))]","[('each image', (2, 4)), ('face size', (7, 9)), ('160160 pixels', (10, 12))]","[['each image', 'to set', 'face size'], ['face size', 'to set', '160160 pixels'], ['face size', 'to', '160160 pixels']]",[],"[['Experimental setup', 'resize', 'each image']]",[],face_alignment,6,221
1537,experimental-setup,We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,"[('generate', (1, 2)), ('in', (6, 7)), ('of', (12, 13))]","[('Z = 25 initializations', (2, 6)), ('robust soft POSIT scheme', (8, 12)), ('g 0', (13, 15))]","[['Z = 25 initializations', 'in', 'robust soft POSIT scheme'], ['robust soft POSIT scheme', 'of', 'g 0']]",[],"[['Experimental setup', 'generate', 'Z = 25 initializations']]",[],face_alignment,6,223
1538,experimental-setup,"We augment the shapes of each face training image to create a set , SA , of at least N A = 60000 samples to train the cascade .","[('augment', (1, 2)), ('of', (4, 5)), ('to create', (9, 11)), ('of', (16, 17)), ('to train', (24, 26))]","[('shapes', (3, 4)), ('each face training image', (5, 9)), ('set , SA', (12, 15)), ('at least N A = 60000 samples', (17, 24)), ('cascade', (27, 28))]","[['shapes', 'of', 'each face training image'], ['set , SA', 'of', 'at least N A = 60000 samples'], ['shapes', 'to create', 'set , SA'], ['each face training image', 'to create', 'set , SA'], ['shapes', 'of', 'each face training image'], ['set , SA', 'of', 'at least N A = 60000 samples'], ['at least N A = 60000 samples', 'to train', 'cascade']]",[],"[['Experimental setup', 'augment', 'shapes']]",[],face_alignment,6,224
1539,experimental-setup,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,"[('To avoid', (0, 2)), ('use', (4, 5))]","[('overfitting', (2, 3)), ('shrinkage factor', (6, 8)), ('0.1', (10, 11)), ('subsampling factor', (12, 14)), ('0.5', (16, 17))]","[['overfitting', 'use', 'shrinkage factor'], ['overfitting', 'use', 'subsampling factor']]","[['shrinkage factor', 'has', '0.1'], ['subsampling factor', 'has', '0.5']]","[['Experimental setup', 'To avoid', 'overfitting']]",[],face_alignment,6,225
1540,experimental-setup,"Training the CNN and the coarse - to - fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20 GHz ( 210 cores / 20 threads , 128 GB of RAM ) with a batch size of 32 images .","[('using', (16, 17)), ('at', (36, 37)), ('with', (51, 52)), ('of', (55, 56))]","[('NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU', (18, 28)), ('dual Intel Xeon Silver 4114 CPU', (30, 36)), ('2.20 GHz', (37, 39)), ('210 cores', (40, 42)), ('20 threads', (43, 45)), ('128 GB of RAM', (46, 50)), ('batch size', (53, 55)), ('32 images', (56, 58))]","[['dual Intel Xeon Silver 4114 CPU', 'at', '2.20 GHz'], ['2.20 GHz', 'with', 'batch size'], ['batch size', 'of', '32 images']]",[],"[['Experimental setup', 'using', 'NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU']]",[],face_alignment,6,227
1541,results,"Overall , 3 DDE is better than any other providing a public implementation in the literature .","[('than', (6, 7))]","[('DDE', (3, 4)), ('better', (5, 6)), ('any other providing a public implementation in the literature', (7, 16))]","[['better', 'than', 'any other providing a public implementation in the literature']]","[['DDE', 'has', 'better']]",[],[],face_alignment,6,237
1542,results,"In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .","[('able to', (4, 6)), ('by', (7, 8)), ('as', (14, 15))]","[('improve', (6, 7)), ('large margin', (9, 11)), ('other ERT methods', (11, 14)), ('RCPR , ERT or c GPRT', (15, 21))]","[['improve', 'by', 'large margin'], ['other ERT methods', 'as', 'RCPR , ERT or c GPRT']]","[['improve', 'has', 'large margin'], ['large margin', 'has', 'other ERT methods'], ['other ERT methods', 'name', 'RCPR , ERT or c GPRT']]","[['Results', 'able to', 'improve']]",[],face_alignment,6,239
1543,results,"We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .","[('outperform', (2, 3))]","[('RCN', (3, 4))]",[],[],"[['Results', 'outperform', 'RCN']]",[],face_alignment,6,240
1544,results,"Even DAN and LAB , that implement a cascade of CNN regressors , can not compete with the regularization obtained by using the cascade of ERT in 3 DDE ( see ) .","[('can not compete with', (13, 17)), ('using', (21, 22))]","[('DAN and LAB', (1, 4)), ('regularization', (18, 19)), ('cascade of ERT', (23, 26))]","[['regularization', 'using', 'cascade of ERT']]",[],[],[],face_alignment,6,241
1545,results,Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition ( see ) and in the full subset of the 300W public test set ( see ) .,"[('obtains', (2, 3)), ('in', (7, 8)), ('of', (13, 14)), ('of', (25, 26))]","[('best overall performance', (4, 7)), ('indoor and outdoor subsets', (9, 13)), ('private competition', (15, 17)), ('full subset', (23, 25)), ('300W public test set', (27, 31))]","[['best overall performance', 'in', 'indoor and outdoor subsets'], ['best overall performance', 'in', 'full subset'], ['indoor and outdoor subsets', 'of', 'private competition'], ['full subset', 'of', '300W public test set'], ['full subset', 'of', '300W public test set']]",[],"[['Results', 'obtains', 'best overall performance']]",[],face_alignment,6,247
1546,results,"In the challenging subset of the 300W public competition , SHN gets better results than 3DDE .","[('In', (0, 1)), ('of', (4, 5)), ('gets', (11, 12)), ('than', (14, 15))]","[('challenging subset', (2, 4)), ('300W public competition', (6, 9)), ('SHN', (10, 11)), ('better results', (12, 14)), ('3DDE', (15, 16))]","[['challenging subset', 'of', '300W public competition'], ['SHN', 'gets', 'better results'], ['better results', 'than', '3DDE']]","[['challenging subset', 'has', '300W public competition'], ['300W public competition', 'has', 'SHN']]","[['Results', 'In', 'challenging subset']]",[],face_alignment,6,249
1547,ablation-analysis,"When combined with the cascaded ERT , the 3D initialization is key to achieve top overall performance , see CNN + MS + DE vs CNN + 3D + DE in the full subset .","[('When combined with', (0, 3)), ('key to achieve', (11, 14))]","[('cascaded ERT', (4, 6)), ('3D initialization', (8, 10)), ('top overall performance', (14, 17))]","[['3D initialization', 'key to achieve', 'top overall performance']]","[['cascaded ERT', 'has', '3D initialization']]","[['Ablation analysis', 'When combined with', 'cascaded ERT']]",[],face_alignment,6,296
1548,ablation-analysis,"Of course , the 3D initialization is fundamental to achieve good performance in presence of large face rotations .","[('achieve', (9, 10)), ('in presence of', (12, 15))]","[('3D initialization', (4, 6)), ('good performance', (10, 12)), ('large face rotations', (15, 18))]","[['3D initialization', 'achieve', 'good performance'], ['good performance', 'in presence of', 'large face rotations']]",[],[],"[['Ablation analysis', 'has', '3D initialization']]",face_alignment,6,299
1549,ablation-analysis,"The large receptive fields of CNNs are specially helpful in challenging situations , specifically those in the pose and occlusion subsets .","[('in', (9, 10))]","[('large receptive fields of CNNs', (1, 6)), ('specially helpful', (7, 9)), ('challenging situations', (10, 12)), ('pose and occlusion subsets', (17, 21))]","[['specially helpful', 'in', 'challenging situations']]","[['large receptive fields of CNNs', 'has', 'specially helpful']]",[],"[['Ablation analysis', 'has', 'large receptive fields of CNNs']]",face_alignment,6,302
1550,ablation-analysis,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .","[('in', (7, 8)), ('provides', (11, 12)), ('in', (15, 16))]","[('coarse - to - fine strategy', (1, 7)), ('our cascaded ERT', (8, 11)), ('significative local improvements', (12, 15)), ('difficult cases', (16, 18))]","[['coarse - to - fine strategy', 'in', 'our cascaded ERT'], ['significative local improvements', 'in', 'difficult cases'], ['coarse - to - fine strategy', 'provides', 'significative local improvements'], ['our cascaded ERT', 'provides', 'significative local improvements'], ['significative local improvements', 'in', 'difficult cases']]",[],[],"[['Ablation analysis', 'has', 'coarse - to - fine strategy']]",face_alignment,6,303
1551,research-problem,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,[],"[('Joint 3D Face Reconstruction and Dense Alignment', (0, 7))]",[],[],[],[],face_alignment,7,2
1552,code,Code is available at https://github.com/YadiraF/PRNet .,[],"[('https://github.com/YadiraF/PRNet', (4, 5))]",[],[],[],[],face_alignment,7,10
1553,research-problem,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .,[],"[('3 D face reconstruction', (0, 4)), ('face alignment', (5, 7))]",[],[],[],[],face_alignment,7,12
1554,model,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .","[('propose', (5, 6)), ('called', (13, 14)), ('to jointly predict', (21, 24)), ('reconstruct', (27, 28))]","[('end - to - end method', (7, 13)), ('Position map Regression Network ( PRN )', (14, 21)), ('dense alignment', (24, 26)), ('3 D face shape', (28, 32))]","[['end - to - end method', 'called', 'Position map Regression Network ( PRN )'], ['Position map Regression Network ( PRN )', 'to jointly predict', 'dense alignment'], ['Position map Regression Network ( PRN )', 'reconstruct', '3 D face shape']]","[['end - to - end method', 'name', 'Position map Regression Network ( PRN )']]","[['Model', 'propose', 'end - to - end method']]",[],face_alignment,7,27
1555,model,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .","[('design', (3, 4)), ('which is', (9, 11)), ('recording', (14, 15)), ('of', (18, 19)), ('at', (26, 27)), ('keeping', (30, 31))]","[('UV position map', (5, 8)), ('2D image', (12, 14)), ('3D coordinates', (16, 18)), ('complete facial point cloud', (20, 24)), ('semantic meaning', (32, 34)), ('each UV place', (35, 38))]","[['UV position map', 'which is', '2D image'], ['2D image', 'recording', '3D coordinates'], ['3D coordinates', 'of', 'complete facial point cloud'], ['semantic meaning', 'at', 'each UV place'], ['UV position map', 'keeping', 'semantic meaning']]","[['UV position map', 'has', '2D image']]","[['Model', 'design', 'UV position map']]",[],face_alignment,7,31
1556,model,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,"[('train', (2, 3)), ('with', (9, 10)), ('on', (16, 17)), ('to', (19, 20)), ('regress', (20, 21)), ('from', (25, 26))]","[('simple encoder - decoder network', (4, 9)), ('weighted loss', (11, 13)), ('focuses', (14, 15)), ('more', (15, 16)), ('discriminative region', (17, 19)), ('UV position map', (22, 25)), ('single 2 D facial image', (27, 32))]","[['simple encoder - decoder network', 'with', 'weighted loss'], ['focuses', 'on', 'discriminative region'], ['more', 'on', 'discriminative region'], ['discriminative region', 'regress', 'UV position map'], ['UV position map', 'from', 'single 2 D facial image']]","[['weighted loss', 'has', 'focuses'], ['focuses', 'has', 'more']]","[['Model', 'train', 'simple encoder - decoder network']]",[],face_alignment,7,32
1557,experimental-setup,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .","[('For', (0, 1)), ('use', (4, 5)), ('with', (7, 8)), ('begins at', (11, 13)), ('after', (17, 18))]","[('optimization', (1, 2)), ('Adam optimizer', (5, 7)), ('learning rate', (9, 11)), ('0.0001', (13, 14)), ('decays', (15, 16)), ('half', (16, 17)), ('each 5 epochs', (18, 21))]","[['optimization', 'use', 'Adam optimizer'], ['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'begins at', '0.0001'], ['half', 'after', 'each 5 epochs']]","[['decays', 'has', 'half']]","[['Experimental setup', 'For', 'optimization']]",[],face_alignment,7,146
1558,experimental-setup,The batch size is set as 16 .,"[('set as', (4, 6))]","[('batch size', (1, 3)), ('16', (6, 7))]","[['batch size', 'set as', '16']]","[['batch size', 'has', '16']]",[],"[['Experimental setup', 'has', 'batch size']]",face_alignment,7,147
1559,experimental-setup,All of our training codes are implemented with TensorFlow .,"[('implemented with', (6, 8))]","[('All of our training codes', (0, 5)), ('TensorFlow', (8, 9))]","[['All of our training codes', 'implemented with', 'TensorFlow']]",[],[],"[['Experimental setup', 'has', 'All of our training codes']]",face_alignment,7,148
1560,results,3D Face Alignment,[],"[('3D Face Alignment', (0, 3))]",[],[],[],"[['Results', 'has', '3D Face Alignment']]",face_alignment,7,168
1561,results,"As shown in figure 5 , our result slightly outperforms the state - of - the - art method 3D - FAN when calculating per distance with 2D coordinates .","[('when', (22, 23)), ('per', (24, 25)), ('with', (26, 27))]","[('our result', (6, 8)), ('slightly outperforms', (8, 10)), ('state - of - the - art method 3D - FAN', (11, 22)), ('calculating', (23, 24)), ('distance', (25, 26)), ('2D coordinates', (27, 29))]","[['state - of - the - art method 3D - FAN', 'when', 'calculating'], ['calculating', 'per', 'distance'], ['distance', 'with', '2D coordinates']]","[['our result', 'has', 'slightly outperforms'], ['slightly outperforms', 'has', 'state - of - the - art method 3D - FAN'], ['calculating', 'has', 'distance']]",[],"[['Results', 'has', 'our result']]",face_alignment,7,172
1562,results,"When considering the depth value , the performance discrepancy between our method and 3D - FAN increases .","[('When considering', (0, 2)), ('between', (9, 10)), ('and', (12, 13))]","[('depth value', (3, 5)), ('performance discrepancy', (7, 9)), ('our method', (10, 12)), ('3D - FAN', (13, 16)), ('increases', (16, 17))]","[['performance discrepancy', 'between', 'our method'], ['performance discrepancy', 'and', '3D - FAN']]","[['depth value', 'has', 'performance discrepancy'], ['3D - FAN', 'has', 'increases']]","[['Results', 'When considering', 'depth value']]",[],face_alignment,7,173
1563,results,The result shows that our method is robust to the change of pose and datasets .,"[('to', (8, 9)), ('of', (11, 12))]","[('our method', (4, 6)), ('robust', (7, 8)), ('change', (10, 11)), ('pose and datasets', (12, 15))]","[['robust', 'to', 'change'], ['change', 'of', 'pose and datasets']]","[['our method', 'has', 'robust']]",[],"[['Results', 'has', 'our method']]",face_alignment,7,178
1564,results,Examples from AFLW2000 - 3 D dataset show that our predictions are more accurate than ground truth in some cases .,"[('from', (1, 2)), ('show', (7, 8)), ('than', (14, 15))]","[('Examples', (0, 1)), ('AFLW2000 - 3 D dataset', (2, 7)), ('our predictions', (9, 11)), ('more accurate', (12, 14)), ('ground truth', (15, 17))]","[['Examples', 'from', 'AFLW2000 - 3 D dataset'], ['AFLW2000 - 3 D dataset', 'show', 'our predictions'], ['more accurate', 'than', 'ground truth']]","[['AFLW2000 - 3 D dataset', 'has', 'our predictions'], ['our predictions', 'has', 'more accurate']]",[],[],face_alignment,7,184
1565,results,"As shown in figure 7 , our method outperforms the best methods with a large margin of more than 27 % on both 2 D and 3D coordinates . : CED curves on AFLW2000 - 3D .","[('outperforms', (8, 9)), ('with', (12, 13)), ('of', (16, 17)), ('on', (21, 22))]","[('best methods', (10, 12)), ('large margin', (14, 16)), ('more than 27 %', (17, 21)), ('both 2 D and 3D coordinates', (22, 28))]","[['best methods', 'with', 'large margin'], ['large margin', 'of', 'more than 27 %'], ['more than 27 %', 'on', 'both 2 D and 3D coordinates']]",[],[],[],face_alignment,7,189
1566,ablation-analysis,Network trained without using weight mask has worst performance compared with other two settings .,"[('without using', (2, 4)), ('compared with', (9, 11))]","[('Network', (0, 1)), ('trained', (1, 2)), ('weight mask', (4, 6)), ('worst performance', (7, 9)), ('other two settings', (11, 14))]","[['Network', 'without using', 'weight mask'], ['trained', 'without using', 'weight mask'], ['worst performance', 'compared with', 'other two settings']]","[['Network', 'has', 'trained']]",[],"[['Ablation analysis', 'has', 'Network']]",face_alignment,7,207
1567,ablation-analysis,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .","[('adding', (1, 2)), ('to', (3, 4)), ('such as', (6, 8)), ('shows', (19, 20)), ('on', (22, 23)), ('over', (26, 27))]","[('weights', (2, 3)), ('specific regions', (4, 6)), ('68 facial landmarks or central face region', (8, 15)), ('weight ratio', (16, 18)), ('considerable improvement', (20, 22)), ('68 points datasets', (23, 26)), ('weight ratio', (27, 29))]","[['weights', 'to', 'specific regions'], ['specific regions', 'such as', '68 facial landmarks or central face region'], ['specific regions', 'such as', 'weight ratio'], ['weight ratio', 'shows', 'considerable improvement'], ['considerable improvement', 'on', '68 points datasets'], ['68 points datasets', 'over', 'weight ratio']]","[['weights', 'has', 'specific regions'], ['weight ratio', 'has', 'considerable improvement']]","[['Ablation analysis', 'adding', 'weights']]",[],face_alignment,7,208
1568,research-problem,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning,[],"[('Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image', (0, 12))]",[],[],[],[],face_alignment,8,2
1569,research-problem,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,[],"[('3 D face reconstruction from a single 2D image', (0, 9))]",[],[],[],[],face_alignment,8,9
1570,research-problem,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,[],"[('3 D face reconstruction from a single 2D image', (0, 9))]",[],[],[],[],face_alignment,8,23
1571,research-problem,3 D face reconstruction is an important task in the field of computer vision and graphics .,[],"[('3 D face reconstruction', (0, 4))]",[],[],[],[],face_alignment,8,31
1572,model,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .","[('to overcome', (2, 4)), ('of', (7, 8)), ('propose', (16, 17)), ('leverages', (22, 23)), ('supervise and facilitate', (35, 38))]","[('intrinsic limitation', (5, 7)), ('existing 3 D face recovery models', (8, 14)), ('novel learning method', (18, 21)), ('2D "" in - the - wild "" face images', (23, 33)), ('3D face model learning', (39, 43))]","[['intrinsic limitation', 'of', 'existing 3 D face recovery models'], ['existing 3 D face recovery models', 'propose', 'novel learning method'], ['novel learning method', 'leverages', '2D "" in - the - wild "" face images']]","[['intrinsic limitation', 'has', 'existing 3 D face recovery models']]","[['Model', 'to overcome', 'intrinsic limitation']]",[],face_alignment,8,41
1573,model,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,"[('design', (1, 2)), ('to train', (12, 14)), ('with', (19, 20)), ('from', (22, 23))]","[('novel self - supervised learning method', (3, 9)), ('3 D face model', (15, 19)), ('weak supervision', (20, 22)), ('2D images', (23, 25))]","[['novel self - supervised learning method', 'to train', '3 D face model'], ['3 D face model', 'with', 'weak supervision'], ['weak supervision', 'from', '2D images']]",[],"[['Model', 'design', 'novel self - supervised learning method']]",[],face_alignment,8,45
1574,model,The model should be able to recover 2D landmarks from predicted 3D ones via direct 3D - to - 2D projection .,"[('recover', (6, 7)), ('from', (9, 10)), ('via', (13, 14))]","[('model', (1, 2)), ('2D landmarks', (7, 9)), ('predicted 3D ones', (10, 13)), ('direct 3D - to - 2D projection', (14, 21))]","[['model', 'recover', '2D landmarks'], ['2D landmarks', 'from', 'predicted 3D ones'], ['predicted 3D ones', 'via', 'direct 3D - to - 2D projection']]",[],[],"[['Model', 'has', 'model']]",face_alignment,8,47
1575,model,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .","[('exploits', (6, 7)), ('over', (10, 11))]","[('our proposed method', (2, 5)), ('cycle - consistency', (7, 10)), ('2D landmark predictions', (12, 15))]","[['our proposed method', 'exploits', 'cycle - consistency'], ['cycle - consistency', 'over', '2D landmark predictions']]",[],[],"[['Model', 'has', 'our proposed method']]",face_alignment,8,49
1576,model,"To facilitate the overall learning procedure , our method also exploits self - critic learning .","[('To facilitate', (0, 2)), ('exploits', (10, 11))]","[('overall learning procedure', (3, 6)), ('our method', (7, 9)), ('self - critic learning', (11, 15))]","[['our method', 'exploits', 'self - critic learning']]","[['overall learning procedure', 'has', 'our method']]","[['Model', 'To facilitate', 'overall learning procedure']]",[],face_alignment,8,51
1577,experimental-setup,Our proposed 2 DASL is implemented with Pytorch .,"[('implemented with', (5, 7))]","[('Our proposed 2 DASL', (0, 4)), ('Pytorch', (7, 8))]","[['Our proposed 2 DASL', 'implemented with', 'Pytorch']]",[],[],"[['Experimental setup', 'has', 'Our proposed 2 DASL']]",face_alignment,8,205
1578,experimental-setup,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .","[('for', (4, 5)), ('with', (8, 9)), ('beginning at', (12, 14)), ('uses', (23, 24)), ('as', (26, 27)), ('with', (28, 29))]","[('SGD optimizer', (2, 4)), ('CNN regressor', (6, 8)), ('learning rate', (10, 12)), ('5 10 ?5', (14, 17)), ('decays exponentially', (18, 20)), ('discriminator', (22, 23)), ('Adam', (25, 26)), ('optimizer', (27, 28)), ('fixed learning rate', (30, 33)), ('1 10 ?4', (33, 36))]","[['SGD optimizer', 'for', 'CNN regressor'], ['CNN regressor', 'with', 'learning rate'], ['CNN regressor', 'with', 'decays exponentially'], ['optimizer', 'with', 'fixed learning rate'], ['learning rate', 'beginning at', '5 10 ?5'], ['discriminator', 'uses', 'Adam'], ['Adam', 'as', 'optimizer'], ['Adam', 'with', 'fixed learning rate'], ['optimizer', 'with', 'fixed learning rate']]","[['fixed learning rate', 'has', '1 10 ?4']]",[],[],face_alignment,8,206
1579,experimental-setup,We use a two - stage strategy to train our model .,"[('use', (1, 2)), ('to train', (7, 9))]","[('two - stage strategy', (3, 7)), ('our model', (9, 11))]","[['two - stage strategy', 'to train', 'our model']]",[],"[['Experimental setup', 'use', 'two - stage strategy']]",[],face_alignment,8,209
1580,experimental-setup,"In the first stage , we train the model using the overall loss L .","[('In', (0, 1)), ('train', (6, 7)), ('using', (9, 10))]","[('first stage', (2, 4)), ('model', (8, 9)), ('overall loss L', (11, 14))]","[['first stage', 'train', 'model'], ['model', 'using', 'overall loss L']]",[],"[['Experimental setup', 'In', 'first stage']]",[],face_alignment,8,210
1581,experimental-setup,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .","[('fine - tune', (6, 9)), ('using', (11, 12))]","[('second stage', (2, 4)), ('our model', (9, 11)), ('Vertex Distance Cost', (13, 16))]","[['second stage', 'fine - tune', 'our model'], ['our model', 'using', 'Vertex Distance Cost']]",[],[],[],face_alignment,8,211
1582,experiments,Dense face alignment,[],"[('Dense face alignment', (0, 3))]",[],[],[],[],face_alignment,8,227
1583,experiments,"The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .","[('achieves', (13, 14)), ('on the evaluation of', (20, 24)), ('among', (30, 31))]","[('our 2 DASL', (10, 13)), ('lowest NME', (15, 17)), ('2 D and 3D coordinates', (25, 30)), ('all the methods', (31, 34))]","[['our 2 DASL', 'achieves', 'lowest NME'], ['lowest NME', 'on the evaluation of', '2 D and 3D coordinates'], ['2 D and 3D coordinates', 'among', 'all the methods']]",[],[],[],face_alignment,8,239
1584,experiments,"For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .","[('For', (0, 1)), ('by', (16, 17)), ('on', (20, 21))]","[('3 DMM - based methods', (1, 6)), ('3 DDFA and DeFA', (7, 11)), ('our method', (12, 14)), ('outperforms', (14, 15)), ('large margin', (18, 20)), ('68 spare landmarks', (23, 26)), ('dense coordinates', (28, 30))]","[['outperforms', 'by', 'large margin'], ['large margin', 'on', '68 spare landmarks'], ['large margin', 'on', 'dense coordinates']]","[['3 DMM - based methods', 'name', '3 DDFA and DeFA'], ['our method', 'has', 'outperforms'], ['outperforms', 'has', 'large margin']]",[],[],face_alignment,8,240
1585,results,"As can be observed , our method achieves the lowest mean NME on both of the two datasets , and the lowest NME across all poses on AFLW2000 - 3D .","[('achieves', (7, 8)), ('on', (12, 13)), ('across', (23, 24)), ('on', (26, 27))]","[('our method', (5, 7)), ('lowest mean NME', (9, 12)), ('both of the two datasets', (13, 18)), ('lowest NME', (21, 23)), ('all poses', (24, 26)), ('AFLW2000 - 3D', (27, 30))]","[['our method', 'achieves', 'lowest mean NME'], ['our method', 'achieves', 'lowest NME'], ['lowest mean NME', 'on', 'both of the two datasets'], ['all poses', 'on', 'AFLW2000 - 3D'], ['lowest NME', 'across', 'all poses'], ['all poses', 'on', 'AFLW2000 - 3D']]",[],[],"[['Results', 'has', 'our method']]",face_alignment,8,245
1586,results,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .","[('performs', (3, 4)), ('than', (5, 6)), ('reducing', (8, 9)), ('by', (10, 11)), ('on', (14, 15))]","[('Our 2DASL', (0, 2)), ('better', (4, 5)), ('PRNet', (6, 7)), ('NME', (9, 10)), ('0.09 and 0.08', (11, 14)), ('AFLW2000 - 3D and AFLW - LFPA', (15, 22))]","[['Our 2DASL', 'performs', 'better'], ['better', 'than', 'PRNet'], ['Our 2DASL', 'reducing', 'NME'], ['NME', 'by', '0.09 and 0.08'], ['0.09 and 0.08', 'on', 'AFLW2000 - 3D and AFLW - LFPA']]",[],[],[],face_alignment,8,246
1587,results,"Es - pecially on large poses ( from 60 to 90 ) , 2 DASL achieves 0.2 lower NME than PRNet .","[('on', (3, 4)), ('achieves', (15, 16)), ('than', (19, 20))]","[('large poses ( from 60 to 90 )', (4, 12)), ('2 DASL', (13, 15)), ('0.2 lower NME', (16, 19)), ('PRNet', (20, 21))]","[['2 DASL', 'achieves', '0.2 lower NME'], ['0.2 lower NME', 'than', 'PRNet']]","[['large poses ( from 60 to 90 )', 'has', '2 DASL']]",[],[],face_alignment,8,247
1588,experiments,3 D face reconstruction,[],"[('3 D face reconstruction', (0, 4))]",[],[],[],[],face_alignment,8,249
1589,results,"As can be seen , the 3D reconstruction results of 2 DASL outperforms 3 DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .","[('of', (9, 10)), ('outperforms', (12, 13)), ('by', (15, 16)), ('for', (20, 21))]","[('3D reconstruction results', (6, 9)), ('2 DASL', (10, 12)), ('3 DDFA', (13, 15)), ('0.39', (16, 17)), ('2.29', (19, 20)), ('DeFA', (21, 22))]","[['3D reconstruction results', 'of', '2 DASL'], ['3D reconstruction results', 'of', '2.29'], ['2 DASL', 'outperforms', '3 DDFA'], ['2 DASL', 'outperforms', '2.29'], ['3 DDFA', 'by', '0.39'], ['2.29', 'for', 'DeFA']]",[],[],[],face_alignment,8,255
1590,ablation-analysis,"2 . Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages , respectively .","[('Adding', (2, 3)), ('to', (4, 5)), ('of', (7, 8)), ('reduces', (11, 12)), ('by', (14, 15)), ('on', (18, 19))]","[('weights', (3, 4)), ('central points', (5, 7)), ('facial landmarks', (9, 11)), ('NME', (13, 14)), ('0.09 to 0.23', (15, 18)), ('two stages', (20, 22))]","[['weights', 'to', 'central points'], ['central points', 'of', 'facial landmarks'], ['facial landmarks', 'reduces', 'NME'], ['NME', 'by', '0.09 to 0.23'], ['0.09 to 0.23', 'on', 'two stages']]",[],"[['Ablation analysis', 'Adding', 'weights']]",[],face_alignment,8,265
1591,ablation-analysis,"If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .","[('not used', (7, 9)), ('by', (13, 14)), ('for', (15, 16))]","[('self - critic learning', (2, 6)), ('NME', (11, 12)), ('increases', (12, 13)), ('0.04/0.18', (14, 15)), ('with / without weight mask', (16, 21))]","[['self - critic learning', 'not used', 'NME'], ['increases', 'by', '0.04/0.18'], ['0.04/0.18', 'for', 'with / without weight mask']]","[['self - critic learning', 'has', 'NME'], ['NME', 'has', 'increases']]",[],"[['Ablation analysis', 'has', 'self - critic learning']]",face_alignment,8,267
1592,ablation-analysis,"While the self - supervision scheme reduce NME by 0.1 when the weight mask is used , and 0.23 if the weight mask is removed , no significant improvement is observed .","[('reduce', (6, 7)), ('by', (8, 9)), ('when', (10, 11)), ('used', (15, 16)), ('if', (19, 20))]","[('self - supervision scheme', (2, 6)), ('NME', (7, 8)), ('0.1', (9, 10)), ('weight mask', (12, 14)), ('0.23', (18, 19)), ('weight mask', (21, 23)), ('removed', (24, 25))]","[['self - supervision scheme', 'reduce', 'NME'], ['NME', 'by', '0.1'], ['0.1', 'when', 'weight mask'], ['0.1', 'when', '0.23'], ['weight mask', 'used', '0.23'], ['0.23', 'if', 'weight mask']]","[['self - supervision scheme', 'has', 'NME']]",[],"[['Ablation analysis', 'has', 'self - supervision scheme']]",face_alignment,8,268
1593,ablation-analysis,"Moreover , in our experiments , we found taking the FLMs as input can accelerate the convergence of training process .","[('found', (7, 8)), ('as', (11, 12)), ('accelerate', (14, 15)), ('of', (17, 18))]","[('FLMs', (10, 11)), ('input', (12, 13)), ('convergence', (16, 17)), ('training process', (18, 20))]","[['FLMs', 'as', 'input'], ['FLMs', 'accelerate', 'convergence'], ['input', 'accelerate', 'convergence'], ['convergence', 'of', 'training process']]","[['FLMs', 'has', 'input']]","[['Ablation analysis', 'found', 'FLMs']]",[],face_alignment,8,270
1594,research-problem,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection,[],"[('Facial Landmark Detection', (10, 13))]",[],[],[],[],face_alignment,9,2
1595,model,"In this paper , we propose a novel Semantic Alignment method which reduces the ' semantic ambiguity ' intrinsi-cally .","[('propose', (5, 6)), ('reduces', (12, 13))]","[('novel Semantic Alignment method', (7, 11)), (""' semantic ambiguity ' intrinsi-cally"", (14, 19))]","[['novel Semantic Alignment method', 'reduces', ""' semantic ambiguity ' intrinsi-cally""]]",[],"[['Model', 'propose', 'novel Semantic Alignment method']]",[],face_alignment,9,30
1596,model,"We model the ' real ' ground - truth as a latent variable to optimize , and the optimized ' real ' ground - truth then supervises the landmark detection network training .","[('model', (1, 2)), ('as', (9, 10)), ('to', (13, 14)), ('supervises', (26, 27))]","[(""' real ' ground - truth"", (3, 9)), ('latent variable', (11, 13)), ('optimize', (14, 15)), (""optimized ' real ' ground - truth"", (18, 25)), ('landmark detection network training', (28, 32))]","[[""' real ' ground - truth"", 'as', 'latent variable'], ['latent variable', 'to', 'optimize'], [""optimized ' real ' ground - truth"", 'supervises', 'landmark detection network training']]",[],"[['Model', 'model', ""' real ' ground - truth""]]",[],face_alignment,9,32
1597,model,"Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .","[('search', (10, 11)), ('train', (19, 20)), ('in', (24, 25))]","[('probabilistic model', (5, 7)), (""' real ' ground - truth"", (12, 18)), ('landmark detection network', (21, 24)), ('end - to - end way', (26, 32))]","[['probabilistic model', 'search', ""' real ' ground - truth""], ['probabilistic model', 'train', 'landmark detection network'], ['landmark detection network', 'in', 'end - to - end way']]",[],[],[],face_alignment,9,33
1598,model,"In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .","[('constrain', (10, 11)), ('to be close to', (14, 18)), ('of', (20, 21))]","[('probabilistic model', (2, 4)), ('prior model', (6, 8)), ('latent variable', (12, 14)), ('observations', (19, 20)), (""' real ' ground truth"", (22, 27))]","[['prior model', 'constrain', 'latent variable'], ['latent variable', 'to be close to', 'observations'], ['observations', 'of', ""' real ' ground truth""]]","[['probabilistic model', 'has', 'prior model']]",[],"[['Model', 'has', 'probabilistic model']]",face_alignment,9,34
1599,model,The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .,"[('reduce', (5, 6)), ('between', (10, 11)), ('of', (17, 18))]","[('likelihood model', (1, 3)), ('Pearson Chi-square distance', (7, 10)), ('expected and the predicted distributions', (12, 17)), (""' real ' ground - truth"", (18, 24))]","[['likelihood model', 'reduce', 'Pearson Chi-square distance'], ['Pearson Chi-square distance', 'between', 'expected and the predicted distributions'], ['expected and the predicted distributions', 'of', ""' real ' ground - truth""]]",[],[],"[['Model', 'has', 'likelihood model']]",face_alignment,9,35
1600,model,The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .,"[('generated by', (2, 4)), ('represents', (7, 8)), ('of', (10, 11)), ('used to model', (18, 21)), ('of', (24, 25))]","[('heatmap', (1, 2)), ('hourglass architecture', (5, 7)), ('confidence', (9, 10)), ('each pixel', (11, 13)), ('confidence distribution', (15, 17)), ('predicted distribution', (22, 24)), ('likelihood', (25, 26))]","[['heatmap', 'generated by', 'hourglass architecture'], ['hourglass architecture', 'represents', 'confidence'], ['confidence', 'of', 'each pixel'], ['predicted distribution', 'of', 'likelihood'], ['confidence distribution', 'used to model', 'predicted distribution'], ['predicted distribution', 'of', 'likelihood']]",[],[],"[['Model', 'has', 'heatmap']]",face_alignment,9,36
1601,model,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .","[('maintains', (19, 20)), ('recovers', (26, 27)), ('caused by', (31, 33)), ('such as', (35, 37)), ('of', (41, 42))]","[('global heatmap correction unit ( GHCU )', (11, 18)), ('global face shape constraint', (21, 25)), ('unconfidently predicted landmarks', (28, 31)), ('challenging factors', (33, 35)), ('occlusions', (37, 38)), ('low resolution', (39, 41)), ('images', (42, 43))]","[['global heatmap correction unit ( GHCU )', 'maintains', 'global face shape constraint'], ['global heatmap correction unit ( GHCU )', 'recovers', 'unconfidently predicted landmarks'], ['unconfidently predicted landmarks', 'caused by', 'challenging factors'], ['challenging factors', 'such as', 'occlusions'], ['low resolution', 'of', 'images']]",[],[],[],face_alignment,9,37
1602,experimental-setup,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .","[('To perform', (0, 2)), ('randomly sample', (6, 8)), ('from', (17, 18))]","[('data augmentation', (2, 4)), ('angle of rotation', (9, 12)), ('bounding box scale', (14, 17)), ('Gaussian distribution', (18, 20))]","[['data augmentation', 'randomly sample', 'angle of rotation'], ['data augmentation', 'randomly sample', 'bounding box scale'], ['bounding box scale', 'from', 'Gaussian distribution']]",[],"[['Experimental setup', 'To perform', 'data augmentation']]",[],face_alignment,9,201
1603,experimental-setup,We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,"[('use', (1, 2)), ('as', (9, 10)), ('trained by', (14, 16))]","[('four - stage stacked hourglass network', (3, 9)), ('backbone', (11, 12)), ('optimizer RMSprop', (17, 19))]","[['four - stage stacked hourglass network', 'as', 'backbone'], ['backbone', 'trained by', 'optimizer RMSprop']]",[],"[['Experimental setup', 'use', 'four - stage stacked hourglass network']]",[],face_alignment,9,202
1604,experimental-setup,"When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .","[('When', (0, 1)), ('with', (6, 7)), ('decayed', (20, 21)), ('to', (21, 22)), ('after', (26, 27))]","[('training', (1, 2)), ('roughly converged model', (3, 6)), ('human annotations', (7, 9)), ('initial learning rate', (11, 14)), ('2.5 10 ?4', (15, 18)), ('2.5 10 ? 6', (22, 26)), ('120 epochs', (27, 29))]","[['roughly converged model', 'with', 'human annotations'], ['initial learning rate', 'decayed', '2.5 10 ? 6'], ['2.5 10 ?4', 'decayed', '2.5 10 ? 6'], ['2.5 10 ? 6', 'after', '120 epochs']]","[['training', 'has', 'roughly converged model'], ['initial learning rate', 'has', '2.5 10 ?4']]","[['Experimental setup', 'When', 'training']]",[],face_alignment,9,207
1605,experimental-setup,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .","[('with', (2, 3)), ('divided by', (26, 28)), ('at', (33, 34))]","[('Semantic Alignment', (3, 5)), ('initial learning rate', (16, 19)), ('2.5 10 ? 6', (20, 24)), ('5 , 2 and 2', (28, 33)), ('epoch 30 , 60 and 90', (34, 40))]","[['initial learning rate', 'divided by', '5 , 2 and 2'], ['5 , 2 and 2', 'at', 'epoch 30 , 60 and 90']]","[['initial learning rate', 'has', '2.5 10 ? 6']]","[['Experimental setup', 'with', 'Semantic Alignment']]",[],face_alignment,9,208
1606,experimental-setup,We set batch size to 10 for network training .,"[('set', (1, 2)), ('to', (4, 5)), ('for', (6, 7))]","[('batch size', (2, 4)), ('10', (5, 6)), ('network training', (7, 9))]","[['batch size', 'to', '10'], ['10', 'for', 'network training']]","[['batch size', 'has', '10']]","[['Experimental setup', 'set', 'batch size']]",[],face_alignment,9,211
1607,experimental-setup,1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .,"[('trained', (6, 7)), ('with', (7, 8)), ('on', (12, 13))]","[('our models', (3, 5)), ('PyTorch', (8, 9)), ('2 Titan X GPUs', (13, 17))]","[['our models', 'with', 'PyTorch'], ['our models', 'on', '2 Titan X GPUs'], ['PyTorch', 'on', '2 Titan X GPUs']]",[],[],"[['Experimental setup', 'has', 'our models']]",face_alignment,9,213
1608,baselines,300 W .,[],"[('300 W', (0, 2))]",[],[],[],[],face_alignment,9,215
1609,results,"2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .","[('see that', (4, 6)), ('with', (7, 8)), ('in terms of', (29, 32)), ('on', (33, 34)), ('showing', (37, 38)), ('of', (41, 42))]","[('HGs', (6, 7)), ('our Semantic Alignment ( HGs + SA )', (8, 16)), ('greatly outperform', (16, 18)), ('hourglass ( HGs ) only', (18, 23)), ('4.37 % vs 5.04 %', (24, 29)), ('NME', (32, 33)), ('Full set', (34, 36)), ('great effectiveness', (39, 41)), ('our Semantic Alignment ( SA )', (42, 48))]","[['HGs', 'with', 'our Semantic Alignment ( HGs + SA )'], ['4.37 % vs 5.04 %', 'in terms of', 'NME'], ['NME', 'on', 'Full set'], ['our Semantic Alignment ( HGs + SA )', 'showing', 'great effectiveness'], ['greatly outperform', 'showing', 'great effectiveness'], ['4.37 % vs 5.04 %', 'showing', 'great effectiveness'], ['great effectiveness', 'of', 'our Semantic Alignment ( SA )']]","[['our Semantic Alignment ( HGs + SA )', 'has', 'greatly outperform'], ['greatly outperform', 'has', 'hourglass ( HGs ) only']]","[['Results', 'see that', 'HGs']]",[],face_alignment,9,220
1610,results,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .","[('adding', (1, 2)), ('see', (6, 7)), ('slightly outperforms', (13, 15))]","[('GHCU', (2, 3)), ('HGs + SA + GHCU', (8, 13)), ('HGs + SA', (16, 19))]","[['GHCU', 'see', 'HGs + SA + GHCU'], ['HGs + SA + GHCU', 'slightly outperforms', 'HGs + SA']]","[['GHCU', 'has', 'HGs + SA + GHCU']]","[['Results', 'adding', 'GHCU']]",[],face_alignment,9,221
1611,results,"Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .","[('normalize', (3, 4)), ('by training', (10, 12)), ('achieve', (30, 31)), ('on', (36, 37))]","[('in - plane - rotation', (5, 10)), ('preprocessing network', (13, 15)), ('state of the art performance', (31, 36)), ('Challenge set and Full set', (37, 42)), ('6.38 % and 4.02 %', (43, 48))]","[['in - plane - rotation', 'by training', 'preprocessing network'], ['state of the art performance', 'on', 'Challenge set and Full set']]",[],[],[],face_alignment,9,223
1612,results,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .","[('on', (3, 4)), ('significantly outperform', (8, 10)), ('vs', (27, 28))]","[('Challenge set', (4, 6)), ('state of the art method', (11, 16)), ('6.38 %', (17, 19)), ('HGs + SA +GHCU + Norm', (20, 26)), ('6.98 %', (28, 30)), ('LAB', (31, 32))]","[['Challenge set', 'significantly outperform', 'state of the art method'], ['state of the art method', 'vs', '6.98 %'], ['6.38 %', 'vs', '6.98 %'], ['HGs + SA +GHCU + Norm', 'vs', '6.98 %']]","[['state of the art method', 'has', '6.38 %'], ['6.38 %', 'name', 'HGs + SA +GHCU + Norm'], ['6.98 %', 'has', 'LAB']]","[['Results', 'on', 'Challenge set']]",[],face_alignment,9,224
1613,results,AFLW . 300W has 68 facial points which contain many weak semantic landmarks ( e.g. those on face contours ) .,[],"[('AFLW', (0, 1))]",[],[],[],"[['Results', 'has', 'AFLW']]",face_alignment,9,225
1614,results,"As shown in Tab. 3 , HGs + SA outperforms",[],"[('HGs + SA', (6, 9)), ('outperforms', (9, 10))]",[],"[['HGs + SA', 'has', 'outperforms']]",[],"[['Results', 'has', 'HGs + SA']]",face_alignment,9,230
1615,results,"HGs , 1.62 % vs 1.95 % .",[],"[('HGs', (0, 1)), ('1.62 % vs 1.95 %', (2, 7))]",[],"[['HGs', 'has', '1.62 % vs 1.95 %']]",[],[],face_alignment,9,231
1616,results,It is also observed that HGs + SA + GHCU works better than HGs + SA .,"[('observed that', (3, 5)), ('works', (10, 11)), ('than', (12, 13))]","[('HGs + SA + GHCU', (5, 10)), ('better', (11, 12)), ('HGs + SA', (13, 16))]","[['HGs + SA + GHCU', 'works', 'better'], ['better', 'than', 'HGs + SA']]","[['HGs + SA + GHCU', 'has', 'better']]","[['Results', 'observed that', 'HGs + SA + GHCU']]",[],face_alignment,9,233
1617,baselines,300 - VW .,[],"[('300 - VW', (0, 3))]",[],[],[],[],face_alignment,9,234
1618,results,"4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .","[('see', (4, 5))]","[('HGs + SA', (6, 9)), ('greatly outperforms', (9, 11)), ('HGs', (11, 12))]",[],"[['HGs + SA', 'has', 'greatly outperforms'], ['greatly outperforms', 'has', 'HGs']]","[['Results', 'see', 'HGs + SA']]",[],face_alignment,9,238
1619,results,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .","[('compared with', (2, 4)), ('reduce', (13, 14)), ('by', (20, 21)), ('on', (23, 24))]","[('HGs + SA', (4, 7)), ('HGs + SA + GHCU', (8, 13)), ('error rate ( RMSE )', (15, 20)), ('18 %', (21, 23)), ('Category 3 test set', (24, 28))]","[['HGs + SA + GHCU', 'reduce', 'error rate ( RMSE )'], ['error rate ( RMSE )', 'by', '18 %'], ['18 %', 'on', 'Category 3 test set']]","[['HGs + SA', 'has', 'HGs + SA + GHCU']]","[['Results', 'compared with', 'HGs + SA']]",[],face_alignment,9,239
1620,ablation-analysis,"To verify the effectiveness of different components in our framework , we conduct this ablation study on 300 - VW .","[('on', (16, 17))]","[('300 - VW', (17, 20))]",[],[],"[['Ablation analysis', 'on', '300 - VW']]",[],face_alignment,9,287
1621,ablation-analysis,"9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .","[('consistently improve', (5, 7)), ('on', (9, 10))]","[('Semantic alignment', (2, 4)), ('performance', (8, 9)), ('all subset sets', (10, 13))]","[['Semantic alignment', 'consistently improve', 'performance'], ['performance', 'on', 'all subset sets']]",[],[],"[['Ablation analysis', 'has', 'Semantic alignment']]",face_alignment,9,290
1622,ablation-analysis,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .","[('on', (4, 5))]","[('GHCU', (0, 1)), ('more effective', (2, 4)), ('challenge data set ( Category 3 )', (6, 13)), ('8.15 % vs 9.91 %', (14, 19))]","[['more effective', 'on', 'challenge data set ( Category 3 )']]","[['GHCU', 'has', 'more effective'], ['challenge data set ( Category 3 )', 'has', '8.15 % vs 9.91 %']]",[],"[['Ablation analysis', 'has', 'GHCU']]",face_alignment,9,291
1623,research-problem,Accurate Face Detection for High Performance,[],"[('Face Detection', (1, 3))]",[],[],[],[],face_detection,0,2
1624,model,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .","[('modify', (6, 7)), ('to perform', (14, 16))]","[('popular one - stage RetinaNet method', (8, 14)), ('face detection', (16, 18))]","[['popular one - stage RetinaNet method', 'to perform', 'face detection']]",[],"[['Model', 'modify', 'popular one - stage RetinaNet method']]",[],face_detection,0,23
1625,model,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,"[('namely', (16, 17))]","[('AInnoFace', (17, 18))]",[],[],"[['Model', 'namely', 'AInnoFace']]",[],face_detection,0,24
1626,model,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,"[('Employing', (3, 4)), ('for', (11, 12)), ('Applying', (17, 18)), ('for', (27, 28)), ('Revisiting', (33, 34)), ('based on', (37, 39)), ('for', (44, 45)), ('Utilizing', (50, 51)), ('for', (56, 57)), ('Using', (63, 64)), ('for', (68, 69))]","[('two - step classification and regression', (5, 11)), ('detection', (12, 13)), ('Intersection over Union ( IoU ) loss function', (19, 27)), ('regression', (28, 29)), ('data augmentation', (35, 37)), ('data - anchor - sampling', (39, 44)), ('training', (45, 46)), ('max - out operation', (52, 56)), ('robuster classification', (57, 59)), ('multi-scale testing strategy', (65, 68)), ('inference', (69, 70))]","[['two - step classification and regression', 'for', 'detection'], ['Intersection over Union ( IoU ) loss function', 'for', 'regression'], ['multi-scale testing strategy', 'for', 'inference'], ['Intersection over Union ( IoU ) loss function', 'for', 'regression'], ['data augmentation', 'based on', 'data - anchor - sampling'], ['data - anchor - sampling', 'for', 'training'], ['max - out operation', 'for', 'robuster classification'], ['multi-scale testing strategy', 'for', 'inference']]",[],"[['Model', 'Employing', 'two - step classification and regression']]",[],face_detection,0,25
1627,experimental-setup,The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset .,"[('in', (3, 4)), ('initialized by', (9, 11)), ('on', (14, 15))]","[('backbone network', (1, 3)), ('proposed AInnoFace detector', (5, 8)), ('pretrained model', (12, 14)), ('ImageNet dataset', (16, 18))]","[['backbone network', 'in', 'proposed AInnoFace detector'], ['proposed AInnoFace detector', 'initialized by', 'pretrained model'], ['pretrained model', 'on', 'ImageNet dataset']]","[['backbone network', 'has', 'proposed AInnoFace detector']]",[],"[['Experimental setup', 'has', 'backbone network']]",face_detection,0,134
1628,experimental-setup,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .","[('use', (1, 2)), ('randomly initialize', (8, 10)), ('in', (12, 13))]","[('"" xavier "" method', (3, 7)), ('parameters', (11, 12)), ('newly added convolutional layers', (14, 18))]","[['"" xavier "" method', 'randomly initialize', 'parameters'], ['parameters', 'in', 'newly added convolutional layers']]",[],"[['Experimental setup', 'use', '"" xavier "" method']]",[],face_detection,0,135
1629,experimental-setup,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .","[('fine - tune', (11, 14)), ('with', (16, 17))]","[('stochastic gradient descent ( SGD ) algorithm', (1, 8)), ('model', (15, 16)), ('0.9 momentum', (17, 19)), ('0.0001 weight decay', (20, 23)), ('batch size 32', (24, 27))]","[['stochastic gradient descent ( SGD ) algorithm', 'fine - tune', 'model'], ['model', 'with', '0.9 momentum'], ['model', 'with', 'batch size 32']]",[],[],"[['Experimental setup', 'has', 'stochastic gradient descent ( SGD ) algorithm']]",face_detection,0,136
1630,experimental-setup,The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,"[('applied to', (4, 6)), ('from', (12, 13)), ('to', (14, 15)), ('at', (16, 17))]","[('warmup strategy', (1, 3)), ('gradually ramp up', (6, 9)), ('learning rate', (10, 12)), ('0.0003125', (13, 14)), ('0.01', (15, 16)), ('first 5 epochs', (18, 21))]","[['warmup strategy', 'applied to', 'gradually ramp up'], ['learning rate', 'from', '0.0003125'], ['0.0003125', 'to', '0.01'], ['0.01', 'at', 'first 5 epochs']]","[['gradually ramp up', 'has', 'learning rate']]",[],"[['Experimental setup', 'has', 'warmup strategy']]",face_detection,0,137
1631,experimental-setup,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .","[('switches to', (4, 6)), ('dividing by', (14, 16)), ('at', (17, 18)), ('ending at', (23, 25))]","[('regular learning rate schedule', (7, 11)), ('10', (16, 17)), ('100 and 120 epochs', (18, 22)), ('130 epochs', (25, 27))]","[['regular learning rate schedule', 'dividing by', '10'], ['10', 'at', '100 and 120 epochs'], ['regular learning rate schedule', 'ending at', '130 epochs']]",[],[],[],face_detection,0,138
1632,experimental-setup,The full training and testing codes are built on the PyTorch library .,"[('built on', (7, 9))]","[('full training and testing codes', (1, 6)), ('PyTorch library', (10, 12))]","[['full training and testing codes', 'built on', 'PyTorch library']]",[],[],"[['Experimental setup', 'has', 'full training and testing codes']]",face_detection,0,139
1633,results,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .","[('sets', (7, 8)), ('based on', (18, 20)), ('across', (23, 24)), ('on both', (27, 29)), ('for', (53, 54))]","[('our face detector', (4, 7)), ('new state - of - the - art results', (9, 18)), ('AP score', (21, 23)), ('three subsets', (25, 27)), ('validation and testing subsets', (29, 33)), ('97.0 % ( Easy )', (36, 41)), ('96.1 % ( Medium )', (42, 47)), ('91.8 % ( Hard )', (48, 53)), ('validation subset', (54, 56)), ('96.5 % ( Easy )', (58, 63)), ('95.7 % ( Medium )', (64, 69)), ('91.2 % ( Hard )', (70, 75)), ('testing subset', (76, 78))]","[['our face detector', 'sets', 'new state - of - the - art results'], ['new state - of - the - art results', 'based on', 'AP score'], ['AP score', 'across', 'three subsets'], ['three subsets', 'on both', 'validation and testing subsets'], ['91.8 % ( Hard )', 'for', 'validation subset']]","[['our face detector', 'has', 'new state - of - the - art results']]",[],"[['Results', 'has', 'our face detector']]",face_detection,0,141
1634,results,These results outperform all the compared state - of - the - art methods and demonstrate the superiority of our AInnoFace detector .,"[('outperform', (2, 3)), ('demonstrate', (15, 16)), ('of', (18, 19))]","[('results', (1, 2)), ('all the compared state - of - the - art methods', (3, 14)), ('superiority', (17, 18)), ('our AInnoFace detector', (19, 22))]","[['results', 'outperform', 'all the compared state - of - the - art methods'], ['results', 'demonstrate', 'superiority'], ['superiority', 'of', 'our AInnoFace detector']]",[],[],[],face_detection,0,142
1635,research-problem,"Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications , and many studies are proposed from the beginning of the computer vision research .",[],"[('Detecting faces in an image', (0, 5))]",[],[],[],[],face_detection,1,10
1636,research-problem,"After the advent of deep neural networks , many face detection algorithms applying the deep network have reported significant performance improvement to the conventional face detectors .","[('face detection', (9, 11))]",[],[],[],[],[],face_detection,1,11
1637,model,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .","[('propose', (5, 6)), ('with', (11, 12))]","[('new multi-scale face detector', (7, 11)), ('extremely tiny size ( EXTD )', (12, 18))]","[['new multi-scale face detector', 'with', 'extremely tiny size ( EXTD )']]",[],"[['Model', 'propose', 'new multi-scale face detector']]",[],face_detection,1,28
1638,model,"The main discovery is that we can share the network in generating each feature - map , as shown in .","[('share', (7, 8)), ('in generating', (10, 12))]","[('main discovery', (1, 3)), ('network', (9, 10)), ('each feature - map', (12, 16))]","[['main discovery', 'share', 'network'], ['network', 'in generating', 'each feature - map']]",[],[],"[['Model', 'has', 'main discovery']]",face_detection,1,29
1639,model,"As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .","[('design', (6, 7)), ('reduces', (12, 13)), ('by', (19, 20))]","[('backbone network', (8, 10)), ('size of the feature map', (14, 19)), ('half', (20, 21))]","[['backbone network', 'reduces', 'size of the feature map'], ['size of the feature map', 'by', 'half']]",[],"[['Model', 'design', 'backbone network']]",[],face_detection,1,30
1640,model,"The sharing can significantly reduce the number of parameters , and this enables our model to use more layers to generate the low - level feature maps used for detecting small faces .","[('significantly reduce', (3, 5)), ('enables', (12, 13)), ('to use', (15, 17)), ('to generate', (19, 21)), ('for detecting', (28, 30))]","[('sharing', (1, 2)), ('number of parameters', (6, 9)), ('model', (14, 15)), ('more layers', (17, 19)), ('low - level feature maps', (22, 27)), ('small faces', (30, 32))]","[['sharing', 'significantly reduce', 'number of parameters'], ['sharing', 'enables', 'model'], ['model', 'to use', 'more layers'], ['more layers', 'to generate', 'low - level feature maps'], ['low - level feature maps', 'for detecting', 'small faces']]",[],[],"[['Model', 'has', 'sharing']]",face_detection,1,31
1641,model,"Also , the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations , and hence offer abundant semantic information to the network , without adding additional parameters .","[('makes', (6, 7)), ('to observe', (9, 11)), ('from', (13, 14)), ('offer', (26, 27)), ('to', (30, 31)), ('without adding', (34, 36))]","[('proposed iterative architecture', (3, 6)), ('network', (8, 9)), ('features', (12, 13)), ('various scale of faces', (14, 18)), ('various layer locations', (20, 23)), ('abundant semantic information', (27, 30)), ('network', (32, 33)), ('additional parameters', (36, 38))]","[['proposed iterative architecture', 'makes', 'network'], ['network', 'to observe', 'features'], ['features', 'from', 'various scale of faces'], ['features', 'from', 'various layer locations'], ['proposed iterative architecture', 'offer', 'abundant semantic information'], ['abundant semantic information', 'to', 'network'], ['abundant semantic information', 'without adding', 'additional parameters']]","[['proposed iterative architecture', 'has', 'network']]",[],"[['Model', 'has', 'proposed iterative architecture']]",face_detection,1,32
1642,model,"Our baseline framework follows FPN - like structures , but can also be applied to SSD - like architecture .","[('follows', (3, 4)), ('applied to', (13, 15))]","[('Our baseline framework', (0, 3)), ('FPN - like structures', (4, 8)), ('SSD - like architecture', (15, 19))]","[['Our baseline framework', 'follows', 'FPN - like structures'], ['Our baseline framework', 'applied to', 'SSD - like architecture']]","[['Our baseline framework', 'has', 'FPN - like structures']]",[],"[['Model', 'has', 'Our baseline framework']]",face_detection,1,33
1643,code,Code will be available at https ://github.com/clovaai .,[],"[('https ://github.com/clovaai', (5, 7))]",[],[],[],[],face_detection,1,142
1644,hyperparameters,"Also , we tested different activation functions : ReLU , PReLU , and Leaky - ReLU for each model .","[('tested', (3, 4))]","[('different activation functions', (4, 7)), ('ReLU', (8, 9)), ('PReLU', (10, 11)), ('Leaky - ReLU', (13, 16))]",[],"[['different activation functions', 'name', 'ReLU']]","[['Hyperparameters', 'tested', 'different activation functions']]",[],face_detection,1,162
1645,hyperparameters,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .","[('of', (3, 4)), ('set to', (9, 11)), ('identical to', (15, 17)), ('of', (21, 22))]","[('negative slope', (1, 3)), ('Leaky - ReLU', (5, 8)), ('0.25', (11, 12)), ('initial negative slope', (18, 21)), ('PReLU', (23, 24))]","[['negative slope', 'of', 'Leaky - ReLU'], ['initial negative slope', 'of', 'PReLU'], ['Leaky - ReLU', 'set to', '0.25'], ['0.25', 'identical to', 'initial negative slope'], ['initial negative slope', 'of', 'PReLU']]",[],[],"[['Hyperparameters', 'has', 'negative slope']]",face_detection,1,163
1646,results,"The results in 138 times lighter in model size and are 28.3 , 19.2 , and 11 times lighter in Madds .","[('in', (2, 3)), ('in', (6, 7))]","[('138 times lighter', (3, 6)), ('model size', (7, 9)), ('28.3 , 19.2 , and 11 times lighter', (11, 19)), ('Madds', (20, 21))]","[['138 times lighter', 'in', 'model size'], ['28.3 , 19.2 , and 11 times lighter', 'in', 'Madds']]",[],[],[],face_detection,1,178
1647,results,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .","[('compared to', (1, 3)), ('such as', (6, 8)), ('achieved', (24, 25))]","[('SOTA face detectors', (3, 6)), ('Pyra - midBox and DSFD', (8, 13)), ('our best model EXTD - FPN - 64 - PReLU', (14, 24)), ('lower results', (25, 27))]","[['SOTA face detectors', 'such as', 'Pyra - midBox and DSFD'], ['our best model EXTD - FPN - 64 - PReLU', 'achieved', 'lower results']]","[['SOTA face detectors', 'name', 'Pyra - midBox and DSFD']]",[],[],face_detection,1,179
1648,results,The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,"[('between', (2, 3)), ('on', (8, 9)), ('was', (13, 14))]","[('margin', (1, 2)), ('PyramidBox and the proposed model', (3, 8)), ('WIDER FACE hard case', (9, 13)), ('3.4 %', (14, 16))]","[['margin', 'between', 'PyramidBox and the proposed model'], ['PyramidBox and the proposed model', 'on', 'WIDER FACE hard case'], ['WIDER FACE hard case', 'was', '3.4 %']]",[],[],"[['Results', 'has', 'margin']]",face_detection,1,180
1649,results,"The m AP gap to DSFD , which is tremendously heavier , is about 5.0 % , but it would be safe to suggest that the proposed method offers more decent trade - off in that DSFD uses about 2860 times more parameters than the proposed method .","[('about', (13, 14))]","[('m AP gap to DSFD', (1, 6)), ('tremendously heavier', (9, 11)), ('5.0 %', (14, 16))]","[['tremendously heavier', 'about', '5.0 %']]","[['m AP gap to DSFD', 'has', 'tremendously heavier']]",[],"[['Results', 'has', 'm AP gap to DSFD']]",face_detection,1,182
1650,results,"When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .","[('got', (11, 12)), ('than', (15, 16))]","[('our SSD - based variations', (4, 9)), ('lower mAP results', (12, 15)), ('FPN - based variants', (16, 20))]","[['our SSD - based variations', 'got', 'lower mAP results'], ['lower mAP results', 'than', 'FPN - based variants']]","[['our SSD - based variations', 'has', 'lower mAP results']]",[],[],face_detection,1,186
1651,results,"However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .","[('compared with', (3, 5)), ('trained with', (8, 10)), ('achieved', (21, 22))]","[('S3FD version', (6, 8)), ('Mo - bile FaceNet backbone network', (10, 16)), ('proposed SSD variants', (18, 21)), ('comparable or better detection performance', (22, 27))]","[['S3FD version', 'trained with', 'Mo - bile FaceNet backbone network'], ['proposed SSD variants', 'achieved', 'comparable or better detection performance']]",[],"[['Results', 'compared with', 'S3FD version']]",[],face_detection,1,187
1652,results,"From the table , we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases .","[('see that', (6, 8)), ('achieved', (10, 11)), ('in', (13, 14))]","[('our method', (8, 10)), ('higher performance', (11, 13)), ('WIDER FACE hard dataset', (14, 18))]","[['our method', 'achieved', 'higher performance'], ['higher performance', 'in', 'WIDER FACE hard dataset']]",[],"[['Results', 'see that', 'our method']]",[],face_detection,1,196
1653,results,"First , for all the different channel width , FPN based architecture achieved better detection performance compared to SSD based architecture , especially for detecting small faces .","[('for', (2, 3)), ('achieved', (12, 13)), ('compared to', (16, 18))]","[('different channel width', (5, 8)), ('FPN based architecture', (9, 12)), ('better detection performance', (13, 16)), ('SSD based architecture', (18, 21))]","[['FPN based architecture', 'achieved', 'better detection performance'], ['better detection performance', 'compared to', 'SSD based architecture']]","[['different channel width', 'has', 'FPN based architecture']]","[['Results', 'for', 'different channel width']]",[],face_detection,1,204
1654,results,"When tested with SSD based architecture , PReLU outperformed Leaky - ReLU with larger margin than those using FPN structure .","[('When tested with', (0, 3)), ('outperformed', (8, 9)), ('with', (12, 13)), ('than those using', (15, 18))]","[('SSD based architecture', (3, 6)), ('PReLU', (7, 8)), ('Leaky - ReLU', (9, 12)), ('larger margin', (13, 15)), ('FPN structure', (18, 20))]","[['PReLU', 'outperformed', 'Leaky - ReLU'], ['Leaky - ReLU', 'with', 'larger margin'], ['larger margin', 'than those using', 'FPN structure']]","[['SSD based architecture', 'has', 'PReLU']]","[['Results', 'When tested with', 'SSD based architecture']]",[],face_detection,1,214
1655,model,"In this paper , we propose a novel face detection network with three novel contributions that address three key aspects of face detection , including better feature learning , progressive loss design and anchor assign based data augmentation , respectively .",[],"[('face detection', (8, 10))]",[],[],[],[],face_detection,10,4
1656,model,"In this paper , we propose three novel techniques to address the above three issues , respectively .","[('propose', (5, 6))]","[('three novel techniques', (6, 9))]",[],[],"[['Model', 'propose', 'three novel techniques']]",[],face_detection,10,30
1657,model,"First , we introduce a Feature Enhance Module ( FEM ) to enhance the discriminability and robustness of the features , which combines the advantages of the FPN in PyramidBox and Receptive Field Block ( RFB ) in RFBNet .","[('introduce', (3, 4)), ('to enhance', (11, 13)), ('of', (17, 18)), ('combines', (22, 23)), ('of', (25, 26)), ('in', (28, 29)), ('in', (37, 38))]","[('Feature Enhance Module ( FEM )', (5, 11)), ('discriminability and robustness', (14, 17)), ('features', (19, 20)), ('advantages', (24, 25)), ('FPN', (27, 28)), ('PyramidBox', (29, 30)), ('Receptive Field Block ( RFB )', (31, 37)), ('RFBNet', (38, 39))]","[['Feature Enhance Module ( FEM )', 'to enhance', 'discriminability and robustness'], ['discriminability and robustness', 'of', 'features'], ['advantages', 'of', 'FPN'], ['discriminability and robustness', 'of', 'features'], ['advantages', 'of', 'FPN'], ['advantages', 'of', 'Receptive Field Block ( RFB )'], ['FPN', 'in', 'PyramidBox'], ['FPN', 'in', 'RFBNet'], ['Receptive Field Block ( RFB )', 'in', 'RFBNet'], ['FPN', 'in', 'RFBNet'], ['Receptive Field Block ( RFB )', 'in', 'RFBNet']]",[],"[['Model', 'introduce', 'Feature Enhance Module ( FEM )']]",[],face_detection,10,31
1658,model,"Second , motivated by the hierarchical loss and pyramid anchor in PyramidBox , we design Progressive Anchor Loss ( PAL ) that uses progressive anchor sizes for not only different levels , but also different shots .","[('in', (10, 11)), ('design', (14, 15)), ('uses', (22, 23)), ('for', (26, 27))]","[('Progressive Anchor Loss ( PAL )', (15, 21)), ('progressive anchor sizes', (23, 26)), ('different levels', (29, 31)), ('different shots', (34, 36))]","[['Progressive Anchor Loss ( PAL )', 'uses', 'progressive anchor sizes'], ['progressive anchor sizes', 'for', 'different levels'], ['progressive anchor sizes', 'for', 'different shots']]",[],"[['Model', 'in', 'Progressive Anchor Loss ( PAL )']]",[],face_detection,10,32
1659,model,"Specifically , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .","[('assign', (3, 4)), ('in', (7, 8)), ('use', (13, 14))]","[('smaller anchor sizes', (4, 7)), ('first shot', (9, 11)), ('larger sizes', (14, 16)), ('second shot', (18, 20))]","[['smaller anchor sizes', 'in', 'first shot'], ['larger sizes', 'in', 'second shot']]",[],"[['Model', 'assign', 'smaller anchor sizes']]",[],face_detection,10,33
1660,model,"Third , we propose Improved Anchor Matching ( IAM ) , which integrates anchor partition strategy and anchor-based data augmentation to better match anchors and ground truth faces , and thus provides better initialization for the regressor .","[('propose', (3, 4)), ('integrates', (12, 13)), ('to better match', (20, 23)), ('provides', (31, 32)), ('for', (34, 35))]","[('Improved Anchor Matching ( IAM )', (4, 10)), ('anchor partition strategy and anchor-based data augmentation', (13, 20)), ('anchors and ground truth faces', (23, 28)), ('better initialization', (32, 34)), ('regressor', (36, 37))]","[['Improved Anchor Matching ( IAM )', 'integrates', 'anchor partition strategy and anchor-based data augmentation'], ['anchor partition strategy and anchor-based data augmentation', 'to better match', 'anchors and ground truth faces'], ['Improved Anchor Matching ( IAM )', 'provides', 'better initialization'], ['better initialization', 'for', 'regressor']]",[],"[['Model', 'propose', 'Improved Anchor Matching ( IAM )']]",[],face_detection,10,34
1661,model,"Besides , since these techniques are all related to two - stream design , we name the proposed network as Dual Shot Face Detector ( DSFD ) .","[('as', (19, 20))]","[('proposed network', (17, 19)), ('Dual Shot Face Detector ( DSFD )', (20, 27))]","[['proposed network', 'as', 'Dual Shot Face Detector ( DSFD )']]","[['proposed network', 'name', 'Dual Shot Face Detector ( DSFD )']]",[],[],face_detection,10,36
1662,hyperparameters,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,"[('initialized by', (4, 6)), ('on', (11, 12))]","[('backbone networks', (1, 3)), ('pretrained VGG / ResNet', (7, 11)), ('Image Net', (12, 14))]","[['backbone networks', 'initialized by', 'pretrained VGG / ResNet'], ['pretrained VGG / ResNet', 'on', 'Image Net']]",[],[],"[['Hyperparameters', 'has', 'backbone networks']]",face_detection,10,127
1663,hyperparameters,All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,"[('initialized by', (8, 10))]","[(""All newly added convolution layers ' parameters"", (0, 7)), (""' xavier ' method"", (11, 15))]","[[""All newly added convolution layers ' parameters"", 'initialized by', ""' xavier ' method""]]",[],[],"[['Hyperparameters', 'has', ""All newly added convolution layers ' parameters""]]",face_detection,10,128
1664,hyperparameters,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .","[('use', (1, 2)), ('with', (3, 4)), ('to fine - tune', (10, 14))]","[('SGD', (2, 3)), ('0.9', (4, 5)), ('momentum', (5, 6)), ('0.0005', (7, 8)), ('weight decay', (8, 10)), ('our DSFD model', (14, 17))]","[['SGD', 'with', '0.9'], ['weight decay', 'to fine - tune', 'our DSFD model']]","[['0.9', 'has', 'momentum'], ['0.0005', 'has', 'weight decay']]","[['Hyperparameters', 'use', 'SGD']]",[],face_detection,10,129
1665,hyperparameters,The batch size is set to 16 .,"[('set to', (4, 6))]","[('batch size', (1, 3)), ('16', (6, 7))]","[['batch size', 'set to', '16']]","[['batch size', 'has', '16']]",[],"[['Hyperparameters', 'has', 'batch size']]",face_detection,10,130
1666,hyperparameters,"The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .","[('set to', (4, 6)), ('for', (8, 9)), ('decay', (17, 18)), ('to', (19, 20)), ('for', (27, 28))]","[('learning rate', (1, 3)), ('10 ?3', (6, 8)), ('first 40 k steps', (10, 14)), ('10 ? 4 and 10 ? 5', (20, 27)), ('two 10 k steps', (28, 32))]","[['learning rate', 'set to', '10 ?3'], ['10 ?3', 'for', 'first 40 k steps'], ['10 ? 4 and 10 ? 5', 'for', 'two 10 k steps'], ['learning rate', 'decay', '10 ? 4 and 10 ? 5'], ['10 ? 4 and 10 ? 5', 'for', 'two 10 k steps']]","[['learning rate', 'has', '10 ?3']]",[],"[['Hyperparameters', 'has', 'learning rate']]",face_detection,10,131
1667,hyperparameters,"During inference , the first shot 's outputs are ignored and the second shot predicts top 5 k high confident detections .","[('During', (0, 1)), ('are', (8, 9)), ('predicts', (14, 15))]","[('inference', (1, 2)), (""first shot 's outputs"", (4, 8)), ('ignored', (9, 10)), ('second shot', (12, 14)), ('top 5 k high confident detections', (15, 21))]","[[""first shot 's outputs"", 'are', 'ignored'], ['second shot', 'predicts', 'top 5 k high confident detections']]","[['inference', 'has', ""first shot 's outputs""], [""first shot 's outputs"", 'has', 'ignored']]","[['Hyperparameters', 'During', 'inference']]",[],face_detection,10,132
1668,hyperparameters,Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,"[('applied with', (3, 5)), ('of', (7, 8)), ('to produce', (9, 11)), ('per', (17, 18))]","[('Non-maximum suppression', (0, 2)), ('jaccard overlap', (5, 7)), ('0.3', (8, 9)), ('top 750 high confident bounding boxes', (11, 17)), ('image', (18, 19))]","[['Non-maximum suppression', 'applied with', 'jaccard overlap'], ['jaccard overlap', 'of', '0.3'], ['0.3', 'to produce', 'top 750 high confident bounding boxes'], ['top 750 high confident bounding boxes', 'per', 'image']]",[],[],"[['Hyperparameters', 'has', 'Non-maximum suppression']]",face_detection,10,133
1669,hyperparameters,"For 4 bounding box coordinates , we round down top left coordinates and roundup width and height to expand the detection bounding box .","[('For', (0, 1)), ('round down', (7, 9)), ('to expand', (17, 19))]","[('bounding box coordinates', (2, 5)), ('top left coordinates', (9, 12)), ('roundup', (13, 14)), ('width and height', (14, 17)), ('detection bounding box', (20, 23))]","[['bounding box coordinates', 'round down', 'top left coordinates'], ['width and height', 'to expand', 'detection bounding box']]","[['roundup', 'has', 'width and height']]","[['Hyperparameters', 'For', 'bounding box coordinates']]",[],face_detection,10,134
1670,code,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,[],"[('https://github.com/TencentYoutuResearch/FaceDetection-DSFD', (8, 9))]",[],[],[],[],face_detection,10,135
1671,experiments,"In this subsection , we conduct extensive experiments and ablation studies on the WIDER FACE dataset to evaluate the effectiveness of several contributions of our proposed framework , including feature enhance module , progressive anchor loss , and improved anchor matching .","[('on', (11, 12))]","[('WIDER FACE dataset', (13, 16))]",[],[],[],[],face_detection,10,139
1672,baselines,"Feature Enhance Module First ,",[],"[('Feature Enhance Module First', (0, 4))]",[],[],[],"[['Baselines', 'has', 'Feature Enhance Module First']]",face_detection,10,143
1673,experiments,"shows that our feature enhance module can improve VGG16 - based FSSD from 92.6 % , 90.2 % , 79.1 % to 93.0 % , 91.4 % , 84.6 % .","[('shows', (0, 1)), ('improve', (7, 8)), ('from', (12, 13)), ('to', (21, 22))]","[('our feature enhance module', (2, 6)), ('VGG16 - based FSSD', (8, 12)), ('92.6 % , 90.2 % , 79.1 %', (13, 21)), ('93.0 % , 91.4 % , 84.6 %', (22, 30))]","[['our feature enhance module', 'improve', 'VGG16 - based FSSD'], ['VGG16 - based FSSD', 'from', '92.6 % , 90.2 % , 79.1 %'], ['92.6 % , 90.2 % , 79.1 %', 'to', '93.0 % , 91.4 % , 84.6 %']]",[],[],[],face_detection,10,146
1674,baselines,"Progressive Anchor Loss Second , we use Res50 - based FSSD as the baseline to add progressive anchor loss for comparison .","[('to', (14, 15))]","[('Progressive Anchor Loss Second', (0, 4)), ('Res50 - based FSSD', (7, 11))]",[],[],[],[],face_detection,10,147
1675,experiments,"shows our progressive anchor loss can improve Res50 - based FSSD using FEM from 95.0 % , 94.1 % , 88.0 % to 95.3 % , 94.4 % , 88.6 % .","[('shows', (0, 1)), ('improve', (6, 7)), ('using', (11, 12)), ('from', (13, 14))]","[('our progressive anchor loss', (1, 5)), ('FEM', (12, 13)), ('95.0 % , 94.1 % , 88.0 %', (14, 22)), ('95.3 % , 94.4 % , 88.6 %', (23, 31))]","[['FEM', 'from', '95.0 % , 94.1 % , 88.0 %']]",[],[],[],face_detection,10,150
1676,experiments,Improved Anchor Matching,[],"[('Improved Anchor Matching', (0, 3))]",[],[],[],[],face_detection,10,151
1677,experiments,"shows that our improved anchor matching can improve Res101 based FSSD using FEM from 95.8 % , 95.1 % , 89.7 % to 96.1 % , 95.2 % , 90.0 % .","[('shows', (0, 1)), ('improve', (7, 8)), ('using', (11, 12)), ('from', (13, 14)), ('to', (22, 23))]","[('our improved anchor matching', (2, 6)), ('Res101 based FSSD', (8, 11)), ('FEM', (12, 13)), ('95.8 % , 95.1 % , 89.7 %', (14, 22)), ('96.1 % , 95.2 % , 90.0 %', (23, 31))]","[['our improved anchor matching', 'improve', 'Res101 based FSSD'], ['Res101 based FSSD', 'using', 'FEM'], ['FEM', 'from', '95.8 % , 95.1 % , 89.7 %'], ['95.8 % , 95.1 % , 89.7 %', 'to', '96.1 % , 95.2 % , 90.0 %']]",[],[],[],face_detection,10,153
1678,results,WIDER FACE Dataset,[],"[('WIDER FACE Dataset', (0, 3))]",[],[],[],"[['Results', 'has', 'WIDER FACE Dataset']]",face_detection,10,178
1679,results,"As shown in , our DSFD achieves the best performance among all of the state - of - the - art face detectors based on the average precision ( AP ) across the three subsets , i.e. , 96.6 % ( Easy ) , 95.7 % ( Medium ) and 90.4 % ( Hard ) on validation set , and 96.0 % ( Easy ) , 95.3 % ( Medium ) and 90.0 % ( Hard ) on test set .","[('achieves', (6, 7)), ('among', (10, 11)), ('based on', (23, 25)), ('across', (31, 32)), ('on', (55, 56))]","[('our DSFD', (4, 6)), ('best performance', (8, 10)), ('all of the state - of - the - art face detectors', (11, 23)), ('average precision ( AP )', (26, 31)), ('three subsets', (33, 35)), ('96.6 %', (38, 40)), ('Easy', (41, 42)), ('95.7 %', (44, 46)), ('Medium', (47, 48)), ('90.4 %', (50, 52)), ('Hard', (53, 54)), ('validation set', (56, 58)), ('96.0 %', (60, 62)), ('Easy', (63, 64)), ('95.3 %', (66, 68)), ('Medium', (69, 70)), ('90.0 %', (72, 74)), ('Hard', (75, 76)), ('test set', (78, 80))]","[['our DSFD', 'achieves', 'best performance'], ['best performance', 'among', 'all of the state - of - the - art face detectors'], ['best performance', 'among', '90.0 %'], ['all of the state - of - the - art face detectors', 'based on', 'average precision ( AP )'], ['average precision ( AP )', 'across', 'three subsets']]","[['our DSFD', 'has', 'best performance'], ['96.6 %', 'has', 'Easy'], ['95.7 %', 'has', 'Medium'], ['90.4 %', 'has', 'Hard'], ['96.0 %', 'has', 'Easy'], ['95.3 %', 'has', 'Medium'], ['90.0 %', 'has', 'Hard']]",[],"[['Results', 'has', 'our DSFD']]",face_detection,10,182
1680,experiments,FDDB Dataset,[],"[('FDDB Dataset', (0, 2))]",[],[],[],[],face_detection,10,184
1681,experiments,"As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .","[('achieves', (6, 7)), ('on both', (15, 17)), ('i.e.', (23, 24)), ('when', (29, 30)), ('equals to', (35, 37))]","[('our DSFD', (4, 6)), ('state - of - the - art performance', (7, 15)), ('discontinuous and continuous ROC curves', (17, 22)), ('99.1 % and 86.2 %', (24, 29)), ('number of false positives', (31, 35)), ('1 , 000', (37, 40))]","[['our DSFD', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on both', 'discontinuous and continuous ROC curves'], ['discontinuous and continuous ROC curves', 'i.e.', '99.1 % and 86.2 %'], ['99.1 % and 86.2 %', 'when', 'number of false positives'], ['number of false positives', 'equals to', '1 , 000']]",[],[],[],face_detection,10,187
1682,experiments,"After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .","[('After adding', (0, 2)), ('to', (4, 5)), ('of', (12, 13)), ('can be', (15, 17))]","[('additional annotations', (2, 4)), ('unlabeled faces', (6, 8)), ('false positives', (10, 12)), ('our model', (13, 15)), ('further reduced', (17, 19)), ('outperform', (20, 21)), ('all other methods', (21, 24))]","[['additional annotations', 'to', 'unlabeled faces'], ['false positives', 'of', 'our model'], ['false positives', 'can be', 'further reduced'], ['false positives', 'can be', 'outperform'], ['our model', 'can be', 'further reduced'], ['our model', 'can be', 'outperform']]","[['additional annotations', 'has', 'unlabeled faces'], ['outperform', 'has', 'all other methods']]",[],[],face_detection,10,188
1683,research-problem,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,[],"[('Fast Object Detection', (8, 11))]",[],[],[],[],face_detection,11,2
1684,research-problem,"A unified deep neural network , denoted the multi -scale CNN ( MS - CNN ) , is proposed for fast multi-scale object detection .",[],"[('fast multi-scale object detection', (20, 24))]",[],[],[],[],face_detection,11,4
1685,research-problem,"State - of - the - art object detection performance , at up to 15 fps , is reported on datasets , such as KITTI and Caltech , containing a substantial number of small objects .",[],"[('object detection', (7, 9))]",[],[],[],[],face_detection,11,10
1686,model,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .","[('proposes', (2, 3)), ('denoted', (9, 10)), ('for', (20, 21))]","[('unified multi-scale deep CNN', (4, 8)), ('multi -scale CNN ( MS - CNN )', (11, 19)), ('fast object detection', (21, 24))]","[['unified multi-scale deep CNN', 'denoted', 'multi -scale CNN ( MS - CNN )'], ['unified multi-scale deep CNN', 'for', 'fast object detection'], ['multi -scale CNN ( MS - CNN )', 'for', 'fast object detection']]","[['unified multi-scale deep CNN', 'name', 'multi -scale CNN ( MS - CNN )']]","[['Model', 'proposes', 'unified multi-scale deep CNN']]",[],face_detection,11,29
1687,model,"Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .","[('consists of', (5, 7))]","[('this network', (3, 5)), ('two sub-networks', (7, 9)), ('object proposal network', (11, 14)), ('accurate detection network', (16, 19))]","[['this network', 'consists of', 'two sub-networks']]","[['two sub-networks', 'name', 'object proposal network']]",[],"[['Model', 'has', 'this network']]",face_detection,11,30
1688,model,Both of them are learned end - to - end and share computations .,"[('learned', (4, 5)), ('share', (11, 12))]","[('end - to - end', (5, 10)), ('computations', (12, 13))]",[],[],[],[],face_detection,11,31
1689,model,"However , to ease the inconsistency between the sizes of objects and receptive fields , object detection is performed with multiple output layers , each focusing on objects within certain scale ranges ( see ) .","[('to ease', (2, 4)), ('between', (6, 7)), ('of', (9, 10)), ('performed with', (18, 20)), ('focusing on', (25, 27)), ('within', (28, 29))]","[('inconsistency', (5, 6)), ('sizes', (8, 9)), ('objects and receptive fields', (10, 14)), ('object detection', (15, 17)), ('multiple output layers', (20, 23)), ('objects', (27, 28)), ('certain scale ranges', (29, 32))]","[['inconsistency', 'between', 'sizes'], ['sizes', 'of', 'objects and receptive fields'], ['object detection', 'performed with', 'multiple output layers'], ['multiple output layers', 'focusing on', 'objects'], ['objects', 'within', 'certain scale ranges']]","[['inconsistency', 'has', 'sizes']]","[['Model', 'to ease', 'inconsistency']]",[],face_detection,11,32
1690,model,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,"[('at', (3, 4)), ('combined', (8, 9)), ('to form', (9, 11))]","[('complimentary detectors', (1, 3)), ('different output layers', (4, 7)), ('strong multi-scale detector', (12, 15))]","[['complimentary detectors', 'at', 'different output layers'], ['complimentary detectors', 'to form', 'strong multi-scale detector']]",[],[],"[['Model', 'has', 'complimentary detectors']]",face_detection,11,35
1691,results,The performance of the MS - CNN detector was evaluated on the KITTI and Caltech Pedestrian benchmarks .,"[('of', (2, 3)), ('evaluated on', (9, 11))]","[('performance', (1, 2)), ('MS - CNN detector', (4, 8)), ('KITTI and Caltech Pedestrian benchmarks', (12, 17))]","[['performance', 'of', 'MS - CNN detector'], ['MS - CNN detector', 'evaluated on', 'KITTI and Caltech Pedestrian benchmarks']]","[['performance', 'has', 'MS - CNN detector']]",[],"[['Results', 'has', 'performance']]",face_detection,11,191
1692,code,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn .",[],"[('https://github.com/zhaoweicai/mscnn', (18, 19))]",[],[],[],[],face_detection,11,202
1693,experimental-setup,All times are reported for implementation on a single CPU core ( 2.40 GHz ) of an Intel Xeon E5 - 2630 server with 64 GB of RAM .,"[('reported for', (3, 5)), ('on', (6, 7)), ('of', (15, 16)), ('with', (23, 24)), ('of', (26, 27))]","[('All times', (0, 2)), ('implementation', (5, 6)), ('single CPU core ( 2.40 GHz )', (8, 15)), ('Intel Xeon E5 - 2630 server', (17, 23)), ('64 GB', (24, 26)), ('RAM', (27, 28))]","[['All times', 'reported for', 'implementation'], ['implementation', 'on', 'single CPU core ( 2.40 GHz )'], ['single CPU core ( 2.40 GHz )', 'of', 'Intel Xeon E5 - 2630 server'], ['Intel Xeon E5 - 2630 server', 'with', '64 GB'], ['64 GB', 'of', 'RAM']]",[],[],"[['Experimental setup', 'has', 'All times']]",face_detection,11,203
1694,experimental-setup,An NVIDIA Titan GPU was used for CNN computations .,"[('used for', (5, 7))]","[('NVIDIA Titan GPU', (1, 4)), ('CNN computations', (7, 9))]","[['NVIDIA Titan GPU', 'used for', 'CNN computations']]",[],[],"[['Experimental setup', 'has', 'NVIDIA Titan GPU']]",face_detection,11,204
1695,results,The MS - CNN set a new record for the detection of pedestrians and cyclists .,"[('set', (4, 5)), ('for', (8, 9)), ('of', (11, 12))]","[('MS - CNN', (1, 4)), ('new record', (6, 8)), ('detection', (10, 11)), ('pedestrians and cyclists', (12, 15))]","[['MS - CNN', 'set', 'new record'], ['new record', 'for', 'detection'], ['detection', 'of', 'pedestrians and cyclists']]","[['MS - CNN', 'has', 'new record']]",[],"[['Results', 'has', 'MS - CNN']]",face_detection,11,260
1696,results,"We also led a nontrivial margin over the very recent SDP + RPN , which used scale depen - dent pooling .","[('led', (2, 3)), ('over', (6, 7)), ('used', (15, 16))]","[('nontrivial margin', (4, 6)), ('recent SDP + RPN', (9, 13)), ('scale depen - dent pooling', (16, 21))]","[['nontrivial margin', 'over', 'recent SDP + RPN'], ['recent SDP + RPN', 'used', 'scale depen - dent pooling']]",[],"[['Results', 'led', 'nontrivial margin']]",[],face_detection,11,262
1697,results,Pedestrian detection on Caltech The MS - CNN detector was also evaluated on the Caltech pedestrian benchmark .,"[('on', (2, 3))]","[('MS - CNN', (5, 8)), ('Caltech pedestrian benchmark', (14, 17))]",[],[],"[['Results', 'on', 'MS - CNN']]",[],face_detection,11,266
1698,results,"As shown in , the MS - CNN has state - of the - art performance . and ( c ) show that it performs very well for small and occluded objects , outperforming DeepParts , which explicitly addresses occlusion .","[('performs', (24, 25)), ('for', (27, 28)), ('outperforming', (33, 34))]","[('state - of the - art performance', (9, 16)), ('very well', (25, 27)), ('small and occluded objects', (28, 32)), ('DeepParts', (34, 35))]","[['very well', 'for', 'small and occluded objects'], ['very well', 'outperforming', 'DeepParts']]",[],[],[],face_detection,11,268
1699,research-problem,Region proposal mechanisms are essential for existing deep learning approaches to object detection in images .,[],"[('object detection in images', (11, 15))]",[],[],[],[],face_detection,12,4
1700,research-problem,"Existing deep learning approaches to solve this task ( e.g. , R - CNN and its variants ) mainly rely on region proposal mechanisms ( e.g. , region proposal networks ( RPN s ) ) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection .",[],"[('object detection', (51, 53))]",[],[],[],[],face_detection,12,12
1701,model,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .","[('propose', (9, 10)), ('uses', (24, 25)), ('to achieve', (27, 29)), ('without', (35, 36))]","[('weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', (11, 22)), ('segmentation models', (25, 27)), ('accurate and robust object detection', (30, 35)), ('NMS', (36, 37))]","[['weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach', 'uses', 'segmentation models'], ['segmentation models', 'to achieve', 'accurate and robust object detection'], ['accurate and robust object detection', 'without', 'NMS']]",[],"[['Model', 'propose', 'weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach']]",[],face_detection,12,19
1702,model,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .","[('In', (0, 1)), ('first converts', (8, 10)), ('in', (15, 16)), ('to', (18, 19)), ('called', (25, 26)), ('trained using', (35, 37)), ('as', (39, 40)), ('to learn', (41, 43)), ('for', (45, 46))]","[('training phase', (2, 4)), ('WSMA - Seg', (5, 8)), ('weakly supervised bounding box annotations', (10, 15)), ('detection tasks', (16, 18)), ('multi-channel segmentation - like masks', (19, 24)), ('multimodal annotations', (26, 28)), ('segmentation model', (32, 34)), ('multimodal annotations', (37, 39)), ('labels', (40, 41)), ('multimodal heatmaps', (43, 45)), ('training images', (47, 49))]","[['weakly supervised bounding box annotations', 'In', 'detection tasks'], ['WSMA - Seg', 'first converts', 'weakly supervised bounding box annotations'], ['weakly supervised bounding box annotations', 'in', 'detection tasks'], ['detection tasks', 'to', 'multi-channel segmentation - like masks'], ['multi-channel segmentation - like masks', 'called', 'multimodal annotations'], ['segmentation model', 'trained using', 'multimodal annotations'], ['multimodal annotations', 'as', 'labels'], ['labels', 'to learn', 'multimodal heatmaps'], ['multimodal heatmaps', 'for', 'training images']]","[['training phase', 'has', 'WSMA - Seg']]","[['Model', 'In', 'training phase']]",[],face_detection,12,21
1703,model,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .","[('of', (8, 9)), ('converted into', (14, 16)), ('based on', (22, 24)), ('generate', (40, 41)), ('for', (42, 43)), ('using', (44, 45)), ('of', (53, 54)), ('created as', (56, 58)), ('of', (60, 61))]","[('testing phase', (2, 4)), ('resulting heatmaps', (6, 8)), ('given test image', (10, 13)), ('instance - aware segmentation map', (17, 22)), ('pixel - level logic operation', (25, 30)), ('contour tracing operation', (34, 37)), ('contours', (41, 42)), ('objects', (43, 44)), ('segmentation map', (46, 48)), ('bounding boxes', (51, 53)), ('objects', (54, 55)), ('circumscribed quadrilaterals', (58, 60)), ('their corresponding contours', (61, 64))]","[['resulting heatmaps', 'of', 'given test image'], ['circumscribed quadrilaterals', 'of', 'their corresponding contours'], ['resulting heatmaps', 'converted into', 'instance - aware segmentation map'], ['instance - aware segmentation map', 'based on', 'pixel - level logic operation'], ['contour tracing operation', 'generate', 'contours'], ['contours', 'for', 'objects'], ['objects', 'using', 'segmentation map'], ['bounding boxes', 'of', 'objects'], ['circumscribed quadrilaterals', 'of', 'their corresponding contours'], ['bounding boxes', 'created as', 'circumscribed quadrilaterals'], ['circumscribed quadrilaterals', 'of', 'their corresponding contours']]","[['testing phase', 'has', 'resulting heatmaps']]",[],[],face_detection,12,22
1704,experiments,Rebar Head Detection,[],"[('Rebar Head Detection', (0, 3))]",[],[],[],[],face_detection,12,90
1705,experiments,"As shown in , our proposed method with Stack = 2 , Base = 40 , Depth = 5 has achieved the best performance among all solutions in terms of F1 Score .","[('with', (7, 8)), ('achieved', (20, 21)), ('among', (24, 25)), ('in terms of', (27, 30))]","[('our proposed method', (4, 7)), ('Stack = 2 , Base = 40 , Depth = 5', (8, 19)), ('best performance', (22, 24)), ('all solutions', (25, 27)), ('F1 Score', (30, 32))]","[['our proposed method', 'with', 'Stack = 2 , Base = 40 , Depth = 5'], ['Stack = 2 , Base = 40 , Depth = 5', 'achieved', 'best performance'], ['best performance', 'among', 'all solutions'], ['all solutions', 'in terms of', 'F1 Score']]","[['our proposed method', 'has', 'Stack = 2 , Base = 40 , Depth = 5'], ['Stack = 2 , Base = 40 , Depth = 5', 'has', 'best performance']]",[],[],face_detection,12,98
1706,experiments,"Therefore , we can conclude that , compared to the state - of - the - art baselines , WSMA - Seg is much simpler , more effective , and more efficient .","[('compared to', (7, 9))]","[('state - of - the - art baselines', (10, 18)), ('WSMA - Seg', (19, 22)), ('much simpler', (23, 25)), ('more effective', (26, 28)), ('more efficient', (30, 32))]",[],"[['state - of - the - art baselines', 'has', 'WSMA - Seg'], ['WSMA - Seg', 'has', 'much simpler']]",[],[],face_detection,12,100
1707,experiments,WIDER Face Detection,[],"[('WIDER Face Detection', (0, 3))]",[],[],[],[],face_detection,12,101
1708,experiments,WIDER Face results in a much lower detection accuracy compared to other face detection datasets .,"[('results in', (2, 4)), ('compared to', (9, 11))]","[('WIDER Face', (0, 2)), ('much lower detection accuracy', (5, 9)), ('other face detection datasets', (11, 15))]","[['WIDER Face', 'results in', 'much lower detection accuracy'], ['much lower detection accuracy', 'compared to', 'other face detection datasets']]",[],[],[],face_detection,12,104
1709,experiments,"The results show that our proposed WSMA - Seg outperforms the state - of - the - art baselines in all three categories , reaching 94.70 , 93.41 , and 87.23 in Easy , Medium , and Hard categories , respectively .","[('show', (2, 3)), ('outperforms', (9, 10)), ('in', (19, 20)), ('reaching', (24, 25)), ('in', (31, 32))]","[('our proposed WSMA - Seg', (4, 9)), ('state - of - the - art baselines', (11, 19)), ('all three categories', (20, 23)), ('94.70 , 93.41 , and 87.23', (25, 31)), ('Easy , Medium , and Hard categories', (32, 39))]","[['our proposed WSMA - Seg', 'outperforms', 'state - of - the - art baselines'], ['state - of - the - art baselines', 'in', 'all three categories'], ['state - of - the - art baselines', 'reaching', '94.70 , 93.41 , and 87.23'], ['94.70 , 93.41 , and 87.23', 'in', 'Easy , Medium , and Hard categories']]",[],[],[],face_detection,12,111
1710,experiments,MS COCO Detection,[],"[('MS COCO Detection', (0, 3))]",[],[],[],[],face_detection,12,112
1711,experiments,"The results show that our WSMA - Seg approach outperforms all state - of - the - art baselines in terms of most metrics , including the most challenging metrics , AP , AP s , AR 1 , and AR s .","[('outperforms', (9, 10)), ('in terms of', (19, 22)), ('including', (25, 26))]","[('our WSMA - Seg approach', (4, 9)), ('all state - of - the - art baselines', (10, 19)), ('most metrics', (22, 24)), ('AP', (31, 32)), ('AP s', (33, 35)), ('AR', (36, 37)), ('AR s', (40, 42))]","[['our WSMA - Seg approach', 'outperforms', 'all state - of - the - art baselines'], ['all state - of - the - art baselines', 'in terms of', 'most metrics'], ['most metrics', 'including', 'AP'], ['most metrics', 'including', 'AR'], ['most metrics', 'including', 'AR s']]",[],[],[],face_detection,12,130
1712,experiments,"For the other metrics , the performance of our proposed approach is also close to those of the best baselines .","[('For', (0, 1)), ('of', (7, 8))]","[('other metrics', (2, 4)), ('performance', (6, 7)), ('our proposed approach', (8, 11)), ('close', (13, 14)), ('best baselines', (18, 20))]","[['performance', 'of', 'our proposed approach']]","[['other metrics', 'has', 'performance']]",[],[],face_detection,12,131
1713,experiments,This proves that the proposed WSMA - Seg approach generally achieves more accurate and robust object detection than the state - of - the - art approaches without NMS .,"[('proves', (1, 2)), ('generally achieves', (9, 11)), ('than', (17, 18)), ('without', (27, 28))]","[('proposed WSMA - Seg approach', (4, 9)), ('more accurate and robust', (11, 15)), ('object detection', (15, 17)), ('state - of - the - art approaches', (19, 27)), ('NMS', (28, 29))]","[['proposed WSMA - Seg approach', 'generally achieves', 'more accurate and robust'], ['object detection', 'than', 'state - of - the - art approaches'], ['state - of - the - art approaches', 'without', 'NMS']]","[['more accurate and robust', 'has', 'object detection']]",[],[],face_detection,12,132
1714,research-problem,RetinaFace : Single - stage Dense Face Localisation in the Wild,[],"[('Face Localisation in the Wild', (6, 11))]",[],[],[],[],face_detection,13,2
1715,research-problem,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .",[],"[('face localisation', (14, 16))]",[],[],[],[],face_detection,13,4
1716,code,Extra annotations and code have been made available at : https://github.com/deepinsight/insightface/tree/master/RetinaFace .,[],"[('https://github.com/deepinsight/insightface/tree/master/RetinaFace', (10, 11))]",[],[],[],[],face_detection,13,13
1717,model,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .","[('Based on', (0, 2)), ('propose', (9, 10)), ('named', (18, 19)), ('employs', (23, 24)), ('to simultaneously predict', (28, 31)), ('of', (46, 47))]","[('single - stage design', (3, 7)), ('novel pixel - wise face localisation method', (11, 18)), ('Reti- naFace', (19, 21)), ('multi-task learning strategy', (25, 28)), ('face score', (31, 33)), ('face box', (34, 36)), ('five facial landmarks', (37, 40)), ('3D position and correspondence', (42, 46)), ('each facial pixel', (47, 50))]","[['single - stage design', 'propose', 'novel pixel - wise face localisation method'], ['novel pixel - wise face localisation method', 'named', 'Reti- naFace'], ['novel pixel - wise face localisation method', 'employs', 'multi-task learning strategy'], ['multi-task learning strategy', 'to simultaneously predict', 'face score'], ['multi-task learning strategy', 'to simultaneously predict', 'face box'], ['multi-task learning strategy', 'to simultaneously predict', 'five facial landmarks'], ['multi-task learning strategy', 'to simultaneously predict', '3D position and correspondence'], ['3D position and correspondence', 'of', 'each facial pixel']]","[['single - stage design', 'name', 'novel pixel - wise face localisation method'], ['novel pixel - wise face localisation method', 'name', 'Reti- naFace']]","[['Model', 'Based on', 'single - stage design']]",[],face_detection,13,43
1718,experiments,Training Details .,[],"[('Training Details', (0, 2))]",[],[],[],[],face_detection,13,149
1719,experimental-setup,"We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .","[('train', (1, 2)), ('using', (4, 5)), ('at', (9, 10)), ('at', (14, 15)), ('of', (19, 20)), ('on', (23, 24))]","[('RetinaFace', (3, 4)), ('SGD optimiser', (5, 7)), ('momentum', (8, 9)), ('0.9', (10, 11)), ('weight decay', (12, 14)), ('0.0005', (15, 16)), ('batch size', (17, 19)), ('8', (20, 21)), ('four NVIDIA Tesla P40 ( 24GB ) GPUs', (24, 32))]","[['RetinaFace', 'using', 'SGD optimiser'], ['momentum', 'at', '0.9'], ['weight decay', 'at', '0.0005'], ['weight decay', 'at', '0.0005'], ['batch size', 'of', '8']]","[['SGD optimiser', 'has', 'momentum'], ['weight decay', 'has', '0.0005'], ['batch size', 'has', '8']]","[['Experimental setup', 'train', 'RetinaFace']]",[],face_detection,13,150
1720,experimental-setup,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .","[('starts from', (3, 5)), ('rising to', (9, 11)), ('after', (14, 15)), ('at', (22, 23))]","[('learning rate', (1, 3)), ('10 ? 3', (5, 8)), ('10 ? 2', (11, 14)), ('5 epochs', (15, 17))]","[['learning rate', 'starts from', '10 ? 3'], ['learning rate', 'rising to', '10 ? 2'], ['10 ? 3', 'rising to', '10 ? 2'], ['10 ? 2', 'after', '5 epochs']]",[],[],"[['Experimental setup', 'has', 'learning rate']]",face_detection,13,151
1721,experimental-setup,The training process terminates at 80 epochs .,"[('terminates', (3, 4))]","[('training process', (1, 3)), ('80 epochs', (5, 7))]","[['training process', 'terminates', '80 epochs']]",[],[],"[['Experimental setup', 'has', 'training process']]",face_detection,13,152
1722,experimental-setup,Testing Details .,[],"[('Testing Details', (0, 2))]",[],[],[],"[['Experimental setup', 'has', 'Testing Details']]",face_detection,13,153
1723,experimental-setup,"For testing on WIDER FACE , we follow the standard practices of and employ flip as well as multi-scale ( the short edge of image at [ 500 , 800 , 1100 , 1400 , 1700 ] ) strategies .","[('For testing on', (0, 3)), ('of', (11, 12)), ('employ', (13, 14)), ('as well as', (15, 18)), ('at', (25, 26))]","[('WIDER FACE', (3, 5)), ('flip', (14, 15)), ('multi-scale', (18, 19))]","[['WIDER FACE', 'employ', 'flip'], ['flip', 'as well as', 'multi-scale']]",[],"[['Experimental setup', 'For testing on', 'WIDER FACE']]",[],face_detection,13,154
1724,experimental-setup,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,"[('applied on', (6, 8)), ('using', (15, 16))]","[('Box voting', (0, 2)), ('union set', (9, 11)), ('predicted face boxes', (12, 15)), ('IoU threshold', (17, 19)), ('0.4', (20, 21))]","[['Box voting', 'applied on', 'union set'], ['predicted face boxes', 'using', 'IoU threshold']]","[['IoU threshold', 'has', '0.4']]",[],"[['Experimental setup', 'has', 'Box voting']]",face_detection,13,155
1725,ablation-analysis,"Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .","[('Adding', (0, 1)), ('significantly improves', (8, 10)), ('on', (24, 25))]","[('branch of five facial landmark regression', (2, 8)), ('face box AP', (11, 14)), ('0.408 %', (15, 17)), ('mAP', (19, 20)), ('0.775 %', (21, 23)), ('Hard subset', (26, 28))]","[['branch of five facial landmark regression', 'significantly improves', 'face box AP'], ['branch of five facial landmark regression', 'significantly improves', 'mAP'], ['0.775 %', 'on', 'Hard subset']]","[['face box AP', 'has', '0.408 %'], ['mAP', 'has', '0.775 %']]","[['Ablation analysis', 'Adding', 'branch of five facial landmark regression']]",[],face_detection,13,161
1726,ablation-analysis,"By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .","[('adding', (3, 4)), ('increases', (8, 9)), ('on', (13, 14)), ('on', (23, 24))]","[('dense regression branch', (5, 8)), ('face box AP', (10, 13)), ('Easy and Medium subsets', (14, 18)), ('slightly deteriorates', (19, 21)), ('results', (22, 23)), ('Hard subset', (25, 27))]","[['dense regression branch', 'increases', 'face box AP'], ['face box AP', 'on', 'Easy and Medium subsets'], ['results', 'on', 'Hard subset'], ['results', 'on', 'Hard subset']]","[['slightly deteriorates', 'has', 'results']]","[['Ablation analysis', 'adding', 'dense regression branch']]",[],face_detection,13,162
1727,ablation-analysis,"Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .","[('learning', (2, 3)), ('enables', (8, 9)), ('compared to adding', (12, 15))]","[('landmark and dense regression', (3, 7)), ('jointly', (7, 8)), ('further improvement', (10, 12)), ('regression only', (16, 18))]","[['landmark and dense regression', 'enables', 'further improvement'], ['jointly', 'enables', 'further improvement'], ['further improvement', 'compared to adding', 'regression only']]","[['landmark and dense regression', 'has', 'jointly']]","[['Ablation analysis', 'learning', 'landmark and dense regression']]",[],face_detection,13,163
1728,results,Face box Accuracy,[],"[('Face box Accuracy', (0, 3))]",[],[],[],"[['Results', 'has', 'Face box Accuracy']]",face_detection,13,167
1729,results,"More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .","[('produces', (4, 5)), ('in', (8, 9)), ('for', (37, 38))]","[('RetinaFace', (3, 4)), ('best AP', (6, 8)), ('all subsets of both validation and test sets', (9, 17)), ('96.9 %', (20, 22)), ('Easy', (23, 24)), ('96.1 %', (26, 28)), ('Medium', (29, 30)), ('91.8 %', (32, 34)), ('Hard', (35, 36)), ('validation set', (38, 40)), ('96.3 %', (42, 44)), ('Easy', (45, 46)), ('95.6 %', (48, 50)), ('Medium', (51, 52)), ('91.4 %', (54, 56)), ('Hard', (57, 58)), ('test set', (60, 62))]","[['RetinaFace', 'produces', 'best AP'], ['best AP', 'in', 'all subsets of both validation and test sets']]","[['RetinaFace', 'has', 'best AP'], ['96.9 %', 'has', 'Easy'], ['96.1 %', 'has', 'Medium'], ['91.8 %', 'has', 'Hard'], ['96.3 %', 'has', 'Easy'], ['95.6 %', 'has', 'Medium'], ['91.4 %', 'has', 'Hard']]",[],"[['Results', 'has', 'RetinaFace']]",face_detection,13,172
1730,results,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .","[('Compared to', (0, 2)), ('sets up', (12, 14)), ('on', (25, 26))]","[('recent best performed method', (3, 7)), ('Reti - na Face', (8, 12)), ('new impressive record', (15, 18)), ('91.4 % v.s. 90.3 %', (19, 24)), ('Hard subset', (27, 29))]","[['Reti - na Face', 'sets up', 'new impressive record'], ['new impressive record', 'on', 'Hard subset']]","[['recent best performed method', 'has', 'Reti - na Face'], ['new impressive record', 'has', '91.4 % v.s. 90.3 %']]","[['Results', 'Compared to', 'recent best performed method']]",[],face_detection,13,173
1731,results,"RetinaFace successfully finds about 900 faces ( threshold at 0.5 ) out of the reported 1 , 151 faces .","[('successfully finds', (1, 3)), ('at', (8, 9)), ('out of', (11, 13))]","[('about 900 faces', (3, 6)), ('threshold', (7, 8)), ('0.5', (9, 10)), ('reported 1 , 151 faces', (14, 19))]","[['threshold', 'at', '0.5'], ['about 900 faces', 'out of', 'reported 1 , 151 faces']]","[['about 900 faces', 'has', 'threshold']]",[],[],face_detection,13,175
1732,results,Five Facial Landmark Accuracy,[],"[('Five Facial Landmark Accuracy', (0, 4))]",[],[],[],"[['Results', 'has', 'Five Facial Landmark Accuracy']]",face_detection,13,178
1733,results,RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .,"[('significantly decreases', (1, 3)), ('from', (10, 11)), ('to', (13, 14)), ('compared to', (17, 19))]","[('RetinaFace', (0, 1)), ('normalised mean errors ( NME )', (4, 10)), ('2.72 %', (11, 13)), ('2.21 %', (14, 16)), ('MTCNN', (19, 20))]","[['RetinaFace', 'significantly decreases', 'normalised mean errors ( NME )'], ['normalised mean errors ( NME )', 'from', '2.72 %'], ['2.72 %', 'to', '2.21 %'], ['normalised mean errors ( NME )', 'compared to', 'MTCNN'], ['2.21 %', 'compared to', 'MTCNN']]",[],[],[],face_detection,13,183
1734,results,"Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .","[('Compared to', (0, 2)), ('significantly decreases', (5, 7)), ('from', (10, 11)), ('to', (13, 14))]","[('MTCNN', (2, 3)), ('RetinaFace', (4, 5)), ('failure rate', (8, 10)), ('26.31 %', (11, 13)), ('9.37 %', (14, 16))]","[['RetinaFace', 'significantly decreases', 'failure rate'], ['failure rate', 'from', '26.31 %'], ['failure rate', 'to', '9.37 %'], ['26.31 %', 'to', '9.37 %']]","[['MTCNN', 'has', 'RetinaFace']]","[['Results', 'Compared to', 'MTCNN']]",[],face_detection,13,185
1735,results,Dense Facial Landmark Accuracy,[],"[('Dense Facial Landmark Accuracy', (0, 4))]",[],[],[],"[['Results', 'has', 'Dense Facial Landmark Accuracy']]",face_detection,13,186
1736,results,"Even though the performance gap exists between supervised and self - supervised methods , the dense regression results of RetinaFace are comparable with these state - of - the - art methods .","[('of', (18, 19)), ('comparable with', (21, 23))]","[('dense regression results', (15, 18)), ('RetinaFace', (19, 20)), ('state - of - the - art methods', (24, 32))]","[['dense regression results', 'of', 'RetinaFace'], ['dense regression results', 'comparable with', 'state - of - the - art methods']]",[],[],[],face_detection,13,191
1737,ablation-analysis,"More specifically , we observe that ( 1 ) five facial landmarks regression can alleviate the training difficulty of dense regression branch and significantly improve the dense regression results .","[('observe', (4, 5)), ('alleviate', (14, 15)), ('of', (18, 19)), ('significantly improve', (23, 25))]","[('five facial landmarks regression', (9, 13)), ('training difficulty', (16, 18)), ('dense regression branch', (19, 22)), ('dense regression results', (26, 29))]","[['five facial landmarks regression', 'alleviate', 'training difficulty'], ['training difficulty', 'of', 'dense regression branch'], ['five facial landmarks regression', 'significantly improve', 'dense regression results']]",[],"[['Ablation analysis', 'observe', 'five facial landmarks regression']]",[],face_detection,13,192
1738,ablation-analysis,( 2 ) using single - stage features ( as in RetinaFace ) to predict dense correspondence parameters is much harder than employing ( Region of Interest ) RoI features ( as in Mesh Decoder ) .,"[('using', (3, 4)), ('as in', (9, 11)), ('to predict', (13, 15)), ('than employing', (21, 23)), ('as in', (31, 33))]","[('single - stage features', (4, 8)), ('RetinaFace', (11, 12)), ('dense correspondence parameters', (15, 18)), ('much harder', (19, 21)), ('( Region of Interest ) RoI features', (23, 30)), ('Mesh Decoder', (33, 35))]","[['single - stage features', 'as in', 'RetinaFace'], ['single - stage features', 'to predict', 'dense correspondence parameters'], ['much harder', 'than employing', '( Region of Interest ) RoI features'], ['( Region of Interest ) RoI features', 'as in', 'Mesh Decoder']]","[['single - stage features', 'name', 'RetinaFace']]","[['Ablation analysis', 'using', 'single - stage features']]",[],face_detection,13,193
1739,results,Face Recognition Accuracy,[],"[('Face Recognition Accuracy', (0, 3))]",[],[],[],"[['Results', 'has', 'Face Recognition Accuracy']]",face_detection,13,197
1740,results,"The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .","[('on', (2, 3)), ('demonstrate', (7, 8)), ('boost', (14, 15)), ('from', (19, 20)), ('to', (22, 23))]","[('CFP - FP', (3, 6)), ('Reti - na Face', (9, 13)), (""ArcFace 's verification accuracy"", (15, 19)), ('98.37 %', (20, 22)), ('99.49 %', (23, 25))]","[['CFP - FP', 'demonstrate', 'Reti - na Face'], ['Reti - na Face', 'boost', ""ArcFace 's verification accuracy""], [""ArcFace 's verification accuracy"", 'from', '98.37 %'], ['98.37 %', 'to', '99.49 %']]","[['CFP - FP', 'has', 'Reti - na Face']]","[['Results', 'on', 'CFP - FP']]",[],face_detection,13,204
1741,research-problem,WIDER FACE : A Face Detection Benchmark,[],"[('Face Detection', (4, 6))]",[],[],[],[],face_detection,14,2
1742,research-problem,We introduce a large - scale face detection dataset called WIDER FACE .,"[('introduce', (1, 2)), ('called', (9, 10))]","[('large - scale face detection dataset', (3, 9)), ('WIDER FACE', (10, 12))]","[['large - scale face detection dataset', 'called', 'WIDER FACE']]",[],"[['Research problem', 'introduce', 'large - scale face detection dataset']]",[],face_detection,14,32
1743,experiments,"It consists of 32 , 203 images with 393 , 703 labeled faces , which is 10 times larger than the current largest face detection dataset .","[('consists of', (1, 3)), ('with', (7, 8)), ('than', (19, 20))]","[('32 , 203 images', (3, 7)), ('393 , 703 labeled faces', (8, 13)), ('10 times larger', (16, 19)), ('current largest face detection dataset', (21, 26))]","[['32 , 203 images', 'with', '393 , 703 labeled faces'], ['10 times larger', 'than', 'current largest face detection dataset']]",[],[],[],face_detection,14,33
1744,experiments,"The faces vary largely in appearance , pose , and scale , as shown in .","[('in', (4, 5)), ('pose', (7, 8))]","[('vary largely', (2, 4)), ('appearance', (5, 6)), ('scale', (10, 11))]","[['vary largely', 'in', 'appearance']]",[],[],[],face_detection,14,34
1745,model,"In order to quantify different types of errors , we annotate multiple attributes : occlusion , pose , and event categories , which allows in depth analysis of existing algorithms .","[('to quantify', (2, 4)), ('annotate', (10, 11)), ('allows', (23, 24)), ('of', (27, 28))]","[('different types of errors', (4, 8)), ('multiple attributes', (11, 13)), ('occlusion', (14, 15)), ('pose', (16, 17)), ('event categories', (19, 21)), ('in depth analysis', (24, 27)), ('existing algorithms', (28, 30))]","[['different types of errors', 'annotate', 'multiple attributes'], ['event categories', 'allows', 'in depth analysis'], ['in depth analysis', 'of', 'existing algorithms']]","[['multiple attributes', 'name', 'occlusion']]","[['Model', 'to quantify', 'different types of errors']]",[],face_detection,14,35
1746,baselines,"We select VJ , ACF , DPM , and Faceness as baselines .","[('select', (1, 2))]","[('VJ', (2, 3)), ('ACF', (4, 5)), ('DPM', (6, 7)), ('Faceness', (9, 10))]",[],[],"[['Baselines', 'select', 'VJ']]",[],face_detection,14,149
1747,results,Overall .,[],"[('Overall', (0, 1))]",[],[],[],"[['Results', 'has', 'Overall']]",face_detection,14,155
1748,results,"Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .","[('outperforms', (1, 2)), ('on', (4, 5)), ('with', (8, 9)), ('as', (12, 13))]","[('Faceness', (0, 1)), ('other methods', (2, 4)), ('three subsets', (5, 7)), ('DPM and ACF', (9, 12)), ('marginal second and third', (13, 17))]","[['Faceness', 'outperforms', 'other methods'], ['other methods', 'on', 'three subsets'], ['other methods', 'with', 'DPM and ACF'], ['three subsets', 'with', 'DPM and ACF'], ['DPM and ACF', 'as', 'marginal second and third']]",[],[],[],face_detection,14,158
1749,results,"For the easy set , the average precision ( AP ) of most methods are over 60 % , but none of them surpasses 75 % .","[('For', (0, 1)), ('of', (11, 12))]","[('easy set', (2, 4)), ('average precision ( AP )', (6, 11)), ('most methods', (12, 14)), ('over 60 %', (15, 18))]","[['average precision ( AP )', 'of', 'most methods']]","[['easy set', 'has', 'average precision ( AP )']]","[['Results', 'For', 'easy set']]",[],face_detection,14,159
1750,results,The performance drops 10 % for all methods on the medium set .,"[('on', (8, 9))]","[('performance', (1, 2)), ('drops 10 %', (2, 5)), ('medium set', (10, 12))]","[['drops 10 %', 'on', 'medium set']]","[['performance', 'has', 'drops 10 %']]",[],"[['Results', 'has', 'performance']]",face_detection,14,160
1751,results,The hard set is even more challenging .,[],"[('hard set', (1, 3)), ('even more challenging', (4, 7))]",[],"[['hard set', 'has', 'even more challenging']]",[],[],face_detection,14,161
1752,results,Scale .,[],"[('Scale', (0, 1))]",[],[],[],[],face_detection,14,164
1753,results,The results of small scale are abysmal : none of the algorithms is able to achieve more than 12 % AP .,"[('of', (2, 3)), ('able to achieve', (13, 16))]","[('results', (1, 2)), ('small scale', (3, 5)), ('abysmal', (6, 7)), ('none of the algorithms', (8, 12)), ('more than 12 % AP', (16, 21))]","[['results', 'of', 'small scale'], ['none of the algorithms', 'able to achieve', 'more than 12 % AP']]","[['results', 'has', 'small scale'], ['small scale', 'has', 'abysmal'], ['abysmal', 'has', 'none of the algorithms']]",[],"[['Results', 'has', 'results']]",face_detection,14,166
1754,results,Occlusion .,[],"[('Occlusion', (0, 1))]",[],[],[],"[['Results', 'has', 'Occlusion']]",face_detection,14,168
1755,results,"With partial occlusion , the performance drops significantly .","[('With', (0, 1))]","[('partial occlusion', (1, 3)), ('performance', (5, 6)), ('drops significantly', (6, 8))]",[],"[['partial occlusion', 'has', 'performance'], ['performance', 'has', 'drops significantly']]","[['Results', 'With', 'partial occlusion']]",[],face_detection,14,172
1756,results,The maximum AP is only 26.5 % achieved by Faceness .,"[('achieved by', (7, 9))]","[('maximum AP', (1, 3)), ('only 26.5 %', (4, 7)), ('Faceness', (9, 10))]","[['only 26.5 %', 'achieved by', 'Faceness']]","[['maximum AP', 'has', 'only 26.5 %']]",[],"[['Results', 'has', 'maximum AP']]",face_detection,14,173
1757,results,Pose .,[],"[('Pose', (0, 1))]",[],[],[],"[['Results', 'has', 'Pose']]",face_detection,14,177
1758,results,"The best performance is achieved by Faceness , with a recall below 20 % .","[('achieved by', (4, 6)), ('with', (8, 9))]","[('best performance', (1, 3)), ('Faceness', (6, 7)), ('recall', (10, 11)), ('below 20 %', (11, 14))]","[['best performance', 'achieved by', 'Faceness'], ['Faceness', 'with', 'recall']]","[['recall', 'has', 'below 20 %']]",[],"[['Results', 'has', 'best performance']]",face_detection,14,183
1759,research-problem,"Although tremendous strides have been made in face detection , one of the remaining open challenges is to achieve real - time speed on the CPU as well as maintain high performance , since effective models for face detection tend to be computationally prohibitive .",[],"[('face detection', (7, 9))]",[],[],[],[],face_detection,15,4
1760,code,Code is available at https://github.com/sfzhang15/FaceBoxes .,[],"[('https://github.com/sfzhang15/FaceBoxes', (4, 5))]",[],[],[],[],face_detection,15,13
1761,model,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .","[('develop', (21, 22)), ('with', (32, 33)), ('on', (37, 38))]","[('state - of - the - art face detector', (23, 32)), ('real - time speed', (33, 37)), ('CPU', (39, 40))]","[['state - of - the - art face detector', 'with', 'real - time speed'], ['real - time speed', 'on', 'CPU']]",[],"[['Model', 'develop', 'state - of - the - art face detector']]",[],face_detection,15,41
1762,model,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .","[('propose', (3, 4)), ('named', (8, 9)), ('contains', (13, 14)), ('trained', (23, 24))]","[('novel face detector', (5, 8)), ('FaceBoxes', (9, 10)), ('single fully convolutional neural network', (15, 20)), ('end - to - end', (24, 29))]","[['novel face detector', 'named', 'FaceBoxes'], ['FaceBoxes', 'contains', 'single fully convolutional neural network'], ['novel face detector', 'trained', 'end - to - end']]","[['novel face detector', 'name', 'FaceBoxes']]","[['Model', 'propose', 'novel face detector']]",[],face_detection,15,42
1763,model,The proposed method has a lightweight yet powerful network structure ( as shown in ) that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .,"[('consists of', (16, 18))]","[('proposed method', (1, 3)), ('lightweight yet powerful network structure', (5, 10)), ('Rapidly Digested Convolutional Layers ( RDCL )', (19, 26)), ('Multiple Scale Convolutional Layers ( MSCL )', (28, 35))]","[['lightweight yet powerful network structure', 'consists of', 'Rapidly Digested Convolutional Layers ( RDCL )'], ['lightweight yet powerful network structure', 'consists of', 'Multiple Scale Convolutional Layers ( MSCL )']]","[['proposed method', 'has', 'lightweight yet powerful network structure']]",[],"[['Model', 'has', 'proposed method']]",face_detection,15,43
1764,model,"The RDCL is designed to enable FaceBoxes to achieve real - time speed on the CPU , and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces .","[('designed to', (3, 5)), ('enable', (5, 6)), ('to achieve', (7, 9)), ('on', (13, 14)), ('aims at', (20, 22)), ('discretizing', (27, 28)), ('over', (29, 30)), ('to handle', (32, 34))]","[('RDCL', (1, 2)), ('FaceBoxes', (6, 7)), ('real - time speed', (9, 13)), ('CPU', (15, 16)), ('MSCL', (19, 20)), ('enriching', (22, 23)), ('receptive fields', (24, 26)), ('anchors', (28, 29)), ('different layers', (30, 32)), ('various scales of faces', (34, 38))]","[['RDCL', 'enable', 'FaceBoxes'], ['FaceBoxes', 'to achieve', 'real - time speed'], ['real - time speed', 'on', 'CPU'], ['MSCL', 'aims at', 'enriching'], ['MSCL', 'discretizing', 'anchors'], ['anchors', 'over', 'different layers'], ['different layers', 'to handle', 'various scales of faces']]","[['enriching', 'has', 'receptive fields']]",[],"[['Model', 'has', 'RDCL']]",face_detection,15,44
1765,model,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the input image , which significantly improves the recall rate of small faces .","[('to make', (9, 11)), ('have', (15, 16)), ('on', (19, 20))]","[('new anchor densification strategy', (5, 9)), ('different types of anchors', (11, 15)), ('same density', (17, 19)), ('input image', (21, 23))]","[['new anchor densification strategy', 'to make', 'different types of anchors'], ['different types of anchors', 'have', 'same density'], ['same density', 'on', 'input image']]","[['different types of anchors', 'has', 'same density']]",[],[],face_detection,15,45
1766,experimental-setup,"We first filter out most boxes by a confidence threshold of 0.05 and keep the top 400 boxes before applying NMS , then we perform NMS with jaccard overlap of 0.3 and keep the top 200 boxes .","[('filter out', (2, 4)), ('by', (6, 7)), ('of', (10, 11)), ('keep', (13, 14)), ('before applying', (18, 20)), ('perform', (24, 25)), ('with', (26, 27)), ('of', (29, 30)), ('keep', (32, 33))]","[('most boxes', (4, 6)), ('confidence threshold', (8, 10)), ('0.05', (11, 12)), ('top 400 boxes', (15, 18)), ('NMS', (20, 21)), ('NMS', (25, 26)), ('jaccard overlap', (27, 29)), ('0.3', (30, 31)), ('top 200 boxes', (34, 37))]","[['most boxes', 'by', 'confidence threshold'], ['confidence threshold', 'of', '0.05'], ['jaccard overlap', 'of', '0.3'], ['top 400 boxes', 'before applying', 'NMS'], ['NMS', 'with', 'jaccard overlap'], ['jaccard overlap', 'of', '0.3'], ['NMS', 'keep', 'top 200 boxes']]",[],"[['Experimental setup', 'filter out', 'most boxes']]",[],face_detection,15,176
1767,experimental-setup,We measure the speed using Titan X ( Pascal ) and cuDNN v 5.1 with Intel Xeon E5-2660v3@2.60 GHz .,"[('measure', (1, 2)), ('using', (4, 5)), ('with', (14, 15))]","[('speed', (3, 4)), ('Titan X ( Pascal ) and cuDNN v 5.1', (5, 14)), ('Intel Xeon E5-2660v3@2.60 GHz', (15, 19))]","[['speed', 'using', 'Titan X ( Pascal ) and cuDNN v 5.1'], ['Titan X ( Pascal ) and cuDNN v 5.1', 'with', 'Intel Xeon E5-2660v3@2.60 GHz']]",[],"[['Experimental setup', 'measure', 'speed']]",[],face_detection,15,177
1768,ablation-analysis,FaceBoxes Anchor densification strategy is crucial .,[],"[('FaceBoxes Anchor densification strategy', (0, 4)), ('crucial', (5, 6))]",[],"[['FaceBoxes Anchor densification strategy', 'has', 'crucial']]",[],"[['Ablation analysis', 'has', 'FaceBoxes Anchor densification strategy']]",face_detection,15,194
1769,ablation-analysis,"Our anchor densification strategy is used to increase the density of small anchors ( i.e. , 32 32 and 64 64 ) in order to improve the recall rate of small faces .","[('used to', (5, 7)), ('of', (10, 11)), ('to improve', (24, 26)), ('of', (29, 30))]","[('Our anchor densification strategy', (0, 4)), ('increase', (7, 8)), ('density', (9, 10)), ('small anchors', (11, 13)), ('recall rate', (27, 29)), ('small faces', (30, 32))]","[['Our anchor densification strategy', 'used to', 'increase'], ['density', 'of', 'small anchors'], ['small anchors', 'to improve', 'recall rate'], ['recall rate', 'of', 'small faces']]","[['increase', 'has', 'density']]",[],"[['Ablation analysis', 'has', 'Our anchor densification strategy']]",face_detection,15,195
1770,ablation-analysis,"2 , we can see that the m AP on FDDB is reduced from 96.0 % to 94.9 % after ablating the anchor densification strategy .","[('see', (4, 5)), ('on', (9, 10)), ('from', (13, 14)), ('to', (16, 17)), ('after ablating', (19, 21))]","[('m AP', (7, 9)), ('FDDB', (10, 11)), ('reduced', (12, 13)), ('96.0 %', (14, 16)), ('94.9 %', (17, 19)), ('anchor densification strategy', (22, 25))]","[['m AP', 'on', 'FDDB'], ['reduced', 'from', '96.0 %'], ['96.0 %', 'to', '94.9 %'], ['94.9 %', 'after ablating', 'anchor densification strategy']]",[],"[['Ablation analysis', 'see', 'm AP']]",[],face_detection,15,197
1771,ablation-analysis,RDCL is efficient and accuracy - preserving .,[],"[('RDCL', (0, 1)), ('efficient and accuracy - preserving', (2, 7))]",[],"[['RDCL', 'has', 'efficient and accuracy - preserving']]",[],"[['Ablation analysis', 'has', 'RDCL']]",face_detection,15,202
1772,results,AFW dataset .,[],"[('AFW dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'AFW dataset']]",face_detection,15,209
1773,results,"As illustrated in , our FaceBoxes outperforms all others by a large margin .","[('outperforms', (6, 7)), ('by', (9, 10))]","[('our FaceBoxes', (4, 6)), ('large margin', (11, 13))]",[],[],[],"[['Results', 'has', 'our FaceBoxes']]",face_detection,15,212
1774,results,PASCAL face dataset .,[],"[('PASCAL face dataset', (0, 3))]",[],[],[],"[['Results', 'has', 'PASCAL face dataset']]",face_detection,15,214
1775,results,"Our method significantly outperforms all other methods and commercial face detectors ( e.g. , SkyBiometry , Face + + and Picasa ) .","[('significantly outperforms', (2, 4))]","[('Our method', (0, 2)), ('all other methods', (4, 7)), ('commercial face detectors', (8, 11))]","[['Our method', 'significantly outperforms', 'all other methods'], ['Our method', 'significantly outperforms', 'commercial face detectors']]",[],[],"[['Results', 'has', 'Our method']]",face_detection,15,217
1776,results,FDDB dataset .,[],"[('FDDB dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'FDDB dataset']]",face_detection,15,219
1777,results,Our FaceBoxes achieves the state - of - the - art performance and outperforms all others by a large margin on discontinuous and continuous ROC curves .,"[('achieves', (2, 3)), ('by', (16, 17)), ('on', (20, 21))]","[('Our FaceBoxes', (0, 2)), ('state - of - the - art performance', (4, 12)), ('outperforms', (13, 14)), ('large margin', (18, 20)), ('discontinuous and continuous ROC curves', (21, 26))]","[['Our FaceBoxes', 'achieves', 'state - of - the - art performance'], ['Our FaceBoxes', 'achieves', 'outperforms'], ['outperforms', 'by', 'large margin'], ['large margin', 'on', 'discontinuous and continuous ROC curves']]","[['outperforms', 'has', 'large margin']]",[],"[['Results', 'has', 'Our FaceBoxes']]",face_detection,15,226
1778,research-problem,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition",[],"[('Face Detection', (8, 10)), ('Landmark Localization', (11, 13)), ('Pose Estimation', (14, 16)), ('Gender Recognition', (18, 20))]",[],[],[],[],face_detection,16,2
1779,model,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .","[('present', (5, 6)), ('based on', (9, 11)), ('for simultaneous', (12, 14))]","[('novel framework', (7, 9)), ('CNNs', (11, 12)), ('face detection', (14, 16)), ('facial landmarks localization', (17, 20)), ('head pose estimation', (21, 24)), ('gender recognition', (25, 27))]","[['novel framework', 'based on', 'CNNs'], ['CNNs', 'for simultaneous', 'face detection'], ['CNNs', 'for simultaneous', 'head pose estimation'], ['CNNs', 'for simultaneous', 'gender recognition']]",[],"[['Model', 'present', 'novel framework']]",[],face_detection,16,15
1780,model,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,"[('design', (1, 2)), ('to learn', (5, 7)), ('exploit', (13, 14))]","[('CNN architecture', (3, 5)), ('common features', (7, 9)), ('synergy', (15, 16))]","[['CNN architecture', 'to learn', 'common features'], ['CNN architecture', 'exploit', 'synergy']]",[],"[['Model', 'design', 'CNN architecture']]",[],face_detection,16,16
1781,model,We refer the set of intermediate layer features as hyperfeatures .,"[('refer', (1, 2)), ('of', (4, 5)), ('as', (8, 9))]","[('set', (3, 4)), ('intermediate layer features', (5, 8)), ('hyperfeatures', (9, 10))]","[['set', 'of', 'intermediate layer features'], ['intermediate layer features', 'as', 'hyperfeatures']]",[],"[['Model', 'refer', 'set']]",[],face_detection,16,22
1782,model,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .","[('construct', (3, 4)), ('to fuse', (9, 11))]","[('separate fusion - CNN', (5, 9)), ('hyperfeatures', (12, 13))]","[['separate fusion - CNN', 'to fuse', 'hyperfeatures']]",[],"[['Model', 'construct', 'separate fusion - CNN']]",[],face_detection,16,29
1783,model,"In order to learn the tasks , we train them simultaneously using multiple loss functions .","[('to learn', (2, 4)), ('train', (8, 9)), ('using', (11, 12))]","[('tasks', (5, 6)), ('simultaneously', (10, 11)), ('multiple loss functions', (12, 15))]","[['tasks', 'train', 'simultaneously'], ['simultaneously', 'using', 'multiple loss functions']]",[],"[['Model', 'to learn', 'tasks']]",[],face_detection,16,30
1784,model,The deep CNN combined with the fusion - CNN can be learned together in an end -toend fashion .,"[('combined with', (3, 5)), ('learned together', (11, 13)), ('in', (13, 14))]","[('deep CNN', (1, 3)), ('fusion - CNN', (6, 9)), ('end -toend fashion', (15, 18))]","[['deep CNN', 'combined with', 'fusion - CNN']]",[],[],"[['Model', 'has', 'deep CNN']]",face_detection,16,32
1785,experiments,Face Detection,[],"[('Face Detection', (0, 2))]",[],[],[],[],face_detection,16,321
1786,experiments,"As can be seen from these figures , both HyperFace and HF - ResNet outperform all the reported academic and commercial detectors on the AFW and PASCAL datasets .","[('outperform', (14, 15)), ('on', (22, 23))]","[('both HyperFace and HF - ResNet', (8, 14)), ('all the reported academic and commercial detectors', (15, 22)), ('AFW and PASCAL datasets', (24, 28))]","[['both HyperFace and HF - ResNet', 'outperform', 'all the reported academic and commercial detectors'], ['all the reported academic and commercial detectors', 'on', 'AFW and PASCAL datasets']]",[],[],[],face_detection,16,333
1787,experiments,"HyperFace achieves a high mean average precision ( m AP ) of 97.9 % and 92.46 % , for AFW and PASCAL datasets respectively .","[('achieves', (1, 2)), ('of', (11, 12)), ('for', (18, 19))]","[('HyperFace', (0, 1)), ('high mean average precision ( m AP )', (3, 11)), ('97.9 % and 92.46 %', (12, 17)), ('AFW and PASCAL datasets', (19, 23))]","[['HyperFace', 'achieves', 'high mean average precision ( m AP )'], ['high mean average precision ( m AP )', 'of', '97.9 % and 92.46 %'], ['97.9 % and 92.46 %', 'for', 'AFW and PASCAL datasets']]",[],[],[],face_detection,16,334
1788,experiments,HF - ResNet further improves the m AP to 99.4 % and 96.2 %,"[('further improves', (3, 5)), ('to', (8, 9))]","[('HF - ResNet', (0, 3)), ('m AP', (6, 8)), ('99.4 % and 96.2 %', (9, 14))]","[['HF - ResNet', 'further improves', 'm AP'], ['m AP', 'to', '99.4 % and 96.2 %']]",[],[],[],face_detection,16,335
1789,experiments,"In spite of these issues , HyperFace performance is comparable to recently published deep learning - based face detection methods such as DP2MFD and Faceness on the FDDB dataset 1 with m AP of 90.1 % .","[('of', (2, 3)), ('comparable to', (9, 11)), ('on', (25, 26)), ('with', (30, 31))]","[('HyperFace performance', (6, 8)), ('DP2MFD', (22, 23)), ('Faceness', (24, 25)), ('FDDB dataset', (27, 29)), ('m AP', (31, 33)), ('90.1 %', (34, 36))]","[['m AP', 'of', '90.1 %'], ['Faceness', 'on', 'FDDB dataset']]",[],[],[],face_detection,16,340
1790,experiments,clearly show that multitask CNNs ( Multitask Face and HyperFace ) outperform R - CNN Face by a wide margin .,"[('show', (1, 2)), ('outperform', (11, 12)), ('by', (16, 17))]","[('multitask CNNs', (3, 5)), ('Multitask Face', (6, 8)), ('HyperFace', (9, 10)), ('R - CNN Face', (12, 16)), ('wide margin', (18, 20))]","[['multitask CNNs', 'outperform', 'R - CNN Face'], ['R - CNN Face', 'by', 'wide margin']]","[['multitask CNNs', 'name', 'Multitask Face']]",[],[],face_detection,16,342
1791,experiments,Landmarks Localization,[],"[('Landmarks Localization', (0, 2))]",[],[],[],[],face_detection,16,350
1792,experiments,shows that HyperFace performs consistently accurate overall pose angles .,"[('shows', (0, 1)), ('performs', (3, 4))]","[('HyperFace', (2, 3)), ('consistently accurate', (4, 6)), ('overall', (6, 7)), ('pose angles', (7, 9))]","[['HyperFace', 'performs', 'consistently accurate']]","[['consistently accurate', 'has', 'overall'], ['overall', 'has', 'pose angles']]",[],[],face_detection,16,370
1793,experiments,"Moreover , we find that R - CNN Fiducial and Multitask Face attain similar performance .","[('find', (3, 4)), ('attain', (12, 13))]","[('R - CNN Fiducial and Multitask Face', (5, 12)), ('similar performance', (13, 15))]","[['R - CNN Fiducial and Multitask Face', 'attain', 'similar performance']]",[],[],[],face_detection,16,372
1794,experiments,"Additionally , we observe that HF - ResNet significantly improves the performance over HyperFace for both AFW and AFLW datasets .","[('significantly improves', (8, 10)), ('over', (12, 13)), ('for both', (14, 16))]","[('performance', (11, 12)), ('HyperFace', (13, 14)), ('AFW and AFLW datasets', (16, 20))]","[['performance', 'over', 'HyperFace'], ['HyperFace', 'for both', 'AFW and AFLW datasets']]",[],[],[],face_detection,16,376
1795,experiments,"We observe that HyperFace achieves a comparable NME of 10.88 , while HF - ResNet achieves the state - of - theart result on IBUG with NME of 8.18 .","[('observe', (1, 2)), ('achieves', (4, 5)), ('of', (8, 9)), ('achieves', (15, 16)), ('on', (23, 24)), ('with', (25, 26)), ('of', (27, 28))]","[('HyperFace', (3, 4)), ('comparable NME', (6, 8)), ('10.88', (9, 10)), ('HF - ResNet', (12, 15)), ('state - of - theart result', (17, 23)), ('IBUG', (24, 25)), ('NME', (26, 27)), ('8.18', (28, 29))]","[['HyperFace', 'achieves', 'comparable NME'], ['comparable NME', 'of', '10.88'], ['NME', 'of', '8.18'], ['HF - ResNet', 'achieves', 'state - of - theart result'], ['state - of - theart result', 'on', 'IBUG'], ['IBUG', 'with', 'NME'], ['NME', 'of', '8.18']]",[],[],[],face_detection,16,389
1796,experiments,Pose Estimation,[],"[('Pose Estimation', (0, 2))]",[],[],[],[],face_detection,16,393
1797,experiments,"As can be seen from the figure , both HyperFace and HF - ResNet outperform existing methods by a large margin .","[('outperform', (14, 15)), ('by', (17, 18))]","[('both HyperFace and HF - ResNet', (8, 14)), ('existing methods', (15, 17)), ('large margin', (19, 21))]","[['both HyperFace and HF - ResNet', 'outperform', 'existing methods'], ['existing methods', 'by', 'large margin']]",[],[],[],face_detection,16,401
1798,results,"HF - ResNet further improves the performance for roll , pitch as well as yaw .","[('further improves', (3, 5)), ('for', (7, 8))]","[('HF - ResNet', (0, 3)), ('performance', (6, 7)), ('roll', (8, 9)), ('pitch', (10, 11)), ('yaw', (14, 15))]","[['HF - ResNet', 'further improves', 'performance'], ['performance', 'for', 'roll'], ['performance', 'for', 'yaw']]",[],[],"[['Results', 'has', 'HF - ResNet']]",face_detection,16,409
1799,results,Gender Recognition,[],"[('Gender Recognition', (0, 2))]",[],[],[],"[['Results', 'has', 'Gender Recognition']]",face_detection,16,410
1800,experiments,"On the LFWA dataset , our method outperforms PANDA and FaceTracer , and is equal to .","[('On', (0, 1)), ('outperforms', (7, 8))]","[('LFWA dataset', (2, 4)), ('our method', (5, 7)), ('PANDA and FaceTracer', (8, 11))]","[['our method', 'outperforms', 'PANDA and FaceTracer']]","[['LFWA dataset', 'has', 'our method']]",[],[],face_detection,16,418
1801,results,HF - ResNet achieves state - of - the - art results on both CelebA and LFWA datasets .,"[('achieves', (3, 4)), ('on both', (12, 14))]","[('HF - ResNet', (0, 3)), ('state - of - the - art results', (4, 12)), ('CelebA and LFWA datasets', (14, 18))]","[['HF - ResNet', 'achieves', 'state - of - the - art results'], ['state - of - the - art results', 'on both', 'CelebA and LFWA datasets']]",[],[],"[['Results', 'has', 'HF - ResNet']]",face_detection,16,423
1802,results,The HyperFace with a linear bounding box regression and traditional NMS achieves a m AP of 94 % .,"[('with', (2, 3)), ('achieves', (11, 12)), ('of', (15, 16))]","[('HyperFace', (1, 2)), ('linear bounding box regression', (4, 8)), ('traditional NMS', (9, 11)), ('m AP', (13, 15)), ('94 %', (16, 18))]","[['HyperFace', 'with', 'linear bounding box regression'], ['traditional NMS', 'achieves', 'm AP'], ['m AP', 'of', '94 %']]",[],[],[],face_detection,16,427
1803,research-problem,"Face detection has been well studied for many years and one of remaining challenges is to detect small , blurred and partially occluded faces in uncontrolled environment .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,17,4
1804,code,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,[],"[('https://github.com/PaddlePaddle/models/tree/develop/', (9, 10))]",[],[],[],[],face_detection,17,12
1805,model,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .","[('use', (5, 6)), ('to generate', (9, 11)), ('for', (13, 14)), ('related to', (16, 18)), ('called', (24, 25)), ('invented to be', (27, 30)), ('to', (32, 33))]","[('semi-supervised solution', (7, 9)), ('approximate labels', (11, 13)), ('contextual parts', (14, 16)), ('faces and a series of anchors', (18, 24)), ('PyramidAnchors', (25, 26)), ('easily added', (30, 32)), ('general anchor - based architectures', (33, 38))]","[['semi-supervised solution', 'to generate', 'approximate labels'], ['approximate labels', 'for', 'contextual parts'], ['contextual parts', 'related to', 'faces and a series of anchors'], ['faces and a series of anchors', 'called', 'PyramidAnchors'], ['PyramidAnchors', 'invented to be', 'easily added'], ['easily added', 'to', 'general anchor - based architectures']]",[],"[['Model', 'use', 'semi-supervised solution']]",[],face_detection,17,31
1806,model,We investigate the performance of Feature Pyramid Networks ( FPN ) and modify it into a Low - level Feature Pyramid Network ( LFPN ) to join mutually helpful features together .,"[('investigate', (1, 2)), ('of', (4, 5)), ('modify', (12, 13)), ('into', (14, 15)), ('to join', (25, 27))]","[('performance', (3, 4)), ('Feature Pyramid Networks ( FPN )', (5, 11)), ('Low - level Feature Pyramid Network ( LFPN )', (16, 25)), ('mutually helpful features', (27, 30))]","[['performance', 'of', 'Feature Pyramid Networks ( FPN )'], ['Low - level Feature Pyramid Network ( LFPN )', 'to join', 'mutually helpful features']]",[],"[['Model', 'investigate', 'performance']]",[],face_detection,17,34
1807,model,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,"[('introduce', (1, 2)), ('to incorporate', (11, 13)), ('around', (15, 16)), ('with', (19, 20))]","[('Context - sensitive prediction module ( CPM )', (3, 11)), ('context information', (13, 15)), ('target face', (17, 19)), ('wider and deeper network', (21, 25))]","[['Context - sensitive prediction module ( CPM )', 'to incorporate', 'context information'], ['context information', 'around', 'target face'], ['context information', 'with', 'wider and deeper network'], ['target face', 'with', 'wider and deeper network']]",[],"[['Model', 'introduce', 'Context - sensitive prediction module ( CPM )']]",[],face_detection,17,36
1808,model,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .","[('propose', (3, 4)), ('for', (11, 12)), ('to further improve', (15, 18)), ('of', (20, 21))]","[('max - in - out layer', (5, 11)), ('prediction module', (13, 15)), ('capability', (19, 20)), ('classification network', (21, 23))]","[['max - in - out layer', 'for', 'prediction module'], ['max - in - out layer', 'to further improve', 'capability'], ['prediction module', 'to further improve', 'capability'], ['capability', 'of', 'classification network']]",[],"[['Model', 'propose', 'max - in - out layer']]",[],face_detection,17,37
1809,model,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .","[('named', (8, 9)), ('to make', (15, 17)), ('on', (19, 20)), ('of', (22, 23))]","[('training strategy', (6, 8)), ('Data - anchor - sampling', (10, 15)), ('adjustment', (18, 19)), ('distribution', (21, 22)), ('training dataset', (24, 26))]","[['Data - anchor - sampling', 'to make', 'adjustment'], ['adjustment', 'on', 'distribution'], ['distribution', 'of', 'training dataset']]","[['training strategy', 'name', 'Data - anchor - sampling']]",[],[],face_detection,17,38
1810,results,FDDB Dataset .,[],"[('FDDB Dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'FDDB Dataset']]",face_detection,17,219
1811,results,The PyramidBox achieves state - ofart performance and the result is shown in and .,"[('achieves', (2, 3))]","[('PyramidBox', (1, 2)), ('state - ofart performance', (3, 7))]","[['PyramidBox', 'achieves', 'state - ofart performance']]",[],[],"[['Results', 'has', 'PyramidBox']]",face_detection,17,223
1812,results,WIDER FACE Dataset .,[],"[('WIDER FACE Dataset', (0, 3))]",[],[],[],"[['Results', 'has', 'WIDER FACE Dataset']]",face_detection,17,224
1813,results,"Our PyramidBox outperforms others across all three subsets , i.e. 0.961 ( easy ) , 0.950 ( medium ) , 0.889 ( hard ) for validation set , and 0.956 ( easy ) , 0.946 ( medium ) , 0.887 ( hard ) for testing set .","[('across', (4, 5)), ('for', (24, 25))]","[('Our PyramidBox', (0, 2)), ('outperforms', (2, 3)), ('others', (3, 4)), ('all three subsets', (5, 8)), ('0.961', (10, 11)), ('easy', (12, 13)), ('0.950', (15, 16)), ('medium', (17, 18)), ('0.889', (20, 21)), ('hard', (22, 23)), ('validation set', (25, 27)), ('0.956', (29, 30)), ('easy', (31, 32)), ('0.946', (34, 35)), ('medium', (36, 37)), ('0.887', (39, 40)), ('hard', (41, 42)), ('testing set', (44, 46))]","[['outperforms', 'across', 'all three subsets'], ['others', 'across', 'all three subsets'], ['0.889', 'for', 'validation set']]","[['Our PyramidBox', 'has', 'outperforms'], ['outperforms', 'has', 'others'], ['0.961', 'has', 'easy'], ['0.950', 'has', 'medium'], ['0.889', 'has', 'hard'], ['0.956', 'has', 'easy'], ['0.946', 'has', 'medium'], ['0.887', 'has', 'hard']]",[],[],face_detection,17,228
1814,research-problem,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection,[],"[('Unconstrained Face Detection', (11, 14))]",[],[],[],[],face_detection,18,2
1815,research-problem,"Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e. unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3 D facial model construction , etc .",[],"[('Robust face detection in the wild', (0, 6))]",[],[],[],[],face_detection,18,4
1816,research-problem,"Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real - world scenarios due to numerous challenges , e.g. heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc .",[],"[('face detection', (2, 4))]",[],[],[],[],face_detection,18,5
1817,model,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .","[('presents', (2, 3)), ('named', (8, 9)), ('to handle', (22, 24)), ('of', (26, 27)), ('in', (29, 30)), ('collected under', (33, 35))]","[('advanced CNN based approach', (4, 8)), ('Contextual Multi - Scale Region - based CNN ( CMS - RCNN )', (9, 22)), ('problem', (25, 26)), ('face detection', (27, 29)), ('digital face images', (30, 33)), ('numerous challenging conditions', (35, 38))]","[['advanced CNN based approach', 'named', 'Contextual Multi - Scale Region - based CNN ( CMS - RCNN )'], ['Contextual Multi - Scale Region - based CNN ( CMS - RCNN )', 'to handle', 'problem'], ['problem', 'of', 'face detection'], ['face detection', 'in', 'digital face images'], ['digital face images', 'collected under', 'numerous challenging conditions']]","[['advanced CNN based approach', 'name', 'Contextual Multi - Scale Region - based CNN ( CMS - RCNN )']]","[['Model', 'presents', 'advanced CNN based approach']]",[],face_detection,18,21
1818,model,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .","[('allows', (7, 8)), ('to simultaneously look at', (10, 14)), ('as', (17, 18)), ('to explicitly look outside', (20, 24))]","[('Our designed region - based CNN architecture', (0, 7)), ('network', (9, 10)), ('multi-scale features', (14, 16)), ('facial regions', (24, 26)), ('potential body regions', (28, 31))]","[['Our designed region - based CNN architecture', 'allows', 'network'], ['network', 'to simultaneously look at', 'multi-scale features'], ['Our designed region - based CNN architecture', 'to explicitly look outside', 'facial regions']]",[],[],"[['Model', 'has', 'Our designed region - based CNN architecture']]",face_detection,18,22
1819,model,Additionally this architecture also helps to synchronize both the global semantic features in high level layers and the localization features in low level layers for facial representation .,"[('helps to', (4, 6)), ('synchronize', (6, 7)), ('in', (12, 13)), ('in', (20, 21)), ('for', (24, 25))]","[('global semantic features', (9, 12)), ('high level layers', (13, 16)), ('localization features', (18, 20)), ('low level layers', (21, 24)), ('facial representation', (25, 27))]","[['global semantic features', 'in', 'high level layers'], ['localization features', 'in', 'low level layers'], ['localization features', 'in', 'low level layers'], ['localization features', 'for', 'facial representation'], ['low level layers', 'for', 'facial representation']]",[],"[['Model', 'helps to', 'global semantic features']]",[],face_detection,18,24
1820,model,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,"[('introduces', (5, 6)), ('to generate', (18, 20)), ('of', (22, 23)), ('to do', (39, 41)), ('on', (42, 43)), ('of', (46, 47))]","[('Our CMS - RCNN method', (0, 5)), ('Multi - Scale Region Proposal Network ( MS - RPN )', (7, 18)), ('set', (21, 22)), ('region candidates', (23, 25)), ('Contextual Multi - Scale Convolution Neural Network ( CMS - CNN )', (27, 39)), ('inference', (41, 42)), ('region candidates', (44, 46)), ('facial regions', (47, 49))]","[['Our CMS - RCNN method', 'introduces', 'Multi - Scale Region Proposal Network ( MS - RPN )'], ['Our CMS - RCNN method', 'introduces', 'Contextual Multi - Scale Convolution Neural Network ( CMS - CNN )'], ['Multi - Scale Region Proposal Network ( MS - RPN )', 'to generate', 'set'], ['Multi - Scale Region Proposal Network ( MS - RPN )', 'to generate', 'Contextual Multi - Scale Convolution Neural Network ( CMS - CNN )'], ['set', 'of', 'region candidates'], ['Contextual Multi - Scale Convolution Neural Network ( CMS - CNN )', 'to do', 'inference'], ['inference', 'on', 'region candidates'], ['region candidates', 'of', 'facial regions']]","[['Our CMS - RCNN method', 'name', 'Multi - Scale Region Proposal Network ( MS - RPN )']]",[],"[['Model', 'has', 'Our CMS - RCNN method']]",face_detection,18,26
1821,experimental-setup,Our CMS - RCNN is implemented in the Caffe deep learning framework .,"[('implemented in', (5, 7))]","[('Our CMS - RCNN', (0, 4)), ('Caffe deep learning framework', (8, 12))]","[['Our CMS - RCNN', 'implemented in', 'Caffe deep learning framework']]",[],[],"[['Experimental setup', 'has', 'Our CMS - RCNN']]",face_detection,18,228
1822,experimental-setup,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .","[('have', (7, 8)), ('as', (11, 12)), ('during', (20, 21)), ('initialized from', (25, 27))]","[('first 5 sets of convolution layers', (1, 7)), ('same architecture', (9, 11)), ('deep VGG - 16 model', (13, 18)), ('training', (21, 22)), ('parameters', (23, 24)), ('pre-trained VGG - 16', (28, 32))]","[['first 5 sets of convolution layers', 'have', 'same architecture'], ['same architecture', 'as', 'deep VGG - 16 model'], ['first 5 sets of convolution layers', 'during', 'training'], ['parameters', 'initialized from', 'pre-trained VGG - 16']]","[['first 5 sets of convolution layers', 'has', 'same architecture'], ['training', 'has', 'parameters']]",[],"[['Experimental setup', 'has', 'first 5 sets of convolution layers']]",face_detection,18,229
1823,experimental-setup,"In the MS - RPN , we want ' conv3 ' , ' conv4 ' , and ' conv5 ' to be synchronized to the same size so that concatenation can be applied .","[('In', (0, 1)), ('be', (21, 22)), ('synchronized to', (22, 24)), ('so', (27, 28))]","[('MS - RPN', (2, 5)), (""' conv3 ' , ' conv4 ' , and ' conv5 '"", (8, 20)), ('same size', (25, 27)), ('concatenation', (29, 30)), ('applied', (32, 33))]","[[""' conv3 ' , ' conv4 ' , and ' conv5 '"", 'synchronized to', 'same size'], ['same size', 'so', 'concatenation']]","[['concatenation', 'has', 'applied']]","[['Experimental setup', 'In', 'MS - RPN']]",[],face_detection,18,232
1824,baselines,So ' conv3 ' is followed by pooling layer to perform down - sampling .,"[('followed by', (5, 7)), ('to perform', (9, 11))]","[(""' conv3 '"", (1, 4)), ('pooling layer', (7, 9)), ('down - sampling', (11, 14))]","[[""' conv3 '"", 'followed by', 'pooling layer'], ['pooling layer', 'to perform', 'down - sampling']]",[],[],"[['Baselines', 'has', ""' conv3 '""]]",face_detection,18,233
1825,experimental-setup,"Then ' conv3 ' , ' conv4 ' , and ' conv5 ' are normalized along the channel axis to a learnable re-weighting scale and concatenated together .","[('along', (15, 16)), ('to', (19, 20))]","[(""' conv3 ' , ' conv4 ' , and ' conv5 '"", (1, 13)), ('normalized', (14, 15)), ('channel axis', (17, 19)), ('learnable re-weighting scale', (21, 24)), ('concatenated', (25, 26))]","[['normalized', 'along', 'channel axis'], ['normalized', 'to', 'concatenated'], ['channel axis', 'to', 'learnable re-weighting scale'], ['channel axis', 'to', 'concatenated']]","[[""' conv3 ' , ' conv4 ' , and ' conv5 '"", 'has', 'normalized']]",[],"[['Experimental setup', 'has', ""' conv3 ' , ' conv4 ' , and ' conv5 '""]]",face_detection,18,234
1826,experimental-setup,"To ensure training convergence , the initial re-weighting scale needs to be carefully set .","[('To ensure', (0, 2)), ('needs to be', (9, 12))]","[('training convergence', (2, 4)), ('initial re-weighting scale', (6, 9)), ('carefully set', (12, 14))]","[['initial re-weighting scale', 'needs to be', 'carefully set']]","[['training convergence', 'has', 'initial re-weighting scale'], ['initial re-weighting scale', 'has', 'carefully set']]","[['Experimental setup', 'To ensure', 'training convergence']]",[],face_detection,18,235
1827,experimental-setup,"Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .","[('set', (2, 3)), ('of', (6, 7)), ('to be', (19, 21))]","[('initial scale', (4, 6)), (""' conv3 ' , ' conv4 ' , and ' conv5 '"", (7, 19)), ('66.84 , 94.52 , and 94.52', (21, 27))]","[['initial scale', 'of', ""' conv3 ' , ' conv4 ' , and ' conv5 '""], [""' conv3 ' , ' conv4 ' , and ' conv5 '"", 'to be', '66.84 , 94.52 , and 94.52']]",[],"[['Experimental setup', 'set', 'initial scale']]",[],face_detection,18,236
1828,experimental-setup,"In the CMS - CNN , the RoI pooling layer already ensure that the pooled feature maps have the same size .","[('ensure', (11, 12)), ('have', (17, 18))]","[('CMS - CNN', (2, 5)), ('RoI pooling layer', (7, 10)), ('pooled feature maps', (14, 17)), ('same size', (19, 21))]","[['RoI pooling layer', 'ensure', 'pooled feature maps'], ['pooled feature maps', 'have', 'same size']]","[['CMS - CNN', 'has', 'RoI pooling layer']]",[],[],face_detection,18,237
1829,experimental-setup,"Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .","[('pooled from', (3, 5)), ('initialized with', (18, 20)), ('to be', (21, 23)), ('for', (31, 32))]","[('features', (2, 3)), (""' conv3 ' , ' conv4 ' , and ' conv5 '"", (5, 17)), ('scale', (20, 21)), ('57.75 , 81.67 , and 81.67', (23, 29)), ('both face and body pipelines', (32, 37))]","[['features', 'pooled from', ""' conv3 ' , ' conv4 ' , and ' conv5 '""], [""' conv3 ' , ' conv4 ' , and ' conv5 '"", 'initialized with', 'scale'], ['scale', 'to be', '57.75 , 81.67 , and 81.67'], ['57.75 , 81.67 , and 81.67', 'for', 'both face and body pipelines']]",[],[],"[['Experimental setup', 'has', 'features']]",face_detection,18,239
1830,experimental-setup,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .","[('share', (9, 10)), ('for', (13, 14)), ('so that', (17, 19)), ('done', (22, 23)), ('resulting in', (25, 27))]","[('MS - RPN and the CMS - CNN', (1, 9)), ('same parameters', (11, 13)), ('all convolution layers', (14, 17)), ('computation', (19, 20)), ('once', (23, 24)), ('higher efficiency', (27, 29))]","[['MS - RPN and the CMS - CNN', 'share', 'same parameters'], ['same parameters', 'for', 'all convolution layers'], ['all convolution layers', 'so that', 'computation'], ['computation', 'done', 'once'], ['computation', 'resulting in', 'higher efficiency'], ['once', 'resulting in', 'higher efficiency']]",[],[],"[['Experimental setup', 'has', 'MS - RPN and the CMS - CNN']]",face_detection,18,240
1831,experimental-setup,"Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .","[('to shrink', (4, 6)), ('of', (9, 10))]","[('channel size', (7, 9)), ('concatenated feature map', (11, 14)), ('11 convolution layer', (16, 19)), ('employed', (21, 22))]","[['channel size', 'of', 'concatenated feature map']]","[['11 convolution layer', 'has', 'employed']]","[['Experimental setup', 'to shrink', 'channel size']]",[],face_detection,18,241
1832,experiments,Experiments on WIDER FACE Dataset,"[('on', (1, 2))]","[('WIDER FACE Dataset', (2, 5))]",[],[],"[['Experiments', 'on', 'WIDER FACE Dataset']]",[],face_detection,18,249
1833,results,"It achieves the best average precision in all level faces , i.e. AP = 0.902 ( Easy ) , 0.874 ( Medium ) and 0.643 ( Hard ) , and outperforms the second best baseline by 26.0 % ( Easy ) , 37.4 % ( Medium ) and 60.8 % ( Hard ) .","[('achieves', (1, 2)), ('in', (6, 7)), ('i.e.', (11, 12)), ('outperforms', (30, 31)), ('by', (35, 36))]","[('best average precision', (3, 6)), ('all level faces', (7, 10)), ('AP', (12, 13)), ('0.902', (14, 15)), ('Easy', (16, 17)), ('0.874', (19, 20)), ('Medium', (21, 22)), ('0.643', (24, 25)), ('Hard', (26, 27)), ('second best baseline', (32, 35)), ('26.0 %', (36, 38)), ('Easy', (39, 40)), ('37.4 %', (42, 44)), ('Medium', (45, 46)), ('60.8 %', (48, 50)), ('Hard', (51, 52))]","[['best average precision', 'in', 'all level faces'], ['all level faces', 'i.e.', 'AP'], ['best average precision', 'outperforms', 'second best baseline'], ['second best baseline', 'by', '26.0 %'], ['second best baseline', 'by', '60.8 %']]","[['AP', 'has', '0.902'], ['0.902', 'has', 'Easy'], ['0.874', 'has', 'Medium'], ['0.643', 'has', 'Hard'], ['26.0 %', 'has', 'Easy'], ['37.4 %', 'has', 'Medium'], ['60.8 %', 'has', 'Hard']]",[],[],face_detection,18,268
1834,results,Experiments on FDDB Face Database,"[('on', (1, 2))]","[('FDDB Face Database', (2, 5))]",[],[],"[['Results', 'on', 'FDDB Face Database']]",[],face_detection,18,295
1835,results,Our method achieves the best recall rate on this database .,"[('achieves', (2, 3))]","[('Our method', (0, 2)), ('best recall rate', (4, 7)), ('this database', (8, 10))]","[['Our method', 'achieves', 'best recall rate']]",[],[],"[['Results', 'has', 'Our method']]",face_detection,18,304
1836,results,The proposed CMS - RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods ( as shown ) .,"[('outperforms', (6, 7)), ('achieves', (15, 16)), ('comparing against', (21, 23))]","[('proposed CMS - RCNN approach', (1, 6)), ('most of the published face detection methods', (7, 14)), ('very high recall rate', (17, 21)), ('all other methods', (23, 26))]","[['proposed CMS - RCNN approach', 'outperforms', 'most of the published face detection methods'], ['proposed CMS - RCNN approach', 'achieves', 'very high recall rate'], ['very high recall rate', 'comparing against', 'all other methods']]",[],[],"[['Results', 'has', 'proposed CMS - RCNN approach']]",face_detection,18,307
1837,research-problem,Face Detection Using Improved Faster RCNN,[],"[('Face Detection', (0, 2))]",[],[],[],[],face_detection,19,2
1838,research-problem,"In this report , we propose a detailed design Faster RCNN method named FDNet1.0 for face detection , which achieves more decent performance than previous methods .","[('propose', (5, 6)), ('named', (12, 13)), ('for', (14, 15)), ('achieves', (19, 20))]","[('detailed design Faster RCNN method', (7, 12)), ('FDNet1.0', (13, 14)), ('face detection', (15, 17)), ('more decent performance', (20, 23))]","[['detailed design Faster RCNN method', 'named', 'FDNet1.0'], ['detailed design Faster RCNN method', 'for', 'face detection'], ['FDNet1.0', 'for', 'face detection'], ['detailed design Faster RCNN method', 'achieves', 'more decent performance']]","[['detailed design Faster RCNN method', 'name', 'FDNet1.0']]","[['Research problem', 'propose', 'detailed design Faster RCNN method']]",[],face_detection,19,18
1839,model,"A deformable layer with fewer channels is attached to the backbone network to produce a "" thin "" feature map , which is subsequently fed to a full connected layer , building an efficient yet accurate two - stage detector .","[('with', (3, 4)), ('attached to', (7, 9)), ('to produce', (12, 14)), ('fed to', (24, 26)), ('building', (31, 32))]","[('deformable layer', (1, 3)), ('fewer channels', (4, 6)), ('backbone network', (10, 12)), ('"" thin "" feature map', (15, 20)), ('full connected layer', (27, 30)), ('efficient yet accurate two - stage detector', (33, 40))]","[['deformable layer', 'with', 'fewer channels'], ['fewer channels', 'attached to', 'backbone network'], ['backbone network', 'to produce', '"" thin "" feature map'], ['"" thin "" feature map', 'fed to', 'full connected layer'], ['full connected layer', 'building', 'efficient yet accurate two - stage detector']]","[['deformable layer', 'has', 'fewer channels']]",[],[],face_detection,19,19
1840,research-problem,"At testing time , we also find a comparable mean average precision ( m AP ) be achieved when the top - ranked proposals ( e.g. , 6000 ) are directly selected without NMS in the RPN stage over WIDER FACE dataset .","[('At', (0, 1)), ('find', (6, 7)), ('achieved when', (17, 19)), ('in', (34, 35)), ('over', (38, 39))]","[('testing time', (1, 3)), ('comparable mean average precision ( m AP )', (8, 16)), ('top - ranked proposals ( e.g. , 6000 )', (20, 29)), ('directly selected', (30, 32)), ('without NMS', (32, 34)), ('RPN stage', (36, 38)), ('WIDER FACE dataset', (39, 42))]","[['testing time', 'find', 'comparable mean average precision ( m AP )'], ['comparable mean average precision ( m AP )', 'achieved when', 'top - ranked proposals ( e.g. , 6000 )'], ['without NMS', 'in', 'RPN stage'], ['RPN stage', 'over', 'WIDER FACE dataset']]","[['testing time', 'has', 'comparable mean average precision ( m AP )'], ['directly selected', 'has', 'without NMS']]","[['Research problem', 'At', 'testing time']]",[],face_detection,19,20
1841,model,"Furthermore , the multi-scale training and testing strategy are also applied in our work .","[('applied in', (10, 12))]","[('multi-scale training and testing strategy', (3, 8)), ('work', (13, 14))]","[['multi-scale training and testing strategy', 'applied in', 'work']]","[['multi-scale training and testing strategy', 'has', 'work']]",[],"[['Model', 'has', 'multi-scale training and testing strategy']]",face_detection,19,22
1842,experimental-setup,Single NVIDIA Tesla K80 is used for training and testing .,"[('used for', (5, 7))]","[('Single NVIDIA Tesla K80', (0, 4)), ('training and testing', (7, 10))]","[['Single NVIDIA Tesla K80', 'used for', 'training and testing']]",[],[],"[['Experimental setup', 'has', 'Single NVIDIA Tesla K80']]",face_detection,19,84
1843,experimental-setup,Mini batch size is set to 1 considering memory consumption .,"[('set to', (4, 6)), ('considering', (7, 8))]","[('Mini batch size', (0, 3)), ('1', (6, 7)), ('memory consumption', (8, 10))]","[['Mini batch size', 'set to', '1'], ['1', 'considering', 'memory consumption']]","[['Mini batch size', 'has', '1']]",[],"[['Experimental setup', 'has', 'Mini batch size']]",face_detection,19,85
1844,experimental-setup,"Specifically , ResNet_v1_101 trained on ImageNet - 128w is used for Faster RCNN feature extraction .","[('trained on', (3, 5)), ('used for', (9, 11))]","[('ResNet_v1_101', (2, 3)), ('ImageNet - 128w', (5, 8)), ('Faster RCNN feature extraction', (11, 15))]","[['ResNet_v1_101', 'trained on', 'ImageNet - 128w'], ['ImageNet - 128w', 'used for', 'Faster RCNN feature extraction']]",[],[],"[['Experimental setup', 'has', 'ResNet_v1_101']]",face_detection,19,86
1845,experimental-setup,"Aspect ratios ( 1 , 1.5 , 2 ) and scales ( 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 ) are carefully designed to capture better locations of faces in the RPN stage , and the number of filters for the RPN layer is set as 512 .","[('to capture', (33, 35)), ('of', (37, 38)), ('in', (39, 40)), ('for', (49, 50)), ('set as', (54, 56))]","[('Aspect ratios', (0, 2)), ('1', (3, 4)), ('1.5', (5, 6)), ('2', (7, 8)), ('scales', (10, 11)), ('16 2', (12, 14)), ('32 2', (15, 17)), ('64 2', (18, 20)), ('128 2', (21, 23)), ('256 2', (24, 26)), ('512 2', (27, 29)), ('carefully designed', (31, 33)), ('better locations', (35, 37)), ('faces', (38, 39)), ('RPN stage', (41, 43)), ('number of filters', (46, 49)), ('RPN layer', (51, 53)), ('512', (56, 57))]","[['carefully designed', 'to capture', 'better locations'], ['better locations', 'of', 'faces'], ['faces', 'in', 'RPN stage'], ['number of filters', 'for', 'RPN layer']]","[['Aspect ratios', 'has', '1'], ['scales', 'has', '16 2']]",[],"[['Experimental setup', 'has', 'Aspect ratios']]",face_detection,19,89
1846,experimental-setup,"By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .","[('of', (7, 8)), ('assigned as', (15, 17))]","[('batch size', (5, 7)), ('RPN and R - CNN', (8, 13)), ('256 and 128', (17, 20))]","[['batch size', 'of', 'RPN and R - CNN'], ['RPN and R - CNN', 'assigned as', '256 and 128']]",[],[],"[['Experimental setup', 'has', 'batch size']]",face_detection,19,94
1847,experimental-setup,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .","[('set to', (5, 7)), ('decrease to', (12, 14)), ('after', (17, 18))]","[('initial learning rate', (1, 4)), ('1e - 3', (7, 10)), ('1e - 4', (14, 17)), ('20w iterations', (18, 20))]","[['initial learning rate', 'set to', '1e - 3'], ['initial learning rate', 'decrease to', '1e - 4'], ['1e - 4', 'after', '20w iterations']]","[['initial learning rate', 'has', '1e - 3']]",[],"[['Experimental setup', 'has', 'initial learning rate']]",face_detection,19,95
1848,experimental-setup,Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,"[('set to', (6, 8))]","[('Weight decay is and momentum', (0, 5)), ('1e - 4 and 0.9', (8, 13))]","[['Weight decay is and momentum', 'set to', '1e - 4 and 0.9']]",[],[],"[['Experimental setup', 'has', 'Weight decay is and momentum']]",face_detection,19,96
1849,results,"Compared with the recently published top approaches , FDNet1.0 wins two 1st places ( easy set = 95.9 % , medium set = 94.5 % ) and one 2nd place ( hard set = 87.9 % ) on the validation set , as illustrated in .","[('wins', (9, 10)), ('on', (37, 38))]","[('FDNet1.0', (8, 9)), ('two 1st places', (10, 13)), ('easy set', (14, 16)), ('95.9 %', (17, 19)), ('medium set', (20, 22)), ('94.5 %', (23, 25)), ('one 2nd place', (27, 30)), ('hard set', (31, 33)), ('87.9 %', (34, 36)), ('validation set', (39, 41))]","[['FDNet1.0', 'wins', 'two 1st places'], ['FDNet1.0', 'wins', 'one 2nd place'], ['one 2nd place', 'on', 'validation set']]","[['two 1st places', 'has', 'easy set'], ['easy set', 'has', '95.9 %'], ['medium set', 'has', '94.5 %'], ['one 2nd place', 'has', 'hard set'], ['hard set', 'has', '87.9 %']]",[],[],face_detection,19,103
1850,research-problem,Selective Refinement Network for High Performance Face Detection,[],"[('High Performance Face Detection', (4, 8))]",[],[],[],[],face_detection,2,2
1851,research-problem,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .",[],"[('face detection', (2, 4))]",[],[],[],[],face_detection,2,4
1852,model,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .","[('investigate', (5, 6)), ('of', (8, 9)), ('on', (15, 16)), ('of', (18, 19)), ('propose', (22, 23)), ('named', (29, 30)), ('selectively applies', (38, 40)), ('to', (46, 47)), ('of', (49, 50))]","[('effects', (7, 8)), ('two - step classification and regression', (9, 15)), ('different levels', (16, 18)), ('detection layers', (19, 21)), ('novel face detection framework', (24, 28)), ('Selective Refinement Network ( SRN )', (30, 36)), ('two - step classification and regression', (40, 46)), ('specific levels', (47, 49)), ('detection layers', (50, 52))]","[['effects', 'of', 'two - step classification and regression'], ['two - step classification and regression', 'on', 'different levels'], ['different levels', 'of', 'detection layers'], ['two - step classification and regression', 'propose', 'novel face detection framework'], ['novel face detection framework', 'named', 'Selective Refinement Network ( SRN )'], ['Selective Refinement Network ( SRN )', 'selectively applies', 'two - step classification and regression'], ['two - step classification and regression', 'to', 'specific levels'], ['specific levels', 'of', 'detection layers']]","[['novel face detection framework', 'name', 'Selective Refinement Network ( SRN )']]","[['Model', 'investigate', 'effects']]",[],face_detection,2,43
1853,model,"The network structure of SRN is shown in , which consists of two key modules , named as the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .","[('of', (3, 4)), ('consists of', (10, 12)), ('named as', (16, 18))]","[('network structure', (1, 3)), ('SRN', (4, 5)), ('two key modules', (12, 15)), ('Selective Two - step Classification ( STC ) module', (19, 28)), ('Selective Two - step Regression ( STR ) module', (30, 39))]","[['network structure', 'of', 'SRN'], ['SRN', 'consists of', 'two key modules'], ['two key modules', 'named as', 'Selective Two - step Classification ( STC ) module'], ['two key modules', 'named as', 'Selective Two - step Regression ( STR ) module']]","[['two key modules', 'name', 'Selective Two - step Classification ( STC ) module']]",[],"[['Model', 'has', 'network structure']]",face_detection,2,44
1854,model,"Specifically , the STC is applied to filter out most simple negative samples ( illustrated in ( a ) ) from the low levels of detection layers , which contains 88.9 % samples .","[('applied to', (5, 7)), ('from', (20, 21)), ('contains', (29, 30))]","[('STC', (3, 4)), ('filter out', (7, 9)), ('most simple negative samples', (9, 13)), ('low levels of detection layers', (22, 27)), ('88.9 % samples', (30, 33))]","[['STC', 'applied to', 'filter out'], ['most simple negative samples', 'from', 'low levels of detection layers'], ['low levels of detection layers', 'contains', '88.9 % samples']]","[['filter out', 'has', 'most simple negative samples']]",[],"[['Model', 'has', 'STC']]",face_detection,2,45
1855,model,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .","[('design', (4, 5)), ('to provide', (12, 14)), ('to better capture', (18, 21))]","[('Receptive Field Enhancement ( RFE )', (6, 12)), ('more diverse receptive fields', (14, 18)), ('extreme - pose faces', (22, 26))]","[['Receptive Field Enhancement ( RFE )', 'to provide', 'more diverse receptive fields'], ['more diverse receptive fields', 'to better capture', 'extreme - pose faces']]",[],"[['Model', 'design', 'Receptive Field Enhancement ( RFE )']]",[],face_detection,2,48
1856,experimental-setup,"The loss function for SRN is just the sum of the STC loss and the STR loss , i.e. , L = L STC + L STR .","[('for', (3, 4)), ('is', (5, 6)), ('of', (9, 10))]","[('loss function', (1, 3)), ('SRN', (4, 5)), ('sum', (8, 9)), ('STC loss and the STR loss', (11, 17))]","[['loss function', 'for', 'SRN'], ['loss function', 'is', 'sum'], ['SRN', 'is', 'sum'], ['sum', 'of', 'STC loss and the STR loss']]",[],[],"[['Experimental setup', 'has', 'loss function']]",face_detection,2,175
1857,experimental-setup,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .","[('initialized by', (4, 6)), ('in', (16, 17)), ('initialized by', (23, 25))]","[('backbone network', (1, 3)), ('pretrained ResNet - 50 model and all the parameters', (7, 16)), ('newly added convolution layers', (18, 22)), ('"" xavier "" method', (26, 30))]","[['backbone network', 'initialized by', 'pretrained ResNet - 50 model and all the parameters'], ['pretrained ResNet - 50 model and all the parameters', 'in', 'newly added convolution layers'], ['newly added convolution layers', 'initialized by', '"" xavier "" method']]",[],[],"[['Experimental setup', 'has', 'backbone network']]",face_detection,2,176
1858,experimental-setup,"We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .","[('fine - tune', (1, 4)), ('using', (7, 8)), ('with', (9, 10))]","[('SRN model', (5, 7)), ('SGD', (8, 9)), ('0.9 momentum', (10, 12)), ('0.0001 weight decay', (13, 16)), ('batch size 32', (18, 21))]","[['SRN model', 'using', 'SGD'], ['SRN model', 'using', 'batch size 32'], ['SGD', 'with', '0.9 momentum'], ['SGD', 'with', 'batch size 32']]",[],"[['Experimental setup', 'fine - tune', 'SRN model']]",[],face_detection,2,177
1859,experimental-setup,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .","[('set', (1, 2)), ('to', (5, 6)), ('for', (8, 9)), ('to', (17, 18)), ('for', (25, 26))]","[('learning rate', (3, 5)), ('10 ?2', (6, 8)), ('first 100 epochs', (10, 13)), ('decay', (15, 16)), ('10 ? 3 and 10 ? 4', (18, 25)), ('another 20 and 10 epochs', (26, 31))]","[['learning rate', 'to', '10 ?2'], ['decay', 'to', '10 ? 3 and 10 ? 4'], ['10 ?2', 'for', 'first 100 epochs'], ['10 ? 3 and 10 ? 4', 'for', 'another 20 and 10 epochs'], ['decay', 'to', '10 ? 3 and 10 ? 4'], ['10 ? 3 and 10 ? 4', 'for', 'another 20 and 10 epochs']]","[['learning rate', 'has', '10 ?2'], ['decay', 'has', '10 ? 3 and 10 ? 4']]","[['Experimental setup', 'set', 'learning rate']]",[],face_detection,2,178
1860,experimental-setup,We implement SRN using the Py - Torch library .,"[('implement', (1, 2)), ('using', (3, 4))]","[('SRN', (2, 3)), ('Py - Torch library', (5, 9))]","[['SRN', 'using', 'Py - Torch library']]",[],"[['Experimental setup', 'implement', 'SRN']]",[],face_detection,2,179
1861,results,AFW Dataset .,[],"[('AFW Dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'AFW Dataset']]",face_detection,2,213
1862,results,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .","[('with', (15, 16))]","[('SRN', (4, 5)), ('outperforms', (5, 6)), ('state - of - the - art methods', (7, 15)), ('top AP score ( 99.87 % )', (17, 24))]","[['state - of - the - art methods', 'with', 'top AP score ( 99.87 % )']]","[['SRN', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art methods']]",[],[],face_detection,2,217
1863,results,PASCAL Face Dataset .,[],"[('PASCAL Face Dataset', (0, 3))]",[],[],[],"[['Results', 'has', 'PASCAL Face Dataset']]",face_detection,2,218
1864,results,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,"[('achieves', (1, 2)), ('by improving', (11, 13)), ('compared to', (17, 19))]","[('SRN', (0, 1)), ('state - of - the - art results', (3, 11)), ('4.99 % AP score', (13, 17)), ('second best method STN', (20, 24))]","[['SRN', 'achieves', 'state - of - the - art results'], ['state - of - the - art results', 'by improving', '4.99 % AP score'], ['4.99 % AP score', 'compared to', 'second best method STN']]","[['SRN', 'has', 'state - of - the - art results']]",[],"[['Results', 'has', 'SRN']]",face_detection,2,221
1865,results,FDDB Dataset .,[],"[('FDDB Dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'FDDB Dataset']]",face_detection,2,222
1866,results,"As shown in ( c ) , our SRN sets a new state - of - the - art performance , i.e. , 98.8 % true positive rate when the number of false positives is equal to 1000 .","[('sets', (9, 10)), ('i.e.', (21, 22)), ('when', (28, 29)), ('equal to', (35, 37))]","[('our SRN', (7, 9)), ('new state - of - the - art performance', (11, 20)), ('98.8 % true positive rate', (23, 28)), ('number of false positives', (30, 34)), ('1000', (37, 38))]","[['our SRN', 'sets', 'new state - of - the - art performance'], ['new state - of - the - art performance', 'i.e.', '98.8 % true positive rate'], ['98.8 % true positive rate', 'when', 'number of false positives'], ['number of false positives', 'equal to', '1000']]","[['our SRN', 'has', 'new state - of - the - art performance']]",[],"[['Results', 'has', 'our SRN']]",face_detection,2,225
1867,results,WIDER FACE Dataset .,[],"[('WIDER FACE Dataset', (0, 3))]",[],[],[],"[['Results', 'has', 'WIDER FACE Dataset']]",face_detection,2,227
1868,results,"As shown in , we find that SRN performs favourably against the state - of - the - art based on the average precision ( AP ) across the three subsets , especially on the Hard subset which contains a large amount of small faces .","[('find', (5, 6)), ('performs', (8, 9)), ('against', (10, 11)), ('based on', (19, 21)), ('across', (27, 28))]","[('SRN', (7, 8)), ('favourably', (9, 10)), ('state - of - the - art', (12, 19)), ('average precision ( AP )', (22, 27)), ('three subsets', (29, 31))]","[['SRN', 'performs', 'favourably'], ['favourably', 'against', 'state - of - the - art'], ['state - of - the - art', 'based on', 'average precision ( AP )'], ['average precision ( AP )', 'across', 'three subsets']]",[],"[['Results', 'find', 'SRN']]",[],face_detection,2,230
1869,research-problem,Aggregate Channel Features for Multi-view Face Detection,[],"[('Multi-view Face Detection', (4, 7))]",[],[],[],[],face_detection,20,2
1870,research-problem,Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,20,4
1871,research-problem,Human face detection have long been one of the most fundamental problems in computer vision and humancomputer interaction .,[],"[('Human face detection', (0, 3))]",[],[],[],[],face_detection,20,11
1872,model,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .","[('adopt', (5, 6)), ('called', (11, 12)), ('as', (20, 21)), ('on', (23, 24))]","[('variant of channel features', (7, 11)), ('aggregate channel features', (12, 15)), ('extracted directly', (18, 20)), ('pixel values', (21, 23)), ('subsampled channels', (24, 26))]","[['variant of channel features', 'called', 'aggregate channel features'], ['aggregate channel features', 'as', 'pixel values'], ['extracted directly', 'as', 'pixel values'], ['pixel values', 'on', 'subsampled channels']]","[['variant of channel features', 'name', 'aggregate channel features'], ['extracted directly', 'has', 'pixel values']]","[['Model', 'adopt', 'variant of channel features']]",[],face_detection,20,25
1873,model,"With these two superiorities , the aggregate channel features breakthrough the bottleneck in VJ framework and have the potential to make great advance in face detection .","[('breakthrough', (9, 10)), ('in', (12, 13))]","[('aggregate channel features', (6, 9)), ('bottleneck', (11, 12)), ('VJ framework', (13, 15))]","[['aggregate channel features', 'breakthrough', 'bottleneck'], ['bottleneck', 'in', 'VJ framework']]","[['aggregate channel features', 'has', 'bottleneck']]",[],"[['Model', 'has', 'aggregate channel features']]",face_detection,20,27
1874,model,"To do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , subsampling method , feature scale and soon , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners .","[('make', (5, 6)), ('into', (13, 14)), ('concerning', (18, 19))]","[('deep and all - round investigation', (7, 13)), ('specific feature parameters', (15, 18)), ('channel types', (19, 21)), ('feature pool size', (22, 25)), ('subsampling method', (26, 28)), ('feature scale', (29, 31))]","[['deep and all - round investigation', 'into', 'specific feature parameters'], ['specific feature parameters', 'concerning', 'channel types'], ['specific feature parameters', 'concerning', 'subsampling method'], ['specific feature parameters', 'concerning', 'feature scale']]",[],"[['Model', 'make', 'deep and all - round investigation']]",[],face_detection,20,29
1875,experiments,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .","[('find that', (6, 8)), ('enriches', (16, 17)), ('impact', (37, 38)), ('proven to be', (73, 76)), ('with', (79, 80))]","[('multi-scaling', (11, 12)), ('feature representation', (13, 15)), ('representation capacity', (18, 20)), ('aggregate channel features', (22, 25)), ('different combinations of channel types', (32, 37)), ('performance', (39, 40)), ('greatly', (40, 41)), ('multi-view detection', (70, 72)), ('good match', (77, 79))]","[['feature representation', 'enriches', 'representation capacity'], ['different combinations of channel types', 'impact', 'performance'], ['multi-view detection', 'proven to be', 'good match']]","[['multi-scaling', 'has', 'feature representation'], ['performance', 'has', 'greatly'], ['multi-view detection', 'has', 'good match']]",[],[],face_detection,20,30
1876,results,"As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .","[('in', (2, 3)), ('achieves', (10, 11)), ('of', (14, 15)), ('outperforming', (18, 19)), ('by', (22, 23))]","[('AFW', (5, 6)), ('our multi-scale detector', (7, 10)), ('ap value', (12, 14)), ('96.8 %', (15, 17)), ('other academic methods', (19, 22)), ('large margin', (24, 26))]","[['our multi-scale detector', 'achieves', 'ap value'], ['ap value', 'of', '96.8 %'], ['ap value', 'outperforming', 'other academic methods'], ['other academic methods', 'by', 'large margin']]","[['AFW', 'has', 'our multi-scale detector'], ['our multi-scale detector', 'has', 'ap value']]",[],[],face_detection,20,243
1877,results,"When it comes to commercial systems , ours is better than Face.com and almost equal to Face ++ and Google Picasa .","[('comes to', (2, 4)), ('better than', (9, 11)), ('almost equal to', (13, 16))]","[('commercial systems', (4, 6)), ('ours', (7, 8)), ('Face.com', (11, 12)), ('Face ++', (16, 18)), ('Google Picasa', (19, 21))]","[['commercial systems', 'better than', 'Face.com'], ['ours', 'better than', 'Face.com'], ['commercial systems', 'almost equal to', 'Face ++'], ['commercial systems', 'almost equal to', 'Google Picasa'], ['ours', 'almost equal to', 'Face ++'], ['ours', 'almost equal to', 'Google Picasa']]","[['commercial systems', 'has', 'ours']]",[],[],face_detection,20,244
1878,results,"In discrete score where evaluation metric is the same as in AFW , our detector achieves 83.7 % , which is a little better than Yan et al ..","[('In', (0, 1)), ('where', (3, 4)), ('same as', (8, 10)), ('achieves', (15, 16)), ('than', (24, 25))]","[('discrete score', (1, 3)), ('evaluation metric', (4, 6)), ('AFW', (11, 12)), ('our detector', (13, 15)), ('83.7 %', (16, 18)), ('little better', (22, 24)), ('Yan et al', (25, 28))]","[['discrete score', 'where', 'evaluation metric'], ['our detector', 'achieves', '83.7 %'], ['little better', 'than', 'Yan et al']]","[['discrete score', 'has', 'evaluation metric'], ['evaluation metric', 'has', 'AFW']]",[],[],face_detection,20,248
1879,results,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .","[('When using', (0, 2)), ('takes', (5, 6)), ('as', (9, 10)), ('gets', (15, 16)), ('at', (21, 22)), ('for', (24, 25)), ('surpassing', (28, 29)), ('output', (32, 33)), ('by', (35, 36))]","[('continuous score', (2, 4)), ('overlap ratio', (7, 9)), ('score', (11, 12)), ('our method', (13, 15)), ('61.9 % true positive rate', (16, 21)), ('1 FPPI', (22, 24)), ('multiscale version', (25, 27)), ('other methods', (29, 31)), ('rectangular detections', (33, 35)), ('notable margin', (37, 39))]","[['continuous score', 'takes', 'overlap ratio'], ['overlap ratio', 'as', 'score'], ['continuous score', 'gets', '61.9 % true positive rate'], ['our method', 'gets', '61.9 % true positive rate'], ['61.9 % true positive rate', 'at', '1 FPPI'], ['1 FPPI', 'for', 'multiscale version'], ['61.9 % true positive rate', 'surpassing', 'other methods'], ['other methods', 'output', 'rectangular detections'], ['rectangular detections', 'by', 'notable margin']]","[['continuous score', 'has', 'overlap ratio']]","[['Results', 'When using', 'continuous score']]",[],face_detection,20,250
1880,results,Our detector using single - scale features performs a little worse with the benefit of faster detection speed .,"[('using', (2, 3)), ('performs', (7, 8)), ('with', (11, 12)), ('of', (14, 15))]","[('Our detector', (0, 2)), ('single - scale features', (3, 7)), ('little worse', (9, 11)), ('benefit', (13, 14)), ('faster detection speed', (15, 18))]","[['Our detector', 'using', 'single - scale features'], ['single - scale features', 'performs', 'little worse'], ['little worse', 'with', 'benefit'], ['benefit', 'of', 'faster detection speed']]",[],[],"[['Results', 'has', 'Our detector']]",face_detection,20,251
1881,research-problem,Supervised Transformer Network for Efficient Face Detection,[],"[('Efficient Face Detection', (4, 7))]",[],[],[],[],face_detection,21,2
1882,research-problem,Large pose variations remain to be a challenge that confronts real - word face detection .,[],"[('real - word face detection', (10, 15))]",[],[],[],[],face_detection,21,4
1883,model,"In contrast , we propose a new cascade Convolutional Neural Network that is trained end - to - end .","[('propose', (4, 5)), ('trained', (13, 14))]","[('new cascade Convolutional Neural Network', (6, 11)), ('end - to - end', (14, 19))]","[['new cascade Convolutional Neural Network', 'trained', 'end - to - end']]",[],"[['Model', 'propose', 'new cascade Convolutional Neural Network']]",[],face_detection,21,33
1884,model,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .","[('is', (3, 4)), ('simultaneously proposes', (14, 16)), ('along with', (19, 21))]","[('first stage', (1, 3)), ('multi-task Region Proposal Network ( RPN )', (5, 12)), ('candidate face regions', (16, 19)), ('associated facial landmarks', (21, 24))]","[['first stage', 'is', 'multi-task Region Proposal Network ( RPN )'], ['multi-task Region Proposal Network ( RPN )', 'simultaneously proposes', 'candidate face regions'], ['candidate face regions', 'along with', 'associated facial landmarks']]","[['first stage', 'name', 'multi-task Region Proposal Network ( RPN )']]",[],"[['Model', 'has', 'first stage']]",face_detection,21,34
1885,model,"Inspired by Chen et al. , we jointly conduct face detection and face alignment , since face alignment is helpful to distinguish faces / non - faces patterns .","[('jointly conduct', (7, 9))]","[('face detection and face alignment', (9, 14))]",[],[],"[['Model', 'jointly conduct', 'face detection and face alignment']]",[],face_detection,21,35
1886,model,"Different from Li et al. , this network is calculated on the original resolution to better leverage more discriminative information .","[('calculated on', (9, 11)), ('to better leverage', (14, 17))]","[('network', (7, 8)), ('original resolution', (12, 14)), ('more discriminative information', (17, 20))]","[['network', 'calculated on', 'original resolution'], ['original resolution', 'to better leverage', 'more discriminative information']]",[],[],"[['Model', 'has', 'network']]",face_detection,21,36
1887,model,"The aligned candidate face region is then fed into the second - stage network , a RCNN , for further verification .","[('fed into', (7, 9)), ('for', (18, 19))]","[('aligned candidate face region', (1, 5)), ('second - stage network', (10, 14)), ('RCNN', (16, 17)), ('further verification', (19, 21))]","[['aligned candidate face region', 'fed into', 'second - stage network'], ['second - stage network', 'for', 'further verification'], ['RCNN', 'for', 'further verification']]","[['second - stage network', 'name', 'RCNN']]",[],"[['Model', 'has', 'aligned candidate face region']]",face_detection,21,38
1888,model,Note we only keep the K face candidate regions with top responses in a local neighborhood from the RPN .,"[('keep', (3, 4)), ('with', (9, 10)), ('in', (12, 13)), ('from', (16, 17))]","[('K face candidate regions', (5, 9)), ('top responses', (10, 12)), ('local neighborhood', (14, 16)), ('RPN', (18, 19))]","[['K face candidate regions', 'with', 'top responses'], ['top responses', 'in', 'local neighborhood'], ['local neighborhood', 'from', 'RPN']]","[['K face candidate regions', 'has', 'top responses']]","[['Model', 'keep', 'K face candidate regions']]",[],face_detection,21,39
1889,model,"We concatenate the feature maps from the two cascaded networks together to form an architecture that is trained end - to - end , as shown in .","[('concatenate', (1, 2)), ('from', (5, 6)), ('to form', (11, 13)), ('trained', (17, 18))]","[('feature maps', (3, 5)), ('two cascaded networks together', (7, 11)), ('architecture', (14, 15)), ('end - to - end', (18, 23))]","[['feature maps', 'from', 'two cascaded networks together'], ['two cascaded networks together', 'to form', 'architecture'], ['architecture', 'trained', 'end - to - end']]",[],"[['Model', 'concatenate', 'feature maps']]",[],face_detection,21,43
1890,model,Note that the canonical positions of the facial landmarks in the aligned face image and the predicted facial landmarks in the candidate face region jointly defines the transform from the candidate face region .,"[('of', (5, 6)), ('in', (9, 10)), ('in', (19, 20)), ('jointly defines', (24, 26)), ('from', (28, 29))]","[('canonical positions', (3, 5)), ('facial landmarks', (7, 9)), ('aligned face image', (11, 14)), ('predicted facial landmarks', (16, 19)), ('candidate face region', (21, 24)), ('transform', (27, 28)), ('candidate face region', (30, 33))]","[['canonical positions', 'of', 'facial landmarks'], ['facial landmarks', 'in', 'aligned face image'], ['predicted facial landmarks', 'in', 'candidate face region'], ['predicted facial landmarks', 'in', 'candidate face region'], ['predicted facial landmarks', 'jointly defines', 'transform'], ['candidate face region', 'jointly defines', 'transform'], ['transform', 'from', 'candidate face region']]",[],[],"[['Model', 'has', 'canonical positions']]",face_detection,21,45
1891,model,"In the end - to - end training , the training of the first - stage RPN to predict facial landmarks is also supervised by annotated facial landmarks in each true face regions .","[('In', (0, 1)), ('of', (11, 12)), ('to predict', (17, 19)), ('supervised by', (23, 25)), ('in each', (28, 30))]","[('end - to - end training', (2, 8)), ('training', (10, 11)), ('first - stage RPN', (13, 17)), ('facial landmarks', (19, 21)), ('annotated facial landmarks', (25, 28)), ('true face regions', (30, 33))]","[['annotated facial landmarks', 'In', 'true face regions'], ['training', 'of', 'first - stage RPN'], ['first - stage RPN', 'to predict', 'facial landmarks'], ['facial landmarks', 'supervised by', 'annotated facial landmarks'], ['annotated facial landmarks', 'in each', 'true face regions']]","[['end - to - end training', 'has', 'training']]","[['Model', 'In', 'end - to - end training']]",[],face_detection,21,46
1892,model,We hence call our network a Supervised Transformer Network .,"[('call', (2, 3))]","[('Supervised Transformer Network', (6, 9))]",[],[],"[['Model', 'call', 'Supervised Transformer Network']]",[],face_detection,21,47
1893,model,"Therefore , we propose a region - of - interest ( ROI ) convolution scheme to make the run-time of the Supervised Transformer Network to be more efficient .","[('to make', (15, 17)), ('of', (19, 20)), ('to be', (24, 26))]","[('region - of - interest ( ROI ) convolution scheme', (5, 15)), ('run-time', (18, 19)), ('Supervised Transformer Network', (21, 24)), ('more efficient', (26, 28))]","[['region - of - interest ( ROI ) convolution scheme', 'to make', 'run-time'], ['run-time', 'of', 'Supervised Transformer Network'], ['run-time', 'to be', 'more efficient'], ['Supervised Transformer Network', 'to be', 'more efficient']]",[],[],[],face_detection,21,51
1894,model,It first uses a conventional boosting cascade to obtain a set of face candidate areas .,"[('first uses', (1, 3)), ('to obtain', (7, 9)), ('of', (11, 12))]","[('conventional boosting cascade', (4, 7)), ('set', (10, 11)), ('face candidate areas', (12, 15))]","[['conventional boosting cascade', 'to obtain', 'set'], ['set', 'of', 'face candidate areas']]",[],"[['Model', 'first uses', 'conventional boosting cascade']]",[],face_detection,21,52
1895,model,"Then , we combine these regions into irregular binary ROI mask .","[('into', (6, 7))]","[('irregular binary ROI mask', (7, 11))]",[],[],[],[],face_detection,21,53
1896,model,"All DNN operations ( including convolution , ReLU , pooling , and concatenation ) are all processed inside the ROI mask , and hence significantly reduce the computation .","[('including', (4, 5)), ('processed inside', (16, 18)), ('significantly reduce', (24, 26))]","[('All DNN operations', (0, 3)), ('convolution', (5, 6)), ('ReLU', (7, 8)), ('pooling', (9, 10)), ('concatenation', (12, 13)), ('ROI mask', (19, 21)), ('computation', (27, 28))]","[['All DNN operations', 'including', 'convolution'], ['All DNN operations', 'processed inside', 'ROI mask'], ['All DNN operations', 'significantly reduce', 'computation']]","[['All DNN operations', 'name', 'convolution']]",[],"[['Model', 'has', 'All DNN operations']]",face_detection,21,54
1897,results,"As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .","[('bring about', (14, 16))]","[('multi-task RPN , Supervised Transformer , and feature combination', (4, 13)), ('1 % , 1 % , and 2 %', (16, 25)), ('recall improvement', (25, 27))]","[['multi-task RPN , Supervised Transformer , and feature combination', 'bring about', '1 % , 1 % , and 2 %']]","[['1 % , 1 % , and 2 %', 'has', 'recall improvement']]",[],"[['Results', 'has', 'multi-task RPN , Supervised Transformer , and feature combination']]",face_detection,21,255
1898,ablation-analysis,"Besides , these three parts are complementary , remove anyone part will cause a recall drop .","[('remove', (8, 9)), ('cause', (12, 13))]","[('anyone part', (9, 11)), ('recall drop', (14, 16))]",[],[],[],[],face_detection,21,256
1899,results,We found that NMS tend to include too much noisy low confidence candidates .,"[('found that', (1, 3)), ('include', (6, 7))]","[('NMS', (3, 4)), ('too much noisy low confidence candidates', (7, 13))]","[['NMS', 'include', 'too much noisy low confidence candidates']]",[],"[['Results', 'found that', 'NMS']]",[],face_detection,21,262
1900,results,"Our non - top K suppression is very close to using all candidates , and achieved consistently better results than NMS under the same number of candidates .","[('to using', (9, 11)), ('achieved', (15, 16)), ('than', (19, 20)), ('under', (21, 22)), ('of', (25, 26))]","[('Our non - top K suppression', (0, 6)), ('close', (8, 9)), ('all candidates', (11, 13)), ('consistently better results', (16, 19)), ('NMS', (20, 21)), ('same number', (23, 25)), ('candidates', (26, 27))]","[['close', 'to using', 'all candidates'], ['Our non - top K suppression', 'achieved', 'consistently better results'], ['consistently better results', 'than', 'NMS'], ['consistently better results', 'under', 'same number'], ['NMS', 'under', 'same number'], ['same number', 'of', 'candidates']]","[['Our non - top K suppression', 'has', 'close']]",[],"[['Results', 'has', 'Our non - top K suppression']]",face_detection,21,264
1901,results,"On the FDDB dataset , we compare with all public methods .","[('On', (0, 1)), ('compare with', (6, 8))]","[('FDDB dataset', (2, 4)), ('all public methods', (8, 11))]","[['FDDB dataset', 'compare with', 'all public methods']]",[],"[['Results', 'On', 'FDDB dataset']]",[],face_detection,21,285
1902,baselines,"On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .","[('compare with', (9, 11)), ('e.g.', (19, 20)), ('e.g .', (38, 40))]","[('AFW and PASCAL faces datasets', (2, 7)), ('deformable part based methods', (14, 18)), ('structure model', (20, 22)), ('Tree Parts Model', (23, 26)), ('cascade - based methods', (33, 37))]","[['deformable part based methods', 'e.g.', 'structure model'], ['deformable part based methods', 'e.g.', 'Tree Parts Model']]","[['deformable part based methods', 'name', 'structure model']]",[],[],face_detection,21,287
1903,baselines,"Headhunter ; ( 3 ) commercial system , e.g. face.com , Face ++ and Picasa .","[('e.g.', (8, 9))]","[('Headhunter', (0, 1)), ('commercial system', (5, 7)), ('face.com', (9, 10)), ('Face ++', (11, 13)), ('Picasa', (14, 15))]","[['commercial system', 'e.g.', 'face.com'], ['commercial system', 'e.g.', 'Face ++'], ['commercial system', 'e.g.', 'Picasa']]","[['commercial system', 'name', 'face.com']]",[],"[['Baselines', 'has', 'Headhunter']]",face_detection,21,288
1904,research-problem,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .",[],"[('face detection', (9, 11))]",[],[],[],[],face_detection,3,4
1905,research-problem,It is the first step in automatic face recognition applications .,[],"[('automatic face recognition', (6, 9))]",[],[],[],[],face_detection,3,14
1906,research-problem,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .",[],"[('unconstrained face detection problem', (15, 19))]",[],[],[],[],face_detection,3,19
1907,model,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .","[('called', (11, 12))]","[('simple pixel - level feature', (5, 10)), ('Normalized Pixel Difference ( NPD )', (13, 19))]","[['simple pixel - level feature', 'called', 'Normalized Pixel Difference ( NPD )']]",[],[],[],face_detection,3,34
1908,model,"An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values , in the same form as the Weber Fraction in experimental psychology .","[('computed as', (3, 5)), ('of', (7, 8)), ('between', (10, 11)), ('to', (16, 17))]","[('NPD', (1, 2)), ('ratio', (6, 7)), ('difference', (9, 10)), ('any two pixel intensity values', (11, 16)), ('sum of their values', (18, 22))]","[['NPD', 'computed as', 'ratio'], ['ratio', 'of', 'difference'], ['difference', 'between', 'any two pixel intensity values'], ['any two pixel intensity values', 'to', 'sum of their values']]",[],[],"[['Model', 'has', 'NPD']]",face_detection,3,35
1909,model,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .","[('such as', (8, 10))]","[('NPD feature', (1, 3)), ('several desirable properties', (4, 7)), ('scale invariance', (10, 12)), ('boundedness', (13, 14)), ('ability to reconstruct the original image', (16, 22))]","[['several desirable properties', 'such as', 'scale invariance'], ['several desirable properties', 'such as', 'boundedness'], ['several desirable properties', 'such as', 'ability to reconstruct the original image']]","[['NPD feature', 'has', 'several desirable properties']]",[],"[['Model', 'has', 'NPD feature']]",face_detection,3,36
1910,model,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .","[('show', (2, 3)), ('obtained from', (8, 10)), ('scaled for', (23, 25))]","[('NPD features', (4, 6)), ('lookup table', (11, 13)), ('resulting face detection template', (16, 20)), ('multiscale face detection', (25, 28))]","[['NPD features', 'obtained from', 'lookup table'], ['resulting face detection template', 'scaled for', 'multiscale face detection']]",[],"[['Model', 'show', 'NPD features']]",[],face_detection,3,37
1911,model,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .","[('propose', (3, 4)), ('construct', (11, 12)), ('to handle', (19, 21))]","[('deep quadratic tree learning method', (5, 10)), ('single soft - cascade AdaBoost classifier', (13, 19)), ('complex face manifolds', (21, 24)), ('arbitrary pose and occlusion conditions', (25, 30))]","[['deep quadratic tree learning method', 'construct', 'single soft - cascade AdaBoost classifier'], ['single soft - cascade AdaBoost classifier', 'to handle', 'complex face manifolds'], ['single soft - cascade AdaBoost classifier', 'to handle', 'arbitrary pose and occlusion conditions']]",[],"[['Model', 'propose', 'deep quadratic tree learning method']]",[],face_detection,3,38
1912,model,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .","[('automatically divided into', (10, 13)), ('of', (15, 16)), ('in', (25, 26)), ('partitioned in', (32, 34))]","[('different types of faces', (4, 8)), ('different leaves', (13, 15)), ('tree classifier', (17, 19)), ('complex face manifold', (22, 25)), ('high dimensional space', (27, 30)), ('learning process', (35, 37))]","[['different types of faces', 'automatically divided into', 'different leaves'], ['different leaves', 'of', 'tree classifier'], ['complex face manifold', 'in', 'high dimensional space'], ['high dimensional space', 'partitioned in', 'learning process']]",[],[],"[['Model', 'has', 'different types of faces']]",face_detection,3,40
1913,model,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .","[('tackle', (10, 11)), ('in', (14, 15)), ('without', (19, 20)), ('of', (21, 22)), ('in', (23, 24)), ('of', (27, 28))]","[('"" divide and conquer "" strategy', (3, 9)), ('unconstrained face detection', (11, 14)), ('single classifier', (16, 18)), ('pre-labeling', (20, 21)), ('views', (22, 23)), ('training set', (25, 27)), ('face images', (28, 30))]","[['"" divide and conquer "" strategy', 'tackle', 'unconstrained face detection'], ['unconstrained face detection', 'in', 'single classifier'], ['views', 'in', 'training set'], ['unconstrained face detection', 'without', 'pre-labeling'], ['single classifier', 'without', 'pre-labeling'], ['pre-labeling', 'of', 'views'], ['views', 'in', 'training set'], ['pre-labeling', 'of', 'views'], ['training set', 'of', 'face images']]",[],[],"[['Model', 'has', '"" divide and conquer "" strategy']]",face_detection,3,41
1914,model,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .","[('robust to', (5, 7)), ('in', (8, 9))]","[('resulting face detector', (1, 4)), ('variations', (7, 8)), ('pose , occlusion , and illumination', (9, 15)), ('blur and low image resolution', (20, 25))]","[['resulting face detector', 'robust to', 'variations'], ['resulting face detector', 'robust to', 'blur and low image resolution'], ['variations', 'in', 'pose , occlusion , and illumination']]",[],[],"[['Model', 'has', 'resulting face detector']]",face_detection,3,42
1915,code,The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / .,[],"[('http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface /', (10, 15))]",[],[],[],[],face_detection,3,52
1916,hyperparameters,We used a detection template of 24 24 pixels .,"[('used', (1, 2)), ('of', (5, 6))]","[('detection template', (3, 5)), ('24 24 pixels', (6, 9))]","[['detection template', 'of', '24 24 pixels']]","[['detection template', 'has', '24 24 pixels']]","[['Hyperparameters', 'used', 'detection template']]",[],face_detection,3,232
1917,hyperparameters,"We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .","[('set', (1, 2)), ('of', (5, 6)), ('learned as', (11, 13)), ('evaluated for', (25, 27))]","[('maximum depth', (3, 5)), ('tree classifiers', (7, 9)), ('eight', (13, 14)), ('at most eight NPD features', (17, 22)), ('each tree classifier', (27, 30))]","[['maximum depth', 'of', 'tree classifiers'], ['tree classifiers', 'learned as', 'eight'], ['at most eight NPD features', 'evaluated for', 'each tree classifier']]",[],"[['Hyperparameters', 'set', 'maximum depth']]",[],face_detection,3,233
1918,hyperparameters,"In the soft cascade training , we set the threshold of each exit as the minimal score of positive samples , i.e. we did not reject positive samples during training .","[('In', (0, 1)), ('set', (7, 8)), ('of', (10, 11)), ('as', (13, 14)), ('of', (17, 18))]","[('soft cascade training', (2, 5)), ('threshold', (9, 10)), ('each exit', (11, 13)), ('minimal score', (15, 17)), ('positive samples', (18, 20))]","[['soft cascade training', 'set', 'threshold'], ['threshold', 'of', 'each exit'], ['minimal score', 'of', 'positive samples'], ['threshold', 'as', 'minimal score'], ['each exit', 'as', 'minimal score'], ['minimal score', 'of', 'positive samples']]",[],"[['Hyperparameters', 'In', 'soft cascade training']]",[],face_detection,3,234
1919,hyperparameters,"Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .","[('contains', (3, 4))]","[('Our final detector', (0, 3)), ('1,226 deep quadratic trees', (4, 8)), ('46,401 NPD features', (10, 13))]","[['Our final detector', 'contains', '1,226 deep quadratic trees'], ['Our final detector', 'contains', '46,401 NPD features']]",[],[],"[['Hyperparameters', 'has', 'Our final detector']]",face_detection,3,235
1920,baselines,"For an analysis , we also trained a near frontal face detector using the proposed NPD features and the classic cascade of regression trees ( CART ) with depth of four .","[('trained', (6, 7)), ('using', (12, 13)), ('with', (27, 28))]","[('near frontal face detector', (8, 12)), ('proposed NPD features', (14, 17)), ('classic cascade of regression trees ( CART )', (19, 27)), ('depth', (28, 29)), ('four', (30, 31))]","[['near frontal face detector', 'using', 'proposed NPD features'], ['near frontal face detector', 'using', 'classic cascade of regression trees ( CART )'], ['classic cascade of regression trees ( CART )', 'with', 'depth']]",[],"[['Baselines', 'trained', 'near frontal face detector']]",[],face_detection,3,237
1921,hyperparameters,"A subset of the training data 2 in was used , including 12,102 face images and 12,315 nonface images .","[('including', (11, 12))]","[('subset of the training data', (1, 6)), ('12,102 face images', (12, 15)), ('12,315 nonface images', (16, 19))]","[['subset of the training data', 'including', '12,102 face images'], ['subset of the training data', 'including', '12,315 nonface images']]",[],[],"[['Hyperparameters', 'has', 'subset of the training data']]",face_detection,3,238
1922,hyperparameters,The detection template is 20 20 pixels .,"[('is', (3, 4))]","[('detection template', (1, 3)), ('20 20 pixels', (4, 7))]","[['detection template', 'is', '20 20 pixels']]","[['detection template', 'has', '20 20 pixels']]",[],"[['Hyperparameters', 'has', 'detection template']]",face_detection,3,239
1923,hyperparameters,"The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .","[('contains', (3, 4)), ('for', (8, 9)), ('was', (17, 18)), ('with', (20, 21)), ('of', (24, 25))]","[('detector cascade', (1, 3)), ('15 stages', (4, 6)), ('each stage', (9, 11)), ('target false accept rate', (13, 17)), ('0.5', (18, 19)), ('detection rate', (22, 24)), ('0.999', (25, 26))]","[['detector cascade', 'contains', '15 stages'], ['detector cascade', 'for', 'each stage'], ['target false accept rate', 'was', '0.5'], ['0.5', 'with', 'detection rate'], ['detection rate', 'of', '0.999']]","[['detector cascade', 'has', '15 stages'], ['each stage', 'has', 'target false accept rate'], ['target false accept rate', 'has', '0.5']]",[],"[['Hyperparameters', 'has', 'detector cascade']]",face_detection,3,240
1924,results,Evaluation on FDDB Database,[],"[('FDDB Database', (2, 4))]",[],[],[],[],face_detection,3,254
1925,results,"It can be observed that the proposed method outperforms most of the baseline methods except four methods , , , published recently .","[('observed that', (3, 5)), ('outperforms', (8, 9))]","[('proposed method', (6, 8)), ('most of the baseline methods', (9, 14))]","[['proposed method', 'outperforms', 'most of the baseline methods']]",[],"[['Results', 'observed that', 'proposed method']]",[],face_detection,3,265
1926,results,The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,"[('is', (5, 6)), ('at', (10, 11)), ('for', (14, 15)), ('for', (23, 24))]","[('proposed NPD face detector', (1, 5)), ('second best one', (7, 10)), ('FP = 0', (11, 14)), ('discrete metric', (16, 18)), ('third best one', (20, 23)), ('continuous metric', (25, 27))]","[['proposed NPD face detector', 'is', 'second best one'], ['proposed NPD face detector', 'is', 'third best one'], ['second best one', 'at', 'FP = 0'], ['FP = 0', 'for', 'discrete metric'], ['third best one', 'for', 'continuous metric'], ['third best one', 'for', 'continuous metric']]","[['proposed NPD face detector', 'has', 'second best one']]",[],"[['Results', 'has', 'proposed NPD face detector']]",face_detection,3,266
1927,results,"It can be observed that the proposed NPD detector is among the top performers for the discrete metric , though it is not as good as the four recent methods for the continuous metric .","[('is among', (9, 11)), ('for', (14, 15))]","[('proposed NPD detector', (6, 9)), ('top performers', (12, 14)), ('discrete metric', (16, 18))]","[['proposed NPD detector', 'is among', 'top performers'], ['top performers', 'for', 'discrete metric']]","[['proposed NPD detector', 'has', 'top performers']]",[],"[['Results', 'has', 'proposed NPD detector']]",face_detection,3,274
1928,results,"Compared to recent methods , the Joint Cascade algorithm is the most competitive one to us in terms of accuracy and speed ( see Sec. 5.6 ) .","[('is', (9, 10)), ('in terms of', (16, 19))]","[('Joint Cascade algorithm', (6, 9)), ('most competitive', (11, 13)), ('accuracy and speed', (19, 22))]","[['Joint Cascade algorithm', 'is', 'most competitive'], ['most competitive', 'in terms of', 'accuracy and speed']]","[['Joint Cascade algorithm', 'has', 'most competitive']]",[],"[['Results', 'has', 'Joint Cascade algorithm']]",face_detection,3,278
1929,results,Evaluation on GENKI Database,[],"[('GENKI Database', (2, 4))]",[],[],[],[],face_detection,3,295
1930,results,The results show that the proposed NPD face detector significantly outperforms both the Viola - Jones and PittPatt face detectors .,"[('significantly outperforms', (9, 11)), ('both', (11, 12))]","[('proposed NPD face detector', (5, 9)), ('Viola - Jones and PittPatt face detectors', (13, 20))]","[['proposed NPD face detector', 'significantly outperforms', 'Viola - Jones and PittPatt face detectors'], ['proposed NPD face detector', 'both', 'Viola - Jones and PittPatt face detectors']]",[],[],[],face_detection,3,307
1931,results,Evaluation on CMU - MIT Database,[],"[('CMU - MIT Database', (2, 6))]",[],[],[],[],face_detection,3,308
1932,results,"The results show that , compared to the Viola - Jones frontal face detector , the NPD detector performs better when the number of false positives , FP < 50 , while it is slightly worse than Viola - Jones at higher FPs .","[('compared to', (5, 7)), ('performs', (18, 19)), ('when', (20, 21)), ('than', (36, 37)), ('at', (40, 41))]","[('Viola - Jones frontal face detector', (8, 14)), ('NPD detector', (16, 18)), ('better', (19, 20)), ('number of false positives', (22, 26)), ('FP < 50', (27, 30)), ('slightly worse', (34, 36)), ('Viola - Jones', (37, 40)), ('higher FPs', (41, 43))]","[['NPD detector', 'performs', 'better'], ['better', 'when', 'number of false positives'], ['slightly worse', 'than', 'Viola - Jones'], ['Viola - Jones', 'at', 'higher FPs']]","[['Viola - Jones frontal face detector', 'has', 'NPD detector']]","[['Results', 'compared to', 'Viola - Jones frontal face detector']]",[],face_detection,3,314
1933,research-problem,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,4,4
1934,model,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .","[('propose', (5, 6)), ('for', (15, 16))]","[('Light and Fast Face Detector ( LFFD )', (7, 15)), ('edge devices', (16, 18)), ('considerably balancing', (19, 21)), ('accuracy and running efficiency', (22, 26))]","[['Light and Fast Face Detector ( LFFD )', 'for', 'edge devices']]","[['edge devices', 'has', 'considerably balancing'], ['considerably balancing', 'has', 'accuracy and running efficiency']]","[['Model', 'propose', 'Light and Fast Face Detector ( LFFD )']]",[],face_detection,4,42
1935,model,The proposed method is inspired by the one - stage and multi-scale object detection method SSD which also enlightens some other face detectors .,"[('inspired by', (4, 6)), ('enlightens', (18, 19))]","[('one - stage and multi-scale object detection method SSD', (7, 16)), ('some other face detectors', (19, 23))]","[['one - stage and multi-scale object detection method SSD', 'enlightens', 'some other face detectors']]",[],"[['Model', 'inspired by', 'one - stage and multi-scale object detection method SSD']]",[],face_detection,4,43
1936,model,One of the characteristics of SSD is that pre-defined anchor boxes are manually designed for each detection branch .,"[('of', (4, 5)), ('is', (6, 7)), ('for', (14, 15))]","[('One of the characteristics', (0, 4)), ('SSD', (5, 6)), ('pre-defined anchor boxes', (8, 11)), ('manually designed', (12, 14)), ('each detection branch', (15, 18))]","[['One of the characteristics', 'of', 'SSD'], ['SSD', 'is', 'pre-defined anchor boxes'], ['pre-defined anchor boxes', 'for', 'each detection branch'], ['manually designed', 'for', 'each detection branch']]","[['One of the characteristics', 'has', 'SSD'], ['SSD', 'has', 'pre-defined anchor boxes']]",[],"[['Model', 'has', 'One of the characteristics']]",face_detection,4,44
1937,experimental-setup,We initialize all parameters with xavier method and train the network from scratch .,"[('initialize', (1, 2)), ('with', (4, 5)), ('train', (8, 9)), ('from', (11, 12))]","[('parameters', (3, 4)), ('xavier method', (5, 7)), ('network', (10, 11)), ('scratch', (12, 13))]","[['parameters', 'with', 'xavier method'], ['network', 'from', 'scratch']]",[],"[['Experimental setup', 'initialize', 'parameters']]",[],face_detection,4,227
1938,experimental-setup,"The optimization method is SGD with 0.9 momentum , zero weight decay and batch size 32 .","[('is', (3, 4)), ('with', (5, 6))]","[('optimization method', (1, 3)), ('SGD', (4, 5)), ('0.9 momentum', (6, 8)), ('zero weight decay', (9, 12)), ('batch size 32', (13, 16))]","[['optimization method', 'is', 'SGD'], ['SGD', 'with', '0.9 momentum'], ['SGD', 'with', 'batch size 32']]","[['optimization method', 'has', 'SGD']]",[],"[['Experimental setup', 'has', 'optimization method']]",face_detection,4,229
1939,experimental-setup,The initial learning rate is 0.1 .,"[('is', (4, 5))]","[('initial learning rate', (1, 4)), ('0.1', (5, 6))]","[['initial learning rate', 'is', '0.1']]","[['initial learning rate', 'has', '0.1']]",[],"[['Experimental setup', 'has', 'initial learning rate']]",face_detection,4,232
1940,experimental-setup,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .","[('reduce', (5, 6)), ('by', (9, 10)), ('at', (12, 13))]","[('1,500,000 iterations', (2, 4)), ('learning rate', (7, 9)), ('multiplying', (10, 11)), ('0.1', (11, 12)), ('iteration', (13, 14)), ('600,000', (14, 15)), ('1,000,000', (16, 17)), ('1,200,000', (18, 19)), ('1,400,000', (20, 21))]","[['1,500,000 iterations', 'reduce', 'learning rate'], ['learning rate', 'by', 'multiplying'], ['learning rate', 'by', '1,400,000'], ['multiplying', 'at', '1,400,000'], ['0.1', 'at', 'iteration'], ['0.1', 'at', '1,400,000']]","[['multiplying', 'has', '0.1'], ['iteration', 'has', '600,000']]",[],[],face_detection,4,233
1941,experimental-setup,The training time is about 5 days with two NVIDIA GTX 1080 TI .,"[('about', (4, 5)), ('with', (7, 8))]","[('training time', (1, 3)), ('5 days', (5, 7)), ('two NVIDIA GTX 1080 TI', (8, 13))]","[['training time', 'about', '5 days'], ['5 days', 'with', 'two NVIDIA GTX 1080 TI']]","[['training time', 'has', '5 days']]",[],"[['Experimental setup', 'has', 'training time']]",face_detection,4,234
1942,experimental-setup,Our method is implemented using MXNet and the source code is released .,"[('implemented using', (3, 5))]","[('Our method', (0, 2)), ('MXNet', (5, 6))]","[['Our method', 'implemented using', 'MXNet']]",[],[],"[['Experimental setup', 'has', 'Our method']]",face_detection,4,235
1943,baselines,"Finally , the following methods are taken for comparison : DSFD ( Resnet152 backbone ) , Pyramid Box ( VGG16 backbone ) , S3 FD ( VGG16 backbone ) , SSH ( VGG16 backbone ) and FaceBoxes .",[],"[('DSFD ( Resnet152 backbone )', (10, 15)), ('Pyramid Box ( VGG16 backbone )', (16, 22)), ('S3 FD ( VGG16 backbone )', (23, 29)), ('SSH ( VGG16 backbone )', (30, 35)), ('FaceBoxes', (36, 37))]",[],[],[],[],face_detection,4,253
1944,results,FDDB dataset .,[],"[('FDDB dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'FDDB dataset']]",face_detection,4,261
1945,results,"DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .","[('achieve', (9, 10)), ('with', (12, 13))]","[('DSFD , Pyramid Box , S3FD and SSH', (0, 8)), ('high accuracy', (10, 12)), ('marginal gaps', (13, 15))]","[['DSFD , Pyramid Box , S3FD and SSH', 'achieve', 'high accuracy'], ['high accuracy', 'with', 'marginal gaps']]",[],[],"[['Results', 'has', 'DSFD , Pyramid Box , S3FD and SSH']]",face_detection,4,268
1946,results,"The proposed LFFD gains slightly lower accuracy than the first four methods , but outperforms FaceBoxes evidently .","[('gains', (3, 4)), ('than', (7, 8)), ('outperforms', (14, 15))]","[('proposed LFFD', (1, 3)), ('slightly lower accuracy', (4, 7)), ('first four methods', (9, 12)), ('FaceBoxes', (15, 16))]","[['proposed LFFD', 'gains', 'slightly lower accuracy'], ['slightly lower accuracy', 'than', 'first four methods'], ['proposed LFFD', 'outperforms', 'FaceBoxes']]",[],[],"[['Results', 'has', 'proposed LFFD']]",face_detection,4,269
1947,results,The results indicate that LFFD is superior for detecting unconstrained faces .,"[('for detecting', (7, 9))]","[('LFFD', (4, 5)), ('superior', (6, 7)), ('unconstrained faces', (9, 11))]","[['superior', 'for detecting', 'unconstrained faces']]","[['LFFD', 'has', 'superior']]",[],[],face_detection,4,270
1948,results,WIDER FACE dataset .,[],"[('WIDER FACE dataset', (0, 3))]",[],[],[],"[['Results', 'has', 'WIDER FACE dataset']]",face_detection,4,271
1949,results,"Firstly , performance drop is evident for DSFD , PyramidBox , S3FD and SSH compared to their original results .","[('evident for', (5, 7))]","[('performance drop', (2, 4)), ('DSFD', (7, 8)), ('PyramidBox', (9, 10)), ('S3FD and SSH', (11, 14))]","[['performance drop', 'evident for', 'DSFD'], ['performance drop', 'evident for', 'S3FD and SSH']]",[],[],"[['Results', 'has', 'performance drop']]",face_detection,4,282
1950,results,"Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .","[('obtains', (4, 5)), ('on', (8, 9)), ('of', (15, 16)), ('on', (17, 18)), ('due to', (24, 26))]","[('Pyramid Box', (2, 4)), ('best results', (6, 8)), ('Hard parts', (9, 11)), ('performance', (14, 15)), ('SSH', (16, 17)), ('Hard parts', (18, 20)), ('decreased dramatically', (21, 23)), ('neglect of some tiny faces', (27, 32))]","[['Pyramid Box', 'obtains', 'best results'], ['best results', 'on', 'Hard parts'], ['SSH', 'on', 'Hard parts'], ['performance', 'of', 'SSH'], ['SSH', 'on', 'Hard parts'], ['decreased dramatically', 'due to', 'neglect of some tiny faces']]","[['Pyramid Box', 'has', 'best results']]",[],"[['Results', 'has', 'Pyramid Box']]",face_detection,4,285
1951,results,"Thirdly , FaceBoxes does not get desirable results on Medium and Hard parts .","[('not get', (4, 6)), ('on', (8, 9))]","[('FaceBoxes', (2, 3)), ('desirable results', (6, 8)), ('Medium and Hard parts', (9, 13))]","[['FaceBoxes', 'not get', 'desirable results'], ['desirable results', 'on', 'Medium and Hard parts']]",[],[],"[['Results', 'has', 'FaceBoxes']]",face_detection,4,286
1952,results,"Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .","[('consistently outperforms', (6, 8)), ('having', (13, 14)), ('with', (15, 16))]","[('proposed method LFFD', (3, 6)), ('Face - Boxes', (8, 11)), ('gaps', (14, 15)), ('state of the art methods', (16, 21))]","[['proposed method LFFD', 'consistently outperforms', 'Face - Boxes'], ['proposed method LFFD', 'having', 'gaps'], ['gaps', 'with', 'state of the art methods']]",[],[],[],face_detection,4,292
1953,results,"Additionally , LFFD is better than SSH that uses VGG16 as the backbone on Hard parts .","[('better than', (4, 6)), ('uses', (8, 9)), ('as', (10, 11)), ('on', (13, 14))]","[('LFFD', (2, 3)), ('SSH', (6, 7)), ('VGG16', (9, 10)), ('backbone', (12, 13)), ('Hard parts', (14, 16))]","[['LFFD', 'better than', 'SSH'], ['SSH', 'uses', 'VGG16'], ['VGG16', 'as', 'backbone'], ['backbone', 'on', 'Hard parts']]",[],[],"[['Results', 'has', 'LFFD']]",face_detection,4,293
1954,research-problem,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",[],"[('Face detection and alignment in unconstrained environment', (2, 9))]",[],[],[],[],face_detection,5,5
1955,research-problem,"However , most of the available face detection and face alignment methods ignore the inherent correlation between these two tasks .",[],"[('face detection and face alignment', (6, 11))]",[],[],[],[],face_detection,5,30
1956,model,"In this paper , we propose a new framework to integrate these two tasks using unified cascaded CNNs by multi-task learning .","[('propose', (5, 6)), ('integrate', (10, 11)), ('using', (14, 15)), ('by', (18, 19))]","[('new framework', (7, 9)), ('two tasks', (12, 14)), ('unified cascaded CNNs', (15, 18)), ('multi-task learning', (19, 21))]","[['new framework', 'integrate', 'two tasks'], ['two tasks', 'using', 'unified cascaded CNNs'], ['unified cascaded CNNs', 'by', 'multi-task learning']]",[],"[['Model', 'propose', 'new framework']]",[],face_detection,5,40
1957,model,The proposed CNNs consist of three stages .,"[('consist of', (3, 5))]","[('proposed CNNs', (1, 3)), ('three stages', (5, 7))]","[['proposed CNNs', 'consist of', 'three stages']]","[['proposed CNNs', 'has', 'three stages']]",[],"[['Model', 'has', 'proposed CNNs']]",face_detection,5,41
1958,model,"In the first stage , it produces candidate windows quickly through a shallow CNN .","[('In', (0, 1)), ('produces', (6, 7)), ('through', (10, 11))]","[('first stage', (2, 4)), ('candidate windows', (7, 9)), ('shallow CNN', (12, 14))]","[['first stage', 'produces', 'candidate windows'], ['candidate windows', 'through', 'shallow CNN']]",[],"[['Model', 'In', 'first stage']]",[],face_detection,5,42
1959,model,"Then , it refines the windows to reject a large number of non-faces windows through a more complex CNN .","[('refines', (3, 4)), ('to reject', (6, 8)), ('through', (14, 15))]","[('windows', (5, 6)), ('large number', (9, 11)), ('non-faces windows', (12, 14)), ('more complex CNN', (16, 19))]","[['windows', 'to reject', 'large number'], ['non-faces windows', 'through', 'more complex CNN']]",[],"[['Model', 'refines', 'windows']]",[],face_detection,5,43
1960,model,"Finally , it uses a more powerful CNN to refine the result and output facial landmarks positions .","[('uses', (3, 4)), ('to refine', (8, 10)), ('output', (13, 14))]","[('more powerful CNN', (5, 8)), ('result', (11, 12)), ('facial landmarks positions', (14, 17))]","[['more powerful CNN', 'to refine', 'result'], ['more powerful CNN', 'output', 'facial landmarks positions']]",[],"[['Model', 'uses', 'more powerful CNN']]",[],face_detection,5,44
1961,experiments,"Then we compare our face detector and alignment against the state - of - the - art methods in Face Detection Data Set and Benchmark ( FDDB ) , WIDER FACE , and Annotated Facial Landmarks in the Wild ( AFLW ) benchmark .","[('compare', (2, 3)), ('against', (8, 9)), ('in', (18, 19))]","[('face detector and alignment', (4, 8)), ('state - of - the - art methods', (10, 18)), ('Face Detection Data Set and Benchmark ( FDDB )', (19, 28)), ('WIDER FACE', (29, 31)), ('Annotated Facial Landmarks in the Wild ( AFLW ) benchmark', (33, 43))]","[['face detector and alignment', 'against', 'state - of - the - art methods'], ['state - of - the - art methods', 'in', 'Face Detection Data Set and Benchmark ( FDDB )'], ['state - of - the - art methods', 'in', 'Annotated Facial Landmarks in the Wild ( AFLW ) benchmark']]",[],[],[],face_detection,5,108
1962,experiments,The effectiveness of online hard sample mining,"[('of', (2, 3))]","[('effectiveness', (1, 2)), ('online hard sample mining', (3, 7))]","[['effectiveness', 'of', 'online hard sample mining']]",[],[],[],face_detection,5,122
1963,results,It is very clear that the hard sample mining is beneficial to performance improvement .,"[('beneficial to', (10, 12))]","[('very clear', (2, 4)), ('hard sample mining', (6, 9)), ('performance improvement', (12, 14))]","[['hard sample mining', 'beneficial to', 'performance improvement']]","[['very clear', 'has', 'hard sample mining']]",[],"[['Results', 'has', 'very clear']]",face_detection,5,128
1964,experiments,The effectiveness of joint detection and alignment,[],"[('joint detection and alignment', (3, 7))]",[],[],[],[],face_detection,5,130
1965,results,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,"[('suggests', (15, 16)), ('beneficial for', (23, 25))]","[('joint landmarks localization task learning', (17, 22)), ('face classification and bounding box regression tasks', (26, 33))]","[['joint landmarks localization task learning', 'beneficial for', 'face classification and bounding box regression tasks']]",[],"[['Results', 'suggests', 'joint landmarks localization task learning']]",[],face_detection,5,132
1966,experiments,D. Evaluation on face detection,"[('on', (2, 3))]","[('face detection', (3, 5))]",[],[],[],[],face_detection,5,133
1967,results,( a ) - ( d ) shows that our method consistently outperforms all the previous approaches by a large margin in both the benchmarks .,"[('consistently outperforms', (11, 13)), ('by', (17, 18)), ('in', (21, 22))]","[('all the previous approaches', (13, 17)), ('large margin', (19, 21)), ('benchmarks', (24, 25))]","[['all the previous approaches', 'by', 'large margin'], ['large margin', 'in', 'benchmarks']]",[],[],[],face_detection,5,135
1968,experiments,Evaluation on face alignment,[],"[('face alignment', (2, 4))]",[],[],[],[],face_detection,5,138
1969,results,( e ) shows that our method outperforms all the state - of - the - art methods with a margin .,"[('with', (18, 19))]","[('outperforms', (7, 8)), ('all the state - of - the - art methods', (8, 18)), ('margin', (20, 21))]","[['outperforms', 'with', 'margin'], ['all the state - of - the - art methods', 'with', 'margin']]","[['outperforms', 'has', 'all the state - of - the - art methods']]",[],"[['Results', 'has', 'outperforms']]",face_detection,5,143
1970,research-problem,Robust Face Detection via Learning Small Faces on Hard Images,[],"[('Robust Face Detection', (0, 3))]",[],[],[],[],face_detection,6,2
1971,research-problem,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .",[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,6,13
1972,approach,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .","[('propose', (6, 7)), ('mine', (8, 9)), ('at', (11, 12)), ('in parallel with', (14, 17))]","[('hard examples', (9, 11)), ('image level', (12, 14)), ('anchor level', (17, 19))]","[['hard examples', 'at', 'image level'], ['hard examples', 'in parallel with', 'anchor level']]",[],"[['Approach', 'propose', 'hard examples']]",[],face_detection,6,27
1973,approach,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .","[('to', (5, 6)), ('dynamically assign', (6, 8)), ('during', (13, 14)), ('determine whether', (20, 22)), ('for', (32, 33))]","[('difficulty scores', (8, 10)), ('training images', (11, 13)), ('learning process', (15, 17)), ('image', (23, 24)), ('well - detected', (26, 29)), ('useful', (31, 32)), ('further training', (33, 35))]","[['difficulty scores', 'to', 'training images'], ['training images', 'during', 'learning process'], ['learning process', 'determine whether', 'image'], ['useful', 'for', 'further training']]",[],"[['Approach', 'to', 'difficulty scores']]",[],face_detection,6,28
1974,approach,This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process .,"[('fully utilize', (4, 6))]","[('images', (7, 8)), ('not perfectly detected', (10, 13)), ('better facilitate', (14, 16)), ('learning process', (18, 20))]",[],"[['images', 'has', 'not perfectly detected'], ['better facilitate', 'has', 'learning process']]","[['Approach', 'fully utilize', 'images']]",[],face_detection,6,29
1975,approach,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .","[('improve', (11, 12)), ('by exclusively exploiting', (15, 18))]","[('detection quality', (13, 15)), ('small faces', (18, 20))]","[['detection quality', 'by exclusively exploiting', 'small faces']]",[],"[['Approach', 'improve', 'detection quality']]",[],face_detection,6,31
1976,approach,"Compared with these methods , our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training .","[('is', (7, 8)), ('leveraging', (17, 18)), ('during', (21, 22))]","[('our detector', (5, 7)), ('more efficient', (8, 10)), ('specially designed', (13, 15)), ('small faces', (19, 21)), ('training', (22, 23))]","[['our detector', 'is', 'more efficient'], ['small faces', 'during', 'training']]","[['our detector', 'has', 'more efficient']]",[],"[['Approach', 'has', 'our detector']]",face_detection,6,34
1977,experimental-setup,"We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .","[('use', (1, 2)), ('to initialize', (7, 9)), ('with', (21, 22))]","[('ImageNet pretrained VGG16 model', (3, 7)), ('network backbone', (10, 12)), ('our newly introduced layers', (14, 18)), ('randomly initialized', (19, 21)), ('Gaussian initialization', (22, 24))]","[['ImageNet pretrained VGG16 model', 'to initialize', 'network backbone'], ['randomly initialized', 'with', 'Gaussian initialization']]","[['our newly introduced layers', 'has', 'randomly initialized']]","[['Experimental setup', 'use', 'ImageNet pretrained VGG16 model']]",[],face_detection,6,165
1978,experimental-setup,"We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .","[('train', (1, 2)), ('with', (4, 5)), ('to be', (7, 9)), ('for', (11, 12)), ('with', (16, 17)), ('of', (20, 21)), ('with', (30, 31)), ('of', (35, 36))]","[('model', (3, 4)), ('itersize', (6, 7)), ('2', (9, 10)), ('46 k iterations', (12, 15)), ('learning rate', (18, 20)), ('0.004', (21, 22)), ('another 14 k iterations', (26, 30)), ('smaller learning rate', (32, 35)), ('0.0004', (36, 37))]","[['model', 'with', 'itersize'], ['model', 'with', 'learning rate'], ['46 k iterations', 'with', 'learning rate'], ['another 14 k iterations', 'with', 'smaller learning rate'], ['itersize', 'to be', '2'], ['itersize', 'for', '46 k iterations'], ['46 k iterations', 'with', 'learning rate'], ['another 14 k iterations', 'with', 'smaller learning rate'], ['learning rate', 'of', '0.004'], ['smaller learning rate', 'of', '0.0004'], ['another 14 k iterations', 'with', 'smaller learning rate'], ['smaller learning rate', 'of', '0.0004']]","[['learning rate', 'has', '0.004']]","[['Experimental setup', 'train', 'model']]",[],face_detection,6,166
1979,experimental-setup,"During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .","[('During', (0, 1)), ('use', (4, 5)), ('to compute', (9, 11)), ('update', (14, 15)), ('by', (17, 18)), ('with', (20, 21))]","[('training', (1, 2)), ('4 GPUs', (5, 7)), ('gradient', (12, 13)), ('weight', (16, 17)), ('synchronized SGD', (18, 20)), ('Momentum', (21, 22))]","[['training', 'use', '4 GPUs'], ['4 GPUs', 'to compute', 'gradient'], ['4 GPUs', 'update', 'weight'], ['weight', 'by', 'synchronized SGD'], ['synchronized SGD', 'with', 'Momentum']]",[],"[['Experimental setup', 'During', 'training']]",[],face_detection,6,167
1980,experimental-setup,"The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .","[('of', (4, 5)), ('are', (6, 7)), ('during', (8, 9)), ('of', (16, 17)), ('set to have', (19, 22))]","[('first two blocks', (1, 4)), ('VGG16', (5, 6)), ('frozen', (7, 8)), ('training', (10, 11)), ('rest layers', (14, 16)), ('VGG16', (17, 18)), ('double learning rate', (23, 26))]","[['first two blocks', 'of', 'VGG16'], ['rest layers', 'of', 'VGG16'], ['VGG16', 'are', 'frozen'], ['frozen', 'during', 'training'], ['rest layers', 'of', 'VGG16'], ['rest layers', 'set to have', 'double learning rate']]",[],[],"[['Experimental setup', 'has', 'first two blocks']]",face_detection,6,168
1981,experimental-setup,"Specifically , we resize the testing image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels for evaluation on WIDER FACE dataset .","[('resize', (3, 4)), ('contains', (12, 13)), ('for', (23, 24)), ('on', (25, 26))]","[('testing image', (5, 7)), ('short side', (10, 12)), ('100 , 300 , 600 , 1000 and 1400 pixels', (13, 23)), ('evaluation', (24, 25)), ('WIDER FACE dataset', (26, 29))]","[['short side', 'contains', '100 , 300 , 600 , 1000 and 1400 pixels'], ['100 , 300 , 600 , 1000 and 1400 pixels', 'for', 'evaluation'], ['evaluation', 'on', 'WIDER FACE dataset']]",[],"[['Experimental setup', 'resize', 'testing image']]",[],face_detection,6,170
1982,experimental-setup,We also follow the testing strategies used in Pyra - midBox 2 such as horizontal flip and bounding - box voting .,"[('follow', (2, 3)), ('used in', (6, 8)), ('such as', (12, 14))]","[('testing strategies', (4, 6)), ('Pyra - midBox', (8, 11)), ('horizontal flip', (14, 16)), ('bounding - box voting', (17, 21))]","[['testing strategies', 'used in', 'Pyra - midBox'], ['Pyra - midBox', 'such as', 'horizontal flip'], ['Pyra - midBox', 'such as', 'bounding - box voting']]",[],"[['Experimental setup', 'follow', 'testing strategies']]",[],face_detection,6,171
1983,results,"In , we show the precision - recall ( PR ) curve and average precision ( AP ) for our model compared with many other state - of - the - arts on these three subsets .","[('show', (3, 4)), ('for', (18, 19)), ('compared with', (21, 23))]","[('precision - recall ( PR ) curve and average precision ( AP )', (5, 18)), ('our model', (19, 21)), ('many other state - of - the - arts', (23, 32))]","[['precision - recall ( PR ) curve and average precision ( AP )', 'for', 'our model'], ['precision - recall ( PR ) curve and average precision ( AP )', 'compared with', 'many other state - of - the - arts'], ['our model', 'compared with', 'many other state - of - the - arts']]",[],"[['Results', 'show', 'precision - recall ( PR ) curve and average precision ( AP )']]",[],face_detection,6,174
1984,results,"As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .","[('achieves', (7, 8)), ('on', (11, 12)), ('outperforms', (17, 18)), ('by', (27, 28))]","[('our method', (5, 7)), ('best performance', (9, 11)), ('hard subset', (13, 15)), ('current state - of - the - art', (19, 27)), ('large margin', (29, 31))]","[['our method', 'achieves', 'best performance'], ['best performance', 'on', 'hard subset'], ['our method', 'outperforms', 'current state - of - the - art'], ['current state - of - the - art', 'by', 'large margin']]",[],[],"[['Results', 'has', 'our method']]",face_detection,6,175
1985,results,"Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - thearts .","[('on', (2, 3)), ('to', (8, 9)), ('on', (22, 23))]","[('Our performance', (0, 2)), ('medium subset', (4, 6)), ('comparable', (7, 8)), ('most recent state - of - the - art', (10, 19)), ('performance', (21, 22)), ('easy subset', (24, 26)), ('bit worse', (28, 30))]","[['Our performance', 'on', 'medium subset'], ['performance', 'on', 'easy subset'], ['comparable', 'to', 'most recent state - of - the - art'], ['performance', 'on', 'easy subset']]",[],[],"[['Results', 'has', 'Our performance']]",face_detection,6,177
1986,results,"We show the discontinuous ROC curve at compared with , and our method achieves the state - of - the - art performance of TPR = 98.7 % given 1000 false positives .","[('achieves', (13, 14)), ('of', (23, 24)), ('given', (28, 29))]","[('discontinuous ROC curve', (3, 6)), ('our method', (11, 13)), ('state - of - the - art performance', (15, 23)), ('TPR = 98.7 %', (24, 28)), ('1000 false positives', (29, 32))]","[['our method', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'of', 'TPR = 98.7 %'], ['TPR = 98.7 %', 'given', '1000 false positives']]","[['discontinuous ROC curve', 'has', 'our method']]",[],[],face_detection,6,183
1987,results,"We show the PR curve at compared with , and our method achieves a new the state - of - the - art performance of AP = 99.0 .","[('achieves', (12, 13)), ('of', (24, 25))]","[('PR curve', (3, 5)), ('our method', (10, 12)), ('new the state - of - the - art performance', (14, 24)), ('AP = 99.0', (25, 28))]","[['our method', 'achieves', 'new the state - of - the - art performance'], ['new the state - of - the - art performance', 'of', 'AP = 99.0']]","[['PR curve', 'has', 'our method']]",[],[],face_detection,6,185
1988,results,"As shown in compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 .","[('with', (4, 5)), ('of', (24, 25))]","[('state - of - the - art and almost perfect performance', (9, 20)), ('AP', (23, 24)), ('99.60', (25, 26))]","[['state - of - the - art and almost perfect performance', 'with', 'AP'], ['AP', 'of', '99.60']]","[['AP', 'has', '99.60']]",[],[],face_detection,6,187
1989,results,"Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors .","[('with', (2, 3)), ('performs', (7, 8)), ('than', (9, 10)), ('despite', (18, 19))]","[('Our model', (0, 2)), ('single detection feature map', (3, 7)), ('better', (8, 9)), ('one with three detection feature maps', (11, 17)), ('shallower structure', (20, 22)), ('fewer parameters and anchors', (23, 27))]","[['Our model', 'with', 'single detection feature map'], ['single detection feature map', 'performs', 'better'], ['better', 'than', 'one with three detection feature maps'], ['better', 'despite', 'shallower structure'], ['one with three detection feature maps', 'despite', 'shallower structure'], ['one with three detection feature maps', 'despite', 'fewer parameters and anchors']]",[],[],"[['Results', 'has', 'Our model']]",face_detection,6,196
1990,results,HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead .,"[('improve', (2, 3)), ('on', (5, 6)), ('without involving', (9, 11))]","[('HIM', (0, 1)), ('performance', (4, 5)), ('hard subset', (6, 8)), ('significantly', (8, 9)), ('more complex network architecture', (11, 15))]","[['HIM', 'improve', 'performance'], ['performance', 'on', 'hard subset'], ['performance', 'without involving', 'more complex network architecture'], ['hard subset', 'without involving', 'more complex network architecture'], ['significantly', 'without involving', 'more complex network architecture']]","[['hard subset', 'has', 'significantly']]",[],"[['Results', 'has', 'HIM']]",face_detection,6,199
1991,results,"DH itself can also boost the performance , which shows the effectiveness of designing larger convolution for larger anchors .","[('boost', (4, 5)), ('shows', (9, 10)), ('of designing', (12, 14)), ('for', (16, 17))]","[('DH', (0, 1)), ('performance', (6, 7)), ('effectiveness', (11, 12)), ('larger convolution', (14, 16)), ('larger anchors', (17, 19))]","[['DH', 'boost', 'performance'], ['performance', 'shows', 'effectiveness'], ['effectiveness', 'of designing', 'larger convolution'], ['larger convolution', 'for', 'larger anchors']]",[],[],"[['Results', 'has', 'DH']]",face_detection,6,200
1992,results,Combining HIM and DH together can improve further towards the state - of - the - art performance .,"[('towards', (8, 9))]","[('Combining HIM and DH together', (0, 5)), ('improve further', (6, 8)), ('state - of - the - art performance', (10, 18))]","[['improve further', 'towards', 'state - of - the - art performance']]","[['Combining HIM and DH together', 'has', 'improve further']]",[],"[['Results', 'has', 'Combining HIM and DH together']]",face_detection,6,201
1993,ablation-analysis,Both photometric distortion and cropping can contribute to a more robust face detector .,"[('contribute to', (6, 8))]","[('Both photometric distortion and cropping', (0, 5)), ('more robust face detector', (9, 13))]","[['Both photometric distortion and cropping', 'contribute to', 'more robust face detector']]",[],[],"[['Ablation analysis', 'has', 'Both photometric distortion and cropping']]",face_detection,6,211
1994,results,"Diagnosis of multi-scale testing. , the extra small scales are crucial to detect easy faces .","[('to detect', (11, 13))]","[('extra small scales', (6, 9)), ('easy faces', (13, 15))]",[],[],[],[],face_detection,6,219
1995,research-problem,Recurrent Scale Approximation for Object Detection in CNN,[],"[('Object Detection', (4, 6))]",[],[],[],[],face_detection,7,2
1996,research-problem,"Since convolutional neural network ( CNN ) lacks an inherent mechanism to handle large scale variations , we always need to compute feature maps multiple times for multiscale object detection , which has the bottleneck of computational cost in practice .",[],"[('multiscale object detection', (27, 30))]",[],[],[],[],face_detection,7,4
1997,code,"Our codes and annotations mentioned in Sec.4.1 can be accessed at github.com/sciencefans/RSA-for-object-detection dle the variations caused by appearance , location and scale .",[],"[('github.com/sciencefans/RSA-for-object-detection', (11, 12))]",[],[],[],[],face_detection,7,17
1998,model,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .","[('propose', (5, 6))]","[('recurrent scale approximation ( RSA , see ) unit', (7, 16))]",[],[],"[['Model', 'propose', 'recurrent scale approximation ( RSA , see ) unit']]",[],face_detection,7,35
1999,model,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,"[('to be', (5, 7)), ('at', (8, 9)), ('in', (12, 13)), ('with', (19, 20)), ('at', (24, 25))]","[('RSA unit', (1, 3)), ('plugged', (7, 8)), ('some specific depths', (9, 12)), ('network', (14, 15)), ('fed', (18, 19)), ('initial feature map', (21, 24)), ('largest scale', (26, 28))]","[['RSA unit', 'to be', 'plugged'], ['RSA unit', 'to be', 'fed'], ['plugged', 'at', 'some specific depths'], ['initial feature map', 'at', 'largest scale'], ['some specific depths', 'in', 'network'], ['fed', 'with', 'initial feature map'], ['initial feature map', 'at', 'largest scale']]",[],[],"[['Model', 'has', 'RSA unit']]",face_detection,7,36
2000,model,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,"[('convolves', (2, 3)), ('in', (5, 6)), ('to generate', (9, 11)), ('of', (13, 14)), ('that is', (17, 19)), ('half', (19, 20))]","[('unit', (1, 2)), ('input', (4, 5)), ('recurrent manner', (7, 9)), ('prediction', (12, 13)), ('feature map', (15, 17)), ('size of the input', (21, 25))]","[['unit', 'convolves', 'input'], ['input', 'in', 'recurrent manner'], ['unit', 'to generate', 'prediction'], ['recurrent manner', 'to generate', 'prediction'], ['prediction', 'of', 'feature map'], ['prediction', 'half', 'size of the input'], ['feature map', 'half', 'size of the input']]",[],[],"[['Model', 'has', 'unit']]",face_detection,7,37
2001,model,Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit - a balance considering both efficiency and accuracy .,"[('feed', (4, 5)), ('with', (7, 8)), ('at', (9, 10)), ('approximate', (14, 15)), ('at', (18, 19)), ('through', (21, 22))]","[('network', (6, 7)), ('input', (8, 9)), ('one scale only', (10, 13)), ('rest features', (16, 18)), ('smaller scales', (19, 21)), ('learnable RSA unit', (23, 26))]","[['network', 'with', 'input'], ['input', 'at', 'one scale only'], ['rest features', 'at', 'smaller scales'], ['rest features', 'at', 'smaller scales'], ['smaller scales', 'through', 'learnable RSA unit']]",[],[],[],face_detection,7,38
2002,model,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,"[('is', (2, 3)), ('to globally predict', (8, 11)), ('for', (13, 14)), ('compute', (19, 20)), ('for', (22, 23)), ('based on', (29, 31))]","[('first', (1, 2)), ('scale - forecast network', (4, 8)), ('scales', (12, 13)), ('novel image', (15, 17)), ('feature pyramids', (20, 22)), ('certain set of scales', (25, 29)), ('prediction', (32, 33))]","[['first', 'is', 'scale - forecast network'], ['scales', 'for', 'novel image'], ['first', 'compute', 'feature pyramids'], ['feature pyramids', 'for', 'certain set of scales'], ['certain set of scales', 'based on', 'prediction']]","[['first', 'has', 'scale - forecast network']]",[],"[['Model', 'has', 'first']]",face_detection,7,40
2003,model,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,"[('is', (2, 3)), ('retraces', (8, 9)), ('of', (11, 12)), ('in', (15, 16)), ('generates', (20, 21)), ('for', (24, 25)), ('based on', (27, 29))]","[('second', (1, 2)), ('landmark retracing network', (4, 7)), ('location', (10, 11)), ('regressed landmarks', (13, 15)), ('preceding layers', (17, 19)), ('confidence score', (22, 24)), ('each landmark', (25, 27)), ('landmark feature set', (30, 33))]","[['second', 'is', 'landmark retracing network'], ['landmark retracing network', 'retraces', 'location'], ['location', 'of', 'regressed landmarks'], ['regressed landmarks', 'in', 'preceding layers'], ['second', 'generates', 'confidence score'], ['landmark retracing network', 'generates', 'confidence score'], ['confidence score', 'for', 'each landmark'], ['each landmark', 'based on', 'landmark feature set']]","[['second', 'has', 'landmark retracing network']]",[],"[['Model', 'has', 'second']]",face_detection,7,42
2004,model,The final score of identifying a face within an anchor is thereby revised by the LRN network .,"[('of identifying', (3, 5)), ('within', (7, 8)), ('revised by', (12, 14))]","[('final score', (1, 3)), ('face', (6, 7)), ('an anchor', (8, 10)), ('LRN network', (15, 17))]","[['final score', 'of identifying', 'face'], ['face', 'within', 'an anchor']]",[],[],"[['Model', 'has', 'final score']]",face_detection,7,43
2005,model,The three components can be incorporated into a unified CNN framework and trained end - to - end .,"[('be incorporated into', (4, 7)), ('trained', (12, 13))]","[('three components', (1, 3)), ('unified CNN framework', (8, 11)), ('end - to - end', (13, 18))]","[['three components', 'be incorporated into', 'unified CNN framework'], ['three components', 'trained', 'end - to - end']]",[],[],"[['Model', 'has', 'three components']]",face_detection,7,46
2006,experiments,"The structure of our model is a shallow version of the ResNet where the first seven ResNet blocks are used , i.e. , from conv1 to res3c .","[('of', (2, 3)), ('is', (5, 6)), ('of', (9, 10))]","[('structure', (1, 2)), ('our model', (3, 5)), ('shallow version', (7, 9)), ('ResNet', (11, 12))]","[['structure', 'of', 'our model'], ['shallow version', 'of', 'ResNet'], ['structure', 'is', 'shallow version'], ['our model', 'is', 'shallow version'], ['shallow version', 'of', 'ResNet']]","[['structure', 'has', 'our model'], ['our model', 'has', 'shallow version']]",[],[],face_detection,7,178
2007,experiments,We use this model in scale - forecast network and LRN .,"[('use', (1, 2)), ('in', (4, 5))]","[('model', (3, 4)), ('scale - forecast network and LRN', (5, 11))]","[['model', 'in', 'scale - forecast network and LRN']]",[],[],[],face_detection,7,179
2008,experiments,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .","[('set to', (5, 7)), ('of', (8, 9)), ('for', (14, 15)), ('of', (17, 18))]","[('numbers of channels', (1, 4)), ('half', (7, 8)), ('original ResNet model', (10, 13)), ('consideration', (16, 17)), ('time efficiency', (18, 20))]","[['numbers of channels', 'set to', 'half'], ['half', 'of', 'original ResNet model'], ['consideration', 'of', 'time efficiency'], ['half', 'for', 'consideration'], ['original ResNet model', 'for', 'consideration'], ['consideration', 'of', 'time efficiency']]","[['consideration', 'has', 'time efficiency']]",[],[],face_detection,7,180
2009,experiments,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,"[('train', (2, 3)), ('use', (10, 11)), ('of', (13, 14)), ('to launch', (16, 18))]","[('scale - forecast network', (4, 8)), ('output', (12, 13)), ('predicted scales', (14, 16)), ('RSA unit and LRN', (19, 23))]","[['output', 'of', 'predicted scales'], ['predicted scales', 'to launch', 'RSA unit and LRN']]",[],[],[],face_detection,7,181
2010,experiments,The ratio of the positive and the negative is 1 : 1 in all experiments .,"[('of', (2, 3)), ('is', (8, 9)), ('in', (12, 13))]","[('ratio', (1, 2)), ('the positive and the negative', (3, 8)), ('1 : 1', (9, 12)), ('all experiments', (13, 15))]","[['ratio', 'of', 'the positive and the negative'], ['the positive and the negative', 'is', '1 : 1'], ['1 : 1', 'in', 'all experiments']]","[['the positive and the negative', 'has', '1 : 1']]",[],[],face_detection,7,183
2011,experiments,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .","[('is', (3, 4)), ('set to', (10, 12)), ('with', (13, 14)), ('of', (16, 17)), ('every', (19, 20))]","[('batch size', (1, 3)), ('4', (4, 5)), ('base learning rate', (6, 9)), ('0.001', (12, 13)), ('decrease', (15, 16)), ('6 %', (17, 19)), ('10,000 iterations', (20, 22))]","[['batch size', 'is', '4'], ['base learning rate', 'set to', '0.001'], ['0.001', 'with', 'decrease'], ['decrease', 'of', '6 %'], ['6 %', 'every', '10,000 iterations']]","[['batch size', 'has', '4'], ['base learning rate', 'has', '0.001']]",[],[],face_detection,7,184
2012,experiments,"The maximum training iteration is 1,000,000 .","[('is', (4, 5))]","[('maximum training iteration', (1, 4)), ('1,000,000', (5, 6))]","[['maximum training iteration', 'is', '1,000,000']]","[['maximum training iteration', 'has', '1,000,000']]",[],[],face_detection,7,185
2013,experiments,We use stochastic gradient descent as the optimizer .,"[('as', (5, 6))]","[('stochastic gradient descent', (2, 5)), ('optimizer', (7, 8))]","[['stochastic gradient descent', 'as', 'optimizer']]",[],[],[],face_detection,7,186
2014,experiments,The scale - forecast network is of vital importance to the computational cost and accuracy in the networks afterwards .,"[('of', (6, 7)), ('to', (9, 10)), ('in', (15, 16))]","[('scale - forecast network', (1, 5)), ('vital importance', (7, 9)), ('computational cost and accuracy', (11, 15)), ('networks', (17, 18))]","[['scale - forecast network', 'of', 'vital importance'], ['vital importance', 'to', 'computational cost and accuracy'], ['computational cost and accuracy', 'in', 'networks']]",[],[],[],face_detection,7,187
2015,experiments,"We can observe from the results that our trained scale network recalls almost 99 % at x = 1 , indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales .","[('observe', (2, 3)), ('recalls', (11, 12))]","[('our trained scale network', (7, 11)), ('almost 99 % at x = 1', (12, 19))]","[['our trained scale network', 'recalls', 'almost 99 % at x = 1']]",[],[],[],face_detection,7,190
2016,experiments,"We can conclude that the deeper the RSA is branched out , the worse the feature approximation at smaller scales will be .","[('is', (8, 9)), ('at', (17, 18))]","[('deeper', (5, 6)), ('RSA', (7, 8)), ('branched out', (9, 11)), ('worse', (13, 14)), ('feature approximation', (15, 17)), ('smaller scales', (18, 20))]","[['deeper', 'is', 'branched out'], ['RSA', 'is', 'branched out'], ['feature approximation', 'at', 'smaller scales']]","[['deeper', 'has', 'RSA'], ['RSA', 'has', 'branched out'], ['worse', 'has', 'feature approximation']]",[],[],face_detection,7,194
2017,baselines,Theoretically RSA can handle all scales of features in a deep CNN model and therefore can be branched out at any depth of the network .,"[('handle', (3, 4)), ('in', (8, 9)), ('can be', (15, 17)), ('at', (19, 20))]","[('all scales of features', (4, 8)), ('deep CNN model', (10, 13)), ('branched out', (17, 19)), ('any depth of the network', (20, 25))]","[['all scales of features', 'in', 'deep CNN model'], ['branched out', 'at', 'any depth of the network']]",[],[],[],face_detection,7,206
2018,hyperparameters,The minimum operation in each component means only the scaleforecast network is used where no face appears in the image ; and the maximum operation indicates the amount when faces appear at all scales .,"[('in', (3, 4)), ('means', (6, 7)), ('is', (11, 12)), ('used', (12, 13)), ('indicates', (25, 26)), ('when', (28, 29)), ('at', (31, 32))]","[('minimum operation', (1, 3)), ('each component', (4, 6)), ('only the scaleforecast network', (7, 11)), ('maximum operation', (23, 25)), ('amount', (27, 28)), ('faces appear', (29, 31)), ('all scales', (32, 34))]","[['minimum operation', 'in', 'each component'], ['minimum operation', 'means', 'only the scaleforecast network'], ['maximum operation', 'indicates', 'amount'], ['amount', 'when', 'faces appear'], ['faces appear', 'at', 'all scales']]","[['minimum operation', 'has', 'each component'], ['maximum operation', 'has', 'amount']]",[],"[['Hyperparameters', 'has', 'minimum operation']]",face_detection,7,214
2019,experiments,Most of the computation happens before layer res2 b and it has an acceptable error rate of 3.44 % .,"[('happens before', (4, 6)), ('of', (16, 17))]","[('Most of the computation', (0, 4)), ('layer res2 b', (6, 9)), ('acceptable error rate', (13, 16)), ('3.44 %', (17, 19))]","[['Most of the computation', 'happens before', 'layer res2 b'], ['acceptable error rate', 'of', '3.44 %']]",[],[],[],face_detection,7,219
2020,experiments,"For a particular case , as the times of the recurrent operation increase , the error rate goes up due to the cumulative effect of rolling out the predictions .","[('of', (8, 9)), ('goes up', (17, 19)), ('due to', (19, 21)), ('of', (24, 25))]","[('times', (7, 8)), ('recurrent operation', (10, 12)), ('increase', (12, 13)), ('error rate', (15, 17)), ('cumulative effect', (22, 24)), ('rolling out', (25, 27)), ('predictions', (28, 29))]","[['times', 'of', 'recurrent operation'], ['cumulative effect', 'of', 'rolling out'], ['cumulative effect', 'of', 'rolling out']]","[['recurrent operation', 'has', 'increase'], ['increase', 'has', 'error rate'], ['rolling out', 'has', 'predictions']]",[],[],face_detection,7,222
2021,research-problem,Detecting Faces Using Region - based Fully Convolutional Networks,[],"[('Detecting Faces', (0, 2))]",[],[],[],[],face_detection,8,2
2022,research-problem,Face detection has achieved great success using the region - based methods .,[],"[('Face detection', (0, 2))]",[],[],[],[],face_detection,8,4
2023,model,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .","[('develop', (5, 6)), ('on the top of', (9, 13)), ('with', (16, 17)), ('of', (19, 20))]","[('face detector', (7, 9)), ('R - FCN', (13, 16)), ('elaborate design', (17, 19)), ('details', (21, 22))]","[['face detector', 'on the top of', 'R - FCN'], ['R - FCN', 'with', 'elaborate design'], ['elaborate design', 'of', 'details']]","[['elaborate design', 'has', 'details']]","[['Model', 'develop', 'face detector']]",[],face_detection,8,22
2024,model,"According to the size of the general face , we carefully design size of anchors and RoIs .","[('of', (4, 5)), ('design', (11, 12))]","[('size', (3, 4)), ('anchors and RoIs', (14, 17))]",[],[],[],[],face_detection,8,23
2025,model,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .","[('of', (3, 4)), ('introduce', (12, 13)), ('to generate', (19, 21)), ('for enhancing', (23, 25)), ('eliminate', (28, 29)), ('in', (34, 35))]","[('position - sensitive average pooling', (14, 19)), ('embedding features', (21, 23)), ('discrimination', (25, 26)), ('effect', (30, 31)), ('non-uniformed contribution', (32, 34)), ('each facial part', (35, 38))]","[['position - sensitive average pooling', 'to generate', 'embedding features'], ['embedding features', 'for enhancing', 'discrimination'], ['position - sensitive average pooling', 'eliminate', 'effect'], ['non-uniformed contribution', 'in', 'each facial part']]",[],[],[],face_detection,8,24
2026,model,"Furthermore , we also apply the multi-scale training and testing strategy in this work .","[('apply', (4, 5))]","[('multi-scale training and testing strategy', (6, 11))]",[],[],"[['Model', 'apply', 'multi-scale training and testing strategy']]",[],face_detection,8,25
2027,model,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,"[('integrated into', (12, 14)), ('for', (18, 19)), ('on', (22, 23))]","[('on - line hard example mining ( OHEM ) technique', (1, 11)), ('our network', (14, 16)), ('boosting', (19, 20)), ('learning', (21, 22)), ('hard examples', (23, 25))]","[['on - line hard example mining ( OHEM ) technique', 'integrated into', 'our network'], ['on - line hard example mining ( OHEM ) technique', 'for', 'boosting'], ['learning', 'on', 'hard examples']]","[['boosting', 'has', 'learning']]",[],"[['Model', 'has', 'on - line hard example mining ( OHEM ) technique']]",face_detection,8,26
2028,hyperparameters,Our training hyper - parameters are similar to Face R - CNN .,"[('similar to', (6, 8))]","[('training hyper - parameters', (1, 5)), ('Face R - CNN', (8, 12))]","[['training hyper - parameters', 'similar to', 'Face R - CNN']]",[],[],"[['Hyperparameters', 'has', 'training hyper - parameters']]",face_detection,8,107
2029,hyperparameters,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .","[('initialize', (8, 9)), ('with', (11, 12)), ('of', (15, 16)), ('trained on', (20, 22))]","[('our network', (9, 11)), ('pre-trained weights', (13, 15)), ('101 - layer ResNet', (16, 20)), ('Image Net', (22, 24))]","[['our network', 'with', 'pre-trained weights'], ['pre-trained weights', 'of', '101 - layer ResNet'], ['101 - layer ResNet', 'trained on', 'Image Net']]",[],"[['Hyperparameters', 'initialize', 'our network']]",[],face_detection,8,108
2030,hyperparameters,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .","[('freeze', (3, 4)), ('of', (9, 10)), ('throughout', (20, 21)), ('to keep', (27, 29)), ('trained on', (33, 35))]","[('general kernels', (5, 7)), ('pre-trained model', (18, 20)), ('entire training process', (22, 25)), ('essential feature extractor', (30, 33)), ('ImageNet', (35, 36))]","[['pre-trained model', 'throughout', 'entire training process'], ['pre-trained model', 'to keep', 'essential feature extractor'], ['entire training process', 'to keep', 'essential feature extractor'], ['essential feature extractor', 'trained on', 'ImageNet']]",[],"[['Hyperparameters', 'freeze', 'general kernels']]",[],face_detection,8,109
2031,baselines,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .","[('In terms of', (0, 3)), ('enumerates', (11, 12)), ('of', (14, 15))]","[('RPN stage', (4, 6)), ('Face R - FCN', (7, 11)), ('multiple configurations', (12, 14)), ('anchor', (16, 17))]","[['Face R - FCN', 'enumerates', 'multiple configurations'], ['multiple configurations', 'of', 'anchor']]","[['RPN stage', 'has', 'Face R - FCN']]","[['Baselines', 'In terms of', 'RPN stage']]",[],face_detection,8,110
2032,hyperparameters,We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,"[('combine', (1, 2)), ('of', (4, 5)), ('to construct', (11, 13))]","[('range', (3, 4)), ('multiple scales and aspect ratios', (5, 10)), ('multi-scale anchors', (13, 15))]","[['range', 'of', 'multiple scales and aspect ratios'], ['multiple scales and aspect ratios', 'to construct', 'multi-scale anchors']]","[['range', 'has', 'multiple scales and aspect ratios']]","[['Hyperparameters', 'combine', 'range']]",[],face_detection,8,111
2033,hyperparameters,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,"[('with', (10, 11))]","[('RPN and R - FCN', (1, 6)), ('learned jointly', (8, 10)), ('softmax loss', (12, 14)), ('smooth L1 loss', (16, 19))]","[['learned jointly', 'with', 'softmax loss'], ['learned jointly', 'with', 'smooth L1 loss']]","[['RPN and R - FCN', 'has', 'learned jointly']]",[],"[['Hyperparameters', 'has', 'RPN and R - FCN']]",face_detection,8,115
2034,hyperparameters,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,"[('adopted for', (7, 9)), ('with', (12, 13))]","[('Non- maximum suppression ( NMS )', (0, 6)), ('regularizing', (9, 10)), ('anchors', (11, 12)), ('certain IoU scores', (13, 16))]","[['Non- maximum suppression ( NMS )', 'adopted for', 'regularizing'], ['anchors', 'with', 'certain IoU scores']]","[['regularizing', 'has', 'anchors']]",[],"[['Hyperparameters', 'has', 'Non- maximum suppression ( NMS )']]",face_detection,8,116
2035,hyperparameters,We set the 256 for the size of RPN mini-batch and 128 for R - FCN respectively .,"[('set', (1, 2)), ('for', (4, 5)), ('of', (7, 8)), ('for', (12, 13))]","[('256', (3, 4)), ('size', (6, 7)), ('RPN mini-batch', (8, 10)), ('128', (11, 12)), ('R - FCN', (13, 16))]","[['256', 'for', 'size'], ['256', 'for', '128'], ['256', 'for', 'R - FCN'], ['128', 'for', 'R - FCN'], ['size', 'of', 'RPN mini-batch'], ['128', 'for', 'R - FCN']]",[],"[['Hyperparameters', 'set', '256']]",[],face_detection,8,118
2036,hyperparameters,"We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .","[('utilize', (1, 2)), ('where', (4, 5)), ('with', (10, 11)), ('to', (13, 14))]","[('multi-scale training', (2, 4)), ('input image', (6, 8)), ('resized', (9, 10)), ('bilinear interpolation', (11, 13)), ('various scales ( say , 1024 or 1200 )', (14, 23))]","[['multi-scale training', 'where', 'input image'], ['resized', 'with', 'bilinear interpolation'], ['bilinear interpolation', 'to', 'various scales ( say , 1024 or 1200 )']]",[],"[['Hyperparameters', 'utilize', 'multi-scale training']]",[],face_detection,8,120
2037,hyperparameters,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .","[('In', (0, 1)), ('into', (12, 13)), ('for', (16, 17)), ('on both', (19, 21))]","[('testing stage', (2, 4)), ('multi-scale testing', (5, 7)), ('scale', (10, 11)), ('image', (11, 12)), ('image pyramid', (14, 16)), ('better detecting', (17, 19)), ('tiny and general faces', (21, 25))]","[['scale', 'into', 'image pyramid'], ['image', 'into', 'image pyramid'], ['image pyramid', 'for', 'better detecting'], ['better detecting', 'on both', 'tiny and general faces']]","[['testing stage', 'has', 'multi-scale testing'], ['scale', 'has', 'image']]","[['Hyperparameters', 'In', 'testing stage']]",[],face_detection,8,121
2038,results,"As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .","[('consistently wins', (7, 9)), ('across', (12, 13)), ('on both', (16, 18)), ('of', (24, 25)), ('significantly outperforms', (28, 30))]","[('our proposed approach', (4, 7)), ('1st place', (10, 12)), ('three subsets', (14, 16)), ('validation set and test set', (19, 24)), ('WIDER FACE', (25, 27)), ('existing results', (31, 33))]","[['our proposed approach', 'consistently wins', '1st place'], ['1st place', 'across', 'three subsets'], ['three subsets', 'on both', 'validation set and test set'], ['validation set and test set', 'of', 'WIDER FACE'], ['our proposed approach', 'significantly outperforms', 'existing results']]",[],[],"[['Results', 'has', 'our proposed approach']]",face_detection,8,125
2039,results,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .","[('on', (3, 4)), ('to', (13, 14)), ('by', (20, 21))]","[('WIDER FACE hard subset', (4, 8)), ('our approach', (9, 11)), ('superior', (12, 13)), ('prior best - performing one', (15, 20)), ('clear margin', (22, 24))]","[['superior', 'to', 'prior best - performing one'], ['prior best - performing one', 'by', 'clear margin']]","[['WIDER FACE hard subset', 'has', 'our approach'], ['our approach', 'has', 'superior']]","[['Results', 'on', 'WIDER FACE hard subset']]",[],face_detection,8,126
2040,experiments,FDDB,[],"[('FDDB', (0, 1))]",[],[],[],[],face_detection,8,127
2041,experiments,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .","[('consistently achieves', (10, 12)), ('in terms of both', (15, 19))]","[('Face R - FCN', (6, 10)), ('impressive performance', (13, 15)), ('discrete ROC curve and continuous ROC curve', (20, 27))]","[['Face R - FCN', 'consistently achieves', 'impressive performance'], ['impressive performance', 'in terms of both', 'discrete ROC curve and continuous ROC curve']]",[],[],[],face_detection,8,133
2042,experiments,Our discrete ROC curve is superior to the prior best - performing method .,"[('to', (6, 7))]","[('Our discrete ROC curve', (0, 4)), ('superior', (5, 6)), ('prior best - performing method', (8, 13))]","[['superior', 'to', 'prior best - performing method']]","[['Our discrete ROC curve', 'has', 'superior']]",[],[],face_detection,8,134
2043,experiments,We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,"[('obtain', (2, 3)), ('of', (8, 9)), ('at', (13, 14))]","[('best true positive rate', (4, 8)), ('discrete ROC curve', (10, 13)), ('1000/2000 false positives', (14, 17)), ('98.49%/99.07 %', (18, 20))]","[['best true positive rate', 'of', 'discrete ROC curve'], ['best true positive rate', 'at', '1000/2000 false positives'], ['discrete ROC curve', 'at', '1000/2000 false positives']]","[['1000/2000 false positives', 'has', '98.49%/99.07 %']]",[],[],face_detection,8,135
2044,experiments,"Face R - FCN shows the superior performance over the prior methods across the three subsets ( easy , medium and hard ) in both validation and test sets .","[('shows', (4, 5)), ('over', (8, 9)), ('across', (12, 13)), ('in', (23, 24))]","[('superior performance', (6, 8)), ('prior methods', (10, 12)), ('three subsets', (14, 16)), ('easy , medium and hard', (17, 22)), ('both validation and test sets', (24, 29))]","[['superior performance', 'over', 'prior methods'], ['prior methods', 'across', 'three subsets'], ['three subsets', 'in', 'both validation and test sets']]","[['three subsets', 'name', 'easy , medium and hard']]",[],[],face_detection,8,141
2045,experiments,"Finally , we obtain the true positive rate 98. 99 % of the discrete ROC curve at 1000 false positives and 99. 42 % at 2000 false positives , which are new state - of - the - art among all the published methods on FDDB .","[('of', (11, 12)), ('at', (16, 17)), ('at', (24, 25)), ('among', (39, 40))]","[('true positive rate', (5, 8)), ('98. 99 %', (8, 11)), ('discrete ROC curve', (13, 16)), ('1000 false positives', (17, 20)), ('99. 42 %', (21, 24)), ('2000 false positives', (25, 28)), ('new state - of - the - art', (31, 39)), ('all the published methods', (40, 44))]","[['98. 99 %', 'of', 'discrete ROC curve'], ['98. 99 %', 'of', '99. 42 %'], ['discrete ROC curve', 'at', '1000 false positives'], ['99. 42 %', 'at', '2000 false positives'], ['discrete ROC curve', 'at', '1000 false positives'], ['99. 42 %', 'at', '2000 false positives'], ['new state - of - the - art', 'among', 'all the published methods']]","[['true positive rate', 'has', '98. 99 %']]",[],[],face_detection,8,147
2046,research-problem,Finding Tiny Faces,[],"[('Finding Tiny Faces', (0, 3))]",[],[],[],[],face_detection,9,2
2047,research-problem,"Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .",[],"[('object recognition', (7, 9)), ('detecting small objects', (17, 20))]",[],[],[],[],face_detection,9,7
2048,research-problem,"We explore three aspects of the problem in the context of finding small faces : the role of scale invariance , image resolution , and contextual reasoning .",[],"[('finding small faces', (11, 14))]",[],[],[],[],face_detection,9,8
2049,research-problem,"lem in the context of face detection : the role of scale invariance , image resolution and contextual reasoning .",[],"[('face detection', (5, 7))]",[],[],[],[],face_detection,9,26
2050,model,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .","[('train', (11, 12)), ('tuned for', (14, 16))]","[('separate detectors', (12, 14)), ('different scales ( and aspect ratios )', (16, 23))]","[['separate detectors', 'tuned for', 'different scales ( and aspect ratios )']]",[],"[['Model', 'train', 'separate detectors']]",[],face_detection,9,33
2051,model,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .","[('train and run', (6, 9)), ('in', (13, 14)), ('make use of', (19, 22)), ('defined over', (23, 25)), ('of', (27, 28))]","[('scale - specific detectors', (9, 13)), ('multitask fashion', (15, 17)), ('features', (22, 23)), ('multiple layers', (25, 27)), ('single ( deep ) feature hierarchy', (28, 34))]","[['scale - specific detectors', 'in', 'multitask fashion'], ['scale - specific detectors', 'make use of', 'features'], ['multitask fashion', 'make use of', 'features'], ['features', 'defined over', 'multiple layers'], ['multiple layers', 'of', 'single ( deep ) feature hierarchy']]",[],"[['Model', 'train and run', 'scale - specific detectors']]",[],face_detection,9,35
2052,model,"To extend features fine - tuned from these networks to objects of novel sizes , we employ a simply strategy : resize images at test - time by interpolation and decimation .","[('To extend', (0, 2)), ('from', (6, 7)), ('to', (9, 10)), ('of', (11, 12)), ('employ', (16, 17)), ('resize', (21, 22)), ('at', (23, 24)), ('by', (27, 28))]","[('features', (2, 3)), ('fine - tuned', (3, 6)), ('these networks', (7, 9)), ('objects', (10, 11)), ('novel sizes', (12, 14)), ('simply strategy', (18, 20)), ('images', (22, 23)), ('test - time', (24, 27)), ('interpolation and decimation', (28, 31))]","[['fine - tuned', 'from', 'these networks'], ['fine - tuned', 'to', 'objects'], ['these networks', 'to', 'objects'], ['objects', 'of', 'novel sizes'], ['simply strategy', 'resize', 'images'], ['images', 'at', 'test - time'], ['test - time', 'by', 'interpolation and decimation']]","[['features', 'has', 'fine - tuned']]","[['Model', 'To extend', 'features']]",[],face_detection,9,41
2053,research-problem,"While many recognition systems are applied in a "" multi-resolution "" fashion by processing an image pyramid , we find that interpolating the lowest layer of the pyramid is particularly crucial for finding small objects .","[('find that', (19, 21)), ('interpolating', (21, 22)), ('of', (25, 26)), ('for finding', (31, 33))]","[('pyramid', (16, 17)), ('lowest layer', (23, 25)), ('particularly crucial', (29, 31)), ('small objects', (33, 35))]","[['pyramid', 'interpolating', 'lowest layer'], ['particularly crucial', 'for finding', 'small objects']]",[],[],[],face_detection,9,42
2054,model,Hence our final approach is a delicate mixture of scale - specific detectors that are used in a scale - invariant fashion ( by processing an image pyramid to capture large scale variations ) .,"[('is', (4, 5)), ('of', (8, 9)), ('used in', (15, 17))]","[('our final approach', (1, 4)), ('delicate mixture', (6, 8)), ('scale - specific detectors', (9, 13)), ('scale - invariant fashion', (18, 22))]","[['our final approach', 'is', 'delicate mixture'], ['delicate mixture', 'of', 'scale - specific detectors'], ['scale - specific detectors', 'used in', 'scale - invariant fashion']]","[['our final approach', 'has', 'delicate mixture']]",[],"[['Model', 'has', 'our final approach']]",face_detection,9,43
2055,model,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .","[('demonstrate', (1, 2)), ('extracted from', (6, 8)), ('capture both', (26, 28)), ('across', (38, 39))]","[('convolutional deep features', (3, 6)), ('multiple layers', (8, 10)), ('effective "" foveal "" descriptors', (20, 25)), ('high - resolution detail', (28, 32)), ('coarse low - resolution cues', (33, 38)), ('large receptive field', (39, 42))]","[['convolutional deep features', 'extracted from', 'multiple layers'], ['effective "" foveal "" descriptors', 'capture both', 'high - resolution detail'], ['effective "" foveal "" descriptors', 'capture both', 'coarse low - resolution cues'], ['coarse low - resolution cues', 'across', 'large receptive field']]",[],"[['Model', 'demonstrate', 'convolutional deep features']]",[],face_detection,9,52
2056,model,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,"[('show', (1, 2)), ('of', (5, 6)), ('extracted from', (10, 12)), ('for', (18, 19))]","[('highresolution components', (3, 5)), ('our foveal descriptors', (6, 9)), ('lower convolutional layers', (12, 15)), ('crucial', (17, 18)), ('accurate localization', (20, 22))]","[['highresolution components', 'of', 'our foveal descriptors'], ['our foveal descriptors', 'extracted from', 'lower convolutional layers'], ['crucial', 'for', 'accurate localization']]",[],"[['Model', 'show', 'highresolution components']]",[],face_detection,9,53
2057,results,WIDER FACE :,[],"[('WIDER FACE', (0, 2))]",[],[],[],"[['Results', 'has', 'WIDER FACE']]",face_detection,9,218
2058,results,"As shows , our hybrid - resolution model ( HR ) achieves state - of - the - art performance on all difficulty levels , but most importantly , reduces error on the "" hard "" set by 2X .","[('achieves', (11, 12)), ('on', (20, 21)), ('on', (31, 32)), ('by', (37, 38))]","[('our hybrid - resolution model ( HR )', (3, 11)), ('state - of - the - art performance', (12, 20)), ('all difficulty levels', (21, 24)), ('reduces', (29, 30)), ('error', (30, 31)), ('"" hard "" set', (33, 37)), ('2X', (38, 39))]","[['our hybrid - resolution model ( HR )', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', 'all difficulty levels'], ['error', 'on', '"" hard "" set'], ['error', 'on', '"" hard "" set'], ['error', 'by', '2X'], ['"" hard "" set', 'by', '2X']]","[['reduces', 'has', 'error']]",[],"[['Results', 'has', 'our hybrid - resolution model ( HR )']]",face_detection,9,220
2059,baselines,FDDB :,[],"[('FDDB', (0, 1))]",[],[],[],"[['Baselines', 'has', 'FDDB']]",face_detection,9,224
2060,results,"Our out - of - the - box detector ( HR ) outperforms all published results on the discrete score , which uses a standard 50 % intersection - over - union threshold to define correctness .","[('outperforms', (12, 13)), ('on', (16, 17))]","[('Our out - of - the - box detector ( HR )', (0, 12)), ('all published results', (13, 16)), ('discrete score', (18, 20))]","[['Our out - of - the - box detector ( HR )', 'outperforms', 'all published results'], ['all published results', 'on', 'discrete score']]",[],[],[],face_detection,9,226
2061,results,"With the post - hoc regressor , our detector achieves state - of - the - art performance on the continuous score ( measuring average bounding - box overlap ) as well .","[('With', (0, 1)), ('achieves', (9, 10)), ('on', (18, 19))]","[('post - hoc regressor', (2, 6)), ('our detector', (7, 9)), ('state - of - the - art performance', (10, 18)), ('continuous score', (20, 22))]","[['our detector', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'on', 'continuous score']]","[['post - hoc regressor', 'has', 'our detector']]","[['Results', 'With', 'post - hoc regressor']]",[],face_detection,9,228
2062,hyperparameters,Our regressor is trained with 10 - fold cross validation .,"[('trained with', (3, 5))]","[('Our regressor', (0, 2)), ('10 - fold cross validation', (5, 10))]","[['Our regressor', 'trained with', '10 - fold cross validation']]",[],[],"[['Hyperparameters', 'has', 'Our regressor']]",face_detection,9,229
2063,research-problem,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,[],"[('Unsupervised Hypernym Discovery', (13, 16))]",[],[],[],[],hypernym_discovery,0,2
2064,research-problem,This paper describes a simple but competitive unsupervised system for hypernym discovery .,[],"[('hypernym discovery', (10, 12))]",[],[],[],[],hypernym_discovery,0,4
2065,model,The ADAPT team focused on the two specialised domain English subtasks by developing an unsupervised system that builds word embeddings from the supplied reference corpora for these domains .,"[('focused on', (3, 5)), ('by developing', (11, 13)), ('builds', (17, 18)), ('from', (20, 21))]","[('ADAPT team', (1, 3)), ('two specialised domain English subtasks', (6, 11)), ('unsupervised system', (14, 16)), ('word embeddings', (18, 20)), ('supplied reference corpora', (22, 25))]","[['ADAPT team', 'focused on', 'two specialised domain English subtasks'], ['two specialised domain English subtasks', 'by developing', 'unsupervised system'], ['unsupervised system', 'builds', 'word embeddings'], ['word embeddings', 'from', 'supplied reference corpora']]",[],[],"[['Model', 'has', 'ADAPT team']]",hypernym_discovery,0,20
2066,model,"Even though unsupervised systems tend to rank behind supervised systems in NLP tasks in general , our motivation to focus on an unsupervised approach is derived from the fact that they do not require explicit hand - annotated data , and from the expectation that they are able to generalise more easily to unseen hypernym - hyponym pairs .",[],"[('unsupervised approach', (22, 24)), ('expectation that', (43, 45)), ('generalise', (49, 50)), ('more easily', (50, 52)), ('unseen hypernym - hyponym pairs', (53, 58))]",[],"[['generalise', 'has', 'more easily']]",[],[],hypernym_discovery,0,25
2067,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],hypernym_discovery,0,60
2068,results,Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 .,"[('ranked', (3, 4)), ('out of', (6, 8)), ('on', (9, 10)), ('with', (14, 15)), ('of', (22, 23))]","[('Our official submission', (0, 3)), ('eleven', (5, 6)), ('eighteen', (8, 9)), ('medical domain subtask', (11, 14)), ('Mean Average Precision ( MAP )', (16, 22)), ('8.13', (23, 24))]","[['eleven', 'out of', 'eighteen'], ['eighteen', 'on', 'medical domain subtask'], ['medical domain subtask', 'with', 'Mean Average Precision ( MAP )'], ['Mean Average Precision ( MAP )', 'of', '8.13']]",[],[],[],hypernym_discovery,0,61
2069,results,"However , it ranked first place among all the unsupervised systems on this subtask .","[('among', (6, 7))]","[('first place', (4, 6)), ('all the unsupervised systems', (7, 11))]","[['first place', 'among', 'all the unsupervised systems']]",[],[],[],hypernym_discovery,0,62
2070,results,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .","[('On', (0, 1)), ('ranked', (9, 10)), ('out of', (11, 13)), ('with', (15, 16)), ('of', (18, 19)), ('ranking', (21, 22)), ('among', (23, 24))]","[('music industry domain subtask', (2, 6)), ('our system', (7, 9)), ('13th', (10, 11)), ('16 places', (13, 15)), ('MAP', (17, 18)), ('1.88', (19, 20)), ('4th', (22, 23)), ('unsupervised systems', (25, 27))]","[['our system', 'ranked', '13th'], ['13th', 'out of', '16 places'], ['16 places', 'with', 'MAP'], ['MAP', 'of', '1.88'], ['4th', 'among', 'unsupervised systems']]","[['music industry domain subtask', 'has', 'our system']]","[['Results', 'On', 'music industry domain subtask']]",[],hypernym_discovery,0,63
2071,research-problem,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings,[],"[('Neural Hypernym Discovery', (7, 10))]",[],[],[],[],hypernym_discovery,1,2
2072,research-problem,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .",[],"[('hypernym discovery', (4, 6))]",[],[],[],[],hypernym_discovery,1,4
2073,research-problem,"A relevant well - known scenario is hypernym detection , which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not .",[],"[('hypernym detection', (7, 9))]",[],[],[],[],hypernym_discovery,1,14
2074,model,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .","[('introduce', (5, 6)), ('for', (10, 11)), ('empirically study', (15, 17)), ('to model', (20, 22)), ('for', (25, 26))]","[('neural network architecture', (7, 10)), ('concerned task', (12, 14)), ('various neural networks', (17, 20)), ('distributed representations', (23, 25)), ('words and phrases', (26, 29))]","[['neural network architecture', 'for', 'concerned task'], ['distributed representations', 'for', 'words and phrases'], ['various neural networks', 'to model', 'distributed representations'], ['distributed representations', 'for', 'words and phrases']]",[],"[['Model', 'introduce', 'neural network architecture']]",[],hypernym_discovery,1,23
2075,model,"In our system , we leverage an unambiguous vector representation via term embedding , and we take advantage of deep neural networks to discover the hypernym relationships between terms .","[('leverage', (5, 6)), ('via', (10, 11)), ('take', (16, 17)), ('of', (18, 19)), ('to discover', (22, 24)), ('between', (27, 28))]","[('unambiguous vector representation', (7, 10)), ('term embedding', (11, 13)), ('advantage', (17, 18)), ('deep neural networks', (19, 22)), ('hypernym relationships', (25, 27)), ('terms', (28, 29))]","[['unambiguous vector representation', 'via', 'term embedding'], ['advantage', 'of', 'deep neural networks'], ['deep neural networks', 'to discover', 'hypernym relationships'], ['hypernym relationships', 'between', 'terms']]",[],"[['Model', 'leverage', 'unambiguous vector representation']]",[],hypernym_discovery,1,24
2076,experimental-setup,Our model was implemented using the Theano 1 .,"[('implemented using', (3, 5))]","[('Our model', (0, 2)), ('Theano', (6, 7))]","[['Our model', 'implemented using', 'Theano']]",[],[],"[['Experimental setup', 'has', 'Our model']]",hypernym_discovery,1,83
2077,experimental-setup,The diagonal variant of Ada - Grad is used for neural network training .,"[('of', (3, 4)), ('used for', (8, 10))]","[('diagonal variant', (1, 3)), ('Ada - Grad', (4, 7)), ('neural network training', (10, 13))]","[['diagonal variant', 'of', 'Ada - Grad'], ['Ada - Grad', 'used for', 'neural network training']]",[],[],"[['Experimental setup', 'has', 'diagonal variant']]",hypernym_discovery,1,84
2078,experimental-setup,We tune the hyper - parameters with the following range of values : learning rate ?,"[('tune', (1, 2)), ('with', (6, 7))]","[('hyper - parameters', (3, 6)), ('following range of values', (8, 12)), ('learning rate', (13, 15))]","[['hyper - parameters', 'with', 'following range of values']]","[['following range of values', 'has', 'learning rate']]","[['Experimental setup', 'tune', 'hyper - parameters']]",[],hypernym_discovery,1,85
2079,experimental-setup,"{ 1 e ? 3 , 1 e ? 2 } , dropout probability ? { 0.1 , 0.2 } , CNN filter width ? { 2 , 3 , 4 } .",[],"[('{ 1 e ? 3 , 1 e ? 2 }', (0, 11)), ('dropout probability', (12, 14)), ('{ 0.1 , 0.2 }', (15, 20)), ('CNN filter width', (21, 24)), ('{ 2 , 3 , 4 }', (25, 32))]",[],"[['dropout probability', 'has', '{ 0.1 , 0.2 }'], ['CNN filter width', 'has', '{ 2 , 3 , 4 }']]",[],[],hypernym_discovery,1,86
2080,experimental-setup,The hidden dimension of all neural models are 200 .,"[('of', (3, 4))]","[('hidden dimension', (1, 3)), ('all neural models', (4, 7)), ('200', (8, 9))]","[['hidden dimension', 'of', 'all neural models']]",[],[],"[['Experimental setup', 'has', 'hidden dimension']]",hypernym_discovery,1,87
2081,experimental-setup,The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,"[('set to', (4, 6)), ('set to', (16, 18))]","[('batch size', (1, 3)), ('20', (6, 7)), ('word embedding and sense embedding sizes', (9, 15)), ('300', (18, 19))]","[['batch size', 'set to', '20'], ['word embedding and sense embedding sizes', 'set to', '300']]","[['batch size', 'has', '20'], ['word embedding and sense embedding sizes', 'has', '300']]",[],"[['Experimental setup', 'has', 'batch size']]",hypernym_discovery,1,88
2082,experimental-setup,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .","[('trained on', (5, 7)), ('with', (17, 18)), ('for', (20, 21)), ('for', (25, 26)), ('for', (36, 37))]","[('All of our models', (0, 4)), ('single GPU ( NVIDIA GTX 980 Ti )', (8, 16)), ('roughly 1.5h', (18, 20)), ('general - purpose subtask', (21, 25)), ('English', (26, 27)), ('0.5h domain - specific domain - specific ones', (28, 36)), ('medical and music', (37, 40))]","[['All of our models', 'trained on', 'single GPU ( NVIDIA GTX 980 Ti )'], ['single GPU ( NVIDIA GTX 980 Ti )', 'with', 'roughly 1.5h'], ['roughly 1.5h', 'for', 'general - purpose subtask'], ['roughly 1.5h', 'for', '0.5h domain - specific domain - specific ones'], ['general - purpose subtask', 'for', 'English'], ['0.5h domain - specific domain - specific ones', 'for', 'medical and music']]",[],[],"[['Experimental setup', 'has', 'All of our models']]",hypernym_discovery,1,89
2083,baselines,"Convolution or recurrent gated mechanisms in either CNN - based ( CNN , RCNN ) or RNN ( GRU , LSTM ) based neural networks could essentially be helpful of modeling the semantic connections between words in a phrase , and guide the networks to discover the hypernym relationships .","[('in', (5, 6)), ('helpful of', (28, 30)), ('modeling', (30, 31)), ('between', (34, 35)), ('in', (36, 37)), ('guide', (41, 42)), ('to discover', (44, 46))]","[('Convolution or recurrent gated mechanisms', (0, 5)), ('CNN - based ( CNN , RCNN )', (7, 15)), ('RNN ( GRU , LSTM ) based neural networks', (16, 25)), ('semantic connections', (32, 34)), ('words', (35, 36)), ('phrase', (38, 39)), ('networks', (43, 44)), ('hypernym relationships', (47, 49))]","[['Convolution or recurrent gated mechanisms', 'in', 'CNN - based ( CNN , RCNN )'], ['Convolution or recurrent gated mechanisms', 'in', 'RNN ( GRU , LSTM ) based neural networks'], ['words', 'in', 'phrase'], ['semantic connections', 'between', 'words'], ['words', 'in', 'phrase'], ['semantic connections', 'guide', 'networks'], ['networks', 'to discover', 'hypernym relationships']]",[],[],"[['Baselines', 'has', 'Convolution or recurrent gated mechanisms']]",hypernym_discovery,1,96
2084,results,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .","[('observe', (2, 3)), ('than', (10, 11))]","[('CNN - based network performance', (3, 8)), ('better', (9, 10)), ('RNN - based', (11, 14))]","[['better', 'than', 'RNN - based']]","[['CNN - based network performance', 'has', 'better']]",[],[],hypernym_discovery,1,97
2085,results,"To investigate the performance of neural models on specific domains , we conduct experiments on medical and medicine subtask .","[('investigate', (1, 2)), ('of', (4, 5)), ('on', (7, 8)), ('conduct', (12, 13)), ('on', (14, 15))]","[('performance', (3, 4)), ('neural models', (5, 7)), ('specific domains', (8, 10)), ('experiments', (13, 14)), ('medical and medicine subtask', (15, 19))]","[['performance', 'of', 'neural models'], ['neural models', 'on', 'specific domains'], ['experiments', 'on', 'medical and medicine subtask'], ['performance', 'conduct', 'experiments'], ['experiments', 'on', 'medical and medicine subtask']]",[],"[['Results', 'investigate', 'performance']]",[],hypernym_discovery,1,98
2086,results,"All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .",[],"[('outperform', (4, 5)), ('term embedding', (5, 7))]",[],"[['outperform', 'has', 'term embedding']]",[],"[['Results', 'has', 'outperform']]",hypernym_discovery,1,100
2087,results,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .","[('Compared with', (0, 2)), ('shows', (8, 9))]","[('word embedding', (2, 4)), ('sense embedding', (6, 8)), ('much poorer result', (10, 13))]","[['sense embedding', 'shows', 'much poorer result']]","[['word embedding', 'has', 'sense embedding'], ['sense embedding', 'has', 'much poorer result']]","[['Results', 'Compared with', 'word embedding']]",[],hypernym_discovery,1,101
2088,research-problem,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection,[],"[('Hypernymy Detection', (9, 11))]",[],[],[],[],hypernym_discovery,2,2
2089,research-problem,"In the last two decades , the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy .",[],"[('recognize hypernymy', (19, 21))]",[],[],[],[],hypernym_discovery,2,10
2090,approach,"In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection , using several distributional semantic models that differ by context type and feature weighting .","[('perform', (4, 5)), ('of', (8, 9)), ('for', (13, 14)), ('using', (17, 18)), ('differ by', (23, 25))]","[('extensive evaluation', (6, 8)), ('various unsupervised distributional measures', (9, 13)), ('hypernymy detection', (14, 16)), ('several distributional semantic models', (18, 22)), ('context type and feature weighting', (25, 30))]","[['extensive evaluation', 'of', 'various unsupervised distributional measures'], ['various unsupervised distributional measures', 'for', 'hypernymy detection'], ['hypernymy detection', 'using', 'several distributional semantic models'], ['several distributional semantic models', 'differ by', 'context type and feature weighting']]",[],"[['Approach', 'perform', 'extensive evaluation']]",[],hypernym_discovery,2,21
2091,approach,"We analyze the performance of the measures in different settings and suggest a principled way to select the suitable measure , context type and feature weighting according to the task setting , yielding consistent performance across datasets .","[('analyze', (1, 2)), ('of', (4, 5)), ('in', (7, 8)), ('suggest', (11, 12)), ('to select', (15, 17))]","[('performance', (3, 4)), ('measures', (6, 7)), ('different settings', (8, 10)), ('principled way', (13, 15)), ('suitable measure', (18, 20)), ('context type', (21, 23)), ('feature weighting', (24, 26))]","[['performance', 'of', 'measures'], ['measures', 'in', 'different settings'], ['principled way', 'to select', 'suitable measure'], ['principled way', 'to select', 'feature weighting']]","[['performance', 'has', 'measures']]","[['Approach', 'analyze', 'performance']]",[],hypernym_discovery,2,24
2092,approach,We also compare the unsupervised measures to the state - of - the - art supervised methods .,"[('compare', (2, 3)), ('to', (6, 7))]","[('unsupervised measures', (4, 6)), ('state - of - the - art supervised methods', (8, 17))]","[['unsupervised measures', 'to', 'state - of - the - art supervised methods']]",[],"[['Approach', 'compare', 'unsupervised measures']]",[],hypernym_discovery,2,25
2093,experiments,Experiments,[],"[('Experiments', (0, 1))]",[],[],[],[],hypernym_discovery,2,117
2094,results,Comparing Unsupervised Measures,[],"[('Comparing Unsupervised Measures', (0, 3))]",[],[],[],"[['Results', 'has', 'Comparing Unsupervised Measures']]",hypernym_discovery,2,118
2095,results,"The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .","[('show', (2, 3)), ('to', (4, 5))]","[('preference', (3, 4)), ('syntactic context - types ( dep and joint )', (6, 15))]","[['preference', 'to', 'syntactic context - types ( dep and joint )']]",[],"[['Results', 'show', 'preference']]",[],hypernym_discovery,2,131
2096,results,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .","[('In', (0, 1)), ('appears to be', (13, 16)), ('in', (17, 18))]","[('feature weighting', (1, 3)), ('no consistency', (5, 7)), ('raw frequency', (11, 13)), ('successful', (16, 17)), ('hypernymy detection', (18, 20))]","[['successful', 'In', 'hypernymy detection'], ['raw frequency', 'appears to be', 'successful'], ['successful', 'in', 'hypernymy detection']]","[['feature weighting', 'has', 'no consistency'], ['raw frequency', 'has', 'successful']]","[['Results', 'In', 'feature weighting']]",[],hypernym_discovery,2,132
2097,results,The new SLQS variants are on top of the list in many settings .,[],"[('new SLQS variants', (1, 4)), ('on top', (5, 7)), ('list', (9, 10))]",[],"[['new SLQS variants', 'has', 'on top'], ['on top', 'has', 'list']]",[],"[['Results', 'has', 'new SLQS variants']]",hypernym_discovery,2,133
2098,results,"In particular they perform well in discriminating hypernyms from symmetric relations ( antonymy , synonymy , coordination ) .","[('perform', (3, 4)), ('in discriminating', (5, 7)), ('from', (8, 9))]","[('well', (4, 5)), ('hypernyms', (7, 8)), ('symmetric relations', (9, 11)), ('antonymy', (12, 13)), ('synonymy', (14, 15)), ('coordination', (16, 17))]","[['well', 'in discriminating', 'hypernyms'], ['hypernyms', 'from', 'symmetric relations']]",[],[],[],hypernym_discovery,2,134
2099,results,Comparison to State - of - the - art Supervised Methods,[],"[('Comparison to State - of - the - art Supervised Methods', (0, 11))]",[],[],[],"[['Results', 'has', 'Comparison to State - of - the - art Supervised Methods']]",hypernym_discovery,2,181
2100,results,"The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .","[('of', (4, 5)), ('achieved using', (19, 21)), ('with', (24, 25))]","[('over all performance', (1, 4)), ('embeddingbased classifiers', (6, 8)), ('almost perfect', (9, 11)), ('best performance', (16, 18)), ('concatenation method', (22, 24)), ('either GloVe or the dependency - based embeddings', (25, 33))]","[['over all performance', 'of', 'embeddingbased classifiers'], ['best performance', 'achieved using', 'concatenation method'], ['concatenation method', 'with', 'either GloVe or the dependency - based embeddings']]",[],[],"[['Results', 'has', 'over all performance']]",hypernym_discovery,2,191
2101,results,"As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .","[('perform', (6, 7)), ('than', (8, 9))]","[('unsupervised measures', (4, 6)), ('worse', (7, 8)), ('embedding - based classifiers', (10, 14))]","[['unsupervised measures', 'perform', 'worse'], ['worse', 'than', 'embedding - based classifiers']]",[],[],[],hypernym_discovery,2,192
2102,research-problem,Supervised Distributional Hypernym Discovery via Domain Adaptation,[],"[('Hypernym Discovery', (2, 4)), ('Domain Adaptation', (5, 7))]",[],[],[],[],hypernym_discovery,3,2
2103,research-problem,"In addition , while not being taxonomy learning systems per se , semi-supervised systems for Information Extraction such as NELL rely crucially on taxonomized concepts and their relations within their learning process .",[],"[('taxonomy learning', (6, 8))]",[],[],[],[],hypernym_discovery,3,18
2104,model,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .","[('propose', (4, 5)), ('based on', (12, 14)), ('applied to', (21, 23)), ('of', (25, 26))]","[('TAXOEMBED', (5, 6)), ('hypernym detection algorithm', (9, 12)), ('sense embeddings', (14, 16)), ('construction', (24, 25)), ('lexical taxonomies', (26, 28))]","[['hypernym detection algorithm', 'based on', 'sense embeddings'], ['hypernym detection algorithm', 'applied to', 'construction'], ['sense embeddings', 'applied to', 'construction'], ['construction', 'of', 'lexical taxonomies']]","[['TAXOEMBED', 'has', 'hypernym detection algorithm']]","[['Model', 'propose', 'TAXOEMBED']]",[],hypernym_discovery,3,25
2105,model,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .","[('designed to', (2, 4)), ('discover', (4, 5)), ('by exploiting', (7, 9)), ('in', (11, 12))]","[('hypernymic relations', (5, 7)), ('linear transformations', (9, 11)), ('embedding spaces', (12, 14))]","[['hypernymic relations', 'by exploiting', 'linear transformations'], ['linear transformations', 'in', 'embedding spaces']]",[],[],[],hypernym_discovery,3,26
2106,model,Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :,"[('considers', (14, 15))]","[('best configuration', (1, 3))]",[],[],[],"[['Model', 'has', 'best configuration']]",hypernym_discovery,3,27
2107,model,( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .,"[('from', (6, 7)), ('from', (15, 16)), ('integrates', (19, 20))]","[('Manually curated pairs', (3, 6)), ('Wikidata', (7, 8)), ('Hypernymy relations', (13, 15)), ('KB', (17, 18)), ('several Open Information Extraction ( OIE ) systems', (20, 28))]","[['Manually curated pairs', 'from', 'Wikidata'], ['Hypernymy relations', 'from', 'KB'], ['Hypernymy relations', 'from', 'KB'], ['Hypernymy relations', 'integrates', 'several Open Information Extraction ( OIE ) systems']]",[],[],[],hypernym_discovery,3,28
2108,model,"Since our method uses a very large semantic network as reference sense inventory , we are able to perform jointly hypernym extraction and dis ambiguation , from which 1 The terminology is not entirely unified in this respect .","[('uses', (3, 4)), ('as', (9, 10)), ('perform', (18, 19))]","[('our method', (1, 3)), ('very large semantic network', (5, 9)), ('reference sense inventory', (10, 13)), ('jointly hypernym extraction and dis ambiguation', (19, 25))]","[['our method', 'uses', 'very large semantic network'], ['very large semantic network', 'as', 'reference sense inventory'], ['very large semantic network', 'perform', 'jointly hypernym extraction and dis ambiguation']]",[],[],"[['Model', 'has', 'our method']]",hypernym_discovery,3,29
2109,research-problem,2 Data and source code available from the following link : www.taln.upf.edu/taxoembed . expanding existing ontologies becomes a trivial task .,[],"[('www.taln.upf.edu/taxoembed', (11, 12))]",[],[],[],[],hypernym_discovery,3,31
2110,baselines,"We compare against a number of taxonomy learning and Information Extraction systems , namely , WiBi and DefIE .","[('compare against', (1, 3)), ('namely', (13, 14))]","[('number of taxonomy learning and Information Extraction systems', (4, 12)), ('WiBi and DefIE', (15, 18))]","[['number of taxonomy learning and Information Extraction systems', 'namely', 'WiBi and DefIE']]","[['number of taxonomy learning and Information Extraction systems', 'name', 'WiBi and DefIE']]","[['Baselines', 'compare against', 'number of taxonomy learning and Information Extraction systems']]",[],hypernym_discovery,3,131
2111,baselines,"Finally , DefIE is an automaic OIE system relying on the syntactic structure of pre-dis ambiguated definitions 13 .","[('is', (3, 4)), ('relying on', (8, 10)), ('of', (13, 14))]","[('DefIE', (2, 3)), ('automaic OIE system', (5, 8)), ('syntactic structure', (11, 13)), ('pre-dis ambiguated definitions', (14, 17))]","[['DefIE', 'is', 'automaic OIE system'], ['automaic OIE system', 'relying on', 'syntactic structure'], ['syntactic structure', 'of', 'pre-dis ambiguated definitions']]","[['DefIE', 'has', 'automaic OIE system']]",[],"[['Baselines', 'has', 'DefIE']]",hypernym_discovery,3,136
2112,results,shows the results of TAXOEMBED and all comparison systems .,"[('of', (3, 4))]","[('TAXOEMBED and all comparison systems', (4, 9))]",[],[],"[['Results', 'of', 'TAXOEMBED and all comparison systems']]",[],hypernym_discovery,3,138
2113,results,"As expected , Yago and WiBi achieve the best over all results .","[('achieve', (6, 7))]","[('Yago and WiBi', (3, 6)), ('best', (8, 9)), ('over all results', (9, 12))]","[['Yago and WiBi', 'achieve', 'best']]","[['Yago and WiBi', 'has', 'best'], ['best', 'has', 'over all results']]",[],"[['Results', 'has', 'Yago and WiBi']]",hypernym_discovery,3,139
2114,results,"However , TAXOEM - BED , based solely on distributional information , performed competitively in detecting new hypernyms when compared to DefIE , improving its recall in most domains , and even surpassing Yago in technical areas like biology or health .","[('based solely on', (6, 9)), ('performed', (12, 13)), ('in', (14, 15)), ('compared to', (19, 21)), ('improving', (23, 24)), ('in', (26, 27)), ('like', (37, 38))]","[('TAXOEM - BED', (2, 5)), ('distributional information', (9, 11)), ('competitively', (13, 14)), ('detecting', (15, 16)), ('new hypernyms', (16, 18)), ('DefIE', (21, 22)), ('recall', (25, 26)), ('most domains', (27, 29)), ('surpassing', (32, 33)), ('Yago', (33, 34)), ('technical areas', (35, 37)), ('biology or health', (38, 41))]","[['TAXOEM - BED', 'based solely on', 'distributional information'], ['TAXOEM - BED', 'performed', 'competitively'], ['competitively', 'in', 'detecting'], ['new hypernyms', 'compared to', 'DefIE'], ['new hypernyms', 'improving', 'recall'], ['DefIE', 'improving', 'recall'], ['recall', 'in', 'most domains'], ['technical areas', 'like', 'biology or health']]","[['detecting', 'has', 'new hypernyms'], ['surpassing', 'has', 'Yago']]",[],"[['Results', 'has', 'TAXOEM - BED']]",hypernym_discovery,3,140
2115,results,"However , our model does not perform particularly well on media and physics .","[('on', (9, 10))]","[('our model', (2, 4)), ('does not perform particularly well', (4, 9)), ('media and physics', (10, 13))]","[['does not perform particularly well', 'on', 'media and physics']]","[['our model', 'has', 'does not perform particularly well']]",[],"[['Results', 'has', 'our model']]",hypernym_discovery,3,141
2116,research-problem,Hypernym discovery aims to discover the hypernym word sets given a hyponym word and proper corpus .,[],"[('Hypernym discovery', (0, 2))]",[],[],[],[],hypernym_discovery,4,3
2117,research-problem,"In the past SemEval contest ( Sem Eval - 2015 task 17 1 , SemEval - 2016 task 13 2 ) , the "" Hypernym Detection "" task was treated as a classfication task , i.e. , given a ( hyponym , hypernym ) pair , deciding whether the pair is a true hypernymic relation or not .",[],"[('Hypernym Detection', (24, 26))]",[],[],[],[],hypernym_discovery,4,11
2118,hyperparameters,Word2vec is used to produce the word embeddings .,"[('to produce', (3, 5))]","[('Word2vec', (0, 1)), ('word embeddings', (6, 8))]","[['Word2vec', 'to produce', 'word embeddings']]",[],[],"[['Hyperparameters', 'has', 'Word2vec']]",hypernym_discovery,4,82
2119,hyperparameters,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,"[('used with', (11, 13)), ('set to', (16, 18))]","[('skip - gram model ( - cbow 0 )', (1, 10)), ('embedding dimension', (14, 16)), ('300 ( - size 300 )', (18, 24))]","[['skip - gram model ( - cbow 0 )', 'used with', 'embedding dimension'], ['embedding dimension', 'set to', '300 ( - size 300 )']]",[],[],"[['Hyperparameters', 'has', 'skip - gram model ( - cbow 0 )']]",hypernym_discovery,4,83
2120,results,Results Based on Projection Learning,"[('Based on', (1, 3))]","[('Projection Learning', (3, 5))]",[],[],"[['Results', 'Based on', 'Projection Learning']]",[],hypernym_discovery,4,87
2121,results,"By using the same evaluating metrics as PRF in the cited paper , our best F - value on the validation set is 0.68 ( the paper result is 0.73 ) when the best cluster number is 2 and the threshold is ( 17.7 , 17.3 ) .","[('on', (18, 19)), ('when', (31, 32))]","[('our best F - value', (13, 18)), ('validation set', (20, 22)), ('0.68', (23, 24)), ('best cluster number', (33, 36)), ('2', (37, 38)), ('threshold', (40, 41)), ('( 17.7 , 17.3 )', (42, 47))]","[['our best F - value', 'on', 'validation set'], ['0.68', 'when', 'best cluster number']]","[['best cluster number', 'has', '2'], ['threshold', 'has', '( 17.7 , 17.3 )']]",[],[],hypernym_discovery,4,91
2122,results,"This projection learning method performs not very well on task9 , we think the most probable reason is that in , the problem is formalized as a classification problem , in which the ( hyponym , hypernym ) pairs are given .","[('performs', (4, 5)), ('on', (8, 9)), ('is', (17, 18))]","[('projection learning method', (1, 4)), ('not very well', (5, 8)), ('task9', (9, 10))]","[['projection learning method', 'performs', 'not very well'], ['not very well', 'on', 'task9']]","[['projection learning method', 'has', 'not very well']]",[],"[['Results', 'has', 'projection learning method']]",hypernym_discovery,4,96
2123,results,Results Based on NN,[],"[('NN', (3, 4))]",[],[],[],[],hypernym_discovery,4,99
2124,results,The performance evaluated using either cross validation or the test data is much worse than that of a typical hypernym prediction task reported by .,"[('evaluated using', (2, 4)), ('than that of', (14, 17))]","[('performance', (1, 2)), ('either cross validation or the test data', (4, 11)), ('much worse', (12, 14)), ('typical hypernym prediction task', (18, 22))]","[['performance', 'evaluated using', 'either cross validation or the test data'], ['much worse', 'than that of', 'typical hypernym prediction task']]",[],[],"[['Results', 'has', 'performance']]",hypernym_discovery,4,102
2125,results,"Although the method proposed by us is quite simple , our submissions are the 1st on Spanish , the 2nd on Italian , the 6th on English , ranked by the metric of MAP .","[('on', (15, 16)), ('on', (20, 21)), ('on', (25, 26)), ('ranked by', (28, 30))]","[('submissions', (11, 12)), ('1st', (14, 15)), ('Spanish', (16, 17)), ('2nd', (19, 20)), ('Italian', (21, 22)), ('6th', (24, 25)), ('English', (26, 27)), ('metric of MAP', (31, 34))]","[['1st', 'on', 'Spanish'], ['6th', 'on', 'English'], ['2nd', 'on', 'Italian'], ['6th', 'on', 'English'], ['1st', 'ranked by', 'metric of MAP'], ['6th', 'ranked by', 'metric of MAP']]","[['submissions', 'has', '1st']]",[],[],hypernym_discovery,4,104
2126,results,"Compared with the results got by cross validation , the performance evaluated on the test data ) dropped significantly on English ( MAP dropped by 4 % ) and Italian ( MAP dropped by 8 % ) , but increased by a margin on Spanish ( MAP increased by 3.6 % ) .","[('Compared with', (0, 2)), ('got by', (4, 6)), ('evaluated on', (11, 13)), ('on', (19, 20)), ('dropped by', (23, 25)), ('dropped by', (32, 34)), ('by', (40, 41)), ('on', (43, 44)), ('increased by', (47, 49))]","[('results', (3, 4)), ('cross validation', (6, 8)), ('performance', (10, 11)), ('test data', (14, 16)), ('dropped', (17, 18)), ('significantly', (18, 19)), ('English', (20, 21)), ('MAP', (22, 23)), ('4 %', (25, 27)), ('Italian', (29, 30)), ('MAP', (31, 32)), ('8 %', (34, 36)), ('increased', (39, 40)), ('margin', (42, 43)), ('Spanish', (44, 45)), ('MAP', (46, 47)), ('3.6 %', (49, 51))]","[['results', 'got by', 'cross validation'], ['performance', 'evaluated on', 'test data'], ['dropped', 'on', 'English'], ['significantly', 'on', 'English'], ['dropped', 'dropped by', '4 %'], ['MAP', 'dropped by', '4 %'], ['MAP', 'dropped by', '8 %'], ['increased', 'by', 'margin'], ['margin', 'on', 'Spanish'], ['MAP', 'increased by', '3.6 %']]","[['results', 'has', 'cross validation'], ['cross validation', 'has', 'performance'], ['test data', 'has', 'dropped'], ['dropped', 'has', 'significantly'], ['English', 'has', 'MAP'], ['Italian', 'has', 'MAP'], ['increased', 'has', 'margin']]","[['Results', 'Compared with', 'results']]",[],hypernym_discovery,4,106
2127,research-problem,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery,[],"[('Hypernym Discovery', (10, 12))]",[],[],[],[],hypernym_discovery,5,2
2128,model,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .","[('developed by', (2, 4)), ('of', (10, 11)), ('exploits', (13, 14))]","[('CRIM team', (5, 7)), ('combination', (15, 16)), ('two approaches', (17, 19)), ('unsupervised , pattern - based approach', (21, 27)), ('supervised , projection learning approach', (29, 34))]","[['combination', 'of', 'two approaches']]","[['combination', 'has', 'two approaches'], ['two approaches', 'name', 'unsupervised , pattern - based approach']]","[['Model', 'developed by', 'CRIM team']]",[],hypernym_discovery,5,12
2129,results,Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,"[('ranked', (4, 5)), ('on', (6, 7))]","[('Our hybrid system', (0, 3)), ('1st', (5, 6)), ('all three sub - tasks', (7, 12)), ('submitted runs', (15, 17))]","[['Our hybrid system', 'ranked', '1st'], ['1st', 'on', 'all three sub - tasks']]","[['Our hybrid system', 'has', '1st']]",[],"[['Results', 'has', 'Our hybrid system']]",hypernym_discovery,5,134
2130,results,"As shown in , the scores obtained using this system are much higher than the strongest baselines for this task .","[('than', (13, 14))]","[('scores', (5, 6)), ('much higher', (11, 13)), ('strongest baselines', (15, 17))]","[['much higher', 'than', 'strongest baselines']]","[['scores', 'has', 'much higher'], ['much higher', 'has', 'strongest baselines']]",[],"[['Results', 'has', 'scores']]",hypernym_discovery,5,135
2131,results,"Furthermore , it is likely that we could improve our scores on 2A and 2B , since we only tuned the system on 1A .","[('improve', (8, 9)), ('on', (11, 12))]","[('likely', (4, 5)), ('our scores', (9, 11)), ('2A and 2B', (12, 15))]","[['likely', 'improve', 'our scores'], ['our scores', 'on', '2A and 2B']]",[],[],[],hypernym_discovery,5,136
2132,results,"If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .","[('compare', (2, 3)), ('of', (7, 8)), ('see that', (13, 15)), ('improved', (17, 18)), ('on', (21, 22)), ('by', (29, 30)), ('on', (32, 33))]","[('runs 1 and 2', (3, 7)), ('our hybrid system', (8, 11)), ('data augmentation', (15, 17)), ('our scores', (18, 20)), ('slightly', (20, 21)), ('1A and 2B', (22, 25)), ('increased', (27, 28)), ('several points', (30, 32)), ('2A', (33, 34))]","[['runs 1 and 2', 'of', 'our hybrid system'], ['our hybrid system', 'see that', 'data augmentation'], ['data augmentation', 'improved', 'our scores'], ['our scores', 'on', '1A and 2B'], ['slightly', 'on', '1A and 2B'], ['increased', 'by', 'several points'], ['several points', 'on', '2A']]","[['runs 1 and 2', 'has', 'our hybrid system'], ['our scores', 'has', 'slightly']]",[],[],hypernym_discovery,5,137
2133,results,"Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .","[('than', (5, 6)), ('computed using', (9, 11)), ('training', (17, 18)), ('on', (20, 21)), ('produced', (25, 26)), ('on', (28, 29)), ('than', (35, 36)), ('trained on', (41, 43))]","[('Our cross-evaluation results', (0, 3)), ('better', (4, 5)), ('supervised baseline', (7, 9)), ('normal evaluation setup', (12, 15)), ('our system', (18, 20)), ('general - purpose data', (21, 25)), ('better results', (26, 28)), ('domain - specific test set', (30, 35)), ('strong , supervised baseline', (37, 41)), ('domain - specific data', (44, 48))]","[['better', 'than', 'supervised baseline'], ['domain - specific test set', 'than', 'strong , supervised baseline'], ['supervised baseline', 'computed using', 'normal evaluation setup'], ['Our cross-evaluation results', 'training', 'our system'], ['our system', 'on', 'general - purpose data'], ['general - purpose data', 'produced', 'better results'], ['better results', 'on', 'domain - specific test set'], ['domain - specific test set', 'than', 'strong , supervised baseline'], ['strong , supervised baseline', 'trained on', 'domain - specific data']]","[['Our cross-evaluation results', 'has', 'better']]",[],"[['Results', 'has', 'Our cross-evaluation results']]",hypernym_discovery,5,138
2134,results,"Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task , and even outperformed the supervised baseline on 2A .","[('outperformed', (5, 6)), ('on', (11, 12))]","[('unsupervised system', (3, 5)), ('all other unsupervised systems', (6, 10)), ('supervised baseline', (19, 21)), ('2A', (22, 23))]","[['unsupervised system', 'outperformed', 'all other unsupervised systems'], ['supervised baseline', 'on', '2A']]",[],[],"[['Results', 'has', 'unsupervised system']]",hypernym_discovery,5,140
2135,results,"Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .","[('on', (15, 16)), ('use', (22, 23))]","[('run 1', (10, 12)), ('best', (14, 15)), ('all 3 test sets', (16, 20)), ('hybrid system', (24, 26))]","[['best', 'on', 'all 3 test sets']]","[['run 1', 'has', 'best']]",[],[],hypernym_discovery,5,143
2136,ablation-analysis,"To assess the influence of different aspects of the supervised system and its training algorithm , we carried out a few simple ablation tests on subtask 1 A .","[('carried out', (17, 19)), ('on', (24, 25))]","[('few simple ablation tests', (20, 24)), ('subtask 1 A', (25, 28))]","[['few simple ablation tests', 'on', 'subtask 1 A']]",[],"[['Ablation analysis', 'carried out', 'few simple ablation tests']]",[],hypernym_discovery,5,146
2137,ablation-analysis,"These results show that 2 of the techniques we used , namely subsampling and multitask learning , actually harmed our system 's performance on test set 1 A , although our experiments on the trial set suggested that they would be beneficial .","[('show that', (2, 4)), ('of', (5, 6)), ('namely', (11, 12)), ('harmed', (18, 19)), ('on', (23, 24))]","[('2', (4, 5)), ('techniques', (7, 8)), ('subsampling', (12, 13)), ('multitask learning', (14, 16)), (""our system 's performance"", (19, 23)), ('test set 1 A', (24, 28))]","[['2', 'of', 'techniques'], ['techniques', 'namely', 'subsampling'], ['techniques', 'namely', 'multitask learning'], ['techniques', 'harmed', ""our system 's performance""], ['multitask learning', 'harmed', ""our system 's performance""], [""our system 's performance"", 'on', 'test set 1 A']]","[['2', 'has', 'techniques'], ['techniques', 'name', 'subsampling']]","[['Ablation analysis', 'show that', '2']]",[],hypernym_discovery,5,165
2138,ablation-analysis,"On the other hand , fine - tuning the word embeddings during training seems to be one of the keys to the success of this approach , as are the use of multiple projection matrices , and the sampling of multiple negative examples for each positive example .","[('fine - tuning', (5, 8)), ('during', (11, 12)), ('seems to be', (13, 16)), ('of', (17, 18))]","[('word embeddings', (9, 11)), ('training', (12, 13)), ('keys to the success', (19, 23)), ('approach', (25, 26))]","[['word embeddings', 'during', 'training']]",[],"[['Ablation analysis', 'fine - tuning', 'word embeddings']]",[],hypernym_discovery,5,167
2139,ablation-analysis,"We should also note that the supervised model is prone to overfitting , and we found early stopping to be particularly important .","[('note', (3, 4)), ('prone to', (9, 11))]","[('supervised model', (6, 8)), ('overfitting', (11, 12))]","[['supervised model', 'prone to', 'overfitting']]",[],"[['Ablation analysis', 'note', 'supervised model']]",[],hypernym_discovery,5,173
2140,research-problem,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery,[],"[('Hypernym Discovery', (11, 13))]",[],[],[],[],hypernym_discovery,6,2
2141,model,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .","[('combines', (10, 11)), ('via', (19, 20)), ('concatenating', (20, 21)), ('constructed using', (28, 30)), ('obtained using', (37, 39))]","[('path - based technique and distributional technique', (12, 19)), ('two feature vectors', (21, 24)), ('feature vector', (26, 28)), ('dependency parser', (30, 32)), ('term embeddings', (39, 41))]","[['path - based technique and distributional technique', 'concatenating', 'two feature vectors'], ['feature vector', 'constructed using', 'dependency parser']]","[['two feature vectors', 'name', 'feature vector']]","[['Model', 'combines', 'path - based technique and distributional technique']]",[],hypernym_discovery,6,32
2142,model,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .","[('using', (3, 4)), ('create', (8, 9)), ('based on', (14, 16))]","[('concatenated vector', (5, 7)), ('binary supervised classifier model', (10, 14)), ('support vector machine ( SVM ) algorithm', (16, 23))]","[['concatenated vector', 'create', 'binary supervised classifier model'], ['binary supervised classifier model', 'based on', 'support vector machine ( SVM ) algorithm']]",[],"[['Model', 'using', 'concatenated vector']]",[],hypernym_discovery,6,33
2143,model,The model predicts if a term and its candidate hypernym are hypernym related or not .,"[('predicts', (2, 3))]","[('model', (1, 2)), ('term and its candidate hypernym', (5, 10)), ('hypernym related or not', (11, 15))]",[],[],[],"[['Model', 'has', 'model']]",hypernym_discovery,6,34
2144,experiments,Results and Analysis,[],"[('Results', (0, 1))]",[],[],[],[],hypernym_discovery,6,98
2145,results,"For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .","[('For', (0, 1)), ('performs', (7, 8)), ('than', (9, 10)), ('on', (21, 22))]","[('three corpora', (2, 4)), ('our system', (5, 7)), ('better', (8, 9)), ('STJU system', (10, 12)), ('MFH system', (19, 21)), ('English corpora', (23, 25))]","[['our system', 'performs', 'better'], ['better', 'than', 'STJU system'], ['MFH system', 'on', 'English corpora']]","[['three corpora', 'has', 'our system']]","[['Results', 'For', 'three corpora']]",[],hypernym_discovery,6,101
2146,results,"In addition , the result shows that our system performs well in discovering new hypernyms not defined in the gold hypernyms where it yields good False Positive values in the three corpora and we achieve the best False Positive value in Medical corpus The evaluation results of our system and other supervised systems .","[('shows', (5, 6)), ('performs', (9, 10)), ('in', (11, 12)), ('discovering', (12, 13)), ('not defined', (15, 17)), ('in', (17, 18))]","[('our system', (7, 9)), ('well', (10, 11)), ('new hypernyms', (13, 15)), ('gold hypernyms', (19, 21))]","[['our system', 'performs', 'well'], ['well', 'discovering', 'new hypernyms'], ['new hypernyms', 'in', 'gold hypernyms']]",[],"[['Results', 'shows', 'our system']]",[],hypernym_discovery,6,102
2147,results,"As shown in the table 2 , the candidate hypernym extraction ( CHE ) coverage for English testing terms is 950 ( 63 % ) , that means our system is unable to extract any candidate hypernym for 550 ( 37 % ) terms ( 398 entities and 152 concepts ) .","[('for', (15, 16)), ('is', (19, 20))]","[('candidate hypernym extraction ( CHE ) coverage', (8, 15)), ('English testing terms', (16, 19)), ('950 ( 63 % )', (20, 25))]","[['candidate hypernym extraction ( CHE ) coverage', 'for', 'English testing terms'], ['English testing terms', 'is', '950 ( 63 % )']]",[],[],[],hypernym_discovery,6,114
2148,research-problem,"This paper describes 300 - sparsans ' participation in SemEval - 2018 Task 9 : Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings .",[],"[('Hypernym Discovery', (15, 17))]",[],[],[],[],hypernym_discovery,7,3
2149,model,Here we apply sparse feature pairs to hypernym extraction .,"[('apply', (2, 3)), ('to', (6, 7))]","[('sparse feature pairs', (3, 6)), ('hypernym extraction', (7, 9))]","[['sparse feature pairs', 'to', 'hypernym extraction']]",[],"[['Model', 'apply', 'sparse feature pairs']]",[],hypernym_discovery,7,14
2150,research-problem,Sparse representation is related to hypernymy in various natural ways .,"[('related to', (3, 5))]","[('Sparse representation', (0, 2)), ('hypernymy', (5, 6))]","[['Sparse representation', 'related to', 'hypernymy']]",[],[],[],hypernym_discovery,7,16
2151,research-problem,One of them is through Formal concept Analysis ( FCA ) .,"[('through', (4, 5))]","[('Formal concept Analysis ( FCA )', (5, 11))]",[],[],"[['Research problem', 'through', 'Formal concept Analysis ( FCA )']]",[],hypernym_discovery,7,17
2152,research-problem,"Another natural formulation is related to hierarchical sparse coding , where trees describe the order in which variables "" enter the model "" ( i.e. , take non - zero values ) .","[('where', (10, 11)), ('describe', (12, 13)), ('in which', (15, 17)), ('enter', (19, 20)), ('take', (26, 27))]","[('hierarchical sparse coding', (6, 9)), ('trees', (11, 12)), ('order', (14, 15)), ('variables', (17, 18)), ('model', (21, 22)), ('non - zero values', (27, 31))]","[['hierarchical sparse coding', 'where', 'trees'], ['trees', 'describe', 'order'], ['order', 'in which', 'variables'], ['variables', 'enter', 'model'], ['variables', 'take', 'non - zero values'], ['model', 'take', 'non - zero values']]",[],[],[],hypernym_discovery,7,21
2153,model,Exploiting the correspondence between the variable tree and the hypernym hierarchy offers itself as a natural choice .,"[('Exploiting', (0, 1)), ('between', (3, 4)), ('offers', (11, 12))]","[('correspondence', (2, 3)), ('variable tree and the hypernym hierarchy', (5, 11)), ('natural choice', (15, 17))]","[['correspondence', 'between', 'variable tree and the hypernym hierarchy']]",[],"[['Model', 'Exploiting', 'correspondence']]",[],hypernym_discovery,7,24
2154,results,"Our submission with attribute pairs achieved first place in categories ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .","[('with', (2, 3)), ('achieved', (5, 6)), ('in categories', (8, 10))]","[('Our submission', (0, 2)), ('attribute pairs', (3, 5)), ('first place', (6, 8)), ('( 1B ) Italian ( all and entities )', (10, 19)), ('( 1C ) Spanish entities', (20, 25)), ('( 2B ) music entities', (27, 32))]","[['Our submission', 'with', 'attribute pairs'], ['attribute pairs', 'achieved', 'first place'], ['first place', 'in categories', '( 1B ) Italian ( all and entities )'], ['first place', 'in categories', '( 1C ) Spanish entities'], ['first place', 'in categories', '( 2B ) music entities']]",[],[],[],hypernym_discovery,7,98
2155,research-problem,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies,[],"[('Detecting Hypernymy Relations', (6, 9))]",[],[],[],[],hypernym_discovery,8,2
2156,research-problem,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,"[('presents', (2, 3)), ('for', (8, 9)), ('participated in', (12, 14)), ('of', (16, 17)), ('based on', (19, 21))]","[(""Apollo team 's system"", (4, 8)), ('hypernym discovery', (9, 11)), ('task 9', (14, 16)), ('Semeval 2018', (17, 19)), ('unsupervised machine learning', (21, 24))]","[[""Apollo team 's system"", 'for', 'hypernym discovery'], ['hypernym discovery', 'participated in', 'task 9'], ['task 9', 'of', 'Semeval 2018'], ['Semeval 2018', 'based on', 'unsupervised machine learning']]",[],"[['Research problem', 'presents', ""Apollo team 's system""]]",[],hypernym_discovery,8,8
2157,model,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,"[('exploits', (8, 9)), ('generalize', (13, 14))]","[('rule - based system', (3, 7)), ('syntactic dependency paths', (9, 12)), ('Hearst - style lexical patterns', (14, 19))]","[['rule - based system', 'exploits', 'syntactic dependency paths'], ['syntactic dependency paths', 'generalize', 'Hearst - style lexical patterns']]",[],[],"[['Model', 'has', 'rule - based system']]",hypernym_discovery,8,9
2158,experiments,Results,[],"[('Results', (0, 1))]",[],[],[],[],hypernym_discovery,8,60
2159,results,"While some relations have not been very fruitful ( such as X "" obj "" Y , for insance ) , others , instead , have been very productive , generating tens of thousands relations .",[],"[('relations', (2, 3)), ('not been very fruitful', (4, 8))]",[],"[['relations', 'has', 'not been very fruitful']]",[],[],hypernym_discovery,8,63
2160,results,"The project 's results show that we have managed to accomplish the main objective of this project , to outperform the random strategy .","[('outperform', (19, 20))]","[('random strategy', (21, 23))]",[],[],"[['Results', 'outperform', 'random strategy']]",[],hypernym_discovery,8,64
2161,results,"The lower scores have been obtained for multiword expressions , for which we plan to add dedicated modules .","[('obtained for', (5, 7))]","[('lower scores', (1, 3)), ('multiword expressions', (7, 9))]","[['lower scores', 'obtained for', 'multiword expressions']]",[],[],[],hypernym_discovery,8,65
2162,research-problem,Neural Models for Reasoning over Multiple Mentions using Coreference,[],"[('Reasoning over Multiple Mentions using Coreference', (3, 9))]",[],[],[],[],natural_language_inference,0,2
2163,model,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .",[],"[('coreference - based reasoning', (3, 7))]",[],[],[],[],natural_language_inference,0,12
2164,model,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .","[('given', (2, 3)), ('extracted from', (9, 11)), ('introduce', (16, 17)), ('in', (19, 20)), ('for', (23, 24))]","[('input sequence', (4, 6)), ('coreference clusters', (7, 9)), ('external system', (12, 14)), ('term', (18, 19)), ('update equations', (21, 23)), ('Gated Recurrent Units ( GRU )', (24, 30))]","[['coreference clusters', 'extracted from', 'external system'], ['coreference clusters', 'introduce', 'term'], ['term', 'in', 'update equations'], ['update equations', 'for', 'Gated Recurrent Units ( GRU )']]",[],"[['Model', 'given', 'input sequence']]",[],natural_language_inference,0,20
2165,model,This way hidden states are propagated along coreference chains and the original sequence in parallel .,"[('propagated along', (5, 7))]","[('hidden states', (2, 4)), ('coreference chains', (7, 9)), ('original sequence', (11, 13)), ('in parallel', (13, 15))]","[['hidden states', 'propagated along', 'coreference chains']]","[['original sequence', 'has', 'in parallel']]",[],"[['Model', 'has', 'hidden states']]",natural_language_inference,0,21
2166,model,We compare our Coref - GRU layer with the regular GRU layer by incorporating it in a recent model for reading comprehension .,"[('compare', (1, 2)), ('with', (7, 8)), ('incorporating it in', (13, 16)), ('for', (19, 20))]","[('our Coref - GRU layer', (2, 7)), ('regular GRU layer', (9, 12)), ('recent model', (17, 19)), ('reading comprehension', (20, 22))]","[['our Coref - GRU layer', 'with', 'regular GRU layer'], ['our Coref - GRU layer', 'incorporating it in', 'recent model'], ['regular GRU layer', 'incorporating it in', 'recent model'], ['recent model', 'for', 'reading comprehension']]",[],"[['Model', 'compare', 'our Coref - GRU layer']]",[],natural_language_inference,0,22
2167,results,BAbi AI tasks .,[],"[('BAbi AI tasks', (0, 3))]",[],[],[],"[['Results', 'has', 'BAbi AI tasks']]",natural_language_inference,0,94
2168,experiments,In each case we see clear improvements of using C - GRU layers over GRU layers .,"[('see', (4, 5)), ('of using', (7, 9)), ('over', (13, 14))]","[('clear improvements', (5, 7)), ('C - GRU layers', (9, 13)), ('GRU layers', (14, 16))]","[['clear improvements', 'of using', 'C - GRU layers'], ['C - GRU layers', 'over', 'GRU layers']]",[],[],[],natural_language_inference,0,100
2169,experiments,"Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .","[('Comparing to', (0, 2)), ('found that', (7, 9)), ('on', (15, 16))]","[('QRN baseline', (3, 5)), ('C - GRU', (9, 12)), ('significantly worse', (13, 15)), ('task 15 ( basic deduction )', (16, 22))]","[['QRN baseline', 'found that', 'C - GRU'], ['significantly worse', 'on', 'task 15 ( basic deduction )']]","[['QRN baseline', 'has', 'C - GRU'], ['C - GRU', 'has', 'significantly worse']]",[],[],natural_language_inference,0,105
2170,experiments,"On the other hand , C - GRU was significantly better than QRN on task 16 ( basic induction ) .","[('than', (11, 12)), ('on', (13, 14))]","[('C - GRU', (5, 8)), ('significantly better', (9, 11)), ('QRN', (12, 13)), ('task 16 ( basic induction )', (14, 20))]","[['significantly better', 'than', 'QRN'], ['significantly better', 'on', 'task 16 ( basic induction )'], ['QRN', 'on', 'task 16 ( basic induction )']]","[['C - GRU', 'has', 'significantly better']]",[],[],natural_language_inference,0,107
2171,experiments,Wikihop dataset .,[],"[('Wikihop dataset', (0, 2))]",[],[],[],[],natural_language_inference,0,112
2172,experiments,"We see higher performance for the C - GRU model in the low data regime , and better generalization throughout the training curve for all three settings .","[('see', (1, 2)), ('for', (4, 5)), ('in', (10, 11)), ('throughout', (19, 20)), ('for', (23, 24))]","[('higher performance', (2, 4)), ('C - GRU model', (6, 10)), ('low data regime', (12, 15)), ('better generalization', (17, 19)), ('training curve', (21, 23)), ('all three settings', (24, 27))]","[['higher performance', 'for', 'C - GRU model'], ['training curve', 'for', 'all three settings'], ['C - GRU model', 'in', 'low data regime'], ['better generalization', 'throughout', 'training curve'], ['training curve', 'for', 'all three settings']]",[],[],[],natural_language_inference,0,120
2173,research-problem,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension,[],"[('Reading Comprehension', (12, 14))]",[],[],[],[],natural_language_inference,1,2
2174,research-problem,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .,[],"[('Reading Comprehension ( RC )', (12, 17))]",[],[],[],[],natural_language_inference,1,4
2175,research-problem,"To show the effectiveness of our architecture , we conducted several experiments on the recently proposed and challenging RC dataset ' Nar - rative QA ' .",[],"[('RC', (18, 19))]",[],[],[],[],natural_language_inference,1,7
2176,model,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .","[('develop', (6, 7)), ('for', (17, 18)), ('skip through', (23, 25)), ('of', (27, 28)), ('generate', (31, 32)), ('using', (34, 35))]","[('novel context zoom - in network ( ConZNet )', (8, 17)), ('RC tasks', (18, 20)), ('irrelevant parts', (25, 27)), ('document', (29, 30)), ('answer', (33, 34)), ('only the relevant regions of text', (35, 41))]","[['novel context zoom - in network ( ConZNet )', 'for', 'RC tasks'], ['novel context zoom - in network ( ConZNet )', 'skip through', 'irrelevant parts'], ['irrelevant parts', 'of', 'document'], ['novel context zoom - in network ( ConZNet )', 'generate', 'answer'], ['answer', 'using', 'only the relevant regions of text']]",[],"[['Model', 'develop', 'novel context zoom - in network ( ConZNet )']]",[],natural_language_inference,1,19
2177,model,The ConZNet architecture consists of two phases .,"[('consists of', (3, 5))]","[('ConZNet architecture', (1, 3)), ('two phases', (5, 7))]","[['ConZNet architecture', 'consists of', 'two phases']]","[['ConZNet architecture', 'has', 'two phases']]",[],"[['Model', 'has', 'ConZNet architecture']]",natural_language_inference,1,20
2178,model,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,"[('In', (0, 1)), ('identify', (5, 6)), ('of', (9, 10)), ('by employing', (11, 13))]","[('first phase', (2, 4)), ('relevant regions', (7, 9)), ('text', (10, 11)), ('reinforcement learning algorithm', (14, 17))]","[['first phase', 'identify', 'relevant regions'], ['relevant regions', 'of', 'text'], ['relevant regions', 'by employing', 'reinforcement learning algorithm']]",[],"[['Model', 'In', 'first phase']]",[],natural_language_inference,1,21
2179,model,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .","[('based on', (4, 6)), ('comprehends', (13, 14)), ('generates', (20, 21)), ('by using', (23, 25)), ('as', (31, 32)), ('along with', (38, 40)), ('as', (43, 44))]","[('second phase', (1, 3)), ('encoder - decoder architecture', (7, 11)), ('identified regions of text', (15, 19)), ('answer', (22, 23)), ('residual self - attention network', (26, 31)), ('encoder', (32, 33)), ('RNNbased sequence generator', (35, 38)), ('pointer network', (41, 43)), ('decoder', (45, 46))]","[['second phase', 'based on', 'encoder - decoder architecture'], ['encoder - decoder architecture', 'comprehends', 'identified regions of text'], ['encoder - decoder architecture', 'generates', 'answer'], ['answer', 'by using', 'residual self - attention network'], ['answer', 'by using', 'RNNbased sequence generator'], ['residual self - attention network', 'as', 'encoder'], ['residual self - attention network', 'as', 'RNNbased sequence generator'], ['RNNbased sequence generator', 'along with', 'pointer network'], ['pointer network', 'as', 'decoder']]","[['second phase', 'has', 'encoder - decoder architecture']]",[],"[['Model', 'has', 'second phase']]",natural_language_inference,1,23
2180,model,"Unlike existing approaches , our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other .","[('to select', (9, 11)), ('of', (13, 14)), ('not just based on', (15, 19)), ('also on', (22, 24)), ('related to', (28, 30))]","[('our method', (4, 6)), ('ability', (8, 9)), ('relevant regions', (11, 13)), ('text', (14, 15)), ('question', (20, 21)), ('regions', (26, 27)), ('each other', (30, 32))]","[['ability', 'to select', 'relevant regions'], ['relevant regions', 'of', 'text'], ['text', 'not just based on', 'question'], ['regions', 'related to', 'each other']]","[['our method', 'has', 'ability']]",[],"[['Model', 'has', 'our method']]",natural_language_inference,1,27
2181,model,"Moreover , our decoder combines span prediction and sequence generation .","[('combines', (4, 5))]","[('our decoder', (2, 4)), ('span prediction', (5, 7)), ('sequence generation', (8, 10))]","[['our decoder', 'combines', 'span prediction'], ['our decoder', 'combines', 'sequence generation']]",[],[],"[['Model', 'has', 'our decoder']]",natural_language_inference,1,28
2182,model,This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary .,"[('allows', (1, 2)), ('to copy', (4, 6)), ('from', (7, 8)), ('of', (11, 12)), ('to generate', (16, 18)), ('from', (19, 20))]","[('decoder', (3, 4)), ('words', (6, 7)), ('relevant regions', (9, 11)), ('text', (12, 13)), ('words', (18, 19)), ('fixed vocabulary', (21, 23))]","[['decoder', 'to copy', 'words'], ['words', 'from', 'relevant regions'], ['words', 'from', 'fixed vocabulary'], ['relevant regions', 'of', 'text'], ['decoder', 'to generate', 'words'], ['words', 'from', 'fixed vocabulary']]",[],"[['Model', 'allows', 'decoder']]",[],natural_language_inference,1,29
2183,baselines,"We compare our model against reported models in ( Seq2Seq , ASR , BiDAF ) and the Multi-range Reasoning Unit ( MRU ) in .","[('compare', (1, 2)), ('against', (4, 5)), ('in', (7, 8))]","[('our model', (2, 4)), ('reported models', (5, 7)), ('Seq2Seq', (9, 10)), ('ASR', (11, 12)), ('BiDAF', (13, 14)), ('Multi-range Reasoning Unit ( MRU )', (17, 23))]","[['our model', 'against', 'reported models'], ['our model', 'against', 'Multi-range Reasoning Unit ( MRU )'], ['reported models', 'in', 'Seq2Seq'], ['reported models', 'in', 'ASR'], ['reported models', 'in', 'BiDAF'], ['reported models', 'in', 'Multi-range Reasoning Unit ( MRU )']]",[],"[['Baselines', 'compare', 'our model']]",[],natural_language_inference,1,117
2184,baselines,"We implemented two baseline models ( Baseline 1 , Baseline 2 ) with Context Zoom layer similar to .","[('implemented', (1, 2)), ('with', (12, 13))]","[('two baseline models', (2, 5)), ('Context Zoom layer', (13, 16))]","[['two baseline models', 'with', 'Context Zoom layer']]",[],"[['Baselines', 'implemented', 'two baseline models']]",[],natural_language_inference,1,118
2185,experimental-setup,The model is implemented using Python and Tensorflow .,"[('implemented using', (3, 5))]","[('model', (1, 2)), ('Python', (5, 6)), ('Tensorflow', (7, 8))]","[['model', 'implemented using', 'Python'], ['model', 'implemented using', 'Tensorflow']]",[],[],"[['Experimental setup', 'has', 'model']]",natural_language_inference,1,125
2186,experimental-setup,All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,"[('of', (3, 4)), ('initialized by', (7, 9)), ('initialized with', (14, 16))]","[('weights', (2, 3)), ('model', (5, 6)), ('Glorot Initialization', (9, 11)), ('biases', (12, 13)), ('zeros', (16, 17))]","[['weights', 'of', 'model'], ['weights', 'initialized by', 'Glorot Initialization'], ['model', 'initialized by', 'Glorot Initialization'], ['biases', 'initialized with', 'zeros']]",[],[],"[['Experimental setup', 'has', 'weights']]",natural_language_inference,1,126
2187,experimental-setup,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .","[('use', (1, 2)), ('from', (7, 8)), ('with', (10, 11)), ('to initialize', (16, 18))]","[('300 dimensional word vectors', (3, 7)), ('GloVe', (8, 9)), ('840 billion pre-trained vectors', (11, 15)), ('word embeddings', (19, 21))]","[['300 dimensional word vectors', 'from', 'GloVe'], ['GloVe', 'with', '840 billion pre-trained vectors'], ['300 dimensional word vectors', 'to initialize', 'word embeddings']]",[],"[['Experimental setup', 'use', '300 dimensional word vectors']]",[],natural_language_inference,1,127
2188,experimental-setup,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,"[('not appear in', (5, 8)), ('initialized by', (10, 12)), ('from', (13, 14))]","[('All the words', (0, 3)), ('Glove', (8, 9)), ('sampling', (12, 13)), ('uniform random distribution', (15, 18))]","[['All the words', 'not appear in', 'Glove'], ['All the words', 'initialized by', 'sampling'], ['sampling', 'from', 'uniform random distribution']]",[],[],"[['Experimental setup', 'has', 'All the words']]",natural_language_inference,1,128
2189,experimental-setup,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,"[('apply', (1, 2)), ('between', (3, 4)), ('with', (6, 7)), ('of', (9, 10))]","[('dropout', (2, 3)), ('layers', (5, 6)), ('keep probability', (7, 9)), ('0.8', (10, 11))]","[['dropout', 'between', 'layers'], ['dropout', 'with', 'keep probability'], ['keep probability', 'of', '0.8']]",[],"[['Experimental setup', 'apply', 'dropout']]",[],natural_language_inference,1,129
2190,experimental-setup,The number of hidden units are set to 100 .,"[('set to', (6, 8))]","[('number of hidden units', (1, 5)), ('100', (8, 9))]","[['number of hidden units', 'set to', '100']]","[['number of hidden units', 'has', '100']]",[],"[['Experimental setup', 'has', 'number of hidden units']]",natural_language_inference,1,130
2191,experimental-setup,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .","[('trained', (1, 2)), ('with', (4, 5)), ('for', (13, 14)), ('of', (21, 22)), ('of', (28, 29))]","[('our model', (2, 4)), ('AdaDelta ( Zeiler , 2012 ) optimizer', (6, 13)), ('50 epochs', (14, 16)), ('initial learning rate', (18, 21)), ('0.1', (22, 23)), ('minibatch size', (26, 28)), ('32', (29, 30))]","[['our model', 'with', 'AdaDelta ( Zeiler , 2012 ) optimizer'], ['our model', 'with', 'minibatch size'], ['AdaDelta ( Zeiler , 2012 ) optimizer', 'for', '50 epochs'], ['initial learning rate', 'of', '0.1'], ['minibatch size', 'of', '32']]","[['initial learning rate', 'has', '0.1']]","[['Experimental setup', 'trained', 'our model']]",[],natural_language_inference,1,131
2192,results,The performance of our model gradually dropped from sample size 7 onwards .,"[('of', (2, 3)), ('from', (7, 8))]","[('performance', (1, 2)), ('our model', (3, 5)), ('gradually dropped', (5, 7)), ('sample size 7 onwards', (8, 12))]","[['performance', 'of', 'our model'], ['gradually dropped', 'from', 'sample size 7 onwards']]","[['our model', 'has', 'gradually dropped']]",[],"[['Results', 'has', 'performance']]",natural_language_inference,1,138
2193,results,This result shows evidence that only a few relevant sentences are sufficient to answer a question .,"[('shows', (2, 3)), ('sufficient to', (11, 13)), ('answer', (13, 14))]","[('evidence', (3, 4)), ('only a few relevant sentences', (5, 10)), ('question', (15, 16))]","[['only a few relevant sentences', 'answer', 'question']]","[['evidence', 'has', 'only a few relevant sentences']]","[['Results', 'shows', 'evidence']]",[],natural_language_inference,1,139
2194,results,The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1 .,"[('with', (7, 8)), ('compared to', (13, 15))]","[('the model', (3, 5)), ('improved dramatically', (5, 7)), ('sample sizes 3 and 5', (8, 13)), ('sample size of 1', (16, 20))]","[['improved dramatically', 'with', 'sample sizes 3 and 5'], ['sample sizes 3 and 5', 'compared to', 'sample size of 1']]","[['the model', 'has', 'improved dramatically']]",[],"[['Results', 'has', 'the model']]",natural_language_inference,1,141
2195,ablation-analysis,This result points out that the self - attention mechanism in the Context zoom layer is an important component to identify related relevant sentences .,"[('in', (10, 11)), ('to identify', (19, 21))]","[('self - attention mechanism', (6, 10)), ('Context zoom layer', (12, 15)), ('important component', (17, 19)), ('related relevant sentences', (21, 24))]","[['self - attention mechanism', 'in', 'Context zoom layer'], ['important component', 'to identify', 'related relevant sentences']]",[],[],"[['Ablation analysis', 'has', 'self - attention mechanism']]",natural_language_inference,1,144
2196,research-problem,A Simple and Effective Approach to the Story Cloze Test,[],"[('Story Cloze Test', (7, 10))]",[],[],[],[],natural_language_inference,10,2
2197,model,"Where previous approaches rely on feature engineering or involved neural network architectures , we achieve high accuracy with a fully neural approach involving only a single feedforward network and pre-trained skip - thought embeddings .","[('achieve', (14, 15)), ('with', (17, 18)), ('involving', (22, 23))]","[('high accuracy', (15, 17)), ('fully neural approach', (19, 22)), ('single feedforward network', (25, 28)), ('pre-trained skip - thought embeddings', (29, 34))]","[['high accuracy', 'with', 'fully neural approach'], ['fully neural approach', 'involving', 'single feedforward network'], ['fully neural approach', 'involving', 'pre-trained skip - thought embeddings']]",[],"[['Model', 'achieve', 'high accuracy']]",[],natural_language_inference,10,26
2198,experiments,"Second , we find that considering only the last sentence of the context outperforms models that consider the full context .","[('considering', (5, 6)), ('of', (10, 11)), ('consider', (16, 17))]","[('only the last sentence', (6, 10)), ('context', (12, 13)), ('outperforms', (13, 14)), ('models', (14, 15)), ('full context', (18, 20))]","[['models', 'considering', 'full context'], ['only the last sentence', 'of', 'context'], ['models', 'consider', 'full context']]","[['context', 'has', 'outperforms'], ['outperforms', 'has', 'models']]",[],[],natural_language_inference,10,27
2199,model,"In sum , our approach differs from previous efforts in the joint use of three strategies : ( 1 ) using skip - thought embeddings for sentences in the story in a feed - forward neural network , ( 2 ) training the model on the provided validation set , and ( 3 ) considering the two endings with only the last sentence in the prompt .","[('using', (20, 21)), ('for', (25, 26)), ('in', (27, 28)), ('in', (30, 31)), ('training', (41, 42)), ('on', (44, 45)), ('considering', (54, 55)), ('with', (58, 59)), ('in', (63, 64))]","[('our approach', (3, 5)), ('skip - thought embeddings', (21, 25)), ('sentences', (26, 27)), ('story', (29, 30)), ('feed - forward neural network', (32, 37)), ('model', (43, 44)), ('provided validation set', (46, 49)), ('two endings', (56, 58)), ('only the last sentence', (59, 63)), ('prompt', (65, 66))]","[['skip - thought embeddings', 'for', 'sentences'], ['sentences', 'in', 'story'], ['story', 'in', 'feed - forward neural network'], ['model', 'on', 'provided validation set'], ['two endings', 'with', 'only the last sentence'], ['only the last sentence', 'in', 'prompt']]",[],[],"[['Model', 'has', 'our approach']]",natural_language_inference,10,29
2200,hyperparameters,We use cross-entropy loss and SGD with learning rate of 0.01 .,"[('use', (1, 2)), ('with', (6, 7)), ('of', (9, 10))]","[('cross-entropy loss', (2, 4)), ('SGD', (5, 6)), ('learning rate', (7, 9)), ('0.01', (10, 11))]","[['SGD', 'with', 'learning rate'], ['learning rate', 'of', '0.01']]",[],"[['Hyperparameters', 'use', 'cross-entropy loss']]",[],natural_language_inference,10,83
2201,hyperparameters,"During training , we save the model every 3000 iterations , and calculate the validation accuracy .","[('During', (0, 1)), ('save', (4, 5))]","[('training', (1, 2)), ('model', (6, 7)), ('every 3000 iterations', (7, 10))]","[['training', 'save', 'model']]","[['model', 'has', 'every 3000 iterations']]","[['Hyperparameters', 'During', 'training']]",[],natural_language_inference,10,84
2202,results,The 3 - layer feed - forward neural network trained on the validation set by summing the skip - thought embeddings of the last sentence ( LS ) of the story prompt and the ending gives the best accuracy ( 76.5 % ) .,"[('trained on', (9, 11)), ('summing', (15, 16)), ('of', (21, 22)), ('of', (28, 29)), ('gives', (35, 36))]","[('3 - layer feed - forward neural network', (1, 9)), ('validation set', (12, 14)), ('skip - thought embeddings', (17, 21)), ('last sentence ( LS )', (23, 28)), ('story prompt and the ending', (30, 35)), ('best accuracy ( 76.5 % )', (37, 43))]","[['3 - layer feed - forward neural network', 'trained on', 'validation set'], ['validation set', 'summing', 'skip - thought embeddings'], ['skip - thought embeddings', 'of', 'last sentence ( LS )'], ['skip - thought embeddings', 'of', 'last sentence ( LS )'], ['last sentence ( LS )', 'of', 'story prompt and the ending'], ['story prompt and the ending', 'gives', 'best accuracy ( 76.5 % )']]",[],[],"[['Results', 'has', '3 - layer feed - forward neural network']]",natural_language_inference,10,89
2203,results,"Comparing ' val - LS- skip ' to ' val - LS - Glo Ve ' ( i.e. , using skip - thought embeddings for sentences vs. GloVe word embeddings ) , we confirm that the success of this approach lies in the sizable boost to accuracy from the use of pretrained skip - thought embeddings .","[('Comparing', (0, 1)), ('to', (7, 8)), ('using', (19, 20)), ('for', (24, 25)), ('confirm', (33, 34)), ('of', (37, 38)), ('lies in', (40, 42)), ('to', (45, 46)), ('from the use of', (47, 51))]","[('val - LS- skip', (2, 6)), ('val - LS - Glo Ve', (9, 15)), ('skip - thought embeddings', (20, 24)), ('sentences vs. GloVe word embeddings', (25, 30)), ('success', (36, 37)), ('this approach', (38, 40)), ('sizable boost', (43, 45)), ('accuracy', (46, 47)), ('pretrained skip - thought embeddings', (51, 56))]","[['val - LS- skip', 'to', 'val - LS - Glo Ve'], ['sizable boost', 'to', 'accuracy'], ['val - LS- skip', 'using', 'skip - thought embeddings'], ['val - LS - Glo Ve', 'using', 'skip - thought embeddings'], ['skip - thought embeddings', 'for', 'sentences vs. GloVe word embeddings'], ['val - LS- skip', 'confirm', 'success'], ['val - LS - Glo Ve', 'confirm', 'success'], ['success', 'of', 'this approach'], ['this approach', 'lies in', 'sizable boost'], ['sizable boost', 'to', 'accuracy'], ['accuracy', 'from the use of', 'pretrained skip - thought embeddings']]","[['val - LS- skip', 'name', 'val - LS - Glo Ve']]","[['Results', 'Comparing', 'val - LS- skip']]",[],natural_language_inference,10,91
2204,results,"We note that the model trained using only the last sentence ( LS ) of the story context has higher accuracy compared to the model that uses a GRU to encode the full context ( FC ) , and even the model which encodes the entire context .","[('trained using only', (5, 8)), ('of', (14, 15)), ('compared to', (21, 23)), ('that uses', (25, 27)), ('to encode', (29, 31)), ('encodes', (43, 44))]","[('model', (4, 5)), ('last sentence ( LS )', (9, 14)), ('story context', (16, 18)), ('higher accuracy', (19, 21)), ('model', (24, 25)), ('GRU', (28, 29)), ('full context ( FC )', (32, 37)), ('entire context', (45, 47))]","[['model', 'trained using only', 'last sentence ( LS )'], ['last sentence ( LS )', 'of', 'story context'], ['higher accuracy', 'compared to', 'model'], ['model', 'that uses', 'GRU'], ['GRU', 'to encode', 'full context ( FC )']]",[],[],"[['Results', 'has', 'model']]",natural_language_inference,10,95
2205,research-problem,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING,[],"[('QUESTION ANSWERING', (12, 14))]",[],[],[],[],natural_language_inference,11,2
2206,research-problem,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,[],"[('Question answering ( QA )', (0, 5))]",[],[],[],[],natural_language_inference,11,12
2207,research-problem,"Previous QA datasets tend to be high in quality due to human annotation , but small in size .",[],"[('QA', (1, 2))]",[],[],[],[],natural_language_inference,11,13
2208,model,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .","[('introduce', (1, 2)), ('for', (21, 22))]","[('Dynamic Coattention Network ( DCN )', (3, 9)), ('end - to - end neural network', (14, 21)), ('question answering', (22, 24))]","[['end - to - end neural network', 'for', 'question answering']]",[],"[['Model', 'introduce', 'Dynamic Coattention Network ( DCN )']]",[],natural_language_inference,11,22
2209,model,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .","[('consists of', (2, 4)), ('captures', (8, 9)), ('between', (11, 12)), ('alternates between', (26, 28)), ('of', (33, 34))]","[('The model', (0, 2)), ('coattentive encoder', (5, 7)), ('interactions', (10, 11)), ('question and the document', (13, 17)), ('dynamic pointing decoder', (22, 25)), ('estimating', (28, 29)), ('start and end', (30, 33)), ('answer span', (35, 37))]","[['The model', 'consists of', 'coattentive encoder'], ['coattentive encoder', 'captures', 'interactions'], ['interactions', 'between', 'question and the document'], ['dynamic pointing decoder', 'alternates between', 'estimating'], ['start and end', 'of', 'answer span']]","[['estimating', 'has', 'start and end']]",[],"[['Model', 'has', 'The model']]",natural_language_inference,11,23
2210,experimental-setup,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .","[('To preprocess', (0, 2)), ('use', (6, 7)), ('from', (9, 10))]","[('corpus', (3, 4)), ('tokenizer', (8, 9)), ('Stanford CoreNLP', (10, 12))]","[['corpus', 'use', 'tokenizer'], ['tokenizer', 'from', 'Stanford CoreNLP']]",[],"[['Experimental setup', 'To preprocess', 'corpus']]",[],natural_language_inference,11,127
2211,experimental-setup,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,"[('use', (1, 2)), ('pretrained on', (7, 9))]","[('Glo Ve word vectors', (3, 7)), ('840B Common Crawl corpus', (10, 14))]","[['Glo Ve word vectors', 'pretrained on', '840B Common Crawl corpus']]",[],"[['Experimental setup', 'use', 'Glo Ve word vectors']]",[],natural_language_inference,11,128
2212,experimental-setup,We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,"[('limit', (1, 2)), ('to', (4, 5)), ('present in', (8, 10)), ('set', (15, 16)), ('for', (17, 18)), ('to', (24, 25))]","[('vocabulary', (3, 4)), ('words', (5, 6)), ('Common Crawl corpus', (11, 14)), ('embeddings', (16, 17)), ('out - of - vocabulary words', (18, 24)), ('zero', (25, 26))]","[['vocabulary', 'to', 'words'], ['out - of - vocabulary words', 'to', 'zero'], ['words', 'present in', 'Common Crawl corpus'], ['embeddings', 'for', 'out - of - vocabulary words'], ['out - of - vocabulary words', 'to', 'zero']]","[['vocabulary', 'has', 'words']]","[['Experimental setup', 'limit', 'vocabulary']]",[],natural_language_inference,11,129
2213,experimental-setup,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .","[('of', (6, 7)), ('during', (8, 9)), ('of', (15, 16)), ('for', (17, 18))]","[('max sequence length', (3, 6)), ('600', (7, 8)), ('training', (9, 10)), ('hidden state size', (12, 15)), ('200', (16, 17)), ('recurrent units', (19, 21)), ('maxout layers', (22, 24)), ('linear layers', (26, 28))]","[['max sequence length', 'of', '600'], ['hidden state size', 'of', '200'], ['600', 'during', 'training'], ['hidden state size', 'of', '200'], ['hidden state size', 'for', 'recurrent units'], ['200', 'for', 'recurrent units'], ['200', 'for', 'linear layers']]","[['max sequence length', 'has', '600'], ['hidden state size', 'has', '200']]",[],[],natural_language_inference,11,131
2214,experimental-setup,All LSTMs have randomly initialized parameters and an initial state of zero .,"[('of', (10, 11))]","[('All LSTMs', (0, 2)), ('randomly initialized', (3, 5)), ('parameters', (5, 6)), ('initial state', (8, 10)), ('zero', (11, 12))]","[['initial state', 'of', 'zero']]","[['All LSTMs', 'has', 'randomly initialized'], ['randomly initialized', 'has', 'parameters']]",[],"[['Experimental setup', 'has', 'All LSTMs']]",natural_language_inference,11,132
2215,experimental-setup,Sentinel vectors are randomly initialized and optimized during training .,"[('during', (7, 8))]","[('Sentinel vectors', (0, 2)), ('randomly initialized', (3, 5)), ('optimized', (6, 7)), ('training', (8, 9))]","[['optimized', 'during', 'training']]","[['Sentinel vectors', 'has', 'randomly initialized']]",[],"[['Experimental setup', 'has', 'Sentinel vectors']]",natural_language_inference,11,133
2216,experimental-setup,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .","[('For', (0, 1)), ('set', (6, 7)), ('of', (10, 11)), ('to', (12, 13)), ('use', (15, 16)), ('of', (20, 21))]","[('dynamic decoder', (2, 4)), ('maximum number', (8, 10)), ('iterations', (11, 12)), ('4', (13, 14)), ('maxout pool size', (17, 20)), ('16', (21, 22))]","[['dynamic decoder', 'set', 'maximum number'], ['maximum number', 'of', 'iterations'], ['maxout pool size', 'of', '16'], ['iterations', 'to', '4'], ['dynamic decoder', 'use', 'maxout pool size'], ['maxout pool size', 'of', '16']]","[['dynamic decoder', 'has', 'maximum number'], ['maximum number', 'has', 'iterations'], ['maxout pool size', 'has', '16']]","[['Experimental setup', 'For', 'dynamic decoder']]",[],natural_language_inference,11,134
2217,experimental-setup,"We use dropout to regularize our network during training , and optimize the model using ADAM .","[('to regularize', (3, 5)), ('during', (7, 8)), ('optimize', (11, 12)), ('using', (14, 15))]","[('dropout', (2, 3)), ('our network', (5, 7)), ('training', (8, 9)), ('model', (13, 14)), ('ADAM', (15, 16))]","[['dropout', 'to regularize', 'our network'], ['our network', 'during', 'training'], ['dropout', 'optimize', 'model'], ['model', 'using', 'ADAM']]",[],[],[],natural_language_inference,11,135
2218,experimental-setup,All models are implemented and trained with Chainer .,"[('implemented and trained with', (3, 7))]","[('All models', (0, 2)), ('Chainer', (7, 8))]","[['All models', 'implemented and trained with', 'Chainer']]",[],[],"[['Experimental setup', 'has', 'All models']]",natural_language_inference,11,136
2219,results,"The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .","[('of', (2, 3)), ('to estimate', (30, 32))]","[('DCN', (26, 27)), ('capability', (29, 30)), ('start and end points', (33, 37)), ('answer span', (39, 41)), ('multiple times', (41, 43))]","[['start and end points', 'of', 'answer span'], ['capability', 'to estimate', 'start and end points']]","[['DCN', 'has', 'capability'], ['answer span', 'has', 'multiple times']]","[['Results', 'of', 'DCN']]",[],natural_language_inference,11,145
2220,results,"By doing so , the model is able to explore local maxima corresponding to multiple plausible answers , as is shown in .","[('explore', (9, 10)), ('corresponding to', (12, 14))]","[('model', (5, 6)), ('local maxima', (10, 12)), ('multiple plausible answers', (14, 17))]","[['model', 'explore', 'local maxima'], ['local maxima', 'corresponding to', 'multiple plausible answers']]",[],[],"[['Results', 'has', 'model']]",natural_language_inference,11,146
2221,research-problem,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING,[],"[('TEXT BASED REASONING', (13, 16))]",[],[],[],[],natural_language_inference,12,2
2222,research-problem,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .",[],"[('text - based question and answering', (3, 9))]",[],[],[],[],natural_language_inference,12,4
2223,code,* Code is publicly available at : https://github.com/juung/RMN,[],"[('https://github.com/juung/RMN', (7, 8))]",[],[],[],[],natural_language_inference,12,11
2224,model,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .","[('is', (13, 14)), ('to find', (15, 17)), ('when', (20, 21))]","[('"" Relation Memory Network "" ( RMN )', (4, 12)), ('complex relation', (17, 19)), ('a lot of information', (21, 25)), ('given', (26, 27))]","[['"" Relation Memory Network "" ( RMN )', 'to find', 'complex relation'], ['complex relation', 'when', 'a lot of information']]","[['a lot of information', 'has', 'given']]",[],"[['Model', 'has', '"" Relation Memory Network "" ( RMN )']]",natural_language_inference,12,42
2225,model,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,"[('uses', (1, 2)), ('to find', (3, 5)), ('with', (8, 9)), ('simply erase', (13, 15))]","[('MLP', (2, 3)), ('relevant information', (6, 8)), ('new generalization', (10, 12)), ('information already used', (16, 19))]","[['MLP', 'to find', 'relevant information'], ['relevant information', 'with', 'new generalization'], ['new generalization', 'simply erase', 'information already used']]",[],"[['Model', 'uses', 'MLP']]",[],natural_language_inference,12,43
2226,model,"In other words , RMN inherits RN 's MLP - based output feature map on Memory Network architecture .","[('inherits', (5, 6)), ('on', (14, 15))]","[('RMN', (4, 5)), (""RN 's MLP - based output feature map"", (6, 14)), ('Memory Network architecture', (15, 18))]","[['RMN', 'inherits', ""RN 's MLP - based output feature map""], [""RN 's MLP - based output feature map"", 'on', 'Memory Network architecture']]",[],[],"[['Model', 'has', 'RMN']]",natural_language_inference,12,44
2227,experiments,bAbI story - based QA dataset,[],"[('bAbI story - based QA dataset', (0, 6))]",[],[],[],[],natural_language_inference,12,125
2228,model,"Embedding component is similar to , where story and question are embedded through different LSTMs ; 32 unit word - lookup embeddings ; 32 unit LSTM for story and question .","[('for', (26, 27))]","[('Embedding component', (0, 2)), ('story and question', (7, 10)), ('32 unit', (16, 18)), ('word - lookup embeddings', (18, 22)), ('LSTM', (25, 26))]",[],"[['32 unit', 'has', 'word - lookup embeddings']]",[],"[['Model', 'has', 'Embedding component']]",natural_language_inference,12,129
2229,model,"For attention component , as we use 2 hop RMN , there are g 1 ?","[('For', (0, 1)), ('use', (6, 7))]","[('attention component', (1, 3)), ('2 hop RMN', (7, 10))]","[['attention component', 'use', '2 hop RMN']]","[['attention component', 'has', '2 hop RMN']]","[['Model', 'For', 'attention component']]",[],natural_language_inference,12,130
2230,hyperparameters,"For regularization , we use batch normalization for all MLPs .","[('use', (4, 5)), ('for', (7, 8))]","[('regularization', (1, 2)), ('batch normalization', (5, 7)), ('all MLPs', (8, 10))]","[['regularization', 'use', 'batch normalization'], ['batch normalization', 'for', 'all MLPs']]",[],[],[],natural_language_inference,12,132
2231,hyperparameters,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,"[('optimized with', (4, 6)), ('using', (12, 13)), ('with', (16, 17)), ('of', (20, 21))]","[('softmax output', (1, 3)), ('cross - entropy loss function', (7, 12)), ('Adam optimizer', (14, 16)), ('learning rate', (18, 20)), ('2 e ?4', (21, 24))]","[['softmax output', 'optimized with', 'cross - entropy loss function'], ['cross - entropy loss function', 'using', 'Adam optimizer'], ['cross - entropy loss function', 'with', 'learning rate'], ['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '2 e ?4']]","[['learning rate', 'has', '2 e ?4']]",[],"[['Hyperparameters', 'has', 'softmax output']]",natural_language_inference,12,133
2232,experiments,bAbI dialog dataset,[],"[('bAbI dialog dataset', (0, 3))]",[],[],[],[],natural_language_inference,12,134
2233,model,"We trained on full dialog scripts with every model response as answer , all previous dialog history as sentences to be memorized , and the last user utterance as question .","[('trained on', (1, 3)), ('with', (6, 7)), ('as', (10, 11)), ('as', (17, 18)), ('to be', (19, 21)), ('as', (28, 29))]","[('full dialog scripts', (3, 6)), ('every model response', (7, 10)), ('answer', (11, 12)), ('all previous dialog history', (13, 17)), ('sentences', (18, 19)), ('memorized', (21, 22)), ('last user utterance', (25, 28)), ('question', (29, 30))]","[['full dialog scripts', 'with', 'every model response'], ['every model response', 'as', 'answer'], ['all previous dialog history', 'as', 'sentences'], ['sentences', 'to be', 'memorized'], ['last user utterance', 'as', 'question']]",[],"[['Model', 'trained on', 'full dialog scripts']]",[],natural_language_inference,12,135
2234,model,"Model selects the most probable response from 4,212 candidates which are ranked from a set of all bot utterances appearing in training , validation and test sets ( plain and OOV ) for all tasks combined .","[('selects', (1, 2)), ('from', (6, 7)), ('ranked from', (11, 13)), ('of', (15, 16)), ('in', (20, 21))]","[('most probable response', (3, 6)), ('4,212 candidates', (7, 9)), ('set', (14, 15)), ('all bot utterances', (16, 19)), ('training , validation and test sets', (21, 27))]","[['most probable response', 'from', '4,212 candidates'], ['4,212 candidates', 'ranked from', 'set'], ['set', 'of', 'all bot utterances']]",[],"[['Model', 'selects', 'most probable response']]",[],natural_language_inference,12,136
2235,experiments,BABI DIALOG,[],"[('BABI DIALOG', (0, 2))]",[],[],[],[],natural_language_inference,12,154
2236,experiments,"Without any match type , RN and RMN outperform previous memory - augmented models on both normal and OOV tasks .","[('Without', (0, 1)), ('outperform', (8, 9)), ('on', (14, 15))]","[('any match type', (1, 4)), ('RN and RMN', (5, 8)), ('previous memory - augmented models', (9, 14)), ('normal and OOV tasks', (16, 20))]","[['RN and RMN', 'outperform', 'previous memory - augmented models'], ['previous memory - augmented models', 'on', 'normal and OOV tasks']]","[['any match type', 'has', 'RN and RMN']]",[],[],natural_language_inference,12,156
2237,model,"We converted RMN 's attention component to inner product based attention , and the results revealed the error rate increased to 11.3 % .","[('converted', (1, 2)), ('to', (6, 7)), ('revealed', (15, 16)), ('increased to', (19, 21))]","[(""RMN 's attention component"", (2, 6)), ('inner product based attention', (7, 11)), ('results', (14, 15)), ('error rate', (17, 19)), ('11.3 %', (21, 23))]","[[""RMN 's attention component"", 'to', 'inner product based attention'], ['results', 'revealed', 'error rate'], ['error rate', 'increased to', '11.3 %']]","[['results', 'has', 'error rate']]","[['Model', 'converted', ""RMN 's attention component""]]",[],natural_language_inference,12,160
2238,model,The number of unnecessary object pairs created by the RN not only increases the processing time but also decreases the accuracy .,"[('created by', (6, 8)), ('decreases', (18, 19))]","[('number of unnecessary object pairs', (1, 6)), ('RN', (9, 10)), ('increases', (12, 13)), ('processing time', (14, 16)), ('accuracy', (20, 21))]","[['number of unnecessary object pairs', 'created by', 'RN'], ['number of unnecessary object pairs', 'decreases', 'accuracy']]","[['number of unnecessary object pairs', 'has', 'RN'], ['increases', 'has', 'processing time']]",[],"[['Model', 'has', 'number of unnecessary object pairs']]",natural_language_inference,12,162
2239,experiments,"With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .","[('With', (0, 1)), ('other than', (8, 10)), ('have', (11, 12)), ('significantly improved', (12, 14)), ('except for', (16, 18)), ('compared to', (20, 22))]","[('match type feature', (2, 5)), ('all models', (6, 8)), ('RMN', (10, 11)), ('performance', (15, 16)), ('task 3', (18, 20)), ('plain condition', (23, 25))]","[['all models', 'other than', 'RMN'], ['all models', 'significantly improved', 'performance'], ['performance', 'except for', 'task 3'], ['performance', 'compared to', 'plain condition']]","[['match type feature', 'has', 'all models'], ['all models', 'has', 'RMN']]",[],[],natural_language_inference,12,163
2240,experiments,"Different from other tasks , RMN yields the same error rate 25.1 % with MemN2N and GMe m N2N on the task 3 .","[('yields', (6, 7)), ('with', (13, 14)), ('on', (19, 20))]","[('RMN', (5, 6)), ('same error rate', (8, 11)), ('25.1 %', (11, 13)), ('MemN2N and GMe m N2N', (14, 19)), ('task 3', (21, 23))]","[['RMN', 'yields', 'same error rate'], ['25.1 %', 'with', 'MemN2N and GMe m N2N'], ['MemN2N and GMe m N2N', 'on', 'task 3']]","[['RMN', 'has', 'same error rate'], ['same error rate', 'has', '25.1 %']]",[],[],natural_language_inference,12,171
2241,research-problem,Natural Language Comprehension with the EpiReader,[],"[('Natural Language Comprehension', (0, 3))]",[],[],[],[],natural_language_inference,13,2
2242,research-problem,"We present the EpiReader , a novel model for machine comprehension of text .",[],"[('machine comprehension of text', (9, 13))]",[],[],[],[],natural_language_inference,13,4
2243,research-problem,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",[],"[('Machine comprehension of unstructured , real - world text', (0, 9))]",[],[],[],[],natural_language_inference,13,5
2244,research-problem,"Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text , and evaluate a model 's response to the questions .",[],"[('machine comprehension', (3, 5))]",[],[],[],[],natural_language_inference,13,6
2245,model,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .","[('propose', (1, 2)), ('call', (16, 17))]","[('deep , end - to - end , neural comprehension model', (3, 14)), ('EpiReader', (18, 19))]","[['deep , end - to - end , neural comprehension model', 'call', 'EpiReader']]","[['deep , end - to - end , neural comprehension model', 'name', 'EpiReader']]","[['Model', 'propose', 'deep , end - to - end , neural comprehension model']]",[],natural_language_inference,13,13
2246,model,The EpiReader factors into two components .,"[('factors into', (2, 4))]","[('two components', (4, 6))]",[],[],[],[],natural_language_inference,13,26
2247,model,The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text ; we call this the Extractor .,"[('extracts', (3, 4)), ('based on', (10, 12)), ('of', (15, 16)), ('call', (24, 25))]","[('first component', (1, 3)), ('small set of potential answers', (5, 10)), ('shallow comparison', (13, 15)), ('question with its supporting text', (17, 22)), ('Extractor', (27, 28))]","[['first component', 'extracts', 'small set of potential answers'], ['small set of potential answers', 'based on', 'shallow comparison'], ['shallow comparison', 'of', 'question with its supporting text']]",[],[],"[['Model', 'has', 'first component']]",natural_language_inference,13,27
2248,model,The second component reranks the proposed answers based on deeper semantic comparisons with the text ; we call this the Reasoner .,"[('reranks', (3, 4)), ('based on', (7, 9)), ('with', (12, 13)), ('call', (17, 18))]","[('second component', (1, 3)), ('proposed answers', (5, 7)), ('deeper semantic comparisons', (9, 12)), ('text', (14, 15)), ('Reasoner', (20, 21))]","[['second component', 'reranks', 'proposed answers'], ['proposed answers', 'based on', 'deeper semantic comparisons'], ['deeper semantic comparisons', 'with', 'text'], ['second component', 'call', 'Reasoner']]",[],[],"[['Model', 'has', 'second component']]",natural_language_inference,13,28
2249,model,"The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment ( RTE ) , also known as natural language inference .","[('implemented by', (3, 5)), ('based on', (8, 10)), ('of', (12, 13)), ('known as', (21, 23))]","[('semantic comparisons', (1, 3)), ('Reasoner', (6, 7)), ('concept', (11, 12)), ('recognizing textual entailment ( RTE )', (13, 19)), ('natural language inference', (23, 26))]","[['semantic comparisons', 'implemented by', 'Reasoner'], ['semantic comparisons', 'based on', 'concept'], ['Reasoner', 'based on', 'concept'], ['concept', 'of', 'recognizing textual entailment ( RTE )'], ['recognizing textual entailment ( RTE )', 'known as', 'natural language inference']]",[],[],"[['Model', 'has', 'semantic comparisons']]",natural_language_inference,13,32
2250,model,"Thus , the Extractor serves the important function of filtering a large set of potential answers down to a small , tractable set of likely candidates for more thorough testing .","[('serves', (4, 5)), ('of', (8, 9)), ('filtering', (9, 10)), ('down to', (16, 18)), ('of', (23, 24)), ('for', (26, 27))]","[('Extractor', (3, 4)), ('important function', (6, 8)), ('large set of potential answers', (11, 16)), ('small , tractable set', (19, 23)), ('likely candidates', (24, 26)), ('more thorough testing', (27, 30))]","[['Extractor', 'serves', 'important function'], ['small , tractable set', 'of', 'likely candidates'], ['important function', 'filtering', 'large set of potential answers'], ['large set of potential answers', 'down to', 'small , tractable set'], ['small , tractable set', 'of', 'likely candidates'], ['likely candidates', 'for', 'more thorough testing']]","[['Extractor', 'has', 'important function']]",[],"[['Model', 'has', 'Extractor']]",natural_language_inference,13,34
2251,model,"The Extractor follows the form of a pointer network , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question .","[('follows', (2, 3)), ('of', (5, 6)), ('uses', (11, 12)), ('to indicate', (16, 18)), ('in', (19, 20)), ('potentially answer', (23, 25))]","[('form', (4, 5)), ('pointer network', (7, 9)), ('differentiable attention mechanism', (13, 16)), ('words', (18, 19)), ('text', (21, 22)), ('question', (26, 27))]","[['form', 'of', 'pointer network'], ['differentiable attention mechanism', 'to indicate', 'words'], ['words', 'in', 'text'], ['words', 'potentially answer', 'question']]",[],"[['Model', 'follows', 'form']]",[],natural_language_inference,13,35
2252,model,This approach was used ( on it s own ) for question answering with the Attention Sum Reader .,"[('used ( on it s own ) for', (3, 11)), ('with', (13, 14))]","[('question answering', (11, 13)), ('Attention Sum Reader', (15, 18))]","[['question answering', 'with', 'Attention Sum Reader']]",[],"[['Model', 'used ( on it s own ) for', 'question answering']]",[],natural_language_inference,13,36
2253,model,The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness .,"[('outputs', (2, 3)), ('of', (6, 7)), ('along with', (9, 11)), ('of', (14, 15))]","[('small set', (4, 6)), ('answer candidates', (7, 9)), ('estimated probabilities', (12, 14)), ('correctness', (15, 16))]","[['small set', 'of', 'answer candidates'], ['estimated probabilities', 'of', 'correctness'], ['answer candidates', 'along with', 'estimated probabilities'], ['estimated probabilities', 'of', 'correctness']]",[],"[['Model', 'outputs', 'small set']]",[],natural_language_inference,13,37
2254,model,"The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .","[('forms', (2, 3)), ('by inserting', (4, 6)), ('into', (9, 10)), ('estimates', (14, 15)), ('of', (17, 18)), ('with', (20, 21)), ('in', (23, 24))]","[('Reasoner', (1, 2)), ('hypotheses', (3, 4)), ('candidate answers', (7, 9)), ('question', (11, 12)), ('concordance', (16, 17)), ('each hypothesis', (18, 20)), ('each sentence', (21, 23)), ('supporting text', (25, 27))]","[['Reasoner', 'forms', 'hypotheses'], ['hypotheses', 'by inserting', 'candidate answers'], ['candidate answers', 'into', 'question'], ['Reasoner', 'estimates', 'concordance'], ['concordance', 'of', 'each hypothesis'], ['each hypothesis', 'with', 'each sentence'], ['each sentence', 'in', 'supporting text']]",[],[],"[['Model', 'has', 'Reasoner']]",natural_language_inference,13,38
2255,model,"We use these estimates as a measure of the evidence for a hypothesis , and aggregate evidence overall sentences .","[('use', (1, 2)), ('as', (4, 5)), ('of', (7, 8)), ('for', (10, 11)), ('aggregate', (15, 16)), ('evidence', (16, 17)), ('overall', (17, 18))]","[('these estimates', (2, 4)), ('measure', (6, 7)), ('evidence', (9, 10)), ('hypothesis', (12, 13)), ('sentences', (18, 19))]","[['these estimates', 'as', 'measure'], ['measure', 'of', 'evidence'], ['evidence', 'for', 'hypothesis']]",[],"[['Model', 'use', 'these estimates']]",[],natural_language_inference,13,39
2256,model,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .","[('combine', (5, 6)), ('with', (10, 11)), ('to produce', (16, 18)), ('of', (21, 22))]","[(""Reasoner 's evidence"", (7, 10)), (""Extractor 's probability estimates"", (12, 16)), ('final ranking', (19, 21)), ('answer candidates', (23, 25))]","[[""Reasoner 's evidence"", 'with', ""Extractor 's probability estimates""], [""Extractor 's probability estimates"", 'to produce', 'final ranking'], ['final ranking', 'of', 'answer candidates']]",[],"[['Model', 'combine', ""Reasoner 's evidence""]]",[],natural_language_inference,13,40
2257,experimental-setup,"To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .","[('train', (1, 2)), ('used', (5, 6)), ('with', (9, 10)), ('of', (26, 27))]","[('our model', (2, 4)), ('stochastic gradient descent', (6, 9)), ('ADAM optimizer ( Kingma and Ba , 2014 )', (11, 20)), ('initial learning rate', (23, 26)), ('0.001', (27, 28))]","[['our model', 'used', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'ADAM optimizer ( Kingma and Ba , 2014 )'], ['stochastic gradient descent', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]",[],"[['Experimental setup', 'train', 'our model']]",[],natural_language_inference,13,221
2258,experimental-setup,"The word embeddings were initialized randomly , drawing from the uniform distribution over .","[('drawing from', (7, 9))]","[('word embeddings', (1, 3)), ('initialized', (4, 5)), ('randomly', (5, 6)), ('uniform distribution', (10, 12))]","[['word embeddings', 'drawing from', 'uniform distribution'], ['initialized', 'drawing from', 'uniform distribution'], ['randomly', 'drawing from', 'uniform distribution']]","[['word embeddings', 'has', 'initialized'], ['initialized', 'has', 'randomly']]",[],"[['Experimental setup', 'has', 'word embeddings']]",natural_language_inference,13,222
2259,experimental-setup,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .","[('used', (1, 2)), ('of', (3, 4)), ('with', (10, 11)), ('of', (13, 14))]","[('batches', (2, 3)), ('32 examples', (4, 6)), ('early stopping', (8, 10)), ('patience', (12, 13)), ('2 epochs', (14, 16))]","[['batches', 'of', '32 examples'], ['patience', 'of', '2 epochs'], ['early stopping', 'with', 'patience'], ['patience', 'of', '2 epochs']]",[],"[['Experimental setup', 'used', 'batches']]",[],natural_language_inference,13,223
2260,experimental-setup,Our model was implement in Theano using the Keras framework .,"[('implement in', (3, 5)), ('using', (6, 7))]","[('model', (1, 2)), ('Theano', (5, 6)), ('Keras framework', (8, 10))]","[['model', 'implement in', 'Theano'], ['Theano', 'using', 'Keras framework']]",[],[],"[['Experimental setup', 'has', 'model']]",natural_language_inference,13,224
2261,experimental-setup,"All our models used 2 - regularization at 0.001 , ? = 50 , and ? = 0.04 .","[('used', (3, 4)), ('at', (7, 8))]","[('All our models', (0, 3)), ('regularization', (6, 7)), ('0.001 , ? = 50 , and ? = 0.04', (8, 18))]","[['regularization', 'at', '0.001 , ? = 50 , and ? = 0.04']]",[],[],"[['Experimental setup', 'has', 'All our models']]",natural_language_inference,13,229
2262,results,The EpiReader achieves state - of - the - art performance across the board for both datasets .,"[('achieves', (2, 3)), ('across', (11, 12)), ('for', (14, 15))]","[('EpiReader', (1, 2)), ('state - of - the - art performance', (3, 11)), ('board', (13, 14)), ('both datasets', (15, 17))]","[['EpiReader', 'achieves', 'state - of - the - art performance'], ['state - of - the - art performance', 'across', 'board'], ['state - of - the - art performance', 'for', 'both datasets'], ['board', 'for', 'both datasets']]",[],[],"[['Results', 'has', 'EpiReader']]",natural_language_inference,13,236
2263,results,"On CNN , we score 2.2 % higher on test than the best previous model of .","[('On', (0, 1)), ('score', (4, 5)), ('on', (8, 9))]","[('CNN', (1, 2)), ('2.2 % higher', (5, 8)), ('test', (9, 10))]","[['2.2 % higher', 'On', 'test'], ['CNN', 'score', '2.2 % higher'], ['2.2 % higher', 'on', 'test']]",[],[],[],natural_language_inference,13,237
2264,results,On CBT - CN our single model scores 4.0 % higher than the previous best of the AS Reader .,"[('scores', (7, 8)), ('than', (11, 12)), ('of', (15, 16))]","[('CBT - CN', (1, 4)), ('our single model', (4, 7)), ('4.0 % higher', (8, 11)), ('previous best', (13, 15)), ('AS Reader', (17, 19))]","[['CBT - CN', 'scores', '4.0 % higher'], ['our single model', 'scores', '4.0 % higher'], ['4.0 % higher', 'than', 'previous best'], ['previous best', 'of', 'AS Reader']]","[['CBT - CN', 'has', 'our single model']]",[],[],natural_language_inference,13,242
2265,results,The improvement on CBT - NE is more modest at 1.1 % .,"[('on', (2, 3)), ('at', (9, 10))]","[('improvement', (1, 2)), ('CBT - NE', (3, 6)), ('more modest', (7, 9)), ('1.1 %', (10, 12))]","[['improvement', 'on', 'CBT - NE'], ['more modest', 'at', '1.1 %']]",[],[],[],natural_language_inference,13,243
2266,results,"Looking more closely at our CBT - NE results , we found that the validation and test accuracies had relatively high variance even in late epochs of training .","[('Looking', (0, 1)), ('at', (3, 4)), ('found that', (11, 13)), ('in', (23, 24)), ('of', (26, 27))]","[('more closely', (1, 3)), ('our CBT - NE results', (4, 9)), ('validation and test accuracies', (14, 18)), ('relatively high variance', (19, 22)), ('late epochs', (24, 26)), ('training', (27, 28))]","[['more closely', 'at', 'our CBT - NE results'], ['our CBT - NE results', 'found that', 'validation and test accuracies'], ['relatively high variance', 'in', 'late epochs'], ['late epochs', 'of', 'training']]","[['more closely', 'has', 'our CBT - NE results'], ['our CBT - NE results', 'has', 'validation and test accuracies'], ['validation and test accuracies', 'has', 'relatively high variance']]","[['Results', 'Looking', 'more closely']]",[],natural_language_inference,13,244
2267,research-problem,End - To - End Memory Networks,[],"[('Memory Networks', (5, 7))]",[],[],[],[],natural_language_inference,14,2
2268,research-problem,"In this work , we present a novel recurrent neural network ( RNN ) architecture where the recurrence reads from a possibly large external memory multiple times before outputting a symbol .","[('present', (5, 6)), ('reads from', (18, 20)), ('before outputting', (27, 29))]","[('novel recurrent neural network ( RNN ) architecture', (7, 15)), ('recurrence', (17, 18)), ('possibly large external memory', (21, 25)), ('multiple times', (25, 27)), ('symbol', (30, 31))]","[['recurrence', 'reads from', 'possibly large external memory'], ['possibly large external memory', 'before outputting', 'symbol'], ['multiple times', 'before outputting', 'symbol']]","[['novel recurrent neural network ( RNN ) architecture', 'has', 'recurrence'], ['possibly large external memory', 'has', 'multiple times']]","[['Research problem', 'present', 'novel recurrent neural network ( RNN ) architecture']]",[],natural_language_inference,14,14
2269,model,Our model can be considered a continuous form of the Memory Network implemented in [ 23 ] .,"[('considered', (4, 5)), ('of', (8, 9))]","[('continuous form', (6, 8)), ('Memory Network', (10, 12))]","[['continuous form', 'of', 'Memory Network']]",[],"[['Model', 'considered', 'continuous form']]",[],natural_language_inference,14,15
2270,research-problem,"The continuity of the model we present here means that it can be trained end - to - end from input - output pairs , and so is applicable to more tasks , i.e. tasks where such supervision is not available , such as in language modeling or realistically supervised question answering tasks .","[('trained', (13, 14)), ('from', (19, 20))]","[('continuity of the model', (1, 5)), ('end - to - end', (14, 19)), ('input - output pairs', (20, 24))]","[['continuity of the model', 'trained', 'end - to - end'], ['end - to - end', 'from', 'input - output pairs']]",[],[],[],natural_language_inference,14,17
2271,model,"Our model can also be seen as a version of RNNsearch [ 2 ] with multiple computational steps ( which we term "" hops "" ) per output symbol .","[('seen as', (5, 7)), ('with', (14, 15)), ('per', (26, 27))]","[('version of RNNsearch', (8, 11)), ('multiple computational steps', (15, 18)), ('output symbol', (27, 29))]","[['version of RNNsearch', 'with', 'multiple computational steps'], ['multiple computational steps', 'per', 'output symbol']]",[],"[['Model', 'seen as', 'version of RNNsearch']]",[],natural_language_inference,14,18
2272,hyperparameters,"For each mini-batch update , the 2 norm of the whole gradient of all parameters is measured 5 and if larger than L = 50 , then it is scaled down to have norm L.","[('For', (0, 1)), ('of', (8, 9)), ('of', (12, 13))]","[('each mini-batch update', (1, 4)), ('2 norm', (6, 8)), ('whole gradient', (10, 12)), ('all parameters', (13, 15)), ('measured', (16, 17)), ('5', (17, 18))]","[['2 norm', 'of', 'whole gradient'], ['whole gradient', 'of', 'all parameters'], ['whole gradient', 'of', 'all parameters']]","[['each mini-batch update', 'has', '2 norm'], ['measured', 'has', '5']]","[['Hyperparameters', 'For', 'each mini-batch update']]",[],natural_language_inference,14,133
2273,hyperparameters,"We use the learning rate annealing schedule from , namely , if the validation cost has not decreased after one epoch , then the learning rate is scaled down by a factor 1.5 .","[('use', (1, 2)), ('if', (11, 12)), ('after', (18, 19)), ('by', (29, 30))]","[('learning rate annealing schedule', (3, 7)), ('validation cost', (13, 15)), ('not decreased', (16, 18)), ('one epoch', (19, 21)), ('learning rate', (24, 26)), ('scaled down', (27, 29)), ('factor 1.5', (31, 33))]","[['learning rate annealing schedule', 'if', 'validation cost'], ['not decreased', 'after', 'one epoch'], ['scaled down', 'by', 'factor 1.5']]","[['learning rate annealing schedule', 'has', 'validation cost'], ['validation cost', 'has', 'not decreased'], ['learning rate', 'has', 'scaled down']]","[['Hyperparameters', 'use', 'learning rate annealing schedule']]",[],natural_language_inference,14,135
2274,hyperparameters,"Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .","[('initialized using', (2, 4)), ('set to', (14, 16))]","[('Weights', (0, 1)), ('N ( 0 , 0.05 )', (4, 10)), ('batch size', (11, 13)), ('128', (16, 17))]","[['Weights', 'initialized using', 'N ( 0 , 0.05 )'], ['batch size', 'set to', '128']]","[['batch size', 'has', '128']]",[],"[['Hyperparameters', 'has', 'Weights']]",natural_language_inference,14,137
2275,hyperparameters,"On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .","[('On', (0, 1)), ('repeat', (7, 8)), ('with', (12, 13)), ('pick', (17, 18)), ('with', (20, 21))]","[('Penn tree dataset', (2, 5)), ('each training', (8, 10)), ('10 times', (10, 12)), ('different random initializations', (13, 16)), ('one', (19, 20)), ('smallest validation cost', (21, 24))]","[['Penn tree dataset', 'repeat', 'each training'], ['10 times', 'with', 'different random initializations'], ['one', 'with', 'smallest validation cost'], ['one', 'with', 'smallest validation cost']]","[['each training', 'has', '10 times']]","[['Hyperparameters', 'On', 'Penn tree dataset']]",[],natural_language_inference,14,138
2276,baselines,"MemNN : The strongly supervised AM + NG + NL Memory Networks approach , proposed in .",[],"[('MemNN', (0, 1)), ('strongly supervised AM + NG + NL Memory Networks approach', (3, 13))]",[],"[['MemNN', 'has', 'strongly supervised AM + NG + NL Memory Networks approach']]",[],"[['Baselines', 'has', 'MemNN']]",natural_language_inference,14,153
2277,baselines,MemNN- WSH :,[],"[('MemNN- WSH', (0, 2))]",[],[],[],"[['Baselines', 'has', 'MemNN- WSH']]",natural_language_inference,14,157
2278,baselines,A weakly supervised heuristic version of MemNN where the supporting sentence labels are not used in training .,"[('of', (5, 6))]","[('weakly supervised heuristic version', (1, 5)), ('MemNN', (6, 7))]","[['weakly supervised heuristic version', 'of', 'MemNN']]","[['weakly supervised heuristic version', 'has', 'MemNN']]",[],"[['Baselines', 'has', 'weakly supervised heuristic version']]",natural_language_inference,14,158
2279,baselines,"LSTM : A standard LSTM model , trained using question / answer pairs only ( i.e. also weakly supervised ) .","[('trained using', (7, 9))]","[('LSTM', (0, 1)), ('standard LSTM model', (3, 6)), ('question / answer pairs only', (9, 14))]","[['LSTM', 'trained using', 'question / answer pairs only'], ['standard LSTM model', 'trained using', 'question / answer pairs only']]","[['LSTM', 'has', 'standard LSTM model']]",[],"[['Baselines', 'has', 'LSTM']]",natural_language_inference,14,161
2280,research-problem,Neural Natural Language Inference Models Enhanced with External Knowledge,[],"[('Neural Natural Language Inference', (0, 4))]",[],[],[],[],natural_language_inference,15,2
2281,research-problem,Modeling natural language inference is a very challenging task .,[],"[('natural language inference', (1, 4))]",[],[],[],[],natural_language_inference,15,4
2282,research-problem,"With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance .",[],"[('neural - network - based inference', (19, 25))]",[],[],[],[],natural_language_inference,15,5
2283,research-problem,"Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?",[],"[('natural language inference ( NLI )', (16, 22))]",[],[],[],[],natural_language_inference,15,6
2284,research-problem,"If not , how can neural - network - based NLI models benefit from external knowledge and how to build NLI models to leverage it ?",[],"[('NLI', (10, 11))]",[],[],[],[],natural_language_inference,15,7
2285,research-problem,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",[],"[('recognizing textual entailment ( RTE )', (10, 16))]",[],[],[],[],natural_language_inference,15,12
2286,model,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .","[('enrich', (4, 5)), ('with', (12, 13)), ('in', (15, 16))]","[('neural - network - based NLI models', (5, 12)), ('external knowledge', (13, 15)), ('coattention', (16, 17)), ('local inference collection', (18, 21)), ('inference composition components', (23, 26))]","[['neural - network - based NLI models', 'with', 'external knowledge'], ['external knowledge', 'in', 'coattention'], ['external knowledge', 'in', 'local inference collection'], ['external knowledge', 'in', 'inference composition components']]",[],"[['Model', 'enrich', 'neural - network - based NLI models']]",[],natural_language_inference,15,23
2287,experiments,We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .,"[('show', (1, 2)), ('improves', (5, 6)), ('to achieve', (16, 18)), ('on', (20, 21))]","[('proposed model', (3, 5)), ('state - of - the - art NLI models', (7, 16)), ('better performances', (18, 20)), ('SNLI and MultiNLI datasets', (22, 26))]","[['proposed model', 'improves', 'state - of - the - art NLI models'], ['state - of - the - art NLI models', 'to achieve', 'better performances'], ['better performances', 'on', 'SNLI and MultiNLI datasets']]",[],[],[],natural_language_inference,15,24
2288,experiments,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .","[('of', (2, 3)), ('using', (3, 4)), ('is', (6, 7)), ('when', (9, 10))]","[('advantage', (1, 2)), ('external knowledge', (4, 6)), ('more significant', (7, 9)), ('size', (11, 12)), ('training data', (13, 15)), ('restricted', (16, 17))]","[['size', 'of', 'training data'], ['advantage', 'using', 'external knowledge'], ['external knowledge', 'is', 'more significant'], ['training data', 'is', 'restricted'], ['external knowledge', 'when', 'size'], ['more significant', 'when', 'size']]",[],[],[],natural_language_inference,15,25
2289,hyperparameters,The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,"[('are', (4, 5)), ('of', (10, 11)), ('of', (14, 15))]","[('dimension', (9, 10)), ('hidden states', (12, 14)), ('LSTMs and word embeddings', (15, 19)), ('300', (20, 21))]","[['hidden states', 'are', '300'], ['LSTMs and word embeddings', 'are', '300'], ['dimension', 'of', 'hidden states'], ['hidden states', 'of', 'LSTMs and word embeddings'], ['hidden states', 'of', 'LSTMs and word embeddings']]",[],"[['Hyperparameters', 'are', 'dimension']]",[],natural_language_inference,15,160
2290,hyperparameters,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .","[('initialized by', (4, 6))]","[('word embeddings', (1, 3)), ('300D GloVe 840B', (6, 9)), ('out - of - vocabulary words', (11, 17)), ('initialized', (20, 21)), ('randomly', (21, 22))]","[['word embeddings', 'initialized by', '300D GloVe 840B']]","[['initialized', 'has', 'randomly']]",[],"[['Hyperparameters', 'has', 'word embeddings']]",natural_language_inference,15,161
2291,hyperparameters,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .","[('used for', (9, 11)), ('with', (12, 13)), ('of', (17, 18))]","[('Adam ( Kingma and Ba , 2014 )', (0, 8)), ('optimization', (11, 12)), ('initial learning rate', (14, 17)), ('0.0004', (18, 19))]","[['Adam ( Kingma and Ba , 2014 )', 'used for', 'optimization'], ['Adam ( Kingma and Ba , 2014 )', 'with', 'initial learning rate'], ['optimization', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.0004']]",[],[],"[['Hyperparameters', 'has', 'Adam ( Kingma and Ba , 2014 )']]",natural_language_inference,15,163
2292,hyperparameters,The mini - batch size is set to 32 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('32', (8, 9))]","[['mini - batch size', 'set to', '32']]","[['mini - batch size', 'has', '32']]",[],"[['Hyperparameters', 'has', 'mini - batch size']]",natural_language_inference,15,164
2293,results,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .","[('namely', (4, 5)), ('enriches', (15, 16)), ('with', (17, 18)), ('obtains', (21, 22)), ('of', (24, 25))]","[('proposed model', (1, 3)), ('Knowledge - based Inference Model ( KIM )', (5, 13)), ('ESIM', (16, 17)), ('external knowledge', (18, 20)), ('accuracy', (23, 24)), ('88.6 %', (25, 27))]","[['proposed model', 'namely', 'Knowledge - based Inference Model ( KIM )'], ['Knowledge - based Inference Model ( KIM )', 'enriches', 'ESIM'], ['ESIM', 'with', 'external knowledge'], ['accuracy', 'of', '88.6 %']]","[['proposed model', 'name', 'Knowledge - based Inference Model ( KIM )']]",[],"[['Results', 'has', 'proposed model']]",natural_language_inference,15,170
2294,results,The difference between ESIM and KIM is statistically significant under the one - tailed paired t- test at the 99 % significance level .,"[('between', (2, 3)), ('under', (9, 10)), ('at', (17, 18))]","[('difference', (1, 2)), ('ESIM and KIM', (3, 6)), ('statistically significant', (7, 9)), ('one - tailed paired t- test', (11, 17)), ('99 % significance level', (19, 23))]","[['difference', 'between', 'ESIM and KIM'], ['statistically significant', 'under', 'one - tailed paired t- test'], ['one - tailed paired t- test', 'at', '99 % significance level']]","[['difference', 'has', 'ESIM and KIM']]",[],"[['Results', 'has', 'difference']]",natural_language_inference,15,171
2295,results,shows the performance of models on the MultiNLI dataset .,"[('on', (5, 6))]","[('MultiNLI dataset', (7, 9))]",[],[],"[['Results', 'on', 'MultiNLI dataset']]",[],natural_language_inference,15,177
2296,results,"The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .","[('achieves', (3, 4)), ('on', (9, 10))]","[('baseline ESIM', (1, 3)), ('76.8 % and 75.8 %', (4, 9)), ('in - domain and cross - domain test set', (10, 19))]","[['baseline ESIM', 'achieves', '76.8 % and 75.8 %'], ['76.8 % and 75.8 %', 'on', 'in - domain and cross - domain test set']]",[],[],"[['Results', 'has', 'baseline ESIM']]",natural_language_inference,15,178
2297,results,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .","[('extend', (2, 3)), ('with', (5, 6)), ('achieve', (10, 11)), ('to', (13, 14))]","[('ESIM', (4, 5)), ('external knowledge', (6, 8)), ('significant gains', (11, 13)), ('77.2 % and 76.4 %', (14, 19))]","[['ESIM', 'with', 'external knowledge'], ['ESIM', 'achieve', 'significant gains'], ['external knowledge', 'achieve', 'significant gains'], ['significant gains', 'to', '77.2 % and 76.4 %']]",[],"[['Results', 'extend', 'ESIM']]",[],natural_language_inference,15,179
2298,research-problem,Text Understanding with the Attention Sum Reader Network,[],"[('Text Understanding', (0, 2))]",[],[],[],[],natural_language_inference,16,2
2299,research-problem,"Thanks to the size of these datasets , the associated text comprehension task is well suited for deep - learning techniques that currently seem to outperform all alternative approaches .",[],"[('text comprehension', (10, 12))]",[],[],[],[],natural_language_inference,16,5
2300,model,Our model called the Attention Sum Reader ( AS Reader ) 4 is tailor - made to leverage the fact that the answer is a word from the context document .,"[('called', (2, 3)), ('is', (12, 13)), ('tailor - made', (13, 16)), ('to leverage', (16, 18)), ('that', (20, 21)), ('from', (26, 27))]","[('Attention Sum Reader ( AS Reader )', (4, 11)), ('fact', (19, 20)), ('answer', (22, 23)), ('word', (25, 26)), ('context document', (28, 30))]","[['answer', 'is', 'word'], ['fact', 'that', 'answer'], ['word', 'from', 'context document']]","[['fact', 'has', 'answer'], ['answer', 'has', 'word']]","[['Model', 'called', 'Attention Sum Reader ( AS Reader )']]",[],natural_language_inference,16,65
2301,model,We compute a vector embedding of the query .,"[('compute', (1, 2))]","[('vector embedding', (3, 5))]",[],[],"[['Model', 'compute', 'vector embedding']]",[],natural_language_inference,16,70
2302,model,We compute a vector embedding of each individual word in the context of the whole document ( contextual embedding ) .,"[('of', (5, 6)), ('in', (9, 10))]","[('each individual word', (6, 9)), ('context', (11, 12)), ('whole document ( contextual embedding )', (14, 20))]","[['context', 'of', 'whole document ( contextual embedding )'], ['each individual word', 'in', 'context']]",[],"[['Model', 'of', 'each individual word']]",[],natural_language_inference,16,72
2303,model,"Using a dot product between the question embedding and the contextual embedding of each occurrence of a candidate answer in the document , we select the most likely answer .","[('Using', (0, 1)), ('between', (4, 5)), ('of', (12, 13)), ('of', (15, 16)), ('in', (19, 20))]","[('dot product', (2, 4)), ('question embedding and the contextual embedding', (6, 12)), ('each occurrence', (13, 15)), ('candidate answer', (17, 19)), ('document', (21, 22))]","[['dot product', 'between', 'question embedding and the contextual embedding'], ['question embedding and the contextual embedding', 'of', 'each occurrence'], ['each occurrence', 'of', 'candidate answer'], ['each occurrence', 'of', 'candidate answer'], ['candidate answer', 'in', 'document']]","[['dot product', 'has', 'question embedding and the contextual embedding']]","[['Model', 'Using', 'dot product']]",[],natural_language_inference,16,74
2304,hyperparameters,To train the model we used stochastic gradient descent with the ADAM update rule and learning rate of 0.001 or 0.0005 .,"[('train', (1, 2)), ('used', (5, 6)), ('with', (9, 10)), ('of', (17, 18))]","[('model', (3, 4)), ('stochastic gradient descent', (6, 9)), ('ADAM update rule', (11, 14)), ('learning rate', (15, 17)), ('0.001 or 0.0005', (18, 21))]","[['model', 'used', 'stochastic gradient descent'], ['stochastic gradient descent', 'with', 'ADAM update rule'], ['stochastic gradient descent', 'with', 'learning rate'], ['learning rate', 'of', '0.001 or 0.0005']]",[],"[['Hyperparameters', 'train', 'model']]",[],natural_language_inference,16,172
2305,hyperparameters,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,"[('in', (1, 2)), ('initialized by', (6, 8)), ('initialized to', (14, 16))]","[('Weights', (0, 1)), ('GRU networks', (3, 5)), ('random orthogonal matrices and biases', (8, 13)), ('zero', (16, 17))]","[['Weights', 'in', 'GRU networks'], ['Weights', 'initialized by', 'random orthogonal matrices and biases'], ['GRU networks', 'initialized by', 'random orthogonal matrices and biases'], ['random orthogonal matrices and biases', 'initialized to', 'zero']]",[],[],"[['Hyperparameters', 'has', 'Weights']]",natural_language_inference,16,178
2306,hyperparameters,We also used a gradient clipping threshold of 10 and batches of size 32 .,"[('used', (2, 3)), ('of', (7, 8)), ('of', (11, 12))]","[('gradient clipping threshold', (4, 7)), ('10', (8, 9)), ('batches', (10, 11)), ('size 32', (12, 14))]","[['gradient clipping threshold', 'of', '10'], ['batches', 'of', 'size 32'], ['batches', 'of', 'size 32']]","[['gradient clipping threshold', 'has', '10']]","[['Hyperparameters', 'used', 'gradient clipping threshold']]",[],natural_language_inference,16,179
2307,results,mance of our single model is a little bit worse than performance of simultaneously published models .,"[('of', (1, 2)), ('is', (5, 6)), ('than', (10, 11))]","[('single model', (3, 5)), ('little bit worse', (7, 10)), ('performance', (11, 12)), ('simultaneously published models', (13, 16))]","[['performance', 'of', 'simultaneously published models'], ['single model', 'is', 'little bit worse'], ['little bit worse', 'than', 'performance']]",[],[],[],natural_language_inference,16,212
2308,results,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .","[('of', (3, 4)), ('outperforms', (6, 7)), ('use', (12, 13))]","[('ensemble', (2, 3)), ('our models', (4, 6)), ('these models', (7, 9)), ('pre-trained word embeddings', (13, 16))]","[['ensemble', 'of', 'our models'], ['our models', 'outperforms', 'these models'], ['these models', 'use', 'pre-trained word embeddings']]","[['ensemble', 'has', 'our models']]",[],"[['Results', 'has', 'ensemble']]",natural_language_inference,16,214
2309,results,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,"[('On the CNN dataset', (0, 4)), ('with', (7, 8)), ('achieves', (11, 12)), ('of', (15, 16))]","[('our single model', (4, 7)), ('best validation accuracy', (8, 11)), ('test accuracy', (13, 15)), ('69.5 %', (16, 18))]","[['our single model', 'with', 'best validation accuracy'], ['best validation accuracy', 'achieves', 'test accuracy'], ['test accuracy', 'of', '69.5 %']]",[],[],[],natural_language_inference,16,215
2310,results,The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,"[('of', (3, 4)), ('than', (22, 23))]","[('average performance', (1, 3)), ('top 20 % models', (5, 9)), ('69.9 %', (14, 16)), ('even 0.5 % better', (18, 22)), ('single best - validation model', (24, 29))]","[['average performance', 'of', 'top 20 % models'], ['even 0.5 % better', 'than', 'single best - validation model']]",[],[],"[['Results', 'has', 'average performance']]",natural_language_inference,16,216
2311,results,Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets ..,"[('Fusing', (0, 1)), ('gives', (4, 5)), ('in', (9, 10)), ('on', (11, 12))]","[('multiple models', (1, 3)), ('significant further increase', (6, 9)), ('accuracy', (10, 11)), ('CNN and Daily Mail datasets', (13, 18))]","[['multiple models', 'gives', 'significant further increase'], ['significant further increase', 'in', 'accuracy'], ['accuracy', 'on', 'CNN and Daily Mail datasets']]",[],"[['Results', 'Fusing', 'multiple models']]",[],natural_language_inference,16,218
2312,results,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .","[('In', (0, 1)), ('with', (8, 9)), ('of', (10, 11)), ('performs', (13, 14)), ('than', (18, 19)), ('with', (21, 22)), ('performs', (28, 29)), ('than', (33, 34))]","[('named entity prediction', (1, 4)), ('our best single model', (4, 8)), ('accuracy', (9, 10)), ('68.6 %', (11, 13)), ('2 % absolute', (14, 17)), ('better', (17, 18)), ('MemNN', (20, 21)), ('self supervision', (22, 24)), ('averaging ensemble', (26, 28)), ('4 % absolute', (29, 32)), ('better', (32, 33)), ('best previous result', (35, 38))]","[['our best single model', 'with', 'accuracy'], ['MemNN', 'with', 'self supervision'], ['accuracy', 'of', '68.6 %'], ['68.6 %', 'performs', '2 % absolute'], ['better', 'than', 'MemNN'], ['MemNN', 'with', 'self supervision'], ['averaging ensemble', 'performs', '4 % absolute'], ['better', 'than', 'best previous result']]","[['named entity prediction', 'has', 'our best single model'], ['2 % absolute', 'has', 'better'], ['4 % absolute', 'has', 'better']]","[['Results', 'In', 'named entity prediction']]",[],natural_language_inference,16,220
2313,results,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,"[('than', (12, 13)), ('improves', (18, 19)), ('to', (21, 22)), ('than', (30, 31))]","[('common noun prediction', (1, 4)), ('our single models', (4, 7)), ('0.4 % absolute', (8, 11)), ('better', (11, 12)), ('Mem NN', (13, 15)), ('ensemble', (17, 18)), ('performance', (20, 21)), ('69 %', (22, 24)), ('6 % absolute', (26, 29)), ('better', (29, 30)), ('MemNN', (31, 32))]","[['better', 'than', 'Mem NN'], ['ensemble', 'improves', 'performance'], ['performance', 'to', '69 %'], ['better', 'than', 'MemNN']]","[['common noun prediction', 'has', 'our single models'], ['0.4 % absolute', 'has', 'better'], ['6 % absolute', 'has', 'better']]",[],[],natural_language_inference,16,221
2314,research-problem,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING,[],"[('NATURAL LANGUAGE UNDERSTAND - ING', (11, 16))]",[],[],[],[],natural_language_inference,17,2
2315,research-problem,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .",[],"[('natural language understanding ( NLU )', (1, 7))]",[],[],[],[],natural_language_inference,17,4
2316,research-problem,"However , the low absolute performance of our best model indicates the need for improved general NLU systems .",[],"[('NLU', (16, 17))]",[],[],[],[],natural_language_inference,17,9
2317,research-problem,"To facilitate research in this direction , we present the General Language Understanding Evaluation ( GLUE ) benchmark : a collection of NLU tasks including question answering , sentiment analysis , and textual entailment , and an associated online platform for model evaluation , comparison , and analysis .","[('present', (8, 9)), ('of', (21, 22)), ('including', (24, 25))]","[('General Language Understanding Evaluation ( GLUE ) benchmark', (10, 18)), ('collection', (20, 21)), ('NLU tasks', (22, 24)), ('question answering', (25, 27)), ('sentiment analysis', (28, 30)), ('textual entailment', (32, 34))]","[['collection', 'of', 'NLU tasks'], ['NLU tasks', 'including', 'question answering'], ['NLU tasks', 'including', 'sentiment analysis'], ['NLU tasks', 'including', 'textual entailment']]","[['General Language Understanding Evaluation ( GLUE ) benchmark', 'has', 'collection']]","[['Research problem', 'present', 'General Language Understanding Evaluation ( GLUE ) benchmark']]",[],natural_language_inference,17,15
2318,model,GLUE does not place any constraints on model architecture beyond the ability to process single - sentence and sentence - pair inputs and to make corresponding predictions .,"[('not place', (2, 4)), ('on', (6, 7)), ('beyond', (9, 10)), ('to process', (12, 14)), ('to make', (23, 25))]","[('GLUE', (0, 1)), ('any constraints', (4, 6)), ('model architecture', (7, 9)), ('ability', (11, 12)), ('single - sentence and sentence - pair inputs', (14, 22)), ('corresponding predictions', (25, 27))]","[['GLUE', 'not place', 'any constraints'], ['any constraints', 'on', 'model architecture'], ['model architecture', 'beyond', 'ability'], ['ability', 'to process', 'single - sentence and sentence - pair inputs'], ['GLUE', 'to make', 'corresponding predictions']]",[],[],"[['Model', 'has', 'GLUE']]",natural_language_inference,17,16
2319,research-problem,"For some GLUE tasks , training data is plentiful , but for others it is limited or fails to match the genre of the test set .","[('For', (0, 1)), ('fails to match', (17, 20)), ('of', (22, 23))]","[('some GLUE tasks', (1, 4)), ('training data', (5, 7)), ('plentiful', (8, 9)), ('others', (12, 13)), ('limited', (15, 16)), ('genre', (21, 22)), ('test set', (24, 26))]","[['training data', 'fails to match', 'genre'], ['genre', 'of', 'test set']]","[['some GLUE tasks', 'has', 'training data'], ['others', 'has', 'limited']]","[['Research problem', 'For', 'some GLUE tasks']]",[],natural_language_inference,17,17
2320,experiments,"Four of the datasets feature privately - held test data , which will be used to ensure that the benchmark is used fairly .","[('of', (1, 2)), ('used', (14, 15)), ('to ensure', (15, 17))]","[('Four', (0, 1)), ('datasets', (3, 4)), ('feature', (4, 5)), ('privately - held test data', (5, 10)), ('benchmark', (19, 20)), ('fairly', (22, 23))]","[['Four', 'of', 'datasets'], ['benchmark', 'used', 'fairly'], ['privately - held test data', 'to ensure', 'benchmark']]","[['Four', 'has', 'datasets'], ['datasets', 'has', 'feature'], ['feature', 'has', 'privately - held test data']]",[],[],natural_language_inference,17,20
2321,code,Original code for the baselines is available at https://github.com/nyu-mll/GLUE-baselines and a newer version is available at https://github.com/jsalt18-sentence-repl/jiant .,[],"[('https://github.com/nyu-mll/GLUE-baselines', (8, 9)), ('https://github.com/jsalt18-sentence-repl/jiant', (16, 17))]",[],[],[],[],natural_language_inference,17,157
2322,model,"Our simplest baseline architecture is based on sentence - to - vector encoders , and sets aside GLUE 's ability to evaluate models with more complex structures .","[('based on', (5, 7))]","[('Our simplest baseline architecture', (0, 4)), ('sentence - to - vector encoders', (7, 13))]","[['Our simplest baseline architecture', 'based on', 'sentence - to - vector encoders']]",[],[],"[['Model', 'has', 'Our simplest baseline architecture']]",natural_language_inference,17,159
2323,experiments,We find that multi-task training yields better overall scores over single - task training amongst models using attention or ELMo .,"[('find that', (1, 3)), ('yields', (5, 6)), ('over', (9, 10)), ('amongst', (14, 15)), ('using', (16, 17))]","[('multi-task training', (3, 5)), ('better overall scores', (6, 9)), ('single - task training', (10, 14)), ('models', (15, 16)), ('attention or ELMo', (17, 20))]","[['multi-task training', 'yields', 'better overall scores'], ['better overall scores', 'over', 'single - task training'], ['single - task training', 'amongst', 'models'], ['models', 'using', 'attention or ELMo']]",[],[],[],natural_language_inference,17,183
2324,experiments,"We see a consistent improvement in using ELMo embeddings in place of GloVe or CoVe embeddings , particularly for single - sentence tasks .","[('see', (1, 2)), ('in using', (5, 7)), ('in place of', (9, 12)), ('particularly for', (17, 19))]","[('consistent improvement', (3, 5)), ('ELMo embeddings', (7, 9)), ('GloVe or CoVe embeddings', (12, 16)), ('single - sentence tasks', (19, 23))]","[['consistent improvement', 'in using', 'ELMo embeddings'], ['ELMo embeddings', 'in place of', 'GloVe or CoVe embeddings'], ['ELMo embeddings', 'particularly for', 'single - sentence tasks'], ['GloVe or CoVe embeddings', 'particularly for', 'single - sentence tasks']]",[],[],[],natural_language_inference,17,185
2325,experiments,"Among the pre-trained sentence representation models , we observe fairly consistent gains moving from CBoW to Skip - Thought to Infersent and GenSen .","[('Among', (0, 1)), ('observe', (8, 9)), ('moving from', (12, 14))]","[('pre-trained sentence representation models', (2, 6)), ('fairly consistent gains', (9, 12)), ('CBoW to Skip - Thought to Infersent and GenSen', (14, 23))]","[['pre-trained sentence representation models', 'observe', 'fairly consistent gains'], ['fairly consistent gains', 'moving from', 'CBoW to Skip - Thought to Infersent and GenSen']]",[],[],[],natural_language_inference,17,187
2326,experiments,"Relative to the models trained directly on the GLUE tasks , InferSent is competitive and GenSen outperforms all but the two best .","[('Relative to', (0, 2)), ('trained directly on', (4, 7))]","[('models', (3, 4)), ('GLUE tasks', (8, 10)), ('InferSent', (11, 12)), ('competitive', (13, 14)), ('GenSen', (15, 16)), ('outperforms', (16, 17)), ('all but the two best', (17, 22))]","[['models', 'trained directly on', 'GLUE tasks']]","[['GenSen', 'has', 'outperforms'], ['outperforms', 'has', 'all but the two best']]",[],[],natural_language_inference,17,188
2327,experiments,"Looking at results per task , we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task .","[('on', (15, 16)), ('compared to', (17, 19)), ('trained on', (22, 24))]","[('task', (4, 5)), ('sentence representation models', (10, 13)), ('substantially underperform', (13, 15)), ('CoLA', (16, 17)), ('models', (20, 21))]","[['substantially underperform', 'on', 'CoLA'], ['substantially underperform', 'compared to', 'models'], ['CoLA', 'compared to', 'models']]","[['task', 'has', 'sentence representation models'], ['sentence representation models', 'has', 'substantially underperform']]",[],[],natural_language_inference,17,189
2328,experiments,"On the other hand , for STS - B , models trained directly on the task lag significantly behind the performance of the best sentence representation model .","[('for', (5, 6)), ('trained directly on', (11, 14)), ('significantly behind', (17, 19)), ('of', (21, 22))]","[('STS - B', (6, 9)), ('models', (10, 11)), ('task', (15, 16)), ('lag', (16, 17)), ('performance', (20, 21)), ('best sentence representation model', (23, 27))]","[['models', 'trained directly on', 'task'], ['models', 'trained directly on', 'lag'], ['lag', 'significantly behind', 'performance'], ['performance', 'of', 'best sentence representation model']]","[['STS - B', 'has', 'models'], ['task', 'has', 'lag']]",[],[],natural_language_inference,17,190
2329,experiments,"On WNLI , no model exceeds most - frequent - class guessing ( 65.1 % ) and we substitute the model predictions for the most -frequent baseline .","[('On', (0, 1)), ('exceeds', (5, 6))]","[('WNLI', (1, 2)), ('no model', (3, 5)), ('most - frequent - class guessing ( 65.1 % )', (6, 16))]","[['no model', 'exceeds', 'most - frequent - class guessing ( 65.1 % )']]","[['WNLI', 'has', 'no model']]",[],[],natural_language_inference,17,192
2330,experiments,"On RTE and in aggregate , even our best baselines leave room for improvement .",[],"[('RTE and in aggregate', (1, 5)), ('our best baselines', (7, 10)), ('room for improvement', (11, 14))]",[],"[['RTE and in aggregate', 'has', 'our best baselines']]",[],[],natural_language_inference,17,193
2331,research-problem,Parameter Re-Initialization through Cyclical Batch Size Schedules,[],"[('Parameter Re-Initialization', (0, 2))]",[],[],[],[],natural_language_inference,18,2
2332,research-problem,Optimal parameter initialization remains a crucial problem for neural network training .,[],"[('Optimal parameter initialization', (0, 3))]",[],[],[],[],natural_language_inference,18,4
2333,model,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,"[('of', (5, 6)), ('to', (10, 11))]","[('adapting', (6, 7)), ('weight initialization', (8, 10)), ('optimization dynamics', (12, 14)), ('specific learning task at hand', (16, 21))]","[['weight initialization', 'of', 'optimization dynamics'], ['optimization dynamics', 'of', 'specific learning task at hand'], ['weight initialization', 'to', 'optimization dynamics']]","[['adapting', 'has', 'weight initialization']]","[['Model', 'of', 'adapting']]",[],natural_language_inference,18,19
2334,model,"From the Bayesian perspective , improved weight initialization can be viewed as starting with a better prior , which leads to a more accurate posterior and thus better generalization ability .","[('From', (0, 1)), ('leads to', (19, 21))]","[('Bayesian perspective', (2, 4)), ('improved weight initialization', (5, 8)), ('more accurate posterior', (22, 25)), ('better generalization ability', (27, 30))]",[],"[['Bayesian perspective', 'has', 'improved weight initialization']]","[['Model', 'From', 'Bayesian perspective']]",[],natural_language_inference,18,20
2335,model,"For example , in the seminal works , an adaptive prior is implemented via Markov Chain Monte Carlo ( MCMC ) methods .","[('implemented via', (12, 14))]","[('adaptive prior', (9, 11)), ('Markov Chain Monte Carlo ( MCMC ) methods', (14, 22))]","[['adaptive prior', 'implemented via', 'Markov Chain Monte Carlo ( MCMC ) methods']]",[],[],"[['Model', 'has', 'adaptive prior']]",natural_language_inference,18,22
2336,model,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .","[('incorporate', (6, 7)), ('for', (12, 13)), ('use', (26, 27)), ('to control', (31, 33)), ('of', (39, 40))]","[('adaptive initialization', (9, 11)), ('neural network training', (13, 16)), ('cyclical batch size schedules', (27, 31)), ('noise ( or temperature )', (34, 39)), ('SGD', (40, 41))]","[['adaptive initialization', 'for', 'neural network training'], ['adaptive initialization', 'use', 'cyclical batch size schedules'], ['cyclical batch size schedules', 'to control', 'noise ( or temperature )'], ['noise ( or temperature )', 'of', 'SGD']]",[],"[['Model', 'incorporate', 'adaptive initialization']]",[],natural_language_inference,18,23
2337,model,"Here , we build upon this work by studying different cyclical annealing strategies for a wide range of problems .","[('studying', (8, 9)), ('for', (13, 14)), ('of', (17, 18))]","[('different cyclical annealing strategies', (9, 13)), ('wide range', (15, 17)), ('problems', (18, 19))]","[['different cyclical annealing strategies', 'for', 'wide range'], ['wide range', 'of', 'problems']]",[],"[['Model', 'studying', 'different cyclical annealing strategies']]",[],natural_language_inference,18,26
2338,experiments,Language Results,[],"[('Language Results', (0, 2))]",[],[],[],[],natural_language_inference,18,82
2339,research-problem,Language modeling is a challenging problem due to the complex and long - range interactions between distant words .,"[('due to', (6, 8)), ('between', (15, 16))]","[('Language modeling', (0, 2)), ('challenging problem', (4, 6)), ('complex and long - range interactions', (9, 15)), ('distant words', (16, 18))]","[['challenging problem', 'due to', 'complex and long - range interactions'], ['complex and long - range interactions', 'between', 'distant words']]","[['Language modeling', 'has', 'challenging problem']]",[],[],natural_language_inference,18,83
2340,results,"CBS schedules effectively help us avoid overfitting , and in addition snapshot ensembling enables even greater performance .","[('help us avoid', (3, 6)), ('enables', (13, 14))]","[('overfitting', (6, 7)), ('snapshot ensembling', (11, 13)), ('even greater performance', (14, 17))]","[['snapshot ensembling', 'enables', 'even greater performance']]",[],[],[],natural_language_inference,18,85
2341,results,"As we can see , the best performing CBS schedules result in significant improvements in perplexity ( up to 7.91 ) over the baseline schedules and also offer reductions in the number of SGD training iterations ( up to 33 % ) .","[('result in', (10, 12)), ('in', (14, 15)), ('over', (21, 22)), ('offer', (27, 28)), ('in', (29, 30)), ('of', (32, 33))]","[('best performing CBS schedules', (6, 10)), ('significant improvements', (12, 14)), ('perplexity ( up to 7.91 )', (15, 21)), ('baseline schedules', (23, 25)), ('reductions', (28, 29)), ('number', (31, 32)), ('SGD training iterations ( up to 33 % )', (33, 42))]","[['best performing CBS schedules', 'result in', 'significant improvements'], ['significant improvements', 'in', 'perplexity ( up to 7.91 )'], ['significant improvements', 'over', 'baseline schedules'], ['perplexity ( up to 7.91 )', 'over', 'baseline schedules'], ['significant improvements', 'offer', 'reductions'], ['reductions', 'in', 'number'], ['number', 'of', 'SGD training iterations ( up to 33 % )']]",[],[],[],natural_language_inference,18,88
2342,results,Notice that almost all CBS schedules outperform the baseline schedule .,"[('outperform', (6, 7))]","[('all CBS schedules', (3, 6)), ('baseline schedule', (8, 10))]","[['all CBS schedules', 'outperform', 'baseline schedule']]",[],[],[],natural_language_inference,18,90
2343,results,"In our experiments , CBS schedules do not yield large performance improvements on models like E1 which exhibit smaller disparities between training and testing performance .","[('do not yield', (6, 9)), ('on', (12, 13)), ('like', (14, 15)), ('exhibit', (17, 18)), ('between', (20, 21))]","[('CBS schedules', (4, 6)), ('large performance improvements', (9, 12)), ('models', (13, 14)), ('E1', (15, 16)), ('smaller disparities', (18, 20)), ('training and testing performance', (21, 25))]","[['CBS schedules', 'do not yield', 'large performance improvements'], ['large performance improvements', 'on', 'models'], ['models', 'like', 'E1'], ['E1', 'exhibit', 'smaller disparities'], ['smaller disparities', 'between', 'training and testing performance']]",[],[],[],natural_language_inference,18,97
2344,results,Image Classification Results,[],"[('Image Classification Results', (0, 3))]",[],[],[],"[['Results', 'has', 'Image Classification Results']]",natural_language_inference,18,102
2345,results,"As seen in , the training curves of CBS schedules also exhibit the aforementioned cyclical spikes both in training loss and testing accuracy .","[('in', (2, 3)), ('of', (7, 8)), ('exhibit', (11, 12))]","[('training curves', (5, 7)), ('CBS schedules', (8, 10)), ('aforementioned cyclical spikes', (13, 16)), ('training loss', (18, 20)), ('testing accuracy', (21, 23))]","[['training curves', 'of', 'CBS schedules'], ['CBS schedules', 'exhibit', 'aforementioned cyclical spikes']]","[['training curves', 'has', 'CBS schedules']]",[],[],natural_language_inference,18,103
2346,results,We observe that CBS achieves similar performance to the baseline .,"[('observe', (1, 2)), ('achieves', (4, 5)), ('to', (7, 8))]","[('CBS', (3, 4)), ('similar performance', (5, 7)), ('baseline', (9, 10))]","[['CBS', 'achieves', 'similar performance'], ['similar performance', 'to', 'baseline']]",[],"[['Results', 'observe', 'CBS']]",[],natural_language_inference,18,105
2347,results,"With CBS - 15 , we see 90.71 % training accuracy and 56. 44 % testing accuracy , which is a larger improvement than that offered by CBS on convolutional models on Cifar - 10 .","[('With', (0, 1)), ('see', (6, 7)), ('than', (23, 24)), ('by', (26, 27)), ('on', (28, 29)), ('on', (31, 32))]","[('CBS - 15', (1, 4)), ('90.71 %', (7, 9)), ('training accuracy', (9, 11)), ('56. 44 %', (12, 15)), ('testing accuracy', (15, 17)), ('larger improvement', (21, 23)), ('offered', (25, 26)), ('CBS', (27, 28)), ('convolutional models', (29, 31)), ('Cifar - 10', (32, 35))]","[['CBS - 15', 'see', '90.71 %'], ['CBS - 15', 'see', '56. 44 %'], ['larger improvement', 'than', 'offered'], ['larger improvement', 'by', 'CBS'], ['offered', 'by', 'CBS'], ['CBS', 'on', 'convolutional models'], ['convolutional models', 'on', 'Cifar - 10'], ['convolutional models', 'on', 'Cifar - 10']]","[['90.71 %', 'has', 'training accuracy'], ['56. 44 %', 'has', 'testing accuracy']]","[['Results', 'With', 'CBS - 15']]",[],natural_language_inference,18,107
2348,results,Combining CBS - 15 on C2 with this strategy improves accuracy to 94.82 % .,"[('Combining', (0, 1)), ('on', (4, 5)), ('improves', (9, 10)), ('to', (11, 12))]","[('CBS - 15', (1, 4)), ('C2', (5, 6)), ('accuracy', (10, 11)), ('94.82 %', (12, 14))]","[['CBS - 15', 'on', 'C2'], ['CBS - 15', 'improves', 'accuracy'], ['accuracy', 'to', '94.82 %']]",[],"[['Results', 'Combining', 'CBS - 15']]",[],natural_language_inference,18,109
2349,results,Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % .,"[('Applying', (0, 1)), ('on', (3, 4)), ('trained with', (5, 7)), ('leads to', (12, 14)), ('of', (16, 17)), ('compared to', (21, 23))]","[('snapshot ensembling', (1, 3)), ('C3', (4, 5)), ('CBS - 15 - 2', (7, 12)), ('improved accuracy', (14, 16)), ('93. 56 %', (17, 20)), ('92.58 %', (23, 25))]","[['snapshot ensembling', 'on', 'C3'], ['snapshot ensembling', 'trained with', 'CBS - 15 - 2'], ['C3', 'trained with', 'CBS - 15 - 2'], ['snapshot ensembling', 'leads to', 'improved accuracy'], ['CBS - 15 - 2', 'leads to', 'improved accuracy'], ['improved accuracy', 'of', '93. 56 %']]",[],"[['Results', 'Applying', 'snapshot ensembling']]",[],natural_language_inference,18,111
2350,results,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .","[('After', (0, 1)), ('ensembling', (1, 2)), ('on', (3, 4)), ('with', (5, 6)), ('from', (7, 8)), ('to', (16, 17)), ('from', (19, 20))]","[('ResNet50', (2, 3)), ('Imagenet', (4, 5)), ('snapshots', (6, 7)), ('last two cycles', (9, 12)), ('performance', (14, 15)), ('increases', (15, 16)), ('76.401 %', (17, 19)), ('75.336 %', (20, 22))]","[['ResNet50', 'on', 'Imagenet'], ['Imagenet', 'with', 'snapshots'], ['snapshots', 'from', 'last two cycles'], ['76.401 %', 'from', '75.336 %'], ['increases', 'to', '76.401 %'], ['76.401 %', 'from', '75.336 %']]","[['performance', 'has', 'increases'], ['increases', 'has', '76.401 %']]","[['Results', 'After', 'ResNet50']]",[],natural_language_inference,18,112
2351,research-problem,Natural Language Inference by Tree - Based Convolution and Heuristic Matching,[],"[('Natural Language Inference', (0, 3))]",[],[],[],[],natural_language_inference,19,2
2352,research-problem,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .",[],"[('recognize entailment and contradiction', (10, 14))]",[],[],[],[],natural_language_inference,19,4
2353,research-problem,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .,[],"[('Recognizing entailment and contradiction', (0, 4)), ('natural language inference ( NLI )', (18, 24))]",[],[],[],[],natural_language_inference,19,8
2354,research-problem,"Several examples are illustrated in NLI is in the core of natural language understanding and has wide applications in NLP , e.g. , question answering and automatic summarization .",[],"[('NLI', (5, 6))]",[],[],[],[],natural_language_inference,19,10
2355,model,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .","[('propose', (5, 6)), ('to recognize', (12, 14)), ('between', (17, 18))]","[('TBCNN - pair neural model', (7, 12)), ('entailment and contradiction', (14, 17)), ('two sentences', (18, 20))]","[['TBCNN - pair neural model', 'to recognize', 'entailment and contradiction'], ['entailment and contradiction', 'between', 'two sentences']]",[],"[['Model', 'propose', 'TBCNN - pair neural model']]",[],natural_language_inference,19,33
2356,model,"We lever- age our newly proposed TBCNN model to capture structural information in sentences , which is important to NLI .","[('lever- age', (1, 3)), ('to capture', (8, 10)), ('in', (12, 13))]","[('our newly proposed TBCNN model', (3, 8)), ('structural information', (10, 12)), ('sentences', (13, 14))]","[['our newly proposed TBCNN model', 'to capture', 'structural information'], ['structural information', 'in', 'sentences']]",[],"[['Model', 'lever- age', 'our newly proposed TBCNN model']]",[],natural_language_inference,19,34
2357,experiments,"As we can see , TBCNN is more robust than sequential convolution in terms of word order distortion , which maybe introduced by determinators , modifiers , etc .","[('than', (9, 10)), ('in terms of', (12, 15))]","[('TBCNN', (5, 6)), ('more robust', (7, 9)), ('sequential convolution', (10, 12)), ('word order distortion', (15, 18))]","[['more robust', 'than', 'sequential convolution'], ['sequential convolution', 'in terms of', 'word order distortion']]","[['TBCNN', 'has', 'more robust']]",[],[],natural_language_inference,19,36
2358,model,"A pooling layer then aggregates information along the tree , serving as away of semantic compositonality .","[('along', (6, 7)), ('serving', (10, 11))]","[('pooling layer', (1, 3)), ('aggregates', (4, 5)), ('information', (5, 6)), ('tree', (8, 9)), ('semantic compositonality', (14, 16))]","[['information', 'along', 'tree']]","[['pooling layer', 'has', 'aggregates'], ['aggregates', 'has', 'information']]",[],"[['Model', 'has', 'pooling layer']]",natural_language_inference,19,37
2359,model,"Finally , two sentences ' information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity .","[('combined by', (7, 9)), ('including', (14, 15))]","[(""two sentences ' information"", (2, 6)), ('several heuristic matching layers', (9, 13)), ('concatenation', (15, 16)), ('element - wise product and difference', (17, 23))]","[[""two sentences ' information"", 'combined by', 'several heuristic matching layers'], ['several heuristic matching layers', 'including', 'concatenation'], ['several heuristic matching layers', 'including', 'element - wise product and difference']]",[],[],"[['Model', 'has', ""two sentences ' information""]]",natural_language_inference,19,38
2360,hyperparameters,"All our neural layers , including embeddings , were set to 300 dimensions .","[('including', (5, 6)), ('set to', (9, 11))]","[('All our neural layers', (0, 4)), ('embeddings', (6, 7)), ('300 dimensions', (11, 13))]","[['All our neural layers', 'including', 'embeddings'], ['All our neural layers', 'set to', '300 dimensions']]",[],[],"[['Hyperparameters', 'has', 'All our neural layers']]",natural_language_inference,19,128
2361,hyperparameters,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,"[('by', (5, 6)), ('on', (7, 8)), ('during', (15, 16)), ('as apart of', (17, 20))]","[('Word embeddings', (0, 2)), ('pretrained', (3, 4)), ('word2vec', (6, 7)), ('English Wikipedia corpus', (9, 12)), ('fined tuned', (13, 15)), ('training', (16, 17)), ('model parameters', (20, 22))]","[['pretrained', 'by', 'word2vec'], ['word2vec', 'on', 'English Wikipedia corpus'], ['fined tuned', 'during', 'training'], ['training', 'as apart of', 'model parameters']]","[['Word embeddings', 'has', 'pretrained']]",[],"[['Hyperparameters', 'has', 'Word embeddings']]",natural_language_inference,19,130
2362,hyperparameters,We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,"[('of', (4, 5)), ('chosen by', (11, 13)), ('with', (14, 15)), ('of', (17, 18))]","[('penalty', (3, 4)), ('310 ? 4', (5, 8)), ('dropout', (9, 10)), ('validation', (13, 14)), ('granularity', (16, 17)), ('0.1', (18, 19))]","[['penalty', 'of', '310 ? 4'], ['granularity', 'of', '0.1'], ['dropout', 'chosen by', 'validation'], ['validation', 'with', 'granularity'], ['granularity', 'of', '0.1']]","[['penalty', 'has', '310 ? 4']]",[],[],natural_language_inference,19,131
2363,hyperparameters,"Initial learning rate was set to 1 , and a power decay was applied .","[('set to', (4, 6)), ('applied', (13, 14))]","[('Initial learning rate', (0, 3)), ('1', (6, 7)), ('power decay', (10, 12))]","[['Initial learning rate', 'set to', '1']]","[['Initial learning rate', 'has', '1']]",[],"[['Hyperparameters', 'has', 'Initial learning rate']]",natural_language_inference,19,134
2364,hyperparameters,We used stochastic gradient descent with a batch size of 50 .,"[('used', (1, 2)), ('with', (5, 6)), ('of', (9, 10))]","[('stochastic gradient descent', (2, 5)), ('batch size', (7, 9)), ('50', (10, 11))]","[['stochastic gradient descent', 'with', 'batch size'], ['batch size', 'of', '50']]",[],"[['Hyperparameters', 'used', 'stochastic gradient descent']]",[],natural_language_inference,19,135
2365,results,"As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of humanengineered features , long short term memory .","[('followed by', (9, 11)), ('outperforms', (15, 16)), ('without', (23, 24)), ('including', (27, 28))]","[('TBCNN sentence pair model', (4, 8)), ('simple concatenation alone', (11, 14)), ('existing sentence encoding - based approaches', (16, 22)), ('pretraining', (24, 25)), ('feature - rich method', (29, 33))]","[['TBCNN sentence pair model', 'followed by', 'simple concatenation alone'], ['TBCNN sentence pair model', 'outperforms', 'existing sentence encoding - based approaches'], ['simple concatenation alone', 'outperforms', 'existing sentence encoding - based approaches'], ['existing sentence encoding - based approaches', 'without', 'pretraining'], ['existing sentence encoding - based approaches', 'including', 'feature - rich method']]","[['existing sentence encoding - based approaches', 'name', 'pretraining']]",[],[],natural_language_inference,19,138
2366,results,Model Variant,[],"[('Model Variant', (0, 2))]",[],[],[],"[['Results', 'has', 'Model Variant']]",natural_language_inference,19,140
2367,results,We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,"[('using', (7, 8)), ('than', (16, 17))]","[('element - wise product alone', (8, 13)), ('significantly worse', (14, 16)), ('concatenation or element - wise difference', (17, 23))]","[['significantly worse', 'than', 'concatenation or element - wise difference']]","[['element - wise product alone', 'has', 'significantly worse']]",[],[],natural_language_inference,19,144
2368,results,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .","[('Combining', (0, 1)), ('improves', (4, 5)), ('with', (13, 14)), ('yields', (22, 23)), ('of', (26, 27))]","[('different matching heuristics', (1, 4)), ('result', (6, 7)), ('TBCNN - pair model', (9, 13)), ('concatenation , element - wise product and difference', (14, 22)), ('highest performance', (24, 26)), ('82.1 %', (27, 29))]","[['different matching heuristics', 'improves', 'result'], ['TBCNN - pair model', 'with', 'concatenation , element - wise product and difference'], ['concatenation , element - wise product and difference', 'yields', 'highest performance'], ['highest performance', 'of', '82.1 %']]","[['different matching heuristics', 'has', 'result'], ['result', 'has', 'TBCNN - pair model']]","[['Results', 'Combining', 'different matching heuristics']]",[],natural_language_inference,19,145
2369,results,Further applying element - wise product improves the accuracy by another 0.5 % .,"[('applying', (1, 2)), ('improves', (6, 7)), ('by', (9, 10))]","[('element - wise product', (2, 6)), ('accuracy', (8, 9)), ('another 0.5 %', (10, 13))]","[['element - wise product', 'improves', 'accuracy'], ['accuracy', 'by', 'another 0.5 %']]",[],"[['Results', 'applying', 'element - wise product']]",[],natural_language_inference,19,148
2370,results,"The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .","[('outperforms', (6, 7)), ('in - cluding', (15, 18)), ('with', (29, 30))]","[('full TBCNN - pair model', (1, 6)), ('all existing sentence encoding - based approaches', (7, 14)), ('1024d gated recurrent unit ( GRU ) - based RNN', (19, 29)), ('"" skip - thought "" pretraining', (30, 36))]","[['full TBCNN - pair model', 'outperforms', 'all existing sentence encoding - based approaches'], ['all existing sentence encoding - based approaches', 'in - cluding', '1024d gated recurrent unit ( GRU ) - based RNN'], ['1024d gated recurrent unit ( GRU ) - based RNN', 'with', '"" skip - thought "" pretraining']]",[],[],"[['Results', 'has', 'full TBCNN - pair model']]",natural_language_inference,19,149
2371,research-problem,Stochastic Answer Networks for Natural Language Inference,[],"[('Natural Language Inference', (4, 7))]",[],[],[],[],natural_language_inference,2,2
2372,research-problem,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .",[],"[('recognizing textual entailment ( RTE )', (9, 15))]",[],[],[],[],natural_language_inference,2,9
2373,research-problem,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .","[('on', (8, 9)), ('explore', (17, 18))]","[('multi-step inference strategies', (19, 22)), ('NLI', (23, 24))]","[['multi-step inference strategies', 'on', 'NLI']]",[],"[['Research problem', 'on', 'multi-step inference strategies']]",[],natural_language_inference,2,17
2374,model,"Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .","[('maintains', (12, 13)), ('iteratively refines', (16, 18))]","[('state', (14, 15)), ('predictions', (19, 20))]",[],[],"[['Model', 'maintains', 'state']]",[],natural_language_inference,2,18
2375,experimental-setup,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .,"[('used to', (5, 7)), ('used to', (14, 16)), ('implement', (16, 17))]","[('spaCy tool', (1, 3)), ('tokenize', (7, 8)), ('all the dataset', (8, 11)), ('PyTorch', (12, 13)), ('our models', (17, 19))]","[['spaCy tool', 'used to', 'tokenize'], ['PyTorch', 'implement', 'our models']]","[['tokenize', 'has', 'all the dataset']]",[],"[['Experimental setup', 'has', 'spaCy tool']]",natural_language_inference,2,80
2376,experimental-setup,We fix word embedding with 300 - dimensional GloVe word vectors .,"[('fix', (1, 2)), ('with', (4, 5))]","[('word embedding', (2, 4)), ('300 - dimensional GloVe word vectors', (5, 11))]","[['word embedding', 'with', '300 - dimensional GloVe word vectors']]",[],"[['Experimental setup', 'fix', 'word embedding']]",[],natural_language_inference,2,81
2377,experimental-setup,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .","[('For', (0, 1)), ('use', (6, 7)), ('of', (9, 10)), ('with', (15, 16))]","[('character encoding', (2, 4)), ('concatenation', (8, 9)), ('multi-filter Convolutional Neural Nets', (11, 15)), ('windows', (16, 17)), ('1 , 3 , 5', (17, 22)), ('hidden size', (24, 26)), ('50 , 100 , 150', (26, 31))]","[['character encoding', 'use', 'concatenation'], ['concatenation', 'of', 'multi-filter Convolutional Neural Nets'], ['multi-filter Convolutional Neural Nets', 'with', 'windows'], ['multi-filter Convolutional Neural Nets', 'with', 'hidden size']]","[['character encoding', 'has', 'concatenation'], ['windows', 'has', '1 , 3 , 5'], ['hidden size', 'has', '50 , 100 , 150']]","[['Experimental setup', 'For', 'character encoding']]",[],natural_language_inference,2,82
2378,experimental-setup,So lexicon embeddings are d =600 - dimensions .,"[('are', (3, 4))]","[('lexicon embeddings', (1, 3)), ('d =600 - dimensions', (4, 8))]","[['lexicon embeddings', 'are', 'd =600 - dimensions']]","[['lexicon embeddings', 'has', 'd =600 - dimensions']]",[],"[['Experimental setup', 'has', 'lexicon embeddings']]",natural_language_inference,2,84
2379,experimental-setup,The embedding for the out - of - vocabulary is zeroed .,"[('for', (2, 3))]","[('embedding', (1, 2)), ('out - of - vocabulary', (4, 9)), ('zeroed', (10, 11))]","[['embedding', 'for', 'out - of - vocabulary']]",[],[],"[['Experimental setup', 'has', 'embedding']]",natural_language_inference,2,85
2380,experimental-setup,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .","[('of', (3, 4)), ('in', (5, 6)), ('set to', (15, 17)), ('of', (23, 24))]","[('hidden size', (1, 3)), ('LSTM', (4, 5)), ('contextual encoding layer , memory generation layer', (7, 14)), ('128', (17, 18)), ('input size', (21, 23)), ('output layer', (24, 26)), ('1024 ( 128 * 2 * 4 )', (27, 35))]","[['hidden size', 'of', 'LSTM'], ['input size', 'of', 'output layer'], ['LSTM', 'in', 'contextual encoding layer , memory generation layer'], ['contextual encoding layer , memory generation layer', 'set to', '128'], ['input size', 'of', 'output layer']]","[['hidden size', 'has', 'LSTM']]",[],"[['Experimental setup', 'has', 'hidden size']]",natural_language_inference,2,86
2381,experimental-setup,The projection size in the attention layer is set to 256 .,"[('in', (3, 4)), ('set to', (8, 10))]","[('projection size', (1, 3)), ('attention layer', (5, 7)), ('256', (10, 11))]","[['projection size', 'in', 'attention layer'], ['projection size', 'set to', '256']]",[],[],"[['Experimental setup', 'has', 'projection size']]",natural_language_inference,2,87
2382,experimental-setup,"To speedup training , we use weight normalization .","[('use', (5, 6))]","[('speedup', (1, 2)), ('training', (2, 3)), ('weight normalization', (6, 8))]","[['speedup', 'use', 'weight normalization'], ['training', 'use', 'weight normalization']]","[['speedup', 'has', 'training']]",[],[],natural_language_inference,2,88
2383,experimental-setup,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .","[('fixed through', (11, 13)), ('in', (15, 16))]","[('dropout rate', (1, 3)), ('0.2', (4, 5)), ('dropout mask', (8, 10)), ('time steps', (13, 15)), ('LSTM', (16, 17))]","[['dropout mask', 'fixed through', 'time steps'], ['time steps', 'in', 'LSTM']]","[['dropout rate', 'has', '0.2']]",[],"[['Experimental setup', 'has', 'dropout rate']]",natural_language_inference,2,89
2384,experimental-setup,The mini - batch size is set to 32 .,"[('set to', (6, 8))]","[('mini - batch size', (1, 5)), ('32', (8, 9))]","[['mini - batch size', 'set to', '32']]","[['mini - batch size', 'has', '32']]",[],"[['Experimental setup', 'has', 'mini - batch size']]",natural_language_inference,2,90
2385,experimental-setup,Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,"[('is', (2, 3)), ('initialized as', (9, 11)), ('decreased by', (13, 15)), ('after', (16, 17))]","[('Our optimizer', (0, 2)), ('Adamax', (3, 4)), ('learning rate', (6, 8)), ('0.002', (11, 12)), ('0.5', (15, 16)), ('each 10 epochs', (17, 20))]","[['Our optimizer', 'is', 'Adamax'], ['learning rate', 'initialized as', '0.002'], ['learning rate', 'decreased by', '0.5'], ['0.5', 'after', 'each 10 epochs']]","[['Our optimizer', 'has', 'Adamax']]",[],"[['Experimental setup', 'has', 'Our optimizer']]",natural_language_inference,2,91
2386,results,shows that our multi-step model consistently outperforms the single - step model on the dev set of all four datasets in terms of accuracy .,"[('shows', (0, 1)), ('consistently outperforms', (5, 7)), ('on', (12, 13)), ('of', (16, 17)), ('in terms of', (20, 23))]","[('our multi-step model', (2, 5)), ('single - step model', (8, 12)), ('dev set', (14, 16)), ('all four datasets', (17, 20)), ('accuracy', (23, 24))]","[['our multi-step model', 'consistently outperforms', 'single - step model'], ['single - step model', 'on', 'dev set'], ['dev set', 'of', 'all four datasets'], ['all four datasets', 'in terms of', 'accuracy']]",[],"[['Results', 'shows', 'our multi-step model']]",[],natural_language_inference,2,101
2387,results,"For example , on SciTail dataset , SAN outperforms the single - step model by .","[('on', (3, 4)), ('outperforms', (8, 9))]","[('SciTail dataset', (4, 6)), ('SAN', (7, 8)), ('single - step model', (10, 14))]","[['SciTail dataset', 'outperforms', 'single - step model'], ['SAN', 'outperforms', 'single - step model']]","[['SciTail dataset', 'has', 'SAN']]",[],[],natural_language_inference,2,102
2388,results,"On SciTail dataset , SAN even outperforms GPT .","[('On', (0, 1)), ('outperforms', (6, 7))]","[('SciTail dataset', (1, 3)), ('SAN', (4, 5)), ('GPT', (7, 8))]","[['SAN', 'outperforms', 'GPT']]","[['SciTail dataset', 'has', 'SAN']]","[['Results', 'On', 'SciTail dataset']]",[],natural_language_inference,2,105
2389,results,"Comparing with Single - step baseline , the proposed model obtains + 2.8 improvement on the Sc - iTail test set ( 94.0 vs 91.2 ) and + 2.1 improvement on the SciTail dev set ( 96.1 vs 93.9 ) .","[('Comparing with', (0, 2)), ('obtains', (10, 11)), ('on', (14, 15)), ('on', (30, 31))]","[('Single - step baseline', (2, 6)), ('proposed model', (8, 10)), ('+ 2.8 improvement', (11, 14)), ('Sc - iTail test set', (16, 21)), ('94.0 vs 91.2', (22, 25)), ('+ 2.1 improvement', (27, 30)), ('SciTail dev set', (32, 35)), ('96.1 vs 93.9', (36, 39))]","[['proposed model', 'obtains', '+ 2.8 improvement'], ['proposed model', 'obtains', '+ 2.1 improvement'], ['+ 2.8 improvement', 'on', 'Sc - iTail test set'], ['+ 2.8 improvement', 'on', '+ 2.1 improvement'], ['+ 2.1 improvement', 'on', 'SciTail dev set'], ['+ 2.8 improvement', 'on', 'Sc - iTail test set'], ['+ 2.1 improvement', 'on', 'SciTail dev set']]","[['Single - step baseline', 'has', 'proposed model'], ['Sc - iTail test set', 'has', '94.0 vs 91.2']]","[['Results', 'Comparing with', 'Single - step baseline']]",[],natural_language_inference,2,108
2390,results,"Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .","[('outperforms', (2, 3)), ('in', (6, 7)), ('except on', (12, 14))]","[('Our model', (0, 2)), ('best system', (4, 6)), ('RepEval 2017', (7, 9)), ('"" Conditional "" and "" Tense Difference "" categories', (14, 23))]","[['Our model', 'outperforms', 'best system'], ['best system', 'in', 'RepEval 2017']]",[],[],"[['Results', 'has', 'Our model']]",natural_language_inference,2,128
2391,results,"We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .","[('find', (2, 3)), ('works', (5, 6)), ('on', (8, 9))]","[('SAN', (4, 5)), ('extremely well', (6, 8)), ('"" Active / Passive "" and "" Paraphrase "" categories', (9, 19))]","[['SAN', 'works', 'extremely well'], ['extremely well', 'on', '"" Active / Passive "" and "" Paraphrase "" categories']]",[],"[['Results', 'find', 'SAN']]",[],natural_language_inference,2,129
2392,results,"Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .","[('of', (9, 10)), ('on', (23, 24))]","[(""Chen 's model"", (2, 5)), ('biggest improvement', (7, 9)), ('SAN', (10, 11)), ('"" Antonym "" category', (33, 37))]","[['biggest improvement', 'of', 'SAN']]","[[""Chen 's model"", 'has', 'biggest improvement']]",[],[],natural_language_inference,2,130
2393,results,"In particular , on the most challenging "" Long Sentence "" and "" Quantity / Time "" categories , SAN 's result is substantially better than previous systems .","[('than', (25, 26))]","[('most challenging "" Long Sentence "" and "" Quantity / Time "" categories', (5, 18)), (""SAN 's result"", (19, 22)), ('substantially better', (23, 25)), ('previous systems', (26, 28))]","[['substantially better', 'than', 'previous systems']]","[['most challenging "" Long Sentence "" and "" Quantity / Time "" categories', 'has', ""SAN 's result""], [""SAN 's result"", 'has', 'substantially better']]",[],[],natural_language_inference,2,131
2394,research-problem,Neural Tree Indexers for Text Understanding,[],"[('Text Understanding', (4, 6))]",[],[],[],[],natural_language_inference,20,2
2395,model,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .","[('introduce', (5, 6)), ('for', (19, 20))]","[('Neural Tree Indexers ( NTI )', (6, 12)), ('class of tree structured models', (14, 19)), ('NLP tasks', (20, 22))]","[['class of tree structured models', 'for', 'NLP tasks']]",[],"[['Model', 'introduce', 'Neural Tree Indexers ( NTI )']]",[],natural_language_inference,20,19
2396,model,NTI takes a sequence of tokens and produces its representation by constructing a full n-ary tree in a bottom - up fashion .,"[('takes', (1, 2)), ('of', (4, 5)), ('produces', (7, 8)), ('by constructing', (10, 12)), ('in', (16, 17))]","[('NTI', (0, 1)), ('sequence', (3, 4)), ('tokens', (5, 6)), ('representation', (9, 10)), ('full n-ary tree', (13, 16)), ('bottom - up fashion', (18, 22))]","[['NTI', 'takes', 'sequence'], ['sequence', 'of', 'tokens'], ['NTI', 'produces', 'representation'], ['representation', 'by constructing', 'full n-ary tree'], ['full n-ary tree', 'in', 'bottom - up fashion']]",[],[],"[['Model', 'has', 'NTI']]",natural_language_inference,20,20
2397,model,Each node in NTI is associated with one of the node transformation functions : leaf node mapping and non-leaf node composition functions .,"[('associated with', (5, 7))]","[('Each node in NTI', (0, 4)), ('one of the node transformation functions', (7, 13)), ('leaf node mapping', (14, 17)), ('non-leaf node composition functions', (18, 22))]","[['Each node in NTI', 'associated with', 'one of the node transformation functions']]","[['one of the node transformation functions', 'name', 'leaf node mapping']]",[],"[['Model', 'has', 'Each node in NTI']]",natural_language_inference,20,21
2398,model,"Furthermore , we propose different variants of node composition function and attention over tree for our NTI models .","[('propose', (3, 4)), ('of', (6, 7)), ('over', (12, 13)), ('for', (14, 15))]","[('different variants', (4, 6)), ('node composition function and attention', (7, 12)), ('tree', (13, 14)), ('our NTI models', (15, 18))]","[['different variants', 'of', 'node composition function and attention'], ['node composition function and attention', 'over', 'tree'], ['tree', 'for', 'our NTI models']]",[],"[['Model', 'propose', 'different variants']]",[],natural_language_inference,20,23
2399,model,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :","[('such as', (6, 8)), ('forms', (15, 16)), ('taking advantage of', (22, 25)), ('of', (30, 31))]","[('sequential leaf node transformer', (2, 6)), ('LSTM', (8, 9)), ('chosen', (10, 11)), ('NTI network', (13, 15)), ('sequence - tree hybrid model', (17, 22)), ('conditional and compositional powers', (26, 30)), ('sequential and recursive models', (31, 35))]","[['sequential leaf node transformer', 'such as', 'LSTM'], ['NTI network', 'forms', 'sequence - tree hybrid model'], ['sequence - tree hybrid model', 'taking advantage of', 'conditional and compositional powers'], ['conditional and compositional powers', 'of', 'sequential and recursive models']]","[['sequential leaf node transformer', 'name', 'LSTM'], ['chosen', 'has', 'NTI network']]",[],[],natural_language_inference,20,24
2400,experiments,Natural Language Inference,[],"[('Natural Language Inference', (0, 3))]",[],[],[],[],natural_language_inference,20,147
2401,experiments,Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model .,"[('obtained with', (10, 12))]","[('Our best score', (0, 3)), ('87.3 % accuracy', (7, 10)), ('full tree matching NTI model', (13, 18))]","[['87.3 % accuracy', 'obtained with', 'full tree matching NTI model']]","[['Our best score', 'has', '87.3 % accuracy']]",[],[],natural_language_inference,20,193
2402,experiments,Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % .,"[('improved', (7, 8)), ('of', (10, 11)), ('by', (15, 16))]","[('NTI - SLSTM', (4, 7)), ('performance', (9, 10)), ('sequential LSTM encoder', (12, 15)), ('approximately 2 %', (16, 19))]","[['NTI - SLSTM', 'improved', 'performance'], ['performance', 'of', 'sequential LSTM encoder'], ['sequential LSTM encoder', 'by', 'approximately 2 %']]",[],[],[],natural_language_inference,20,195
2403,experiments,"The node - by - node attention models improve the performance , indicating that modeling inter-sentence interaction is an important element in NLI .","[('improve', (8, 9))]","[('node - by - node attention models', (1, 8)), ('performance', (10, 11))]","[['node - by - node attention models', 'improve', 'performance']]","[['node - by - node attention models', 'has', 'performance']]",[],[],natural_language_inference,20,198
2404,experiments,Answer Sentence Selection,[],"[('Answer Sentence Selection', (0, 3))]",[],[],[],[],natural_language_inference,20,204
2405,experiments,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .","[('outperform', (7, 8)), ('by', (12, 13)), ('nearly', (17, 18))]","[('Deep LSTM and LSTM attention models', (1, 7)), ('previous best result', (9, 12)), ('large margin', (14, 16)), ('5 - 6 %', (18, 22))]","[['Deep LSTM and LSTM attention models', 'outperform', 'previous best result'], ['previous best result', 'by', 'large margin'], ['previous best result', 'nearly', '5 - 6 %'], ['large margin', 'nearly', '5 - 6 %']]",[],[],[],natural_language_inference,20,221
2406,experiments,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,"[('improves', (1, 2)), ('sets', (6, 7)), ('by combining', (10, 12)), ('with', (14, 15))]","[('NASM', (0, 1)), ('result further', (3, 5)), ('strong baseline', (8, 10)), ('variational autoencoder', (12, 14)), ('soft attention', (16, 18))]","[['NASM', 'improves', 'result further'], ['NASM', 'sets', 'strong baseline'], ['strong baseline', 'by combining', 'variational autoencoder'], ['variational autoencoder', 'with', 'soft attention']]",[],[],[],natural_language_inference,20,222
2407,experiments,Our NTI model exceeds NASM by approximately 0.4 % on MAP for this task .,"[('exceeds', (3, 4)), ('by', (5, 6)), ('on', (9, 10))]","[('Our NTI model', (0, 3)), ('NASM', (4, 5)), ('approximately 0.4 %', (6, 9)), ('MAP', (10, 11))]","[['Our NTI model', 'exceeds', 'NASM'], ['NASM', 'by', 'approximately 0.4 %'], ['approximately 0.4 %', 'on', 'MAP']]","[['Our NTI model', 'has', 'NASM']]",[],[],natural_language_inference,20,224
2408,experiments,Sentence Classification,[],"[('Sentence Classification', (0, 2))]",[],[],[],[],natural_language_inference,20,225
2409,experiments,Our NTI - SLSTM model performed slightly worse A dog mouth holds a retrieved ball .,"[('performed', (5, 6))]","[('Our NTI - SLSTM model', (0, 5)), ('slightly worse', (6, 8))]","[['Our NTI - SLSTM model', 'performed', 'slightly worse']]",[],[],[],natural_language_inference,20,234
2410,experiments,"After we transformed the input with the LSTM leaf node function , we achieved the best performance on this task .","[('transformed', (2, 3)), ('with', (5, 6)), ('achieved', (13, 14)), ('on', (17, 18))]","[('input', (4, 5)), ('LSTM leaf node function', (7, 11)), ('best performance', (15, 17)), ('task', (19, 20))]","[['input', 'with', 'LSTM leaf node function'], ['input', 'achieved', 'best performance'], ['LSTM leaf node function', 'achieved', 'best performance'], ['best performance', 'on', 'task']]",[],[],[],natural_language_inference,20,255
2411,research-problem,Attention - over - Attention Neural Networks for Reading Comprehension,[],"[('Reading Comprehension', (8, 10))]",[],[],[],[],natural_language_inference,21,2
2412,research-problem,Cloze - style reading comprehension is a representative problem in mining relationship between document and query .,[],"[('Cloze - style reading comprehension', (0, 5))]",[],[],[],[],natural_language_inference,21,4
2413,research-problem,"To read and comprehend the human languages are challenging tasks for the machines , which requires that the understanding of natural languages and the ability to do reasoning over various clues .",[],"[('read and comprehend the human languages', (1, 7))]",[],[],[],[],natural_language_inference,21,11
2414,model,"In this paper , we present a novel neural network architecture , called attention - over - attention model .","[('present', (5, 6)), ('called', (12, 13))]","[('novel neural network architecture', (7, 11)), ('attention - over - attention model', (13, 19))]","[['novel neural network architecture', 'called', 'attention - over - attention model']]","[['novel neural network architecture', 'name', 'attention - over - attention model']]","[['Model', 'present', 'novel neural network architecture']]",[],natural_language_inference,21,20
2415,model,"As we can understand the meaning literally , our model aims to place another attention mechanism over the existing document - level attention .","[('place', (12, 13)), ('over', (16, 17))]","[('another attention mechanism', (13, 16)), ('existing document - level attention', (18, 23))]","[['another attention mechanism', 'over', 'existing document - level attention']]",[],"[['Model', 'place', 'another attention mechanism']]",[],natural_language_inference,21,21
2416,model,"Unlike the previous works , that are using heuristic merging functions , or setting various pre-defined non-trainable terms , our model could automatically generate an "" attended attention "" over various document - level attentions , and make a mutual look not only from query - to - document but also document - to - query , which will benefit from the interactive information .","[('automatically generate', (22, 24)), ('over', (29, 30))]","[('our model', (19, 21)), ('attended attention', (26, 28)), ('various document - level attentions', (30, 35))]","[['our model', 'automatically generate', 'attended attention'], ['attended attention', 'over', 'various document - level attentions']]",[],[],"[['Model', 'has', 'our model']]",natural_language_inference,21,22
2417,experimental-setup,Embedding Layer :,[],"[('Embedding Layer', (0, 2))]",[],[],[],"[['Experimental setup', 'has', 'Embedding Layer']]",natural_language_inference,21,141
2418,experimental-setup,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ] .","[('with', (6, 7)), ('in', (10, 11))]","[('embedding weights', (1, 3)), ('randomly initialized', (4, 6)), ('uniformed distribution', (8, 10)), ('interval [ ? 0.05 , 0.05 ]', (12, 19))]","[['randomly initialized', 'with', 'uniformed distribution'], ['uniformed distribution', 'in', 'interval [ ? 0.05 , 0.05 ]']]","[['embedding weights', 'has', 'randomly initialized']]",[],"[['Experimental setup', 'has', 'embedding weights']]",natural_language_inference,21,142
2419,experimental-setup,Hidden Layer : Internal weights of GRUs are initialized with random orthogonal matrices .,"[('of', (5, 6)), ('with', (9, 10))]","[('Hidden Layer', (0, 2)), ('Internal weights', (3, 5)), ('GRUs', (6, 7)), ('initialized', (8, 9)), ('random orthogonal matrices', (10, 13))]","[['Internal weights', 'of', 'GRUs'], ['Hidden Layer', 'with', 'random orthogonal matrices'], ['initialized', 'with', 'random orthogonal matrices']]","[['Hidden Layer', 'has', 'Internal weights']]",[],"[['Experimental setup', 'has', 'Hidden Layer']]",natural_language_inference,21,148
2420,experimental-setup,"We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .","[('adopted', (1, 2)), ('for', (4, 5)), ('with', (8, 9)), ('of', (13, 14))]","[('ADAM optimizer', (2, 4)), ('weight updating', (5, 7)), ('initial learning rate', (10, 13)), ('0.001', (14, 15))]","[['ADAM optimizer', 'for', 'weight updating'], ['ADAM optimizer', 'with', 'initial learning rate'], ['weight updating', 'with', 'initial learning rate'], ['initial learning rate', 'of', '0.001']]","[['initial learning rate', 'has', '0.001']]","[['Experimental setup', 'adopted', 'ADAM optimizer']]",[],natural_language_inference,21,150
2421,experimental-setup,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .","[('set', (13, 14)), ('to', (18, 19))]","[('gradient clipping threshold', (15, 18)), ('5', (19, 20))]","[['gradient clipping threshold', 'to', '5']]","[['gradient clipping threshold', 'has', '5']]","[['Experimental setup', 'set', 'gradient clipping threshold']]",[],natural_language_inference,21,151
2422,experimental-setup,We used batched training strategy of 32 samples .,"[('used', (1, 2)), ('of', (5, 6))]","[('batched training strategy', (2, 5)), ('32 samples', (6, 8))]","[['batched training strategy', 'of', '32 samples']]",[],"[['Experimental setup', 'used', 'batched training strategy']]",[],natural_language_inference,21,152
2423,experimental-setup,"In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .","[('In', (0, 1)), ('generate', (5, 6)), ('from', (10, 11))]","[('re-ranking step', (1, 3)), ('5 - best list', (6, 10)), ('baseline neural network model', (12, 16))]","[['re-ranking step', 'generate', '5 - best list'], ['5 - best list', 'from', 'baseline neural network model']]",[],"[['Experimental setup', 'In', 're-ranking step']]",[],natural_language_inference,21,154
2424,experimental-setup,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .","[('trained on', (5, 7)), ('of', (10, 11)), ('with', (14, 15)), ('trained by', (25, 27))]","[('All language model features', (0, 4)), ('training proportion', (8, 10)), ('each dataset', (11, 13)), ('8 - gram wordbased setting', (15, 20)), ('Kneser - Ney smoothing', (21, 25)), ('SRILM toolkit', (27, 29))]","[['All language model features', 'trained on', 'training proportion'], ['All language model features', 'trained on', 'Kneser - Ney smoothing'], ['training proportion', 'of', 'each dataset'], ['training proportion', 'with', '8 - gram wordbased setting'], ['each dataset', 'with', '8 - gram wordbased setting'], ['Kneser - Ney smoothing', 'trained by', 'SRILM toolkit']]",[],[],"[['Experimental setup', 'has', 'All language model features']]",natural_language_inference,21,155
2425,experimental-setup,"Implementation is done with Theano ( Theano Development Team , 2016 ) and Keras , and all models are trained on Tesla K40 GPU . :","[('done with', (2, 4)), ('trained on', (19, 21))]","[('Implementation', (0, 1)), ('Theano ( Theano Development Team , 2016 )', (4, 12)), ('Keras', (13, 14)), ('all models', (16, 18)), ('Tesla K40 GPU', (21, 24))]","[['Implementation', 'done with', 'Theano ( Theano Development Team , 2016 )'], ['Implementation', 'done with', 'Keras'], ['all models', 'trained on', 'Tesla K40 GPU']]",[],[],"[['Experimental setup', 'has', 'Implementation']]",natural_language_inference,21,158
2426,results,"As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .","[('outperforms', (9, 10)), ('by', (18, 19)), ('where', (23, 24)), ('over', (31, 32)), ('in', (33, 34))]","[('our AoA Reader', (6, 9)), ('state - of - the - art systems', (10, 18)), ('large margin', (20, 22)), ('2.3 % and 2.0 % absolute improvements', (24, 31)), ('EpiReader', (32, 33)), ('CBTest NE and CN test sets', (34, 40))]","[['our AoA Reader', 'outperforms', 'state - of - the - art systems'], ['state - of - the - art systems', 'by', 'large margin'], ['large margin', 'where', '2.3 % and 2.0 % absolute improvements'], ['2.3 % and 2.0 % absolute improvements', 'over', 'EpiReader'], ['EpiReader', 'in', 'CBTest NE and CN test sets']]",[],[],[],natural_language_inference,21,163
2427,results,"Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .","[('by adding', (1, 3)), ('in', (5, 6)), ('over', (20, 21)), ('in', (24, 25))]","[('additional features', (3, 5)), ('re-ranking step', (7, 9)), ('another significant boost', (12, 15)), ('2.0 % to 3.7 %', (15, 20)), ('Ao A Reader', (21, 24)), ('CBTest NE / CN test sets', (25, 31))]","[['additional features', 'in', 're-ranking step'], ['Ao A Reader', 'in', 'CBTest NE / CN test sets'], ['another significant boost', 'over', 'Ao A Reader'], ['2.0 % to 3.7 %', 'over', 'Ao A Reader'], ['Ao A Reader', 'in', 'CBTest NE / CN test sets']]","[['another significant boost', 'has', '2.0 % to 3.7 %']]","[['Results', 'by adding', 'additional features']]",[],natural_language_inference,21,164
2428,results,"We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .","[('stay on par with', (9, 13)), ('beyond', (29, 30)), ('in', (38, 39))]","[('single model', (6, 8)), ('previous best ensemble system', (14, 18)), ('absolute improvement', (24, 26)), ('0.9 %', (27, 29)), ('best ensemble model ( Iterative Attention )', (31, 38)), ('CBTest NE validation set', (40, 44))]","[['single model', 'stay on par with', 'previous best ensemble system'], ['0.9 %', 'beyond', 'best ensemble model ( Iterative Attention )'], ['best ensemble model ( Iterative Attention )', 'in', 'CBTest NE validation set']]",[],[],[],natural_language_inference,21,165
2429,results,"When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .","[('shows', (11, 12)), ('over', (14, 15)), ('by', (19, 20)), ('setup', (24, 25))]","[('ensemble model', (4, 6)), ('our AoA Reader', (7, 10)), ('significant improvements', (12, 14)), ('previous best ensemble models', (15, 19)), ('large margin', (21, 23)), ('new state - of - the - art system', (26, 35))]","[['our AoA Reader', 'shows', 'significant improvements'], ['significant improvements', 'over', 'previous best ensemble models'], ['significant improvements', 'by', 'large margin'], ['previous best ensemble models', 'by', 'large margin'], ['significant improvements', 'setup', 'new state - of - the - art system'], ['previous best ensemble models', 'setup', 'new state - of - the - art system']]","[['ensemble model', 'has', 'our AoA Reader']]",[],[],natural_language_inference,21,166
2430,results,"Instead of using pre-defined merging heuristics , and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance , where 4.1 % and 3.7 % improvements can be made in CNN validation and test set against CAS Reader .","[('in', (19, 20)), ('where', (27, 28)), ('made in', (36, 38)), ('against', (43, 44))]","[('significant boost', (21, 23)), ('performance', (25, 26)), ('4.1 % and 3.7 % improvements', (28, 34)), ('CNN validation and test set', (38, 43)), ('CAS Reader', (44, 46))]","[['significant boost', 'where', '4.1 % and 3.7 % improvements'], ['performance', 'where', '4.1 % and 3.7 % improvements'], ['4.1 % and 3.7 % improvements', 'made in', 'CNN validation and test set'], ['CNN validation and test set', 'against', 'CAS Reader']]",[],[],[],natural_language_inference,21,168
2431,results,"From the results in , we found that the NE and CN category both benefit a lot from the re-ranking features , but the proportions are quite different .","[('found that', (6, 8)), ('from', (17, 18))]","[('NE and CN category', (9, 13)), ('benefit a lot', (14, 17)), ('re-ranking features', (19, 21))]","[['benefit a lot', 'from', 're-ranking features']]","[['NE and CN category', 'has', 'benefit a lot']]","[['Results', 'found that', 'NE and CN category']]",[],natural_language_inference,21,172
2432,results,"Generally speaking , in NE category , the performance is mainly boosted by the LM local feature .","[('by', (12, 13))]","[('NE category', (4, 6)), ('performance', (8, 9)), ('mainly boosted', (10, 12)), ('LM local feature', (14, 17))]","[['mainly boosted', 'by', 'LM local feature']]","[['NE category', 'has', 'performance'], ['performance', 'has', 'mainly boosted']]",[],[],natural_language_inference,21,173
2433,research-problem,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems,[],"[('Question Answering', (9, 11))]",[],[],[],[],natural_language_inference,22,2
2434,research-problem,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,[],"[('Question Answering ( QA )', (0, 5))]",[],[],[],[],natural_language_inference,22,4
2435,research-problem,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,[],"[('QA', (8, 9))]",[],[],[],[],natural_language_inference,22,5
2436,model,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .","[('employ', (1, 2)), ('to depict', (5, 7)), ('between', (10, 11)), ('for', (12, 13)), ('propose', (18, 19)), ('solves', (27, 28))]","[('connected graph', (3, 5)), ('paraphrase relation', (8, 10)), ('sentences', (11, 12)), ('PI task', (14, 16)), ('multi-task sentence - encoding model', (20, 25)), ('paraphrase identification task', (29, 32)), ('sentence intent classification task', (34, 38)), ('simultaneously', (38, 39))]","[['connected graph', 'to depict', 'paraphrase relation'], ['paraphrase relation', 'between', 'sentences'], ['sentences', 'for', 'PI task'], ['connected graph', 'propose', 'multi-task sentence - encoding model'], ['multi-task sentence - encoding model', 'solves', 'paraphrase identification task'], ['multi-task sentence - encoding model', 'solves', 'sentence intent classification task']]","[['sentence intent classification task', 'has', 'simultaneously']]","[['Model', 'employ', 'connected graph']]",[],natural_language_inference,22,28
2437,model,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .","[('integrates', (7, 8)), ('with', (15, 16)), ('find', (27, 28)), ('from', (34, 35))]","[('semantic retrieval framework', (3, 6)), ('encoding - based sentence matching model', (9, 15)), ('approximate nearest neighbor search technology', (17, 22)), ('most similar question', (29, 32)), ('very quickly', (32, 34)), ('all available questions', (35, 38))]","[['semantic retrieval framework', 'integrates', 'encoding - based sentence matching model'], ['encoding - based sentence matching model', 'with', 'approximate nearest neighbor search technology'], ['approximate nearest neighbor search technology', 'find', 'most similar question'], ['most similar question', 'from', 'all available questions'], ['very quickly', 'from', 'all available questions']]","[['most similar question', 'has', 'very quickly']]",[],[],natural_language_inference,22,29
2438,hyperparameters,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .","[('For', (0, 1)), ('use', (5, 6)), ('as', (13, 14))]","[('Quora dataset', (1, 3)), ('Glove - 840B - 300D vector', (7, 13)), ('pre-trained word embedding', (15, 18))]","[['Quora dataset', 'use', 'Glove - 840B - 300D vector'], ['Glove - 840B - 300D vector', 'as', 'pre-trained word embedding']]",[],"[['Hyperparameters', 'For', 'Quora dataset']]",[],natural_language_inference,22,153
2439,hyperparameters,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,"[('with', (6, 7)), ('of', (13, 14)), ('set to', (16, 18))]","[('character embedding', (1, 3)), ('randomly initialized', (4, 6)), ('150 D', (7, 9)), ('hidden size', (11, 13)), ('BiGRU', (14, 15)), ('300', (18, 19))]","[['randomly initialized', 'with', '150 D'], ['hidden size', 'of', 'BiGRU']]","[['character embedding', 'has', 'randomly initialized']]",[],"[['Hyperparameters', 'has', 'character embedding']]",natural_language_inference,22,154
2440,hyperparameters,We set = 0.8 in the multi - task loss function .,"[('set', (1, 2)), ('in', (4, 5))]","[('0.8', (3, 4)), ('multi - task loss function', (6, 11))]","[['0.8', 'in', 'multi - task loss function']]",[],"[['Hyperparameters', 'set', '0.8']]",[],natural_language_inference,22,155
2441,hyperparameters,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .","[('applied to', (4, 6)), ('of', (8, 9)), ('with', (14, 15)), ('of', (18, 19))]","[('Dropout layer', (0, 2)), ('output', (7, 8)), ('attentive pooling layer', (10, 13)), ('dropout rate', (16, 18)), ('0.1', (19, 20))]","[['Dropout layer', 'applied to', 'output'], ['output', 'of', 'attentive pooling layer'], ['dropout rate', 'of', '0.1'], ['Dropout layer', 'with', 'dropout rate'], ['attentive pooling layer', 'with', 'dropout rate'], ['dropout rate', 'of', '0.1']]","[['dropout rate', 'has', '0.1']]",[],"[['Hyperparameters', 'has', 'Dropout layer']]",natural_language_inference,22,157
2442,hyperparameters,An Adam optimizer is used to optimize all the trainable weights .,"[('used to', (4, 6))]","[('Adam optimizer', (1, 3)), ('optimize', (6, 7)), ('all the trainable weights', (7, 11))]","[['Adam optimizer', 'used to', 'optimize']]","[['optimize', 'has', 'all the trainable weights']]",[],"[['Hyperparameters', 'has', 'Adam optimizer']]",natural_language_inference,22,158
2443,hyperparameters,The learning rate is set to 4e - 4 and the batch size is set to 200 .,"[('set to', (4, 6)), ('set to', (14, 16))]","[('learning rate', (1, 3)), ('4e - 4', (6, 9)), ('batch size', (11, 13)), ('200', (16, 17))]","[['learning rate', 'set to', '4e - 4'], ['batch size', 'set to', '200']]","[['learning rate', 'has', '4e - 4'], ['batch size', 'has', '200']]",[],"[['Hyperparameters', 'has', 'learning rate']]",natural_language_inference,22,159
2444,hyperparameters,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .","[('of', (3, 4)), ('with', (14, 15)), ('to find', (24, 26))]","[('SGD optimizer', (12, 14)), ('learning rate', (16, 18)), ('1e - 3', (19, 22)), ('better local optimum', (27, 30))]","[['learning rate', 'of', '1e - 3'], ['SGD optimizer', 'with', 'learning rate'], ['1e - 3', 'to find', 'better local optimum']]","[['learning rate', 'has', '1e - 3']]","[['Hyperparameters', 'of', 'SGD optimizer']]",[],natural_language_inference,22,160
2445,baselines,ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .,"[('is', (6, 7)), ('for', (12, 13))]","[('ESIM', (0, 1)), ('Enhanced Sequential Inference Model', (2, 6)), ('interaction - based model', (8, 12)), ('natural language inference', (13, 16))]","[['Enhanced Sequential Inference Model', 'is', 'interaction - based model'], ['interaction - based model', 'for', 'natural language inference']]","[['ESIM', 'has', 'Enhanced Sequential Inference Model']]",[],"[['Baselines', 'has', 'ESIM']]",natural_language_inference,22,163
2446,baselines,BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .,"[('with', (15, 16))]","[('BiMPM', (0, 1)), ('Bilateral Multi- Perspective Matching model', (2, 7)), ('interaction - based sentence matching model', (9, 15)), ('superior performance', (16, 18))]","[['interaction - based sentence matching model', 'with', 'superior performance']]","[['BiMPM', 'has', 'Bilateral Multi- Perspective Matching model']]",[],"[['Baselines', 'has', 'BiMPM']]",natural_language_inference,22,166
2447,baselines,"SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .","[('enhances', (16, 17)), ('with', (21, 22))]","[('SSE', (0, 1)), ('Shortcut - Stacked Sentence Encoder', (2, 7)), ('encodingbased sentence - matching model', (9, 14)), ('multi - layer BiLSTM', (17, 21)), ('short - cut connections', (22, 26))]","[['encodingbased sentence - matching model', 'enhances', 'multi - layer BiLSTM'], ['multi - layer BiLSTM', 'with', 'short - cut connections']]","[['SSE', 'has', 'Shortcut - Stacked Sentence Encoder'], ['Shortcut - Stacked Sentence Encoder', 'has', 'encodingbased sentence - matching model']]",[],"[['Baselines', 'has', 'SSE']]",natural_language_inference,22,168
2448,baselines,DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .,"[('for', (12, 13))]","[('DIIN', (0, 1)), ('Densely Interactive Inference Network', (2, 6)), ('interaction - based model', (8, 12)), ('natural language inference ( NLI )', (13, 19))]","[['interaction - based model', 'for', 'natural language inference ( NLI )']]","[['DIIN', 'has', 'Densely Interactive Inference Network']]",[],"[['Baselines', 'has', 'DIIN']]",natural_language_inference,22,170
2449,results,"Quora dataset : BiMPM and ESIM models without any sentence interaction information , and is very close to DIIN , the state - of - the - art interaction - based model , but we do n't any external knowledge in our method .","[('without', (7, 8))]","[('Quora dataset', (0, 2)), ('BiMPM and ESIM models', (3, 7)), ('any sentence interaction information', (8, 12)), ('very close', (15, 17)), ('DIIN', (18, 19)), ('state - of - the - art interaction - based model', (21, 32))]","[['BiMPM and ESIM models', 'without', 'any sentence interaction information']]","[['Quora dataset', 'has', 'BiMPM and ESIM models']]",[],"[['Results', 'has', 'Quora dataset']]",natural_language_inference,22,178
2450,results,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,[],"[('LCQMC dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'LCQMC dataset']]",natural_language_inference,22,179
2451,results,The experimental results show that our model outperforms state - of the - art models .,"[('show', (3, 4))]","[('our model', (5, 7)), ('outperforms', (7, 8)), ('state - of the - art models', (8, 15))]",[],"[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'state - of the - art models']]","[['Results', 'show', 'our model']]",[],natural_language_inference,22,180
2452,results,BQ dataset : BQ dataset is a specific - domain dataset with a low average overlap rate .,[],"[('BQ dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'BQ dataset']]",natural_language_inference,22,181
2453,results,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .","[('by', (15, 16)), ('reaching', (20, 21)), ('recording', (26, 27))]","[('our model', (4, 6)), ('outperforms', (6, 7)), ('state - of - the - art models', (7, 15)), ('large margin', (17, 19)), ('83 . 62 %', (21, 25)), ('state - of - the - art performance', (28, 36))]","[['state - of - the - art models', 'by', 'large margin'], ['state - of - the - art models', 'reaching', '83 . 62 %'], ['large margin', 'reaching', '83 . 62 %'], ['state - of - the - art models', 'recording', 'state - of - the - art performance'], ['83 . 62 %', 'recording', 'state - of - the - art performance']]","[['our model', 'has', 'outperforms'], ['outperforms', 'has', 'state - of - the - art models']]",[],"[['Results', 'has', 'our model']]",natural_language_inference,22,182
2454,results,TCS dataset :,[],"[('TCS dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'TCS dataset']]",natural_language_inference,22,183
2455,results,As shown in show that our MSEM model achieves the best performance .,"[('achieves', (8, 9))]","[('our MSEM model', (5, 8)), ('best performance', (10, 12))]","[['our MSEM model', 'achieves', 'best performance']]","[['our MSEM model', 'has', 'best performance']]",[],"[['Results', 'has', 'our MSEM model']]",natural_language_inference,22,184
2456,ablation-analysis,We first study the contribution of the ARU component .,"[('study', (2, 3)), ('of', (5, 6))]","[('contribution', (4, 5)), ('ARU component', (7, 9))]","[['contribution', 'of', 'ARU component']]",[],"[['Ablation analysis', 'study', 'contribution']]",[],natural_language_inference,22,193
2457,ablation-analysis,"The accuracy decreases , the accuracy will drop to 88.25 % .","[('drop to', (7, 9))]","[('accuracy', (1, 2)), ('decreases', (2, 3)), ('88.25 %', (9, 11))]",[],"[['accuracy', 'has', 'decreases']]",[],"[['Ablation analysis', 'has', 'accuracy']]",natural_language_inference,22,194
2458,ablation-analysis,It turns out that the attentive pooling is better than max pooling .,"[('than', (9, 10))]","[('attentive pooling', (5, 7)), ('better', (8, 9)), ('max pooling', (10, 12))]","[['better', 'than', 'max pooling']]","[['attentive pooling', 'has', 'better']]",[],"[['Ablation analysis', 'has', 'attentive pooling']]",natural_language_inference,22,196
2459,ablation-analysis,"Then if we remove the highway network , the accuracy will drop to 88.36 % .","[('remove', (3, 4)), ('to', (12, 13))]","[('highway network', (5, 7)), ('accuracy', (9, 10)), ('drop', (11, 12)), ('88.36 %', (13, 15))]","[['drop', 'to', '88.36 %']]","[['highway network', 'has', 'accuracy'], ['accuracy', 'has', 'drop']]","[['Ablation analysis', 'remove', 'highway network']]",[],natural_language_inference,22,197
2460,ablation-analysis,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .","[('to', (14, 15))]","[('character - level embedding', (5, 9)), ('accuracy', (11, 12)), ('drop', (13, 14)), ('88.26 %', (15, 17))]","[['drop', 'to', '88.26 %']]","[['character - level embedding', 'has', 'accuracy']]",[],[],natural_language_inference,22,198
2461,research-problem,Deep Fusion LSTMs for Text Semantic Matching,[],"[('Text Semantic Matching', (4, 7))]",[],[],[],[],natural_language_inference,23,2
2462,research-problem,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .",[],"[('modelling the relevance / similarity of a pair of texts', (25, 35))]",[],[],[],[],natural_language_inference,23,14
2463,model,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .","[('adopt', (5, 6)), ('to model', (10, 12)), ('of', (15, 16))]","[('deep fusion strategy', (7, 10)), ('strong interactions', (13, 15)), ('two sentences', (16, 18))]","[['deep fusion strategy', 'to model', 'strong interactions'], ['strong interactions', 'of', 'two sentences']]",[],"[['Model', 'adopt', 'deep fusion strategy']]",[],natural_language_inference,23,25
2464,model,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .","[('regarded as', (6, 8)), ('of', (11, 12)), ('in', (14, 15))]","[('text matching', (2, 4)), ('modelling', (8, 9)), ('interaction', (10, 11)), ('two texts', (12, 14)), ('recursive matching way', (16, 19))]","[['text matching', 'regarded as', 'modelling'], ['interaction', 'of', 'two texts'], ['two texts', 'in', 'recursive matching way']]","[['text matching', 'has', 'modelling'], ['modelling', 'has', 'interaction']]",[],"[['Model', 'has', 'text matching']]",natural_language_inference,23,28
2465,model,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .","[('propose', (5, 6)), ('to model', (20, 22))]","[('deep fusion long short - term memory neural networks ( DF - LSTMs )', (6, 20)), ('interactions', (23, 24)), ('recursively', (24, 25))]","[['deep fusion long short - term memory neural networks ( DF - LSTMs )', 'to model', 'interactions']]","[['interactions', 'has', 'recursively']]","[['Model', 'propose', 'deep fusion long short - term memory neural networks ( DF - LSTMs )']]",[],natural_language_inference,23,29
2466,model,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .","[('consist of', (6, 8)), ('under', (20, 21))]","[('DF - LSTMs', (3, 6)), ('two interconnected conditional LSTMs', (8, 12)), ('models', (16, 17)), ('apiece of text', (17, 20)), ('influence of another', (22, 25))]","[['DF - LSTMs', 'consist of', 'two interconnected conditional LSTMs'], ['apiece of text', 'under', 'influence of another']]","[['models', 'has', 'apiece of text']]",[],"[['Model', 'has', 'DF - LSTMs']]",natural_language_inference,23,30
2467,model,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,"[('of', (3, 4)), ('fed into', (8, 10)), ('to compute', (16, 18))]","[('output vector', (1, 3)), ('DF - LSTMs', (4, 7)), ('task - specific output layer', (11, 16)), ('match - ing score', (19, 23))]","[['output vector', 'of', 'DF - LSTMs'], ['DF - LSTMs', 'fed into', 'task - specific output layer'], ['task - specific output layer', 'to compute', 'match - ing score']]",[],[],"[['Model', 'has', 'output vector']]",natural_language_inference,23,31
2468,baselines,Neural bag - of - words ( NBOW ) :,[],"[('Neural bag - of - words ( NBOW )', (0, 9))]",[],[],[],"[['Baselines', 'has', 'Neural bag - of - words ( NBOW )']]",natural_language_inference,23,139
2469,baselines,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .","[('represented as', (3, 5)), ('of', (10, 11)), ('fed to', (21, 23))]","[('Each sequence', (0, 2)), ('sum of the embeddings', (6, 10)), ('words', (12, 13)), ('concatenated', (19, 20)), ('MLP', (24, 25))]","[['Each sequence', 'represented as', 'sum of the embeddings'], ['sum of the embeddings', 'of', 'words'], ['Each sequence', 'fed to', 'MLP']]",[],[],"[['Baselines', 'has', 'Each sequence']]",natural_language_inference,23,140
2470,baselines,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .","[('encoded by', (6, 8))]","[('Single LSTM', (0, 2)), ('Two sequences', (3, 5)), ('single LSTM', (9, 11))]","[['Two sequences', 'encoded by', 'single LSTM']]","[['Single LSTM', 'has', 'Two sequences']]",[],"[['Baselines', 'has', 'Single LSTM']]",natural_language_inference,23,141
2471,baselines,"Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .","[('first encoded', (6, 8)), ('by', (8, 9))]","[('Parallel LSTMs', (0, 2)), ('Two sequences', (3, 5)), ('two LSTMs', (9, 11)), ('separately', (11, 12))]",[],"[['Parallel LSTMs', 'has', 'Two sequences'], ['two LSTMs', 'has', 'separately']]",[],"[['Baselines', 'has', 'Parallel LSTMs']]",natural_language_inference,23,142
2472,baselines,"Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .","[('encoded by', (6, 8)), ('with', (9, 10))]","[('Attention LSTMs', (0, 2)), ('Two sequences', (3, 5)), ('LSTMs', (8, 9)), ('attention mechanism', (10, 12))]","[['Two sequences', 'encoded by', 'LSTMs'], ['LSTMs', 'with', 'attention mechanism']]","[['Attention LSTMs', 'has', 'Two sequences']]",[],"[['Baselines', 'has', 'Attention LSTMs']]",natural_language_inference,23,143
2473,baselines,"Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :","[('of', (11, 12))]","[('Word - by - word Attention LSTMs', (0, 7)), ('improved strategy', (9, 11)), ('attention LSTMs', (12, 14))]","[['improved strategy', 'of', 'attention LSTMs']]","[['Word - by - word Attention LSTMs', 'has', 'improved strategy']]",[],"[['Baselines', 'has', 'Word - by - word Attention LSTMs']]",natural_language_inference,23,144
2474,results,"we can see that the proposed model also shows its superiority on this task , which outperforms the stateof - the - arts methods on both metrics ( P@1 ( 5 ) and P@1 ( 10 ) ) with a large margin .","[('see that', (2, 4)), ('shows', (8, 9)), ('on', (11, 12)), ('outperforms', (16, 17)), ('on', (24, 25)), ('with', (38, 39))]","[('proposed model', (5, 7)), ('superiority', (10, 11)), ('task', (13, 14)), ('stateof - the - arts methods', (18, 24)), ('both metrics ( P@1 ( 5 ) and P@1 ( 10 ) )', (25, 38)), ('large margin', (40, 42))]","[['proposed model', 'shows', 'superiority'], ['superiority', 'on', 'task'], ['stateof - the - arts methods', 'on', 'both metrics ( P@1 ( 5 ) and P@1 ( 10 ) )'], ['superiority', 'outperforms', 'stateof - the - arts methods'], ['task', 'outperforms', 'stateof - the - arts methods'], ['stateof - the - arts methods', 'on', 'both metrics ( P@1 ( 5 ) and P@1 ( 10 ) )'], ['both metrics ( P@1 ( 5 ) and P@1 ( 10 ) )', 'with', 'large margin']]","[['proposed model', 'has', 'superiority']]","[['Results', 'see that', 'proposed model']]",[],natural_language_inference,23,155
2475,results,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .","[('consistently outperform', (25, 27)), ('with', (37, 38))]","[('strong interaction models', (13, 16)), ('attention LSTMs', (17, 19)), ('our DF - LSTMs', (20, 24)), ('weak interaction models', (28, 31)), ('NBOW', (32, 33)), ('parallel LSTMs', (34, 36)), ('large margin', (39, 41))]","[['strong interaction models', 'consistently outperform', 'weak interaction models'], ['weak interaction models', 'with', 'large margin']]","[['strong interaction models', 'has', 'attention LSTMs'], ['weak interaction models', 'name', 'NBOW']]",[],[],natural_language_inference,23,156
2476,research-problem,Reading Wikipedia to Answer Open-Domain Questions,[],"[('Answer Open-Domain Questions', (3, 6))]",[],[],[],[],natural_language_inference,24,2
2477,research-problem,"This paper considers the problem of answering factoid questions in an open - domain setting using Wikipedia as the unique knowledge source , such as one does when looking for answers in an encyclopedia .",[],"[('answering factoid questions in an open - domain setting', (6, 15))]",[],[],[],[],natural_language_inference,24,9
2478,research-problem,"Unlike knowledge bases ( KBs ) such as Freebase or DB - Pedia , which are easier for computers to process but too sparsely populated for open - domain question answering , Wikipedia contains up - to - date knowledge that humans are interested in .",[],"[('open - domain question answering', (26, 31))]",[],[],[],[],natural_language_inference,24,11
2479,model,"In order to answer any question , one must first retrieve the few relevant articles among more than 5 million items , and then scan them carefully to identify the answer .","[('to answer', (2, 4)), ('retrieve', (10, 11)), ('among', (15, 16)), ('to identify', (27, 29))]","[('any question', (4, 6)), ('few relevant articles', (12, 15)), ('more than 5 million items', (16, 21)), ('scan', (24, 25)), ('answer', (30, 31))]","[['any question', 'retrieve', 'few relevant articles'], ['few relevant articles', 'among', 'more than 5 million items'], ['scan', 'to identify', 'answer']]",[],"[['Model', 'to answer', 'any question']]",[],natural_language_inference,24,14
2480,research-problem,"We term this setting , machine reading at scale ( MRS ) .","[('term', (1, 2))]","[('machine reading at scale ( MRS )', (5, 12))]",[],[],"[['Research problem', 'term', 'machine reading at scale ( MRS )']]",[],natural_language_inference,24,15
2481,model,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,"[('treats', (2, 3)), ('as', (4, 5)), ('of', (7, 8))]","[('Wikipedia', (3, 4)), ('collection', (6, 7)), ('articles', (8, 9))]","[['Wikipedia', 'as', 'collection'], ['collection', 'of', 'articles']]",[],"[['Model', 'treats', 'Wikipedia']]",[],natural_language_inference,24,16
2482,model,"As a result , our approach is generic and could be switched to other collections of documents , books , or even daily updated newspapers .","[('could be', (9, 11)), ('switched', (11, 12)), ('to', (12, 13)), ('of', (15, 16))]","[('our approach', (4, 6)), ('generic', (7, 8)), ('other collections', (13, 15)), ('documents', (16, 17)), ('books', (18, 19)), ('daily updated newspapers', (22, 25))]","[['our approach', 'switched', 'other collections'], ['other collections', 'of', 'documents'], ['other collections', 'of', 'books'], ['other collections', 'of', 'daily updated newspapers']]","[['our approach', 'has', 'generic']]",[],"[['Model', 'has', 'our approach']]",natural_language_inference,24,17
2483,model,Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once .,"[('Having', (0, 1)), ('forces', (5, 6)), ('to be', (8, 10)), ('for', (14, 15))]","[('single knowledge source', (2, 5)), ('model', (7, 8)), ('very precise', (10, 12)), ('searching', (13, 14)), ('an answer', (15, 17))]","[['single knowledge source', 'forces', 'model'], ['model', 'to be', 'very precise'], ['searching', 'for', 'an answer']]","[['searching', 'has', 'an answer']]","[['Model', 'Having', 'single knowledge source']]",[],natural_language_inference,24,20
2484,experimental-setup,We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,"[('use', (1, 2)), ('with', (7, 8)), ('for', (13, 14))]","[('3 - layer bidirectional LSTMs', (2, 7)), ('h = 128 hidden units', (8, 13)), ('both paragraph and question encoding', (14, 19))]","[['3 - layer bidirectional LSTMs', 'with', 'h = 128 hidden units'], ['h = 128 hidden units', 'for', 'both paragraph and question encoding']]",[],"[['Experimental setup', 'use', '3 - layer bidirectional LSTMs']]",[],natural_language_inference,24,174
2485,experimental-setup,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .","[('apply', (1, 2)), ('for', (6, 7)), ('generating', (10, 11))]","[('Stanford CoreNLP toolkit', (3, 6)), ('tokenization', (7, 8)), ('lemma', (11, 12)), ('partof - speech', (13, 16)), ('named entity tags', (18, 21))]","[['Stanford CoreNLP toolkit', 'for', 'tokenization'], ['Stanford CoreNLP toolkit', 'generating', 'named entity tags']]",[],"[['Experimental setup', 'apply', 'Stanford CoreNLP toolkit']]",[],natural_language_inference,24,175
2486,experimental-setup,"Lastly , all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each .","[('sorted by', (7, 9)), ('of', (11, 12)), ('divided into', (14, 16)), ('of', (17, 18))]","[('all the training examples', (2, 6)), ('length', (10, 11)), ('paragraph', (12, 13)), ('minibatches', (16, 17)), ('32 examples each', (18, 21))]","[['all the training examples', 'sorted by', 'length'], ['length', 'of', 'paragraph'], ['minibatches', 'of', '32 examples each'], ['all the training examples', 'divided into', 'minibatches'], ['minibatches', 'of', '32 examples each']]",[],[],"[['Experimental setup', 'has', 'all the training examples']]",natural_language_inference,24,176
2487,experimental-setup,We use Adamax for optimization as described in .,"[('for', (3, 4))]","[('Adamax', (2, 3)), ('optimization', (4, 5))]","[['Adamax', 'for', 'optimization']]",[],[],[],natural_language_inference,24,177
2488,experimental-setup,Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,"[('with', (1, 2)), ('applied to', (6, 8)), ('of', (15, 16))]","[('Dropout', (0, 1)), ('p = 0.3', (2, 5)), ('word embeddings', (8, 10)), ('hidden units', (13, 15)), ('LSTMs', (16, 17))]","[['Dropout', 'with', 'p = 0.3'], ['Dropout', 'applied to', 'hidden units'], ['p = 0.3', 'applied to', 'word embeddings'], ['p = 0.3', 'applied to', 'hidden units'], ['hidden units', 'of', 'LSTMs']]","[['Dropout', 'has', 'p = 0.3']]",[],"[['Experimental setup', 'has', 'Dropout']]",natural_language_inference,24,178
2489,results,"Our system ( single model ) can achieve 70.0 % exact match and 79.0 % F 1 scores on the test set , which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing .","[('achieve', (7, 8)), ('on', (18, 19)), ('surpasses', (24, 25))]","[('Our system ( single model )', (0, 6)), ('70.0 %', (8, 10)), ('exact match', (10, 12)), ('79.0 %', (13, 15)), ('F 1 scores', (15, 18)), ('test set', (20, 22)), ('all the published results', (25, 29))]","[['Our system ( single model )', 'achieve', '70.0 %'], ['Our system ( single model )', 'achieve', '79.0 %'], ['F 1 scores', 'on', 'test set'], ['F 1 scores', 'surpasses', 'all the published results'], ['test set', 'surpasses', 'all the published results']]","[['70.0 %', 'has', 'exact match'], ['79.0 %', 'has', 'F 1 scores']]",[],"[['Results', 'has', 'Our system ( single model )']]",natural_language_inference,24,182
2490,results,"Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .","[('Without', (0, 1)), ('achieve', (23, 24))]","[('aligned question embedding feature', (2, 6)), ('our system', (17, 19)), ('F1', (24, 25)), ('over 77 %', (25, 28))]","[['our system', 'achieve', 'F1']]","[['aligned question embedding feature', 'has', 'our system'], ['F1', 'has', 'over 77 %']]","[['Results', 'Without', 'aligned question embedding feature']]",[],natural_language_inference,24,186
2491,ablation-analysis,"More interestingly , if we remove both f aligned and f exact match , the performance drops dramatically , so we conclude that both features play a similar but complementary role in the feature representation related to the paraphrased nature of a question vs. the context around an answer .","[('remove both', (5, 7))]","[('f aligned and f exact match', (7, 13)), ('performance', (15, 16)), ('drops', (16, 17)), ('dramatically', (17, 18))]",[],"[['f aligned and f exact match', 'has', 'performance'], ['performance', 'has', 'drops'], ['drops', 'has', 'dramatically']]","[['Ablation analysis', 'remove both', 'f aligned and f exact match']]",[],natural_language_inference,24,187
2492,research-problem,A Deep Cascade Model for Multi - Document Reading Comprehension,[],"[('Multi - Document Reading Comprehension', (5, 10))]",[],[],[],[],natural_language_inference,25,2
2493,research-problem,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .",[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[],natural_language_inference,25,13
2494,research-problem,"From the early cloze - style test to answer extraction from a single paragraph , and to the more complex open - domain question answering from web data , great efforts have been made to push the MRC technique to more practical applications .",[],"[('MRC', (37, 38))]",[],[],[],[],natural_language_inference,25,14
2495,model,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .","[('propose', (7, 8)), ('combines', (13, 14)), ('of', (16, 17)), ('in', (19, 20))]","[('deep cascade model', (9, 12)), ('advantages', (15, 16)), ('both methods', (17, 19)), ('coarse - to - fine manner', (21, 27))]","[['deep cascade model', 'combines', 'advantages'], ['advantages', 'of', 'both methods'], ['advantages', 'in', 'coarse - to - fine manner'], ['both methods', 'in', 'coarse - to - fine manner']]",[],"[['Model', 'propose', 'deep cascade model']]",[],natural_language_inference,25,29
2496,model,The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .,"[('designed to', (5, 7)), ('keep', (8, 9)), ('between', (11, 12))]","[('deep cascade model', (1, 4)), ('balance', (10, 11)), ('effectiveness and efficiency', (13, 16))]","[['deep cascade model', 'keep', 'balance'], ['balance', 'between', 'effectiveness and efficiency']]",[],[],"[['Model', 'has', 'deep cascade model']]",natural_language_inference,25,30
2497,model,"At early stages of the model , simple features and ranking functions are used to select a candidate set of most relevant contents , filtering out the irrelevant documents and paragraphs as much as possible .","[('of', (3, 4)), ('used to', (13, 15)), ('select', (15, 16)), ('filtering out', (24, 26))]","[('simple features and ranking functions', (7, 12)), ('candidate set', (17, 19)), ('most relevant contents', (20, 23)), ('irrelevant documents and paragraphs', (27, 31))]","[['candidate set', 'of', 'most relevant contents'], ['simple features and ranking functions', 'select', 'candidate set'], ['candidate set', 'filtering out', 'irrelevant documents and paragraphs']]",[],"[['Model', 'of', 'simple features and ranking functions']]",[],natural_language_inference,25,31
2498,model,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,"[('passed to', (5, 7)), ('for', (14, 15)), ('at', (20, 21))]","[('selected paragraphs', (2, 4)), ('attention - based deep MRC model', (8, 14)), ('extracting', (15, 16)), ('actual answer span', (17, 20)), ('word level', (21, 23))]","[['selected paragraphs', 'passed to', 'attention - based deep MRC model'], ['attention - based deep MRC model', 'for', 'extracting'], ['actual answer span', 'at', 'word level']]","[['extracting', 'has', 'actual answer span']]",[],"[['Model', 'has', 'selected paragraphs']]",natural_language_inference,25,32
2499,model,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .","[('To better support', (0, 3)), ('introduce', (9, 10)), ('as', (16, 17)), ('helps to', (22, 24)), ('narrow down', (25, 27))]","[('answer extraction', (4, 6)), ('document extraction and paragraph extraction', (11, 16)), ('two auxiliary tasks', (17, 20)), ('entire search space', (28, 31))]","[['answer extraction', 'introduce', 'document extraction and paragraph extraction'], ['document extraction and paragraph extraction', 'as', 'two auxiliary tasks']]",[],"[['Model', 'To better support', 'answer extraction']]",[],natural_language_inference,25,33
2500,model,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .","[('jointly optimize', (1, 3)), ('in', (7, 8)), ('shares', (15, 16))]","[('all the three tasks', (3, 7)), ('unified deep MRC model', (9, 13)), ('some common bottom layers', (16, 20))]","[['all the three tasks', 'in', 'unified deep MRC model'], ['unified deep MRC model', 'shares', 'some common bottom layers']]",[],"[['Model', 'jointly optimize', 'all the three tasks']]",[],natural_language_inference,25,34
2501,model,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .","[('enables', (3, 4)), ('to perform', (6, 8)), ('at', (15, 16)), ('learnt', (23, 24))]","[('cascaded structure', (1, 3)), ('models', (5, 6)), ('coarse - to - fine pruning', (9, 15)), ('different stages', (16, 18)), ('better models', (19, 21)), ('effectively and efficiently', (24, 27))]","[['cascaded structure', 'enables', 'models'], ['cascaded structure', 'to perform', 'coarse - to - fine pruning'], ['models', 'to perform', 'coarse - to - fine pruning'], ['coarse - to - fine pruning', 'at', 'different stages'], ['better models', 'learnt', 'effectively and efficiently']]",[],[],"[['Model', 'has', 'cascaded structure']]",natural_language_inference,25,35
2502,model,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .","[('consists of', (11, 13))]","[('overall framework', (1, 3)), ('our model', (4, 6)), ('document retrieval', (16, 18)), ('paragraph retrieval', (19, 21)), ('answer extraction', (22, 24))]",[],"[['overall framework', 'name', 'our model']]",[],"[['Model', 'has', 'overall framework']]",natural_language_inference,25,36
2503,model,The first module takes the question and a collection of raw documents as input .,"[('takes', (3, 4)), ('as', (12, 13))]","[('first module', (1, 3)), ('question and a collection of raw documents', (5, 12)), ('input', (13, 14))]","[['first module', 'takes', 'question and a collection of raw documents'], ['question and a collection of raw documents', 'as', 'input']]",[],[],"[['Model', 'has', 'first module']]",natural_language_inference,25,37
2504,model,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .","[('at', (2, 3)), ('consumes', (6, 7)), ('from', (9, 10)), ('further', (15, 16)), ('prunes', (16, 17)), ('given', (24, 25))]","[('module', (1, 2)), ('each subsequent stage', (3, 6)), ('output', (8, 9)), ('previous stage', (11, 13)), ('documents , paragraphs and answer spans', (18, 24)), ('question', (26, 27))]","[['module', 'at', 'each subsequent stage'], ['module', 'consumes', 'output'], ['each subsequent stage', 'consumes', 'output'], ['output', 'from', 'previous stage'], ['documents , paragraphs and answer spans', 'given', 'question']]","[['module', 'has', 'each subsequent stage']]",[],"[['Model', 'has', 'module']]",natural_language_inference,25,38
2505,model,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .","[('used as', (5, 7)), ('to discard', (10, 12))]","[('ranking function', (1, 3)), ('preliminary filter', (8, 10)), ('most of the irrelevant documents or paragraphs', (12, 19))]","[['ranking function', 'used as', 'preliminary filter'], ['preliminary filter', 'to discard', 'most of the irrelevant documents or paragraphs']]","[['ranking function', 'has', 'preliminary filter']]",[],"[['Model', 'has', 'ranking function']]",natural_language_inference,25,40
2506,model,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .","[('deal with', (7, 9)), ('with', (21, 22)), ('for', (27, 28))]","[('extraction function', (1, 3)), ('auxiliary document and paragraph extraction tasks', (10, 16)), ('jointly optimized', (19, 21)), ('final answer extraction module', (23, 27)), ('better extraction performance', (28, 31))]","[['extraction function', 'deal with', 'auxiliary document and paragraph extraction tasks'], ['jointly optimized', 'with', 'final answer extraction module'], ['final answer extraction module', 'for', 'better extraction performance']]",[],[],"[['Model', 'has', 'extraction function']]",natural_language_inference,25,41
2507,experimental-setup,We choose K = 4 and N = 2 for the good performance when evaluating on the dev set .,"[('choose', (1, 2)), ('for', (9, 10)), ('when', (13, 14)), ('on', (15, 16))]","[('K = 4 and N = 2', (2, 9)), ('good performance', (11, 13)), ('evaluating', (14, 15)), ('dev set', (17, 19))]","[['K = 4 and N = 2', 'for', 'good performance'], ['good performance', 'when', 'evaluating'], ['evaluating', 'on', 'dev set']]",[],"[['Experimental setup', 'choose', 'K = 4 and N = 2']]",[],natural_language_inference,25,208
2508,experimental-setup,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .","[('adopt', (8, 9)), ('for', (12, 13)), ('with', (15, 16)), ('of', (19, 20)), ('of', (25, 26))]","[('multi-task deep attention framework', (2, 6)), ('Adam optimizer', (10, 12)), ('training', (13, 14)), ('mini-batch size', (17, 19)), ('32', (20, 21)), ('initial learning rate', (22, 25)), ('0.0005', (26, 27))]","[['multi-task deep attention framework', 'adopt', 'Adam optimizer'], ['Adam optimizer', 'for', 'training'], ['Adam optimizer', 'with', 'mini-batch size'], ['Adam optimizer', 'with', 'initial learning rate'], ['training', 'with', 'mini-batch size'], ['training', 'with', 'initial learning rate'], ['mini-batch size', 'of', '32'], ['initial learning rate', 'of', '0.0005']]","[['mini-batch size', 'has', '32'], ['initial learning rate', 'has', '0.0005']]",[],[],natural_language_inference,25,211
2509,experimental-setup,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,"[('use', (1, 2)), ('in', (8, 9)), ('train', (11, 12)), ('with', (17, 18)), ('for', (22, 23))]","[('GloVe 300 dimensional word embeddings', (3, 8)), ('TriviaQA', (9, 10)), ('word2 vec word embeddings', (13, 17)), ('whole DuReader corpus', (19, 22)), ('DuReader', (23, 24))]","[['GloVe 300 dimensional word embeddings', 'in', 'TriviaQA'], ['word2 vec word embeddings', 'with', 'whole DuReader corpus'], ['whole DuReader corpus', 'for', 'DuReader']]",[],"[['Experimental setup', 'use', 'GloVe 300 dimensional word embeddings']]",[],natural_language_inference,25,212
2510,experimental-setup,The word embeddings are fixed during training .,"[('during', (5, 6))]","[('word embeddings', (1, 3)), ('fixed', (4, 5)), ('training', (6, 7))]","[['fixed', 'during', 'training']]","[['word embeddings', 'has', 'fixed']]",[],"[['Experimental setup', 'has', 'word embeddings']]",natural_language_inference,25,213
2511,experimental-setup,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,"[('of', (3, 4)), ('set as', (6, 8)), ('for', (9, 10)), ('for', (13, 14))]","[('hidden size', (1, 3)), ('LSTM', (4, 5)), ('150', (8, 9)), ('TriviaQA', (10, 11)), ('128', (12, 13)), ('DuReader', (14, 15))]","[['hidden size', 'of', 'LSTM'], ['hidden size', 'set as', '150'], ['LSTM', 'set as', '150'], ['150', 'for', 'TriviaQA'], ['128', 'for', 'DuReader'], ['128', 'for', 'DuReader']]",[],[],"[['Experimental setup', 'has', 'hidden size']]",natural_language_inference,25,214
2512,experimental-setup,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,"[('set as', (16, 18))]","[('task - specific hyper - parameters', (1, 7)), ('? 1 = ? 2 = 0.5', (18, 25)), ('Regularization parameter', (26, 28))]",[],[],[],"[['Experimental setup', 'has', 'task - specific hyper - parameters']]",natural_language_inference,25,215
2513,experimental-setup,16 is set as a small value of 0.01 .,"[('set as', (2, 4)), ('of', (7, 8))]","[('small value', (5, 7)), ('0.01', (8, 9))]","[['small value', 'of', '0.01']]",[],[],[],natural_language_inference,25,216
2514,experimental-setup,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,"[('trained on', (3, 5)), ('with', (9, 10)), ('in', (13, 14))]","[('All models', (0, 2)), ('Nvidia Tesla M40 GPU', (5, 9)), ('Cudnn LSTM cell', (10, 13)), ('Tensorflow 1.3', (14, 16))]","[['All models', 'trained on', 'Nvidia Tesla M40 GPU'], ['Nvidia Tesla M40 GPU', 'with', 'Cudnn LSTM cell'], ['Cudnn LSTM cell', 'in', 'Tensorflow 1.3']]",[],[],"[['Experimental setup', 'has', 'All models']]",natural_language_inference,25,217
2515,results,"We can see that by adopting the deep cascade learning framework , the proposed model outperforms the previous state - of - the - art methods by an evident margin on both datasets , which validates the effectiveness of the proposed method in addressing the challenging multi-document MRC task .","[('by', (4, 5)), ('adopting', (5, 6)), ('outperforms', (15, 16))]","[('deep cascade learning framework', (7, 11)), ('proposed model', (13, 15)), ('previous state - of - the - art methods', (17, 26)), ('evident margin', (28, 30))]","[['previous state - of - the - art methods', 'by', 'evident margin'], ['proposed model', 'outperforms', 'previous state - of - the - art methods']]","[['deep cascade learning framework', 'has', 'proposed model']]","[['Results', 'by', 'deep cascade learning framework']]",[],natural_language_inference,25,221
2516,ablation-analysis,"From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .","[('plays', (14, 15)), ('in', (18, 19)), ('among', (21, 22)), ('normalize', (37, 38)), ('from', (42, 43)), ('so that', (45, 47)), ('extracted from', (49, 51)), ('keep', (62, 63)), ('from', (66, 67))]","[('shared LSTM', (12, 14)), ('important role', (16, 18)), ('answer extraction', (19, 21)), ('multiple documents', (22, 24)), ('helps', (35, 36)), ('content probability score', (39, 42)), ('multiple documents', (43, 45)), ('answers', (48, 49)), ('different documents', (51, 53)), ('directly compared', (55, 57)), ('ranking order', (64, 66)), ('document ranking component', (67, 70))]","[['shared LSTM', 'plays', 'important role'], ['important role', 'in', 'answer extraction'], ['answer extraction', 'among', 'multiple documents'], ['helps', 'normalize', 'content probability score'], ['content probability score', 'from', 'multiple documents'], ['answers', 'extracted from', 'different documents'], ['shared LSTM', 'keep', 'ranking order'], ['ranking order', 'from', 'document ranking component']]","[['shared LSTM', 'has', 'important role']]",[],"[['Ablation analysis', 'has', 'shared LSTM']]",natural_language_inference,25,226
2517,ablation-analysis,"By incorporating the manual features , the performance can be further improved slightly .","[('incorporating', (1, 2))]","[('manual features', (3, 5)), ('performance', (7, 8)), ('further improved slightly', (10, 13))]",[],"[['manual features', 'has', 'performance'], ['performance', 'has', 'further improved slightly']]","[['Ablation analysis', 'incorporating', 'manual features']]",[],natural_language_inference,25,227
2518,ablation-analysis,"2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .","[('for', (14, 15)), ('serve', (20, 21)), ('between', (27, 28))]","[('preliminary cascade ranking and multi-task answer extraction strategy', (4, 12)), ('vital', (13, 14)), ('final performance', (16, 18)), ('good trade - off', (23, 27)), ('pure pipeline method', (29, 32)), ('fully joint learning method', (33, 37))]","[['vital', 'for', 'final performance'], ['final performance', 'serve', 'good trade - off'], ['good trade - off', 'between', 'pure pipeline method'], ['good trade - off', 'between', 'fully joint learning method']]","[['preliminary cascade ranking and multi-task answer extraction strategy', 'has', 'vital']]",[],"[['Ablation analysis', 'has', 'preliminary cascade ranking and multi-task answer extraction strategy']]",natural_language_inference,25,228
2519,ablation-analysis,"Jointly training the three extraction tasks can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .","[('Jointly training', (0, 2)), ('provide', (7, 8))]","[('three extraction tasks', (3, 6)), ('great benefits', (8, 10))]","[['three extraction tasks', 'provide', 'great benefits']]",[],"[['Ablation analysis', 'Jointly training', 'three extraction tasks']]",[],natural_language_inference,25,230
2520,research-problem,U - Net : Machine Reading Comprehension with Unanswerable Questions,[],"[('Machine Reading Comprehension with Unanswerable Questions', (4, 10))]",[],[],[],[],natural_language_inference,26,2
2521,research-problem,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .",[],"[('Machine reading comprehension ( MRC )', (0, 6))]",[],[],[],[],natural_language_inference,26,12
2522,research-problem,"Benefiting from the rapid development of deep learning techniques and large - scale benchmarks , the end - to - end neural methods have achieved promising results on MRC task .",[],"[('MRC', (28, 29))]",[],[],[],[],natural_language_inference,26,13
2523,research-problem,"In this paper , we decompose the problem of MRC with unanswerable questions into three sub - tasks : answer pointer , no - answer pointer , and answer verifier .",[],"[('three sub - tasks', (14, 18)), ('answer pointer', (19, 21)), ('no - answer pointer', (22, 26)), ('answer verifier', (28, 30))]",[],"[['three sub - tasks', 'name', 'answer pointer']]",[],[],natural_language_inference,26,37
2524,model,We propose the U - Net to incorporate these three sub - tasks into a unified model :,"[('propose', (1, 2)), ('to incorporate', (6, 8)), ('into', (13, 14))]","[('U - Net', (3, 6)), ('three sub - tasks', (9, 13)), ('unified model', (15, 17))]","[['U - Net', 'to incorporate', 'three sub - tasks'], ['three sub - tasks', 'into', 'unified model']]","[['U - Net', 'name', 'three sub - tasks']]","[['Model', 'propose', 'U - Net']]",[],natural_language_inference,26,39
2525,model,"1 ) an answer pointer to predict a can - didate answer span for a question ; 2 ) a no -answer pointer to avoid selecting any text span when a question has no answer ; and 3 ) an answer verifier to determine the probability of the "" unanswerability "" of a question with candidate answer information .","[('to predict', (5, 7)), ('for', (13, 14)), ('to avoid', (23, 25)), ('selecting', (25, 26)), ('when', (29, 30)), ('to determine', (42, 44)), ('of', (46, 47)), ('of', (51, 52)), ('with', (54, 55))]","[('answer pointer', (3, 5)), ('can - didate answer span', (8, 13)), ('question', (15, 16)), ('no -answer pointer', (20, 23)), ('any text span', (26, 29)), ('question', (31, 32)), ('no answer', (33, 35)), ('answer verifier', (40, 42)), ('probability', (45, 46)), ('unanswerability', (49, 50)), ('question', (53, 54)), ('candidate answer information', (55, 58))]","[['answer pointer', 'to predict', 'can - didate answer span'], ['can - didate answer span', 'for', 'question'], ['no -answer pointer', 'selecting', 'any text span'], ['any text span', 'when', 'question'], ['answer verifier', 'to determine', 'probability'], ['probability', 'of', 'unanswerability'], ['probability', 'of', 'question'], ['unanswerability', 'of', 'question'], ['question', 'with', 'candidate answer information']]",[],[],[],natural_language_inference,26,40
2526,model,"Additionally , we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens , which greatly improves the conciseness of U - Net .","[('introduce', (4, 5)), ('process', (9, 10)), ('as', (16, 17)), ('greatly improves', (25, 27)), ('of', (29, 30))]","[('universal node', (6, 8)), ('question and its context passage', (11, 16)), ('single contiguous sequence of tokens', (18, 23)), ('conciseness', (28, 29)), ('U - Net', (30, 33))]","[['question and its context passage', 'as', 'single contiguous sequence of tokens'], ['single contiguous sequence of tokens', 'greatly improves', 'conciseness'], ['conciseness', 'of', 'U - Net']]",[],"[['Model', 'introduce', 'universal node']]",[],natural_language_inference,26,41
2527,model,The universal node acts on both question and passage to learn whether the question is answerable .,"[('acts on', (3, 5)), ('to learn', (9, 11)), ('is', (14, 15))]","[('universal node', (1, 3)), ('both question and passage', (5, 9)), ('question', (13, 14)), ('answerable', (15, 16))]","[['universal node', 'acts on', 'both question and passage'], ['question', 'is', 'answerable']]",[],[],"[['Model', 'has', 'universal node']]",natural_language_inference,26,42
2528,experimental-setup,"We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .","[('use', (1, 2)), ('to process', (3, 5)), ('to obtain', (9, 11))]","[('Spacy', (2, 3)), ('each question and passage', (5, 9)), ('tokens', (11, 12)), ('POS tags', (13, 15)), ('NER tags', (16, 18)), ('lemmas tags', (19, 21))]","[['Spacy', 'to process', 'each question and passage'], ['each question and passage', 'to obtain', 'tokens']]",[],"[['Experimental setup', 'use', 'Spacy']]",[],natural_language_inference,26,180
2529,experimental-setup,"We use 12 dimensions to embed POS tags , 8 for NER tags .","[('to embed', (4, 6)), ('for', (10, 11))]","[('12 dimensions', (2, 4)), ('POS tags', (6, 8)), ('NER tags', (11, 13))]","[['12 dimensions', 'to embed', 'POS tags'], ['POS tags', 'for', 'NER tags']]",[],[],[],natural_language_inference,26,181
2530,experimental-setup,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .","[('between', (16, 17))]","[('3 binary features', (2, 5)), ('exact match', (6, 8)), ('lower - case match', (9, 13)), ('lemma match', (14, 16)), ('question and passage', (18, 21))]","[['3 binary features', 'between', 'question and passage'], ['lemma match', 'between', 'question and passage']]",[],[],[],natural_language_inference,26,182
2531,experimental-setup,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,[],"[('100 - dim Glove pretrained word embeddings', (2, 9)), ('1024 - dim Elmo embeddings', (10, 15))]",[],[],[],[],natural_language_inference,26,183
2532,experimental-setup,All the LSTM blocks are bi-directional with one single layer .,"[('are', (4, 5)), ('with', (6, 7))]","[('All the LSTM blocks', (0, 4)), ('bi-directional', (5, 6)), ('one single layer', (7, 10))]","[['All the LSTM blocks', 'are', 'bi-directional'], ['bi-directional', 'with', 'one single layer']]","[['All the LSTM blocks', 'has', 'bi-directional']]",[],"[['Experimental setup', 'has', 'All the LSTM blocks']]",natural_language_inference,26,184
2533,experimental-setup,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .","[('set', (1, 2)), ('as', (6, 7)), ('as', (12, 13))]","[('hidden layer dimension', (3, 6)), ('125', (7, 8)), ('attention layer dimension', (9, 12)), ('250', (13, 14))]","[['hidden layer dimension', 'as', '125'], ['hidden layer dimension', 'as', '250'], ['attention layer dimension', 'as', '250']]","[['hidden layer dimension', 'has', '125'], ['attention layer dimension', 'has', '250']]","[['Experimental setup', 'set', 'hidden layer dimension']]",[],natural_language_inference,26,185
2534,experimental-setup,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .","[('added', (1, 2)), ('overall', (5, 6)), ('including', (10, 11)), ('at', (15, 16)), ('of', (19, 20))]","[('dropout layer', (3, 5)), ('modeling layers', (7, 9)), ('embedding layer', (12, 14)), ('dropout rate', (17, 19)), ('0.3', (20, 21))]","[['dropout layer', 'overall', 'modeling layers'], ['modeling layers', 'including', 'embedding layer'], ['dropout layer', 'at', 'dropout rate'], ['dropout rate', 'of', '0.3']]","[['dropout layer', 'has', 'modeling layers'], ['dropout rate', 'has', '0.3']]","[['Experimental setup', 'added', 'dropout layer']]",[],natural_language_inference,26,186
2535,experimental-setup,We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,"[('with', (4, 5)), ('of', (8, 9))]","[('Adam optimizer', (2, 4)), ('learning rate', (6, 8)), ('0.002', (9, 10))]","[['Adam optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.002']]",[],[],[],natural_language_inference,26,187
2536,results,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .","[('of', (7, 8)), ('of', (13, 14)), ('on', (15, 16)), ('of', (25, 26)), ('of', (31, 32))]","[('Our model', (0, 2)), ('F 1 score', (4, 7)), ('74.0', (8, 9)), ('EM score', (11, 13)), ('70.3', (14, 15)), ('development set', (17, 19)), ('F 1 score', (22, 25)), ('72.6', (26, 27)), ('EM score', (29, 31)), ('69.2', (32, 33)), ('Test set', (34, 36))]","[['F 1 score', 'of', '74.0'], ['EM score', 'of', '70.3'], ['F 1 score', 'of', '72.6'], ['EM score', 'of', '70.3'], ['F 1 score', 'of', '72.6'], ['EM score', 'of', '69.2'], ['70.3', 'on', 'development set'], ['69.2', 'on', 'Test set'], ['F 1 score', 'of', '72.6'], ['EM score', 'of', '69.2']]",[],[],"[['Results', 'has', 'Our model']]",natural_language_inference,26,191
2537,results,Our model outperforms most of the previous approaches .,"[('outperforms', (2, 3))]","[('most of the previous approaches', (3, 8))]",[],[],[],[],natural_language_inference,26,192
2538,results,"Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .","[('Comparing to', (0, 2))]","[('best - performing systems', (3, 7)), ('our model', (8, 10)), ('simple architecture', (12, 14)), ('end - to - end model', (17, 23))]",[],"[['best - performing systems', 'has', 'our model'], ['our model', 'has', 'simple architecture']]","[['Results', 'Comparing to', 'best - performing systems']]",[],natural_language_inference,26,193
2539,results,"In fact , among all the end - to - end models , we achieve the best F1 scores .","[('among', (3, 4)), ('achieve', (14, 15))]","[('all the end - to - end models', (4, 12)), ('best F1 scores', (16, 19))]","[['all the end - to - end models', 'achieve', 'best F1 scores']]",[],"[['Results', 'among', 'all the end - to - end models']]",[],natural_language_inference,26,194
2540,ablation-analysis,"Our results showed that when node U is shared , as it is called ' universal ' , it learns information interaction between the question and passage , and when it is not shared , the performance slightly degraded .","[('when', (4, 5)), ('is', (7, 8)), ('called', (13, 14)), ('learns', (19, 20)), ('between', (22, 23))]","[('node U', (5, 7)), ('shared', (8, 9)), ('universal', (15, 16)), ('information interaction', (20, 22)), ('question and passage', (24, 27)), ('not shared', (32, 34)), ('performance', (36, 37)), ('slightly degraded', (37, 39))]","[['node U', 'is', 'shared'], ['node U', 'called', 'universal'], ['shared', 'called', 'universal'], ['universal', 'learns', 'information interaction'], ['information interaction', 'between', 'question and passage']]","[['node U', 'has', 'shared'], ['not shared', 'has', 'performance'], ['performance', 'has', 'slightly degraded']]","[['Ablation analysis', 'when', 'node U']]",[],natural_language_inference,26,203
2541,ablation-analysis,"Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .","[('sharing', (9, 10)), ('improve', (16, 17)), ('of', (19, 20))]","[('BiLSTM', (10, 11)), ('effective method', (13, 15)), ('quality', (18, 19)), ('encoder', (21, 22))]","[['effective method', 'improve', 'quality'], ['quality', 'of', 'encoder']]",[],"[['Ablation analysis', 'sharing', 'BiLSTM']]",[],natural_language_inference,26,206
2542,ablation-analysis,"After removing the plausible answer pointer , the performance also dropped , indicating the plausible answers are useful to improve the model even though they are incorrect .","[('After removing', (0, 2))]","[('plausible answer pointer', (3, 6)), ('performance', (8, 9)), ('dropped', (10, 11))]",[],"[['plausible answer pointer', 'has', 'performance'], ['performance', 'has', 'dropped']]","[['Ablation analysis', 'After removing', 'plausible answer pointer']]",[],natural_language_inference,26,207
2543,ablation-analysis,"After removing the answer verifier , the performance dropped greatly , indicating it is vital for our model .","[('dropped', (8, 9))]","[('answer verifier', (3, 5)), ('performance', (7, 8)), ('greatly', (9, 10))]","[['performance', 'dropped', 'greatly']]","[['answer verifier', 'has', 'performance'], ['performance', 'has', 'greatly']]",[],[],natural_language_inference,26,208
2544,ablation-analysis,"In the second block ( multi - level attention ) of the U - Net , we do not split the output of the encoded presentation and let it pass through a self - attention layer .","[('In', (0, 1)), ('of', (10, 11)), ('split', (19, 20)), ('of', (22, 23)), ('pass through', (29, 31))]","[('second block ( multi - level attention )', (2, 10)), ('U - Net', (12, 15)), ('output', (21, 22)), ('encoded presentation', (24, 26)), ('self - attention layer', (32, 36))]","[['second block ( multi - level attention )', 'of', 'U - Net'], ['output', 'of', 'encoded presentation'], ['second block ( multi - level attention )', 'split', 'output'], ['output', 'of', 'encoded presentation'], ['second block ( multi - level attention )', 'pass through', 'self - attention layer']]","[['second block ( multi - level attention )', 'has', 'U - Net']]","[['Ablation analysis', 'In', 'second block ( multi - level attention )']]",[],natural_language_inference,26,210
2545,research-problem,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING,[],"[('CONVERSATIONAL QUESTION AN - SWERING', (9, 14))]",[],[],[],[],natural_language_inference,27,2
2546,research-problem,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,[],"[('Conversational question answering ( CQA )', (0, 6))]",[],[],[],[],natural_language_inference,27,4
2547,research-problem,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",[],"[('machine reading comprehension ( MRC )', (6, 12)), ('CQA', (14, 15))]",[],[],[],[],natural_language_inference,27,5
2548,model,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .","[('propose', (5, 6)), ('for', (16, 17)), ('of', (19, 20))]","[('SDNet', (6, 7)), ('contextual attention - based deep neural network', (9, 16)), ('task', (18, 19)), ('conversational question answering', (20, 23))]","[['contextual attention - based deep neural network', 'for', 'task'], ['task', 'of', 'conversational question answering']]","[['SDNet', 'has', 'contextual attention - based deep neural network']]","[['Model', 'propose', 'SDNet']]",[],natural_language_inference,27,19
2549,model,"Firstly , we apply both inter-attention and self - attention on passage and question to obtain a more effective understanding of the passage and dialogue history .","[('apply', (3, 4)), ('on', (10, 11)), ('to obtain', (14, 16)), ('of', (20, 21))]","[('both inter-attention and self - attention', (4, 10)), ('passage and question', (11, 14)), ('more effective understanding', (17, 20)), ('passage and dialogue history', (22, 26))]","[['both inter-attention and self - attention', 'on', 'passage and question'], ['passage and question', 'to obtain', 'more effective understanding'], ['more effective understanding', 'of', 'passage and dialogue history']]",[],"[['Model', 'apply', 'both inter-attention and self - attention']]",[],natural_language_inference,27,21
2550,model,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .","[('leverages', (3, 4)), ('in', (7, 8))]","[('SDNet', (2, 3)), ('latest breakthrough', (5, 7)), ('NLP', (8, 9)), ('BERT contextual embedding', (10, 13))]","[['SDNet', 'leverages', 'latest breakthrough'], ['latest breakthrough', 'in', 'NLP']]",[],[],"[['Model', 'has', 'SDNet']]",natural_language_inference,27,22
2551,model,"Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .","[('of', (5, 6)), ('employed', (18, 19)), ('with', (27, 28))]","[('weighted sum', (20, 22)), ('BERT layer outputs', (23, 26)), ('locked BERT parameters', (28, 31))]","[['weighted sum', 'of', 'BERT layer outputs'], ['BERT layer outputs', 'with', 'locked BERT parameters']]",[],[],[],natural_language_inference,27,23
2552,model,"Thirdly , we prepend previous rounds of questions and answers to the current question to incorporate contextual information .","[('prepend', (3, 4)), ('of', (6, 7)), ('to', (10, 11)), ('to incorporate', (14, 16))]","[('previous rounds', (4, 6)), ('questions and answers', (7, 10)), ('current question', (12, 14)), ('contextual information', (16, 18))]","[['previous rounds', 'of', 'questions and answers'], ['questions and answers', 'to', 'current question'], ['current question', 'to incorporate', 'contextual information']]",[],"[['Model', 'prepend', 'previous rounds']]",[],natural_language_inference,27,24
2553,results,"As shown , SDNet achieves significantly better results than baseline models .","[('than', (8, 9))]","[('significantly better results', (5, 8)), ('baseline models', (9, 11))]","[['significantly better results', 'than', 'baseline models']]",[],[],[],natural_language_inference,27,128
2554,results,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .","[('improves', (7, 8)), ('by', (11, 12)), ('compared with', (15, 17)), ('on', (24, 25))]","[('single SDNet model', (4, 7)), ('overall F 1', (8, 11)), ('1.6 %', (12, 14)), ('previous state - of - art model', (17, 24)), ('CoQA', (25, 26)), ('Flow QA', (27, 29))]","[['single SDNet model', 'improves', 'overall F 1'], ['overall F 1', 'by', '1.6 %'], ['previous state - of - art model', 'on', 'CoQA'], ['previous state - of - art model', 'on', 'Flow QA']]",[],[],[],natural_language_inference,27,129
2555,results,"Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .","[('further improves', (3, 5)), ('overall', (5, 6)), ('by', (9, 10)), ('achieve', (20, 21)), ('on', (27, 28))]","[('Ensemble SDNet model', (0, 3)), ('F 1 score', (6, 9)), ('2.7 %', (10, 12)), ('over 80 % F 1 score', (21, 27)), ('in - domain datasets ( 80.7 % )', (28, 36))]","[['F 1 score', 'by', '2.7 %'], ['over 80 % F 1 score', 'on', 'in - domain datasets ( 80.7 % )']]",[],[],"[['Results', 'has', 'Ensemble SDNet model']]",natural_language_inference,27,130
2556,results,"As seen , SDNet overpasses all but one baseline models after the second epoch , and achieves state - of - the - art results only after 8 epochs .","[('overpasses', (4, 5)), ('after', (10, 11)), ('achieves', (16, 17)), ('only after', (25, 27))]","[('SDNet', (3, 4)), ('all but one baseline models', (5, 10)), ('second epoch', (12, 14)), ('state - of - the - art results', (17, 25)), ('8 epochs', (27, 29))]","[['SDNet', 'overpasses', 'all but one baseline models'], ['all but one baseline models', 'after', 'second epoch'], ['SDNet', 'achieves', 'state - of - the - art results'], ['state - of - the - art results', 'only after', '8 epochs']]",[],[],"[['Results', 'has', 'SDNet']]",natural_language_inference,27,132
2557,research-problem,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS,[],"[('TRACKING THE WORLD STATE', (0, 4))]",[],[],[],[],natural_language_inference,28,2
2558,research-problem,"In this paper , we investigate this problem through a simple story understanding scenario , in which the agent is given a sequence of textual statements and events , and then given another series of statements about the final state of the world .",[],"[('simple story understanding scenario', (10, 14))]",[],[],[],[],natural_language_inference,28,23
2559,model,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,"[('with', (6, 7)), ('uses', (17, 18))]","[('new kind of memory - augmented neural network', (8, 16)), ('distributed memory and processor architecture', (19, 24)), ('Recurrent Entity Network ( EntNet )', (26, 32))]","[['new kind of memory - augmented neural network', 'uses', 'distributed memory and processor architecture']]","[['distributed memory and processor architecture', 'name', 'Recurrent Entity Network ( EntNet )']]","[['Model', 'with', 'new kind of memory - augmented neural network']]",[],natural_language_inference,28,28
2560,model,"The model consists of a fixed number of dynamic memory cells , each containing a vector key w j and a vector value ( or content ) h j .","[('consists of', (2, 4)), ('of', (7, 8)), ('containing', (13, 14))]","[('fixed number', (5, 7)), ('dynamic memory cells', (8, 11)), ('vector key w j', (15, 19)), ('vector value ( or content ) h j', (21, 29))]","[['fixed number', 'of', 'dynamic memory cells'], ['dynamic memory cells', 'containing', 'vector key w j'], ['dynamic memory cells', 'containing', 'vector value ( or content ) h j']]",[],"[['Model', 'consists of', 'fixed number']]",[],natural_language_inference,28,29
2561,model,"Each cell is associated with its own "" processor "" , a simple gated recurrent network that may update the cell value given an input .","[('associated with', (3, 5)), ('may update', (17, 19)), ('given', (22, 23))]","[('Each cell', (0, 2)), ('own "" processor ""', (6, 10)), ('simple gated recurrent network', (12, 16)), ('cell value', (20, 22)), ('input', (24, 25))]","[['Each cell', 'associated with', 'own "" processor ""'], ['simple gated recurrent network', 'may update', 'cell value'], ['cell value', 'given', 'input']]","[['own "" processor ""', 'has', 'simple gated recurrent network']]",[],"[['Model', 'has', 'Each cell']]",natural_language_inference,28,30
2562,model,"Alternatively , the EntNet can be seen as a bank of gated RNNs ( all sharing the same parameters ) , whose hidden states correspond to latent concepts and attributes , and whose parameters describe the laws of the world according to which the attributes of objects are updated .","[('can be seen as', (4, 8)), ('of', (10, 11)), ('correspond to', (24, 26)), ('describe', (34, 35)), ('of', (37, 38))]","[('EntNet', (3, 4)), ('bank', (9, 10)), ('gated RNNs', (11, 13)), ('parameters', (18, 19)), ('hidden states', (22, 24)), ('latent concepts and attributes', (26, 30)), ('laws', (36, 37)), ('world', (39, 40))]","[['EntNet', 'can be seen as', 'bank'], ['bank', 'of', 'gated RNNs'], ['hidden states', 'correspond to', 'latent concepts and attributes'], ['laws', 'of', 'world']]","[['EntNet', 'has', 'bank']]",[],"[['Model', 'has', 'EntNet']]",natural_language_inference,28,33
2563,model,"Their hidden state is updated only when new information relevant to their concept is received , and remains otherwise unchanged .","[('when', (6, 7)), ('relevant to', (9, 11))]","[('hidden state', (1, 3)), ('updated', (4, 5)), ('new information', (7, 9)), ('concept', (12, 13)), ('received', (14, 15))]","[['updated', 'when', 'new information'], ['new information', 'relevant to', 'concept']]","[['hidden state', 'has', 'updated']]",[],"[['Model', 'has', 'hidden state']]",natural_language_inference,28,35
2564,model,"The keys used in the addressing / gating mechanism also correspond to concepts or entities , but are modified only during learning , not during inference .","[('used in', (2, 4)), ('correspond to', (10, 12)), ('modified only during', (18, 21)), ('not during', (23, 25))]","[('keys', (1, 2)), ('addressing / gating mechanism', (5, 9)), ('concepts or entities', (12, 15)), ('learning', (21, 22)), ('inference', (25, 26))]","[['keys', 'used in', 'addressing / gating mechanism'], ['addressing / gating mechanism', 'correspond to', 'concepts or entities'], ['keys', 'modified only during', 'learning'], ['learning', 'not during', 'inference']]",[],[],"[['Model', 'has', 'keys']]",natural_language_inference,28,36
2565,experiments,SYNTHETIC WORLD MODEL TASK,[],"[('SYNTHETIC WORLD MODEL TASK', (0, 4))]",[],[],[],[],natural_language_inference,28,127
2566,hyperparameters,"For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .","[('For', (0, 1)), ('set', (5, 6)), ('equal to', (10, 12)), ('to', (19, 20))]","[('MemN2N', (2, 3)), ('number of hops', (7, 10)), ('T ? 2', (12, 15)), ('embedding dimension', (17, 19)), ('d = 20', (20, 23))]","[['MemN2N', 'set', 'number of hops'], ['number of hops', 'equal to', 'T ? 2'], ['embedding dimension', 'to', 'd = 20']]","[['number of hops', 'has', 'T ? 2'], ['embedding dimension', 'has', 'd = 20']]","[['Hyperparameters', 'For', 'MemN2N']]",[],natural_language_inference,28,134
2567,hyperparameters,"The EntNet had embedding dimension d = 20 and 5 memory slots , and the LSTM had 50 hidden units which resulted in it having significantly more parameters than the other two models .","[('resulted in', (21, 23)), ('than', (28, 29))]","[('EntNet', (1, 2)), ('embedding dimension d = 20', (3, 8)), ('5 memory slots', (9, 12)), ('LSTM', (15, 16)), ('50 hidden units', (17, 20)), ('significantly', (25, 26)), ('more parameters', (26, 28)), ('other two models', (30, 33))]","[['more parameters', 'than', 'other two models']]","[['EntNet', 'has', 'embedding dimension d = 20'], ['LSTM', 'has', '50 hidden units'], ['significantly', 'has', 'more parameters']]",[],"[['Hyperparameters', 'has', 'EntNet']]",natural_language_inference,28,135
2568,hyperparameters,"All models were trained with ADAM with initial learning rates set by grid search over { 0.1 , 0.01 , 0.001 } and divided by 2 every 10,000 updates .","[('trained with', (3, 5)), ('with', (6, 7)), ('set by', (10, 12)), ('over', (14, 15)), ('divided by', (23, 25)), ('every', (26, 27))]","[('All models', (0, 2)), ('ADAM', (5, 6)), ('initial learning rates', (7, 10)), ('grid search', (12, 14)), ('{ 0.1 , 0.01 , 0.001 }', (15, 22)), ('2', (25, 26)), ('10,000 updates', (27, 29))]","[['All models', 'trained with', 'ADAM'], ['ADAM', 'with', 'initial learning rates'], ['initial learning rates', 'set by', 'grid search'], ['grid search', 'over', '{ 0.1 , 0.01 , 0.001 }'], ['grid search', 'divided by', '2'], ['2', 'every', '10,000 updates']]",[],[],"[['Hyperparameters', 'has', 'All models']]",natural_language_inference,28,137
2569,results,"The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .","[('as', (10, 11)), ('of', (13, 14))]","[('MemN2N', (1, 2)), ('worst performance', (4, 6)), ('degrades quickly', (8, 10)), ('length', (12, 13)), ('sequence', (15, 16)), ('increases', (16, 17))]","[['degrades quickly', 'as', 'length'], ['length', 'of', 'sequence']]","[['MemN2N', 'has', 'worst performance'], ['worst performance', 'has', 'degrades quickly'], ['sequence', 'has', 'increases']]",[],"[['Results', 'has', 'MemN2N']]",natural_language_inference,28,139
2570,results,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .","[('performs', (2, 3)), ('as', (9, 10)), ('of', (12, 13))]","[('LSTM', (1, 2)), ('better', (3, 4)), ('loses accuracy', (7, 9)), ('length', (11, 12)), ('sequence', (14, 15)), ('increases', (15, 16))]","[['LSTM', 'performs', 'better'], ['loses accuracy', 'as', 'length'], ['length', 'of', 'sequence']]","[['sequence', 'has', 'increases']]",[],"[['Results', 'has', 'LSTM']]",natural_language_inference,28,140
2571,experiments,CHILDRE N'S BOOK TEST ( CBT ),[],"[(""CHILDRE N'S BOOK TEST ( CBT )"", (0, 7))]",[],[],[],[],natural_language_inference,28,171
2572,results,"It was shown in that methods with limited memory such as LSTMs perform well on more frequent , syntax based words such as prepositions and verbs , being similar to human performance , but poorly relative to humans on more semantically meaningful words such as named entities and common nouns .","[('with', (6, 7)), ('such as', (9, 11)), ('perform', (12, 13)), ('on', (14, 15)), ('such as', (21, 23))]","[('methods', (5, 6)), ('limited memory', (7, 9)), ('LSTMs', (11, 12)), ('well', (13, 14)), ('more frequent , syntax based words', (15, 21)), ('prepositions and verbs', (23, 26))]","[['methods', 'with', 'limited memory'], ['limited memory', 'such as', 'LSTMs'], ['limited memory', 'perform', 'well'], ['well', 'on', 'more frequent , syntax based words'], ['more frequent , syntax based words', 'such as', 'prepositions and verbs']]","[['methods', 'has', 'limited memory']]",[],[],natural_language_inference,28,175
2573,research-problem,PHASE CONDUCTOR ON MULTI - LAYERED ATTEN - TIONS FOR MACHINE COMPREHENSION,[],"[('MACHINE COMPREHENSION', (10, 12))]",[],[],[],[],natural_language_inference,29,2
2574,research-problem,"Benefiting from the availability of large - scale benchmark datasets such as SQuAD , the attention - based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors .",[],"[('question answering', (26, 28))]",[],[],[],[],natural_language_inference,29,13
2575,model,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .","[('introduce', (12, 13)), ('for', (17, 18)), ('of', (20, 21))]","[('general framework PhaseCond', (14, 17)), ('use', (19, 20)), ('multiple attention layers', (21, 24))]","[['general framework PhaseCond', 'for', 'use'], ['use', 'of', 'multiple attention layers']]",[],"[['Model', 'introduce', 'general framework PhaseCond']]",[],natural_language_inference,29,19
2576,model,"First , previous research on the self - attention model is to purely capture long - distance dependencies , and therefore a multi-hops architecture is used to alternatively captures question - aware passage representations and refines the results by using a self - attention model .","[('used to', (25, 27)), ('captures', (28, 29)), ('refines', (35, 36)), ('by using', (38, 40))]","[('self - attention model', (6, 10)), ('multi-hops architecture', (22, 24)), ('question - aware passage representations', (29, 34)), ('results', (37, 38))]","[['multi-hops architecture', 'captures', 'question - aware passage representations'], ['multi-hops architecture', 'refines', 'results']]",[],[],"[['Model', 'has', 'self - attention model']]",natural_language_inference,29,21
2577,model,"Second , unlike the domains such as machine translation which jointly align and translate words , question - passage attention models for machine comprehension and question answering calculate the alignment matrix corresponding to all question and passage word pairs .","[('such as', (5, 7)), ('jointly align and translate', (10, 14)), ('for', (21, 22)), ('calculate', (27, 28)), ('corresponding to', (31, 33))]","[('domains', (4, 5)), ('machine translation', (7, 9)), ('words , question - passage attention models', (14, 21)), ('machine comprehension', (22, 24)), ('question answering', (25, 27)), ('alignment matrix', (29, 31)), ('all question and passage word pairs', (33, 39))]","[['domains', 'such as', 'machine translation'], ['machine translation', 'jointly align and translate', 'words , question - passage attention models'], ['words , question - passage attention models', 'for', 'machine comprehension'], ['words , question - passage attention models', 'for', 'question answering'], ['question answering', 'calculate', 'alignment matrix'], ['alignment matrix', 'corresponding to', 'all question and passage word pairs']]",[],[],"[['Model', 'has', 'domains']]",natural_language_inference,29,24
2578,results,"The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .","[('of', (3, 4)), ('than', (10, 11))]","[('EM result', (1, 3)), ('baseline Iterative Aligner', (5, 8)), ('lower', (9, 10)), ('RNET', (11, 12))]","[['EM result', 'of', 'baseline Iterative Aligner'], ['lower', 'than', 'RNET']]","[['EM result', 'has', 'baseline Iterative Aligner']]",[],"[['Results', 'has', 'EM result']]",natural_language_inference,29,126
2579,results,"Our explanations is that 1 ) RNET uses a different feature set ( e.g. , Glo Ve 300 dimensional word vectors are employed ) and different encoding steps ( e.g. , three GRU layers are used for encoding question and passage representations ) , and 2 ) RNET uses a different ensemble method from our implementation .",[],"[('RNET', (6, 7))]",[],[],[],"[['Results', 'has', 'RNET']]",natural_language_inference,29,127
2580,results,shows the performance with different number of layers for both question - passage attention phase and self - attention phase .,"[('shows', (0, 1)), ('with', (3, 4)), ('for', (8, 9))]","[('performance', (2, 3)), ('different number of layers', (4, 8)), ('question - passage attention phase', (10, 15)), ('self - attention phase', (16, 20))]","[['performance', 'with', 'different number of layers'], ['different number of layers', 'for', 'question - passage attention phase'], ['different number of layers', 'for', 'self - attention phase']]","[['performance', 'has', 'different number of layers']]","[['Results', 'shows', 'performance']]",[],natural_language_inference,29,128
2581,results,"For the question - passage attention phase , using single layer does n't degrade the performance significantly from the default setting of two layers , resulting in a different conclusion from ; .","[('For', (0, 1)), ('using', (8, 9)), (""does n't degrade"", (11, 14)), ('from', (17, 18)), ('of', (21, 22)), ('in', (26, 27))]","[('question - passage attention phase', (2, 7)), ('single layer', (9, 11)), ('performance', (15, 16)), ('default setting', (19, 21)), ('two layers', (22, 24)), ('resulting', (25, 26)), ('different conclusion from', (28, 31))]","[['question - passage attention phase', 'using', 'single layer'], ['single layer', ""does n't degrade"", 'performance'], ['performance', 'from', 'default setting'], ['default setting', 'of', 'two layers'], ['resulting', 'in', 'different conclusion from']]","[['question - passage attention phase', 'has', 'single layer'], ['resulting', 'has', 'different conclusion from']]","[['Results', 'For', 'question - passage attention phase']]",[],natural_language_inference,29,130
2582,baselines,"In contrast , multiple stacking layers are needed to allow the evidence fully propagated through the passage .","[('to allow', (8, 10)), ('through', (14, 15))]","[('multiple stacking layers', (3, 6)), ('needed', (7, 8)), ('evidence', (11, 12)), ('fully propagated', (12, 14)), ('passage', (16, 17))]","[['multiple stacking layers', 'to allow', 'evidence'], ['needed', 'to allow', 'evidence'], ['fully propagated', 'through', 'passage']]","[['multiple stacking layers', 'has', 'needed'], ['evidence', 'has', 'fully propagated']]",[],[],natural_language_inference,29,132
2583,research-problem,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering,[],"[('Neural - Network - Based Question Answering', (6, 13))]",[],[],[],[],natural_language_inference,3,2
2584,research-problem,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .,[],"[('machine comprehension ( MC )', (15, 20)), ('question answering ( QA )', (21, 26))]",[],[],[],[],natural_language_inference,3,4
2585,model,"In this paper , we take a closer look at modeling questions in such an end - to - end neural network framework , since we regard question understanding is of importance for such problems .","[('take', (5, 6)), ('at', (9, 10)), ('in', (12, 13))]","[('closer look', (7, 9)), ('modeling questions', (10, 12)), ('end - to - end neural network framework', (15, 23))]","[['closer look', 'at', 'modeling questions'], ['modeling questions', 'in', 'end - to - end neural network framework']]","[['modeling questions', 'has', 'end - to - end neural network framework']]","[['Model', 'take', 'closer look']]",[],natural_language_inference,3,13
2586,model,We first introduced syntactic information to help encode questions .,"[('introduced', (2, 3)), ('to help', (5, 7)), ('encode', (7, 8))]","[('syntactic information', (3, 5)), ('questions', (8, 9))]",[],[],"[['Model', 'introduced', 'syntactic information']]",[],natural_language_inference,3,14
2587,model,We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them .,"[('viewed and modelled', (2, 5)), ('of', (7, 8)), ('among them', (13, 15)), ('as an', (15, 17))]","[('different types', (5, 7)), ('questions', (8, 9)), ('information', (11, 12)), ('shared', (12, 13)), ('adaptation problem', (17, 19))]","[['different types', 'of', 'questions'], ['shared', 'among them', 'adaptation problem'], ['information', 'as an', 'adaptation problem'], ['shared', 'as an', 'adaptation problem']]","[['information', 'has', 'shared']]","[['Model', 'viewed and modelled', 'different types']]",[],natural_language_inference,3,15
2588,results,We test our models on Stanford Question Answering Dataset ( SQuAD ) .,"[('test', (1, 2)), ('on', (4, 5))]","[('our models', (2, 4)), ('Stanford Question Answering Dataset ( SQuAD )', (5, 12))]","[['our models', 'on', 'Stanford Question Answering Dataset ( SQuAD )']]",[],"[['Results', 'test', 'our models']]",[],natural_language_inference,3,163
2589,results,"The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles , and the answer to each question is a span of text in the Wikipedia articles .","[('consists of', (3, 5)), ('annotated by', (9, 11)), ('on', (13, 14)), ('of', (17, 18))]","[('SQuAD dataset', (1, 3)), ('more than 100,000 questions', (5, 9)), ('crowdsourcing workers', (11, 13)), ('selected set', (15, 17)), ('Wikipedia articles', (18, 20))]","[['SQuAD dataset', 'consists of', 'more than 100,000 questions'], ['more than 100,000 questions', 'annotated by', 'crowdsourcing workers'], ['crowdsourcing workers', 'on', 'selected set'], ['selected set', 'of', 'Wikipedia articles']]",[],[],"[['Results', 'has', 'SQuAD dataset']]",natural_language_inference,3,164
2590,hyperparameters,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,"[('use', (1, 2)), ('to initialize', (9, 11))]","[('pre-trained 300 - D Glove 840B vectors', (2, 9)), ('our word embeddings', (11, 14))]","[['pre-trained 300 - D Glove 840B vectors', 'to initialize', 'our word embeddings']]",[],"[['Hyperparameters', 'use', 'pre-trained 300 - D Glove 840B vectors']]",[],natural_language_inference,3,168
2591,hyperparameters,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,"[('initialized', (10, 11)), ('with', (12, 13))]","[('Out - of - vocabulary ( OOV ) words', (0, 9)), ('randomly', (11, 12)), ('Gaussian samples', (13, 15))]","[['Out - of - vocabulary ( OOV ) words', 'initialized', 'randomly'], ['randomly', 'with', 'Gaussian samples']]",[],[],"[['Hyperparameters', 'has', 'Out - of - vocabulary ( OOV ) words']]",natural_language_inference,3,169
2592,hyperparameters,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .","[('is', (3, 4)), ('each is', (10, 12))]","[('CharCNN filter length', (0, 3)), ('1 , 3 , 5', (4, 9)), ('50 dimensions', (12, 14))]","[['CharCNN filter length', 'is', '1 , 3 , 5'], ['CharCNN filter length', 'is', '50 dimensions'], ['CharCNN filter length', 'each is', '50 dimensions']]","[['CharCNN filter length', 'has', '1 , 3 , 5']]",[],"[['Hyperparameters', 'has', 'CharCNN filter length']]",natural_language_inference,3,170
2593,hyperparameters,The cluster number K in discriminative block is 100 .,"[('in', (4, 5)), ('is', (7, 8))]","[('cluster number K', (1, 4)), ('discriminative block', (5, 7)), ('100', (8, 9))]","[['cluster number K', 'in', 'discriminative block'], ['discriminative block', 'is', '100']]",[],[],"[['Hyperparameters', 'has', 'cluster number K']]",natural_language_inference,3,172
2594,hyperparameters,The Adam method is used for optimization .,"[('used for', (4, 6))]","[('Adam method', (1, 3)), ('optimization', (6, 7))]","[['Adam method', 'used for', 'optimization']]",[],[],"[['Hyperparameters', 'has', 'Adam method']]",natural_language_inference,3,173
2595,hyperparameters,And the first momentum is set to be 0.9 and the second 0.999 .,"[('set to be', (5, 8))]","[('first momentum', (2, 4)), ('0.9', (8, 9)), ('second', (11, 12)), ('0.999', (12, 13))]","[['first momentum', 'set to be', '0.9']]","[['first momentum', 'has', '0.9'], ['second', 'has', '0.999']]",[],"[['Hyperparameters', 'has', 'first momentum']]",natural_language_inference,3,174
2596,hyperparameters,The initial learning rate is 0.0004 and the batch size is 32 .,[],"[('initial learning rate', (1, 4)), ('0.0004', (5, 6)), ('batch size', (8, 10)), ('32', (11, 12))]",[],"[['initial learning rate', 'has', '0.0004'], ['batch size', 'has', '32']]",[],"[['Hyperparameters', 'has', 'initial learning rate']]",natural_language_inference,3,175
2597,hyperparameters,"All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .",[],"[('All hidden states of GRUs , and TreeLSTMs', (0, 8)), ('500 dimensions', (9, 11)), ('word - level embedding d w', (13, 19)), ('300 dimensions', (20, 22))]",[],"[['All hidden states of GRUs , and TreeLSTMs', 'has', '500 dimensions'], ['word - level embedding d w', 'has', '300 dimensions']]",[],"[['Hyperparameters', 'has', 'All hidden states of GRUs , and TreeLSTMs']]",natural_language_inference,3,178
2598,hyperparameters,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .","[('set', (1, 2)), ('of', (4, 5)), ('to', (6, 7))]","[('max length', (2, 4)), ('document', (5, 6)), ('500', (7, 8))]","[['max length', 'of', 'document'], ['document', 'to', '500']]",[],"[['Hyperparameters', 'set', 'max length']]",[],natural_language_inference,3,179
2599,hyperparameters,Explicit question - type dimension d ET is 50 .,[],"[('Explicit question - type dimension d ET', (0, 7)), ('50', (8, 9))]",[],"[['Explicit question - type dimension d ET', 'has', '50']]",[],"[['Hyperparameters', 'has', 'Explicit question - type dimension d ET']]",natural_language_inference,3,180
2600,hyperparameters,We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,"[('apply', (1, 2)), ('to', (3, 4)), ('with', (10, 11)), ('of', (14, 15))]","[('dropout', (2, 3)), ('Encoder layer and aggregation layer', (5, 10)), ('dropout rate', (12, 14)), ('0.5', (15, 16))]","[['dropout', 'to', 'Encoder layer and aggregation layer'], ['Encoder layer and aggregation layer', 'with', 'dropout rate'], ['dropout rate', 'of', '0.5']]",[],"[['Hyperparameters', 'apply', 'dropout']]",[],natural_language_inference,3,181
2601,results,"Our model achieves a 68.73 % EM score and 77.39 % F1 score , which is ranked among the state of the art single models ( without model ensembling shows the ablation performances of various Q- code on the development set .","[('achieves', (2, 3)), ('ranked among', (16, 18))]","[('Our model', (0, 2)), ('68.73 %', (4, 6)), ('EM score', (6, 8)), ('77.39 %', (9, 11)), ('F1 score', (11, 13)), ('state of the art single models', (19, 25))]","[['Our model', 'achieves', '68.73 %'], ['Our model', 'achieves', '77.39 %'], ['F1 score', 'ranked among', 'state of the art single models']]","[['68.73 %', 'has', 'EM score'], ['77.39 %', 'has', 'F1 score']]",[],"[['Results', 'has', 'Our model']]",natural_language_inference,3,192
2602,results,"Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .","[('using', (3, 4)), ('achieved', (7, 8))]","[('Our baseline model', (0, 3)), ('no Q- code', (4, 7)), ('68.00 % and 77.36 %', (9, 14)), ('EM and F 1 scores', (14, 19))]","[['Our baseline model', 'using', 'no Q- code'], ['no Q- code', 'achieved', '68.00 % and 77.36 %']]","[['68.00 % and 77.36 %', 'has', 'EM and F 1 scores']]",[],[],natural_language_inference,3,194
2603,results,"When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .","[('added', (2, 3)), ('into', (10, 11)), ('to', (20, 21))]","[('explicit question type T - code', (4, 10)), ('baseline model', (12, 14)), ('performance', (16, 17)), ('improved slightly', (18, 20)), ('68.16 % ( EM ) and 77.58 % ( F1 )', (21, 32))]","[['explicit question type T - code', 'into', 'baseline model'], ['improved slightly', 'to', '68.16 % ( EM ) and 77.58 % ( F1 )']]","[['performance', 'has', 'improved slightly']]","[['Results', 'added', 'explicit question type T - code']]",[],natural_language_inference,3,195
2604,results,"We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .","[('used', (2, 3)), ('introduce', (4, 5)), ('for', (7, 8)), ('shows', (25, 26))]","[('TreeLSTM', (3, 4)), ('syntactic parses', (5, 7)), ('question representation and understanding', (8, 12)), ('further improvement', (26, 28))]","[['TreeLSTM', 'introduce', 'syntactic parses'], ['syntactic parses', 'for', 'question representation and understanding'], ['syntactic parses', 'shows', 'further improvement']]","[['TreeLSTM', 'has', 'syntactic parses']]","[['Results', 'used', 'TreeLSTM']]",[],natural_language_inference,3,196
2605,results,"Take our best model as an example , we observed a 78.38 % F1 score on the whole development set , which can be separated into two parts : one is where F1 score equals to 100 % , which means an exact match .","[('observed', (9, 10)), ('on', (15, 16))]","[('78.38 % F1 score', (11, 15)), ('whole development set', (17, 20))]","[['78.38 % F1 score', 'on', 'whole development set']]",[],[],[],natural_language_inference,3,204
2606,research-problem,Convolutional Neural Network Architectures for Matching Natural Language Sentences,[],"[('Matching Natural Language Sentences', (5, 9))]",[],[],[],[],natural_language_inference,30,2
2607,research-problem,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",[],"[('Semantic matching', (0, 2))]",[],[],[],[],natural_language_inference,30,4
2608,research-problem,Matching two potentially heterogenous language objects is central to many natural language applications .,[],"[('Matching two potentially heterogenous language objects', (0, 6))]",[],[],[],[],natural_language_inference,30,11
2609,research-problem,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .","[('have', (3, 4))]","[('Natural language sentences', (0, 3)), ('complicated structures', (4, 6)), ('sequential', (8, 9)), ('hierarchical', (10, 11))]","[['Natural language sentences', 'have', 'complicated structures']]","[['Natural language sentences', 'has', 'complicated structures']]",[],[],natural_language_inference,30,14
2610,research-problem,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,"[('to capture', (8, 10)), ('of', (15, 16)), ('in', (22, 23))]","[('successful sentence - matching algorithm', (1, 6)), ('internal structures', (13, 15)), ('sentences', (16, 17)), ('rich patterns', (20, 22)), ('interactions', (24, 25))]","[['successful sentence - matching algorithm', 'to capture', 'internal structures'], ['successful sentence - matching algorithm', 'to capture', 'rich patterns'], ['internal structures', 'of', 'sentences'], ['rich patterns', 'in', 'interactions']]",[],[],[],natural_language_inference,30,15
2611,model,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .","[('propose', (5, 6)), ('adapt', (12, 13)), ('to', (24, 25))]","[('deep neural network models', (6, 10)), ('convolutional strategy', (14, 16)), ('natural language', (25, 27))]","[['deep neural network models', 'adapt', 'convolutional strategy'], ['convolutional strategy', 'to', 'natural language']]",[],"[['Model', 'propose', 'deep neural network models']]",[],natural_language_inference,30,16
2612,model,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .","[('for', (25, 26)), ('of', (35, 36)), ('with', (38, 39))]","[('sentences', (7, 8)), ('naturally host', (19, 21)), ('hierarchical composition', (23, 25)), ('simple - to - comprehensive fusion', (29, 35)), ('matching patterns', (36, 38)), ('same convolutional architecture', (40, 43))]","[['simple - to - comprehensive fusion', 'of', 'matching patterns'], ['matching patterns', 'with', 'same convolutional architecture']]","[['naturally host', 'has', 'hierarchical composition']]",[],[],natural_language_inference,30,17
2613,model,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .","[('requiring', (5, 6)), ('of', (9, 10)), ('putting', (19, 20)), ('on', (23, 24))]","[('generic', (3, 4)), ('no prior knowledge', (6, 9)), ('natural language', (10, 12)), ('no constraints', (21, 23)), ('matching tasks', (25, 27))]","[['generic', 'requiring', 'no prior knowledge'], ['no prior knowledge', 'of', 'natural language'], ['no constraints', 'on', 'matching tasks']]",[],[],"[['Model', 'has', 'generic']]",natural_language_inference,30,18
2614,results,Experiment I : Sentence Completion,[],"[('Experiment I : Sentence Completion', (0, 5))]",[],[],[],"[['Results', 'has', 'Experiment I : Sentence Completion']]",natural_language_inference,30,148
2615,results,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .","[('outperforms', (3, 4))]","[('ARC - II', (0, 3)), ('ARC - I', (4, 7))]","[['ARC - II', 'outperforms', 'ARC - I']]",[],[],"[['Results', 'has', 'ARC - II']]",natural_language_inference,30,157
2616,results,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .","[('performs', (8, 9))]","[('SENNA + MLP', (5, 8)), ('fairly well', (9, 11))]","[['SENNA + MLP', 'performs', 'fairly well']]",[],[],"[['Results', 'has', 'SENNA + MLP']]",natural_language_inference,30,158
2617,results,Experiment III : Paraphrase Identification,[],"[('Experiment III : Paraphrase Identification', (0, 5))]",[],[],[],"[['Results', 'has', 'Experiment III : Paraphrase Identification']]",natural_language_inference,30,164
2618,results,"Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .","[('perform', (9, 10)), ('achieving', (13, 14)), ('close to', (19, 21)), ('in', (24, 25))]","[('generic matching models', (3, 6)), ('reasonably well', (10, 12)), ('accuracy and F1 score', (15, 19)), ('best performer', (22, 24)), ('2008', (25, 26))]","[['generic matching models', 'perform', 'reasonably well'], ['generic matching models', 'achieving', 'accuracy and F1 score'], ['reasonably well', 'achieving', 'accuracy and F1 score'], ['accuracy and F1 score', 'close to', 'best performer'], ['best performer', 'in', '2008']]",[],[],[],natural_language_inference,30,170
2619,research-problem,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes,[],"[('Scaling Memory - Augmented Neural Networks', (0, 6))]",[],[],[],[],natural_language_inference,31,2
2620,approach,We refer to this class of models as memory augmented neural networks ( MANNs ) .,[],"[('memory augmented neural networks', (8, 12))]",[],[],[],[],natural_language_inference,31,15
2621,approach,"In this paper , we present a MANN named SAM ( sparse access memory ) .","[('present', (5, 6)), ('named', (8, 9))]","[('MANN', (7, 8)), ('SAM ( sparse access memory )', (9, 15))]","[['MANN', 'named', 'SAM ( sparse access memory )']]","[['MANN', 'name', 'SAM ( sparse access memory )']]","[['Approach', 'present', 'MANN']]",[],natural_language_inference,31,23
2622,approach,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .","[('By thresholding', (0, 2)), ('to', (4, 5)), ('using', (10, 11)), ('for', (14, 15)), ('in', (25, 26)), ('with respect to', (29, 32))]","[('memory modifications', (2, 4)), ('sparse subset', (6, 8)), ('efficient data structures', (11, 14)), ('content - based read operations', (15, 20)), ('our model', (21, 23)), ('optimal', (24, 25)), ('space and time', (26, 29)), ('memory size', (32, 34))]","[['memory modifications', 'to', 'sparse subset'], ['efficient data structures', 'for', 'content - based read operations'], ['optimal', 'in', 'space and time'], ['space and time', 'with respect to', 'memory size']]","[['memory modifications', 'has', 'sparse subset'], ['our model', 'has', 'optimal']]","[['Approach', 'By thresholding', 'memory modifications']]",[],natural_language_inference,31,24
2623,experiments,Learning with sparse memory access,[],"[('Learning with sparse memory access', (0, 5))]",[],[],[],[],natural_language_inference,31,137
2624,results,"shows that sparse models are able to learn with comparable efficiency to the dense models and , surprisingly , learn more effectively for some tasks - notably priority sort and associative recall .","[('shows', (0, 1)), ('to', (6, 7)), ('learn with', (7, 9)), ('learn', (19, 20)), ('for', (22, 23))]","[('sparse models', (2, 4)), ('comparable efficiency', (9, 11)), ('dense models', (13, 15)), ('more effectively', (20, 22)), ('priority sort and associative recall', (27, 32))]","[['comparable efficiency', 'to', 'dense models'], ['sparse models', 'learn with', 'comparable efficiency'], ['sparse models', 'learn', 'more effectively']]",[],"[['Results', 'shows', 'sparse models']]",[],natural_language_inference,31,145
2625,results,Scaling with a curriculum,[],"[('Scaling with a curriculum', (0, 4))]",[],[],[],"[['Results', 'has', 'Scaling with a curriculum']]",natural_language_inference,31,148
2626,results,"For all tasks , SAM was able to advance further than the other models , and in the associative recall task , SAM was able to advance through the curriculum to sequences greater than 4000 ( ) .","[('For', (0, 1)), ('to', (7, 8)), ('than', (10, 11)), ('in', (16, 17)), ('through', (27, 28))]","[('all tasks', (1, 3)), ('SAM', (4, 5)), ('advance further', (8, 10)), ('other models', (12, 14)), ('associative recall task', (18, 21)), ('advance', (26, 27)), ('curriculum', (29, 30)), ('sequences', (31, 32)), ('greater than 4000', (32, 35))]","[['SAM', 'to', 'advance further'], ['curriculum', 'to', 'sequences'], ['advance further', 'than', 'other models'], ['advance', 'through', 'curriculum']]","[['all tasks', 'has', 'SAM'], ['advance further', 'has', 'other models'], ['sequences', 'has', 'greater than 4000']]","[['Results', 'For', 'all tasks']]",[],natural_language_inference,31,160
2627,experiments,Question answering on the Babi tasks,[],"[('Question answering on the Babi tasks', (0, 6))]",[],[],[],[],natural_language_inference,31,164
2628,results,"The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .","[('except', (3, 4)), ('able to learn', (8, 11)), ('comparable to', (12, 14)), ('failing at', (19, 21)), ('of', (23, 24))]","[('MANNs', (1, 2)), ('NTM', (5, 6)), ('solutions', (11, 12)), ('previous best results', (15, 18)), ('only 2', (21, 23)), ('tasks', (25, 26))]","[['MANNs', 'except', 'NTM'], ['MANNs', 'able to learn', 'solutions'], ['solutions', 'comparable to', 'previous best results'], ['solutions', 'failing at', 'only 2'], ['only 2', 'of', 'tasks']]",[],[],[],natural_language_inference,31,169
2629,results,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .","[('solve', (4, 5)), ('of', (8, 9)), ('on', (16, 17))]","[('SDNC', (1, 2)), ('all but 1', (5, 8)), ('tasks', (10, 11)), ('best reported result', (13, 16)), ('Babi', (17, 18))]","[['SDNC', 'solve', 'all but 1'], ['all but 1', 'of', 'tasks'], ['best reported result', 'on', 'Babi']]",[],[],"[['Results', 'has', 'SDNC']]",natural_language_inference,31,170
2630,results,We believe the NTM may perform poorly since it lacks a mechanism which allows it to allocate memory effectively .,[],"[('NTM', (3, 4)), ('perform poorly', (5, 7))]",[],"[['NTM', 'has', 'perform poorly']]",[],[],natural_language_inference,31,174
2631,results,Learning on real world data,[],"[('Learning on real world data', (0, 5))]",[],[],[],"[['Results', 'has', 'Learning on real world data']]",natural_language_inference,31,175
2632,results,"SAM outperformed other models , presumably due to its much larger memory capacity .","[('outperformed', (1, 2))]","[('SAM', (0, 1)), ('other models', (2, 4))]","[['SAM', 'outperformed', 'other models']]",[],[],[],natural_language_inference,31,187
2633,results,All of the MANNs were able to perform much better than chance with ?,"[('able to', (5, 7)), ('perform', (7, 8)), ('than', (10, 11))]","[('All of the MANNs', (0, 4)), ('much better', (8, 10)), ('chance', (11, 12))]","[['All of the MANNs', 'perform', 'much better'], ['much better', 'than', 'chance']]",[],[],"[['Results', 'has', 'All of the MANNs']]",natural_language_inference,31,192
2634,research-problem,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller,[],"[('Large - Scale Reading Comprehension', (2, 7))]",[],[],[],[],natural_language_inference,4,2
2635,research-problem,Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text .,[],"[('Machine reading comprehension', (0, 3))]",[],[],[],[],natural_language_inference,4,4
2636,research-problem,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .",[],"[('RC', (20, 21))]",[],[],[],[],natural_language_inference,4,6
2637,research-problem,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,[],"[('Reading comprehension ( RC )', (0, 5))]",[],[],[],[],natural_language_inference,4,13
2638,model,"To overcome this issue , we propose two novel strategies that improve the memory - handling capability while mitigating the information distortion .","[('propose', (6, 7)), ('improve', (11, 12)), ('mitigating', (18, 19))]","[('two novel strategies', (7, 10)), ('memory - handling capability', (13, 17)), ('information distortion', (20, 22))]","[['two novel strategies', 'improve', 'memory - handling capability'], ['two novel strategies', 'mitigating', 'information distortion']]",[],"[['Model', 'propose', 'two novel strategies']]",[],natural_language_inference,4,23
2639,model,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,"[('extend', (1, 2)), ('with', (5, 6)), ('to alleviate', (9, 11))]","[('memory controller', (3, 5)), ('residual connection', (7, 9)), ('information distortion', (12, 14))]","[['memory controller', 'with', 'residual connection'], ['residual connection', 'to alleviate', 'information distortion']]","[['memory controller', 'has', 'residual connection']]","[['Model', 'extend', 'memory controller']]",[],natural_language_inference,4,24
2640,model,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,"[('expand', (2, 3)), ('with', (10, 11)), ('conveys', (15, 16)), ('to', (18, 19)), ('containing', (22, 23)), ('as well as', (25, 28))]","[('gated recurrent unit ( GRU )', (4, 10)), ('dense connection', (12, 14)), ('enriched features', (16, 18)), ('next layer', (20, 22)), ('original', (24, 25)), ('transformed information', (29, 31))]","[['gated recurrent unit ( GRU )', 'with', 'dense connection'], ['dense connection', 'conveys', 'enriched features'], ['enriched features', 'to', 'next layer'], ['next layer', 'containing', 'original']]","[['gated recurrent unit ( GRU )', 'has', 'dense connection']]","[['Model', 'expand', 'gated recurrent unit ( GRU )']]",[],natural_language_inference,4,25
2641,experimental-setup,Implementation details . to build the model and Sonnet 2 to implement the memory interface .,"[('build', (4, 5)), ('to implement', (10, 12))]","[('model and Sonnet', (6, 9)), ('memory interface', (13, 15))]","[['model and Sonnet', 'to implement', 'memory interface']]",[],"[['Experimental setup', 'build', 'model and Sonnet']]",[],natural_language_inference,4,120
2642,experimental-setup,NLTK is used for tokenizing words .,"[('used for', (2, 4))]","[('NLTK', (0, 1)), ('tokenizing', (4, 5)), ('words', (5, 6))]","[['NLTK', 'used for', 'tokenizing']]","[['tokenizing', 'has', 'words']]",[],"[['Experimental setup', 'has', 'NLTK']]",natural_language_inference,4,121
2643,experimental-setup,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .","[('In', (0, 1)), ('use', (6, 7)), ('set to', (20, 22)), ('initialized as', (27, 29))]","[('memory controller', (2, 4)), ('four read heads and one write head', (7, 14)), ('memory size', (17, 19)), ('100 36', (22, 24)), ('0', (29, 30))]","[['memory controller', 'use', 'four read heads and one write head'], ['memory size', 'set to', '100 36']]","[['memory size', 'has', '100 36']]","[['Experimental setup', 'In', 'memory controller']]",[],natural_language_inference,4,122
2644,experimental-setup,The hidden vector dimension l is set to 200 .,"[('set to', (6, 8))]","[('hidden vector dimension l', (1, 5)), ('200', (8, 9))]","[['hidden vector dimension l', 'set to', '200']]","[['hidden vector dimension l', 'has', '200']]",[],"[['Experimental setup', 'has', 'hidden vector dimension l']]",natural_language_inference,4,123
2645,experimental-setup,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .","[('use', (1, 2)), ('as', (8, 9)), ('with', (11, 12)), ('of', (15, 16))]","[('AdaDelta ( Zeiler , 2012 )', (2, 8)), ('optimizer', (10, 11)), ('learning rate', (13, 15)), ('0.5', (16, 17))]","[['AdaDelta ( Zeiler , 2012 )', 'as', 'optimizer'], ['AdaDelta ( Zeiler , 2012 )', 'with', 'learning rate'], ['optimizer', 'with', 'learning rate'], ['learning rate', 'of', '0.5']]","[['learning rate', 'has', '0.5']]","[['Experimental setup', 'use', 'AdaDelta ( Zeiler , 2012 )']]",[],natural_language_inference,4,124
2646,experimental-setup,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,"[('set to', (4, 6)), ('for', (7, 8)), ('for', (11, 12))]","[('batch size', (1, 3)), ('20', (6, 7)), ('TriviaQA', (8, 9)), ('30', (10, 11)), ('SQuAD and QUASAR - T', (12, 17))]","[['batch size', 'set to', '20'], ['20', 'for', 'TriviaQA'], ['30', 'for', 'SQuAD and QUASAR - T'], ['30', 'for', 'SQuAD and QUASAR - T']]","[['batch size', 'has', '20']]",[],"[['Experimental setup', 'has', 'batch size']]",natural_language_inference,4,125
2647,experimental-setup,We use an exponential moving average of weights with a decaying factor of 0.001 .,"[('of', (6, 7)), ('with', (8, 9)), ('of', (12, 13))]","[('exponential moving average', (3, 6)), ('weights', (7, 8)), ('decaying factor', (10, 12)), ('0.001', (13, 14))]","[['exponential moving average', 'of', 'weights'], ['decaying factor', 'of', '0.001'], ['weights', 'with', 'decaying factor'], ['decaying factor', 'of', '0.001']]",[],[],[],natural_language_inference,4,126
2648,experimental-setup,"Our model does require more memory than existing methods , but a single GPU ( e.g. , M40 with 12 GB memory ) was enough to train model within a reasonable amount of time .","[('require', (3, 4)), ('than', (6, 7)), ('train', (26, 27)), ('within', (28, 29))]","[('Our model', (0, 2)), ('more memory', (4, 6)), ('existing methods', (7, 9)), ('single GPU', (12, 14)), ('enough', (24, 25)), ('model', (27, 28)), ('reasonable amount of time', (30, 34))]","[['Our model', 'require', 'more memory'], ['more memory', 'than', 'existing methods'], ['enough', 'train', 'model'], ['model', 'within', 'reasonable amount of time']]","[['single GPU', 'name', 'enough']]",[],"[['Experimental setup', 'has', 'Our model']]",natural_language_inference,4,127
2649,results,"Overall , in lengthy - document cases such as Trivi aQA and QUASAR - T , our model outperforms all the published results , as seen in Tables 2 and 3 , while in the short - document case such as SQuAD , we mostly achieve the best results , as seen in .","[('such as', (7, 9)), ('outperforms', (18, 19)), ('such as', (39, 41)), ('achieve', (45, 46))]","[('lengthy - document cases', (3, 7)), ('Trivi aQA and QUASAR - T', (9, 15)), ('our model', (16, 18)), ('all the published results', (19, 23)), ('short - document case', (35, 39)), ('SQuAD', (41, 42)), ('best results', (47, 49))]","[['lengthy - document cases', 'such as', 'Trivi aQA and QUASAR - T'], ['our model', 'outperforms', 'all the published results'], ['short - document case', 'such as', 'SQuAD'], ['short - document case', 'achieve', 'best results']]","[['lengthy - document cases', 'name', 'Trivi aQA and QUASAR - T'], ['short - document case', 'has', 'SQuAD']]",[],[],natural_language_inference,4,131
2650,ablation-analysis,We assume that the concatenation of the layer outputs in DEBS helps the memory controller store contextual representations clearly .,"[('assume', (1, 2)), ('of', (5, 6)), ('in', (9, 10)), ('helps', (11, 12)), ('store', (15, 16))]","[('concatenation', (4, 5)), ('layer outputs', (7, 9)), ('DEBS', (10, 11)), ('memory controller', (13, 15)), ('contextual representations', (16, 18))]","[['concatenation', 'of', 'layer outputs'], ['layer outputs', 'in', 'DEBS'], ['concatenation', 'helps', 'memory controller'], ['memory controller', 'store', 'contextual representations']]",[],"[['Ablation analysis', 'assume', 'concatenation']]",[],natural_language_inference,4,164
2651,results,"As can be seen in , using DEBS in all the places improves the performance most , and furthermore , the memory controller with DEBS gives the largest performance margin .","[('in', (4, 5)), ('using', (6, 7)), ('improves', (12, 13)), ('with', (23, 24)), ('gives', (25, 26))]","[('DEBS', (7, 8)), ('all the places', (9, 12)), ('performance', (14, 15)), ('most', (15, 16)), ('memory controller', (21, 23)), ('DEBS', (24, 25)), ('largest performance margin', (27, 30))]","[['DEBS', 'in', 'all the places'], ['DEBS', 'improves', 'performance'], ['all the places', 'improves', 'performance'], ['memory controller', 'with', 'DEBS'], ['memory controller', 'gives', 'largest performance margin'], ['DEBS', 'gives', 'largest performance margin']]","[['DEBS', 'has', 'all the places'], ['performance', 'has', 'most'], ['memory controller', 'has', 'DEBS']]","[['Results', 'in', 'DEBS']]",[],natural_language_inference,4,166
2652,research-problem,Sentence Similarity Learning by Lexical Decomposition and Composition,[],"[('Sentence Similarity Learning', (0, 3))]",[],[],[],[],natural_language_inference,5,2
2653,research-problem,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",[],"[('sentence similarity', (2, 4))]",[],[],[],[],natural_language_inference,5,4
2654,model,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .","[('propose', (5, 6)), ('decomposing and composing', (16, 19)), ('over', (21, 22))]","[('novel model', (7, 9)), ('lexical semantics', (19, 21)), ('sentences', (22, 23))]","[['novel model', 'decomposing and composing', 'lexical semantics'], ['lexical semantics', 'over', 'sentences']]",[],"[['Model', 'propose', 'novel model']]",[],natural_language_inference,5,45
2655,model,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .","[('Given', (0, 1)), ('represents', (7, 8)), ('as', (10, 11)), ('calculates', (21, 22)), ('for', (26, 27)), ('based on', (29, 31)), ('in', (33, 34))]","[('sentence pair', (2, 4)), ('model', (6, 7)), ('each word', (8, 10)), ('low -dimensional vector', (12, 15)), ('semantic matching vector', (23, 26)), ('each word', (27, 29)), ('all words', (31, 33)), ('other sentence', (35, 37))]","[['model', 'represents', 'each word'], ['each word', 'as', 'low -dimensional vector'], ['model', 'calculates', 'semantic matching vector'], ['semantic matching vector', 'for', 'each word'], ['each word', 'based on', 'all words'], ['all words', 'in', 'other sentence']]","[['sentence pair', 'has', 'model']]","[['Model', 'Given', 'sentence pair']]",[],natural_language_inference,5,46
2656,model,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .","[('based on', (1, 3)), ('decomposed into', (12, 14))]","[('semantic matching vector', (4, 7)), ('each word vector', (8, 11)), ('two components', (14, 16)), ('similar component', (18, 20)), ('dissimilar component', (22, 24))]","[['each word vector', 'decomposed into', 'two components']]","[['semantic matching vector', 'has', 'each word vector'], ['two components', 'name', 'similar component']]","[['Model', 'based on', 'semantic matching vector']]",[],natural_language_inference,5,47
2657,model,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .","[('use', (1, 2)), ('of', (4, 5)), ('to represent', (8, 10)), ('of', (13, 14)), ('of', (21, 22)), ('to model', (24, 26))]","[('similar components', (2, 4)), ('all the words', (5, 8)), ('similar parts', (11, 13)), ('sentence pair', (15, 17)), ('dissimilar components', (19, 21)), ('every word', (22, 24)), ('dissimilar parts', (27, 29)), ('explicitly', (29, 30))]","[['similar components', 'of', 'all the words'], ['similar parts', 'of', 'sentence pair'], ['dissimilar components', 'of', 'every word'], ['all the words', 'to represent', 'similar parts'], ['similar parts', 'of', 'sentence pair'], ['dissimilar components', 'of', 'every word'], ['dissimilar components', 'of', 'every word'], ['every word', 'to model', 'dissimilar parts']]","[['dissimilar parts', 'has', 'explicitly']]","[['Model', 'use', 'similar components']]",[],natural_language_inference,5,48
2658,model,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .","[('performed', (10, 11)), ('to compose', (11, 13)), ('into', (18, 19))]","[('two - channel CNN operation', (4, 9)), ('similar and dissimilar components', (14, 18)), ('feature vector', (20, 22))]","[['two - channel CNN operation', 'to compose', 'similar and dissimilar components'], ['similar and dissimilar components', 'into', 'feature vector']]",[],[],[],natural_language_inference,5,49
2659,model,"Finally , the composed feature vector is utilized to predict the sentence similarity .","[('utilized', (7, 8)), ('to predict', (8, 10))]","[('composed feature vector', (3, 6)), ('sentence similarity', (11, 13))]","[['composed feature vector', 'to predict', 'sentence similarity']]",[],[],"[['Model', 'has', 'composed feature vector']]",natural_language_inference,5,50
2660,results,QASent dataset .,[],"[('QASent dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'QASent dataset']]",natural_language_inference,5,202
2661,results,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .","[('adding', (1, 2)), ('between', (6, 7))]","[('some word overlap features', (2, 6)), ('two sentences', (8, 10)), ('performance', (12, 13)), ('improved significantly', (14, 16))]","[['some word overlap features', 'between', 'two sentences']]","[['performance', 'has', 'improved significantly']]","[['Results', 'adding', 'some word overlap features']]",[],natural_language_inference,5,207
2662,results,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .","[('see that', (2, 4)), ('got', (12, 13)), ('among', (16, 17)), ('than', (25, 26))]","[('our model', (4, 6)), ('best MAP', (14, 16)), ('all previous work', (17, 20)), ('comparable MRR', (23, 25)), ('dos', (26, 27))]","[['our model', 'got', 'best MAP'], ['our model', 'got', 'comparable MRR'], ['best MAP', 'among', 'all previous work'], ['comparable MRR', 'than', 'dos']]","[['our model', 'has', 'best MAP']]","[['Results', 'see that', 'our model']]",[],natural_language_inference,5,214
2663,results,Wiki QA dataset .,[],"[('Wiki QA dataset', (0, 3))]",[],[],[],"[['Results', 'has', 'Wiki QA dataset']]",natural_language_inference,5,215
2664,results,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,"[('acquired by', (12, 14)), ('combining with', (18, 20))]","[('best performance', (1, 3)), ('bigram CNN model', (15, 18)), ('word overlap features', (21, 24))]","[['best performance', 'acquired by', 'bigram CNN model'], ['bigram CNN model', 'combining with', 'word overlap features']]","[['best performance', 'has', 'bigram CNN model']]",[],"[['Results', 'has', 'best performance']]",natural_language_inference,5,218
2665,results,MSRP dataset .,[],"[('MSRP dataset', (0, 2))]",[],[],[],"[['Results', 'has', 'MSRP dataset']]",natural_language_inference,5,225
2666,results,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .","[('obtained', (10, 11)), ('without using', (20, 22))]","[('our model', (8, 10)), ('comparable performance', (12, 14)), ('any sparse features', (22, 25)), ('extra annotated resources', (26, 29)), ('specific training strategies', (30, 33))]","[['our model', 'obtained', 'comparable performance'], ['comparable performance', 'without using', 'any sparse features'], ['comparable performance', 'without using', 'extra annotated resources'], ['comparable performance', 'without using', 'specific training strategies']]",[],[],"[['Results', 'has', 'our model']]",natural_language_inference,5,237
2667,research-problem,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding,[],"[('Dynamic Self - Attention', (0, 4))]",[],[],[],[],natural_language_inference,6,2
2668,research-problem,"In this paper , we propose Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",[],"[('Dynamic Self - Attention ( DSA )', (6, 13))]",[],[],[],[],natural_language_inference,6,4
2669,research-problem,"We design DSA by modifying dynamic routing in capsule network ( Sabour et al. , 2017 ) for natural language processing .",[],"[('DSA', (2, 3))]",[],[],[],[],natural_language_inference,6,5
2670,model,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .","[('propose', (7, 8)), ('for', (14, 15)), ('namely', (18, 19))]","[('new self - attention mechanism', (9, 14)), ('sentence embedding', (15, 17)), ('Dynamic Self - Attention ( DSA )', (19, 26))]","[['new self - attention mechanism', 'for', 'sentence embedding'], ['sentence embedding', 'namely', 'Dynamic Self - Attention ( DSA )']]",[],"[['Model', 'propose', 'new self - attention mechanism']]",[],natural_language_inference,6,23
2671,model,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .","[('modify', (5, 6)), ('functions as', (11, 13)), ('with', (16, 17))]","[('dynamic routing', (6, 8)), ('self - attention', (13, 16)), ('dynamic weight vector', (18, 21))]","[['dynamic routing', 'functions as', 'self - attention'], ['self - attention', 'with', 'dynamic weight vector']]",[],"[['Model', 'modify', 'dynamic routing']]",[],natural_language_inference,6,24
2672,experiments,"DSA , which is stacked on CNN with Dense Connection , achieves new state - of - the - art results among the sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while obtaining comparative results in Stanford Sentiment Treebank ( SST ) dataset .","[('stacked on', (4, 6)), ('with', (7, 8)), ('achieves', (11, 12)), ('among', (21, 22)), ('in', (26, 27)), ('with', (35, 36)), ('obtaining', (43, 44)), ('in', (46, 47))]","[('DSA', (0, 1)), ('CNN', (6, 7)), ('Dense Connection', (8, 10)), ('new state - of - the - art results', (12, 21)), ('sentence encoding methods', (23, 26)), ('Stanford Natural Language Inference ( SNLI ) dataset', (27, 35)), ('least number of parameters', (37, 41)), ('comparative results', (44, 46)), ('Stanford Sentiment Treebank ( SST ) dataset', (47, 54))]","[['DSA', 'stacked on', 'CNN'], ['CNN', 'with', 'Dense Connection'], ['DSA', 'achieves', 'new state - of - the - art results'], ['new state - of - the - art results', 'among', 'sentence encoding methods'], ['sentence encoding methods', 'in', 'Stanford Natural Language Inference ( SNLI ) dataset'], ['Stanford Natural Language Inference ( SNLI ) dataset', 'with', 'least number of parameters'], ['DSA', 'obtaining', 'comparative results'], ['new state - of - the - art results', 'obtaining', 'comparative results'], ['comparative results', 'in', 'Stanford Sentiment Treebank ( SST ) dataset']]",[],[],[],natural_language_inference,6,25
2673,experiments,It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations .,"[('outperforms', (2, 3)), ('in terms of', (5, 8)), ('due to', (10, 12))]","[('recent models', (3, 5)), ('time efficiency', (8, 10)), ('simplicity and highly parallelized computations', (13, 18))]","[['recent models', 'in terms of', 'time efficiency'], ['time efficiency', 'due to', 'simplicity and highly parallelized computations']]",[],[],[],natural_language_inference,6,26
2674,experiments,Natural Language Inference Results,[],"[('Natural Language Inference Results', (0, 4))]",[],[],[],[],natural_language_inference,6,102
2675,experiments,"Entailment , Contradiction and Neutral .",[],"[('Entailment , Contradiction and Neutral', (0, 5))]",[],[],[],[],natural_language_inference,6,105
2676,experiments,"As the task considers the semantic relationship , SNLI is used as a benchmark for evaluating the performance of a sentence encoder .","[('considers', (3, 4)), ('used as', (10, 12)), ('for', (14, 15)), ('evaluating', (15, 16)), ('of', (18, 19))]","[('semantic relationship', (5, 7)), ('SNLI', (8, 9)), ('benchmark', (13, 14)), ('performance', (17, 18)), ('sentence encoder', (20, 22))]","[['semantic relationship', 'used as', 'benchmark'], ['SNLI', 'used as', 'benchmark'], ['benchmark', 'evaluating', 'performance'], ['performance', 'of', 'sentence encoder']]",[],[],[],natural_language_inference,6,106
2677,results,"With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .","[('With', (0, 1)), ('in terms of', (2, 5)), ('outperforms', (14, 15)), ('by', (17, 18))]","[('tradeoffs', (1, 2)), ('parameters and learning time per epoch', (5, 11)), ('multiple DSA', (12, 14)), ('other models', (15, 17)), ('large margin ( + 1.1 % )', (19, 26))]","[['tradeoffs', 'in terms of', 'parameters and learning time per epoch'], ['multiple DSA', 'outperforms', 'other models'], ['other models', 'by', 'large margin ( + 1.1 % )']]",[],"[['Results', 'With', 'tradeoffs']]",[],natural_language_inference,6,115
2678,results,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .","[('In comparison to', (0, 3)), ('shows', (8, 9)), ('than', (11, 12))]","[('baseline', (4, 5)), ('single DSA', (6, 8)), ('better performance', (9, 11)), ('self - attention ( + 2.2 % )', (12, 20))]","[['single DSA', 'shows', 'better performance'], ['better performance', 'than', 'self - attention ( + 2.2 % )']]","[['baseline', 'has', 'single DSA']]","[['Results', 'In comparison to', 'baseline']]",[],natural_language_inference,6,116
2679,results,"Note that our implementation of the baseline , selfattention stacked on CNN with Dense Connection , shows better performance ( + 0.4 % ) than the one stacked on BiLSTM .","[('of', (4, 5)), ('stacked on', (9, 11)), ('with', (12, 13)), ('shows', (16, 17)), ('than', (24, 25))]","[('our implementation', (2, 4)), ('baseline', (6, 7)), ('selfattention', (8, 9)), ('CNN', (11, 12)), ('Dense Connection', (13, 15)), ('better performance ( + 0.4 % )', (17, 24)), ('stacked on BiLSTM', (27, 30))]","[['our implementation', 'of', 'baseline'], ['selfattention', 'stacked on', 'CNN'], ['CNN', 'with', 'Dense Connection'], ['baseline', 'shows', 'better performance ( + 0.4 % )'], ['selfattention', 'shows', 'better performance ( + 0.4 % )'], ['Dense Connection', 'shows', 'better performance ( + 0.4 % )']]","[['our implementation', 'name', 'baseline'], ['baseline', 'name', 'selfattention']]",[],[],natural_language_inference,6,118
2680,results,Sentiment Analysis Results,[],"[('Sentiment Analysis Results', (0, 3))]",[],[],[],"[['Results', 'has', 'Sentiment Analysis Results']]",natural_language_inference,6,119
2681,results,"Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .","[('outperforms', (2, 3)), ('in', (7, 8)), ('achieves', (14, 15)), ('in', (17, 18)), ('verifies', (24, 25)), ('of', (27, 28))]","[('Single DSA', (0, 2)), ('all the baseline models', (3, 7)), ('SST - 2 dataset', (8, 12)), ('comparative results', (15, 17)), ('SST - 5', (18, 21)), ('effectiveness', (26, 27)), ('dynamic weight vector', (29, 32))]","[['Single DSA', 'outperforms', 'all the baseline models'], ['all the baseline models', 'in', 'SST - 2 dataset'], ['comparative results', 'in', 'SST - 5'], ['Single DSA', 'achieves', 'comparative results'], ['comparative results', 'in', 'SST - 5'], ['comparative results', 'verifies', 'effectiveness'], ['effectiveness', 'of', 'dynamic weight vector']]",[],[],[],natural_language_inference,6,127
2682,results,"In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .","[('in', (6, 7)), ('in', (15, 16)), ('between', (25, 26))]","[('SST dataset', (16, 18)), ('only marginal differences', (19, 22)), ('performance', (24, 25)), ('DSA and the previous self - attentive models', (26, 34)), ('found', (35, 36))]","[['only marginal differences', 'in', 'performance'], ['only marginal differences', 'in', 'performance'], ['performance', 'between', 'DSA and the previous self - attentive models']]","[['SST dataset', 'has', 'only marginal differences']]","[['Results', 'in', 'SST dataset']]",[],natural_language_inference,6,128
2683,research-problem,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",[],"[('Machine reading', (0, 2))]",[],[],[],[],natural_language_inference,7,5
2684,research-problem,"Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars , or information extraction methods of detecting predicate argument triples that can later be queried as a relational database .",[],"[('machine reading and comprehension', (3, 7))]",[],[],[],[],natural_language_inference,7,10
2685,approach,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,"[('address', (7, 8)), ('lack of', (9, 11)), ('introducing', (17, 18)), ('to building', (21, 23))]","[('real natural language training data', (11, 16)), ('novel approach', (19, 21)), ('supervised reading comprehension data set', (24, 29))]","[['real natural language training data', 'introducing', 'novel approach'], ['novel approach', 'to building', 'supervised reading comprehension data set']]",[],"[['Approach', 'address', 'real natural language training data']]",[],natural_language_inference,7,16
2686,approach,"We observe that summary and paraphrase sentences , with their associated documents , can be readily converted to context - query - answer triples using simple entity detection and anonymisation algorithms .","[('observe', (1, 2)), ('with', (8, 9)), ('to', (17, 18)), ('using', (24, 25))]","[('summary and paraphrase sentences', (3, 7)), ('associated documents', (10, 12)), ('readily converted', (15, 17)), ('context - query - answer triples', (18, 24)), ('simple entity detection and anonymisation algorithms', (25, 31))]","[['summary and paraphrase sentences', 'with', 'associated documents'], ['readily converted', 'to', 'context - query - answer triples'], ['context - query - answer triples', 'using', 'simple entity detection and anonymisation algorithms']]","[['summary and paraphrase sentences', 'has', 'associated documents']]","[['Approach', 'observe', 'summary and paraphrase sentences']]",[],natural_language_inference,7,17
2687,approach,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,"[('collected', (5, 6)), ('of', (9, 10)), ('with', (15, 16)), ('from', (18, 19))]","[('two new corpora', (6, 9)), ('roughly a million news stories', (10, 15)), ('associated queries', (16, 18)), ('CNN and Daily Mail websites', (20, 25))]","[['two new corpora', 'of', 'roughly a million news stories'], ['roughly a million news stories', 'with', 'associated queries'], ['associated queries', 'from', 'CNN and Daily Mail websites']]",[],"[['Approach', 'collected', 'two new corpora']]",[],natural_language_inference,7,18
2688,approach,We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension .,"[('demonstrate', (1, 2)), ('of', (4, 5)), ('by building', (8, 10)), ('for', (14, 15))]","[('efficacy', (3, 4)), ('our new corpora', (5, 8)), ('novel deep learning models', (10, 14)), ('reading comprehension', (15, 17))]","[['efficacy', 'of', 'our new corpora'], ['efficacy', 'by building', 'novel deep learning models'], ['our new corpora', 'by building', 'novel deep learning models'], ['novel deep learning models', 'for', 'reading comprehension']]",[],"[['Approach', 'demonstrate', 'efficacy']]",[],natural_language_inference,7,19
2689,results,We expect that the attention - based models would therefore outperform the pure LSTM - based approaches .,"[('outperform', (10, 11))]","[('attention - based models', (4, 8)), ('pure LSTM - based approaches', (12, 17))]","[['attention - based models', 'outperform', 'pure LSTM - based approaches']]",[],[],[],natural_language_inference,7,140
2690,results,Word distance benchmark,[],"[('Word distance benchmark', (0, 3))]",[],[],[],"[['Results', 'has', 'Word distance benchmark']]",natural_language_inference,7,156
2691,results,"More surprising perhaps is the relatively strong performance of the word distance benchmark , particularly relative to the frame - semantic benchmark , which we had expected to perform better .","[('of', (8, 9)), ('relative to', (15, 17)), ('perform', (28, 29))]","[('relatively strong performance', (5, 8)), ('word distance benchmark', (10, 13)), ('frame - semantic benchmark', (18, 22)), ('better', (29, 30))]","[['relatively strong performance', 'of', 'word distance benchmark'], ['relatively strong performance', 'relative to', 'frame - semantic benchmark'], ['word distance benchmark', 'relative to', 'frame - semantic benchmark'], ['frame - semantic benchmark', 'perform', 'better']]","[['relatively strong performance', 'has', 'word distance benchmark']]",[],[],natural_language_inference,7,157
2692,experiments,Neural models,[],"[('Neural models', (0, 2))]",[],[],[],[],natural_language_inference,7,164
2693,experiments,"Within the group of neural models explored here , the results paint a clear picture with the Impatient and the Attentive Readers outperforming all other models .","[('outperforming', (22, 23))]","[('Impatient and the Attentive Readers', (17, 22)), ('all other models', (23, 26))]","[['Impatient and the Attentive Readers', 'outperforming', 'all other models']]",[],[],[],natural_language_inference,7,165
2694,baselines,The Deep LSTM,[],"[('The Deep LSTM', (0, 3))]",[],[],[],"[['Baselines', 'has', 'The Deep LSTM']]",natural_language_inference,7,167
2695,results,"Reader performs surprisingly well , once again demonstrating that this simple sequential architecture can do a reasonable job of learning to abstract long sequences , even when they are up to two thousand tokens in length .","[('performs', (1, 2))]","[('Reader', (0, 1)), ('surprisingly well', (2, 4))]","[['Reader', 'performs', 'surprisingly well']]","[['Reader', 'has', 'surprisingly well']]",[],"[['Results', 'has', 'Reader']]",natural_language_inference,7,168
2696,research-problem,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention,[],"[('Natural Language Inference', (1, 4))]",[],[],[],[],natural_language_inference,8,2
2697,research-problem,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .",[],"[('recognizing text entailment', (13, 16))]",[],[],[],[],natural_language_inference,8,4
2698,research-problem,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",[],"[('recognizing text entailment ( RTE )', (9, 15))]",[],[],[],[],natural_language_inference,8,13
2699,model,"There were three types of relation in RTE , Entailment ( inferred to be true ) , Contradiction ( inferred to be false ) and Neutral ( truth unknown ) .",[],"[('RTE', (7, 8))]",[],[],[],[],natural_language_inference,8,14
2700,model,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .","[('proposed', (5, 6)), ('for', (11, 12))]","[('unified deep learning framework', (7, 11)), ('recognizing', (12, 13)), ('textual entailment', (13, 15))]","[['unified deep learning framework', 'for', 'recognizing']]","[['recognizing', 'has', 'textual entailment']]","[['Model', 'proposed', 'unified deep learning framework']]",[],natural_language_inference,8,30
2701,model,The basic model is based on building biL - STM models on both premises and hypothesis .,"[('based on', (4, 6)), ('on', (11, 12))]","[('basic model', (1, 3)), ('building', (6, 7)), ('biL - STM models', (7, 11)), ('both premises and hypothesis', (12, 16))]","[['basic model', 'based on', 'building'], ['biL - STM models', 'on', 'both premises and hypothesis']]","[['building', 'has', 'biL - STM models']]",[],"[['Model', 'has', 'basic model']]",natural_language_inference,8,31
2702,model,The basic mean pooling encoder can roughly form a intuition about what this sentence is talking about .,"[('roughly form', (6, 8)), ('about', (10, 11))]","[('basic mean pooling encoder', (1, 5)), ('intuition', (9, 10)), ('sentence is talking about', (13, 17))]","[['basic mean pooling encoder', 'roughly form', 'intuition'], ['intuition', 'about', 'sentence is talking about']]","[['basic mean pooling encoder', 'has', 'intuition'], ['intuition', 'has', 'sentence is talking about']]",[],"[['Model', 'has', 'basic mean pooling encoder']]",natural_language_inference,8,32
2703,model,"In addition , we introduced a simple effective input strategy that get ride of same words in hypothesis and premise , which further boosts our performance .","[('introduced', (4, 5)), ('get ride of', (11, 14)), ('in', (16, 17))]","[('simple effective input strategy', (6, 10)), ('same words', (14, 16)), ('hypothesis and premise', (17, 20))]","[['simple effective input strategy', 'get ride of', 'same words'], ['same words', 'in', 'hypothesis and premise']]",[],"[['Model', 'introduced', 'simple effective input strategy']]",[],natural_language_inference,8,35
2704,hyperparameters,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .","[('of', (3, 4)), ('is', (6, 7)), ('use', (14, 15)), ('with', (17, 18)), ('for', (27, 28))]","[('training objective', (1, 3)), ('our model', (4, 6)), ('cross - entropy loss', (7, 11)), ('minibatch SGD', (15, 17)), ('Rmsprop ( Tieleman and Hinton , 2012 )', (19, 27)), ('optimization', (28, 29))]","[['training objective', 'of', 'our model'], ['our model', 'is', 'cross - entropy loss'], ['training objective', 'use', 'minibatch SGD'], ['our model', 'use', 'minibatch SGD'], ['minibatch SGD', 'with', 'Rmsprop ( Tieleman and Hinton , 2012 )'], ['Rmsprop ( Tieleman and Hinton , 2012 )', 'for', 'optimization']]",[],[],"[['Hyperparameters', 'has', 'training objective']]",natural_language_inference,8,77
2705,hyperparameters,The batch size is 128 .,"[('is', (3, 4))]","[('batch size', (1, 3)), ('128', (4, 5))]","[['batch size', 'is', '128']]","[['batch size', 'has', '128']]",[],"[['Hyperparameters', 'has', 'batch size']]",natural_language_inference,8,78
2706,hyperparameters,A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,"[('applied in', (4, 6)), ('of', (8, 9)), ('with', (11, 12)), ('set to', (15, 17))]","[('dropout layer', (1, 3)), ('output', (7, 8)), ('network', (10, 11)), ('dropout rate', (13, 15)), ('0.25', (17, 18))]","[['dropout layer', 'applied in', 'output'], ['output', 'of', 'network'], ['dropout layer', 'with', 'dropout rate'], ['output', 'with', 'dropout rate'], ['network', 'with', 'dropout rate'], ['dropout rate', 'set to', '0.25']]","[['dropout rate', 'has', '0.25']]",[],"[['Hyperparameters', 'has', 'dropout layer']]",natural_language_inference,8,79
2707,hyperparameters,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .","[('used', (5, 6)), ('to initialize', (11, 13))]","[('pretrained 300D Glove 840B vectors', (6, 11)), ('word embedding', (14, 16))]","[['pretrained 300D Glove 840B vectors', 'to initialize', 'word embedding']]",[],"[['Hyperparameters', 'used', 'pretrained 300D Glove 840B vectors']]",[],natural_language_inference,8,80
2708,hyperparameters,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .","[('in', (6, 7)), ('by', (13, 14)), ('uniformly from', (16, 18))]","[('Out - of - vocabulary words', (0, 6)), ('training set', (8, 10)), ('randomly initialized', (11, 13)), ('sampling', (14, 15)), ('values', (15, 16)), ('( 0.05 , 0.05 )', (18, 23))]","[['Out - of - vocabulary words', 'in', 'training set'], ['randomly initialized', 'by', 'sampling'], ['values', 'uniformly from', '( 0.05 , 0.05 )']]","[['sampling', 'has', 'values']]",[],"[['Hyperparameters', 'has', 'Out - of - vocabulary words']]",natural_language_inference,8,81
2709,results,"We observed that more attention was given to Nones , Verbs and Adjectives .","[('observed', (1, 2)), ('given to', (6, 8))]","[('more attention', (3, 5)), ('Nones , Verbs and Adjectives', (8, 13))]","[['more attention', 'given to', 'Nones , Verbs and Adjectives']]",[],"[['Results', 'observed', 'more attention']]",[],natural_language_inference,8,107
2710,baselines,"While mean pooling regarded each word of equal importance , the attention mechanism helps re-weight words according to their importance .","[('helps', (13, 14)), ('re-weight', (14, 15)), ('according to', (16, 18))]","[('importance', (8, 9)), ('attention mechanism', (11, 13)), ('words', (15, 16))]",[],[],[],[],natural_language_inference,8,109
2711,research-problem,A BERT Baseline for the Natural Questions,[],"[('Baseline for the Natural Questions', (2, 7))]",[],[],[],[],natural_language_inference,9,2
2712,model,In this technical note we describe a BERT - based model for the Natural Questions .,"[('describe', (5, 6)), ('for', (11, 12))]","[('BERT - based model', (7, 11)), ('Natural Questions', (13, 15))]","[['BERT - based model', 'for', 'Natural Questions']]",[],"[['Model', 'describe', 'BERT - based model']]",[],natural_language_inference,9,14
2713,model,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .","[('in', (3, 4)), ('jointly predict', (10, 12)), ('than using', (21, 23)), ('split', (30, 31)), ('into', (33, 34)), ('by using', (37, 39)), ('of', (41, 42)), ('aggressively downsample', (58, 60)), ('at', (69, 70)), ('to create', (72, 74)), ('use', (82, 83)), ('at', (90, 91)), ('to predict', (93, 95))]","[('short and long answers', (12, 16)), ('single model', (18, 20)), ('pipeline approach', (24, 26)), ('each document', (31, 33)), ('multiple training instances', (34, 37)), ('overlapping windows', (39, 41)), ('tokens', (42, 43)), ('null instances', (60, 62)), ('training time', (70, 72)), ('balanced training set', (75, 78)), ('"" [ CLS ] "" token', (84, 90)), ('training time', (91, 93)), ('null instances', (95, 97)), ('rank', (98, 99)), ('spans', (99, 100)), ('at', (100, 101)), ('inference time', (101, 103)), ('by', (103, 104)), ('difference', (105, 106)), ('between', (106, 107)), ('span score and the "" [ CLS ] "" score', (108, 118))]","[['short and long answers', 'in', 'single model'], ['each document', 'into', 'multiple training instances'], ['multiple training instances', 'by using', 'overlapping windows'], ['overlapping windows', 'of', 'tokens'], ['training time', 'to create', 'balanced training set'], ['"" [ CLS ] "" token', 'at', 'training time'], ['training time', 'to predict', 'null instances']]","[['rank', 'has', 'spans'], ['spans', 'has', 'at'], ['at', 'has', 'inference time'], ['inference time', 'has', 'by'], ['by', 'has', 'difference'], ['difference', 'has', 'between'], ['between', 'has', 'span score and the "" [ CLS ] "" score']]",[],[],natural_language_inference,9,17
2714,experimental-setup,We initialized our model from a BERT model already finetuned on SQ u AD 1.1 .,"[('initialized', (1, 2)), ('from', (4, 5)), ('on', (10, 11))]","[('our model', (2, 4)), ('BERT model', (6, 8)), ('finetuned', (9, 10)), ('SQ u AD 1.1', (11, 15))]","[['our model', 'from', 'BERT model'], ['finetuned', 'on', 'SQ u AD 1.1']]",[],"[['Experimental setup', 'initialized', 'our model']]",[],natural_language_inference,9,54
2715,experimental-setup,"We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .","[('trained', (1, 2)), ('by minimizing', (4, 6)), ('with', (11, 12)), ('of', (26, 27))]","[('model', (3, 4)), ('loss L', (6, 8)), ('Adam optimizer', (13, 15)), ('batch size', (24, 26)), ('8', (27, 28))]","[['model', 'by minimizing', 'loss L'], ['loss L', 'with', 'Adam optimizer'], ['batch size', 'of', '8']]",[],"[['Experimental setup', 'trained', 'model']]",[],natural_language_inference,9,56
2716,experimental-setup,"As is common practice for BERT models , we only tuned the number of epochs and the initial learning rate for finetuning and found that training for 1 epoch with an initial learning rate of 3 10 ? 5 was the best setting .","[('for', (4, 5)), ('tuned', (10, 11)), ('for', (20, 21)), ('found', (23, 24)), ('with', (29, 30)), ('of', (34, 35))]","[('number of epochs', (12, 15)), ('initial learning rate', (17, 20)), ('finetuning', (21, 22)), ('training', (25, 26)), ('1 epoch', (27, 29)), ('an initial learning rate', (30, 34)), ('3 10 ? 5', (35, 39))]","[['initial learning rate', 'for', 'finetuning'], ['initial learning rate', 'for', 'finetuning'], ['initial learning rate', 'found', 'training'], ['1 epoch', 'with', 'an initial learning rate'], ['an initial learning rate', 'of', '3 10 ? 5']]",[],"[['Experimental setup', 'for', 'number of epochs']]",[],natural_language_inference,9,57
2717,experimental-setup,Evaluation completed in about 5 hours on the NQ dev and test set with a single Tesla P100 GPU .,"[('with', (13, 14))]","[('single Tesla P100 GPU', (15, 19))]",[],[],"[['Experimental setup', 'with', 'single Tesla P100 GPU']]",[],natural_language_inference,9,58
2718,results,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,"[('for', (3, 4)), ('performs', (5, 6)), ('than', (8, 9)), ('presented in', (11, 13))]","[('Our BERT model', (0, 3)), ('NQ', (4, 5)), ('dramatically better', (6, 8)), ('models', (10, 11)), ('original NQ paper', (14, 17))]","[['Our BERT model', 'for', 'NQ'], ['Our BERT model', 'performs', 'dramatically better'], ['NQ', 'performs', 'dramatically better'], ['dramatically better', 'than', 'models'], ['models', 'presented in', 'original NQ paper']]",[],[],"[['Results', 'has', 'Our BERT model']]",natural_language_inference,9,60
2719,results,Our model closes the gap between the F 1 score achieved by the original baseline systems and the super - annotator upper bound by 30 % for the long answer NQ task and by 50 % for the short answer NQ task .,"[('closes', (2, 3)), ('between', (5, 6)), ('achieved by', (10, 12)), ('by', (23, 24)), ('for', (26, 27)), ('for', (36, 37))]","[('Our model', (0, 2)), ('gap', (4, 5)), ('F 1 score', (7, 10)), ('original baseline systems and the super - annotator upper bound', (13, 23)), ('30 %', (24, 26)), ('long answer NQ task', (28, 32)), ('50 %', (34, 36)), ('short answer NQ task', (38, 42))]","[['Our model', 'closes', 'gap'], ['gap', 'between', 'F 1 score'], ['F 1 score', 'achieved by', 'original baseline systems and the super - annotator upper bound'], ['original baseline systems and the super - annotator upper bound', 'by', '30 %'], ['30 %', 'for', 'long answer NQ task'], ['50 %', 'for', 'short answer NQ task']]","[['Our model', 'has', 'gap']]",[],"[['Results', 'has', 'Our model']]",natural_language_inference,9,61
