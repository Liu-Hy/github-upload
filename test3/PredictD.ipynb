{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "christian-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in the hyperparameter space with W&B sweep\n",
    "import logging\n",
    "from ast import literal_eval as load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import sklearn\n",
    "\n",
    "from simpletransformers.classification import (\n",
    "    ClassificationArgs,\n",
    "    ClassificationModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heated-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "df = pd.read_csv('triples.csv')\n",
    "df.insert(loc=0, column='idx', value=np.arange(len(df)))\n",
    "sent_num=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subject-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(arr):\n",
    "    trip_list=[]\n",
    "    ls=[]\n",
    "    for i in range(len(arr)):\n",
    "        #if arr[i,3]=='[]' or ((arr[i,3]!='[]') and (load(arr[i,3])[0][1][0]>load(arr[i,4])[0][1][0])):\n",
    "        if arr[i,4]!='[]':\n",
    "            if arr[i,3]=='[]' or ((arr[i,3]!='[]') and (load(arr[i,3])[0][1][0]>load(arr[i,4])[0][1][0])):\n",
    "                np = load(arr[i,4])[0]\n",
    "                uni_name=arr[i,1]\n",
    "                uni_name=(uni_name[0].upper()+uni_name[1:]).replace('-',' ')\n",
    "                triple=[uni_name,'has',np[0]]\n",
    "                trip_list.append(triple)\n",
    "                word_ls = arr[i,2].split(' ')\n",
    "                word_ls.insert(np[1][0], '[[')\n",
    "                word_ls.insert(np[1][1]+1, ']]')\n",
    "                unit = arr[i,1]\n",
    "                unit = (unit[0].upper()+unit[1:]).replace('-',' ')\n",
    "                unit_ls = ['[[']+(unit.split(' '))+[']]']\n",
    "                word_ls = unit_ls+[':']+word_ls\n",
    "                flg=0\n",
    "                if arr[i,8]=='[]':\n",
    "                    trip_ls = []\n",
    "                else:\n",
    "                    trip_ls = load(arr[i,8])\n",
    "                    for trip in trip_ls:\n",
    "                        if trip[1]=='has' and trip[2]==np[0]:\n",
    "                            flg=1\n",
    "                            break\n",
    "                ls.append([int(arr[i, 0]), unit, np[0], trip_ls, ' '.join(word_ls), flg])\n",
    "        #else:\n",
    "            #missed+=len(load(arr[i,8]))\n",
    "    dataframe = pd.DataFrame(ls)\n",
    "    dataframe.columns = ['idx','info_unit', 'np', 'triples', 'text', 'labels']\n",
    "    return dataframe,trip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "apparent-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.values\n",
    "df,trip_list=convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "curious-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.manual_seed = 1\n",
    "model_args.fp16 = False\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.do_lower_case = True  # when using uncased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cooked-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_F1(ref, pred):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "universal-custody",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6161f0be02a545c3ae2c2758e282bcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2292ebb6a74ee5af8e069d916a67e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hl/lib/python3.9/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 0, 'tn': 632, 'fp': 789, 'fn': 0, 'F1_score': 0, 'eval_loss': 1.677608103773902}\n"
     ]
    }
   ],
   "source": [
    "# Create a TransformerModel\n",
    "model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"../rel/outputsD/best_model\",\n",
    "    args=model_args,\n",
    ")\n",
    "result, model_outputs, wrong_predictions = model.eval_model(df, F1_score=triple_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advisory-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(model_outputs.argmax(axis=1))\n",
    "df['preds']=preds\n",
    "df['cand']=trip_list\n",
    "df.loc[df['preds']==0,'cand']=None\n",
    "data=[]\n",
    "for i in range(sent_num):\n",
    "    temp = list(df[df['idx']==i]['cand'])\n",
    "    temp = [t for t in temp if t]\n",
    "    data.append(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ordinary-mills",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'RNNGs']]\",\n",
       " '[]',\n",
       " \"[['Model', 'has', 'discriminative model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'bi-directional transformer architecture']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'CNN models']]\",\n",
       " \"[['Experimental setup', 'has', 'learning rate']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Our CNN base model']]\",\n",
       " \"[['Results', 'has', 'Named Entity Recognition']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'beam size']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Seq2seq approach']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'embedding layer']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'LSTM + A model']]\",\n",
       " \"[['Results', 'has', 'difference']]\",\n",
       " \"[['Results', 'has', 'LSTM + A']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'RG']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Ensembling']]\",\n",
       " \"[['Ablation analysis', 'has', 'Performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'bottom - up system']]\",\n",
       " \"[['Results', 'has', 'inorder system']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'English constituent results']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Chinese dependency results']]\",\n",
       " \"[['Results', 'has', 'our final model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'three LSTM layers']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'forget gate bias']]\",\n",
       " \"[['Hyperparameters', 'has', 'Dropout']]\",\n",
       " \"[['Hyperparameters', 'has', 'learning rate']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'RNNG']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Gated Attention RNNG']]\",\n",
       " \"[['Results', 'has', 'model']]\",\n",
       " \"[['Results', 'has', 'Headedness in Phrases']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'attention - based tree output']]\",\n",
       " \"[['Results', 'has', 'conversion accuracy']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'mention - ranking model']]\",\n",
       " \"[['Results', 'has', 'cluster - ranking model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'our full model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'reward - rescaled max - margin loss']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Our full approach']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'representations']]\",\n",
       " \"[['Model', 'has', 'learned score']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'Glo Ve word embeddings']]\",\n",
       " \"[['Hyperparameters', 'has', 'Vocabulary']]\",\n",
       " \"[['Hyperparameters', 'has', 'size']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'first feed - forward layer size']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'Dropout']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Performance']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'MR - LSTM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'RNN']]\",\n",
       " \"[['Results', 'has', 'RNN performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'Co-reference resolution']]\",\n",
       " '[]',\n",
       " \"[['Model', 'has', 'external memory block']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'LSTM modules']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'initial learning rate']]\",\n",
       " \"[['Hyperparameters', 'has', 'model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'has', 'posits']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'reasons']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'attention component']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'word embeddings']]\",\n",
       " \"[['Hyperparameters', 'has', 'Outof - vocabulary words']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'convolutions']]\",\n",
       " \"[['Hyperparameters', 'has', 'hidden states']]\",\n",
       " \"[['Hyperparameters', 'has', 'feedforward neural network']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'LSTM weights']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'Dropout masks']]\",\n",
       " \"[['Hyperparameters', 'has', 'learning rate']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Coreference Results']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'most significant gains']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Paragraph Level : GAP']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Wiseman']]\",\n",
       " \"[['Baselines', 'has', 'Li']]\",\n",
       " \"[['Baselines', 'has', 'Puduppully - plan']]\",\n",
       " \"[['Baselines', 'has', 'Puduppully - updt']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'size']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'scenario Hierarchical - kv and Hierarchical -k']]\",\n",
       " \"[['Ablation analysis', 'has', 'Our hierarchical models']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'Our hierarchical models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'individual LSTM models']]\",\n",
       " \"[['Experimental setup', 'has', 'decoder']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'our ensemble model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'WebNLG task']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'GCN model']]\",\n",
       " \"[['Results', 'has', 'GCN EC model']]\",\n",
       " \"[['Results', 'has', 'SR11 Deep task']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'importance']]\",\n",
       " \"[['Ablation analysis', 'has', 'Residual and dense connections']]\",\n",
       " \"[['Ablation analysis', 'has', 'Dense connections']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'has', 'canonical presentation']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'Input feeding']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'Models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'NCP']]\",\n",
       " \"[['Results', 'has', 'NCP']]\",\n",
       " \"[['Results', 'has', 'NCP + CC']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'text planner']]\",\n",
       " \"[['Model', 'has', 'plan']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'StrongNeural and BestPlan systems']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'our model EDA_CS']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'EDA_CS TL']]\",\n",
       " \"[['Results', 'has', 'baseline model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'joint model']]\",\n",
       " \"[['Experimental setup', 'has', 'jPTDP v 2.0']]\",\n",
       " \"[['Experimental setup', 'has', 'Word embeddings']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Approach', 'has', 'BiLSTM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'parsers']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'greedy transition based parser']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'POS tagging results']]\",\n",
       " \"[['Results', 'has', 'BiLSTM - CRF and Mar - MoT']]\",\n",
       " \"[['Results', 'has', 'jPTDP']]\",\n",
       " \"[['Results', 'has', 'MarMoT']]\",\n",
       " \"[['Results', 'has', 'BiLSTM - CRF']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Overall dependency parsing results']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'pre-trained NNdep and Biaffine models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'STACKPTR']]\",\n",
       " \"[['Model', 'has', 'STACKPTR parser']]\",\n",
       " \"[['Model', 'has', 'STACKPTR parser']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Our model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Our model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'interpolating']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Chinese score']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'Beam search']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'our model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'our accuracy']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'unsupervised data augmentation ( UDA ) )']]\",\n",
       " \"[['Baselines', 'has', 'Fine-tune ( Ft )']]\",\n",
       " \"[['Baselines', 'has', 'Fine - tune with UDA ( UDA )']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Self - training']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'teacher model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'UDA algorithm and MLM pre-training']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'MLM method']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'our framework']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'weights']]\",\n",
       " \"[['Model', 'has', 'attention mechanism']]\",\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'size']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'BoW']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'FTS- BRNN']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'NTEE']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'NABoE - full model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[[\\'Model\\', \\'has\\', \"words \\' contextual information and task information\"]]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'BOW method']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Word2 Vec method']]\",\n",
       " \"[['Results', 'has', 'Our method']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'word embedding methods']]\",\n",
       " \"[['Results', 'has', 'Our method']]\",\n",
       " \"[['Results', 'has', 'Our method']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'edge']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'TF - IDF + LR']]\",\n",
       " \"[['Baselines', 'has', 'Logistic Regression']]\",\n",
       " \"[['Baselines', 'has', 'CNN']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'LSTM']]\",\n",
       " \"[['Baselines', 'has', 'Bi- LSTM']]\",\n",
       " \"[['Baselines', 'has', 'PV - DBOW']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'PV - DM']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'PTE']]\",\n",
       " \"[['Baselines', 'has', 'fast Text']]\",\n",
       " \"[['Baselines', 'has', 'SWEM']]\",\n",
       " \"[['Baselines', 'has', 'LEAM']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Graph - CNN - C']]\",\n",
       " \"[['Baselines', 'has', 'Graph - CNN - S']]\",\n",
       " \"[['Baselines', 'has', 'Graph - CNN - F']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Text GCN']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'LSTM - based models']]\",\n",
       " \"[['Results', 'has', 'PV - DBOW']]\",\n",
       " \"[['Results', 'has', 'PV - DM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'network depth']]\",\n",
       " \"[['Model', 'has', 'computational complexity']]\",\n",
       " '[]',\n",
       " \"[['Model', 'has', 'first layer']]\",\n",
       " '[]',\n",
       " \"[['Model', 'has', 'final pooling layer']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Large data results']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Small data results']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'ShallowCNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'LSTM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'one - hot CNN']]\",\n",
       " \"[['Results', 'has', 'previous best performance']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'pre-trained wv - LSTM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'LSTM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'Adversarial perturbations']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Our unidirectional LSTM model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Adversarial training']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'CNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'word vector layer and the LSTM layer']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Sentiment Classification']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Question Type Classification']]\",\n",
       " \"[['Results', 'has', 'consistently outperforms']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'dictionary']]\",\n",
       " \"[['Experimental setup', 'has', 'input text']]\",\n",
       " \"[['Experimental setup', 'has', 'character embedding']]\",\n",
       " \"[['Experimental setup', 'has', 'Training']]\",\n",
       " \"[['Experimental setup', 'has', 'implementation']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'deep architecture']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'most important decrease']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Bag - of - words']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Bag - of - ngrams']]\",\n",
       " \"[['Hyperparameters', 'has', 'bag - of - ngrams models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Word - based ConvNets']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Traditional methods']]\",\n",
       " \"[['Results', 'has', 'Conv Nets']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'dimension']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'BLSTM - 2DCNN model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'BLSTM - 2DPooling']]\",\n",
       " \"[['Results', 'has', 'BLSTM - CNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'LR model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'non-neural LR and SVM baselines']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Binary Sentiment Tweets']]\",\n",
       " \"[['Results', 'has', 'Transformer']]\",\n",
       " \"[['Results', 'has', 'Multi - Label Emotion Tweets']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Sem Eval Tweets']]\",\n",
       " \"[['Results', 'has', 'Our model']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Our models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'SVDCNN experimental settings']]\",\n",
       " \"[['Experimental setup', 'has', 'training']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'network reduction']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'most in - depth model']]\",\n",
       " \"[['Ablation analysis', 'has', 'accuracy results']]\",\n",
       " \"[['Results', 'has', 'performance difference']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'proposed LEAM']]\",\n",
       " \"[['Model', 'has', 'label embedding framework']]\",\n",
       " \"[['Model', 'has', 'LEAM learning procedure']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'Out - Of - Vocabulary ( OOV ) words']]\",\n",
       " \"[['Baselines', 'has', 'final classifier']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'Dropout regularization']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'LEAM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'HDLTex']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'processing']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'CNN']]\",\n",
       " \"[['Results', 'has', 'term weighting']]\",\n",
       " \"[['Results', 'has', 'nave Bayes']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'proposed framework']]\",\n",
       " \"[['Model', 'has', 'word - level encoder']]\",\n",
       " \"[['Model', 'has', 'interaction layer']]\",\n",
       " \"[['Model', 'has', 'last layer']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Multi - Class Classification']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'region size']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'baselines']]\",\n",
       " \"[['Baselines', 'has', 'models']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Models']]\",\n",
       " \"[['Results', 'has', 'Char - based models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Multi - Label Classification']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'accumulated MLP']]\",\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'validation set']]\",\n",
       " \"[['Results', 'has', 'Word - based models']]\",\n",
       " \"[['Results', 'has', 'Our models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Zero - shot cross - lingual document classification']]\",\n",
       " \"[['Results', 'has', 'classifiers']]\",\n",
       " \"[['Results', 'has', 'system']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Joint multilingual document classification']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'hyperparameter']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Our model DRNN']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'Three strategies']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'LSTM / Bi - LSTM']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'Single - Label to Multi - Label Text Classification']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'larger improvement']]\",\n",
       " \"[['Ablation analysis', 'has', 'capsule network']]\",\n",
       " '[]',\n",
       " \"[['Ablation analysis', 'has', 'Connection Strength Visualization']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'Entity vectors']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'Training']]\",\n",
       " \"[['Experimental setup', 'has', 'Our local and global ED models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'our models']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'ELMo representations']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Model', 'has', 'GAS']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'Orthogonal initialization']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Hyperparameters', 'has', 'Training']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Babelfy']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'IMS']]\",\n",
       " \"[['Baselines', 'has', 'IMS +emb']]\",\n",
       " '[]',\n",
       " \"[['Baselines', 'has', 'Bi- LSTM']]\",\n",
       " '[]',\n",
       " \"[['Results', 'has', 'English all - words results']]\",\n",
       " \"[['Results', 'has', 'GAS and GAS ext']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Results', 'has', 'Multiple Passes Analysis']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"[['Experimental setup', 'has', 'embeddings']]\",\n",
       " \"[['Experimental setup', 'has', 'Words']]\",\n",
       " \"[['Results', 'has', 'Our proposed model']]\",\n",
       " '[]',\n",
       " '[]',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "middle-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data,columns=['triple_D'])\n",
    "data.to_csv('D.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-grace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
