(substantial gap||between||model performance)
(Ft ( XLM ) results||has||substantial gap)
(Results||Looking at||Ft ( XLM ) results)
(UDA algorithm and MLM pre-training||offer||significant improvements)
(significant improvements||by utilizing||unlabeled data)
(UDA algorithm and MLM pre-training||has||significant improvements)
(Results||has||UDA algorithm and MLM pre-training)
(Ft ( XLM ft ) model||usnig||MLM pre-training)
(Ft ( XLM ft ) model||provides||larger improvements)
(MLM pre-training||provides||larger improvements)
(larger improvements||compared with||UDA method)
(sentiment classification task||has||Ft ( XLM ft ) model)
(Ft ( XLM ft ) model||has||MLM pre-training)
(Results||In||sentiment classification task)
(MLM method||is||relatively more resource intensive)
(MLM method||takes||longer)
(longer||to||converge)
(MLM method||has||relatively more resource intensive)
(Results||has||MLM method)
(size||of||unlabeled samples)
(size||is||limited)
(unlabeled samples||is||limited)
(MLdoc dataset||has||size)
(UDA method||has||more helpful)
(Results||in||MLdoc dataset)
(consistently improves||over||teacher model)
(self - training technique||has||consistently improves)
(Results||observe||self - training technique)
(best results||in||both XLM and XLM ft based classifiers)
(self - training||achieves||best results over all)
(MLdoc dataset||has||self - training)
(best cross - lingual results and monolingual fine - tune baseline||completely close||performance gap)
(performance gap||by utilizing||unlabeled data)
(unlabeled data||in||target language)
(best cross - lingual results and monolingual fine - tune baseline||has||performance gap)
(Results||comparing with||best cross - lingual results and monolingual fine - tune baseline)
(our framework||reaches||new state - of - the - art results)
(our framework||improving over||vanilla XLM baselines)
(new state - of - the - art results||improving over||vanilla XLM baselines)
(vanilla XLM baselines||by||44 % on average)
(Results||has||our framework)
(Contribution||has||Results)
