(our model||used||stochastic gradient descent)
(stochastic gradient descent||with||ADAM optimizer ( Kingma and Ba , 2014 ))
(stochastic gradient descent||with||initial learning rate)
(initial learning rate||of||0.001)
(Experimental setup||train||our model)
(word embeddings||drawing from||uniform distribution)
(initialized||drawing from||uniform distribution)
(randomly||drawing from||uniform distribution)
(word embeddings||has||initialized)
(initialized||has||randomly)
(Experimental setup||has||word embeddings)
(batches||of||32 examples)
(patience||of||2 epochs)
(early stopping||with||patience)
(patience||of||2 epochs)
(Experimental setup||used||batches)
(model||implement in||Theano)
(Theano||using||Keras framework)
(Experimental setup||has||model)
(regularization||at||0.001 , ? = 50 , and ? = 0.04)
(Experimental setup||has||All our models)
(Contribution||has||Experimental setup)
