(our models||on||Stanford Question Answering Dataset ( SQuAD ))
(Results||test||our models)
(SQuAD dataset||consists of||more than 100,000 questions)
(more than 100,000 questions||annotated by||crowdsourcing workers)
(crowdsourcing workers||on||selected set)
(selected set||of||Wikipedia articles)
(Results||has||SQuAD dataset)
(Our model||achieves||68.73 %)
(Our model||achieves||77.39 %)
(F1 score||ranked among||state of the art single models)
(68.73 %||has||EM score)
(77.39 %||has||F1 score)
(Results||has||Our model)
(Our baseline model||using||no Q- code)
(no Q- code||achieved||68.00 % and 77.36 %)
(68.00 % and 77.36 %||has||EM and F 1 scores)
(explicit question type T - code||into||baseline model)
(improved slightly||to||68.16 % ( EM ) and 77.58 % ( F1 ))
(performance||has||improved slightly)
(Results||added||explicit question type T - code)
(TreeLSTM||introduce||syntactic parses)
(syntactic parses||for||question representation and understanding)
(syntactic parses||shows||further improvement)
(TreeLSTM||has||syntactic parses)
(Results||used||TreeLSTM)
(78.38 % F1 score||on||whole development set)
(Contribution||has||Results)
