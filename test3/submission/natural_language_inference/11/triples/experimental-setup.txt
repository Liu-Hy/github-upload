(corpus||use||tokenizer)
(tokenizer||from||Stanford CoreNLP)
(Experimental setup||To preprocess||corpus)
(Glo Ve word vectors||pretrained on||840B Common Crawl corpus)
(Experimental setup||use||Glo Ve word vectors)
(vocabulary||to||words)
(out - of - vocabulary words||to||zero)
(words||present in||Common Crawl corpus)
(embeddings||for||out - of - vocabulary words)
(out - of - vocabulary words||to||zero)
(vocabulary||has||words)
(Experimental setup||limit||vocabulary)
(max sequence length||of||600)
(hidden state size||of||200)
(600||during||training)
(hidden state size||of||200)
(hidden state size||for||recurrent units)
(200||for||recurrent units)
(200||for||linear layers)
(max sequence length||has||600)
(hidden state size||has||200)
(initial state||of||zero)
(All LSTMs||has||randomly initialized)
(randomly initialized||has||parameters)
(Experimental setup||has||All LSTMs)
(optimized||during||training)
(Sentinel vectors||has||randomly initialized)
(Experimental setup||has||Sentinel vectors)
(dynamic decoder||set||maximum number)
(maximum number||of||iterations)
(maxout pool size||of||16)
(iterations||to||4)
(dynamic decoder||use||maxout pool size)
(maxout pool size||of||16)
(dynamic decoder||has||maximum number)
(maximum number||has||iterations)
(maxout pool size||has||16)
(Experimental setup||For||dynamic decoder)
(dropout||to regularize||our network)
(our network||during||training)
(dropout||optimize||model)
(model||using||ADAM)
(All models||implemented and trained with||Chainer)
(Experimental setup||has||All models)
(Contribution||has||Experimental setup)
