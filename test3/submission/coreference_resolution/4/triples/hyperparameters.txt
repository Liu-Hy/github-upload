(Glo Ve word embeddings||pre-trained on||Gigaword and Wikipedia)
(Hyperparameters||has||Glo Ve word embeddings)
(Vocabulary||built from||words)
(words||in||training data)
(frequency||in||{ 3 , U ( 1 , 10 ) })
(words||with||frequency)
(training data||with||frequency)
(frequency||in||{ 3 , U ( 1 , 10 ) })
(OOV words||replaced with||UNK token)
(Hyperparameters||has||Vocabulary)
(size||of||LSTMs hidden states)
(LSTMs hidden states||set to||{ 100 , qlog - U ( 30 , 150 ) })
(Hyperparameters||has||size)
(weight matrices||of||LSTMs)
(LSTMs||with||random orthogonal matrices)
(Hyperparameters||initialized||weight matrices)
(first feed - forward layer size||set to||value)
(value||in||Optimization)
(first feed - forward layer size||has||value)
(Hyperparameters||has||first feed - forward layer size)
(model||in||minibatches)
(minibatches||using||Adam)
(Adam||with||learning rate)
(Adam||with||maximal batch size)
(learning rate||of||10 ? 4)
(learning rate||has||10 ? 4)
(maximal batch size||has||64)
(Hyperparameters||trained||model)
(gradients||by||global norm)
(global norm||with||clipping value)
(clipping value||in||{ 1.0 , U ( 1 , 100 ) })
(Hyperparameters||clip||gradients)
(Hyperparameters||train||10 epochs)
(l 2 - regularization||with||{ 10 ?5 , log - U (10 ?7 , 10 ?2 ) })
(l 2 - regularization||has||{ 10 ?5 , log - U (10 ?7 , 10 ?2 ) })
(Hyperparameters||used||l 2 - regularization)
(Dropout||with||keep probability k p)
(input||with||k p ? U (0.8 , 1.0 ))
(keep probability k p||applied to||outputs)
({ 0.8 , U( 0.5 , 1.0 ) }||applied to||outputs)
(outputs||of||LSTMs)
(keep probability k p||optionally to||input)
(input||with||k p ? U (0.8 , 1.0 ))
(keep probability k p||has||{ 0.8 , U( 0.5 , 1.0 ) })
(Hyperparameters||has||Dropout)
(Contribution||has||Hyperparameters)
