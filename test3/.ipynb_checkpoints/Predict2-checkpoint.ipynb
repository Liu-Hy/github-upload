{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designing-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "import logging\n",
    "import argparse\n",
    "from simpletransformers.ner import (\n",
    "    NERArgs,\n",
    "    NERModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "productive-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_type = 1\n",
    "\n",
    "labelset = [\"B-p\", \"I-p\", \"B-n\", \"I-n\", \"O\"]\n",
    "type_ls = ['-p', '-n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "animal-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "pos = pd.read_csv('pos_sent.csv')\n",
    "col_name = pos.columns[6+BIO_type]\n",
    "pos = pos.dropna(axis=0, subset=[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriented-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(pos)):\n",
    "    words = pos.loc[i, 'text'].split(' ')\n",
    "    tags = literal_eval(pos.iloc[i, 6+BIO_type])\n",
    "    for j in range(len(words)):\n",
    "        data.append([i, words[j], tags[j]])\n",
    "df = pd.DataFrame(data, columns=['sentence_id', 'words', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "representative-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = NERArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.fp16 = False\n",
    "model_args.manual_seed = 1\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.do_lower_case = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "superior-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_F1(ref, pred):\n",
    "    return 0\n",
    "\n",
    "# Create a TransformerModel\n",
    "model = NERModel(\n",
    "    \"xlmroberta\",\n",
    "    \"../ner/Noutputs/best_model\",\n",
    "    labels=labelset,\n",
    "    args=model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indonesian-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.ner.ner_model: Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12511d014e1e4fb7856d68f749b29cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4455325903a44b269dea00809022dd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hl/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "INFO:simpletransformers.ner.ner_model:{'eval_loss': 2.7722322740975547, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'F1_score': 0}\n"
     ]
    }
   ],
   "source": [
    "result, model_outputs, pred_label = model.eval_model(df, F1_score=phrase_F1)\n",
    "# 'pred_label' is a list, where each element is the list of predicted labels for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "filled-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[str(l) for l in pred_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "powered-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('pos_sent.csv')\n",
    "pos['BIO_1']=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guided-princess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>main_heading</th>\n",
       "      <th>heading</th>\n",
       "      <th>topic</th>\n",
       "      <th>paper_idx</th>\n",
       "      <th>BIO</th>\n",
       "      <th>BIO_1</th>\n",
       "      <th>BIO_2</th>\n",
       "      <th>offset1</th>\n",
       "      <th>pro1</th>\n",
       "      <th>offset2</th>\n",
       "      <th>pro2</th>\n",
       "      <th>offset3</th>\n",
       "      <th>pro3</th>\n",
       "      <th>mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>title</th>\n",
       "      <th>paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Recurrent Neural Network Grammars</td>\n",
       "      <td>title</td>\n",
       "      <td>title</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>['O', 'O', 'O', 'O']</td>\n",
       "      <td>['B-n', 'I-n', 'I-n', 'I-n']</td>\n",
       "      <td>['O', 'O', 'O', 'O']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>research-problem</td>\n",
       "      <td>title</td>\n",
       "      <td>constituency_parsing0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>In this paper , we introduce recurrent neural ...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n',...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>11</td>\n",
       "      <td>0.049327</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>model</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>constituency_parsing0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>RNNGs operate via a recursive syntactic proces...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>12</td>\n",
       "      <td>0.053812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>model</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>constituency_parsing0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>We give two variants of the algorithm , one fo...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>14</td>\n",
       "      <td>0.062780</td>\n",
       "      <td>7</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1</td>\n",
       "      <td>model</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>constituency_parsing0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>The discriminative model also lets us use ance...</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>Refer to for an example .</td>\n",
       "      <td>constituency_parsing</td>\n",
       "      <td>0</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n',...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>23</td>\n",
       "      <td>0.103139</td>\n",
       "      <td>16</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>model</td>\n",
       "      <td>Introduction: Refer to for an example .</td>\n",
       "      <td>constituency_parsing0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>56</td>\n",
       "      <td>We trained the model by minimizing loss L from...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-n', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>55</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>hyperparameters</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>57</td>\n",
       "      <td>As is common practice for BERT models , we onl...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>56</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>4</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1</td>\n",
       "      <td>hyperparameters</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>58</td>\n",
       "      <td>Evaluation completed in about 5 hours on the N...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>57</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>5</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1</td>\n",
       "      <td>results</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>60</td>\n",
       "      <td>Our BERT model for NQ performs dramatically be...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>59</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1</td>\n",
       "      <td>results</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>61</td>\n",
       "      <td>Our model closes the gap between the F 1 score...</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference</td>\n",
       "      <td>9</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>60</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1</td>\n",
       "      <td>results</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>natural_language_inference9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2720 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx                                               text  main_heading  \\\n",
       "0       2                  Recurrent Neural Network Grammars         title   \n",
       "1      12  In this paper , we introduce recurrent neural ...  Introduction   \n",
       "2      13  RNNGs operate via a recursive syntactic proces...  Introduction   \n",
       "3      15  We give two variants of the algorithm , one fo...  Introduction   \n",
       "4      24  The discriminative model also lets us use ance...  Introduction   \n",
       "...   ...                                                ...           ...   \n",
       "2715   56  We trained the model by minimizing loss L from...   Experiments   \n",
       "2716   57  As is common practice for BERT models , we onl...   Experiments   \n",
       "2717   58  Evaluation completed in about 5 hours on the N...   Experiments   \n",
       "2718   60  Our BERT model for NQ performs dramatically be...   Experiments   \n",
       "2719   61  Our model closes the gap between the F 1 score...   Experiments   \n",
       "\n",
       "                        heading                       topic  paper_idx  \\\n",
       "0                         title        constituency_parsing          0   \n",
       "1                  Introduction        constituency_parsing          0   \n",
       "2                  Introduction        constituency_parsing          0   \n",
       "3                  Introduction        constituency_parsing          0   \n",
       "4     Refer to for an example .        constituency_parsing          0   \n",
       "...                         ...                         ...        ...   \n",
       "2715                Experiments  natural_language_inference          9   \n",
       "2716                Experiments  natural_language_inference          9   \n",
       "2717                Experiments  natural_language_inference          9   \n",
       "2718                Experiments  natural_language_inference          9   \n",
       "2719                Experiments  natural_language_inference          9   \n",
       "\n",
       "                                                    BIO  \\\n",
       "0                                  ['O', 'O', 'O', 'O']   \n",
       "1     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "2     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "3     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "4     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "...                                                 ...   \n",
       "2715  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "2716  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "2717  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "2718  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "2719  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "\n",
       "                                                  BIO_1  \\\n",
       "0                          ['B-n', 'I-n', 'I-n', 'I-n']   \n",
       "1     ['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n',...   \n",
       "2     ['B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n'...   \n",
       "3     ['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', ...   \n",
       "4     ['O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'B-n',...   \n",
       "...                                                 ...   \n",
       "2715  ['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-n', ...   \n",
       "2716  ['O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O'...   \n",
       "2717  ['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-...   \n",
       "2718  ['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-...   \n",
       "2719  ['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n...   \n",
       "\n",
       "                                                  BIO_2  offset1      pro1  \\\n",
       "0                                  ['O', 'O', 'O', 'O']        1  0.000000   \n",
       "1     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        4  0.043478   \n",
       "2     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        5  0.054348   \n",
       "3     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        7  0.076087   \n",
       "4     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...       16  0.173913   \n",
       "...                                                 ...      ...       ...   \n",
       "2715  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        3  0.333333   \n",
       "2716  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        4  0.444444   \n",
       "2717  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        5  0.555556   \n",
       "2718  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        7  0.777778   \n",
       "2719  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        8  0.888889   \n",
       "\n",
       "      offset2      pro2  offset3      pro3  mask            labels  \\\n",
       "0           1  0.004484        1  0.000000     1  research-problem   \n",
       "1          11  0.049327        4  0.200000     1             model   \n",
       "2          12  0.053812        5  0.250000     1             model   \n",
       "3          14  0.062780        7  0.350000     1             model   \n",
       "4          23  0.103139       16  0.800000     1             model   \n",
       "...       ...       ...      ...       ...   ...               ...   \n",
       "2715       55  0.846154        3  0.333333     1   hyperparameters   \n",
       "2716       56  0.861538        4  0.444444     1   hyperparameters   \n",
       "2717       57  0.876923        5  0.555556     1           results   \n",
       "2718       59  0.907692        7  0.777778     1           results   \n",
       "2719       60  0.923077        8  0.888889     1           results   \n",
       "\n",
       "                                        title                        paper  \n",
       "0                                       title        constituency_parsing0  \n",
       "1                                Introduction        constituency_parsing0  \n",
       "2                                Introduction        constituency_parsing0  \n",
       "3                                Introduction        constituency_parsing0  \n",
       "4     Introduction: Refer to for an example .        constituency_parsing0  \n",
       "...                                       ...                          ...  \n",
       "2715                              Experiments  natural_language_inference9  \n",
       "2716                              Experiments  natural_language_inference9  \n",
       "2717                              Experiments  natural_language_inference9  \n",
       "2718                              Experiments  natural_language_inference9  \n",
       "2719                              Experiments  natural_language_inference9  \n",
       "\n",
       "[2720 rows x 19 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "offensive-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.to_csv('pos_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "turned-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_spans(k, ls):\n",
    "    spans = [[],[]]\n",
    "    for i in range(len(ls)):\n",
    "        if ls[i]=='B-p':\n",
    "            for j in range(i+1,len(ls)):\n",
    "                if ls[j]!='I-p':\n",
    "                    phrase=' '.join(pos.loc[k,'text'].split(' ')[i:j])\n",
    "                    tup=(phrase, (i, j))\n",
    "                    spans[0].append(tup)\n",
    "                    break\n",
    "                elif j==len(ls)-1:\n",
    "                    phrase=' '.join(pos.loc[k,'text'].split(' ')[i:(j+1)])\n",
    "                    tup=(phrase, (i, (j+1)))\n",
    "                    spans[0].append(tup)\n",
    "        elif ls[i]=='B-n':\n",
    "            for j in range(i+1,len(ls)):\n",
    "                if ls[j]!='I-n':\n",
    "                    phrase=' '.join(pos.loc[k,'text'].split(' ')[i:j])\n",
    "                    tup=(phrase, (i, j))\n",
    "                    spans[1].append(tup)\n",
    "                    break\n",
    "                elif j==len(ls)-1:\n",
    "                    phrase=' '.join(pos.loc[k,'text'].split(' ')[i:(j+1)])\n",
    "                    tup=(phrase, (i, (j+1)))\n",
    "                    spans[1].append(tup)\n",
    "    return spans\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "maritime-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[], [('Recurrent Neural Network Grammars', (0, 4))]],\n",
       " [[('introduce', (5, 6)),\n",
       "   ('of', (21, 22)),\n",
       "   ('explicitly models', (24, 26)),\n",
       "   ('among', (30, 31))],\n",
       "  [('recurrent neural network grammars ( RNNGs', (6, 12)),\n",
       "   ('sentences', (22, 23)),\n",
       "   ('nested , hierarchical relationships', (26, 30)),\n",
       "   ('words and phrases', (31, 34))]],\n",
       " [[('operate via', (1, 3)),\n",
       "   ('reminiscent of', (7, 9)),\n",
       "   ('using', (20, 21)),\n",
       "   ('condition on', (23, 25)),\n",
       "   ('greatly relaxing', (31, 33))],\n",
       "  [('RNNGs', (0, 1)),\n",
       "   ('recursive syntactic process', (4, 7)),\n",
       "   ('probabilistic context - free grammar generation', (9, 15)),\n",
       "   ('decisions', (17, 18)),\n",
       "   ('parameterized', (19, 20)),\n",
       "   ('RNNs', (21, 22)),\n",
       "   ('entire syntactic derivation history', (26, 30)),\n",
       "   ('context - free independence assumptions', (33, 38))]],\n",
       " [[('give', (1, 2)), ('of', (4, 5)), ('one for', (8, 10))],\n",
       "  [('two variants', (2, 4)),\n",
       "   ('algorithm', (6, 7)),\n",
       "   ('parsing', (10, 11)),\n",
       "   ('generation', (27, 28))]],\n",
       " [[('use', (6, 7)),\n",
       "   ('to obtain', (9, 11)),\n",
       "   ('of', (12, 13)),\n",
       "   ('for', (15, 16)),\n",
       "   ('approximating', (31, 32)),\n",
       "   ('of', (38, 39)),\n",
       "   ('under', (41, 42))],\n",
       "  [('discriminative', (1, 2)),\n",
       "   ('ancestor sampling', (7, 9)),\n",
       "   ('samples', (11, 12)),\n",
       "   ('sentences', (16, 17)),\n",
       "   ('marginal likelihood and MAP tree', (33, 38)),\n",
       "   ('sentence', (40, 41)),\n",
       "   ('generative model', (43, 45))]],\n",
       " [[('present', (1, 2)),\n",
       "   ('which uses', (7, 9)),\n",
       "   ('from', (10, 11)),\n",
       "   ('to solve', (14, 16)),\n",
       "   ('in', (18, 19))],\n",
       "  [('simple importance sampling algorithm', (3, 7)),\n",
       "   ('samples', (9, 10)),\n",
       "   ('discriminative parser', (12, 14)),\n",
       "   ('inference problems', (16, 18)),\n",
       "   ('generative model', (20, 22))]],\n",
       " [[('used', (6, 7)), ('of', (9, 10))],\n",
       "  [('hidden dimensions', (7, 9)), ('128 and 2 - layer LSTMs', (10, 16))]],\n",
       " [[('used', (6, 7))],\n",
       "  [('generative model', (2, 4)),\n",
       "   ('256 dimensions and', (7, 10)),\n",
       "   ('2 layer LSTMs', (10, 13))]],\n",
       " [[('tuned', (5, 6)),\n",
       "   ('to maximize', (9, 11)),\n",
       "   ('obtaining', (15, 16)),\n",
       "   ('of', (18, 19))],\n",
       "  [('dropout rate', (7, 9)),\n",
       "   ('validation set likelihood', (11, 14)),\n",
       "   ('optimal rates', (16, 18)),\n",
       "   ('0.2 ( discriminative )', (19, 23)),\n",
       "   ('0.3 ( generative )', (24, 28))]],\n",
       " [[('For', (0, 1)), ('for', (5, 6)), ('found', (12, 13)), ('of', (17, 18))],\n",
       "  [('sequential LSTM baseline', (2, 5)),\n",
       "   ('language model', (7, 9)),\n",
       "   ('optimal dropout rate', (14, 17)),\n",
       "   ('0.3', (18, 19))]],\n",
       " [[('used', (3, 4)), ('with', (7, 8)), ('of', (11, 12))],\n",
       "  [('training', (1, 2)),\n",
       "   ('stochastic gradient descent', (4, 7)),\n",
       "   ('learning rate', (9, 11)),\n",
       "   ('0.1', (12, 13))]],\n",
       " [[], [('Cloze - driven Pretraining of Self - attention Networks', (0, 9))]],\n",
       " [[], [('pretraining', (6, 7))]],\n",
       " [[], [('Language model pretraining', (0, 3))]],\n",
       " [[('show', (5, 6)), ('of', (18, 19))],\n",
       "  [('even larger performance gains', (7, 11)),\n",
       "   ('both directions', (16, 18)),\n",
       "   ('large language - model - inspired self - attention cloze model',\n",
       "    (20, 31))]],\n",
       " [[('predicts', (4, 5)), ('in', (7, 8))],\n",
       "  [('every token', (5, 7)), ('training data', (9, 11))]],\n",
       " [[('introducing', (4, 5)),\n",
       "   ('where', (11, 12)),\n",
       "   ('must predict', (14, 16)),\n",
       "   ('given', (19, 20))],\n",
       "  [('cloze - style training objective', (6, 11)),\n",
       "   ('model', (13, 14)),\n",
       "   ('center word', (17, 19)),\n",
       "   ('left - to - right and right - to - left context representations',\n",
       "    (20, 33))]],\n",
       " [[('separately computes', (2, 4)), ('with', (9, 10))],\n",
       "  [('both forward and backward states', (4, 9)),\n",
       "   ('* Equal contribution', (10, 13))]],\n",
       " [[], [('Experimental setup', (0, 2))]],\n",
       " [[('use', (2, 3)),\n",
       "   ('in', (6, 7)),\n",
       "   ('contains', (12, 13)),\n",
       "   ('with', (18, 19)),\n",
       "   ('followed by', (22, 24)),\n",
       "   ('with', (28, 29)),\n",
       "   ('with', (32, 33)),\n",
       "   ('if', (41, 42)),\n",
       "   ('exceeds', (44, 45))],\n",
       "  [('CNN', (0, 1)),\n",
       "   ('adaptive softmax', (4, 6)),\n",
       "   ('output', (8, 9)),\n",
       "   ('headband', (11, 12)),\n",
       "   ('60K most frequent types', (14, 18)),\n",
       "   ('160 K band', (25, 28)),\n",
       "   ('dimensionality 256', (29, 31)),\n",
       "   ('momentum', (34, 35)),\n",
       "   ('0.99', (36, 37)),\n",
       "   ('renormalize', (39, 40)),\n",
       "   ('gradients', (40, 41)),\n",
       "   ('norm', (43, 44)),\n",
       "   ('0.1', (45, 46))]],\n",
       " [[('from', (7, 8)),\n",
       "   ('for', (13, 14)),\n",
       "   ('annealed using', (19, 21)),\n",
       "   ('with', (26, 27)),\n",
       "   ('to', (30, 31))],\n",
       "  [('learning rate', (1, 3)),\n",
       "   ('linearly warmed up', (4, 7)),\n",
       "   ('10 ? 7 to 1', (8, 13)),\n",
       "   ('16 K steps', (14, 17)),\n",
       "   ('cosine learning rate schedule', (22, 26)),\n",
       "   ('single phase', (28, 30)),\n",
       "   ('0.0001', (31, 32))]],\n",
       " [[('on', (3, 4)), ('with', (8, 9)), ('interconnected by', (16, 18))],\n",
       "  [('DGX - 1 machines', (4, 8)),\n",
       "   ('8 NVIDIA V100 GPUs', (9, 13)),\n",
       "   ('Infiniband', (18, 19))]],\n",
       " [[('use', (2, 3))], [('NCCL2 library', (4, 6)), ('torch', (8, 9))]],\n",
       " [[], []],\n",
       " [[], []],\n",
       " [[('than', (22, 23))],\n",
       "  [('outperform', (3, 4)),\n",
       "   ('uni-directional transformer ( OpenAI GPT )', (5, 11)),\n",
       "   ('our model', (15, 17)),\n",
       "   ('about 50 % larger', (18, 22))]],\n",
       " [[('performs as', (4, 6)), ('in', (9, 10)), ('performs', (23, 24))],\n",
       "  [('Our CNN base model', (0, 4)),\n",
       "   ('STILTs', (8, 9)),\n",
       "   ('aggregate', (10, 11)),\n",
       "   ('much', (24, 25))]],\n",
       " [[], [('Named Entity Recognition', (0, 3))]],\n",
       " [[('with comparison to', (4, 7)), ('gives', (18, 19))],\n",
       "  [('previous published ELMo BASE', (7, 11)),\n",
       "   ('fine tuning', (16, 18)),\n",
       "   ('biggest gain', (20, 22))]],\n",
       " [[], [('Constituency Parsing', (0, 2))]],\n",
       " [[('results in', (1, 3)),\n",
       "   ('dominating', (6, 7)),\n",
       "   ('scaling', (14, 15)),\n",
       "   ('by', (18, 19)),\n",
       "   ('of', (21, 22)),\n",
       "   ('results in', (23, 25))],\n",
       "  [('bilm loss', (4, 6)),\n",
       "   ('triplet loss', (8, 10)),\n",
       "   ('bilm term', (16, 18)),\n",
       "   ('factor', (20, 21)),\n",
       "   ('0.15', (22, 23)),\n",
       "   ('better performance', (25, 27))]],\n",
       " [[('shows', (0, 1)),\n",
       "   ('performs', (5, 6)),\n",
       "   ('than', (8, 9)),\n",
       "   ('combining', (14, 15)),\n",
       "   ('over', (22, 23))],\n",
       "  [('cloze loss', (3, 5)),\n",
       "   ('significantly better', (6, 8)),\n",
       "   ('bilm loss', (10, 12)),\n",
       "   ('two loss types', (16, 19)),\n",
       "   ('does not improve', (19, 22)),\n",
       "   ('cloze loss', (24, 26))]],\n",
       " [[], [('Constituency Parsing', (9, 11))]],\n",
       " [[('of', (14, 15))],\n",
       "  [('Seq2seq approach', (6, 8)), ('constituency parsing', (15, 17))]],\n",
       " [[('provided', (11, 12))],\n",
       "  [('Smaller mini-batch size M and gradient clipping G', (3, 11)),\n",
       "   ('better performance', (13, 15))]],\n",
       " [[('have', (14, 15)),\n",
       "   ('on', (17, 18)),\n",
       "   ('looks', (36, 37)),\n",
       "   ('in terms of', (38, 41))],\n",
       "  [('Larger layer size', (3, 6)),\n",
       "   ('little impact', (15, 17)),\n",
       "   ('performance', (19, 20)),\n",
       "   ('our', (21, 22)),\n",
       "   ('adequate', (37, 38)),\n",
       "   ('speed / performance trade - off', (41, 47))]],\n",
       " [[('using', (8, 9)),\n",
       "   ('as', (11, 12)),\n",
       "   ('instead of', (15, 17)),\n",
       "   ('potential', (22, 23))],\n",
       "  [('subword split', (9, 11)),\n",
       "   ('input token unit', (12, 15)),\n",
       "   ('standard tokenized word unit', (17, 21)),\n",
       "   ('performance', (26, 27))]],\n",
       " [[('using', (2, 3)),\n",
       "   ('as', (5, 6)),\n",
       "   ('is', (7, 8)),\n",
       "   ('for leveraging', (11, 13)),\n",
       "   ('into', (15, 16))],\n",
       "  [('subword information', (3, 5)),\n",
       "   ('features', (6, 7)),\n",
       "   ('promising approach', (9, 11)),\n",
       "   ('subword information', (13, 15)),\n",
       "   ('constituency parsing', (16, 18))]],\n",
       " [[('successfully achieved', (3, 5)), ('as', (8, 9))],\n",
       "  [('Our Seq2seq approach', (0, 3)),\n",
       "   ('competitive level', (6, 8)),\n",
       "   ('current', (10, 11)),\n",
       "   ('RNNG', (16, 17))]],\n",
       " [[], [('Syntactic constituency parsing', (0, 3))]],\n",
       " [[('trained it on', (9, 12)),\n",
       "   ('constructed', (25, 26)),\n",
       "   ('by labelling', (29, 31)),\n",
       "   ('with', (34, 35))],\n",
       "  [('work', (5, 6)),\n",
       "   ('standard human - annotated parsing datasets ( 1M tokens )', (12, 22)),\n",
       "   ('artificial dataset', (27, 29)),\n",
       "   ('large corpus', (32, 34)),\n",
       "   ('BerkeleyParser', (36, 37))]],\n",
       " [[('trained', (1, 2)),\n",
       "   ('with', (9, 10)),\n",
       "   ('on', (11, 12)),\n",
       "   ('of', (28, 29)),\n",
       "   ('on', (30, 31)),\n",
       "   ('of', (33, 34)),\n",
       "   ('without the use of', (36, 40))],\n",
       "  [('sequence - to - sequence model', (3, 9)),\n",
       "   ('attention', (10, 11)),\n",
       "   ('small human - annotated parsing dataset', (13, 19)),\n",
       "   ('F 1 score', (25, 28)),\n",
       "   ('88.3', (29, 30)),\n",
       "   ('section 23', (31, 33)),\n",
       "   ('WSJ', (35, 36)),\n",
       "   ('ensemble', (41, 42)),\n",
       "   ('ensemble', (46, 47))]],\n",
       " [[('constructed', (3, 4)),\n",
       "   ('consisting of', (8, 10)),\n",
       "   ('measured by', (18, 20))],\n",
       "  [('second artificial dataset', (5, 8)),\n",
       "   ('only high - confidence parse trees', (10, 16)),\n",
       "   ('agreement', (21, 22))]],\n",
       " [[('trained', (1, 2)),\n",
       "   ('with', (9, 10)),\n",
       "   ('achieved', (15, 16)),\n",
       "   ('of', (20, 21)),\n",
       "   ('on', (22, 23)),\n",
       "   ('of', (25, 26))],\n",
       "  [('sequence - to - sequence model', (3, 9)),\n",
       "   ('attention', (10, 11)),\n",
       "   ('F 1 score', (17, 20)),\n",
       "   ('92.5', (21, 22)),\n",
       "   ('section 23', (23, 25)),\n",
       "   ('WSJ', (27, 28))]],\n",
       " [[('used', (4, 5)), ('with', (7, 8)), ('call', (20, 21))],\n",
       "  [('model', (6, 7)),\n",
       "   ('3 LSTM layers', (8, 11)),\n",
       "   ('256 units', (12, 14)),\n",
       "   ('each layer', (15, 17)),\n",
       "   ('LSTM + A', (21, 24))]],\n",
       " [[('Training on', (0, 2)), ('used', (7, 8))],\n",
       "  [('small dataset', (3, 5)),\n",
       "   ('2 dropout layers', (8, 11)),\n",
       "   ('LSTM 1 and LSTM 2', (14, 19)),\n",
       "   ('LSTM 2 and LSTM 3', (23, 28))]],\n",
       " [[('for', (3, 4)), ('can be', (7, 9)), ('using', (12, 13))],\n",
       "  [('embedding layer', (1, 3)),\n",
       "   ('our 90K vocabulary', (4, 7)),\n",
       "   ('initialized randomly', (9, 11)),\n",
       "   ('pre-trained word - vector embeddings', (13, 18))]],\n",
       " [[('pre-trained', (1, 2)),\n",
       "   ('of size', (6, 8)),\n",
       "   ('using', (9, 10)),\n",
       "   ('on', (14, 15))],\n",
       "  [('skip - gram embeddings', (2, 6)),\n",
       "   ('512', (8, 9)),\n",
       "   ('word2vec', (10, 11)),\n",
       "   ('10B - word corpus', (16, 20))]],\n",
       " [[('gets to', (5, 7)),\n",
       "   ('achieves', (17, 18)),\n",
       "   ('matching', (19, 20)),\n",
       "   ('on', (25, 26))],\n",
       "  [('single attention model', (2, 5)),\n",
       "   ('88.3', (7, 8)),\n",
       "   ('ensemble of 5 LSTM', (10, 14)),\n",
       "   ('90.5', (18, 19)),\n",
       "   ('single - model BerkeleyParser', (21, 25))]],\n",
       " [[('trained on', (1, 3)), ('achieves', (16, 17))],\n",
       "  [('large high - confidence corpus', (4, 9)),\n",
       "   ('single LSTM + A model', (11, 16)),\n",
       "   ('92.5', (17, 18)),\n",
       "   ('outperforms', (20, 21)),\n",
       "   ('best single model', (24, 27)),\n",
       "   ('best ensemble result', (31, 34))]],\n",
       " [[('further improves', (7, 9)), ('to', (11, 12))],\n",
       "  [('ensemble of 5 LSTM+ A models', (1, 7)),\n",
       "   ('score', (10, 11)),\n",
       "   ('92.8', (12, 13))]],\n",
       " [[('trained on', (5, 7)),\n",
       "   ('for', (13, 14)),\n",
       "   ('in', (19, 20)),\n",
       "   ('trained on', (34, 36)),\n",
       "   ('for', (43, 44))],\n",
       "  [('LSTM + A model', (1, 5)),\n",
       "   ('WSJ dataset', (7, 9)),\n",
       "   ('malformed trees', (11, 13)),\n",
       "   ('25 of the 1700 sentences', (14, 19)),\n",
       "   ('our development set', (20, 23)),\n",
       "   ('full high - confidence dataset', (36, 41)),\n",
       "   ('14 sentences ( 0.8 % )', (44, 50))]],\n",
       " [[('between', (2, 3)),\n",
       "   ('on', (7, 8)),\n",
       "   ('of length', (9, 11)),\n",
       "   ('upto', (11, 12)),\n",
       "   ('upto', (15, 16)),\n",
       "   ('is', (17, 18)),\n",
       "   ('for', (19, 20))],\n",
       "  [('difference', (1, 2)),\n",
       "   ('F 1 score', (4, 7)),\n",
       "   ('sentences', (8, 9)),\n",
       "   ('30', (12, 13)),\n",
       "   ('70', (16, 17)),\n",
       "   ('1.3', (18, 19)),\n",
       "   ('BerkeleyParser', (21, 22)),\n",
       "   ('1.7', (23, 24)),\n",
       "   ('baseline LSTM', (26, 28)),\n",
       "   ('0.7', (30, 31)),\n",
       "   ('LSTM + A', (32, 35))]],\n",
       " [[('trained on', (3, 5)),\n",
       "   ('achieved', (18, 19)),\n",
       "   ('of', (23, 24)),\n",
       "   ('on', (25, 26)),\n",
       "   ('on', (29, 30))],\n",
       "  [('LSTM + A', (0, 3)),\n",
       "   ('high - confidence corpus', (6, 10)),\n",
       "   ('F 1 score', (20, 23)),\n",
       "   ('95.7', (24, 25)),\n",
       "   ('QTB', (26, 27)),\n",
       "   ('84.6', (28, 29)),\n",
       "   ('WEB', (30, 31))]],\n",
       " [[], [('constituency parsing', (9, 11))]],\n",
       " [[], [('neural constituency parsing', (3, 6))]],\n",
       " [[],\n",
       "  [('each', (17, 18)),\n",
       "   ('two state - of -', (19, 24)),\n",
       "   ('Recurrent Neural Network Grammar generative parser ( RG )', (33, 42)),\n",
       "   ('LSTM language modeling generative parser ( LM )', (46, 54))]],\n",
       " [[('present and use', (4, 7)),\n",
       "   ('with', (13, 14)),\n",
       "   ('can search directly in', (19, 23))],\n",
       "  [('beam - based search procedure', (8, 13)),\n",
       "   ('augmented state space', (15, 18)),\n",
       "   ('generative models', (24, 26))]],\n",
       " [[('taking', (10, 11)),\n",
       "   ('of', (14, 15)),\n",
       "   ('of', (17, 18)),\n",
       "   ('when selecting', (20, 22)),\n",
       "   ('from', (24, 25)),\n",
       "   ('of', (37, 38))],\n",
       "  [('weighted average', (12, 14)),\n",
       "   ('scores', (16, 17)),\n",
       "   ('both models', (18, 20)),\n",
       "   ('parse', (23, 24)),\n",
       "   (\"base parser 's candidate list\", (26, 31)),\n",
       "   ('improves', (31, 32)),\n",
       "   ('using', (33, 34)),\n",
       "   ('only the score', (34, 37)),\n",
       "   ('generative model', (39, 41)),\n",
       "   ('substantially', (45, 46))]],\n",
       " [[('Augmenting', (0, 1))], [('candidate set', (2, 4))]],\n",
       " [[('decreases', (1, 2)), ('from', (3, 4)), ('to', (6, 7)), ('on', (9, 10))],\n",
       "  [('RG', (0, 1)),\n",
       "   ('performance', (2, 3)),\n",
       "   ('93.45 F1', (4, 6)),\n",
       "   ('92.78 F1', (7, 9)),\n",
       "   ('development set', (11, 13))]],\n",
       " [[], [('Score combination', (0, 2))]],\n",
       " [[('combining', (3, 4)), ('of', (6, 7)), ('of', (14, 15))],\n",
       "  [('scores', (5, 6)),\n",
       "   ('both models', (7, 9)),\n",
       "   ('improves', (9, 10)),\n",
       "   ('using', (11, 12)),\n",
       "   ('score', (13, 14)),\n",
       "   ('either model alone', (15, 18))]],\n",
       " [[], [('Strengthening model combination', (0, 3))]],\n",
       " [[('Combining', (1, 2)), ('obtain', (15, 16))], [('93.94 F1', (16, 18))]],\n",
       " [[('compare to', (4, 6)), ('of', (16, 17)), ('trained from', (21, 23))],\n",
       "  [('ensembling', (13, 14)),\n",
       "   ('multiple instances', (14, 16)),\n",
       "   ('different random initializations', (23, 26))]],\n",
       " [[('using', (2, 3)), ('with', (20, 21)), ('of', (23, 24))],\n",
       "  [('Performance', (0, 1)),\n",
       "   ('only the ensembled RD models', (3, 8)),\n",
       "   ('lower', (13, 14)),\n",
       "   ('rescoring', (15, 16)),\n",
       "   ('single RD model', (17, 20)),\n",
       "   ('score combinations', (21, 23)),\n",
       "   ('single models', (24, 26)),\n",
       "   ('RD + RG', (28, 31)),\n",
       "   ('RD + LM', (36, 39))]],\n",
       " [[('In', (0, 1)), ('with', (6, 7)), ('achieves', (9, 10)), ('of', (15, 16))],\n",
       "  [('PTB setting', (2, 4)),\n",
       "   ('ensembling', (5, 6)),\n",
       "   ('score combination', (7, 9)),\n",
       "   ('best over all result', (11, 15)),\n",
       "   ('94.25', (16, 17))]],\n",
       " [[], [('In- Order Transition - based Constituent Parsing', (0, 7))]],\n",
       " [[('used for', (11, 13))],\n",
       "  [('Both bottom - up and top - down strategies', (0, 9)),\n",
       "   ('neural transition - based constituent parsing', (13, 19))]],\n",
       " [[('propose', (5, 6)),\n",
       "   ('for', (10, 11)),\n",
       "   ('mitigating', (14, 15)),\n",
       "   ('of', (16, 17)),\n",
       "   ('by finding', (26, 28)),\n",
       "   ('between', (30, 31))],\n",
       "  [('novel transition system', (7, 10)),\n",
       "   ('constituent parsing', (11, 13)),\n",
       "   ('issues', (15, 16)),\n",
       "   ('bottom - up and top - down systems', (18, 26)),\n",
       "   ('compromise', (29, 30)),\n",
       "   ('bottom - up constituent information', (31, 36)),\n",
       "   ('top - down lookahead information', (37, 42))]],\n",
       " [[], [('https://github.com/LeonCrashCode/InOrderParser', (5, 6))]],\n",
       " [[], [('Reranking experiments', (0, 2))]],\n",
       " [[('performs', (5, 6)), ('than', (8, 9))],\n",
       "  [('bottom - up system', (1, 5)),\n",
       "   ('slightly better', (6, 8)),\n",
       "   ('top - down system', (10, 14))]],\n",
       " [[],\n",
       "  [('inorder system', (1, 3)),\n",
       "   ('outperforms', (3, 4)),\n",
       "   ('bottom - up and the top - down system', (6, 15))]],\n",
       " [[('find that', (1, 3)), ('have', (14, 15)), ('under', (17, 18))],\n",
       "  [('bottom - up parser and the top - down parser', (4, 14)),\n",
       "   ('similar results', (15, 17)),\n",
       "   ('greedy setting', (19, 21)),\n",
       "   ('in - order parser', (24, 28)),\n",
       "   ('outperforms', (28, 29))]],\n",
       " [[], [('English constituent results', (0, 3))]],\n",
       " [[('With', (0, 1)), ('outperforms', (11, 12))],\n",
       "  [('fully - supervise setting', (2, 6)),\n",
       "   ('inorder parser', (9, 11)),\n",
       "   ('state - of - the - art discrete parser', (13, 22)),\n",
       "   ('state - of - the - art neural parsers', (24, 33))]],\n",
       " [[], [('Chinese dependency results', (0, 3))]],\n",
       " [[('converting', (5, 6)),\n",
       "   ('achieves', (15, 16)),\n",
       "   ('among', (19, 20)),\n",
       "   ('obtains', (24, 25)),\n",
       "   ('to', (27, 28))],\n",
       "  [('our final model', (12, 15)),\n",
       "   ('best results', (17, 19)),\n",
       "   ('transitionbased parsing', (20, 22)),\n",
       "   ('comparable results', (25, 27)),\n",
       "   ('state - of - the - art graph - based models', (29, 40))]],\n",
       " [[], [('Parsing as Language Modeling', (0, 4))]],\n",
       " [[('recast', (1, 2))],\n",
       "  [('syntactic parsing', (2, 4)),\n",
       "   ('constituency Penn Treebank parsing', (27, 31))]],\n",
       " [[], [('syntactic parsing', (5, 7))]],\n",
       " [[('present', (14, 15)), ('achieves', (22, 23)), ('with', (31, 32))],\n",
       "  [('neural - net parse reranker', (16, 21)),\n",
       "   ('very good results', (23, 26)),\n",
       "   ('93.8 F 1', (27, 30)),\n",
       "   ('comparatively simple architecture', (33, 36))]],\n",
       " [[('with', (6, 7)),\n",
       "   ('trained with', (11, 13)),\n",
       "   ('through', (15, 16)),\n",
       "   ('with', (17, 18))],\n",
       "  [('three LSTM layers', (3, 6)),\n",
       "   ('1,500 units', (7, 9)),\n",
       "   ('truncated backpropagation', (13, 15)),\n",
       "   ('time', (16, 17)),\n",
       "   ('mini-batch size 20', (18, 21)),\n",
       "   ('step size 50', (22, 25))]],\n",
       " [[('initialize', (1, 2)), ('with', (4, 5))],\n",
       "  [('starting states', (2, 4)),\n",
       "   (\"previous minibatch 's last hidden states\", (5, 11))]],\n",
       " [[('initialized to be', (5, 8)), ('sampled from', (16, 18))],\n",
       "  [('forget gate bias', (1, 4)),\n",
       "   ('one', (8, 9)),\n",
       "   ('rest of model parameters', (11, 15)),\n",
       "   ('U ( ? 0.05 , 0.05 )', (18, 25))]],\n",
       " [[('applied to', (2, 4)), ('when', (10, 11))],\n",
       "  [('Dropout', (0, 1)),\n",
       "   ('non-recurrent connections', (4, 6)),\n",
       "   ('gradients', (7, 8)),\n",
       "   ('clipped', (9, 10)),\n",
       "   ('norm', (12, 13)),\n",
       "   ('bigger', (14, 15)),\n",
       "   ('20', (16, 17))]],\n",
       " [[('is', (3, 4))], [('learning rate', (1, 3)), ('0.25 0.85 max', (4, 7))]],\n",
       " [[('use', (4, 5)), ('over', (7, 8)), ('as opposed to', (11, 14))],\n",
       "  [('vanilla softmax', (5, 7)),\n",
       "   ('entire vocabulary', (9, 11)),\n",
       "   ('hierarchical softmax or noise contrastive estimation', (14, 20))]],\n",
       " [[], []],\n",
       " [[('together with', (8, 10)), ('reaches', (14, 15)), ('achieves', (32, 33))],\n",
       "  [('single LSTM - LM ( GS )', (1, 8)),\n",
       "   ('Charniak ( GS )', (10, 14)),\n",
       "   ('93.6', (15, 16)),\n",
       "   ('new state of the art', (34, 39)),\n",
       "   ('93.8', (40, 41))]],\n",
       " [[('converted to', (3, 5)), ('are', (12, 13)), ('higher than', (24, 26))],\n",
       "  [('trees', (1, 2)),\n",
       "   ('Stanford dependencies', (5, 7)),\n",
       "   ('5 UAS and LAS', (8, 12)),\n",
       "   ('95.9 % and 94.1 %', (13, 18)),\n",
       "   ('state of the art dependency parser', (29, 35))]],\n",
       " [[], [('Syntax', (8, 9))]],\n",
       " [[], []],\n",
       " [[('focus on', (1, 3)), ('as', (4, 5))],\n",
       "  [('RNNGs', (3, 4)), ('generative probabilistic models over', (5, 9))]],\n",
       " [[('manipulates', (2, 3)), ('of', (6, 7)), ('to test', (8, 10))],\n",
       "  [('inductive bias', (4, 6)),\n",
       "   ('RNNGs', (7, 8)),\n",
       "   ('linguistic hypotheses', (10, 12))]],\n",
       " [[('begin with', (1, 3)), ('to discover', (6, 8)), ('of', (10, 11))],\n",
       "  [('ablation study', (4, 6)),\n",
       "   ('importance', (9, 10)),\n",
       "   ('composition function', (12, 14))]],\n",
       " [[('augment', (6, 7)),\n",
       "   ('with', (11, 12)),\n",
       "   ('leading to', (18, 20)),\n",
       "   ('to incorporate', (25, 27)),\n",
       "   ('into', (29, 30))],\n",
       "  [('RNNG composition function', (8, 11)),\n",
       "   ('novel gated attention mechanism', (13, 17)),\n",
       "   ('GA - RNNG', (21, 24)),\n",
       "   ('more interpretability', (27, 29)),\n",
       "   ('model', (31, 32))]],\n",
       " [[('Using', (0, 1)), ('play in', (15, 17))],\n",
       "  [('GA - RNNG', (2, 5)),\n",
       "   ('investigating', (9, 10)),\n",
       "   ('role', (11, 12)),\n",
       "   ('individual heads', (13, 15)),\n",
       "   ('phrasal representation', (17, 19)),\n",
       "   ('nonterminal category labels', (26, 29))]],\n",
       " [[('with', (2, 3)), ('is', (6, 7)), ('of', (9, 10)), ('with', (22, 23))],\n",
       "  [('RNNG', (1, 2)),\n",
       "   ('only a stack', (3, 6)),\n",
       "   ('strongest', (8, 9)),\n",
       "   ('ablations', (11, 12)),\n",
       "   ('outperforms', (16, 17)),\n",
       "   ('\" full \" RNNG', (18, 22)),\n",
       "   ('all three data structures', (23, 27))]],\n",
       " [[('Ablating', (0, 1)), ('gives', (3, 4)), ('among', (6, 7))],\n",
       "  [('stack', (2, 3)), ('worst', (5, 6)), ('new results', (8, 10))]],\n",
       " [[('achieves', (15, 16)), ('ablating', (21, 22)), ('is', (24, 25))],\n",
       "  [('stack - only RNNG', (11, 15)),\n",
       "   ('best performance', (17, 19)),\n",
       "   ('stack', (23, 24)),\n",
       "   ('most harmful', (25, 27))]],\n",
       " [[('without', (4, 5)), ('provides', (12, 13)), ('over', (15, 16))],\n",
       "  [('modeling syntax', (2, 4)),\n",
       "   ('explicit composition', (5, 7)),\n",
       "   ('little benefit', (13, 15)),\n",
       "   ('sequential LSTM language model', (17, 21))]],\n",
       " [[('remark', (1, 2)),\n",
       "   ('are', (8, 9)),\n",
       "   ('for', (14, 15)),\n",
       "   ('among', (20, 21))],\n",
       "  [('stack - only results', (4, 8)),\n",
       "   ('best published PTB results', (10, 14)),\n",
       "   ('both phrasestructure and dependency parsing', (15, 20)),\n",
       "   ('supervised models', (21, 23))]],\n",
       " [[], [('Gated Attention', (0, 2))]],\n",
       " [[('with', (10, 11)), ('achieves', (16, 17)), ('with', (19, 20))],\n",
       "  [('model', (5, 6)),\n",
       "   ('outperforms', (6, 7)),\n",
       "   ('baseline RNNG', (8, 10)),\n",
       "   ('competitive performance', (17, 19)),\n",
       "   ('strongest , stack - only , RNNG variant', (21, 29))]],\n",
       " [[], [('Headedness in Phrases', (0, 3))]],\n",
       " [[('with', (6, 7)), ('using', (9, 10)), ('rather than', (17, 19))],\n",
       "  [('higher overlap', (4, 6)),\n",
       "   ('conversion', (8, 9)),\n",
       "   ('Collins head rules ( 49.8 UAS )', (10, 17)),\n",
       "   ('Stanford head rules ( 40.4 UAS )', (20, 27))]],\n",
       " [[('when', (19, 20)), ('is', (22, 23))],\n",
       "  [('attention - based tree output', (4, 9)),\n",
       "   ('high error rate ( ? 90 % )', (11, 19)),\n",
       "   ('dependent', (21, 22)),\n",
       "   ('verb', (24, 25))]],\n",
       " [[('better for', (4, 6)),\n",
       "   ('much better for', (15, 18)),\n",
       "   ('with respect to', (29, 32))],\n",
       "  [('conversion accuracy', (1, 3)),\n",
       "   ('nouns ( ? 50 % error )', (6, 13)),\n",
       "   ('determiners ( 30 % ) and particles ( 6 % )', (18, 29)),\n",
       "   ('Collins head rules', (33, 36))]],\n",
       " [[], [('The Role of Nonterminal Labels', (0, 5))]],\n",
       " [[('On', (0, 1)), ('achieves', (14, 15)), ('achieves', (25, 26))],\n",
       "  [('test data', (1, 3)),\n",
       "   ('usual', (6, 7)),\n",
       "   ('GA - RNNG', (11, 14)),\n",
       "   ('94.2 %', (15, 17)),\n",
       "   ('U - GA - RNNG', (20, 25)),\n",
       "   ('93.5 %', (26, 28))]],\n",
       " [[], [('Constituency Parsing', (0, 2))]],\n",
       " [[('introduce', (5, 6)),\n",
       "   ('combines', (9, 10)),\n",
       "   ('built using', (12, 14)),\n",
       "   ('with', (21, 22)),\n",
       "   ('customized for', (24, 26))],\n",
       "  [('parser', (7, 8)),\n",
       "   ('encoder', (11, 12)),\n",
       "   ('decoder', (23, 24)),\n",
       "   ('parsing', (26, 27))]],\n",
       " [[('uses', (9, 10)), ('performs', (15, 16)), ('than', (17, 18))],\n",
       "  [('character LSTM', (11, 13)),\n",
       "   ('better', (16, 17)),\n",
       "   ('other lexical representationseven', (18, 21))]],\n",
       " [[], [('6 Results', (0, 2))]],\n",
       " [[], [('English ( WSJ )', (0, 4))]],\n",
       " [[('of', (3, 4)),\n",
       "   ('for', (6, 7)),\n",
       "   ('exceeds', (10, 11)),\n",
       "   ('for', (15, 16)),\n",
       "   ('trained on', (20, 22)),\n",
       "   ('without the use of', (26, 30)),\n",
       "   ('such as', (34, 36))],\n",
       "  [('test score', (1, 3)),\n",
       "   ('93.55 F1', (4, 6)),\n",
       "   ('our CharLSTM parser', (7, 10)),\n",
       "   ('previous best numbers', (12, 15)),\n",
       "   ('single - system parsers', (16, 20)),\n",
       "   ('Penn Treebank', (23, 25)),\n",
       "   ('pre-trained', (36, 37))]],\n",
       " [[('augmented with', (4, 6)),\n",
       "   ('achieves', (11, 12)),\n",
       "   ('of', (22, 23)),\n",
       "   ('on', (25, 26))],\n",
       "  [('our parser', (1, 3)),\n",
       "   ('ELMo word representations', (6, 9)),\n",
       "   ('new state - of - the - art score', (13, 22)),\n",
       "   ('95.13 F1', (23, 25)),\n",
       "   ('WSJ test set', (27, 30))]],\n",
       " [[], [('Multilingual ( SPMRL )', (0, 4))]],\n",
       " [[('show', (3, 4)), ('to', (10, 11)), ('that uses', (13, 15))],\n",
       "  [('Development set results', (0, 3)),\n",
       "   ('addition', (6, 7)),\n",
       "   ('word embeddings', (8, 10)),\n",
       "   ('model', (12, 13)),\n",
       "   ('character LSTM', (16, 18)),\n",
       "   ('mixed effect', (20, 22))]],\n",
       " [[('On', (0, 1)), ('exceeds', (11, 12)), ('from', (18, 19))],\n",
       "  [('8 of the 9 languages', (1, 6)),\n",
       "   ('our test set result', (7, 11)),\n",
       "   ('previous best - published numbers', (13, 18))]],\n",
       " [[], [('Coreference Resolution', (1, 3))]],\n",
       " [[('train', (6, 7)),\n",
       "   ('to build', (11, 13)),\n",
       "   ('of', (15, 16)),\n",
       "   ('of', (17, 18))],\n",
       "  [('deep neural network', (8, 11)),\n",
       "   ('distributed representations', (13, 15)),\n",
       "   ('pairs', (16, 17)),\n",
       "   ('coreference clusters', (18, 20))]],\n",
       " [[('captures', (1, 2)), ('with', (6, 7)), ('instead of', (15, 17))],\n",
       "  [('entity - level information', (2, 6)),\n",
       "   ('large number of learned , continuous features', (8, 15)),\n",
       "   ('small number of hand - crafted categorical ones', (18, 26))]],\n",
       " [[('Using', (0, 1)),\n",
       "   ('learns', (9, 10)),\n",
       "   ('combining', (11, 12)),\n",
       "   ('is', (15, 16))],\n",
       "  [('cluster - pair representations', (2, 6)),\n",
       "   ('our network', (7, 9)),\n",
       "   ('two coreference clusters', (12, 15)),\n",
       "   ('desirable', (16, 17))]],\n",
       " [[('At', (0, 1)),\n",
       "   ('builds up', (4, 6)),\n",
       "   ('starting with', (10, 12)),\n",
       "   ('in', (14, 15))],\n",
       "  [('test time', (1, 3)),\n",
       "   ('coreference clusters', (6, 8)),\n",
       "   ('incrementally', (8, 9)),\n",
       "   ('each mention', (12, 14)),\n",
       "   ('merging', (20, 21)),\n",
       "   ('pair of clusters', (22, 25))]],\n",
       " [[('with', (4, 5)), ('combines', (15, 16)), ('of', (18, 19))],\n",
       "  [('decisions', (3, 4)),\n",
       "   ('novel easy - first cluster - ranking procedure', (6, 14)),\n",
       "   ('strengths', (17, 18)),\n",
       "   ('cluster - ranking ( Rahman and and easy - first coreference algorithms',\n",
       "    (19, 31))]],\n",
       " [[('using', (4, 5)), ('inspired by', (12, 14)), ('to train', (15, 17))],\n",
       "  [('learning - to - search algorithm', (6, 12)),\n",
       "   ('SEARN', (14, 15)),\n",
       "   ('our neural network', (17, 20))]],\n",
       " [[('lead to', (28, 30))],\n",
       "  [('which action ( a cluster merge ) available', (7, 15)),\n",
       "   ('current state', (17, 19)),\n",
       "   ('partially completed coreference clustering', (21, 25)),\n",
       "   ('high - scoring coreference partition', (31, 36))]],\n",
       " [[('surpasses', (5, 6))],\n",
       "  [('mention - ranking model', (1, 5)), ('all previous systems', (6, 9))]],\n",
       " [[('improves', (5, 6)), ('further across', (7, 9))],\n",
       "  [('cluster - ranking model', (1, 5)),\n",
       "   ('results', (6, 7)),\n",
       "   ('both languages and all evaluation metrics', (9, 15))]],\n",
       " [[],\n",
       "  [('End - to - end Deep Reinforcement Learning Based Coreference Resolution',\n",
       "    (0, 11))]],\n",
       " [[('propose', (5, 6)), ('to resolve', (17, 19))],\n",
       "  [('goal - directed endto - end deep reinforcement learning framework',\n",
       "    (7, 17)),\n",
       "   ('coreference', (19, 20))]],\n",
       " [[('leverage', (3, 4)),\n",
       "   ('includes', (14, 15)),\n",
       "   ('scoring', (19, 20)),\n",
       "   ('generating', (25, 26)),\n",
       "   ('over', (29, 30)),\n",
       "   ('from', (35, 36)),\n",
       "   ('to', (39, 40))],\n",
       "  [('neural architecture', (5, 7)),\n",
       "   ('our policy network', (9, 12)),\n",
       "   ('learning span representation', (15, 18)),\n",
       "   ('potential entity mentions', (20, 23)),\n",
       "   ('probability distribution', (27, 29)),\n",
       "   ('all possible coreference linking actions', (30, 35)),\n",
       "   ('current mention', (37, 39))]],\n",
       " [[('Once', (0, 1)),\n",
       "   ('are', (6, 7)),\n",
       "   ('made', (7, 8)),\n",
       "   ('to measure', (14, 16)),\n",
       "   ('directly related to', (26, 29))],\n",
       "  [('sequence of linking actions', (2, 6)),\n",
       "   ('our reward function', (9, 12)),\n",
       "   ('generated coreference clusters', (19, 22)),\n",
       "   ('coreference evaluation metrics', (29, 32))]],\n",
       " [[('introduce', (3, 4)),\n",
       "   ('to encourage', (8, 10)),\n",
       "   ('prevent', (12, 13)),\n",
       "   ('from', (15, 16)),\n",
       "   ('to', (18, 19))],\n",
       "  [('entropy regularization term', (5, 8)),\n",
       "   ('exploration', (10, 11)),\n",
       "   ('policy', (14, 15)),\n",
       "   ('prematurely converging', (16, 18)),\n",
       "   ('bad local optimum', (20, 23))]],\n",
       " [[('update', (3, 4)),\n",
       "   ('based on', (9, 11)),\n",
       "   ('associated with', (13, 15)),\n",
       "   ('of', (16, 17)),\n",
       "   ('computed on', (22, 24))],\n",
       "  [('regularized policy network parameters', (5, 9)),\n",
       "   ('rewards', (12, 13)),\n",
       "   ('sequences', (15, 16)),\n",
       "   ('whole input document', (25, 28))]],\n",
       " [[], []],\n",
       " [[('pretrain', (3, 4)),\n",
       "   ('for', (11, 12)),\n",
       "   ('use', (17, 18)),\n",
       "   ('for', (21, 22))],\n",
       "  [('our model', (4, 6)),\n",
       "   ('around 200 K steps', (12, 16)),\n",
       "   ('learned parameters', (19, 21)),\n",
       "   ('initialization', (22, 23))]],\n",
       " [[('set', (3, 4)),\n",
       "   ('tune', (14, 15)),\n",
       "   ('expr in', (19, 21)),\n",
       "   ('set it to', (37, 40)),\n",
       "   ('based on', (43, 45))],\n",
       "  [('number of sampled trajectories N s = 100', (5, 13)),\n",
       "   ('regularization parameter ?', (16, 19)),\n",
       "   ('{ 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 }', (21, 36)),\n",
       "   ('10 ? 4', (40, 43)),\n",
       "   ('development set', (46, 48))]],\n",
       " [[], []],\n",
       " [[('Built on top of', (0, 4)),\n",
       "   ('excluding', (8, 9)),\n",
       "   ('improves', (15, 16)),\n",
       "   ('around', (21, 22))],\n",
       "  [('model', (5, 6)),\n",
       "   ('ELMo', (9, 10)),\n",
       "   ('our base reinforced model', (11, 15)),\n",
       "   ('average F 1 score', (17, 21)),\n",
       "   ('2 points', (22, 24))]],\n",
       " [[('using', (4, 5)),\n",
       "   ('to encourage', (7, 9)),\n",
       "   ('can improve', (10, 12)),\n",
       "   ('by', (14, 15))],\n",
       "  [('entropy regularization', (5, 7)),\n",
       "   ('exploration', (9, 10)),\n",
       "   ('result', (13, 14)),\n",
       "   ('1 point', (15, 17))]],\n",
       " [[('introducing', (2, 3)), ('to', (9, 10)), ('can further boosts', (13, 16))],\n",
       "  [('context - dependent ELMo embedding', (4, 9)),\n",
       "   ('our base model', (10, 13)),\n",
       "   ('performance', (17, 18))]],\n",
       " [[('achieves', (5, 6)),\n",
       "   ('of', (14, 15)),\n",
       "   ('when using', (20, 22)),\n",
       "   ('of', (44, 45)),\n",
       "   ('when using', (47, 49))],\n",
       "  [('our full model', (2, 5)),\n",
       "   ('state - of the - art performance', (7, 14)),\n",
       "   ('73.8 % F1 - score', (15, 20)),\n",
       "   ('ELMo and entropy regularization', (22, 26)),\n",
       "   ('our', (36, 37)),\n",
       "   ('best F1 -score', (41, 44)),\n",
       "   ('70.5 %', (45, 47)),\n",
       "   ('fixed word embedding', (49, 52))]],\n",
       " [[('trained with', (5, 7))],\n",
       "  [('Coreference resolution systems', (0, 3)),\n",
       "   ('heuristic loss functions', (7, 10)),\n",
       "   ('careful tuning', (12, 14))]],\n",
       " [[('explore using', (5, 7)),\n",
       "   ('to directly optimize', (12, 15)),\n",
       "   ('for', (18, 19))],\n",
       "  [('two variants of reinforcement learning', (7, 12)),\n",
       "   ('coreference system', (16, 18)),\n",
       "   ('coreference evaluation metrics', (19, 22))]],\n",
       " [[('modify', (4, 5)),\n",
       "   ('by incorporating', (11, 13)),\n",
       "   ('associated with', (15, 17)),\n",
       "   ('into', (20, 21))],\n",
       "  [('max-margin coreference objective', (6, 9)),\n",
       "   ('reward', (14, 15)),\n",
       "   ('each coreference decision', (17, 20)),\n",
       "   (\"loss 's slack rescaling\", (22, 26))]],\n",
       " [[('test', (2, 3))], [('REINFORCE policy gradient algorithm', (4, 8))]],\n",
       " [[('is', (2, 3))], [('neural mention - ranking model', (4, 9))]],\n",
       " [[], []],\n",
       " [[('find that', (1, 3)),\n",
       "   ('does', (4, 5)),\n",
       "   ('than', (7, 8)),\n",
       "   ('performs', (15, 16))],\n",
       "  [('REINFORCE', (3, 4)),\n",
       "   ('slightly better', (5, 7)),\n",
       "   ('heuristic loss', (9, 11)),\n",
       "   ('reward rescaling', (13, 15)),\n",
       "   ('significantly better', (16, 18))]],\n",
       " [[('combines', (8, 9)), ('resulting in', (15, 17))],\n",
       "  [('reward - rescaled max - margin loss', (1, 8)),\n",
       "   ('best of both worlds', (10, 14)),\n",
       "   ('superior performance', (17, 19))]],\n",
       " [[], [('Higher', (0, 1))]],\n",
       " [[('introduce', (1, 2)), ('of', (4, 5)), ('uses', (10, 11))],\n",
       "  [('approximation', (3, 4)),\n",
       "   ('higher - order inference', (5, 9)),\n",
       "   ('span - ranking architecture', (12, 16))]],\n",
       " [[('At', (0, 1)),\n",
       "   ('used as', (8, 10)),\n",
       "   ('to optionally update', (13, 16)),\n",
       "   ('enabling', (20, 21))],\n",
       "  [('each iteration', (1, 3)),\n",
       "   ('antecedent distribution', (5, 7)),\n",
       "   ('attention mechanism', (11, 13)),\n",
       "   ('existing span representations', (16, 19)),\n",
       "   ('later corefer', (21, 23))]],\n",
       " [[('propose', (13, 14)), ('learned with', (21, 23))],\n",
       "  [('coarseto - fine approach', (15, 19)),\n",
       "   ('single endto - end objective', (24, 29))]],\n",
       " [[('introduce', (1, 2)), ('in', (10, 11))],\n",
       "  [('less accurate but more efficient coarse factor', (3, 10)),\n",
       "   ('pairwise scoring function', (12, 15))]],\n",
       " [[('enables', (3, 4)),\n",
       "   ('during', (8, 9)),\n",
       "   ('that reduces', (10, 12)),\n",
       "   ('considered by', (16, 18))],\n",
       "  [('extra pruning step', (5, 8)),\n",
       "   ('inference', (9, 10)),\n",
       "   ('number of antecedents', (13, 16)),\n",
       "   ('more accurate but inefficient fine factor', (19, 25))]],\n",
       " [[('cheaply computes', (4, 6)),\n",
       "   ('of', (9, 10)),\n",
       "   ('before applying', (12, 14))],\n",
       "  [('rough sketch', (7, 9)),\n",
       "   ('likely antecedents', (10, 12)),\n",
       "   ('more expensive scoring function', (15, 19))]],\n",
       " [[], []],\n",
       " [[('is', (6, 7)), ('from augmented with', (12, 15)), ('achieves', (22, 23))],\n",
       "  [('span - ranking model', (8, 12)),\n",
       "   ('ELMo and hyperparameter tuning', (16, 20)),\n",
       "   ('72.3 F1', (23, 25))]],\n",
       " [[('achieves', (3, 4)), ('setting', (7, 8)), ('for', (14, 15))],\n",
       "  [('Our full approach', (0, 3)),\n",
       "   ('73.0 F1', (4, 6)),\n",
       "   ('new state of the art', (9, 14)),\n",
       "   ('coreference resolution', (15, 17))]],\n",
       " [[('computed for', (23, 25)), ('between', (37, 38)), ('in', (41, 42))],\n",
       "  [('outperforms', (7, 8)),\n",
       "   ('baseline', (9, 10)),\n",
       "   ('all antecedents', (25, 27)),\n",
       "   ('coreference link', (35, 37)),\n",
       "   ('any two spans', (38, 41)),\n",
       "   ('document', (43, 44))]],\n",
       " [[('observe', (5, 6))],\n",
       "  [('much higher recall', (7, 10)),\n",
       "   ('coarse - to - fine approach', (13, 19))]],\n",
       " [[('observe', (2, 3)), ('by including', (5, 7))],\n",
       "  [('further improvement', (3, 5)), ('second - order inference', (8, 12))]],\n",
       " [[], [('Abstract Anaphora Resolution', (6, 9))]],\n",
       " [[],\n",
       "  [('anaphora ( or coreference ) resolution', (3, 9)),\n",
       "   ('resolving noun phrases referring to concrete objects', (12, 19))]],\n",
       " [[('inspired by', (3, 5)),\n",
       "   ('for', (10, 11)),\n",
       "   ('combines it with', (14, 17)),\n",
       "   ('for learning', (21, 23)),\n",
       "   ('between', (24, 25))],\n",
       "  [('mention - ranking model', (6, 10)),\n",
       "   ('coreference resolution', (11, 13)),\n",
       "   ('Siamese Net', (18, 20)),\n",
       "   ('similarity', (23, 24)),\n",
       "   ('sentences', (25, 26))]],\n",
       " [[('Given', (0, 1)),\n",
       "   ('learns', (42, 43)),\n",
       "   ('for', (44, 45)),\n",
       "   ('in', (51, 52))],\n",
       "  [('anaphoric sentence', (2, 4)),\n",
       "   ('representations', (43, 44)),\n",
       "   ('candidate and the anaphoric sentence', (46, 51)),\n",
       "   ('shared space', (53, 55))]],\n",
       " [[('combined into', (3, 5)),\n",
       "   ('to calculate', (9, 11)),\n",
       "   ('characterizes', (14, 15))],\n",
       "  [('joint representation', (6, 8)),\n",
       "   ('score', (12, 13)),\n",
       "   ('relation', (16, 17))]],\n",
       " [[('to select', (5, 7)), ('for', (13, 14))],\n",
       "  [('learned score', (1, 3)),\n",
       "   ('highest - scoring antecedent candidate', (8, 13)),\n",
       "   ('given anaphoric sentence', (15, 18))]],\n",
       " [[('consider', (1, 2)),\n",
       "   ('provide', (8, 9)),\n",
       "   ('of', (11, 12)),\n",
       "   ('of', (14, 15)),\n",
       "   ('of', (23, 24)),\n",
       "   ('to', (27, 28)),\n",
       "   ('to characterize', (30, 32))],\n",
       "  [('one anaphor', (2, 4)),\n",
       "   ('embedding', (10, 11)),\n",
       "   ('context', (13, 14)),\n",
       "   ('anaphor and the embedding of', (16, 21)),\n",
       "   ('head', (22, 23)),\n",
       "   ('input', (29, 30)),\n",
       "   ('each individual', (32, 34)),\n",
       "   ('individuating multiply occurring predicates', (41, 45)),\n",
       "   ('SRL', (46, 47))]],\n",
       " [[('With', (0, 1)),\n",
       "   ('show', (4, 5)),\n",
       "   ('learns', (8, 9)),\n",
       "   ('between', (11, 12)),\n",
       "   ('in', (14, 15))],\n",
       "  [('deeper inspection', (1, 3)),\n",
       "   ('model', (7, 8)),\n",
       "   ('relation', (10, 11)),\n",
       "   ('anaphor', (13, 14)),\n",
       "   ('anaphoric sentence', (16, 18))]],\n",
       " [[('produces', (1, 2)), ('easily adaptable to', (8, 11))],\n",
       "  [('large amounts of instances', (2, 6)), ('other languages', (11, 13))]],\n",
       " [[],\n",
       "  [('data extraction', (11, 13)),\n",
       "   ('https://github.com/amarasovic / neural-abstract-anaphora.', (17, 20))]],\n",
       " [[], [('Baselines and evaluation metrics', (0, 4))]],\n",
       " [[('report', (3, 4)),\n",
       "   ('for', (17, 18)),\n",
       "   ('that', (26, 27)),\n",
       "   ('randomly chooses', (27, 29)),\n",
       "   ('with', (31, 32)),\n",
       "   ('in', (36, 37))],\n",
       "  [('preceding sentence baseline ( PS BL )', (5, 12)),\n",
       "   ('previous sentence', (15, 17)),\n",
       "   ('antecedent and TAGbaseline ( TAG BL )', (19, 26)),\n",
       "   ('candidate', (30, 31)),\n",
       "   ('constituent tag label', (33, 36)),\n",
       "   ('{S , VP , ROOT , SBAR }', (37, 45))]],\n",
       " [[('pre-trained on', (4, 6))],\n",
       "  [('Glo Ve word embeddings', (0, 4)),\n",
       "   ('Gigaword and Wikipedia', (7, 10)),\n",
       "   ('fine - tune', (14, 17))]],\n",
       " [[('built from', (2, 4)),\n",
       "   ('in', (6, 7)),\n",
       "   ('with', (10, 11)),\n",
       "   ('replaced with', (28, 30))],\n",
       "  [('Vocabulary', (0, 1)),\n",
       "   ('words', (5, 6)),\n",
       "   ('training data', (8, 10)),\n",
       "   ('frequency in { 3 , U ( 1 , 10 ) }', (11, 23)),\n",
       "   ('OOV words', (25, 27)),\n",
       "   ('UNK token', (31, 33))]],\n",
       " [[('of', (2, 3)), ('set to', (8, 10))],\n",
       "  [('size', (1, 2)),\n",
       "   ('LSTMs hidden states', (4, 7)),\n",
       "   ('{ 100 , qlog - U ( 30 , 150 ) }', (10, 22))]],\n",
       " [[('initialized', (1, 2)), ('of', (5, 6)), ('with', (8, 9))],\n",
       "  [('weight matrices', (3, 5)),\n",
       "   ('LSTMs', (7, 8)),\n",
       "   ('random orthogonal matrices', (9, 12))]],\n",
       " [[('set to', (8, 10)), ('in', (12, 13))],\n",
       "  [('first', (1, 2)),\n",
       "   ('feed - forward layer size', (2, 7)),\n",
       "   ('value', (11, 12)),\n",
       "   ('Optimization', (13, 14))]],\n",
       " [[('trained', (1, 2)), ('in', (4, 5)), ('using', (6, 7)), ('with', (15, 16))],\n",
       "  [('our model', (2, 4)),\n",
       "   ('minibatches', (5, 6)),\n",
       "   ('Adam ( Kingma and Ba , 2015 )', (7, 15)),\n",
       "   ('learning rate', (17, 19)),\n",
       "   ('10 ? 4', (20, 23)),\n",
       "   ('maximal batch size', (24, 27)),\n",
       "   ('64', (27, 28))]],\n",
       " [[('clip', (1, 2)), ('by', (3, 4)), ('with', (7, 8)), ('in', (11, 12))],\n",
       "  [('gradients', (2, 3)),\n",
       "   ('global norm', (4, 6)),\n",
       "   ('clipping value', (9, 11)),\n",
       "   ('{ 1.0 , U ( 1 , 100 ) }', (12, 22))]],\n",
       " [[('train for', (1, 3)), ('choose', (6, 7)), ('on', (12, 13))],\n",
       "  [('10 epochs', (3, 5)),\n",
       "   ('model', (8, 9)),\n",
       "   ('performs', (10, 11)),\n",
       "   ('best', (11, 12)),\n",
       "   ('devset', (14, 15))]],\n",
       " [[('used', (1, 2)), ('with', (7, 8))],\n",
       "  [('l 2 - regularization', (3, 7)), ('? ? { 10 ?5', (8, 13))]],\n",
       " [[('with', (1, 2)), ('applied to', (18, 20)), ('of', (22, 23))],\n",
       "  [('Dropout', (0, 1)),\n",
       "   ('keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) }', (3, 17)),\n",
       "   ('outputs', (21, 22)),\n",
       "   ('LSTMs', (24, 25)),\n",
       "   ('both feed - forward layers', (26, 31)),\n",
       "   ('input', (35, 36)),\n",
       "   ('k p ? U (0.8 , 1.0 )', (37, 45))]],\n",
       " [[('In terms of', (0, 3)), ('without even necessitating', (17, 20))],\n",
       "  [('s@1 score', (3, 5)),\n",
       "   ('MR - LSTM', (6, 9)),\n",
       "   ('outperforms', (9, 10)),\n",
       "   (\"KZH13 's results\", (11, 14)),\n",
       "   ('TAG BL', (15, 17)),\n",
       "   ('HP tuning', (20, 22))]],\n",
       " [[('with', (7, 8)),\n",
       "   ('tuned on', (9, 11)),\n",
       "   ('obtain', (16, 17)),\n",
       "   ('perform', (29, 30)),\n",
       "   ('than', (31, 32)),\n",
       "   ('when omitting', (43, 45))],\n",
       "  [('HPs', (8, 9)),\n",
       "   ('ARRAU - AA', (11, 14)),\n",
       "   ('results', (17, 18)),\n",
       "   ('well beyond', (18, 20)),\n",
       "   ('KZH13', (20, 21)),\n",
       "   ('all ablated model variants', (25, 29)),\n",
       "   ('worse', (30, 31)),\n",
       "   ('full model', (33, 35)),\n",
       "   ('large performance drop', (40, 43)),\n",
       "   ('syntactic information ( tag , cut )', (45, 52))]],\n",
       " [[('of', (1, 2)),\n",
       "   ('indicates', (5, 6)),\n",
       "   ('able to', (10, 12)),\n",
       "   ('without', (13, 14))],\n",
       "  [('Performance', (0, 1)),\n",
       "   ('68.10 s@1 score', (2, 5)),\n",
       "   ('model', (8, 9)),\n",
       "   ('learn', (12, 13)),\n",
       "   ('syntactic guidance', (14, 16))]],\n",
       " [[('on', (1, 2))], [('ARRAU corpus', (3, 5))]],\n",
       " [[('in', (7, 8)), ('than', (10, 11))],\n",
       "  [('MR - LSTM', (1, 4)),\n",
       "   ('more successful', (5, 7)),\n",
       "   ('resolving', (8, 9)),\n",
       "   ('nominal', (9, 10)),\n",
       "   ('pronominal anaphors', (11, 13))]],\n",
       " [[('for', (2, 3)),\n",
       "   ('in', (6, 7)),\n",
       "   ('achieved', (15, 16)),\n",
       "   ('in', (18, 19)),\n",
       "   ('achieves', (30, 31)),\n",
       "   ('for', (34, 35)),\n",
       "   ('in', (37, 38))],\n",
       "  [('shell noun resolution', (3, 6)),\n",
       "   (\"KZH13 's dataset\", (7, 10)),\n",
       "   ('MR - LSTM', (12, 15)),\n",
       "   ('s@1 scores', (16, 18)),\n",
       "   ('76.09-93.14', (21, 22)),\n",
       "   ('best', (25, 26)),\n",
       "   ('51.89', (31, 32))]],\n",
       " [[('achieves', (26, 27)), ('with', (32, 33))],\n",
       "  [('MR - LSTM without context embedding ( ctx )', (17, 26)),\n",
       "   ('comparable s@ 2 score', (28, 32)),\n",
       "   ('variant', (34, 35)),\n",
       "   ('better s@3 - 4 scores', (41, 46))]],\n",
       " [[], [('Coreference Resolution', (4, 6))]],\n",
       " [[('benefit from', (8, 10)), ('about', (13, 14))],\n",
       "  [('coreference prediction', (5, 7)),\n",
       "   ('modeling', (10, 11)),\n",
       "   ('global information', (11, 13)),\n",
       "   ('entity - clusters', (14, 17))]],\n",
       " [[('posit', (5, 6)),\n",
       "   ('necessary for', (11, 13)),\n",
       "   ('in', (15, 16)),\n",
       "   ('argue', (20, 21)),\n",
       "   ('are', (31, 32))],\n",
       "  [('global context', (7, 9)),\n",
       "   ('further improvements', (13, 15)),\n",
       "   ('coreference resolution', (16, 18)),\n",
       "   ('informative cluster', (22, 24)),\n",
       "   ('level features', (29, 31)),\n",
       "   ('very difficult to', (32, 35))]],\n",
       " [[('propose to', (4, 6)),\n",
       "   ('of', (8, 9)),\n",
       "   ('by embedding', (11, 13)),\n",
       "   ('using', (15, 16))],\n",
       "  [('learn', (6, 7)),\n",
       "   ('representations', (7, 8)),\n",
       "   ('mention clusters', (9, 11)),\n",
       "   ('sequentially', (14, 15)),\n",
       "   ('recurrent neural network', (17, 20))]],\n",
       " [[('learns', (11, 12)), ('from', (15, 16)), ('present in', (19, 21))],\n",
       "  [('manually defined cluster features', (4, 8)),\n",
       "   ('global representation', (13, 15)),\n",
       "   ('individual mentions', (17, 19)),\n",
       "   ('each cluster', (21, 23))]],\n",
       " [[('incorporate', (1, 2)), ('into', (4, 5))],\n",
       "  [('mention - ranking style coreference system', (6, 12))]],\n",
       " [[('including', (4, 5)), ('trained', (17, 18)), ('on', (23, 24))],\n",
       "  [('entire model', (1, 3)),\n",
       "   ('recurrent neural network', (6, 9)),\n",
       "   ('mention - ranking sub-system', (11, 15)),\n",
       "   ('end - to - end', (18, 23)),\n",
       "   ('coreference task', (25, 27))]],\n",
       " [[('train', (1, 2)), ('as', (4, 5)), ('with', (8, 9))],\n",
       "  [('model', (3, 4)),\n",
       "   ('local classifier', (6, 8)),\n",
       "   ('fixed context', (9, 11))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('use', (4, 5)),\n",
       "   ('allows for', (11, 13)),\n",
       "   ('of', (15, 16)),\n",
       "   ('minimize', (21, 22)),\n",
       "   ('with', (28, 29)),\n",
       "   ('after clipping', (31, 33)),\n",
       "   ('to', (35, 36))],\n",
       "  [('training', (1, 2)),\n",
       "   ('document - size minibatches', (5, 9)),\n",
       "   ('efficient pre-computation', (13, 15)),\n",
       "   ('RNN states', (16, 18)),\n",
       "   ('loss', (23, 24)),\n",
       "   ('LSTM gradients', (33, 35))]],\n",
       " [[('find', (1, 2)),\n",
       "   ('chosen for', (7, 9)),\n",
       "   ('on', (14, 15)),\n",
       "   ('choose', (19, 20)),\n",
       "   ('for', (22, 23)),\n",
       "   ('out of', (25, 27))],\n",
       "  [('initial learning rate', (4, 7)),\n",
       "   ('AdaGrad', (9, 10)),\n",
       "   ('significant impact', (12, 14)),\n",
       "   ('results', (15, 16)),\n",
       "   ('learning rates', (20, 22)),\n",
       "   ('each layer', (23, 25)),\n",
       "   ('{ 0.1 , 0.02 , 0.01 , 0.002 , 0.001 }', (27, 38))]],\n",
       " [[('set', (4, 5)), ('to be', (23, 25))],\n",
       "  [('ha ( x n ) , h c ( x n )', (5, 17)), ('hp ( x n , y)', (30, 36))]],\n",
       " [[('use', (1, 2)), ('without', (8, 9)), ('implemented in', (18, 20))],\n",
       "  [('single - layer LSTM', (3, 7)), ('element - rnn library', (21, 25))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('apply', (4, 5)),\n",
       "   ('with', (6, 7)),\n",
       "   ('of', (9, 10)),\n",
       "   ('before applying', (11, 13)),\n",
       "   ('apply', (21, 22)),\n",
       "   ('with', (23, 24)),\n",
       "   ('to', (28, 29))],\n",
       "  [('regularization', (1, 2)),\n",
       "   ('Dropout', (5, 6)),\n",
       "   ('rate', (8, 9)),\n",
       "   ('0.4', (10, 11)),\n",
       "   ('linear', (14, 15)),\n",
       "   ('Dropout', (22, 23)),\n",
       "   ('rate of', (25, 27)),\n",
       "   ('0.3', (27, 28)),\n",
       "   ('LSTM states', (30, 32))]],\n",
       " [[], [('https : //github.com/swiseman/nn_coref', (7, 10))]],\n",
       " [[('makes use of', (2, 5)), ('for', (7, 8))],\n",
       "  [('GPU', (6, 7)),\n",
       "   ('training', (8, 9)),\n",
       "   ('trains', (11, 12)),\n",
       "   ('about two hours', (13, 16))]],\n",
       " [[], []],\n",
       " [[('see', (1, 2)), ('of', (6, 7)), ('over', (12, 13)), ('on', (28, 29))],\n",
       "  [('statistically significant improvement', (3, 6)),\n",
       "   ('over 0.8 Co NLL points', (7, 12)),\n",
       "   ('previous state of the art', (14, 19)),\n",
       "   ('highest F 1 scores', (22, 26))]],\n",
       " [[('see', (2, 3)), ('improves', (6, 7))],\n",
       "  [('RNN', (5, 6)), ('performance over all', (7, 10))]],\n",
       " [[('than', (8, 9)), ('over', (18, 19)), ('even with', (23, 25))],\n",
       "  [('RNN performance', (3, 5)),\n",
       "   ('significantly better', (6, 8)),\n",
       "   ('Avg baseline', (12, 14)),\n",
       "   ('barely improves', (16, 18)),\n",
       "   ('mention - ranking', (19, 22)),\n",
       "   ('oracle history', (25, 27))]],\n",
       " [[], [('Co -reference Resolution', (14, 17))]],\n",
       " [[('present', (5, 6)),\n",
       "   ('that learns', (10, 12)),\n",
       "   ('for improving', (16, 18))],\n",
       "  [('word embedding model', (7, 10)),\n",
       "   ('cross - sentence dependency', (12, 16)),\n",
       "   ('end - to - end co-reference resolution ( E2E - CR )', (18, 30))]],\n",
       " [[('propose', (35, 36)), ('to learn', (44, 46))],\n",
       "  [('linear sentence linking', (36, 39)),\n",
       "   ('attentional sentence linking models', (40, 44)),\n",
       "   ('crosssentence dependency', (46, 48))]],\n",
       " [[], [('Co-reference resolution', (0, 2))]],\n",
       " [[('propose', (46, 47)), ('for', (52, 53))],\n",
       "  [('cross - sentence encoder', (48, 52)),\n",
       "   ('end - to - end co-reference ( E2E - CR )', (53, 64))]],\n",
       " [[('Borrowing', (0, 1)),\n",
       "   ('of', (3, 4)),\n",
       "   ('containing', (14, 15)),\n",
       "   ('from', (19, 20)),\n",
       "   ('added to', (23, 25))],\n",
       "  [('idea', (2, 3)),\n",
       "   ('external memory module', (5, 8)),\n",
       "   ('external memory block', (11, 14)),\n",
       "   ('syntactic and semantic information', (15, 19)),\n",
       "   ('context sentences', (20, 22)),\n",
       "   ('standard LSTM model', (26, 29))]],\n",
       " [[('With', (0, 1)),\n",
       "   ('as', (15, 16)),\n",
       "   ('calculate', (21, 22)),\n",
       "   ('of', (24, 25)),\n",
       "   ('by taking', (27, 29)),\n",
       "   ('into', (35, 36))],\n",
       "  [('context', (2, 3)),\n",
       "   ('proposed', (7, 8)),\n",
       "   ('encode', (12, 13)),\n",
       "   ('input sentences', (13, 15)),\n",
       "   ('batch', (17, 18)),\n",
       "   ('representations', (23, 24)),\n",
       "   ('input words', (25, 27)),\n",
       "   ('target sentences and context sentences', (30, 35)),\n",
       "   ('consideration', (36, 37))]],\n",
       " [[('applied in', (6, 8)), ('have', (10, 11))],\n",
       "  [('LSTM modules', (4, 6)),\n",
       "   ('our model', (8, 10)),\n",
       "   ('200 output units', (11, 14))]],\n",
       " [[('calculate', (4, 5)),\n",
       "   ('using', (9, 10)),\n",
       "   ('with', (13, 14)),\n",
       "   ('consisting of', (17, 19))],\n",
       "  [('ASL', (1, 2)),\n",
       "   ('cross - sentence dependency', (5, 9)),\n",
       "   ('multilayer perceptron', (11, 13)),\n",
       "   ('one hidden layer', (14, 17)),\n",
       "   ('150 hidden units', (19, 22))]],\n",
       " [[('set as', (5, 7)), ('every', (12, 13))],\n",
       "  [('initial learning rate', (1, 4)),\n",
       "   ('0.001', (7, 8)),\n",
       "   ('decays', (9, 10)),\n",
       "   ('0.001 %', (10, 12)),\n",
       "   ('100 steps', (13, 15))]],\n",
       " [[('optimized with', (3, 5))], [('Adam algorithm', (6, 8))]],\n",
       " [[('randomly select', (1, 3)),\n",
       "   ('for', (8, 9)),\n",
       "   ('if', (10, 11)),\n",
       "   ('is', (13, 14))],\n",
       "  [('up to 40 continuous sentences', (3, 8)),\n",
       "   ('training', (9, 10)),\n",
       "   ('input', (12, 13)),\n",
       "   ('too long', (14, 16))]],\n",
       " [[], [('Experiment Results', (0, 2))]],\n",
       " [[('Comparing with', (0, 2)),\n",
       "   ('achieved', (6, 7)),\n",
       "   ('improved', (15, 16)),\n",
       "   ('by', (18, 19)),\n",
       "   ('achieved', (22, 23))],\n",
       "  [('baseline model', (3, 5)),\n",
       "   ('67.2 % F1 score', (7, 11)),\n",
       "   ('ASL model', (13, 15)),\n",
       "   ('performance', (17, 18)),\n",
       "   ('0.6 %', (19, 21)),\n",
       "   ('67.8 % average F1', (23, 27))]],\n",
       " [[('show', (0, 1)),\n",
       "   ('consider', (5, 6)),\n",
       "   ('encodes', (17, 18)),\n",
       "   ('from', (20, 21))],\n",
       "  [('cross - sentence dependency', (6, 10)),\n",
       "   ('significantly outperform', (10, 12)),\n",
       "   ('baseline model', (13, 15)),\n",
       "   ('each sentence', (18, 20)),\n",
       "   ('input document', (22, 24))]],\n",
       " [[('indicated', (2, 3)), ('than', (10, 11))],\n",
       "  [('ASL model', (5, 7)),\n",
       "   ('better performance', (8, 10)),\n",
       "   ('LSL model', (12, 14))]],\n",
       " [[], [('Coreference Resolution with Entity Equalization', (0, 5))]],\n",
       " [[('provides', (6, 7)), ('facilitates', (21, 22))],\n",
       "  [('entity - level representation', (8, 12)),\n",
       "   ('end - to - end optimization', (22, 28))]],\n",
       " [[('posits', (6, 7)), ('represented via', (12, 14)), ('of', (16, 17))],\n",
       "  [('each entity', (8, 10)), ('sum', (15, 16))]],\n",
       " [[('uses', (8, 9)), ('as input', (11, 13))],\n",
       "  [('contextual embeddings', (9, 11)), ('mention representations', (13, 15))]],\n",
       " [[('motivated by', (15, 17)), ('of', (21, 22))],\n",
       "  [('BERT embeddings', (12, 14)),\n",
       "   ('impressive empirical performance', (18, 21)),\n",
       "   ('BERT', (22, 23))]],\n",
       " [[('done by using', (6, 9)), ('in', (10, 11))],\n",
       "  [('BERT', (9, 10)), ('fully convolutional manner', (12, 15))]],\n",
       " [[], []],\n",
       " [[('is', (2, 3)), ('from with', (8, 10)), ('achieves', (21, 22))],\n",
       "  [('span - ranking model', (4, 8)),\n",
       "   ('ELMo input features and second - order span representations', (10, 19)),\n",
       "   ('73.0 % Avg.', (22, 25))]],\n",
       " [[('Replacing', (2, 3)), ('with', (6, 7)), ('achieves', (9, 10))],\n",
       "  [('F1', (0, 1)),\n",
       "   ('ELMo features', (4, 6)),\n",
       "   ('BERT features', (7, 9)),\n",
       "   ('76. 25 % average F1', (10, 15))]],\n",
       " [[('Removing', (0, 1)),\n",
       "   ('while using', (8, 10)),\n",
       "   ('achieves', (12, 13)),\n",
       "   ('achieving', (17, 18)),\n",
       "   ('on', (23, 24))],\n",
       "  [('second - order span - representations', (2, 8)),\n",
       "   ('BERT features', (10, 12)),\n",
       "   ('76.37 % F1', (13, 16)),\n",
       "   ('higher recall and lower precision', (18, 23)),\n",
       "   ('all evaluation metrics', (24, 27))]],\n",
       " [[('Replacing', (0, 1)),\n",
       "   ('achieves', (7, 8)),\n",
       "   ('consistently achieving', (16, 18))],\n",
       "  [('secondorder span representations', (1, 4)),\n",
       "   ('Entity Equalization', (5, 7)),\n",
       "   ('76. 64 % average F1', (8, 13)),\n",
       "   ('highest F 1 score', (19, 23))]],\n",
       " [[('set', (2, 3)),\n",
       "   ('for', (9, 10)),\n",
       "   ('improving', (13, 14)),\n",
       "   ('by', (20, 21))],\n",
       "  [('new state of the art', (4, 9)),\n",
       "   ('coreference resolution', (10, 12)),\n",
       "   ('previous state of the art', (15, 20)),\n",
       "   ('3.6 % average F1', (21, 25))]],\n",
       " [[], [('End - to - end', (0, 5)), ('Neural Coreference Resolution', (5, 8))]],\n",
       " [[('introduce', (1, 2)), ('show', (13, 14)), ('without using', (21, 23))],\n",
       "  [('first end - to - end coreference resolution model', (3, 12)),\n",
       "   ('significantly outperforms', (16, 18)),\n",
       "   ('all previous work', (18, 21)),\n",
       "   ('syntactic parser or handengineered mention detector', (24, 30))]],\n",
       " [[('present', (1, 2)), ('learned', (17, 18)), ('given', (21, 22))],\n",
       "  [('first state - of - the - art neural coreference resolution model',\n",
       "    (3, 15)),\n",
       "   ('end - toend', (18, 21)),\n",
       "   ('only gold mention clusters', (22, 26))]],\n",
       " [[('demonstrate', (1, 2)),\n",
       "   ('by training', (24, 26)),\n",
       "   ('jointly learns', (35, 37)),\n",
       "   ('are', (39, 40))],\n",
       "  [('first time', (4, 6)),\n",
       "   ('resources', (8, 9)),\n",
       "   ('not required', (10, 12)),\n",
       "   ('performance', (16, 17)),\n",
       "   ('improved significantly', (19, 21)),\n",
       "   ('end - to - end neural model', (27, 34)),\n",
       "   ('which spans', (37, 39)),\n",
       "   ('entity mentions', (40, 42)),\n",
       "   ('best cluster', (45, 47))]],\n",
       " [[('over', (3, 4)),\n",
       "   ('spans up to', (8, 11)),\n",
       "   ('directly optimizes', (15, 17)),\n",
       "   ('of', (20, 21)),\n",
       "   ('from', (23, 24))],\n",
       "  [('Our model reasons', (0, 3)),\n",
       "   ('maximum length', (12, 14)),\n",
       "   ('marginal likelihood', (18, 20)),\n",
       "   ('gold coreference clusters', (24, 27))]],\n",
       " [[('includes', (1, 2)),\n",
       "   ('decides', (8, 9)),\n",
       "   ('for', (10, 11)),\n",
       "   ('which of', (14, 16)),\n",
       "   ('is', (23, 24))],\n",
       "  [('span - ranking model', (3, 7)),\n",
       "   ('each span', (11, 13)),\n",
       "   ('previous spans', (17, 19)),\n",
       "   ('good antecedent', (25, 27))]],\n",
       " [[('are', (6, 7)),\n",
       "   ('representing', (9, 10)),\n",
       "   ('in', (13, 14)),\n",
       "   ('combine', (18, 19)),\n",
       "   ('with', (24, 25)),\n",
       "   ('over', (31, 32))],\n",
       "  [('vector embeddings', (7, 9)),\n",
       "   ('spans of text', (10, 13)),\n",
       "   ('document', (15, 16)),\n",
       "   ('context - dependent boundary representations', (19, 24)),\n",
       "   ('head - finding attention mechanism', (26, 31)),\n",
       "   ('span', (33, 34))]],\n",
       " [[('inspired by', (4, 6)),\n",
       "   ('from', (14, 15)),\n",
       "   ('less susceptible to', (20, 23))],\n",
       "  [('attention component', (1, 3)),\n",
       "   ('parser - derived head - word matching features', (6, 14)),\n",
       "   ('previous systems', (15, 17)),\n",
       "   ('cascading errors', (23, 25))]],\n",
       " [[], []],\n",
       " [[('of', (6, 7))],\n",
       "  [('word embeddings area fixed concatenation', (1, 6)),\n",
       "   ('300 - dimensional GloVe embeddings', (7, 12)),\n",
       "   ('50 - dimensional embeddings', (13, 17)),\n",
       "   ('normalized', (20, 21)),\n",
       "   ('unit vectors', (23, 25))]],\n",
       " [[('represented by', (5, 7)), ('of', (9, 10))],\n",
       "  [('Outof - vocabulary words', (0, 4)),\n",
       "   ('vector', (8, 9)),\n",
       "   ('zeros', (10, 11))]],\n",
       " [[('represented as', (7, 9))],\n",
       "  [('character CNN', (2, 4)),\n",
       "   ('characters', (5, 6)),\n",
       "   ('learned 8 - dimensional embeddings', (9, 14))]],\n",
       " [[('have', (2, 3)), ('of', (5, 6)), ('consisting of', (15, 17))],\n",
       "  [('convolutions', (1, 2)),\n",
       "   ('window sizes', (3, 5)),\n",
       "   ('3 , 4 , and 5 characters', (6, 13)),\n",
       "   ('50 filters', (17, 19))]],\n",
       " [[('in', (3, 4)), ('have', (6, 7))],\n",
       "  [('hidden states', (1, 3)), ('LSTMs', (5, 6)), ('200 dimensions', (7, 9))]],\n",
       " [[('consists of', (4, 6)), ('with', (9, 10))],\n",
       "  [('Each feedforward neural network', (0, 4)),\n",
       "   ('two hidden layers', (6, 9)),\n",
       "   ('150 dimensions', (10, 12)),\n",
       "   ('rectified linear units', (13, 16))]],\n",
       " [[('use', (1, 2)), ('for', (3, 4)), ('with', (5, 6)), ('of', (9, 10))],\n",
       "  [('ADAM', (2, 3)),\n",
       "   ('learning', (4, 5)),\n",
       "   ('minibatch size', (7, 9)),\n",
       "   ('1', (10, 11))]],\n",
       " [[('initialized with', (4, 6))],\n",
       "  [('LSTM weights', (1, 3)), ('random orthonormal matrices', (6, 9))]],\n",
       " [[('apply', (1, 2)), ('to', (4, 5))],\n",
       "  [('0.5 dropout', (2, 4)),\n",
       "   ('word embeddings and character CNN outputs', (6, 12))]],\n",
       " [[('apply', (1, 2)), ('to', (4, 5))],\n",
       "  [('0.2 dropout', (2, 4)),\n",
       "   ('all hidden layers and feature embeddings', (5, 11))]],\n",
       " [[('shared across', (3, 5)), ('to preserve', (6, 8))],\n",
       "  [('Dropout masks', (0, 2)),\n",
       "   ('timesteps', (5, 6)),\n",
       "   ('long - distance information', (8, 12))]],\n",
       " [[('by', (5, 6)), ('every', (8, 9))],\n",
       "  [('learning rate', (1, 3)),\n",
       "   ('decayed', (4, 5)),\n",
       "   ('0.1 %', (6, 8)),\n",
       "   ('100 steps', (9, 11))]],\n",
       " [[('trained for', (3, 5)), ('with', (10, 11)), ('based on', (13, 15))],\n",
       "  [('up to 150 epochs', (5, 9)),\n",
       "   ('early stopping', (11, 13)),\n",
       "   ('development set', (16, 18))]],\n",
       " [[('implemented in', (3, 5))], [('Tensor - Flow', (5, 8))]],\n",
       " [[], []],\n",
       " [[], []],\n",
       " [[('in', (4, 5))],\n",
       "  [('outperform', (1, 2)),\n",
       "   ('previous systems', (2, 4)),\n",
       "   ('all metrics', (5, 7))]],\n",
       " [[('improves', (6, 7)),\n",
       "   ('by', (17, 18)),\n",
       "   ('improves', (26, 27)),\n",
       "   ('by', (28, 29))],\n",
       "  [('our single model', (3, 6)),\n",
       "   ('state - of - the - art average F1', (8, 17)),\n",
       "   ('1.5', (18, 19)),\n",
       "   ('our 5 - model ensemble', (21, 26)),\n",
       "   ('3.1', (29, 30))]],\n",
       " [[('come from', (4, 6)), ('in', (7, 8))],\n",
       "  [('most significant gains', (1, 4)),\n",
       "   ('improvements', (6, 7)),\n",
       "   ('recall', (8, 9))]],\n",
       " [[('are', (9, 10)), ('for', (12, 13))],\n",
       "  [('distance between', (1, 3)),\n",
       "   ('spans and the width of spans', (3, 9)),\n",
       "   ('crucial signals', (10, 12)),\n",
       "   ('coreference resolution', (13, 15))]],\n",
       " [[('contribute', (1, 2)), ('to', (4, 5))],\n",
       "  [('3.8 F1', (2, 4)), ('final result', (6, 8))]],\n",
       " [[('see', (10, 11)), ('of', (13, 14)), ('from', (16, 17))],\n",
       "  [('contribution', (12, 13)),\n",
       "   ('0.9 F1', (14, 16)),\n",
       "   ('character - level modeling', (17, 21))]],\n",
       " [[('show', (2, 3)),\n",
       "   ('in', (7, 8)),\n",
       "   ('without', (9, 10)),\n",
       "   ('for finding', (13, 15))],\n",
       "  [('1.3 F1 degradation', (4, 7)),\n",
       "   ('performance', (8, 9)),\n",
       "   ('attention mechanism', (11, 13)),\n",
       "   ('task - specific heads', (15, 19))]],\n",
       " [[('detected by', (7, 9)),\n",
       "   ('over', (14, 15)),\n",
       "   ('degrades', (25, 26)),\n",
       "   ('by', (27, 28))],\n",
       "  [('rule - based system', (10, 14)),\n",
       "   ('predicted parse trees', (15, 18)),\n",
       "   ('performance', (26, 27)),\n",
       "   ('1 F1', (28, 30))]],\n",
       " [[('With', (0, 1)),\n",
       "   ('see', (5, 6)),\n",
       "   ('of', (8, 9)),\n",
       "   ('suggesting', (12, 13))],\n",
       "  [('oracle mentions', (1, 3)),\n",
       "   ('improvement', (7, 8)),\n",
       "   ('17.5 F1', (9, 11))]],\n",
       " [[], [('Coreference Resolution', (2, 4))]],\n",
       " [[('fine - tune', (1, 4)),\n",
       "   ('to coreference', (5, 7)),\n",
       "   ('achieving', (9, 10)),\n",
       "   ('on', (12, 13))],\n",
       "  [('BERT', (4, 5)),\n",
       "   ('strong improvements', (10, 12)),\n",
       "   ('GAP and benchmarks', (14, 17))]],\n",
       " [[('present', (1, 2)), ('of', (4, 5))],\n",
       "  [('two ways', (2, 4)),\n",
       "   ('extending', (5, 6)),\n",
       "   ('c 2f - coref model', (7, 12))]],\n",
       " [[], []],\n",
       " [[('extend', (1, 2)), ('of', (6, 7))],\n",
       "  [('original Tensorflow implementations', (3, 6)),\n",
       "   ('c 2f - coref 3 and BERT', (7, 14))]],\n",
       " [[('fine tune', (1, 3)),\n",
       "   ('on', (5, 6)),\n",
       "   ('for', (10, 11)),\n",
       "   ('using', (13, 14)),\n",
       "   ('of', (16, 17)),\n",
       "   ('of', (22, 23)),\n",
       "   ('with', (31, 32)),\n",
       "   ('for', (34, 35))],\n",
       "  [('all models', (3, 5)),\n",
       "   ('OntoNotes English data', (7, 10)),\n",
       "   ('20 epochs', (11, 13)),\n",
       "   ('dropout', (15, 16)),\n",
       "   ('0.3', (17, 18)),\n",
       "   ('learning rates', (20, 22)),\n",
       "   ('1 10 ?5 and 2 10 ? 4', (23, 31)),\n",
       "   ('linear decay', (32, 34)),\n",
       "   ('BERT parameters', (36, 38)),\n",
       "   ('task parameters', (40, 42))]],\n",
       " [[('trained', (1, 2)),\n",
       "   ('with', (4, 5)),\n",
       "   ('of', (8, 9)),\n",
       "   ('trained on', (20, 22)),\n",
       "   ('performed', (27, 28)),\n",
       "   ('for', (30, 31))],\n",
       "  [('separate models', (2, 4)),\n",
       "   ('max segment len', (5, 8)),\n",
       "   ('128 , 256 , 384 , and 512', (9, 17)),\n",
       "   ('128 and 384 word pieces', (22, 27)),\n",
       "   ('best', (29, 30)),\n",
       "   ('BERT - base', (31, 34))]],\n",
       " [[], [('Paragraph Level', (0, 2))]],\n",
       " [[('shows', (2, 3)),\n",
       "   ('improves', (5, 6)),\n",
       "   ('by', (11, 12)),\n",
       "   ('for', (17, 18))],\n",
       "  [('BERT', (4, 5)),\n",
       "   ('c 2 f - coref', (6, 11)),\n",
       "   ('9 % and 11.5 %', (12, 17)),\n",
       "   ('base and large models', (19, 23))]],\n",
       " [[], [('Document Level : OntoNotes', (0, 4))]],\n",
       " [[('shows', (0, 1)), ('offers', (5, 6)), ('of', (8, 9)), ('over', (11, 12))],\n",
       "  [('BERT - base', (2, 5)),\n",
       "   ('improvement', (7, 8)),\n",
       "   ('0.9 %', (9, 11)),\n",
       "   ('ELMo - based c2 fcoref model', (13, 19))]],\n",
       " [[('improves', (6, 7)), ('by', (12, 13)), ('of', (17, 18))],\n",
       "  [('BERT - large', (0, 3)),\n",
       "   ('c 2 f - coref', (7, 12)),\n",
       "   ('much larger margin', (14, 17)),\n",
       "   ('3.9 %', (18, 20))]],\n",
       " [[('observe', (2, 3)), ('offers', (7, 8)), ('over', (10, 11))],\n",
       "  [('overlap variant', (5, 7)),\n",
       "   ('no improvement', (8, 10)),\n",
       "   ('independent', (11, 12))]],\n",
       " [[], [('Data - to - Text Generation', (4, 10))]],\n",
       " [[],\n",
       "  [('Transcribing structured data into natural language descriptions',\n",
       "    (0, 7))]],\n",
       " [[('propose', (6, 7)), ('assuming that', (13, 15)), ('should be', (16, 18))],\n",
       "  [('new structured - data encoder', (8, 13)),\n",
       "   ('structures', (15, 16)),\n",
       "   ('hierarchically captured', (18, 20))]],\n",
       " [[('focuses on', (2, 4)), ('of', (6, 7)), ('chosen to be', (16, 19))],\n",
       "  [('encoding', (5, 6)),\n",
       "   ('data - structure', (8, 11)),\n",
       "   ('decoder', (14, 15)),\n",
       "   ('classical module', (20, 22))]],\n",
       " [[('model', (2, 3)),\n",
       "   ('of', (6, 7)),\n",
       "   ('using', (9, 10)),\n",
       "   ('first encoding', (16, 18)),\n",
       "   ('on the basis of', (20, 24)),\n",
       "   ('then', (27, 28)),\n",
       "   ('encoding', (28, 29)),\n",
       "   ('on the basis of', (32, 36)),\n",
       "   ('introduce', (41, 42)),\n",
       "   ('in', (45, 46)),\n",
       "   ('to ensure', (52, 54)),\n",
       "   ('of', (56, 57)),\n",
       "   ('in comparison to', (61, 64)),\n",
       "   ('integrate', (75, 76)),\n",
       "   ('to compute', (80, 82)),\n",
       "   ('fed into', (85, 87))],\n",
       "  [('general structure', (4, 6)),\n",
       "   ('two - level architecture', (11, 15)),\n",
       "   ('all entities', (18, 20)),\n",
       "   ('their elements', (24, 26)),\n",
       "   ('data structure', (30, 32)),\n",
       "   ('its entities', (36, 38)),\n",
       "   ('Transformer encoder', (43, 45)),\n",
       "   ('data - to - text models', (46, 52)),\n",
       "   ('robust encoding', (54, 56)),\n",
       "   ('each element / entities', (57, 61)),\n",
       "   ('all others', (64, 66)),\n",
       "   ('hierarchical attention mechanism', (77, 80)),\n",
       "   ('hierarchical context', (83, 85)),\n",
       "   ('decoder', (88, 89))]],\n",
       " [[], []],\n",
       " [[('is', (1, 2)), ('with', (8, 9))],\n",
       "  [('Wiseman', (0, 1)),\n",
       "   ('standard encoder - decoder system', (3, 8)),\n",
       "   ('copy mechanism', (9, 11))]],\n",
       " [[('is', (1, 2)),\n",
       "   ('with', (7, 8)),\n",
       "   ('first generated with', (15, 18)),\n",
       "   ('replaced by', (22, 24)),\n",
       "   ('extracted from', (26, 28)),\n",
       "   ('by', (30, 31))],\n",
       "  [('Li', (0, 1)),\n",
       "   ('standard encoder - decoder', (3, 7)),\n",
       "   ('delayed copy mechanism', (9, 12)),\n",
       "   ('text', (13, 14)),\n",
       "   ('placeholders', (18, 19)),\n",
       "   ('salient records', (24, 26)),\n",
       "   ('table', (29, 30)),\n",
       "   ('pointer network', (32, 34))]],\n",
       " [[('acts in', (3, 5)),\n",
       "   ('generates', (14, 15)),\n",
       "   ('from', (24, 25)),\n",
       "   ('generates', (34, 35))],\n",
       "  [('Puduppully - plan', (0, 3)),\n",
       "   ('two steps', (5, 7)),\n",
       "   ('plan', (16, 17)),\n",
       "   ('table', (26, 27)),\n",
       "   ('text', (35, 36))]],\n",
       " [[], [('Puduppully - updt', (0, 3))]],\n",
       " [[('consists in', (1, 3)),\n",
       "   ('with', (9, 10)),\n",
       "   ('aimed at updating', (13, 16)),\n",
       "   ('during', (18, 19))],\n",
       "  [('standard encoder - decoder', (4, 8)),\n",
       "   ('added module', (11, 13)),\n",
       "   ('record representations', (16, 18)),\n",
       "   ('generation process', (20, 22))]],\n",
       " [[('At', (0, 1)), ('computes', (9, 10)), ('should', (12, 13))],\n",
       "  [('each decoding step', (1, 4)),\n",
       "   ('gated recurrent network', (6, 9)),\n",
       "   ('new representation', (20, 22))]],\n",
       " [[('of', (2, 3)), ('of', (10, 11)), ('set to', (16, 18))],\n",
       "  [('size', (1, 2)),\n",
       "   ('record value embeddings and hidden layers', (4, 10)),\n",
       "   ('Transformer encoders', (12, 14)),\n",
       "   ('300', (18, 19))]],\n",
       " [[('use', (1, 2)), ('at', (3, 4))],\n",
       "  [('dropout', (2, 3)), ('rate 0.5', (4, 6))]],\n",
       " [[('trained with', (3, 5)), ('of', (8, 9))],\n",
       "  [('batch size', (6, 8)), ('64', (9, 10))]],\n",
       " [[('train', (7, 8)),\n",
       "   ('for', (10, 11)),\n",
       "   ('average', (20, 21)),\n",
       "   ('of', (23, 24)),\n",
       "   ('to ensure', (35, 37)),\n",
       "   ('across', (39, 40))],\n",
       "  [('model', (9, 10)),\n",
       "   ('fixed number of 25 K updates', (12, 18)),\n",
       "   ('weights', (22, 23)),\n",
       "   ('last 5 checkpoints', (25, 28)),\n",
       "   ('every 1 K updates', (30, 34)),\n",
       "   ('more stability', (37, 39)),\n",
       "   ('runs', (40, 41))]],\n",
       " [[('trained with', (3, 5)),\n",
       "   ('is', (13, 14)),\n",
       "   ('reduced by', (18, 20)),\n",
       "   ('every', (21, 22))],\n",
       "  [('Adam optimizer', (6, 8)),\n",
       "   ('initial learning rate', (10, 13)),\n",
       "   ('0.001', (14, 15)),\n",
       "   ('half', (20, 21)),\n",
       "   ('10 K steps', (22, 25))]],\n",
       " [[('used', (1, 2)), ('with', (4, 5)), ('of', (7, 8)), ('during', (9, 10))],\n",
       "  [('beam search', (2, 4)),\n",
       "   ('beam size', (5, 7)),\n",
       "   ('5', (8, 9)),\n",
       "   ('inference', (10, 11))]],\n",
       " [[('implemented in', (4, 6))], [('Open NMT - py', (6, 10))]],\n",
       " [[], []],\n",
       " [[('see', (6, 7)), ('obtained by', (10, 12)), ('compared to', (15, 17))],\n",
       "  [('lower results', (8, 10)),\n",
       "   ('Flat scenario', (13, 15)),\n",
       "   ('other scenarios', (18, 20))]],\n",
       " [[('between', (4, 5)),\n",
       "   ('shows', (12, 13)),\n",
       "   ('omitting', (14, 15)),\n",
       "   ('of', (18, 19)),\n",
       "   ('in', (22, 23)),\n",
       "   ('is', (26, 27)),\n",
       "   ('excepted', (39, 40))],\n",
       "  [('comparison', (3, 4)),\n",
       "   ('scenario Hierarchical - kv and Hierarchical -k', (5, 12)),\n",
       "   ('entirely the influence', (15, 18)),\n",
       "   ('record values', (20, 22)),\n",
       "   ('attention mechanism', (24, 26)),\n",
       "   ('more effective', (27, 29))]],\n",
       " [[('achieve', (3, 4)), ('on', (7, 8))],\n",
       "  [('Our hierarchical models', (0, 3)),\n",
       "   ('significantly better scores', (4, 7)),\n",
       "   ('all metrics', (8, 10)),\n",
       "   ('flat architecture Wiseman', (14, 17))]],\n",
       " [[('shows', (1, 2)),\n",
       "   ('obtains', (6, 7)),\n",
       "   ('generates', (18, 19)),\n",
       "   ('with', (21, 22)),\n",
       "   ('included in', (32, 34))],\n",
       "  [('our Flat scenario', (3, 6)),\n",
       "   ('significant higher BLEU score ( 16.7 vs. 14.5 )', (8, 17)),\n",
       "   ('fluent descriptions', (19, 21)),\n",
       "   ('accurate mentions ( RG - P % )', (22, 30)),\n",
       "   ('gold descriptions ( CS - R% )', (35, 42))]],\n",
       " [[('of', (9, 10)), ('on', (15, 16))],\n",
       "  [('Our hierarchical models', (0, 3)),\n",
       "   ('outperform', (3, 4)),\n",
       "   ('two - step decoders', (5, 9)),\n",
       "   ('Li and Puduppully - plan', (10, 15)),\n",
       "   ('BLEU and all qualitative metrics', (17, 22))]],\n",
       " [[], [('Sequence - to - Sequence Natural Language Generation', (8, 16))]],\n",
       " [[], [('Natural language generation', (0, 3))]],\n",
       " [[('present', (2, 3)), ('train and test on', (12, 16)), ('in', (20, 21))],\n",
       "  [('neural ensemble natural language generator', (4, 9)),\n",
       "   ('three large unaligned datasets', (16, 20)),\n",
       "   ('restaurant , television , and laptop domains', (22, 29))]],\n",
       " [[('explore', (1, 2)),\n",
       "   ('to represent', (4, 6)),\n",
       "   ('including', (10, 11)),\n",
       "   ('for', (13, 14)),\n",
       "   ('use of', (28, 30))],\n",
       "  [('novel ways', (2, 4)),\n",
       "   ('MR inputs', (7, 9)),\n",
       "   ('novel methods', (11, 13)),\n",
       "   ('delexicalizing', (14, 15)),\n",
       "   ('slots', (15, 16)),\n",
       "   ('automatic slot alignment', (20, 23)),\n",
       "   ('semantic reranker', (31, 33))]],\n",
       " [[('built', (1, 2)), ('using', (5, 6)), ('for', (9, 10))],\n",
       "  [('our ensemble model', (2, 5)),\n",
       "   ('seq2seq framework', (7, 9)),\n",
       "   ('TensorFlow', (10, 11))]],\n",
       " [[('use', (4, 5)), ('with', (9, 10)), ('use', (19, 20))],\n",
       "  [('bidirectional LSTM encoder', (6, 9)),\n",
       "   ('512 cells per layer', (10, 14)),\n",
       "   ('CNN', (17, 18)),\n",
       "   ('pooling encoder', (21, 23))]],\n",
       " [[('was', (5, 6)), ('with', (12, 13))],\n",
       "  [('decoder', (1, 2)),\n",
       "   ('4 - layer RNN decoder', (7, 12)),\n",
       "   ('512 LSTM cells per', (13, 17)),\n",
       "   ('layer', (17, 18))]],\n",
       " [[('on', (1, 2))], [('E2E Dataset', (3, 5))]],\n",
       " [[], [('Automatic Metric Evaluation', (0, 3))]],\n",
       " [[('show', (3, 4)), ('benefit from', (13, 15)), ('in', (19, 20))],\n",
       "  [('both the LSTM and the CNN models', (5, 12)),\n",
       "   ('additional pseudo - samples', (15, 19)),\n",
       "   ('training set', (21, 23))]],\n",
       " [[('On', (0, 1)),\n",
       "   ('performs', (10, 11)),\n",
       "   ('to', (12, 13)),\n",
       "   ('in terms of', (19, 22))],\n",
       "  [('official E2E test set', (2, 6)),\n",
       "   ('our ensemble model', (7, 10)),\n",
       "   ('comparably', (11, 12)),\n",
       "   ('baseline model', (14, 16)),\n",
       "   ('automatic metrics', (22, 24))]],\n",
       " [[], [('TV and Laptop Datasets', (2, 6))]],\n",
       " [[('performs', (6, 7)),\n",
       "   ('with', (8, 9)),\n",
       "   ('on', (11, 12)),\n",
       "   ('on', (20, 21)),\n",
       "   ('by', (24, 25))],\n",
       "  [('our ensemble model', (3, 6)),\n",
       "   ('competitively', (7, 8)),\n",
       "   ('baseline', (10, 11)),\n",
       "   ('TV dataset', (13, 15)),\n",
       "   ('outperforms', (18, 19)),\n",
       "   ('Laptop dataset', (22, 24)),\n",
       "   ('wide margin', (26, 28))]],\n",
       " [[], [('Structured Data to Text Generation', (5, 10))]],\n",
       " [[], [('neural text generation', (4, 7))]],\n",
       " [[], [('text generation', (9, 11)), ('sequenceto - sequence', (14, 17))]],\n",
       " [[], []],\n",
       " [[], [('WebNLG task', (0, 2))]],\n",
       " [[('employs', (14, 15)), ('with', (19, 20))],\n",
       "  [('GCN encoder', (7, 9)),\n",
       "   ('outperforms', (9, 10)),\n",
       "   ('strong baseline', (11, 13)),\n",
       "   ('LSTM encoder', (16, 18)),\n",
       "   ('.009 BLEU points', (20, 23))]],\n",
       " [[('is', (3, 4)), ('than', (7, 8)), ('with', (10, 11)), ('of', (14, 15))],\n",
       "  [('GCN model', (1, 3)),\n",
       "   ('more stable', (5, 7)),\n",
       "   ('baseline', (9, 10)),\n",
       "   ('standard deviation', (12, 14)),\n",
       "   ('.004 vs . 010', (15, 19))]],\n",
       " [[('that uses', (6, 8)), ('by', (19, 20)), ('by', (26, 27))],\n",
       "  [('GCN EC model', (1, 4)),\n",
       "   ('outperforms', (4, 5)),\n",
       "   ('PKUWRITER', (5, 6)),\n",
       "   ('ensemble of 7 models', (9, 13)),\n",
       "   ('further reinforcement learning step', (15, 19)),\n",
       "   ('.047 BLEU points', (20, 23)),\n",
       "   ('MELBOURNE', (25, 26)),\n",
       "   ('.014 BLEU points', (27, 30))]],\n",
       " [[], [('SR11 Deep task', (0, 3))]],\n",
       " [[('by', (14, 15)),\n",
       "   ('of', (18, 19)),\n",
       "   ('obtains', (26, 27)),\n",
       "   ('outperforming', (34, 35))],\n",
       "  [('pipeline model', (16, 18)),\n",
       "   ('STUMBA - D and TBDIL model', (20, 26)),\n",
       "   ('.794 and . 805 BLUE', (28, 33)),\n",
       "   ('GCN - based model', (36, 40))]],\n",
       " [[('notice', (4, 5)), ('of', (8, 9)), ('between', (11, 12))],\n",
       "  [('importance', (7, 8)),\n",
       "   ('skip connections', (9, 11)),\n",
       "   ('GCN layers', (12, 14))]],\n",
       " [[('lead to', (4, 6))],\n",
       "  [('Residual and dense connections', (0, 4)), ('similar results', (6, 8))]],\n",
       " [[('produce', (9, 10)), ('than', (18, 19))],\n",
       "  [('Dense connections', (0, 2)),\n",
       "   ('models', (10, 11)),\n",
       "   ('bigger', (11, 12)),\n",
       "   ('slightly less accurate', (14, 17)),\n",
       "   ('residual connections', (19, 21))]],\n",
       " [[], [('Pragmatically Informative Text Generation', (0, 4))]],\n",
       " [[], [('gametheoretic or Bayesian inference procedures', (10, 15))]],\n",
       " [[('builds on', (2, 4)),\n",
       "   ('in which', (16, 18)),\n",
       "   ('selected to optimize', (21, 24))],\n",
       "  [('line of learned Rational Speech Acts ( RSA ) models', (5, 15)),\n",
       "   ('generated strings', (18, 20)),\n",
       "   ('behav - Human - written', (25, 30))]],\n",
       " [[('of', (3, 4)),\n",
       "   ('grounded in', (15, 17)),\n",
       "   ('in the presence of', (27, 31))],\n",
       "  [('canonical presentation', (1, 3)),\n",
       "   ('RSA framework', (5, 7)),\n",
       "   ('reference resolution', (17, 19))]],\n",
       " [[], [('Abstractive Summarization', (0, 2))]],\n",
       " [[('obtain', (3, 4)),\n",
       "   ('of', (5, 6)),\n",
       "   ('in', (7, 8)),\n",
       "   ('over', (13, 14)),\n",
       "   ('with', (20, 21)),\n",
       "   ('outperforming', (28, 29))],\n",
       "  [('pragmatic methods', (1, 3)),\n",
       "   ('improvements', (4, 5)),\n",
       "   ('0.2-0.5', (6, 7)),\n",
       "   ('ROUGE scores', (8, 10)),\n",
       "   ('0.2-1.8 METEOR', (11, 13)),\n",
       "   ('base S 0 model', (15, 19)),\n",
       "   ('distractor - based approach', (22, 26)),\n",
       "   ('SD 1', (26, 28)),\n",
       "   ('reconstructorbased approach S R 1', (30, 35))]],\n",
       " [[('is', (2, 3)),\n",
       "   ('across', (4, 5)),\n",
       "   ('obtaining', (8, 9)),\n",
       "   ('by', (23, 24)),\n",
       "   ('for', (29, 30)),\n",
       "   ('when constructing', (42, 44)),\n",
       "   ('by masking', (46, 48))],\n",
       "  [('SD 1', (0, 2)),\n",
       "   ('strong', (3, 4)),\n",
       "   ('all metrics', (5, 7)),\n",
       "   ('results', (9, 10)),\n",
       "   ('best previous abstractive systems', (13, 17)),\n",
       "   ('Coverage ratios', (21, 23)),\n",
       "   ('attribute type ( columns )', (24, 29)),\n",
       "   ('base model S0', (31, 34)),\n",
       "   ('pragmatic system SD 1', (38, 42)),\n",
       "   ('distractor', (45, 46)),\n",
       "   ('specified attribute ( rows )', (49, 54))]],\n",
       " [[], [('Data - to - Text Generation', (0, 6))]],\n",
       " [[('learns', (2, 3)),\n",
       "   ('from', (6, 7)),\n",
       "   ('on', (11, 12)),\n",
       "   ('to generate', (17, 19))],\n",
       "  [('content plan', (4, 6)),\n",
       "   ('input and conditions', (8, 11)),\n",
       "   ('content plan', (13, 15)),\n",
       "   ('output document', (20, 22))]],\n",
       " [[('train', (1, 2)), ('using', (9, 10))],\n",
       "  [('our model end - to - end', (2, 9)), ('neural networks', (10, 12))]],\n",
       " [[('used', (1, 2)), ('during', (7, 8)), ('during', (16, 17))],\n",
       "  [('one - layer pointer networks', (2, 7)),\n",
       "   ('content planning', (8, 10)),\n",
       "   ('two - layer LSTMs', (12, 16)),\n",
       "   ('text generation', (17, 19))]],\n",
       " [[('employed for', (3, 5))],\n",
       "  [('Input feeding', (0, 2)), ('text decoder', (6, 8))]],\n",
       " [[('applied', (1, 2)), ('at', (4, 5)), ('of', (7, 8))],\n",
       "  [('dropout', (2, 3)), ('rate', (6, 7)), ('0.3', (8, 9))]],\n",
       " [[('trained for', (2, 4)), ('with', (6, 7)), ('selected from', (22, 24))],\n",
       "  [('25 epochs', (4, 6)),\n",
       "   ('Adagrad optimizer', (8, 10)),\n",
       "   ('initial learning rate', (12, 15)),\n",
       "   ('0.15', (16, 17)),\n",
       "   ('learning rate decay', (18, 21)),\n",
       "   ('{ 0.5 , 0.97 }', (24, 29)),\n",
       "   ('batch size', (31, 33)),\n",
       "   ('5', (34, 35))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('made use of', (5, 8)),\n",
       "   ('set', (11, 12)),\n",
       "   ('to', (15, 16))],\n",
       "  [('text decoding', (1, 3)),\n",
       "   ('BPTT', (8, 9)),\n",
       "   ('truncation size', (13, 15)),\n",
       "   ('100', (16, 17))]],\n",
       " [[('set', (1, 2)), ('to', (5, 6)), ('during', (7, 8))],\n",
       "  [('beam size', (3, 5)), ('5', (6, 7)), ('inference', (8, 9))]],\n",
       " [[('implemented in', (3, 5))], [('Open NMT - py', (5, 9))]],\n",
       " [[], []],\n",
       " [[('improves upon', (6, 8))],\n",
       "  [('NCP', (5, 6)),\n",
       "   ('vanilla encoderdecoder models ( ED + JC , ED + CC )', (8, 20))]],\n",
       " [[('achieves', (4, 5)), ('with', (7, 8))],\n",
       "  [('NCP', (3, 4)),\n",
       "   ('comparable scores', (5, 7)),\n",
       "   ('joint or conditional copy mechanism', (9, 14))]],\n",
       " [[('achieves', (5, 6)), ('in terms of', (13, 16))],\n",
       "  [('NCP + CC', (2, 5)),\n",
       "   ('best content selection and content ordering scores', (6, 13)),\n",
       "   ('BLEU', (16, 17))]],\n",
       " [[('Compared to', (0, 2)),\n",
       "   ('in', (6, 7)),\n",
       "   ('achieve', (12, 13)),\n",
       "   ('of', (16, 17)),\n",
       "   ('in terms of', (20, 23)),\n",
       "   ('improves', (30, 31))],\n",
       "  [('best reported system', (3, 6)),\n",
       "   ('Wiseman et al.', (7, 10)),\n",
       "   ('absolute improvement', (14, 16)),\n",
       "   ('approximately 12 %', (17, 20)),\n",
       "   ('relation generation', (23, 25)),\n",
       "   ('content selection precision', (26, 29)),\n",
       "   ('5 %', (32, 34)),\n",
       "   ('recall', (35, 36)),\n",
       "   ('15 %', (37, 39)),\n",
       "   ('content ordering', (40, 42)),\n",
       "   ('increases', (42, 43)),\n",
       "   ('3 %', (44, 46)),\n",
       "   ('BLEU', (48, 49)),\n",
       "   ('1.5 points', (50, 52))]],\n",
       " [[('observe', (12, 13)),\n",
       "   ('obtains', (15, 16)),\n",
       "   ('scores high', (22, 24)),\n",
       "   ('on', (24, 25))],\n",
       "  [('template - based system', (4, 8)),\n",
       "   ('low BLEU and CS precision', (16, 21)),\n",
       "   ('CS recall and RG metrics', (25, 30))]],\n",
       " [[], [('Neural Data - to - Text Generation', (11, 18))]],\n",
       " [[],\n",
       "  [('Data - to - text generation', (0, 6)),\n",
       "   ('ordering and structuring the information ( planning )', (14, 22)),\n",
       "   ('generating fluent language', (24, 27))]],\n",
       " [[('propose', (2, 3)), ('whose', (12, 13)), ('fed into', (15, 17))],\n",
       "  [('explicit , symbolic , text planning stage', (4, 11)),\n",
       "   ('output', (13, 14)),\n",
       "   ('neural generation system', (18, 21))]],\n",
       " [[('determines', (3, 4)), ('expresses it', (8, 10))],\n",
       "  [('text planner', (1, 3)),\n",
       "   ('information structure', (5, 7)),\n",
       "   ('unambiguously', (10, 11)),\n",
       "   ('sequence of ordered trees', (17, 21))]],\n",
       " [[('determined', (4, 5)), ('into', (16, 17))],\n",
       "  [('plan', (2, 3)), ('fluent , natural language text', (17, 22))]],\n",
       " [[], [('https://github.com/AmitMY/ chimera', (12, 14))]],\n",
       " [[('compare', (1, 2)), ('in', (6, 7)), ('on', (23, 24))],\n",
       "  [('best submissions', (4, 6)),\n",
       "   ('WebNLG challenge', (8, 10)),\n",
       "   ('Melbourne', (11, 12)),\n",
       "   ('end - to - end system', (14, 20)),\n",
       "   ('best', (22, 23)),\n",
       "   ('all categories', (24, 26))]],\n",
       " [[('developed', (3, 4))],\n",
       "  [('end - to - end neural baseline', (5, 12)),\n",
       "   ('outperforms', (13, 14)),\n",
       "   ('WebNLG neural systems', (15, 18))]],\n",
       " [[('uses', (1, 2)), ('with', (9, 10))],\n",
       "  [('set encoder', (3, 5)),\n",
       "   ('LSTM decoder', (7, 9)),\n",
       "   ('attention', (10, 11)),\n",
       "   ('copy - attention mechanism', (13, 17)),\n",
       "   ('neural checklist model', (19, 22)),\n",
       "   ('entity dropout', (27, 29))]],\n",
       " [[], []],\n",
       " [[('on', (12, 13))],\n",
       "  [('outperform', (6, 7)),\n",
       "   ('all the WebNLG participating systems', (7, 12)),\n",
       "   ('all automatic metrics', (13, 16))]],\n",
       " [[], [('Character - based Data - to - text Generation', (6, 15))]],\n",
       " [[], [('natural language generation', (19, 22))]],\n",
       " [[('allows', (17, 18)),\n",
       "   ('to', (22, 23)),\n",
       "   ('enabling', (26, 27)),\n",
       "   ('for', (33, 34))],\n",
       "  [('more general approach', (19, 22)), ('text generation', (23, 25))]],\n",
       " [[('present', (15, 16)), ('with', (26, 27)), ('results in', (30, 32))],\n",
       "  [('character - level sequence - to - sequence model', (17, 26)),\n",
       "   ('attention mechanism', (27, 29)),\n",
       "   ('completely neural end - to - end architecture', (33, 41))]],\n",
       " [[('does not require', (10, 13))],\n",
       "  [('delexicalization', (13, 14)),\n",
       "   ('tokenization', (15, 16)),\n",
       "   ('lowercasing', (17, 18))]],\n",
       " [[('achieves', (8, 9)), ('produces', (14, 15))],\n",
       "  [('rather interesting performance results', (9, 13)),\n",
       "   ('vocabulary - free model', (16, 20)),\n",
       "   ('inherently more general', (22, 25))]],\n",
       " [[('shows', (5, 6)),\n",
       "   ('with respect to', (10, 13)),\n",
       "   ('consisting in', (33, 35)),\n",
       "   ('between', (38, 39)),\n",
       "   ('disengages', (45, 46)),\n",
       "   ('to learn', (48, 50)),\n",
       "   ('improves', (67, 68)),\n",
       "   ('enhancing', (73, 74))],\n",
       "  [('two important features', (6, 9)),\n",
       "   ('state - of - art architecture', (14, 20)),\n",
       "   ('character - wise copy mechanism', (27, 32)),\n",
       "   ('soft switch', (36, 38)),\n",
       "   ('generation and copy mode', (39, 43)),\n",
       "   ('model', (47, 48)),\n",
       "   ('rare and unhelpful self - correspondences', (50, 56)),\n",
       "   ('peculiar training procedure', (62, 65)),\n",
       "   ('internal representation capabilities', (69, 72)),\n",
       "   ('recall', (74, 75))]],\n",
       " [[('developed', (1, 2)), ('using', (4, 5))],\n",
       "  [('our system', (2, 4)), ('PyTorch framework', (6, 8))]],\n",
       " [[('minimize', (1, 2)), ('using', (8, 9)), ('computes', (20, 21))],\n",
       "  [('negative log - likelihood loss', (3, 8)),\n",
       "   ('teacher forcing and Adam', (9, 13)),\n",
       "   ('individual adaptive learning rates', (21, 25))]],\n",
       " [[], []],\n",
       " [[('is', (4, 5)),\n",
       "   ('always obtains', (9, 11)),\n",
       "   ('with respect to', (14, 17)),\n",
       "   ('on', (18, 19)),\n",
       "   ('on', (33, 34))],\n",
       "  [('our model EDA_CS', (6, 9)),\n",
       "   ('higher metric values', (11, 14)),\n",
       "   ('TGen', (17, 18)),\n",
       "   ('Hotel and Restaurant datasets', (20, 24)),\n",
       "   ('three out of five higher metrics values', (26, 33)),\n",
       "   ('E2E dataset', (35, 37))]],\n",
       " [[('in the case of', (2, 6)), ('achieves', (10, 11))],\n",
       "  [('E2E +', (6, 8)),\n",
       "   ('TGen', (9, 10)),\n",
       "   ('three out of five higher metrics values', (11, 18))]],\n",
       " [[('allows to obtain', (10, 13)),\n",
       "   ('with respect to', (15, 18)),\n",
       "   ('in', (20, 21)),\n",
       "   ('on', (24, 25))],\n",
       "  [('approach', (7, 8)),\n",
       "   ('better performance', (13, 15)),\n",
       "   ('training', (18, 19)),\n",
       "   ('EDA_CS', (19, 20)),\n",
       "   ('standard way', (22, 24)),\n",
       "   ('Hotel and Restaurant datasets', (26, 30)),\n",
       "   ('outperforms', (43, 44)),\n",
       "   ('EDA_CS', (44, 45)),\n",
       "   ('one case', (47, 49))]],\n",
       " [[('shows', (4, 5)),\n",
       "   ('of', (8, 9)),\n",
       "   ('with respect to', (13, 16)),\n",
       "   ('when compared to', (19, 22))],\n",
       "  [('EDA_CS TL', (2, 4)),\n",
       "   ('bleu increment', (6, 8)),\n",
       "   ('at least 14 %', (9, 13)),\n",
       "   (\"TGen 's score\", (16, 19)),\n",
       "   ('Hotel and Restaurant datasets', (23, 27))]],\n",
       " [[('by', (11, 12))],\n",
       "  [('baseline model', (3, 5)),\n",
       "   ('EDA', (6, 7)),\n",
       "   ('largely outperformed', (9, 11)),\n",
       "   ('all other examined methods', (12, 16))]],\n",
       " [[('for', (5, 6))], [('joint POS tagging and dependency parsing', (6, 12))]],\n",
       " [[],\n",
       "  [('joint part - of - speech ( POS ) tagging and dependency parsing',\n",
       "    (8, 21))]],\n",
       " [[], [('https://github.com/datquocnguyen/jPTDP', (11, 12))]],\n",
       " [[], [('Dependency parsing', (0, 2))]],\n",
       " [[('present', (5, 6)), ('for jointly', (13, 15))],\n",
       "  [('POS tagging and dependency paring', (16, 21))]],\n",
       " [[('extends', (3, 4)), ('with', (14, 15))],\n",
       "  [('well - known BIST graph - based dependency parser', (5, 14)),\n",
       "   ('additional lower - level BiLSTM - based tagging component', (16, 25))]],\n",
       " [[('implemented using', (5, 7)), ('with', (9, 10))],\n",
       "  [('DYNET v2.0', (7, 9)), ('fixed random seed', (11, 14))]],\n",
       " [[('initialized', (3, 4)), ('are', (18, 19))],\n",
       "  [('Word embeddings', (0, 2)),\n",
       "   ('randomly', (5, 6)),\n",
       "   ('pre-trained word vectors', (8, 11)),\n",
       "   ('character and POS tag embeddings', (13, 18)),\n",
       "   ('randomly initialized', (19, 21))]],\n",
       " [[('apply', (1, 2)), ('with', (3, 4)), ('to', (9, 10))],\n",
       "  [('dropout', (2, 3)),\n",
       "   ('67 % keep probability', (5, 9)),\n",
       "   ('inputs of BiLSTMs and MLPs', (11, 16))]],\n",
       " [[('apply', (5, 6)),\n",
       "   ('to learn', (8, 10)),\n",
       "   ('for', (12, 13)),\n",
       "   ('replace', (17, 18)),\n",
       "   ('in', (28, 29)),\n",
       "   ('with', (32, 33)),\n",
       "   ('with', (39, 40))],\n",
       "  [('word dropout', (6, 8)),\n",
       "   ('embedding', (11, 12)),\n",
       "   ('unknown words', (13, 15)),\n",
       "   ('each word token', (18, 21)),\n",
       "   ('appearing # ( w ) times', (22, 28)),\n",
       "   ('training set', (30, 32)),\n",
       "   ('special \" unk \" symbol', (34, 39)),\n",
       "   ('probability punk ( w )', (40, 45))]],\n",
       " [[('optimize', (1, 2)),\n",
       "   ('using', (5, 6)),\n",
       "   ('with', (14, 15)),\n",
       "   ('at', (19, 20))],\n",
       "  [('objective loss', (3, 5)),\n",
       "   ('Adam ( Kingma and Ba , 2014 )', (6, 14)),\n",
       "   ('initial learning rate', (16, 19)),\n",
       "   ('0.001', (20, 21))]],\n",
       " [[('use', (9, 10))],\n",
       "  [('100 - dimensional word embeddings', (10, 15)),\n",
       "   ('50 - dimensional character embeddings', (16, 21)),\n",
       "   ('100 dimensional POS tag embeddings', (22, 27))]],\n",
       " [[('fix', (2, 3)), ('in', (8, 9)), ('at', (10, 11))],\n",
       "  [('number of hidden nodes', (4, 8)), ('MLPs', (9, 10)), ('100', (11, 12))]],\n",
       " [[('produces', (4, 5))],\n",
       "  [('our model', (2, 4)), ('very competitive parsing results', (5, 9))]],\n",
       " [[('obtains', (5, 6)), ('at', (9, 10))],\n",
       "  [('UAS score', (7, 9)),\n",
       "   ('94.51 %', (10, 12)),\n",
       "   ('LAS score', (14, 16)),\n",
       "   ('92.87 %', (17, 19))]],\n",
       " [[('achieve', (1, 2)), ('than', (7, 8))],\n",
       "  [('0.9 % lower parsing scores', (2, 7)),\n",
       "   ('state - of - the - art dependency parser', (9, 18))]],\n",
       " [[('obtain', (2, 3)), ('at', (14, 15)), ('on', (17, 18))],\n",
       "  [('state - of - the - art', (4, 11)),\n",
       "   ('POS tagging accuracy', (11, 14)),\n",
       "   ('97.97 %', (15, 17)),\n",
       "   ('test Section', (19, 21))]],\n",
       " [[], [('Dependency Parsing', (3, 5))]],\n",
       " [[('centered around', (7, 9)), ('which are', (16, 18))],\n",
       "  [('BiRNNs', (9, 10)),\n",
       "   ('BiLSTMs', (14, 15)),\n",
       "   ('strong and trainable sequence models', (18, 23))]],\n",
       " [[('represent', (1, 2)),\n",
       "   ('by', (4, 5)),\n",
       "   ('use', (10, 11)),\n",
       "   ('of', (13, 14)),\n",
       "   ('of', (17, 18)),\n",
       "   ('as', (21, 22)),\n",
       "   ('passed to', (29, 31))],\n",
       "  [('each word', (2, 4)),\n",
       "   ('concatenation', (12, 13)),\n",
       "   ('minimal set', (15, 17)),\n",
       "   ('our feature function', (22, 25)),\n",
       "   ('non-linear scoring function ( multi - layer perceptron )', (32, 41))]],\n",
       " [[('trained with', (5, 7)), ('to learn', (14, 16)), ('for', (20, 21))],\n",
       "  [('BiLSTM', (3, 4)),\n",
       "   ('rest of the parser', (8, 12)),\n",
       "   ('good feature representation', (17, 20)),\n",
       "   ('parsing problem', (22, 24))]],\n",
       " [[('demonstrate', (1, 2)),\n",
       "   ('using', (8, 9)),\n",
       "   ('in', (13, 14)),\n",
       "   ('as well as', (25, 28))],\n",
       "  [('effectiveness', (3, 4)),\n",
       "   ('BiLSTM feature extractor', (10, 13)),\n",
       "   ('two parsing architectures', (14, 17)),\n",
       "   ('transition - based', (18, 21)),\n",
       "   ('graph - based', (29, 32))]],\n",
       " [[('In', (0, 1)),\n",
       "   ('jointly train', (6, 8)),\n",
       "   ('on top of', (13, 16)),\n",
       "   ('propagating', (19, 20)),\n",
       "   ('from', (21, 22))],\n",
       "  [('graphbased parser', (2, 4)),\n",
       "   ('structured - prediction model', (9, 13)),\n",
       "   ('BiLSTM', (17, 18)),\n",
       "   ('errors', (20, 21)),\n",
       "   ('structured objective', (23, 25)),\n",
       "   ('BiLSTM feature - encoder', (31, 35))]],\n",
       " [[('implemented in', (3, 5)), ('using', (7, 8)), ('for', (12, 13))],\n",
       "  [('parsers', (1, 2)),\n",
       "   ('python', (5, 6)),\n",
       "   ('neural network training', (13, 16))]],\n",
       " [[], [('https://github.com/elikip / bist -parser', (8, 12))]],\n",
       " [[('use', (1, 2)), ('implemented in', (5, 7)), ('optimize using', (10, 12))],\n",
       "  [('LSTM variant', (3, 5)), ('PyCNN', (7, 8)), ('Adam optimizer', (13, 15))]],\n",
       " [[('are', (6, 7)), ('using', (11, 12))],\n",
       "  [('our parsers', (4, 6)),\n",
       "   ('very competitive', (7, 9)),\n",
       "   ('very simple parsing architectures', (12, 16)),\n",
       "   ('minimal feature extractors', (17, 20))]],\n",
       " [[('When not using', (0, 3)),\n",
       "   ('with', (14, 15)),\n",
       "   ('thatare', (21, 22)),\n",
       "   ('not using', (22, 24)),\n",
       "   ('including', (27, 28))],\n",
       "  [('external embeddings', (3, 5)),\n",
       "   ('first - order graph - based parser', (7, 14)),\n",
       "   ('2 features', (15, 17)),\n",
       "   ('outperforms', (17, 18)),\n",
       "   ('all other systems', (18, 21)),\n",
       "   ('external resources', (24, 26)),\n",
       "   ('third - order TurboParser', (29, 33))]],\n",
       " [[('with', (5, 6)),\n",
       "   ('including', (16, 17)),\n",
       "   ('with', (23, 24)),\n",
       "   ('when trained using', (42, 45))],\n",
       "  [('greedy transition based parser', (1, 5)),\n",
       "   ('4 features', (6, 8)),\n",
       "   ('matches or outperforms', (9, 12)),\n",
       "   ('most other parsers', (12, 15)),\n",
       "   ('beam - based transition parser', (18, 23)),\n",
       "   ('heavily engineered features', (24, 27)),\n",
       "   ('Stack - LSTM parser', (30, 34)),\n",
       "   ('same parser', (40, 42)),\n",
       "   ('dynamic oracle', (46, 48))]],\n",
       " [[('Moving from', (0, 2)),\n",
       "   ('to', (8, 9)),\n",
       "   ('leads to', (17, 19)),\n",
       "   ('for', (23, 24))],\n",
       "  [('simple ( 4 features )', (3, 8)),\n",
       "   ('extended ( 11 features ) feature set', (10, 17)),\n",
       "   ('some gains in accuracy', (19, 23))]],\n",
       " [[('adding', (3, 4)), ('of', (9, 10))],\n",
       "  [('external word embeddings', (4, 7)),\n",
       "   ('accuracy', (8, 9)),\n",
       "   ('graph - based parser', (11, 15)),\n",
       "   ('degrades', (15, 16))]],\n",
       " [[('give', (1, 2)), ('to', (5, 6)), ('viewing it', (16, 18))],\n",
       "  [('probabilistic interpretation', (3, 5)),\n",
       "   ('ensemble parser', (7, 9)),\n",
       "   ('minimum Bayes risk inference', (22, 26))]],\n",
       " [[('distilling', (7, 8)),\n",
       "   ('into', (10, 11)),\n",
       "   ('with', (15, 16)),\n",
       "   ('by defining', (18, 20))],\n",
       "  [('ensemble', (9, 10)),\n",
       "   ('single FOG parser', (12, 15)),\n",
       "   ('discriminative training', (16, 18)),\n",
       "   ('new cost function', (21, 24))]],\n",
       " [[('derive', (5, 6)),\n",
       "   ('of', (8, 9)),\n",
       "   ('from', (12, 13)),\n",
       "   ('use', (21, 22)),\n",
       "   ('in', (24, 25))],\n",
       "  [('cost', (7, 8)),\n",
       "   ('each possible attachment', (9, 12)),\n",
       "   (\"ensemble 's division of votes\", (14, 19)),\n",
       "   ('cost', (23, 24)),\n",
       "   ('discriminative learning', (25, 27))]],\n",
       " [[], [('graphbased dependency parsing', (9, 12))]],\n",
       " [[('consider', (2, 3)), ('trained with', (7, 9))],\n",
       "  [('neural FOG parser', (4, 7)), ('Hamming cost', (9, 11))]],\n",
       " [[('is', (1, 2))],\n",
       "  [('very strong benchmark', (3, 6)),\n",
       "   ('outperforming', (7, 8)),\n",
       "   ('many higherorder graph - based and neural network models', (8, 17))]],\n",
       " [[('training', (2, 3)),\n",
       "   ('with', (6, 7)),\n",
       "   ('gives', (9, 10)),\n",
       "   ('for', (12, 13))],\n",
       "  [('same model', (4, 6)),\n",
       "   ('distillation cost', (7, 9)),\n",
       "   ('consistent improvements', (10, 12)),\n",
       "   ('all languages', (13, 15))]],\n",
       " [[('see', (4, 5)), ('comes close to', (8, 11))],\n",
       "  [('English', (1, 2)),\n",
       "   ('slower ensemble', (12, 14)),\n",
       "   ('simulate', (18, 19))]],\n",
       " [[('achieves', (4, 5))],\n",
       "  [('Chinese', (1, 2)),\n",
       "   ('best published scores', (6, 9)),\n",
       "   ('German', (11, 12)),\n",
       "   ('best published UAS scores', (13, 17)),\n",
       "   ('LAS', (28, 29))]],\n",
       " [[], [('biomedical event extraction', (7, 10))]],\n",
       " [[], []],\n",
       " [[], [('dependency parsing', (18, 20))]],\n",
       " [[('study', (3, 4)),\n",
       "   ('of', (6, 7)),\n",
       "   ('on', (9, 10)),\n",
       "   ('following', (14, 15)),\n",
       "   ('for', (28, 29))],\n",
       "  [('impact', (5, 6)),\n",
       "   ('parser choice', (7, 9)),\n",
       "   ('biomedical event extraction', (10, 13)),\n",
       "   ('structure', (16, 17)),\n",
       "   ('biomedical event extraction', (29, 32))]],\n",
       " [[('For', (0, 1)), ('utilizes', (20, 21)), ('employ', (26, 27))],\n",
       "  [('three BiLSTM - CRF - based models', (2, 9)),\n",
       "   ('Stanford - NNdep', (10, 13)),\n",
       "   ('jPTDP', (14, 15)),\n",
       "   ('Stanford - Biaffine', (16, 19)),\n",
       "   ('pre-trained word embeddings', (21, 24)),\n",
       "   ('200 dimensional pre-trained word vectors', (27, 32))]],\n",
       " [[('For', (0, 1)), ('use', (18, 19)), ('with', (24, 25))],\n",
       "  [('MarMoT', (7, 8)),\n",
       "   ('original pure Java implementations', (20, 24)),\n",
       "   ('default hyperparameter settings', (25, 28))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('use', (10, 11)),\n",
       "   ('for', (22, 23)),\n",
       "   ('use', (26, 27)),\n",
       "   ('run for', (29, 31))],\n",
       "  [('BiLSTM - CRF - based models', (2, 8)),\n",
       "   ('default', (11, 12)),\n",
       "   ('training', (23, 24)),\n",
       "   ('Nadam', (27, 28)),\n",
       "   ('50 epochs', (31, 33))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('select', (6, 7)),\n",
       "   ('from', (10, 11)),\n",
       "   ('from', (23, 24)),\n",
       "   ('fix', (40, 41)),\n",
       "   ('with', (43, 44))],\n",
       "  [('Stanford - NNdep', (1, 4)),\n",
       "   ('word CutOff', (8, 10)),\n",
       "   ('{ 1 , 2 }', (11, 16)),\n",
       "   ('size of the', (18, 21)),\n",
       "   ('hidden layer', (21, 23)),\n",
       "   ('{ 100 , 150 , 200 , 250 , 300 , 350 , 400 }', (24, 39)),\n",
       "   ('other hyperparameters', (41, 43)),\n",
       "   ('default values', (45, 47))]],\n",
       " [[('For', (0, 1)), ('use', (4, 5)), ('fix', (11, 12)), ('at', (16, 17))],\n",
       "  [('jPTDP', (1, 2)),\n",
       "   ('50 - dimensional character embeddings', (5, 10)),\n",
       "   ('initial learning rate', (13, 16)),\n",
       "   ('0.0005', (17, 18))]],\n",
       " [[('fix', (2, 3)),\n",
       "   ('at', (8, 9)),\n",
       "   ('select', (11, 12)),\n",
       "   ('in', (17, 18)),\n",
       "   ('from', (20, 21))],\n",
       "  [('number of BiLSTM layers', (4, 8)),\n",
       "   ('2', (9, 10)),\n",
       "   ('number of LSTM units', (13, 17)),\n",
       "   ('each layer', (18, 20)),\n",
       "   ('{ 100 , 150 , 200 , 250 , 300 }', (21, 32))]],\n",
       " [[], [('POS tagging results', (0, 3))]],\n",
       " [[('obtain', (7, 8)), ('on', (11, 12))],\n",
       "  [('BiLSTM - CRF and Mar - MoT', (0, 7)),\n",
       "   ('lowest scores', (9, 11)),\n",
       "   ('GENIA and CRAFT', (12, 15))]],\n",
       " [[('obtains', (1, 2)), ('to', (5, 6)), ('on', (9, 10)), ('on', (18, 19))],\n",
       "  [('jPTDP', (0, 1)),\n",
       "   ('similar score', (3, 5)),\n",
       "   ('Mar - MoT', (6, 9)),\n",
       "   ('GENIA', (10, 11)),\n",
       "   ('CRAFT', (19, 20))]],\n",
       " [[('obtains', (4, 5)), ('at', (7, 8)), ('on', (13, 14))],\n",
       "  [('MarMoT', (3, 4)),\n",
       "   ('accuracy results', (5, 7)),\n",
       "   ('98.61 % and 97.07 %', (8, 13)),\n",
       "   ('GENIA and CRAFT', (14, 17))]],\n",
       " [[('obtains', (3, 4)), ('of', (5, 6)), ('on', (8, 9)), ('on', (15, 16))],\n",
       "  [('BiLSTM - CRF', (0, 3)),\n",
       "   ('accuracies', (4, 5)),\n",
       "   ('98.44 %', (6, 8)),\n",
       "   ('GE - NIA', (9, 12)),\n",
       "   ('97.25 %', (13, 15)),\n",
       "   ('CRAFT', (16, 17))]],\n",
       " [[('Note', (0, 1)),\n",
       "   ('for', (2, 3)),\n",
       "   ('provided', (14, 15)),\n",
       "   ('to', (19, 20))],\n",
       "  [('PTB', (3, 4)),\n",
       "   ('CNN - based character - level word embeddings', (5, 13)),\n",
       "   ('0.1 % improvement', (16, 19)),\n",
       "   ('BiLSTM - CRF', (20, 23))]],\n",
       " [[('On', (0, 1)), ('with', (9, 10)), ('obtains', (15, 16))],\n",
       "  [('GENIA and CRAFT', (2, 5)),\n",
       "   ('BiLSTM - CRF', (6, 9)),\n",
       "   ('character - level word embeddings', (10, 15)),\n",
       "   ('highest accuracy scores', (17, 20))]],\n",
       " [[], [('Overall dependency parsing results', (0, 4))]],\n",
       " [[('On', (0, 1)), ('among', (3, 4)), ('obtains', (8, 9))],\n",
       "  [('GENIA', (1, 2)),\n",
       "   ('pre-trained models', (4, 6)),\n",
       "   ('BLLIP', (7, 8)),\n",
       "   ('highest results', (9, 11))]],\n",
       " [[('produces', (9, 10)), ('than', (12, 13)), ('on', (19, 20))],\n",
       "  [('pre-trained Stanford - Biaffine ( v1 ) model', (1, 9)),\n",
       "   ('lower scores', (10, 12)),\n",
       "   ('pre-trained Stanford - NNdep model', (14, 19)),\n",
       "   ('GENIA', (20, 21))]],\n",
       " [[('Note', (0, 1)),\n",
       "   ('result in', (8, 10)),\n",
       "   ('irrespective of', (14, 16)),\n",
       "   ('of', (18, 19)),\n",
       "   ('i.e.', (22, 23))],\n",
       "  [('pre-trained NNdep and Biaffine models', (3, 8)),\n",
       "   ('no significant performance differences', (10, 14)),\n",
       "   ('source', (17, 18)),\n",
       "   ('POS tags', (19, 21))]],\n",
       " [[], [('Dependency Parsing', (5, 7))]],\n",
       " [[('propose', (5, 6)), ('for', (11, 12))],\n",
       "  [('novel neural network architecture', (7, 11)),\n",
       "   ('dependency parsing', (12, 14)),\n",
       "   ('stackpointer networks ( STACKPTR )', (15, 20))]],\n",
       " [[('is', (1, 2)),\n",
       "   ('with', (8, 9)),\n",
       "   ('maintains', (16, 17)),\n",
       "   ('of', (20, 21))],\n",
       "  [('STACKPTR', (0, 1)),\n",
       "   ('transition - based architecture', (3, 7)),\n",
       "   ('corresponding asymptotic efficiency', (10, 13)),\n",
       "   ('global view', (18, 20)),\n",
       "   ('sentence', (22, 23))]],\n",
       " [[('as', (7, 8)),\n",
       "   ('equipped with', (13, 15)),\n",
       "   ('to maintain', (18, 20)),\n",
       "   ('in', (25, 26))],\n",
       "  [('STACKPTR parser', (1, 3)),\n",
       "   ('pointer network', (5, 7)),\n",
       "   ('backbone', (9, 10)),\n",
       "   ('internal stack', (16, 18)),\n",
       "   ('order of head words', (21, 25)),\n",
       "   ('tree structures', (26, 28))]],\n",
       " [[('performs', (3, 4)),\n",
       "   ('in', (5, 6)),\n",
       "   ('generates', (21, 22)),\n",
       "   ('by assigning', (24, 26)),\n",
       "   ('at the top of', (31, 35))],\n",
       "  [('STACKPTR parser', (1, 3)),\n",
       "   ('parsing', (4, 5)),\n",
       "   ('incremental , topdown , depth - first fashion', (7, 15)),\n",
       "   ('child', (27, 28)),\n",
       "   ('headword', (30, 31)),\n",
       "   ('internal stack', (36, 38))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('of', (3, 4)),\n",
       "   ('re-implemented', (9, 10)),\n",
       "   ('achieved', (22, 23)),\n",
       "   ('on', (31, 32))],\n",
       "  [('parsing performance', (5, 7)),\n",
       "   ('graph - based Deep Biaffine ( BIAF ) parser', (11, 20)),\n",
       "   ('state - of - the - art results', (23, 31)),\n",
       "   ('wide range of languages', (33, 37))]],\n",
       " [[], [('Main Results', (0, 2))]],\n",
       " [[('On', (0, 1)),\n",
       "   ('with', (10, 11)),\n",
       "   ('outperforms', (15, 16)),\n",
       "   ('on', (17, 18)),\n",
       "   ('obtains', (21, 22))],\n",
       "  [('UAS and LAS', (1, 4)),\n",
       "   ('Full variation of STACKPTR', (6, 10)),\n",
       "   ('decoding beam size 10', (11, 15)),\n",
       "   ('BIAF', (16, 17)),\n",
       "   ('Chinese', (18, 19)),\n",
       "   ('competitive performance', (22, 24))]],\n",
       " [[('achieves', (8, 9)),\n",
       "   ('on', (12, 13)),\n",
       "   ('performs', (18, 19)),\n",
       "   ('than', (21, 22)),\n",
       "   ('on', (24, 25))],\n",
       "  [('Full model', (6, 8)),\n",
       "   ('best accuracy', (10, 12)),\n",
       "   ('English and Chinese', (13, 16)),\n",
       "   ('slightly worse', (19, 21)),\n",
       "   ('+ sib', (22, 24)),\n",
       "   ('German', (25, 26))]],\n",
       " [[('On', (0, 1)), ('on', (9, 10)), ('showing', (13, 14)), ('on', (19, 20))],\n",
       "  [('LCM and UCM', (1, 4)),\n",
       "   ('STACKPTR', (5, 6)),\n",
       "   ('significantly outperforms', (6, 8)),\n",
       "   ('BIAF', (8, 9)),\n",
       "   ('all languages', (10, 12)),\n",
       "   ('superiority', (15, 16)),\n",
       "   ('complete sentence parsing', (20, 23))]],\n",
       " [[('of', (2, 3)), ('on', (5, 6)), ('slightly worse than', (8, 11))],\n",
       "  [('results', (1, 2)),\n",
       "   ('our parser', (3, 5)),\n",
       "   ('RA', (6, 7)),\n",
       "   ('BIAF', (11, 12))]],\n",
       " [[('of', (1, 2)), ('obtains', (3, 4)), ('than', (6, 7))],\n",
       "  [('re-implementation', (0, 1)),\n",
       "   ('BIAF', (2, 3)),\n",
       "   ('better performance', (4, 6)),\n",
       "   ('original one', (8, 10))]],\n",
       " [[('achieves', (2, 3)), ('on', (11, 12)), ('on', (16, 17))],\n",
       "  [('Our model', (0, 2)),\n",
       "   ('state - of - the - art performance', (3, 11)),\n",
       "   ('UAS and LAS', (13, 16)),\n",
       "   ('Chinese', (17, 18)),\n",
       "   ('best UAS', (20, 22))]],\n",
       " [[('On', (0, 1)), ('with', (7, 8)), ('than', (13, 14))],\n",
       "  [('German', (1, 2)),\n",
       "   ('performance', (4, 5)),\n",
       "   ('competitive', (6, 7)),\n",
       "   ('BIAF', (8, 9)),\n",
       "   ('significantly better', (11, 13)),\n",
       "   ('other models', (14, 16))]],\n",
       " [[], [('Neural Network Transition - Based Parsing', (3, 9))]],\n",
       " [[], [('neural network transition - based dependency parsing', (6, 13))]],\n",
       " [[], [('dependency parsing', (2, 4))]],\n",
       " [[('In', (0, 1)),\n",
       "   ('processed in', (8, 10)),\n",
       "   ('choose', (25, 26)),\n",
       "   ('defined by', (32, 34))],\n",
       "  [('transition - based parsing', (1, 5)),\n",
       "   ('sentences', (6, 7)),\n",
       "   ('linear left to right pass', (11, 16))]],\n",
       " [[('combine', (5, 6)),\n",
       "   ('of', (9, 10)),\n",
       "   ('with', (12, 13)),\n",
       "   ('enabled by', (16, 18)),\n",
       "   ('making', (23, 24))],\n",
       "  [('representational power', (7, 9)),\n",
       "   ('neural networks', (10, 12)),\n",
       "   ('superior search', (14, 16)),\n",
       "   ('structured training and inference', (18, 22)),\n",
       "   ('our parser', (24, 26))]],\n",
       " [[('train', (6, 7)), ('to model', (10, 12)), ('of', (14, 15))],\n",
       "  [('neural network', (8, 10)),\n",
       "   ('probability', (13, 14)),\n",
       "   ('individual parse actions', (15, 18))]],\n",
       " [[('use', (3, 4)),\n",
       "   ('from', (6, 7)),\n",
       "   ('of', (9, 10)),\n",
       "   ('as', (13, 14)),\n",
       "   ('in', (16, 17)),\n",
       "   ('trained with', (23, 25))],\n",
       "  [('activations', (5, 6)),\n",
       "   ('all layers', (7, 9)),\n",
       "   ('neural network', (11, 13)),\n",
       "   ('representation', (15, 16)),\n",
       "   ('structured perceptron model', (18, 21)),\n",
       "   ('beam search and early updates', (25, 30))]],\n",
       " [[('generate', (5, 6)),\n",
       "   ('of', (8, 9)),\n",
       "   ('by parsing', (14, 16)),\n",
       "   ('with', (18, 19)),\n",
       "   ('selecting', (23, 24)),\n",
       "   ('produced', (32, 33))],\n",
       "  [('large quantities', (6, 8)),\n",
       "   ('high - confidence parse trees', (9, 14)),\n",
       "   ('unlabeled data', (16, 18)),\n",
       "   ('two different parsers', (19, 22)),\n",
       "   ('sentences', (26, 27)),\n",
       "   ('two parsers', (30, 32)),\n",
       "   ('same trees', (34, 36))]],\n",
       " [[('known as', (3, 5)),\n",
       "   ('show', (10, 11)),\n",
       "   ('benefits', (13, 14)),\n",
       "   ('than', (20, 21))],\n",
       "  [('tri-training', (6, 7)),\n",
       "   ('our neural network parser', (14, 18)),\n",
       "   ('significantly more', (18, 20)),\n",
       "   ('other approaches', (21, 23))]],\n",
       " [[('used', (1, 2)), ('to learn', (8, 10)), ('following', (12, 13))],\n",
       "  [('publicly available word2vec 2 tool', (3, 8)),\n",
       "   ('CBOW embeddings', (10, 12)),\n",
       "   ('sample configuration', (14, 16))]],\n",
       " [[], []],\n",
       " [[('with', (6, 7)), ('of', (10, 11))],\n",
       "  [('reported accuracy', (8, 10)), ('94.22 % UAS', (11, 14))]],\n",
       " [[('suggests', (16, 17)),\n",
       "   ('is', (20, 21)),\n",
       "   ('with', (22, 23)),\n",
       "   ('for', (29, 30)),\n",
       "   ('on', (31, 32))],\n",
       "  [('UAS', (3, 4)),\n",
       "   ('our model', (18, 20)),\n",
       "   ('competitive', (21, 22)),\n",
       "   ('some of the highest reported accuries', (23, 29)),\n",
       "   ('dependencies', (30, 31)),\n",
       "   ('WSJ', (32, 33))]],\n",
       " [[], [('NEURAL DEPENDENCY PARSING', (4, 7))]],\n",
       " [[('modify', (1, 2)),\n",
       "   ('to achieve', (13, 15)),\n",
       "   ('build', (19, 20)),\n",
       "   ('uses', (26, 27)),\n",
       "   ('replace', (31, 32)),\n",
       "   ('affine', (40, 41)),\n",
       "   ('with', (43, 44)),\n",
       "   ('that reduce', (70, 72))],\n",
       "  [('neural graphbased approach', (3, 6)),\n",
       "   ('few', (11, 12)),\n",
       "   ('competitive performance', (15, 17)),\n",
       "   ('network', (21, 22)),\n",
       "   ('larger', (24, 25)),\n",
       "   ('more regularization', (27, 29)),\n",
       "   ('traditional MLP - based attention mechanism', (33, 39)),\n",
       "   ('label classifier', (41, 43)),\n",
       "   ('biaffine ones', (44, 46)),\n",
       "   ('MLP operations', (68, 70)),\n",
       "   ('dimensionality', (73, 74))]],\n",
       " [[('keeps', (13, 14)),\n",
       "   ('of', (17, 18)),\n",
       "   ('of', (22, 23)),\n",
       "   ('for', (25, 26)),\n",
       "   ('throughout', (28, 29)),\n",
       "   ('divides', (31, 32)),\n",
       "   ('for', (34, 35)),\n",
       "   ('by', (37, 38)),\n",
       "   ('ensuring', (42, 43)),\n",
       "   ('of', (46, 47)),\n",
       "   ('on average be', (50, 53))],\n",
       "  [('optimize', (3, 4)),\n",
       "   ('Adam', (5, 6)),\n",
       "   ('moving average', (15, 17)),\n",
       "   ('L 2 norm', (19, 22)),\n",
       "   ('gradient', (24, 25)),\n",
       "   ('each parameter', (26, 28)),\n",
       "   ('training', (29, 30)),\n",
       "   ('gradient', (33, 34)),\n",
       "   ('each parameter', (35, 37)),\n",
       "   ('moving average', (39, 41)),\n",
       "   ('magnitude', (45, 46)),\n",
       "   ('gradients', (48, 49))]],\n",
       " [[('gets', (2, 3)),\n",
       "   ('on', (8, 9)),\n",
       "   ('as', (13, 14)),\n",
       "   ('gets', (28, 29)),\n",
       "   ('on', (32, 33)),\n",
       "   ('as well', (36, 38)),\n",
       "   ('on', (41, 42))],\n",
       "  [('Our model', (0, 2)),\n",
       "   ('nearly the same UAS performance', (3, 8)),\n",
       "   ('PTB - SD 3.3.0', (9, 13)),\n",
       "   ('current SOTA model', (15, 18)),\n",
       "   ('SOTA UAS performance', (29, 32)),\n",
       "   ('CTB 5.1', (33, 35)),\n",
       "   ('SOTA performance', (39, 41)),\n",
       "   ('all CoNLL 09 languages', (42, 46))]],\n",
       " [[('accounts for', (6, 8)), ('at', (10, 11)), ('improves', (14, 15))],\n",
       "  [('model predictions', (8, 10)),\n",
       "   ('training time', (11, 13)),\n",
       "   ('parsing accuracies', (15, 17))]],\n",
       " [[('formulated', (5, 6)),\n",
       "   ('that read', (11, 13)),\n",
       "   ('in', (14, 15)),\n",
       "   ('to form', (20, 22)),\n",
       "   ('coupled with', (36, 38))],\n",
       "  [('Natural language parsing', (0, 3)),\n",
       "   ('series of decisions', (8, 11)),\n",
       "   ('words', (13, 14)),\n",
       "   ('sequence', (15, 16)),\n",
       "   ('incrementally combine', (17, 19)),\n",
       "   ('syntactic structures', (22, 24)),\n",
       "   ('transitionbased parsing', (30, 32)),\n",
       "   ('greedy search procedure', (39, 42))]],\n",
       " [[('share in', (13, 15)),\n",
       "   ('takes into account', (20, 23)),\n",
       "   ('of', (24, 25)),\n",
       "   ('predicts', (31, 32)),\n",
       "   ('conditioned on', (37, 39))],\n",
       "  [('transition - based parsing', (3, 7)),\n",
       "   ('classification component', (17, 19)),\n",
       "   ('features', (23, 24)),\n",
       "   ('current parser state', (26, 29)),\n",
       "   ('next action', (33, 35))]],\n",
       " [[('adapt', (5, 6)), ('drawn', (15, 16))],\n",
       "  [('training criterion', (7, 9)),\n",
       "   ('parser states', (13, 15)),\n",
       "   ('training data', (20, 22)),\n",
       "   ('model', (27, 28)),\n",
       "   ('learned', (32, 33))]],\n",
       " [[('use', (5, 6)), ('given', (23, 24))],\n",
       "  [('method', (7, 8)),\n",
       "   ('dynamically', (10, 11)),\n",
       "   ('optimal ( relative to the final attachment accuracy ) action', (13, 23)),\n",
       "   ('imperfect history', (25, 27))]],\n",
       " [[('interpolating', (1, 2)),\n",
       "   ('sampled from', (5, 7)),\n",
       "   ('sampled from', (11, 13)),\n",
       "   ('at', (20, 21))],\n",
       "  [('algorithm states', (3, 5)),\n",
       "   ('model', (8, 9)),\n",
       "   ('training data', (14, 16)),\n",
       "   ('more robust predictions', (17, 20)),\n",
       "   ('test time', (21, 23))]],\n",
       " [[], []],\n",
       " [[('achieved by', (2, 4)), ('for', (7, 8)), ('is', (9, 10))],\n",
       "  [('score', (1, 2)),\n",
       "   ('dynamic oracle', (5, 7)),\n",
       "   ('English', (8, 9)),\n",
       "   ('93.56 UAS', (10, 12))]],\n",
       " [[('establishes', (5, 6))],\n",
       "  [('Chinese score', (3, 5)), ('state - of - the - art', (7, 14))]],\n",
       " [[], [('Globally Normalized Transition - Based Neural Networks', (0, 7))]],\n",
       " [[('introduce', (1, 2)), ('achieves', (12, 13))],\n",
       "  [('globally normalized transition - based neural network model', (3, 11)),\n",
       "   ('state - of - the - art', (13, 20)),\n",
       "   ('part - ofspeech tagging', (20, 24)),\n",
       "   ('dependency parsing', (25, 27)),\n",
       "   ('sentence compression results', (28, 31))]],\n",
       " [[('demonstrate', (4, 5)),\n",
       "   ('without', (11, 12)),\n",
       "   ('can achieve', (14, 16)),\n",
       "   ('than', (20, 21)),\n",
       "   ('as', (23, 24)),\n",
       "   ('long as', (24, 26)),\n",
       "   ('are', (27, 28))],\n",
       "  [('simple feed - forward networks', (6, 11)),\n",
       "   ('any recurrence', (12, 14)),\n",
       "   ('comparable or better accuracies', (16, 20)),\n",
       "   ('LSTMs', (21, 22)),\n",
       "   ('globally normalized', (28, 30))]],\n",
       " [[('uses', (10, 11)), ('On leave', (21, 23))],\n",
       "  [('transition system and feature embeddings', (12, 17)),\n",
       "   ('Columbia University', (24, 26))]],\n",
       " [[('use', (3, 4)),\n",
       "   ('perform', (8, 9)),\n",
       "   ('for', (11, 12)),\n",
       "   ('introduce', (16, 17)),\n",
       "   ('with', (19, 20)),\n",
       "   ('to overcome', (28, 30))],\n",
       "  [('any recurrence', (4, 6)),\n",
       "   ('beam search', (9, 11)),\n",
       "   ('maintaining', (12, 13)),\n",
       "   ('multiple hypotheses', (13, 15)),\n",
       "   ('global normalization', (17, 19)),\n",
       "   ('conditional random field ( CRF ) objective', (21, 28)),\n",
       "   ('label bias problem', (31, 34)),\n",
       "   ('locally normalized models', (35, 38))]],\n",
       " [[('use', (2, 3)),\n",
       "   ('approximate', (7, 8)),\n",
       "   ('by', (11, 12)),\n",
       "   ('in', (16, 17)),\n",
       "   ('use', (21, 22))],\n",
       "  [('beam inference', (3, 5)),\n",
       "   ('partition function', (9, 11)),\n",
       "   ('summing', (12, 13)),\n",
       "   ('elements', (15, 16)),\n",
       "   ('beam', (18, 19)),\n",
       "   ('early updates', (22, 24))]],\n",
       " [[('compute', (1, 2)),\n",
       "   ('based on', (3, 5)),\n",
       "   ('perform', (10, 11)),\n",
       "   ('of', (14, 15)),\n",
       "   ('based on', (19, 21))],\n",
       "  [('gradients', (2, 3)),\n",
       "   ('approximate global normalization', (6, 9)),\n",
       "   ('full backpropagation training', (11, 14)),\n",
       "   ('all neural network parameters', (15, 19)),\n",
       "   ('CRF loss', (22, 24))]],\n",
       " [[], []],\n",
       " [[], [('Part of Speech Tagging', (0, 4))]],\n",
       " [[], []],\n",
       " [[],\n",
       "  [('Our globally normalized model', (0, 4)),\n",
       "   ('significantly outperforms', (5, 7)),\n",
       "   ('local model', (8, 10))]],\n",
       " [[('suffers from', (7, 9))],\n",
       "  [('Beam search with a locally normalized model', (0, 7)),\n",
       "   ('severe label bias issues', (9, 13))]],\n",
       " [[('Using', (0, 1)),\n",
       "   ('with', (3, 4)),\n",
       "   ('does', (8, 9)),\n",
       "   ('with', (13, 14)),\n",
       "   ('leads to', (17, 19)),\n",
       "   ('in', (23, 24))],\n",
       "  [('beam search', (1, 3)),\n",
       "   ('locally normalized model', (5, 8)),\n",
       "   ('help', (10, 11)),\n",
       "   ('global normalization', (14, 16)),\n",
       "   ('7 % reduction', (20, 23)),\n",
       "   ('relative error', (24, 26))]],\n",
       " [[('is', (6, 7)),\n",
       "   ('increasing', (10, 11)),\n",
       "   ('on', (13, 14)),\n",
       "   ('by', (18, 19))],\n",
       "  [('set of character ngrams feature', (1, 6)),\n",
       "   ('very important', (7, 9)),\n",
       "   ('average accuracy', (11, 13)),\n",
       "   (\"CoNLL '09 datasets\", (15, 18)),\n",
       "   ('about 0.5 % absolute', (19, 23))]],\n",
       " [[], [('Dependency Parsing', (0, 2))]],\n",
       " [[], []],\n",
       " [[('compares', (10, 11)), ('with', (23, 24))],\n",
       "  [('favorably', (11, 12)), ('94.26 % LAS and 92.41 % UAS', (14, 21))]],\n",
       " [[],\n",
       "  [('significantly outperform', (3, 5)),\n",
       "   ('LSTM - based approaches', (6, 10))]],\n",
       " [[], [('Efficient Text Classification', (4, 7))]],\n",
       " [[('explore', (5, 6)),\n",
       "   ('to', (11, 12)),\n",
       "   ('with', (15, 16)),\n",
       "   ('in the', (21, 23))],\n",
       "  [('scale', (8, 9)),\n",
       "   ('baselines', (10, 11)),\n",
       "   ('very large corpus', (12, 15)),\n",
       "   ('large output space', (17, 20)),\n",
       "   ('text classification', (25, 27))]],\n",
       " [[('show', (12, 13)),\n",
       "   ('with', (16, 17)),\n",
       "   ('train on', (26, 28)),\n",
       "   ('on par with', (38, 41))],\n",
       "  [('linear models', (14, 16)),\n",
       "   ('rank constraint', (18, 20)),\n",
       "   ('fast loss approximation', (22, 25)),\n",
       "   ('billion words', (29, 31)),\n",
       "   ('ten minutes', (32, 34)),\n",
       "   ('performance', (37, 38)),\n",
       "   ('state - of - the - art', (42, 49))]],\n",
       " [[], [('Sentiment analysis', (0, 2))]],\n",
       " [[('use', (1, 2)),\n",
       "   ('run', (6, 7)),\n",
       "   ('for', (8, 9)),\n",
       "   ('with', (11, 12)),\n",
       "   ('selected on', (15, 17)),\n",
       "   ('from', (20, 21))],\n",
       "  [('10 hidden units', (2, 5)),\n",
       "   ('fastText', (7, 8)),\n",
       "   ('5 epochs', (9, 11)),\n",
       "   ('learning rate', (13, 15)),\n",
       "   ('validation set', (18, 20)),\n",
       "   ('{ 0.05 , 0.1 , 0.25 , 0.5 }', (21, 30))]],\n",
       " [[('adding', (4, 5)), ('improves', (7, 8)), ('by', (10, 11))],\n",
       "  [('bigram information', (5, 7)),\n",
       "   ('performance', (9, 10)),\n",
       "   ('1 - 4 %', (11, 15))]],\n",
       " [[('slightly better than', (4, 7)),\n",
       "   ('a', (16, 17)),\n",
       "   ('bit worse than', (17, 20))],\n",
       "  [('our accuracy', (1, 3)),\n",
       "   ('char - CNN and char - CRNN', (7, 14)),\n",
       "   ('VDCNN', (20, 21))]],\n",
       " [[('increase', (4, 5)),\n",
       "   ('by using', (8, 10)),\n",
       "   ('for example with', (13, 16)),\n",
       "   ('performance on', (19, 21)),\n",
       "   ('goes up to', (22, 25))],\n",
       "  [('accuracy slightly', (6, 8)),\n",
       "   ('more n-grams', (10, 12)),\n",
       "   ('trigrams', (16, 17)),\n",
       "   ('Sogou', (21, 22)),\n",
       "   ('97.1 %', (25, 27))]],\n",
       " [[('tune', (1, 2)),\n",
       "   ('on', (4, 5)),\n",
       "   ('using', (11, 12)),\n",
       "   ('up', (13, 14)),\n",
       "   ('leads to', (16, 18))],\n",
       "  [('validation set', (6, 8)),\n",
       "   ('n-grams', (12, 13)),\n",
       "   ('best performance', (19, 21))]],\n",
       " [[], [('Tag prediction', (0, 2))]],\n",
       " [[('consider', (1, 2)), ('predicts', (8, 9))],\n",
       "  [('frequency - based baseline', (3, 7)), ('most frequent tag', (10, 13))]],\n",
       " [[('compare with', (2, 4)), ('based on', (24, 26))],\n",
       "  [('Tagspace ( Weston et al. , 2014 )', (4, 12)),\n",
       "   ('tag', (16, 17)),\n",
       "   ('Wsabie model', (27, 29))]],\n",
       " [[], [('Results', (0, 1)), ('training time', (2, 4))]],\n",
       " [[('achieve', (2, 3)),\n",
       "   ('with', (6, 7)),\n",
       "   ('adding', (13, 14)),\n",
       "   ('gives', (15, 16)),\n",
       "   ('in', (20, 21))],\n",
       "  [('Both', (0, 1)),\n",
       "   ('similar performance', (4, 6)),\n",
       "   ('small hidden layer', (8, 11)),\n",
       "   ('bigrams', (14, 15)),\n",
       "   ('significant boost', (18, 20)),\n",
       "   ('accuracy', (21, 22))]],\n",
       " [[], [('CROSS - LINGUAL DOCUMENT CLASSIFICATION', (5, 10))]],\n",
       " [[], [('cross - lingual understanding ( XLU )', (3, 10))]],\n",
       " [[('combine', (1, 2)),\n",
       "   ('with', (13, 14)),\n",
       "   ('for', (17, 18)),\n",
       "   ('such as', (21, 23)),\n",
       "   ('to simultaneously close', (29, 32)),\n",
       "   ('in', (40, 41))],\n",
       "  [('state - of - the - art cross - lingual methods', (2, 13)),\n",
       "   ('weakly supervised learning', (18, 21)),\n",
       "   ('unsupervised pre-training', (23, 25)),\n",
       "   ('unsupervised data augmentation', (26, 29)),\n",
       "   ('language gap', (34, 36)),\n",
       "   ('XLU', (41, 42))]],\n",
       " [[('focus on', (4, 6)), ('for', (8, 9))],\n",
       "  [('two approaches', (6, 8)), ('domain adaptation', (9, 11))]],\n",
       " [[('based on', (4, 6)), ('using', (17, 18))],\n",
       "  [('first method', (1, 3)),\n",
       "   ('masked language model ( MLM ) pre-training', (6, 13)),\n",
       "   ('unlabeled target language corpora', (18, 22))]],\n",
       " [[('generated from', (16, 18)), ('trained on', (26, 28))],\n",
       "  [('unsupervised data augmentation ( UDA ) )', (4, 11)),\n",
       "   ('synthetic paraphrases', (13, 15)),\n",
       "   ('unlabeled corpus', (19, 21)),\n",
       "   ('label consistency loss', (29, 32))]],\n",
       " [[('with', (11, 12))],\n",
       "  [('Fine-tune ( Ft )', (0, 4)),\n",
       "   ('Fine - tuning', (5, 8)),\n",
       "   ('pre-trained model', (9, 11)),\n",
       "   ('source - domain training set', (13, 18))]],\n",
       " [[('with', (3, 4))], [('Fine - tune', (0, 3)), ('UDA ( UDA )', (4, 8))]],\n",
       " [[('utilizes', (2, 3)), ('from', (6, 7)), ('by optimizing', (10, 12))],\n",
       "  [('unlabeled data', (4, 6)),\n",
       "   ('target domain', (8, 10)),\n",
       "   ('UDA loss function', (13, 16))]],\n",
       " [[('based on', (3, 5))],\n",
       "  [('Self - training', (0, 3)), ('UDA model ( UDA + Self )', (6, 13))]],\n",
       " [[('train', (2, 3)), ('choose', (11, 12)), ('as', (15, 16))],\n",
       "  [('Ft model and UDA model', (4, 9)),\n",
       "   ('better one', (13, 15)),\n",
       "   ('teacher model', (17, 19))]],\n",
       " [[('to train', (5, 7)), ('using', (11, 12)), ('in', (17, 18))],\n",
       "  [('teacher model', (1, 3)),\n",
       "   ('new XLM student', (8, 11)),\n",
       "   ('only unlabeled data U tgt', (12, 17)),\n",
       "   ('target domain', (19, 21))]],\n",
       " [[('Looking at', (0, 2)),\n",
       "   ('without the help of', (12, 16)),\n",
       "   ('from', (18, 19)),\n",
       "   ('between', (29, 30)),\n",
       "   ('of', (33, 34)),\n",
       "   ('when using', (44, 46))],\n",
       "  [('Ft ( XLM ) results', (2, 7)),\n",
       "   ('unlabeled data', (16, 18)),\n",
       "   ('target domain', (20, 22)),\n",
       "   ('substantial gap', (27, 29)),\n",
       "   ('model performance', (31, 33)),\n",
       "   ('cross -lingual settings', (35, 38)),\n",
       "   ('monolingual baselines', (40, 42)),\n",
       "   ('state - of - the - art pre-trained cross -lingual representations',\n",
       "    (46, 57))]],\n",
       " [[('offer', (8, 9)), ('by utilizing', (11, 13))],\n",
       "  [('UDA algorithm and MLM pre-training', (2, 7)),\n",
       "   ('significant improvements', (9, 11)),\n",
       "   ('unlabeled data', (14, 16))]],\n",
       " [[('In', (0, 1)),\n",
       "   ('where', (6, 7)),\n",
       "   ('is', (11, 12)),\n",
       "   ('consistently provides', (23, 25)),\n",
       "   ('compared with', (27, 29))],\n",
       "  [('sentiment classification task', (2, 5)),\n",
       "   ('unlabeled data size', (8, 11)),\n",
       "   ('larger', (12, 13)),\n",
       "   ('Ft ( XLM ft ) model usnig MLM pre-training', (14, 23)),\n",
       "   ('larger improvements', (25, 27)),\n",
       "   ('UDA method', (30, 32))]],\n",
       " [[('takes longer', (14, 16)), ('to', (16, 17))],\n",
       "  [('MLM method', (6, 8)),\n",
       "   ('relatively more resource intensive', (9, 13)),\n",
       "   ('converge', (17, 18))]],\n",
       " [[('in', (3, 4)), ('when', (8, 9)), ('is', (15, 16)), ('is', (21, 22))],\n",
       "  [('MLdoc dataset', (5, 7)),\n",
       "   ('size', (10, 11)),\n",
       "   ('unlabeled samples', (13, 15)),\n",
       "   ('limited', (16, 17)),\n",
       "   ('UDA method', (19, 21)),\n",
       "   ('more helpful', (22, 24))]],\n",
       " [[('In', (0, 1)), ('observe', (7, 8)), ('over', (15, 16))],\n",
       "  [('sentiment classification task', (2, 5)),\n",
       "   ('self - training technique', (9, 13)),\n",
       "   ('consistently improves', (13, 15)),\n",
       "   ('teacher model', (17, 19))]],\n",
       " [[('offers', (1, 2)), ('in', (4, 5))],\n",
       "  [('best results', (2, 4)),\n",
       "   ('both XLM and XLM ft based classifiers', (5, 12))]],\n",
       " [[('In', (0, 1)), ('achieves', (9, 10))],\n",
       "  [('MLdoc dataset', (2, 4)),\n",
       "   ('self - training', (5, 8)),\n",
       "   ('best results', (11, 13))]],\n",
       " [[('comparing with', (2, 4)),\n",
       "   ('able to', (19, 21)),\n",
       "   ('by utilizing', (26, 28)),\n",
       "   ('in', (30, 31))],\n",
       "  [('best cross - lingual results and monolingual fine - tune baseline',\n",
       "    (5, 16)),\n",
       "   ('completely close', (21, 23)),\n",
       "   ('performance gap', (24, 26)),\n",
       "   ('unlabeled data', (28, 30)),\n",
       "   ('target language', (32, 34))]],\n",
       " [[('reaches', (4, 5)), ('improving over', (15, 17)), ('by', (20, 21))],\n",
       "  [('our framework', (2, 4)),\n",
       "   ('new state - of - the - art results', (5, 14)),\n",
       "   ('vanilla XLM baselines', (17, 20)),\n",
       "   ('44 %', (21, 23))]],\n",
       " [[('Leveraging', (0, 1)),\n",
       "   ('from', (4, 5)),\n",
       "   ('does not offer', (7, 10)),\n",
       "   ('can provide', (14, 16)),\n",
       "   ('in', (18, 19))],\n",
       "  [('unlabeled data', (2, 4)),\n",
       "   ('other domains', (5, 7)),\n",
       "   ('consistent improvement', (10, 12)),\n",
       "   ('additional value', (16, 18)),\n",
       "   ('isolated cases', (19, 21))]],\n",
       " [[], [('Text Classification', (9, 11))]],\n",
       " [[], []],\n",
       " [[('proposes', (2, 3)),\n",
       "   ('addresses', (21, 22)),\n",
       "   ('by modeling', (26, 28)),\n",
       "   ('in', (30, 31)),\n",
       "   ('using', (34, 35))],\n",
       "  [('Neural Attentive Bagof - Entities ( NABoE ) model', (4, 13)),\n",
       "   ('text classification problem', (23, 26)),\n",
       "   ('semantics', (29, 30)),\n",
       "   ('target documents', (32, 34)),\n",
       "   ('entities in the', (35, 38)),\n",
       "   ('KB', (38, 39))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('in', (4, 5)),\n",
       "   ('first detects', (17, 19)),\n",
       "   ('referred to by', (22, 25)),\n",
       "   ('represents', (41, 42)),\n",
       "   ('using', (44, 45)),\n",
       "   ('of', (48, 49)),\n",
       "   ('of', (51, 52))],\n",
       "  [('each entity name', (1, 4)),\n",
       "   ('document', (6, 7)),\n",
       "   ('entities', (19, 20)),\n",
       "   ('document', (43, 44)),\n",
       "   ('weighted average', (46, 48)),\n",
       "   ('embeddings', (50, 51))]],\n",
       " [[('computed using', (3, 5)),\n",
       "   ('that enables', (10, 12)),\n",
       "   ('to focus on', (14, 17)),\n",
       "   ('of', (20, 21)),\n",
       "   ('that are', (23, 25)),\n",
       "   ('in', (27, 28))],\n",
       "  [('weights', (1, 2)),\n",
       "   ('novel neural attention mechanism', (6, 10)),\n",
       "   ('model', (13, 14)),\n",
       "   ('small subset', (18, 20)),\n",
       "   ('entities', (22, 23)),\n",
       "   ('less ambiguous', (25, 27)),\n",
       "   ('meaning', (28, 29)),\n",
       "   ('document', (34, 35))]],\n",
       " [[('designed to compute', (8, 11)), ('by jointly addressing', (12, 15))],\n",
       "  [('attention mechanism', (5, 7)),\n",
       "   ('weights', (11, 12)),\n",
       "   ('entity linking and entity salience detection tasks', (15, 22))]],\n",
       " [[('trained using', (3, 5)),\n",
       "   ('with', (7, 8)),\n",
       "   ('controlled by', (11, 13)),\n",
       "   ('set to', (18, 20))],\n",
       "  [('mini-batch SGD', (5, 7)),\n",
       "   ('learning rate', (9, 11)),\n",
       "   ('Adam', (13, 14)),\n",
       "   ('mini-batch size', (16, 18)),\n",
       "   ('32', (20, 21))]],\n",
       " [[('of', (2, 3)), ('of', (5, 6)), ('set to', (10, 12))],\n",
       "  [('size', (1, 2)),\n",
       "   ('embeddings', (4, 5)),\n",
       "   ('words and entities', (6, 9)),\n",
       "   ('d = 300', (12, 15))]],\n",
       " [[], []],\n",
       " [[], []],\n",
       " [[('based on', (3, 5)), ('with', (9, 10))],\n",
       "  [('logistic regression classifier', (6, 9)),\n",
       "   ('conventional binary BoW features', (10, 14))]],\n",
       " [[], [('FTS- BRNN', (0, 2))]],\n",
       " [[('based on', (3, 5))],\n",
       "  [('bidirectional RNN with', (6, 9)),\n",
       "   ('gated recurrent units ( GRU )', (9, 15))]],\n",
       " [[('is', (3, 4)),\n",
       "   ('uses', (14, 15)),\n",
       "   ('with', (21, 22)),\n",
       "   ('computed using', (24, 26)),\n",
       "   ('of', (28, 29)),\n",
       "   ('trained on', (32, 34)),\n",
       "   ('using', (35, 36))],\n",
       "  [('NTEE', (0, 1)),\n",
       "   ('state - of - the - art model', (5, 13)),\n",
       "   ('multi - layer perceptron classifier', (16, 21)),\n",
       "   ('features', (23, 24)),\n",
       "   ('embeddings', (27, 28)),\n",
       "   ('words and entities', (29, 32)),\n",
       "   ('Wikipedia', (34, 35)),\n",
       "   ('neural network model', (37, 40))]],\n",
       " [[], []],\n",
       " [[('yielded', (7, 8)), ('on', (12, 13))],\n",
       "  [('our models', (5, 7)),\n",
       "   ('enhanced over all performance', (8, 12)),\n",
       "   ('both datasets', (13, 15))]],\n",
       " [[('in terms of', (9, 12))],\n",
       "  [('NABoE - full model', (1, 5)),\n",
       "   ('outperformed', (5, 6)),\n",
       "   ('all baseline models', (6, 9)),\n",
       "   ('both measures', (12, 14))]],\n",
       " [[('in terms of', (10, 13)), ('on', (15, 16)), ('on', (25, 26))],\n",
       "  [('NABoE-entity model', (3, 5)),\n",
       "   ('outperformed', (5, 6)),\n",
       "   ('all the baseline models', (6, 10)),\n",
       "   ('both measures', (13, 15)),\n",
       "   ('20NG dataset', (17, 19)),\n",
       "   ('F 1 score', (22, 25)),\n",
       "   ('R8 dataset', (27, 29))]],\n",
       " [[], [('Text Classification', (6, 8))]],\n",
       " [[('propose', (5, 6)), ('denoted as', (14, 16)), ('to solve', (18, 20))],\n",
       "  [('task - oriented word embedding method', (7, 13)), ('ToWE', (16, 17))]],\n",
       " [[('inherently jointed to construct', (13, 17))],\n",
       "  [(\"words ' contextual information and task information\", (5, 12)),\n",
       "   ('word embeddings', (18, 20))]],\n",
       " [[('regularize', (7, 8)),\n",
       "   ('of', (10, 11)),\n",
       "   ('to have', (14, 16)),\n",
       "   ('adjust', (23, 24)),\n",
       "   ('of', (26, 27)),\n",
       "   ('in', (30, 31))],\n",
       "  [('task information', (3, 5)),\n",
       "   ('distribution', (9, 10)),\n",
       "   ('clear classification boundary', (17, 20)),\n",
       "   ('distribution', (25, 26)),\n",
       "   ('other words', (28, 30)),\n",
       "   ('embedding space', (32, 34))]],\n",
       " [[('employed as', (18, 20))],\n",
       "  [('BOW method', (15, 17)), ('basic baseline', (21, 23))]],\n",
       " [[('represents', (1, 2)), ('as', (4, 5)), ('is', (13, 14))],\n",
       "  [('each document', (2, 4)),\n",
       "   ('bag of words', (6, 9)),\n",
       "   ('weighting scheme', (11, 13)),\n",
       "   ('TFIDF', (14, 15))]],\n",
       " [[('is', (7, 8)),\n",
       "   ('which learns', (13, 15)),\n",
       "   ('by maximizing', (17, 19)),\n",
       "   ('leveraging', (22, 23))],\n",
       "  [('Word2 Vec method', (4, 7)),\n",
       "   ('neural network language method', (9, 13)),\n",
       "   ('word embeddings', (15, 17)),\n",
       "   ('conditional probability', (20, 22)),\n",
       "   ('contextual information', (23, 25))]],\n",
       " [[('performs', (5, 6)),\n",
       "   ('than', (7, 8)),\n",
       "   ('proved to', (14, 16)),\n",
       "   ('for', (19, 20))],\n",
       "  [('Our method', (3, 5)),\n",
       "   ('better', (6, 7)),\n",
       "   ('other methods', (9, 11)),\n",
       "   ('highly reliable', (17, 19)),\n",
       "   ('text classification task', (21, 24))]],\n",
       " [[('on', (13, 14))],\n",
       "  [('ToWE - SG method', (4, 8)),\n",
       "   ('significantly outperforms', (8, 10)),\n",
       "   ('other baselines', (11, 13)),\n",
       "   ('20 New s Group', (15, 19)),\n",
       "   ('5 Abstract s Group', (20, 24))]],\n",
       " [[('indicating', (20, 21))],\n",
       "  [('word embedding methods', (4, 7)),\n",
       "   ('outperform', (7, 8)),\n",
       "   ('basic bag - of - words methods', (9, 16))]],\n",
       " [[('achieves', (2, 3)), ('over', (5, 6))],\n",
       "  [('Our method', (0, 2)),\n",
       "   ('better performance', (3, 5)),\n",
       "   ('Retrofit method', (6, 8))]],\n",
       " [[('on', (6, 7))],\n",
       "  [('outperforms', (2, 3)),\n",
       "   ('TWE method', (4, 6)),\n",
       "   ('document - level and sentence - level tasks', (9, 17))]],\n",
       " [[], [('Text Classification', (4, 6))]],\n",
       " [[('propose', (5, 6)), ('for', (12, 13))],\n",
       "  [('new graph neural networkbased method', (7, 12)),\n",
       "   ('text classification', (13, 15))]],\n",
       " [[('construct', (1, 2)), ('from', (6, 7)), ('as', (16, 17))],\n",
       "  [('single large graph', (3, 6)),\n",
       "   ('entire corpus', (8, 10)),\n",
       "   ('words and documents', (13, 16)),\n",
       "   ('nodes', (17, 18))]],\n",
       " [[('model', (1, 2)), ('with', (4, 5)), ('captures', (21, 22))],\n",
       "  [('Graph Convolutional Network ( GCN )', (6, 12)),\n",
       "   ('high order neighborhoods information', (22, 26))]],\n",
       " [[('between', (2, 3)),\n",
       "   ('built byword', (7, 9)),\n",
       "   ('between', (14, 15)),\n",
       "   ('built using', (22, 24))],\n",
       "  [('edge', (1, 2)),\n",
       "   ('two word nodes', (3, 6)),\n",
       "   ('edge', (13, 14)),\n",
       "   ('word node and document node', (16, 21)),\n",
       "   (\"word frequency and word 's document frequency\", (24, 31))]],\n",
       " [[('turn', (2, 3)), ('into', (6, 7))],\n",
       "  [('text classification problem', (3, 6)),\n",
       "   ('anode classification problem', (7, 10))]],\n",
       " [[], [('https://github. com/yao8839836/text_gcn', (6, 8))]],\n",
       " [[], [('Baselines', (0, 1))]],\n",
       " [[('compare', (1, 2)), ('with', (5, 6))], [('Text GCN', (3, 5))]],\n",
       " [[('with', (12, 13))],\n",
       "  [('TF - IDF + LR', (0, 5)),\n",
       "   ('bag - of - words model', (6, 12)),\n",
       "   ('term frequencyinverse document frequency weighting', (13, 18))]],\n",
       " [[('used as', (3, 5))],\n",
       "  [('Logistic Regression', (0, 2)), ('classifier', (6, 7))]],\n",
       " [[], [('CNN', (0, 1)), ('Convolutional Neural Network', (2, 5))]],\n",
       " [[('explored', (1, 2)), ('which uses', (4, 6)), ('which uses', (15, 17))],\n",
       "  [('CNN -rand', (2, 4)),\n",
       "   ('randomly initialized word embeddings', (6, 10)),\n",
       "   ('CNN - non- static', (11, 15)),\n",
       "   ('pre-trained word embeddings', (17, 20))]],\n",
       " [[('uses', (8, 9)), ('as', (13, 14)), ('of', (16, 17))],\n",
       "  [('LSTM', (0, 1)),\n",
       "   ('last hidden state', (10, 13)),\n",
       "   ('representation', (15, 16)),\n",
       "   ('whole text', (18, 20))]],\n",
       " [[('commonly used', (7, 9))],\n",
       "  [('Bi- LSTM', (0, 2)), ('text classification', (10, 12))]],\n",
       " [[],\n",
       "  [('PV - DBOW', (0, 3)),\n",
       "   ('paragraph vector model', (5, 8)),\n",
       "   ('orders of words in text', (12, 17)),\n",
       "   ('ignored', (18, 19))]],\n",
       " [[('used', (1, 2)), ('as', (4, 5))],\n",
       "  [('Logistic Regression', (2, 4)), ('classifier', (6, 7))]],\n",
       " [[('considers', (12, 13))],\n",
       "  [('PV - DM', (0, 3)),\n",
       "   ('paragraph vector model', (5, 8)),\n",
       "   ('word order', (14, 16))]],\n",
       " [[('used', (1, 2)), ('as', (4, 5))],\n",
       "  [('Logistic Regression', (2, 4)), ('classifier', (6, 7))]],\n",
       " [[('firstly learns', (7, 9)),\n",
       "   ('based on', (11, 13)),\n",
       "   ('containing', (16, 17)),\n",
       "   ('averages', (26, 27)),\n",
       "   ('as', (29, 30)),\n",
       "   ('for', (32, 33))],\n",
       "  [('PTE', (0, 1)),\n",
       "   ('predictive text embedding', (2, 5)),\n",
       "   ('word embedding', (9, 11)),\n",
       "   ('heterogeneous text network', (13, 16)),\n",
       "   ('words , documents and labels as', (17, 23)),\n",
       "   ('nodes', (23, 24)),\n",
       "   ('word embeddings', (27, 29)),\n",
       "   ('document embeddings', (30, 32)),\n",
       "   ('text classification', (33, 35))]],\n",
       " [[('treats', (12, 13)),\n",
       "   ('as', (21, 22)),\n",
       "   ('feeds', (26, 27)),\n",
       "   ('into', (29, 30))],\n",
       "  [('fast Text', (0, 2)),\n",
       "   ('text classification', (7, 9)),\n",
       "   ('average of word / n- grams embeddings', (14, 21)),\n",
       "   ('document embeddings', (22, 24)),\n",
       "   ('document embeddings', (27, 29)),\n",
       "   ('linear classifier', (31, 33))]],\n",
       " [[('employs', (8, 9)), ('operated over', (12, 14))],\n",
       "  [('SWEM', (0, 1)),\n",
       "   ('simple word embedding models', (2, 6)),\n",
       "   ('simple pooling strategies', (9, 12)),\n",
       "   ('word embeddings', (14, 16))]],\n",
       " [[('embeds', (9, 10)), ('in', (14, 15)), ('for', (19, 20))],\n",
       "  [('LEAM', (0, 1)),\n",
       "   ('label - embedding attentive models', (2, 7)),\n",
       "   ('words and labels', (11, 14)),\n",
       "   ('same joint space', (16, 19)),\n",
       "   ('text classification', (20, 22))]],\n",
       " [[('utilizes', (1, 2))], [('label descriptions', (2, 4))]],\n",
       " [[('operates', (11, 12)), ('over', (13, 14))],\n",
       "  [('Graph - CNN - C', (0, 5)),\n",
       "   ('convolutions', (12, 13)),\n",
       "   ('word embedding similarity graphs', (14, 18)),\n",
       "   ('Chebyshev filter', (30, 32))]],\n",
       " [[], [('Graph - CNN - S', (0, 5))]],\n",
       " [[('using', (15, 16))],\n",
       "  [('Graph - CNN - F', (0, 5)), ('Fourier filter', (16, 18))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('set', (5, 6)),\n",
       "   ('of', (9, 10)),\n",
       "   ('as', (14, 15)),\n",
       "   ('set', (17, 18)),\n",
       "   ('as', (21, 22))],\n",
       "  [('Text GCN', (1, 3)),\n",
       "   ('embedding size', (7, 9)),\n",
       "   ('first convolution layer', (11, 14)),\n",
       "   ('200', (15, 16)),\n",
       "   ('window size', (19, 21)),\n",
       "   ('20', (22, 23))]],\n",
       " [[('tuned', (1, 2)), ('set', (5, 6)), ('as', (9, 10))],\n",
       "  [('other parameters', (2, 4)),\n",
       "   ('learning rate', (7, 9)),\n",
       "   ('0.02', (10, 11))]],\n",
       " [[('For', (0, 1)), ('using', (3, 4)), ('used', (9, 10))],\n",
       "  [('pre-trained word embeddings', (4, 7)),\n",
       "   ('300 dimensional Glo Ve word embeddings', (10, 16))]],\n",
       " [[('performs', (2, 3)), ('based on', (15, 17))],\n",
       "  [('Text GCN', (0, 2)),\n",
       "   ('best and significantly outperforms', (4, 8)),\n",
       "   ('all baseline models ( p < 0.05', (8, 15))]],\n",
       " [[('When', (0, 1)),\n",
       "   ('provided', (7, 8)),\n",
       "   ('performs', (10, 11)),\n",
       "   ('especially on', (14, 16))],\n",
       "  [('pre-trained Glo Ve word embeddings', (1, 6)),\n",
       "   ('CNN', (9, 10)),\n",
       "   ('much better', (11, 13)),\n",
       "   ('Ohsumed and 20 NG', (16, 20))]],\n",
       " [[('rely on', (7, 9)),\n",
       "   ('tend to perform', (13, 16)),\n",
       "   ('when', (17, 18)),\n",
       "   ('are', (19, 20))],\n",
       "  [('LSTM - based models', (2, 6)),\n",
       "   ('pre-trained word embeddings', (9, 12)),\n",
       "   ('better', (16, 17)),\n",
       "   ('documents', (18, 19)),\n",
       "   ('shorter', (20, 21))]],\n",
       " [[('achieves', (3, 4)), ('to', (6, 7)), ('on', (9, 10))],\n",
       "  [('PV - DBOW', (0, 3)),\n",
       "   ('comparable results', (4, 6)),\n",
       "   ('strong baselines', (7, 9)),\n",
       "   ('20 NG and Ohsumed', (10, 14))]],\n",
       " [[('performs', (3, 4)), ('than', (5, 6))],\n",
       "  [('PV - DM', (0, 3)), ('worse', (4, 5)), ('PV - DBOW', (6, 9))]],\n",
       " [[('show', (5, 6))],\n",
       "  [('Graph - CNN models', (0, 4)), ('competitive performances', (6, 8))]],\n",
       " [[], [('Text Categorization', (6, 8))]],\n",
       " [[('call', (1, 2)), ('as', (10, 11)), ('per', (14, 15)), ('in', (18, 19))],\n",
       "  [('deep pyramid CNN ( DPCNN )', (3, 9)),\n",
       "   ('computation time', (12, 14)),\n",
       "   ('layer', (15, 16)),\n",
       "   ('decreases', (16, 17)),\n",
       "   ('exponentially', (17, 18)),\n",
       "   (\"' pyramid shape\", (20, 23))]],\n",
       " [[('converting', (1, 2)),\n",
       "   ('to', (4, 5)),\n",
       "   ('alternates', (12, 13)),\n",
       "   ('over and', (20, 22)),\n",
       "   ('leading to', (25, 27)),\n",
       "   ('in which', (30, 32)),\n",
       "   ('shrinks in', (42, 44))],\n",
       "  [('discrete text', (2, 4)),\n",
       "   ('continuous representation', (5, 7)),\n",
       "   ('DPCNN architecture', (9, 11)),\n",
       "   ('convolution block and a downsampling layer', (14, 20)),\n",
       "   ('1', (23, 24)),\n",
       "   ('deep network', (28, 30)),\n",
       "   ('internal data size ( as well as per-layer computation )', (32, 42)),\n",
       "   ('pyramid shape', (45, 47))]],\n",
       " [[('treated as', (5, 7))],\n",
       "  [('network depth', (1, 3)), ('meta-parameter', (8, 9))]],\n",
       " [[('of', (3, 4)), ('bounded to be', (7, 10))],\n",
       "  [('computational complexity', (1, 3)),\n",
       "   ('no more than twice', (10, 14)),\n",
       "   ('one convolution block', (16, 19))]],\n",
       " [[('show', (1, 2))],\n",
       "  [('outperforms', (8, 9)),\n",
       "   ('sentiment classification', (18, 20)),\n",
       "   ('topic classification', (21, 23))]],\n",
       " [[('performs', (3, 4)),\n",
       "   ('generalizes', (9, 10)),\n",
       "   ('to', (14, 15)),\n",
       "   ('of', (17, 18)),\n",
       "   ('covering', (20, 21))],\n",
       "  [('first layer', (1, 3)),\n",
       "   ('text region embedding', (4, 7)),\n",
       "   ('commonly used word embedding', (10, 14)),\n",
       "   ('embedding', (16, 17)),\n",
       "   ('text regions', (18, 20)),\n",
       "   ('one or more words', (21, 25))]],\n",
       " [[('followed by', (2, 4)),\n",
       "   ('of', (5, 6)),\n",
       "   ('interleaved with', (16, 18)),\n",
       "   ('with', (20, 21)),\n",
       "   ('for', (23, 24))],\n",
       "  [('stacking', (4, 5)),\n",
       "   ('convolution blocks ( two convolution layers and a shortcut )', (6, 16)),\n",
       "   ('pooling layers', (18, 20)),\n",
       "   ('stride 2', (21, 23)),\n",
       "   ('downsampling', (24, 25))]],\n",
       " [[('aggregates', (4, 5)), ('for', (7, 8)), ('into', (10, 11))],\n",
       "  [('final pooling layer', (1, 4)),\n",
       "   ('internal data', (5, 7)),\n",
       "   ('each document', (8, 10)),\n",
       "   ('one vector', (11, 13))]],\n",
       " [[('use', (1, 2)), ('for', (4, 5))],\n",
       "  [('max pooling', (2, 4)), ('all pooling layers', (5, 8))]],\n",
       " [[], []],\n",
       " [[], [('Large data results', (0, 3))]],\n",
       " [[('On', (0, 1)), ('validates', (15, 16))],\n",
       "  [('DPCNN', (6, 7)),\n",
       "   ('outperforms', (7, 8)),\n",
       "   ('all of the previous results', (8, 13)),\n",
       "   ('effectiveness', (17, 18))]],\n",
       " [[], [('Small data results', (0, 3))]],\n",
       " [[('with', (8, 9)), ('shown', (17, 18)), ('turned out to be', (20, 24))],\n",
       "  [('DPCNN performances', (6, 8)),\n",
       "   ('100 - dim unsupervised embed - dings', (9, 16)),\n",
       "   ('as good', (24, 26)),\n",
       "   ('300 - dim unsupervised embeddings', (29, 34))]],\n",
       " [[('rivals', (5, 6)), ('moved up from', (24, 27)), ('to', (30, 31))],\n",
       "  [('ShallowCNN', (0, 1)),\n",
       "   ('DPCNN', (6, 7)),\n",
       "   ('best linear model', (17, 20)),\n",
       "   ('worst performer', (28, 30)),\n",
       "   ('third best performer', (32, 35))]],\n",
       " [[], [('Supervised and Semi- Supervised Text Categorization', (0, 6))]],\n",
       " [[('shown to be', (11, 14)), ('for', (15, 16))],\n",
       "  [('One - hot', (0, 3)),\n",
       "   ('CNN ( convolutional neural network )', (3, 9)),\n",
       "   ('effective', (14, 15)),\n",
       "   ('text categorization', (16, 18))]],\n",
       " [[('build on', (5, 7)),\n",
       "   ('of', (10, 11)),\n",
       "   ('explore', (18, 19)),\n",
       "   ('via', (24, 25)),\n",
       "   ('seeking to overcome', (34, 37)),\n",
       "   ('in', (41, 42))],\n",
       "  [('general framework', (8, 10)),\n",
       "   ('more sophisticated region embedding', (20, 24)),\n",
       "   ('Long Short - Term Memory ( LSTM )', (25, 33)),\n",
       "   ('shortcomings', (38, 39)),\n",
       "   ('supervised and semi-supervised settings', (43, 47))]],\n",
       " [[('to enable', (3, 5)), ('of', (6, 7)), ('over', (8, 9))],\n",
       "  [('learning', (5, 6)),\n",
       "   ('dependencies', (7, 8)),\n",
       "   ('larger time lags', (9, 12))]],\n",
       " [[('of', (12, 13))],\n",
       "  [('LSTM', (4, 5)),\n",
       "   ('embed', (9, 10)),\n",
       "   ('text regions', (10, 12)),\n",
       "   ('variable ( and possibly large ) sizes', (13, 20))]],\n",
       " [[('including elimination', (12, 14)),\n",
       "   ('of', (14, 15)),\n",
       "   ('to produce', (21, 23)),\n",
       "   ('to', (24, 25))],\n",
       "  [('simplify', (4, 5)),\n",
       "   ('model', (6, 7)),\n",
       "   ('word embedding layer', (16, 19)),\n",
       "   ('input', (23, 24)),\n",
       "   ('LSTM', (25, 26))]],\n",
       " [[], [('http://riejohnson.com/cnn download.html', (8, 10))]],\n",
       " [[], [('Experiments', (0, 1))]],\n",
       " [[('done with', (2, 4)),\n",
       "   ('with', (5, 6)),\n",
       "   ('with', (11, 12)),\n",
       "   ('optionally', (14, 15)),\n",
       "   ('for', (16, 17))],\n",
       "  [('Optimization', (0, 1)),\n",
       "   ('SGD', (4, 5)),\n",
       "   ('mini-batch size 50 or 100', (6, 11)),\n",
       "   ('momentum', (12, 13)),\n",
       "   ('rmsprop', (15, 16)),\n",
       "   ('acceleration', (17, 18))]],\n",
       " [[('Comparing', (0, 1)),\n",
       "   ('see that', (9, 11)),\n",
       "   ('with', (17, 18)),\n",
       "   ('outperforms', (25, 26)),\n",
       "   ('on', (35, 36))],\n",
       "  [('our one - hot bidirectional LSTM', (11, 17)),\n",
       "   ('pooling ( oh - 2 LSTMp )', (18, 25)),\n",
       "   ('word - vector LSTM ( wv - LSTM )', (26, 35)),\n",
       "   ('all the datasets', (36, 39))]],\n",
       " [[('review', (2, 3))], [('non -LSTM baseline methods', (4, 8))]],\n",
       " [[('on', (2, 3))],\n",
       "  [('three out of the four datasets', (3, 9)),\n",
       "   ('oh - 2 LSTMp', (10, 14)),\n",
       "   ('outperforms', (14, 15)),\n",
       "   ('SVM and the CNN', (15, 19))]],\n",
       " [[('is', (6, 7)), ('than', (9, 10))],\n",
       "  [('RCV1', (2, 3)),\n",
       "   ('n-gram SVM', (4, 6)),\n",
       "   ('no better', (7, 9)),\n",
       "   ('bag - of - word SVM', (10, 16)),\n",
       "   ('outperforms', (25, 26)),\n",
       "   ('seq-CNN', (26, 27))]],\n",
       " [[('works', (6, 7))],\n",
       "  [('one - hot CNN', (2, 6)), ('surprising well', (7, 9))]],\n",
       " [[('on', (4, 5)),\n",
       "   ('is', (6, 7)),\n",
       "   ('of', (15, 16)),\n",
       "   ('obtained by', (18, 20)),\n",
       "   ('of', (24, 25)),\n",
       "   ('with', (27, 28))],\n",
       "  [('previous best performance', (1, 4)),\n",
       "   ('20NG', (5, 6)),\n",
       "   ('15.3', (7, 8)),\n",
       "   ('DL15', (16, 17)),\n",
       "   ('pre-training wv - LSTM', (20, 24)),\n",
       "   ('1024 units', (25, 27)),\n",
       "   ('labeled training data', (28, 31))]],\n",
       " [[('achieved', (5, 6))],\n",
       "  [('Our oh - 2 LSTMp', (0, 5)), ('13.32', (6, 7)), ('2 % better', (10, 13))]],\n",
       " [[], [('Semi-supervised experiments', (0, 2))]],\n",
       " [[('with', (18, 19))],\n",
       "  [('pre-trained wv - LSTM', (2, 6)),\n",
       "   ('clearly outperformed', (6, 8)),\n",
       "   ('supervised wv - LSTM', (9, 13)),\n",
       "   ('underperformed', (15, 16)),\n",
       "   ('models', (17, 18)),\n",
       "   ('region tv-embeddings', (19, 21))]],\n",
       " [[('using', (8, 9)), ('performed', (18, 19))],\n",
       "  [('wv - 2 LSTMp', (4, 8)),\n",
       "   ('Google News vectors', (10, 13)),\n",
       "   ('relatively poorly', (19, 21))]],\n",
       " [[('review', (2, 3)),\n",
       "   ('of', (5, 6)),\n",
       "   ('with', (10, 11)),\n",
       "   ('comparable with', (24, 26)),\n",
       "   ('in terms of', (40, 43))],\n",
       "  [('performance', (4, 5)),\n",
       "   ('one - hot CNN', (6, 10)),\n",
       "   ('one 200 - dim CNN tv-embedding', (11, 17)),\n",
       "   ('our LSTM', (26, 28)),\n",
       "   ('two 100 - dim LSTM tv-embeddings', (29, 35)),\n",
       "   ('dimensionality of tv-embeddings', (44, 47))]],\n",
       " [[('rivals', (7, 8)), ('on', (17, 18)), ('on', (24, 25))],\n",
       "  [('LSTM', (1, 2)),\n",
       "   ('CNN', (11, 12)),\n",
       "   ('IMDB / Elec', (18, 21)),\n",
       "   ('underperforms', (22, 23)),\n",
       "   ('RCV1', (25, 26))]],\n",
       " [[('Increasing', (0, 1)),\n",
       "   ('of', (3, 4)),\n",
       "   ('from', (6, 7)),\n",
       "   ('on', (10, 11)),\n",
       "   ('obtain', (14, 15))],\n",
       "  [('dimensionality', (2, 3)),\n",
       "   ('LSTM tvembeddings', (4, 6)),\n",
       "   ('100 to 300', (7, 10)),\n",
       "   ('RCV1', (11, 12)),\n",
       "   ('8.62', (15, 16))]],\n",
       " [[], [('SEMI - SUPERVISED TEXT CLASSIFICATION', (4, 9))]],\n",
       " [[], [('image classification', (11, 13))]],\n",
       " [[('extend', (5, 6)), ('to', (8, 9))],\n",
       "  [('text classification tasks and sequence models', (9, 15))]],\n",
       " [[('consist of', (3, 5)), ('to', (8, 9))],\n",
       "  [('Adversarial perturbations', (0, 2)),\n",
       "   ('making', (5, 6)),\n",
       "   ('small modifications', (6, 8)),\n",
       "   ('very many real - valued inputs', (9, 15))]],\n",
       " [[('For', (0, 1)), ('is', (6, 7)), ('represented as', (11, 13))],\n",
       "  [('text classification', (1, 3)),\n",
       "   ('input', (5, 6)),\n",
       "   ('discrete', (7, 8)),\n",
       "   ('series of', (14, 16)),\n",
       "   ('highdimensional one - hot vectors', (16, 21))]],\n",
       " [[('define', (18, 19)), ('on', (21, 22)), ('instead of', (25, 27))],\n",
       "  [('perturbation', (20, 21)),\n",
       "   ('continuous word embeddings', (22, 25)),\n",
       "   ('discrete word inputs', (27, 30))]],\n",
       " [[('as a means of', (6, 10)), ('by stabilizing', (14, 16))],\n",
       "  [('regularizing', (10, 11)),\n",
       "   ('text classifier', (12, 14)),\n",
       "   ('classification function', (17, 19))]],\n",
       " [[('used', (2, 3)), ('on', (4, 5))],\n",
       "  [('TensorFlow', (3, 4)), ('GPUs', (5, 6))]],\n",
       " [[], []],\n",
       " [[('trained for', (1, 3))], [('100,000 steps', (3, 5))]],\n",
       " [[('applied', (1, 2)),\n",
       "   ('with', (4, 5)),\n",
       "   ('set to', (6, 8)),\n",
       "   ('on', (9, 10)),\n",
       "   ('except', (13, 14))],\n",
       "  [('gradient clipping', (2, 4)),\n",
       "   ('norm', (5, 6)),\n",
       "   ('1.0', (8, 9)),\n",
       "   ('all the parameters', (10, 13)),\n",
       "   ('word embeddings', (14, 16))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('of', (2, 3)),\n",
       "   ('applied', (9, 10)),\n",
       "   ('on', (11, 12)),\n",
       "   ('with', (16, 17))],\n",
       "  [('regularization', (1, 2)),\n",
       "   ('recurrent language model', (4, 7)),\n",
       "   ('dropout', (10, 11)),\n",
       "   ('word embedding layer', (13, 16)),\n",
       "   ('0.5 dropout rate', (17, 20))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('used', (7, 8)),\n",
       "   ('for', (12, 13)),\n",
       "   ('used', (24, 25)),\n",
       "   ('shared with', (31, 33))],\n",
       "  [('bidirectional LSTM model', (2, 5)),\n",
       "   ('512 hidden units LSTM', (8, 12)),\n",
       "   ('standard order and reversed order sequences', (15, 21)),\n",
       "   ('256 dimensional word embeddings', (25, 29)),\n",
       "   ('both of the LSTMs', (33, 37))]],\n",
       " [[('saw', (1, 2)), ('on', (5, 6)), ('much smaller than', (15, 18))],\n",
       "  [('cosine distance', (3, 5)),\n",
       "   ('adversarial and virtual adversarial training ( 0.159-0.331 )', (6, 14)),\n",
       "   ('baseline and random perturbation method ( 0.244-0.399 )', (21, 29))]],\n",
       " [[('on', (4, 5))], [('Elec and RCV1 datasets', (6, 10))]],\n",
       " [[('on', (9, 10))],\n",
       "  [('our proposed method', (3, 6)),\n",
       "   ('improved test performance', (6, 9)),\n",
       "   ('baseline method', (11, 13))]],\n",
       " [[('further improves', (19, 21)), ('on', (22, 23))],\n",
       "  [('Our unidirectional LSTM model', (0, 4)),\n",
       "   ('improves', (4, 5)),\n",
       "   ('state of the art method', (7, 12)),\n",
       "   ('our method', (13, 15)),\n",
       "   ('results', (21, 22)),\n",
       "   ('RCV1', (23, 24))]],\n",
       " [[('on', (3, 4))], [('Rotten Tomatoes dataset', (5, 8))]],\n",
       " [[('able to', (3, 5)),\n",
       "   ('over', (6, 7)),\n",
       "   ('with', (12, 13)),\n",
       "   ('achieved', (20, 21)),\n",
       "   ('as', (25, 26))],\n",
       "  [('Adversarial training', (0, 2)),\n",
       "   ('improve', (5, 6)),\n",
       "   ('baseline method', (8, 10)),\n",
       "   ('almost the same performance', (21, 25)),\n",
       "   ('current state of the art method', (27, 33))]],\n",
       " [[('of', (4, 5))],\n",
       "  [('test performance', (2, 4)),\n",
       "   ('only virtual adversarial training', (5, 9)),\n",
       "   ('worse', (10, 11)),\n",
       "   ('baseline', (13, 14))]],\n",
       " [[], [('Text Classification', (7, 9))]],\n",
       " [[('introduce', (5, 6)),\n",
       "   ('for', (10, 11)),\n",
       "   ('by combining', (14, 16)),\n",
       "   ('to', (19, 20))],\n",
       "  [('C - LSTM', (11, 14)),\n",
       "   ('CNN and LSTM', (16, 19)),\n",
       "   ('model sentences', (20, 22))]],\n",
       " [[('To benefit from', (0, 3)),\n",
       "   ('of', (5, 6)),\n",
       "   ('design', (12, 13)),\n",
       "   ('by feeding', (23, 25)),\n",
       "   ('of', (27, 28)),\n",
       "   ('into', (33, 34))],\n",
       "  [('advantages', (4, 5)),\n",
       "   ('simple end - to - end , unified architecture', (14, 23)),\n",
       "   ('output', (26, 27)),\n",
       "   ('one - layer CNN', (29, 33)),\n",
       "   ('LSTM', (34, 35))]],\n",
       " [[('constructed on top of', (3, 7)),\n",
       "   ('from', (11, 12)),\n",
       "   ('to learn', (16, 18)),\n",
       "   ('of', (22, 23))],\n",
       "  [('CNN', (1, 2)),\n",
       "   ('pre-trained word vectors', (8, 11)),\n",
       "   ('massive unlabeled text data', (12, 16)),\n",
       "   ('higher - level representions', (18, 22)),\n",
       "   ('n-grams', (23, 24))]],\n",
       " [[('to learn', (1, 3)),\n",
       "   ('from', (5, 6)),\n",
       "   ('of', (15, 16)),\n",
       "   ('organized as', (18, 20)),\n",
       "   ('to serve', (23, 25)),\n",
       "   ('of', (28, 29))],\n",
       "  [('sequential correlations', (3, 5)),\n",
       "   ('higher - level suqence representations', (6, 11)),\n",
       "   ('feature maps', (13, 15)),\n",
       "   ('CNN', (16, 17)),\n",
       "   ('sequential window features', (20, 23)),\n",
       "   ('input', (27, 28)),\n",
       "   ('LSTM', (29, 30))]],\n",
       " [[('choose', (1, 2)), ('other than relying on', (6, 10))],\n",
       "  [('sequence - based input', (2, 6)),\n",
       "   ('syntactic parse trees', (11, 14)),\n",
       "   ('neural network', (18, 20))]],\n",
       " [[('implement', (1, 2)),\n",
       "   ('based on', (4, 6)),\n",
       "   ('supports', (14, 15)),\n",
       "   ('of', (21, 22))],\n",
       "  [('Theano', (6, 7)),\n",
       "   ('python library', (10, 12)),\n",
       "   ('efficient symbolic differentiation', (15, 18)),\n",
       "   ('transparent use', (19, 21)),\n",
       "   ('GPU', (23, 24))]],\n",
       " [[('To benefit from', (0, 3)),\n",
       "   ('of', (8, 9)),\n",
       "   ('train', (13, 14)),\n",
       "   ('on', (16, 17))],\n",
       "  [('efficiency of parallel computation', (4, 8)),\n",
       "   ('tensors', (10, 11)),\n",
       "   ('model', (15, 16)),\n",
       "   ('GPU', (18, 19))]],\n",
       " [[('use', (7, 8)), ('for', (15, 16))],\n",
       "  [('one convolutional layer', (8, 11)), ('both tasks', (16, 18))]],\n",
       " [[('For', (0, 1)), ('set to', (8, 10)), ('set to', (17, 19))],\n",
       "  [('TREC', (1, 2)),\n",
       "   ('number of filters', (4, 7)),\n",
       "   ('300', (11, 12)),\n",
       "   ('memory dimension', (14, 16)),\n",
       "   ('300', (20, 21))]],\n",
       " [[('of', (13, 14))],\n",
       "  [('word vector layer and the LSTM layer', (1, 8)),\n",
       "   ('dropped outwith', (9, 11)),\n",
       "   ('probability', (12, 13)),\n",
       "   ('0.5', (14, 15))]],\n",
       " [[('add', (2, 3)),\n",
       "   ('with', (5, 6)),\n",
       "   ('of', (8, 9)),\n",
       "   ('to', (10, 11)),\n",
       "   ('in', (13, 14))],\n",
       "  [('L2 regularization', (3, 5)),\n",
       "   ('factor', (7, 8)),\n",
       "   ('0.001', (9, 10)),\n",
       "   ('weights', (12, 13)),\n",
       "   ('softmax layer', (15, 17))]],\n",
       " [[], [('Results and Model Analysis', (0, 4))]],\n",
       " [[], [('Sentiment Classification', (0, 2))]],\n",
       " [[('achieve', (8, 9)), ('for', (14, 15))],\n",
       "  [('fourth best published result', (10, 14)),\n",
       "   ('5 - class classification task', (16, 21))]],\n",
       " [[('For', (0, 1)), ('achieve', (7, 8)), ('with respect to', (10, 13))],\n",
       "  [('binary classification task', (2, 5)),\n",
       "   ('comparable results', (8, 10)),\n",
       "   ('state - of - the - art ones', (14, 22))]],\n",
       " [[], [('Question Type Classification', (0, 3))]],\n",
       " [[('consistently outperforms', (5, 7))],\n",
       "  [('Our result', (3, 5)), ('all published neural baseline models', (7, 12))]],\n",
       " [[('close to', (6, 8)), ('depends on', (20, 22))],\n",
       "  [('Our result', (3, 5)),\n",
       "   ('state - of - the - art SVM', (11, 19)),\n",
       "   ('highly engineered features', (22, 25))]],\n",
       " [[('with', (11, 12))],\n",
       "  [('single convolutional layer', (8, 11)), ('other cases', (18, 20))]],\n",
       " [[('shown', (2, 3)),\n",
       "   ('with', (7, 8)),\n",
       "   ('performs', (11, 12)),\n",
       "   ('among', (13, 14))],\n",
       "  [('single convolutional layer', (4, 7)),\n",
       "   ('filter length 3', (8, 11)),\n",
       "   ('best', (12, 13)),\n",
       "   ('all filter configurations', (14, 17))]],\n",
       " [[('For the case of', (0, 4)),\n",
       "   ('shown that', (12, 14)),\n",
       "   ('with', (16, 17)),\n",
       "   ('performs', (20, 21)),\n",
       "   ('that', (22, 23))],\n",
       "  [('multiple convolutional layers', (4, 7)),\n",
       "   ('filter configurations', (14, 16)),\n",
       "   ('filter length 3', (17, 20)),\n",
       "   ('better', (21, 22))]],\n",
       " [[], [('Text Classification', (5, 7))]],\n",
       " [[('of', (19, 20))], [('whole sentences', (20, 22))]],\n",
       " [[('of', (10, 11)), ('to approach', (14, 16)), ('using', (19, 20))],\n",
       "  [('deep architectures', (8, 10)),\n",
       "   ('many convolutional layers', (11, 14)),\n",
       "   ('up to 29 layers', (20, 24))]],\n",
       " [[('consists of', (2, 4)), ('add up', (32, 34))],\n",
       "  [('abcdefghijklmnopqrstuvwxyz0123456', (8, 9)),\n",
       "   ('special padding', (24, 26)),\n",
       "   ('unknown token', (29, 31))]],\n",
       " [[('padded to', (4, 6)), ('of', (9, 10))],\n",
       "  [('input text', (1, 3)),\n",
       "   ('fixed size', (7, 9)),\n",
       "   ('1014', (10, 11)),\n",
       "   ('larger text', (12, 14))]],\n",
       " [[('of size', (4, 6))], [('character embedding', (1, 3)), ('16', (6, 7))]],\n",
       " [[('performed with', (2, 4)), ('using', (6, 7)), ('of size', (9, 11))],\n",
       "  [('Training', (0, 1)),\n",
       "   ('SGD', (4, 5)),\n",
       "   ('mini-batch', (8, 9)),\n",
       "   ('128', (11, 12)),\n",
       "   ('initial learning rate', (14, 17)),\n",
       "   ('0.01', (18, 19)),\n",
       "   ('momentum', (20, 21)),\n",
       "   ('0.9', (22, 23))]],\n",
       " [[('done using', (3, 5))], [('Torch', (5, 6))]],\n",
       " [[('performed on', (3, 5))], [('single NVidia K40 GPU', (6, 10))]],\n",
       " [[('use', (13, 14)), ('without', (17, 18))],\n",
       "  [('temporal batch norm', (14, 17)), ('dropout', (18, 19))]],\n",
       " [[('works', (3, 4)), ('on', (5, 6)), ('for', (13, 14))],\n",
       "  [('Our deep architecture', (0, 3)),\n",
       "   ('well', (4, 5)),\n",
       "   ('big data sets', (6, 9)),\n",
       "   ('small depths', (14, 16))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('see that', (13, 15)),\n",
       "   ('performs', (18, 19)),\n",
       "   ('than', (20, 21)),\n",
       "   ('on', (37, 38))],\n",
       "  [('smallest depth', (2, 4)),\n",
       "   ('our model', (15, 17)),\n",
       "   ('better', (19, 20)),\n",
       "   (\"Zhang 's convolutional baselines\", (21, 25)),\n",
       "   ('biggest data sets', (39, 42))]],\n",
       " [[('in', (4, 5)), ('observed on', (9, 11)), ('which', (17, 18))],\n",
       "  [('most important decrease', (1, 4)),\n",
       "   ('classification error', (5, 7)),\n",
       "   ('largest data set Amazon Full', (12, 17)),\n",
       "   ('more than 3 Million training samples', (19, 25))]],\n",
       " [[('observe', (2, 3)),\n",
       "   ('for', (4, 5)),\n",
       "   ('works', (13, 14)),\n",
       "   ('on', (15, 16))],\n",
       "  [('small depth', (6, 8)),\n",
       "   ('temporal max - pooling', (9, 13)),\n",
       "   ('best', (14, 15)),\n",
       "   ('all data sets', (16, 19))]],\n",
       " [[], [('Text Classification', (6, 8))]],\n",
       " [[('explore treating', (4, 6)),\n",
       "   ('as', (7, 8)),\n",
       "   ('at', (13, 14)),\n",
       "   ('applying', (18, 19))],\n",
       "  [('raw signal', (11, 13)),\n",
       "   ('character level', (14, 16)),\n",
       "   ('temporal ( one-dimensional ) ConvNets', (19, 24))]],\n",
       " [[], [('Traditional Methods', (0, 2))]],\n",
       " [[], [('Bag - of - words', (0, 5)), ('TFIDF', (7, 8))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('constructed by selecting', (12, 15)),\n",
       "   ('from', (19, 20))],\n",
       "  [('each dataset', (1, 3)),\n",
       "   ('bag - of - words model', (5, 11)),\n",
       "   ('50,000 most frequent words', (15, 19)),\n",
       "   ('training subset', (21, 23))]],\n",
       " [[], [('Bag - of - ngrams and its TFIDF', (0, 8))]],\n",
       " [[('constructed', (8, 9)), ('from', (23, 24)), ('for', (27, 28))],\n",
       "  [('bag - of - ngrams models', (1, 7)),\n",
       "   ('500,000 most frequent n-grams ( up to 5 - grams )', (12, 23)),\n",
       "   ('training subset', (25, 27)),\n",
       "   ('each dataset', (28, 30))]],\n",
       " [[('on', (5, 6))],\n",
       "  [('Bag - of - means', (0, 5)), ('word embedding', (6, 8))]],\n",
       " [[('uses', (7, 8)),\n",
       "   ('on', (9, 10)),\n",
       "   ('learnt from', (11, 13)),\n",
       "   ('of', (16, 17)),\n",
       "   ('use', (22, 23)),\n",
       "   ('as', (26, 27)),\n",
       "   ('of', (28, 29))],\n",
       "  [('k-means', (8, 9)),\n",
       "   ('word2vec', (10, 11)),\n",
       "   ('training subset', (14, 16)),\n",
       "   ('each dataset', (17, 19)),\n",
       "   ('learnt means', (24, 26)),\n",
       "   ('representatives', (27, 28)),\n",
       "   ('clustered words', (30, 32))]],\n",
       " [[], [('Deep Learning Methods', (0, 3))]],\n",
       " [[], [('Word - based ConvNets', (0, 4))]],\n",
       " [[], [('Long - short term memory', (0, 5))]],\n",
       " [[('could work for', (13, 16)), ('without the need for', (18, 22))],\n",
       "  [('character - level ConvNets', (9, 13)),\n",
       "   ('text classification', (16, 18)),\n",
       "   ('words', (22, 23))]],\n",
       " [[('goes to', (24, 26)), ('observe', (33, 34)), ('start to do', (39, 42))],\n",
       "  [('scale of several millions', (27, 31)),\n",
       "   ('character - level ConvNets', (35, 39)),\n",
       "   ('better', (42, 43))]],\n",
       " [[('for', (5, 6))],\n",
       "  [('Conv Nets', (0, 2)),\n",
       "   ('work', (3, 4)),\n",
       "   ('well', (4, 5)),\n",
       "   ('user - generated data', (6, 10))]],\n",
       " [[('makes', (3, 4))],\n",
       "  [('Choice of alphabet', (0, 3)), ('difference', (5, 6))]],\n",
       " [[], [('Semantics of tasks may not matter', (0, 6))]],\n",
       " [[], [('Text Classification', (0, 2))]],\n",
       " [[('proposes', (5, 6)), ('to capture', (24, 26)), ('on both', (27, 29))],\n",
       "  [('Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling )',\n",
       "    (6, 24)),\n",
       "   ('features', (26, 27)),\n",
       "   ('time - step dimension', (30, 34)),\n",
       "   ('feature vector dimension', (36, 39))]],\n",
       " [[('utilizes', (2, 3)), ('to transform', (13, 15)), ('into', (17, 18))],\n",
       "  [('Bidirectional Long Short - Term Memory Networks ( BLSTM )', (3, 13)),\n",
       "   ('text', (16, 17)),\n",
       "   ('vectors', (18, 19))]],\n",
       " [[('utilized to obtain', (7, 10))],\n",
       "  [('2D max pooling operation', (2, 6)), ('fixed - length vector', (11, 15))]],\n",
       " [[('applies', (3, 4)), ('to capture', (11, 13)), ('to represent', (16, 18))],\n",
       "  [('2D convolution ( BLSTM - 2DCNN )', (4, 11)),\n",
       "   ('more meaningful features', (13, 16)),\n",
       "   ('input text', (19, 21))]],\n",
       " [[('of', (2, 3)), ('is', (5, 6)), ('of', (11, 12)), ('is', (13, 14))],\n",
       "  [('dimension', (1, 2)),\n",
       "   ('word embeddings', (3, 5)),\n",
       "   ('300', (6, 7)),\n",
       "   ('hidden units', (9, 11)),\n",
       "   ('LSTM', (12, 13)),\n",
       "   ('300', (14, 15))]],\n",
       " [[('use', (1, 2)), ('for', (6, 7))],\n",
       "  [('100 convolutional filters each', (2, 6)),\n",
       "   ('window sizes', (7, 9)),\n",
       "   ('( 3 , 3 )', (10, 15)),\n",
       "   ('2D pooling size', (16, 19)),\n",
       "   ('( 2 , 2 )', (20, 25))]],\n",
       " [[('set', (1, 2)), ('as', (5, 6)), ('of', (11, 12)), ('as', (13, 14))],\n",
       "  [('mini-batch size', (3, 5)),\n",
       "   ('10', (6, 7)),\n",
       "   ('learning rate', (9, 11)),\n",
       "   ('AdaDelta', (12, 13)),\n",
       "   ('default value 1.0', (15, 18))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('employ', (4, 5)),\n",
       "   ('with', (7, 8)),\n",
       "   ('of', (10, 11)),\n",
       "   ('for', (12, 13)),\n",
       "   ('use', (31, 32)),\n",
       "   ('with', (35, 36)),\n",
       "   ('over', (40, 41))],\n",
       "  [('regularization', (1, 2)),\n",
       "   ('Dropout operation', (5, 7)),\n",
       "   ('dropout rate', (8, 10)),\n",
       "   ('0.5', (11, 12)),\n",
       "   ('word embeddings', (14, 16)),\n",
       "   ('0.2', (17, 18)),\n",
       "   ('BLSTM layer', (20, 22)),\n",
       "   ('0.4', (23, 24)),\n",
       "   ('penultimate layer', (26, 28)),\n",
       "   ('l 2 penalty', (32, 35)),\n",
       "   ('coefficient 10 ? 5', (36, 40)),\n",
       "   ('parameters', (42, 43))]],\n",
       " [[], []],\n",
       " [[('achieves', (5, 6)), ('on', (8, 9))],\n",
       "  [('BLSTM - 2DCNN model', (1, 5)),\n",
       "   ('excellent performance', (6, 8)),\n",
       "   ('4 out of 6 tasks', (9, 14))]],\n",
       " [[('achieves', (3, 4)), ('on', (11, 12))],\n",
       "  [('52.4 % and 89.5 % test accuracies', (4, 11)),\n",
       "   ('SST - 1 and SST - 2', (12, 19))]],\n",
       " [[('performs', (3, 4)), ('than', (5, 6))],\n",
       "  [('BLSTM - 2DPooling', (0, 3)),\n",
       "   ('worse', (4, 5)),\n",
       "   ('state - of - the - art models', (7, 15))]],\n",
       " [[('beats', (3, 4)), ('on', (6, 7))],\n",
       "  [('BLSTM - CNN', (0, 3)),\n",
       "   ('all baselines', (4, 6)),\n",
       "   ('SST - 1 ,', (7, 11)),\n",
       "   ('SST - 2', (11, 14)),\n",
       "   ('TREC datasets', (16, 18))]],\n",
       " [[('As for', (0, 2)), ('gets', (10, 11))],\n",
       "  [('Subj and MR datasets', (2, 6)),\n",
       "   ('BLSTM - 2DCNN', (7, 10)),\n",
       "   ('second higher accuracies', (12, 15))]],\n",
       " [[('Compared with', (0, 2)), ('achieves', (7, 8))],\n",
       "  [('RCNN', (2, 3)),\n",
       "   ('BLSTM - 2DCNN', (4, 7)),\n",
       "   ('comparable result', (9, 11))]],\n",
       " [[('Compared with', (0, 2)), ('do', (8, 9)), ('such as', (17, 19))],\n",
       "  [('ReNN', (2, 3)), ('external language - specific features', (12, 17))]],\n",
       " [[('Compared with', (0, 2)), ('on', (9, 10))],\n",
       "  [('DSCNN', (2, 3)),\n",
       "   ('BLSTM - 2DCNN', (4, 7)),\n",
       "   ('outperforms', (7, 8)),\n",
       "   ('five datasets', (10, 12))]],\n",
       " [[], [('Document Classification', (6, 8))]],\n",
       " [[], [('document classification', (21, 23))]],\n",
       " [[('find that', (15, 17)), ('with', (25, 26)), ('yields', (28, 29))],\n",
       "  [('simple bi-directional LSTM ( BiLSTM ) architecture', (18, 25)),\n",
       "   ('appropriate regularization', (26, 28)),\n",
       "   ('accuracy and F 1', (29, 33)),\n",
       "   ('competitive', (36, 37)),\n",
       "   ('state of the art', (40, 44)),\n",
       "   ('four', (45, 46))]],\n",
       " [[('involving', (8, 9))],\n",
       "  [('large - scale reproducibility study', (3, 8)),\n",
       "   ('HAN', (9, 10)),\n",
       "   ('KimCNN', (15, 16)),\n",
       "   ('SGM', (18, 19))]],\n",
       " [[('compare', (4, 5)), ('to', (8, 9))],\n",
       "  [('neural approaches', (6, 8)),\n",
       "   ('logistic regression ( LR )', (9, 14)),\n",
       "   ('support vector machines ( SVMs )', (15, 21))]],\n",
       " [[('trained using', (4, 6)), ('trained with', (19, 21))],\n",
       "  [('LR model', (1, 3)),\n",
       "   ('one - vs - rest multi-label objective', (7, 14)),\n",
       "   ('SVM', (17, 18)),\n",
       "   ('linear kernel', (22, 24))]],\n",
       " [[('performed on', (5, 7)), ('with', (16, 17)), ('as', (19, 20))],\n",
       "  [('Nvidia GTX 1080 and RTX 2080 Ti GPUs', (7, 15)),\n",
       "   ('PyTorch 0.4.1', (17, 19)),\n",
       "   ('backend framework', (21, 23))]],\n",
       " [[('use', (1, 2)), ('for computing', (4, 6)), ('implementing', (12, 13))],\n",
       "  [('Scikitlearn 0.19.2', (2, 4)),\n",
       "   ('tf - idf vectors', (7, 11)),\n",
       "   ('LR and SVMs', (13, 16))]],\n",
       " [[('see that', (1, 3)),\n",
       "   ('achieves', (8, 9)),\n",
       "   ('on', (13, 14)),\n",
       "   ('establishing', (26, 27)),\n",
       "   ('for', (33, 34)),\n",
       "   ('on', (39, 40)),\n",
       "   ('of', (43, 44))],\n",
       "  [('our simple LSTM reg model', (3, 8)),\n",
       "   ('state of the art', (9, 13)),\n",
       "   ('Reuters and IMDB', (14, 17)),\n",
       "   ('mean scores', (27, 29)),\n",
       "   ('87.0 and 52.8', (30, 33)),\n",
       "   ('F 1 score', (34, 37)),\n",
       "   ('test sets', (41, 43))]],\n",
       " [[('observe', (1, 2)),\n",
       "   ('upon', (7, 8)),\n",
       "   ('of', (10, 11)),\n",
       "   ('yields', (31, 32)),\n",
       "   ('of', (33, 34)),\n",
       "   ('for', (38, 39))],\n",
       "  [('LSTM reg', (3, 5)),\n",
       "   ('consistently improves', (5, 7)),\n",
       "   ('performance', (9, 10)),\n",
       "   ('LSTM base', (11, 13)),\n",
       "   ('regularization', (30, 31)),\n",
       "   ('increases', (32, 33)),\n",
       "   ('1.5 and 0.5 points', (34, 38)),\n",
       "   ('F 1 score', (39, 42))]],\n",
       " [[('runs', (6, 7)), ('on', (17, 18))],\n",
       "  [('LSTM reg', (4, 6)),\n",
       "   ('attain state - of - theart test F 1 scores', (7, 17)),\n",
       "   ('AAPD', (18, 19))]],\n",
       " [[('perform', (8, 9))],\n",
       "  [('non-neural LR and SVM baselines', (3, 8)), ('remarkably well', (9, 11))]],\n",
       " [[('On', (0, 1)), ('beats', (8, 9)), ('including', (13, 14))],\n",
       "  [('Reuters', (1, 2)),\n",
       "   ('SVM', (7, 8)),\n",
       "   ('many neural baselines', (9, 12)),\n",
       "   ('our non-regularized LSTM base', (14, 18))]],\n",
       " [[('On', (0, 1)), ('losing only to', (13, 16))],\n",
       "  [('AAPD', (1, 2)),\n",
       "   ('SVM', (4, 5)),\n",
       "   ('ties or beats', (6, 9)),\n",
       "   ('other models', (10, 12)),\n",
       "   ('SGM', (16, 17))]],\n",
       " [[('Compared to', (0, 2)), ('appears', (8, 9))],\n",
       "  [('SVM', (3, 4)),\n",
       "   ('LR baseline', (6, 8)),\n",
       "   ('better suited', (9, 11)),\n",
       "   ('single - label datasets', (13, 17))]],\n",
       " [[], [('Practical Text Classification', (0, 3))]],\n",
       " [[('train', (5, 6)),\n",
       "   ('on', (12, 13)),\n",
       "   ('transfer', (21, 22)),\n",
       "   ('to', (24, 25)),\n",
       "   ('including', (33, 34)),\n",
       "   ('based on', (42, 44))],\n",
       "  [('large 40 GB text dataset', (14, 19)),\n",
       "   ('two text classification problems', (25, 29)),\n",
       "   ('binary sentiment', (30, 32)),\n",
       "   ('multidimensional emotion classification', (39, 42))]],\n",
       " [[], []],\n",
       " [[], [('Binary Sentiment Tweets', (0, 3))]],\n",
       " [[('gets', (3, 4)),\n",
       "   ('does not exceed', (6, 9)),\n",
       "   ('on', (14, 15)),\n",
       "   ('exceeds', (20, 21)),\n",
       "   ('on', (38, 39))],\n",
       "  [('Transformer', (2, 3)),\n",
       "   ('close', (4, 5)),\n",
       "   ('state of the art', (10, 14)),\n",
       "   ('SST dataset', (16, 18)),\n",
       "   ('mL - STM and ELMo baseline', (23, 29)),\n",
       "   ('Watson and Google Sentiment APIs', (33, 38)),\n",
       "   ('company tweets', (40, 42))]],\n",
       " [[], [('Multi - Label Emotion Tweets', (0, 5))]],\n",
       " [[('find', (1, 2)), ('on', (7, 8))],\n",
       "  [('our models', (3, 5)),\n",
       "   ('outperform', (5, 6)),\n",
       "   ('Watson', (6, 7)),\n",
       "   ('every emotion category', (8, 11))]],\n",
       " [[], [('Sem Eval Tweets', (0, 3))]],\n",
       " [[('achieved', (2, 3)),\n",
       "   ('among', (8, 9)),\n",
       "   ('with', (12, 13)),\n",
       "   ('for', (17, 18))],\n",
       "  [('Our model', (0, 2)),\n",
       "   ('top macro-averaged F1 score', (4, 8)),\n",
       "   ('all submission', (9, 11)),\n",
       "   ('competitive', (13, 14)),\n",
       "   ('micro -average F1', (19, 22)),\n",
       "   ('Jaccard Index accuracy', (24, 27))]],\n",
       " [[('compare', (2, 3)),\n",
       "   ('of', (7, 8)),\n",
       "   ('find', (18, 19)),\n",
       "   ('across', (26, 27))],\n",
       "  [('deep learning architectures', (4, 7)),\n",
       "   ('Transformer and m LSTM', (9, 13)),\n",
       "   ('Transformer', (21, 22)),\n",
       "   ('outperforms', (22, 23)),\n",
       "   ('m LSTM', (24, 26)),\n",
       "   ('Plutchik categories', (27, 29))]],\n",
       " [[('gets', (2, 3)), ('on', (7, 8)), ('than on', (12, 14))],\n",
       "  [('Our', (0, 1)),\n",
       "   ('lower F 1 scores', (3, 7)),\n",
       "   ('company tweets dataset', (9, 12)),\n",
       "   ('equivalent Se -m Eval categories', (14, 19))]],\n",
       " [[], [('Text Classification', (7, 9))]],\n",
       " [[('investigate', (5, 6)),\n",
       "   ('on', (7, 8)),\n",
       "   ('proposed', (10, 11)),\n",
       "   ('with', (29, 30))],\n",
       "  [('modifications', (6, 7)),\n",
       "   ('network', (9, 10)),\n",
       "   ('reducing', (19, 20)),\n",
       "   ('number of parameters', (21, 24)),\n",
       "   ('storage size', (25, 27)),\n",
       "   ('latency', (28, 29)),\n",
       "   ('minimal', (30, 31))]],\n",
       " [[('used', (5, 6))],\n",
       "  [('Temporal Depthwise Separable Convolution and Global Average Pooling techniques',\n",
       "    (6, 15))]],\n",
       " [[('propose', (7, 8)), ('requires', (24, 25)), ('compared to', (28, 30))],\n",
       "  [('Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )', (9, 18)),\n",
       "   ('text classification model', (20, 23)),\n",
       "   ('significantly fewer parameters', (25, 28)),\n",
       "   ('stateof - the - art CNNs', (31, 37))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('calculated', (8, 9)),\n",
       "   ('from', (12, 13)),\n",
       "   ('implemented in', (16, 18))],\n",
       "  [('SVDCNN and Char - CNN', (1, 6)),\n",
       "   ('network architecture', (14, 16)),\n",
       "   ('PyTorch', (18, 19))]],\n",
       " [[('similar to', (5, 7)), ('using', (12, 13)), ('of', (21, 22))],\n",
       "  [('SVDCNN experimental settings', (1, 4)),\n",
       "   ('original VDCNN paper', (8, 11)),\n",
       "   ('same dictionary', (14, 16)),\n",
       "   ('same embedding size', (18, 21)),\n",
       "   ('16', (22, 23))]],\n",
       " [[('performed with', (4, 6)), ('utilizing', (8, 9)), ('with', (14, 15))],\n",
       "  [('training', (1, 2)),\n",
       "   ('SGD', (6, 7)),\n",
       "   ('size batch of 64', (9, 13)),\n",
       "   ('maximum of 100 epochs', (16, 20))]],\n",
       " [[('use', (1, 2)), ('of', (6, 7))],\n",
       "  [('initial learning rate', (3, 6)),\n",
       "   ('0.01', (7, 8)),\n",
       "   ('momentum', (10, 11)),\n",
       "   ('0.9', (12, 13)),\n",
       "   ('weight decay', (15, 17)),\n",
       "   ('0.001', (18, 19))]],\n",
       " [[('performed on', (4, 6))],\n",
       "  [('NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU', (7, 18))]],\n",
       " [[('obtained by', (3, 5)), ('is', (7, 8))],\n",
       "  [('network reduction', (1, 3)),\n",
       "   ('GAP', (6, 7)),\n",
       "   ('even more representative', (8, 11))]],\n",
       " [[('comparing', (9, 10)),\n",
       "   ('of', (18, 19)),\n",
       "   ('to', (26, 27)),\n",
       "   ('representing', (31, 32)),\n",
       "   ('of', (34, 35))],\n",
       "  [('SVDCNN', (10, 11)),\n",
       "   ('number of parameters', (15, 18)),\n",
       "   ('FC layers', (20, 22)),\n",
       "   ('12.59', (25, 26)),\n",
       "   ('0.02 million parameters', (27, 30)),\n",
       "   ('reduction', (33, 34)),\n",
       "   ('99.84 %', (35, 37))]],\n",
       " [[('to', (7, 8)), ('is', (15, 16)), ('against', (21, 22)), ('of', (24, 25))],\n",
       "  [('Char - CNN', (8, 11)),\n",
       "   ('proposed model', (13, 15)),\n",
       "   ('99.82 % smaller', (16, 19)),\n",
       "   ('0.02', (20, 21)),\n",
       "   ('11.36 million', (22, 24)),\n",
       "   ('FC parameters', (25, 27))]],\n",
       " [[('occupies', (10, 11)),\n",
       "   ('with', (16, 17)),\n",
       "   ('occupies', (20, 21)),\n",
       "   ('of', (24, 25))],\n",
       "  [('our', (1, 2)),\n",
       "   ('most in - depth model', (2, 7)),\n",
       "   ('only 6 MB', (11, 14)),\n",
       "   ('VDCNN', (15, 16)),\n",
       "   ('64. 16 MB', (21, 24)),\n",
       "   ('storage', (25, 26))]],\n",
       " [[('Regarding', (0, 1)), ('of', (16, 17)), ('in comparison to', (18, 21))],\n",
       "  [('accuracy results', (1, 3)),\n",
       "   ('model', (7, 8)),\n",
       "   ('some loss', (14, 16)),\n",
       "   ('accuracy', (17, 18)),\n",
       "   ('original model', (22, 24))]],\n",
       " [[('between', (5, 6)), ('varies between', (10, 12))],\n",
       "  [('performance difference', (3, 5)),\n",
       "   ('VDCNN and SVDCNN models', (6, 10)),\n",
       "   ('0.4 and 1.3 %', (12, 16))]],\n",
       " [[], [('Text Classification', (7, 9))]],\n",
       " [[('making use of', (11, 14)),\n",
       "   ('propose', (20, 21)),\n",
       "   ('to improve', (30, 32))],\n",
       "  [('label embedding framework', (15, 18)),\n",
       "   ('Label - Embedding Attentive Model ( LEAM )', (22, 30)),\n",
       "   ('text classification', (32, 34))]],\n",
       " [[('implemented by', (4, 6)),\n",
       "   ('in', (12, 13)),\n",
       "   ('constructed directly using', (23, 26))],\n",
       "  [('proposed LEAM', (1, 3)),\n",
       "   ('jointly', (6, 7)),\n",
       "   ('word and label', (9, 12)),\n",
       "   ('same latent space', (14, 17)),\n",
       "   ('text representations', (20, 22)),\n",
       "   ('text - label compatibility', (27, 31))]],\n",
       " [[('is', (18, 19)), ('for', (20, 21))],\n",
       "  [('Label - attentive text representation', (13, 18)),\n",
       "   ('informative', (19, 20)),\n",
       "   ('downstream classification task', (22, 25))]],\n",
       " [[('involves', (8, 9)), ('retains', (19, 20)), ('of', (22, 23))],\n",
       "  [('LEAM learning procedure', (4, 7)),\n",
       "   ('series of basic algebraic operations', (10, 15)),\n",
       "   ('interpretability', (21, 22)),\n",
       "   ('label description', (29, 31)),\n",
       "   ('available', (32, 33))]],\n",
       " [[('use', (2, 3)), ('as', (10, 11)), ('for', (12, 13)), ('in', (18, 19))],\n",
       "  [('300 - dimensional Glo Ve word embeddings', (3, 10)),\n",
       "   ('initialization', (11, 12)),\n",
       "   ('word embeddings and label embeddings', (13, 18)),\n",
       "   ('our model', (19, 21))]],\n",
       " [[('initialized from', (10, 12)), ('with', (15, 16))],\n",
       "  [('Out - Of - Vocabulary ( OOV ) words', (0, 9)),\n",
       "   ('uniform distribution', (13, 15)),\n",
       "   ('range [ ? 0.01 , 0.01 ]', (16, 23))]],\n",
       " [[('implemented as', (4, 6)), ('followed by', (9, 11))],\n",
       "  [('final classifier', (1, 3)),\n",
       "   ('MLP layer', (7, 9)),\n",
       "   ('sigmoid or softmax function', (12, 16))]],\n",
       " [[('train', (1, 2)), ('with', (6, 7))],\n",
       "  [(\"our model 's parameters\", (2, 6)),\n",
       "   ('Adam Optimizer', (8, 10)),\n",
       "   ('initial learning rate', (20, 23)),\n",
       "   ('0.001', (24, 25)),\n",
       "   ('minibatch size', (28, 30)),\n",
       "   ('100', (31, 32))]],\n",
       " [[('employed on', (3, 5)), ('with', (10, 11))],\n",
       "  [('Dropout regularization', (0, 2)),\n",
       "   ('final MLP layer', (6, 9)),\n",
       "   ('dropout rate 0.5', (11, 14))]],\n",
       " [[('implemented using', (3, 5)), ('trained on', (8, 10))],\n",
       "  [('Tensorflow', (5, 6)), ('GPU Titan X.', (10, 13))]],\n",
       " [[], [('https://github.com/guoyinwang/LEAM', (9, 10))]],\n",
       " [[('compare', (1, 2)), ('with', (11, 12))],\n",
       "  [('logistic regression model', (8, 11)),\n",
       "   ('bag - ofwords', (12, 15)),\n",
       "   ('bidirectional gated recurrent unit ( Bi - GRU )', (17, 26)),\n",
       "   ('single - layer 1 D convolutional network', (28, 35))]],\n",
       " [[('including', (14, 15))],\n",
       "  [('multi-label classification of clinical text', (8, 13)),\n",
       "   ('Condensed Memory Networks ( C - MemNN )', (15, 23)),\n",
       "   ('Attentive LSTM', (24, 26)),\n",
       "   ('Convolutional Attention ( CAML )', (27, 32))]],\n",
       " [[('provides', (1, 2)), ('than', (13, 14)), ('except', (16, 17))],\n",
       "  [('LEAM', (0, 1)),\n",
       "   ('best AUC score', (3, 6)),\n",
       "   ('better F1 and P@5 values', (8, 13)),\n",
       "   ('all methods', (14, 16)),\n",
       "   ('CNN', (17, 18))]],\n",
       " [[('consistently', (1, 2)), ('performs', (15, 16)), ('than', (17, 18))],\n",
       "  [('CNN', (0, 1)),\n",
       "   ('basic Bi - GRU architecture', (4, 9)),\n",
       "   ('logistic regression baseline', (12, 15)),\n",
       "   ('worse', (16, 17)),\n",
       "   ('all deep learning architectures', (18, 22))]],\n",
       " [[], [('Text Classification', (6, 8))]],\n",
       " [[],\n",
       "  [('document classification', (7, 9)), ('supervised learning', (17, 19))]],\n",
       " [[('perform', (2, 3)), ('call', (9, 10))],\n",
       "  [('hierarchical classification', (3, 5))]],\n",
       " [[('call', (12, 13))],\n",
       "  [('hierarchical', (7, 8)),\n",
       "   ('Hierarchical Deep Learning for Text classification ( HDLTex )',\n",
       "    (13, 22))]],\n",
       " [[('combines', (1, 2)), ('to allow', (5, 7)), ('by', (13, 14))],\n",
       "  [('HDLTex', (0, 1)),\n",
       "   ('deep learning architectures', (2, 5)),\n",
       "   ('over all and specialized learning', (8, 13)),\n",
       "   ('level of the document hierarchy', (14, 19))]],\n",
       " [[('obtained using', (4, 6))],\n",
       "  [('combination', (7, 8)),\n",
       "   ('central processing units ( CPUs ) and graphical processing units ( GPUs )',\n",
       "    (9, 22))]],\n",
       " [[('done on', (3, 5)), ('with', (14, 15)), ('were', (25, 26))],\n",
       "  [('processing', (1, 2)),\n",
       "   ('Xeon E5 ? 2640 ( 2.6 GHz )', (6, 14)),\n",
       "   ('32 cores', (15, 17)),\n",
       "   ('64GB memory', (18, 20)),\n",
       "   ('GPU cards', (23, 25)),\n",
       "   ('N vidia Quadro K620', (26, 30)),\n",
       "   ('N vidia Tesla K20c', (31, 35))]],\n",
       " [[('implemented', (1, 2)), ('using', (6, 7)), ('is', (17, 18))],\n",
       "  [('Compute Unified Device Architecture ( CUDA )', (8, 15)),\n",
       "   ('parallel computing platform', (19, 22))]],\n",
       " [[('used', (2, 3)), ('for creating', (8, 10))],\n",
       "  [('Keras and Tensor Flow libraries', (3, 8)),\n",
       "   ('neural networks', (11, 13))]],\n",
       " [[('compare', (3, 4)), ('stacking', (19, 20)), ('with', (21, 22))],\n",
       "  [('three conventional document classification approaches', (4, 9)),\n",
       "   ('nave Bayes and two versions of SVM', (10, 17)),\n",
       "   ('SVM', (20, 21)),\n",
       "   ('three deep learning approaches', (22, 26)),\n",
       "   ('DNN', (27, 28)),\n",
       "   ('RNN', (29, 30)),\n",
       "   ('CNN', (32, 33))]],\n",
       " [[('for', (10, 11))],\n",
       "  [('RNN', (6, 7)),\n",
       "   ('outperforms', (7, 8)),\n",
       "   ('others', (9, 10)),\n",
       "   ('all three W OS data sets', (11, 17))]],\n",
       " [[('performs', (1, 2)), ('for', (3, 4))],\n",
       "  [('CNN', (0, 1)), ('secondbest', (2, 3)), ('three data sets', (4, 7))]],\n",
       " [[('is', (4, 5))], [('SVM with term weighting', (0, 4)), ('third', (5, 6))]],\n",
       " [[('does', (4, 5)), ('than', (7, 8))],\n",
       "  [('nave Bayes', (2, 4)),\n",
       "   ('much worse', (5, 7)),\n",
       "   ('other methods', (9, 11))]],\n",
       " [[('within', (8, 9)), ('with', (15, 16))],\n",
       "  [('HDLTex approaches', (13, 15)),\n",
       "   ('stacked , deep learning architectures', (16, 21)),\n",
       "   ('superior performance', (23, 25))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('obtained by', (12, 14)),\n",
       "   ('for', (17, 18)),\n",
       "   ('for', (25, 26))],\n",
       "  [('data set W OS ? 11967', (1, 7)),\n",
       "   ('best accuracy', (9, 11)),\n",
       "   ('combination RNN', (15, 17)),\n",
       "   ('first level of classification', (19, 23)),\n",
       "   ('DNN', (24, 25)),\n",
       "   ('second level', (27, 29))]],\n",
       " [[('gives', (1, 2)), ('of', (3, 4)), ('for', (6, 7))],\n",
       "  [('accuracies', (2, 3)),\n",
       "   ('94 %', (4, 6)),\n",
       "   ('first level', (8, 10)),\n",
       "   ('92 %', (11, 13)),\n",
       "   ('second level', (15, 17)),\n",
       "   ('86 %', (18, 20))]],\n",
       " [[('achieved by', (12, 14)), ('for', (15, 16))],\n",
       "  [('data set W OS ? 46985', (1, 7)),\n",
       "   ('best scores', (8, 10)),\n",
       "   ('RNN', (14, 15)),\n",
       "   ('RNN', (22, 23))]],\n",
       " [[], [('Text Classification', (4, 6))]],\n",
       " [[('introduce', (7, 8)),\n",
       "   ('capable of incorporating', (21, 24)),\n",
       "   ('for', (30, 31))],\n",
       "  [('interaction mechanism', (9, 11)),\n",
       "   ('word - level matching signals', (25, 30)),\n",
       "   ('text classification', (31, 33))]],\n",
       " [[('Based upon', (0, 2)), ('devise', (7, 8)), ('as', (14, 15))],\n",
       "  [('EXplicit interAction Model ( dubbed', (9, 14))]],\n",
       " [[('consists of', (5, 7))],\n",
       "  [('three main components', (7, 10)),\n",
       "   ('word - level encoder', (11, 15)),\n",
       "   ('interaction layer', (16, 18)),\n",
       "   ('aggregation layer', (20, 22))]],\n",
       " [[('projects', (5, 6)), ('into', (9, 10))],\n",
       "  [('word - level encoder', (1, 5)),\n",
       "   ('textual contents', (7, 9)),\n",
       "   ('word - level representations', (11, 15))]],\n",
       " [[('calculates', (5, 6)), ('between', (9, 10)), ('constructs', (17, 18))],\n",
       "  [('interaction layer', (3, 5)),\n",
       "   ('matching scores', (7, 9)),\n",
       "   ('words and classes', (11, 14)),\n",
       "   ('interaction matrix', (19, 21))]],\n",
       " [[('aggregates', (5, 6)), ('into', (9, 10)), ('over', (11, 12))],\n",
       "  [('last layer', (3, 5)),\n",
       "   ('matching scores', (7, 9)),\n",
       "   ('predictions', (10, 11)),\n",
       "   ('each class', (12, 14))]],\n",
       " [[], []],\n",
       " [[], [('Multi - Class Classification', (0, 4))]],\n",
       " [[('For', (0, 1)), ('chose', (7, 8)), ('as', (10, 11))],\n",
       "  [('multi -class task', (2, 5)),\n",
       "   ('region embedding', (8, 10)),\n",
       "   ('Encoder in EXAM', (12, 15))]],\n",
       " [[('is', (3, 4)), ('is', (8, 9))],\n",
       "  [('region size', (1, 3)),\n",
       "   ('7', (4, 5)),\n",
       "   ('embedding size', (6, 8)),\n",
       "   ('128', (9, 10))]],\n",
       " [[('used', (1, 2)),\n",
       "   ('as', (9, 10)),\n",
       "   ('with', (12, 13)),\n",
       "   ('set to', (23, 25))],\n",
       "  [('adam ( Kingma and Ba 2014 )', (2, 9)),\n",
       "   ('optimizer', (11, 12)),\n",
       "   ('initial learning rate 0.0001', (14, 18)),\n",
       "   ('batch size', (20, 22)),\n",
       "   ('16', (25, 26))]],\n",
       " [[('set', (7, 8)), ('of', (10, 11)), ('as', (14, 15))],\n",
       "  [('aggregation MLP', (3, 5)),\n",
       "   ('size', (9, 10)),\n",
       "   ('hidden layer', (12, 14)),\n",
       "   ('2 times interaction feature length', (15, 20))]],\n",
       " [[('implemented and trained by', (3, 7)), ('with', (13, 14))],\n",
       "  [('MXNet ( Chen et', (7, 11)), ('single NVIDIA TITAN Xp', (15, 19))]],\n",
       " [[], []],\n",
       " [[('mainly in', (3, 5))],\n",
       "  [('baselines', (1, 2)), ('three variants', (5, 7))]],\n",
       " [[('based on', (3, 5))], [('feature engineering', (5, 7))]],\n",
       " [[],\n",
       "  [('Char - based deep models', (2, 7)),\n",
       "   ('Word - based deep models', (11, 16))]],\n",
       " [[('based on', (1, 3)),\n",
       "   ('get', (5, 6)),\n",
       "   ('on', (9, 10)),\n",
       "   ('compared to', (14, 16))],\n",
       "  [('Models', (0, 1)),\n",
       "   ('feature engineering', (3, 5)),\n",
       "   ('worst results', (7, 9)),\n",
       "   ('all the five datasets', (10, 14)),\n",
       "   ('other methods', (17, 19))]],\n",
       " [[('get', (4, 5)), ('on', (10, 11))],\n",
       "  [('Char - based models', (0, 4)),\n",
       "   ('highest over all scores', (6, 10)),\n",
       "   ('two Amazon datasets', (12, 15))]],\n",
       " [[('exceed', (4, 5)), ('on', (8, 9)), ('lose on', (12, 14))],\n",
       "  [('Word - based baselines', (0, 4)),\n",
       "   ('other variants', (6, 8)),\n",
       "   ('three datasets', (9, 11)),\n",
       "   ('two Amazon datasets', (15, 18))]],\n",
       " [[('performs', (8, 9))], [('W.C Region Emb', (5, 8)), ('best', (10, 11))]],\n",
       " [[('achieves', (7, 8)), ('over', (11, 12))],\n",
       "  [('EXAM', (6, 7)),\n",
       "   ('best performance', (9, 11)),\n",
       "   ('three datasets', (13, 15)),\n",
       "   ('AG', (16, 17)),\n",
       "   ('Yah. A.', (18, 20)),\n",
       "   ('DBP', (21, 22))]],\n",
       " [[('For', (0, 1)), ('improves', (5, 6)), ('by', (9, 10))],\n",
       "  [('Yah.A.', (2, 3)),\n",
       "   ('EXAM', (4, 5)),\n",
       "   ('best performance', (7, 9)),\n",
       "   ('1.1 %', (10, 12))]],\n",
       " [[], [('Multi - Label Classification', (0, 4))]],\n",
       " [[('implemented', (1, 2)), ('by', (7, 8))],\n",
       "  [('baseline models and EXAM', (3, 7)), ('MXNet', (8, 9))]],\n",
       " [[('used', (1, 2)),\n",
       "   ('trained by', (4, 6)),\n",
       "   ('to initialize', (7, 9)),\n",
       "   ('is', (17, 18))],\n",
       "  [('matrix', (3, 4)),\n",
       "   ('word2vec', (6, 7)),\n",
       "   ('embedding layer', (10, 12)),\n",
       "   ('embedding size', (15, 17)),\n",
       "   ('256', (18, 19))]],\n",
       " [[('adopted', (1, 2)), ('as', (3, 4))],\n",
       "  [('GRU', (2, 3)), ('Encoder', (5, 6)), ('1,024', (12, 13))]],\n",
       " [[], [('accumulated MLP', (1, 3)), ('60 hidden units', (4, 7))]],\n",
       " [[('applied', (1, 2)),\n",
       "   ('to optimize', (3, 5)),\n",
       "   ('on', (6, 7)),\n",
       "   ('with', (11, 12)),\n",
       "   ('is', (22, 23))],\n",
       "  [('Adam', (2, 3)),\n",
       "   ('models', (5, 6)),\n",
       "   ('one NVIDIA TITAN Xp', (7, 11)),\n",
       "   ('batch size of 1000', (13, 17)),\n",
       "   ('initial learning rate', (19, 22)),\n",
       "   ('0.001', (23, 24))]],\n",
       " [[('applied for', (4, 6)), ('to avoid', (9, 11))],\n",
       "  [('validation set', (1, 3)),\n",
       "   ('early - stopping', (6, 9)),\n",
       "   ('overfitting', (11, 12))]],\n",
       " [[('better than', (5, 7)), ('in', (11, 12))],\n",
       "  [('Word - based models', (0, 4)),\n",
       "   ('char - based models', (7, 11)),\n",
       "   ('Kanshan - Cup dataset', (12, 16))]],\n",
       " [[('achieve', (2, 3)), ('over', (12, 13))],\n",
       "  [('Our models', (0, 2)),\n",
       "   ('state - of - the - art performance', (4, 12)),\n",
       "   ('two different datasets', (13, 16))]],\n",
       " [[], [('Multilingual Document Classification', (3, 6))]],\n",
       " [[], [('Cross - lingual document classification', (0, 5))]],\n",
       " [[('extend', (1, 2)),\n",
       "   ('use', (5, 6)),\n",
       "   ('in', (8, 9)),\n",
       "   ('to define', (14, 16)),\n",
       "   ('for', (23, 24)),\n",
       "   ('namely', (29, 30))],\n",
       "  [('previous works', (2, 4)),\n",
       "   ('data', (7, 8)),\n",
       "   ('Reuters Corpus Volume 2', (10, 14)),\n",
       "   ('new cross - lingual document classification tasks', (16, 23)),\n",
       "   ('eight very different languages', (24, 28)),\n",
       "   ('English', (30, 31)),\n",
       "   ('French', (32, 33)),\n",
       "   ('Spanish', (34, 35)),\n",
       "   ('Italian', (36, 37)),\n",
       "   ('German', (38, 39)),\n",
       "   ('Russian', (40, 41)),\n",
       "   ('Chinese', (42, 43)),\n",
       "   ('Japanese', (44, 45))]],\n",
       " [[('define', (5, 6))],\n",
       "  [('each language', (1, 3)),\n",
       "   ('train , development and test corpus', (7, 13))]],\n",
       " [[], []],\n",
       " [[('based on', (2, 4)), ('perform', (7, 8)), ('on', (10, 11))],\n",
       "  [('classifiers', (1, 2)),\n",
       "   ('MultiCCA embeddings', (5, 7)),\n",
       "   ('very well', (8, 10)),\n",
       "   ('development corpus', (12, 14)),\n",
       "   ('accuracies', (15, 16)),\n",
       "   ('close or exceeding 90 %', (16, 21))]],\n",
       " [[('trained on', (2, 4)),\n",
       "   ('achieves', (6, 7)),\n",
       "   ('when transfered to', (9, 12)),\n",
       "   ('scores', (17, 18)),\n",
       "   ('for', (19, 20))],\n",
       "  [('system', (1, 2)),\n",
       "   ('English', (4, 5)),\n",
       "   ('excellent results', (7, 9)),\n",
       "   ('different languages', (13, 15)),\n",
       "   ('best', (18, 19)),\n",
       "   ('three out of seven languages ( DE , IT and ZH )', (20, 32))]],\n",
       " [[('are', (5, 6)), ('when', (8, 9)), ('on', (12, 13))],\n",
       "  [('transfer accuracies', (3, 5)),\n",
       "   ('quite low', (6, 8)),\n",
       "   ('training', (9, 10)),\n",
       "   ('classifiers', (11, 12)),\n",
       "   ('other languages than English', (13, 17))]],\n",
       " [[], []],\n",
       " [[('leads to', (6, 8)), ('than', (11, 12))],\n",
       "  [('Training on German or French', (0, 5)),\n",
       "   ('better transfer performance', (8, 11)),\n",
       "   ('training', (12, 13))]],\n",
       " [[], [('Crosslingual transfer between very different languages', (0, 6))]],\n",
       " [[], [('Joint multilingual document classification', (0, 4))]],\n",
       " [[('leads to', (1, 3)), ('for', (5, 6)), ('in comparison to', (9, 12))],\n",
       "  [('important improvement', (3, 5)),\n",
       "   ('all languages', (6, 8)),\n",
       "   ('zero - shot or targeted transfer learning', (12, 19))]],\n",
       " [[], [('Text Categorization', (5, 7))]],\n",
       " [[('incorporate', (5, 6)),\n",
       "   ('into', (7, 8)),\n",
       "   ('propose', (10, 11)),\n",
       "   ('named', (14, 15))],\n",
       "  [('positioninvariance', (6, 7)),\n",
       "   ('RNN', (8, 9)),\n",
       "   ('Disconnected Recurrent Neural Network ( DRNN )', (15, 22))]],\n",
       " [[('To maintain', (0, 2)), ('utilize', (8, 9)), ('to extract', (11, 13))],\n",
       "  [('position - invariance', (3, 6)),\n",
       "   ('max pooling', (9, 11)),\n",
       "   ('important information', (14, 16))]],\n",
       " [[('regarded as', (6, 8)), ('where', (12, 13)), ('replaced with', (16, 18))],\n",
       "  [('special 1D CNN', (9, 12)),\n",
       "   ('convolution kernels', (13, 15)),\n",
       "   ('recurrent units', (18, 20))]],\n",
       " [[('utilize', (1, 2)), ('as', (15, 16))],\n",
       "  [('300D Glo Ve 840B vectors', (3, 8)),\n",
       "   ('pre-trained word embeddings', (17, 20))]],\n",
       " [[('use', (1, 2)), ('to optimize', (8, 10))],\n",
       "  [('Adadelta ( Zeiler , 2012 )', (2, 8)),\n",
       "   ('all the trainable parameters', (10, 14))]],\n",
       " [[('of', (2, 3)),\n",
       "   ('set as', (5, 7)),\n",
       "   ('suggest', (11, 12)),\n",
       "   ('is', (20, 21))],\n",
       "  [('hyperparameter', (1, 2)),\n",
       "   ('Adadelta', (3, 4)),\n",
       "   ('Zeiler ( 2012 )', (7, 11)),\n",
       "   ('1 e ? 6', (14, 18)),\n",
       "   ('0.95', (21, 22))]],\n",
       " [[('To avoid', (0, 2)), ('apply', (8, 9))],\n",
       "  [('gradient explosion problem', (3, 6)),\n",
       "   ('gradient norm clipping', (9, 12))]],\n",
       " [[('set to', (4, 6)),\n",
       "   ('of', (11, 12)),\n",
       "   ('shows', (16, 17)),\n",
       "   ('in', (27, 28))],\n",
       "  [('batch size', (1, 3)),\n",
       "   ('128', (6, 7)),\n",
       "   ('all the dimensions', (8, 11)),\n",
       "   ('input vectors and hidden', (12, 16)),\n",
       "   ('our proposed model', (18, 21)),\n",
       "   ('significantly outperforms', (21, 23)),\n",
       "   ('all the other models', (23, 27)),\n",
       "   ('7 datasets', (28, 30))]],\n",
       " [[('see', (2, 3)), ('performs', (10, 11)), ('in', (12, 13))],\n",
       "  [('very deep CNN ( VDCNN )', (4, 10)),\n",
       "   ('well', (11, 12)),\n",
       "   ('large datasets', (13, 15))]],\n",
       " [[('shows', (0, 1)), ('achieves', (4, 5)), ('compared with', (12, 14))],\n",
       "  [('our model', (2, 4)),\n",
       "   ('10 - 50 % relative error reduction', (5, 12)),\n",
       "   ('char - CRNN', (14, 17))]],\n",
       " [[('shows', (0, 1)), ('performs', (3, 4)), ('than', (6, 7))],\n",
       "  [('DRNN', (2, 3)), ('far better', (4, 6)), ('CNN', (7, 8))]],\n",
       " [[('achieves', (3, 4)), ('than', (7, 8))],\n",
       "  [('Our model DRNN', (0, 3)),\n",
       "   ('much better performance', (4, 7)),\n",
       "   ('GRU and LSTM', (8, 11))]],\n",
       " [[], [('Text Classification', (7, 9))]],\n",
       " [[],\n",
       "  [('https : //github.com/andyweizhao/capsule_text_ classification',\n",
       "    (7, 11))]],\n",
       " [[('called', (3, 4))], [('capsule network', (4, 6))]],\n",
       " [[('introduce', (1, 2)),\n",
       "   ('to decide', (6, 8)),\n",
       "   ('between', (11, 12)),\n",
       "   ('from', (13, 14))],\n",
       "  [('iterative routing process', (3, 6)),\n",
       "   ('credit attribution', (9, 11)),\n",
       "   ('nodes', (12, 13)),\n",
       "   ('lower and higher layers', (14, 18))]],\n",
       " [[('proposed to', (3, 5)),\n",
       "   ('to alleviate', (10, 12)),\n",
       "   ('of', (14, 15)),\n",
       "   ('which may contain', (18, 21)),\n",
       "   ('such as', (25, 27)),\n",
       "   ('unrelated to', (33, 35))],\n",
       "  [('Three strategies', (0, 2)),\n",
       "   ('stabilize', (5, 6)),\n",
       "   ('dynamic routing process', (7, 10)),\n",
       "   ('disturbance', (13, 14)),\n",
       "   ('some noise capsules', (15, 18)),\n",
       "   ('\" background \" information', (21, 25)),\n",
       "   ('stop', (27, 28)),\n",
       "   ('words', (28, 29)),\n",
       "   ('words', (31, 32)),\n",
       "   ('specific categories', (35, 37))]],\n",
       " [[('use', (5, 6)), ('to initialize', (11, 13))],\n",
       "  [('300 - dimensional word2vec vectors', (6, 11)),\n",
       "   ('embedding vectors', (13, 15))]],\n",
       " [[('conduct', (1, 2)), ('with', (3, 4)), ('for', (6, 7))],\n",
       "  [('mini-batch', (2, 3)),\n",
       "   ('size 50', (4, 6)),\n",
       "   (\"AG 's news\", (7, 10)),\n",
       "   ('size 25', (11, 13)),\n",
       "   ('other datasets', (14, 16))]],\n",
       " [[('use', (1, 2)), ('with', (5, 6)), ('to train', (11, 13))],\n",
       "  [('Adam optimization algorithm', (2, 5)),\n",
       "   ('1e - 3 learning rate', (6, 11)),\n",
       "   ('model', (14, 15))]],\n",
       " [[('evaluate', (5, 6)), ('including', (15, 16))],\n",
       "  [('several strong baseline methods', (11, 15)),\n",
       "   ('LSTM / Bi - LSTM', (17, 22)),\n",
       "   ('tree - structured LSTM ( Tree - LSTM )', (23, 32)),\n",
       "   ('LSTM regularized by linguistic knowledge ( LR - LSTM )', (33, 43)),\n",
       "   ('CNNrand / CNN - static / CNN - non-static', (44, 53)),\n",
       "   ('very deep convolutional network ( VD - CNN )', (59, 68)),\n",
       "   ('character - level convolutional network ( CL - CNN )', (70, 80))]],\n",
       " [[], []],\n",
       " [[('observe', (5, 6)),\n",
       "   ('achieve', (10, 11)),\n",
       "   ('on', (13, 14)),\n",
       "   ('verifies', (21, 22))],\n",
       "  [('capsule networks', (8, 10)),\n",
       "   ('best results', (11, 13)),\n",
       "   ('4 out of 6 benchmarks', (14, 19)),\n",
       "   ('effectiveness', (23, 24))]],\n",
       " [[], [('Multi - Label Text Classification', (4, 9))]],\n",
       " [[('investigate', (5, 6)),\n",
       "   ('of', (8, 9)),\n",
       "   ('on', (11, 12)),\n",
       "   ('by using', (15, 17)),\n",
       "   ('as', (23, 24))],\n",
       "  [('capability', (7, 8)),\n",
       "   ('capsule network', (9, 11)),\n",
       "   ('multi-label text classification', (12, 15)),\n",
       "   ('only the single - label samples', (17, 23)),\n",
       "   ('training data', (24, 26))]],\n",
       " [[('observe that', (6, 8)),\n",
       "   ('have', (11, 12)),\n",
       "   ('in terms of', (16, 19)),\n",
       "   ('over', (23, 24)),\n",
       "   ('on', (28, 29)),\n",
       "   ('in both', (32, 34))],\n",
       "  [('capsule networks', (9, 11)),\n",
       "   ('substantial and significant improvement', (12, 16)),\n",
       "   ('all four evaluation metrics', (19, 23)),\n",
       "   ('strong baseline methods', (25, 28)),\n",
       "   ('test sets', (30, 32)),\n",
       "   ('Reuters - Multi-label and Reuters - Full datasets', (34, 42))]],\n",
       " [[('achieved on', (6, 8)), ('in', (20, 21))],\n",
       "  [('larger improvement', (3, 5)),\n",
       "   ('Reuters - Multi - label dataset', (8, 14)),\n",
       "   ('multi-label documents', (18, 20)),\n",
       "   ('test set', (22, 24))]],\n",
       " [[('than', (8, 9))],\n",
       "  [('capsule network', (1, 3)),\n",
       "   ('much stronger transferring capability', (4, 8)),\n",
       "   ('conventional deep neural networks', (10, 14))]],\n",
       " [[('on', (6, 7)),\n",
       "   ('indicate', (11, 12)),\n",
       "   ('over', (19, 20)),\n",
       "   ('on', (21, 22))],\n",
       "  [('Reuters - Full', (7, 10)),\n",
       "   ('capsule network', (14, 16)),\n",
       "   ('robust superiority', (17, 19)),\n",
       "   ('competitors', (20, 21)),\n",
       "   ('single - label documents', (22, 26))]],\n",
       " [[], [('Connection Strength Visualization', (0, 3))]],\n",
       " [[('observe', (5, 6)),\n",
       "   ('correctly recognize and cluster', (10, 14)),\n",
       "   ('with respect to', (17, 20))],\n",
       "  [('capsule networks', (7, 9)),\n",
       "   ('important phrases', (15, 17)),\n",
       "   ('text categories', (21, 23))]],\n",
       " [[], [('Deep Joint Entity Disambiguation', (0, 4))]],\n",
       " [[], [('Entity disambiguation ( ED )', (0, 5))]],\n",
       " [[], [('ED', (0, 1)), ('disambiguation', (12, 13))]],\n",
       " [[('to learn', (13, 15))], [('basic features', (15, 17))]],\n",
       " [[('implemented in', (3, 5))], [('Torch framework', (6, 8))]],\n",
       " [[('For', (0, 1)), ('use', (6, 7)), ('for', (13, 14))],\n",
       "  [('entity embeddings only', (1, 4)),\n",
       "   ('Wikipedia ( Feb 2014 ) corpus', (7, 13)),\n",
       "   ('training', (14, 15))]],\n",
       " [[('initialized', (3, 4)), ('from', (5, 6)), ('with', (11, 12))],\n",
       "  [('Entity vectors', (0, 2)), ('0 mean normal distribution', (7, 11))]],\n",
       " [[('train', (2, 3)), ('on', (6, 7)), ('for', (19, 20))],\n",
       "  [('each entity vector', (3, 6)),\n",
       "   (\"entity 's Wikipedia canonical description page\", (8, 14)),\n",
       "   ('400 iterations', (20, 22))]],\n",
       " [[('use', (1, 2)), ('with', (3, 4)), ('of', (7, 8))],\n",
       "  [('Adagrad', (2, 3)), ('learning rate', (5, 7)), ('0.3', (8, 9))]],\n",
       " [[('choose', (1, 2))],\n",
       "  [('embedding size d', (2, 5)),\n",
       "   ('300', (6, 7)),\n",
       "   ('pre-trained ( fixed ) Word2 Vec word vectors', (8, 16))]],\n",
       " [[('takes', (3, 4)), ('on', (6, 7)), ('with', (11, 12))],\n",
       "  [('20 hours', (4, 6)),\n",
       "   ('single TitanX GPU', (8, 11)),\n",
       "   ('12 GB of memory', (12, 16))]],\n",
       " [[('trained on', (7, 9)),\n",
       "   ('validated on', (17, 19)),\n",
       "   ('tested on', (23, 25))],\n",
       "  [('AIDA - train ( multiple epochs )', (9, 16)), ('AIDA - A', (19, 22))]],\n",
       " [[('use', (1, 2)),\n",
       "   ('with', (3, 4)),\n",
       "   ('of', (6, 7)),\n",
       "   ('until', (10, 11)),\n",
       "   ('exceeds', (13, 14)),\n",
       "   ('setting it to', (18, 21))],\n",
       "  [('Adam', (2, 3)),\n",
       "   ('learning rate', (4, 6)),\n",
       "   ('1e - 4', (7, 10)),\n",
       "   ('validation accuracy', (11, 13)),\n",
       "   ('90 %', (14, 16)),\n",
       "   ('1e - 5', (21, 24))]],\n",
       " [[('use', (4, 5)), ('after', (19, 20))],\n",
       "  [('early stopping', (5, 7)),\n",
       "   ('validation accuracy', (14, 16)),\n",
       "   ('does not increase', (16, 19)),\n",
       "   ('500 epochs', (20, 22))]],\n",
       " [[('Training on', (0, 2)),\n",
       "   ('takes', (5, 6)),\n",
       "   ('on average', (7, 9)),\n",
       "   ('per', (12, 13)),\n",
       "   ('for', (18, 19)),\n",
       "   ('over', (21, 22))],\n",
       "  [('single GPU', (3, 5)),\n",
       "   ('2 ms', (10, 12)),\n",
       "   ('mention', (13, 14)),\n",
       "   ('16 hours', (16, 18)),\n",
       "   ('1250 epochs', (19, 21)),\n",
       "   ('AIDA - train', (22, 25))]],\n",
       " [[('obtain', (1, 2)), ('on', (7, 8))],\n",
       "  [('state of the art accuracy', (2, 7)),\n",
       "   ('AIDA', (8, 9)),\n",
       "   ('manually created ED dataset', (27, 31))]],\n",
       " [[('analyzed', (6, 7)),\n",
       "   ('on', (9, 10)),\n",
       "   ('for', (15, 16)),\n",
       "   ('where', (17, 18)),\n",
       "   ('have', (20, 21))],\n",
       "  [('accuracy', (8, 9)),\n",
       "   ('AIDA - B dataset', (11, 15)),\n",
       "   ('situations', (16, 17)),\n",
       "   ('gold entities', (18, 20)),\n",
       "   ('low frequency or mention prior', (21, 26))]],\n",
       " [[('shows', (0, 1)), ('performs', (4, 5))],\n",
       "  [('our method', (2, 4)), ('well', (5, 6))]],\n",
       " [[], []],\n",
       " [[], [('Named Entity Disambiguation', (10, 13))]],\n",
       " [[('propose', (5, 6)), ('for', (15, 16))],\n",
       "  [('named entity disambiguation ( NED )', (16, 22))]],\n",
       " [[('addressing', (5, 6)), ('using', (7, 8)), ('based on', (12, 14))],\n",
       "  [('NED', (6, 7)),\n",
       "   ('simple NED model', (9, 12)),\n",
       "   ('trained contextualized embeddings', (15, 18))]],\n",
       " [[('for', (15, 16))], [('words', (12, 13)), ('NED', (16, 17))]],\n",
       " [[('based on', (6, 8))], [('bidirectional transformer encoder', (9, 12))]],\n",
       " [[('takes', (1, 2)),\n",
       "   ('in', (8, 9)),\n",
       "   ('produces', (14, 15)),\n",
       "   ('for', (18, 19))],\n",
       "  [('sequence of words and entities', (3, 8)),\n",
       "   ('input text', (10, 12)),\n",
       "   ('contextualized embedding', (16, 18)),\n",
       "   ('each word and entity', (19, 23))]],\n",
       " [[('propose', (5, 6)), ('based on', (25, 27)), ('in', (31, 32))],\n",
       "  [('masked entity prediction', (6, 9)),\n",
       "   ('randomly masked entities', (22, 25)),\n",
       "   ('words and non-masked entities', (27, 31)),\n",
       "   ('input text', (33, 35))]],\n",
       " [[('trained', (1, 2)), ('using', (4, 5)), ('retrieved from', (10, 12))],\n",
       "  [('model', (3, 4)), ('texts', (5, 6)), ('Wikipedia', (12, 13))]],\n",
       " [[], [('outperformed', (5, 6))]],\n",
       " [[('using', (2, 3)), ('boosted', (6, 7)), ('by', (9, 10))],\n",
       "  [('pseudo entity annotations', (3, 6)),\n",
       "   ('accuracy', (8, 9)),\n",
       "   ('0.3 %', (10, 12))]],\n",
       " [[('achieved', (2, 3)),\n",
       "   ('on', (12, 13)),\n",
       "   ('namely', (19, 20)),\n",
       "   ('performed', (32, 33)),\n",
       "   ('on', (34, 35))],\n",
       "  [('new state - of -', (3, 8)),\n",
       "   ('four', (13, 14)),\n",
       "   ('MSNBC', (20, 21)),\n",
       "   ('AQUAINT', (22, 23)),\n",
       "   ('ACE2004', (24, 25)),\n",
       "   ('WNED - WIKI', (27, 30)),\n",
       "   ('competitive', (33, 34)),\n",
       "   ('WNED - CLUEWEB dataset', (36, 40))]],\n",
       " [[('using', (2, 3)), ('improved', (6, 7)), ('on', (9, 10))],\n",
       "  [('pseudo entity annotations', (3, 6)),\n",
       "   ('performance', (8, 9)),\n",
       "   ('AQUAINT and ACE2004 datasets', (11, 15))]],\n",
       " [[], [('Deep contextualized word representations', (0, 4))]],\n",
       " [[], [('deep contextualized word representation', (6, 10))]],\n",
       " [[], [('Pre-trained word representations', (0, 3))]],\n",
       " [[('introduce', (5, 6)), ('in', (36, 37)), ('across', (40, 41))],\n",
       "  [('deep contextualized word representation', (10, 14)),\n",
       "   ('existing models', (25, 27)),\n",
       "   ('significantly improves', (29, 31)),\n",
       "   ('state of the art', (32, 36))]],\n",
       " [[('differ from', (2, 4)),\n",
       "   ('in', (8, 9)),\n",
       "   ('assigned', (13, 14)),\n",
       "   ('of', (20, 21))],\n",
       "  [('traditional word type embeddings', (4, 8)),\n",
       "   ('each token', (10, 12)),\n",
       "   ('representation', (15, 16)),\n",
       "   ('function', (19, 20))]],\n",
       " [[('use', (1, 2)),\n",
       "   ('derived from', (3, 5)),\n",
       "   ('trained with', (10, 12)),\n",
       "   ('on', (22, 23))],\n",
       "  [('vectors', (2, 3)),\n",
       "   ('bidirectional LSTM', (6, 8)),\n",
       "   ('coupled lan - guage model ( LM ) objective', (13, 22)),\n",
       "   ('large text corpus', (24, 27))]],\n",
       " [[('call', (5, 6))],\n",
       "  [('ELMo ( Embeddings from Language Models ) representations', (7, 15))]],\n",
       " [[('are', (11, 12)), ('function of', (21, 23)), ('of', (28, 29))],\n",
       "  [('ELMo representations', (9, 11)),\n",
       "   ('deep', (12, 13)),\n",
       "   ('all of the internal layers', (23, 28)),\n",
       "   ('biLM', (30, 31))]],\n",
       " [[('learn', (4, 5)),\n",
       "   ('stacked above', (11, 13)),\n",
       "   ('for', (16, 17)),\n",
       "   ('markedly', (22, 23)),\n",
       "   ('over', (25, 26))],\n",
       "  [('linear combination of the vectors', (6, 11)),\n",
       "   ('each input word', (13, 16)),\n",
       "   ('each end task', (17, 20)),\n",
       "   ('performance', (24, 25)),\n",
       "   ('using the top LSTM layer', (27, 32))]],\n",
       " [[('Using', (0, 1)),\n",
       "   ('show', (5, 6)),\n",
       "   ('capture', (13, 14)),\n",
       "   ('of', (18, 19)),\n",
       "   ('to perform', (30, 32)),\n",
       "   ('on', (33, 34)),\n",
       "   ('of', (45, 46))],\n",
       "  [('intrinsic evaluations', (1, 3)),\n",
       "   ('higher - level LSTM states', (8, 13)),\n",
       "   ('context - dependent aspects', (14, 18)),\n",
       "   ('word meaning', (19, 21)),\n",
       "   ('well', (32, 33)),\n",
       "   ('supervised word sense disambiguation tasks', (34, 39)),\n",
       "   ('lowerlevel states model aspects', (41, 45))]],\n",
       " [[], [('Question answering', (0, 2))]],\n",
       " [[('adding', (1, 2)),\n",
       "   ('to', (3, 4)),\n",
       "   ('improved by', (12, 14)),\n",
       "   ('from', (16, 17)),\n",
       "   ('to', (19, 20)),\n",
       "   ('improving', (34, 35)),\n",
       "   ('by', (46, 47))],\n",
       "  [('ELMo', (2, 3)),\n",
       "   ('baseline model', (5, 7)),\n",
       "   ('test set F 1', (8, 12)),\n",
       "   ('4.7 %', (14, 16)),\n",
       "   ('81.1 %', (17, 19)),\n",
       "   ('85.8 %', (20, 22)),\n",
       "   ('24.9 % relative error reduction', (24, 29)),\n",
       "   ('overall single model state - of - the - art', (36, 46)),\n",
       "   ('1.4 %', (47, 49))]],\n",
       " [[('with', (5, 6)),\n",
       "   ('is', (7, 8)),\n",
       "   ('then', (11, 12)),\n",
       "   ('from adding', (16, 18)),\n",
       "   ('to', (19, 20))],\n",
       "  [('increase of', (1, 3)),\n",
       "   ('4.7 %', (3, 5)),\n",
       "   ('ELMo', (6, 7)),\n",
       "   ('significantly larger', (9, 11)),\n",
       "   ('1.8 % improvement', (13, 16)),\n",
       "   ('CoVe', (18, 19)),\n",
       "   ('baseline model', (21, 23))]],\n",
       " [[], [('Textual entailment', (0, 2))]],\n",
       " [[('adding', (2, 3)),\n",
       "   ('to', (4, 5)),\n",
       "   ('improves', (8, 9)),\n",
       "   ('by', (10, 11)),\n",
       "   ('across', (16, 17))],\n",
       "  [('ELMo', (3, 4)),\n",
       "   ('ESIM model', (6, 8)),\n",
       "   ('accuracy', (9, 10)),\n",
       "   ('average of 0.7 %', (12, 16)),\n",
       "   ('five random seeds', (17, 20))]],\n",
       " [[], [('Semantic role labeling', (0, 3))]],\n",
       " [[('experiments', (2, 3)),\n",
       "   ('with', (3, 4)),\n",
       "   ('from', (8, 9)),\n",
       "   ('adding', (15, 16)),\n",
       "   ('improved', (17, 18)),\n",
       "   ('by', (22, 23)),\n",
       "   ('from', (25, 26)),\n",
       "   ('to', (27, 28)),\n",
       "   ('establishing', (30, 31)),\n",
       "   ('improving over', (39, 41)),\n",
       "   ('by', (46, 47))],\n",
       "  [('OntoNotes coreference annotations', (5, 8)),\n",
       "   ('CoNLL 2012 shared task', (10, 14)),\n",
       "   ('ELMo', (16, 17)),\n",
       "   ('average F 1', (19, 22)),\n",
       "   ('3.2 %', (23, 25)),\n",
       "   ('67.2', (26, 27)),\n",
       "   ('70.4', (28, 29)),\n",
       "   ('new state of the art', (32, 37)),\n",
       "   ('previous best ensemble result', (42, 46))]],\n",
       " [[], [('Named entity extraction', (0, 3))]],\n",
       " [[('achieves', (10, 11)), ('averaged over', (16, 18))],\n",
       "  [('our', (4, 5)),\n",
       "   ('ELMo enhanced biLSTM - CRF', (5, 10)),\n",
       "   ('92. 22 % F 1', (11, 16)),\n",
       "   ('five runs', (18, 20))]],\n",
       " [[], [('Neural Word Sense Disambiguation', (10, 14))]],\n",
       " [[('propose', (1, 2)), ('of', (10, 11)), ('without', (22, 23))],\n",
       "  [('two different methods', (2, 5)),\n",
       "   ('greatly reduce', (6, 8)),\n",
       "   ('size', (9, 10)),\n",
       "   ('neural WSD models', (11, 14)),\n",
       "   ('improving', (19, 20)),\n",
       "   ('coverage', (21, 22)),\n",
       "   ('additional training data', (23, 26))]],\n",
       " [[('present', (7, 8)),\n",
       "   ('relies on', (12, 14)),\n",
       "   ('to achieve', (20, 22)),\n",
       "   ('that', (23, 24)),\n",
       "   ('on', (31, 32))],\n",
       "  [('WSD system', (9, 11)),\n",
       "   ('pre-trained BERT word vectors', (14, 18)),\n",
       "   ('results', (22, 23)),\n",
       "   ('significantly outperforms', (24, 26)),\n",
       "   ('state of the art', (27, 31)),\n",
       "   ('all WSD evaluation tasks', (32, 36))]],\n",
       " [[], [('Word Sense Disambiguation ( WSD )', (0, 6))]],\n",
       " [[('taking advantage of', (12, 15)),\n",
       "   ('between', (18, 19)),\n",
       "   ('included in', (20, 22)),\n",
       "   ('such as', (24, 26))],\n",
       "  [('semantic relationships', (16, 18)),\n",
       "   ('senses', (19, 20)),\n",
       "   ('WordNet', (22, 23)),\n",
       "   ('hypernymy', (27, 28)),\n",
       "   ('hyponymy', (30, 31)),\n",
       "   ('meronymy', (33, 34)),\n",
       "   ('antonymy', (36, 37))]],\n",
       " [[('based on', (3, 5)), ('share', (27, 28)), ('using', (42, 43))],\n",
       "  [('observation', (6, 7)),\n",
       "   ('a sense and its closest related senses', (8, 15)),\n",
       "   ('common idea or concept', (29, 33)),\n",
       "   ('word', (37, 38)),\n",
       "   ('disambiguated', (41, 42))]],\n",
       " [[('For', (0, 1)),\n",
       "   ('used', (4, 5)),\n",
       "   ('of', (13, 14)),\n",
       "   ('consists of', (20, 22)),\n",
       "   ('of', (23, 24)),\n",
       "   ('trained on', (27, 29))],\n",
       "  [('BERT', (1, 2)),\n",
       "   ('model named', (6, 8)),\n",
       "   ('\" bert - largecased \"', (8, 13)),\n",
       "   ('PyTorch implementation', (15, 17)),\n",
       "   ('vectors', (22, 23)),\n",
       "   ('dimension 1024', (24, 26)),\n",
       "   ('Book s Corpus', (29, 32)),\n",
       "   ('English Wikipedia', (33, 35))]],\n",
       " [[('For', (0, 1)), ('used', (7, 8)), ('as', (11, 12)), ('with', (23, 24))],\n",
       "  [('Transformer encoder layers', (2, 5)),\n",
       "   ('same parameters', (9, 11)),\n",
       "   ('\" base \" model', (13, 17)),\n",
       "   ('6 layers', (21, 23)),\n",
       "   ('8 attention heads', (24, 27)),\n",
       "   ('hidden size', (29, 31)),\n",
       "   ('2048', (32, 33)),\n",
       "   ('dropout', (36, 37)),\n",
       "   ('0.1', (38, 39))]],\n",
       " [[('use', (12, 13)),\n",
       "   ('through', (17, 18)),\n",
       "   ('obtain', (23, 24)),\n",
       "   ('that are', (25, 27)),\n",
       "   ('to', (29, 30))],\n",
       "  [('our systems', (9, 11)),\n",
       "   ('sense vocabulary compression', (14, 17)),\n",
       "   ('hypernyms or', (18, 20)),\n",
       "   ('through all relations', (20, 23)),\n",
       "   ('scores', (24, 25)),\n",
       "   ('overall equivalent', (27, 29)),\n",
       "   ('systems that do not use it', (31, 37))]],\n",
       " [[('thanks to', (7, 9)),\n",
       "   ('added to', (14, 16)),\n",
       "   ('use of', (21, 23)),\n",
       "   ('as', (24, 25)),\n",
       "   ('on', (36, 37))],\n",
       "  [('Princeton WordNet Gloss Corpus', (10, 14)),\n",
       "   ('training data', (17, 19)),\n",
       "   ('BERT', (23, 24)),\n",
       "   ('input embeddings', (25, 27)),\n",
       "   ('outperform', (29, 30)),\n",
       "   ('systematically', (30, 31)),\n",
       "   ('state of the art', (32, 36)),\n",
       "   ('every task', (37, 39))]],\n",
       " [[('the use of', (16, 19)),\n",
       "   ('as', (20, 21)),\n",
       "   ('have', (24, 25)),\n",
       "   ('on', (28, 29)),\n",
       "   ('lead to', (32, 34))],\n",
       "  [('additional training corpus ( WNGC )', (7, 13)),\n",
       "   ('BERT', (19, 20)),\n",
       "   ('input embeddings', (21, 23)),\n",
       "   ('major impact', (26, 28)),\n",
       "   ('our results', (29, 31)),\n",
       "   ('scores above the state of the art', (34, 41))]],\n",
       " [[('Using', (0, 1)),\n",
       "   ('instead of', (2, 4)),\n",
       "   ('improves', (8, 9)),\n",
       "   ('by', (12, 13)),\n",
       "   ('adding', (23, 24)),\n",
       "   ('to', (26, 27)),\n",
       "   ('improves', (30, 31)),\n",
       "   ('by', (32, 33))],\n",
       "  [('BERT', (1, 2)),\n",
       "   ('ELMo or Glo Ve', (4, 8)),\n",
       "   ('score', (11, 12)),\n",
       "   ('approximately 3 and 5 points', (13, 18)),\n",
       "   ('WNGC', (25, 26)),\n",
       "   ('training data', (28, 30)),\n",
       "   ('approximately 2 points', (33, 36))]],\n",
       " [[('using', (2, 3)), ('adds', (4, 5)), ('to', (9, 10))],\n",
       "  [('ensembles', (3, 4)),\n",
       "   ('roughly another 1 point', (5, 9)),\n",
       "   ('final F1 score', (11, 14))]],\n",
       " [[('through', (5, 6)), ('seems to', (8, 10)), ('in', (14, 15))],\n",
       "  [('compression method', (3, 5)),\n",
       "   ('all relations', (6, 8)),\n",
       "   ('negatively impact', (10, 12)),\n",
       "   ('results', (13, 14)),\n",
       "   ('some cases', (15, 17)),\n",
       "   ('ELMo or GloVe', (20, 23))]],\n",
       " [[], [('Neural Word Sense Disambiguation', (3, 7))]],\n",
       " [[], [('Word Sense Disambiguation ( WSD )', (0, 6))]],\n",
       " [[], [('WSD', (13, 14))]],\n",
       " [[('propose', (5, 6)), ('variant of', (21, 23))],\n",
       "  [('novel model GAS', (7, 10)),\n",
       "   ('gloss - augmented WSD neural network', (12, 18)),\n",
       "   ('memory network', (24, 26))]],\n",
       " [[('jointly encodes', (1, 3)),\n",
       "   ('of', (7, 8)),\n",
       "   ('models', (12, 13)),\n",
       "   ('between', (16, 17)),\n",
       "   ('in', (21, 22))],\n",
       "  [('GAS', (0, 1)),\n",
       "   ('context and glosses', (4, 7)),\n",
       "   ('target word', (9, 11)),\n",
       "   ('semantic relationship', (14, 16)),\n",
       "   ('context and glosses', (18, 21)),\n",
       "   ('memory module', (23, 25))]],\n",
       " [[('to measure', (2, 4)),\n",
       "   ('between', (7, 8)),\n",
       "   ('employ', (15, 16)),\n",
       "   ('within', (19, 20)),\n",
       "   ('as', (22, 23)),\n",
       "   ('adopt', (27, 28))],\n",
       "  [('inner relationship', (5, 7)),\n",
       "   ('glosses and context', (8, 11)),\n",
       "   ('more accurately', (11, 13)),\n",
       "   ('multiple passes operation', (16, 19)),\n",
       "   ('memory', (21, 22)),\n",
       "   ('re-reading process', (24, 26)),\n",
       "   ('two memory updating mechanisms', (28, 32))]],\n",
       " [[], [('Neural Word Sense Disambiguation', (3, 7))]],\n",
       " [[('use', (1, 2)),\n",
       "   ('with', (5, 6)),\n",
       "   ('keep', (11, 12)),\n",
       "   ('during', (14, 15))],\n",
       "  [('pre-trained word embeddings', (2, 5)),\n",
       "   ('300 dimensions', (6, 8)),\n",
       "   ('fixed', (13, 14)),\n",
       "   ('training process', (16, 18))]],\n",
       " [[('employ', (1, 2)), ('in both', (5, 7))],\n",
       "  [('256 hidden units', (2, 5)),\n",
       "   ('gloss module', (8, 10)),\n",
       "   ('context module', (12, 14))]],\n",
       " [[('used for', (3, 5)),\n",
       "   ('in', (6, 7)),\n",
       "   ('with', (12, 13)),\n",
       "   ('used for', (21, 23))],\n",
       "  [('Orthogonal initialization', (0, 2)),\n",
       "   ('weights', (5, 6)),\n",
       "   ('LSTM', (7, 8)),\n",
       "   ('random uniform initialization', (9, 12)),\n",
       "   ('range [ - 0.1 , 0.1 ]', (13, 20)),\n",
       "   ('others', (23, 24))]],\n",
       " [[('assign', (1, 2)), ('of', (8, 9))],\n",
       "  [('gloss expansion depth K', (2, 6)), ('value', (7, 8)), ('4', (9, 10))]],\n",
       " [[('experiment with', (2, 4)),\n",
       "   ('from', (12, 13)),\n",
       "   ('in', (16, 17)),\n",
       "   ('finding', (20, 21)),\n",
       "   ('performs', (27, 28))],\n",
       "  [('number of passes | T M |', (5, 12)),\n",
       "   ('our framework', (17, 19)),\n",
       "   ('best', (28, 29))]],\n",
       " [[('use', (1, 2)), ('in', (4, 5)), ('with', (8, 9))],\n",
       "  [('Adam optimizer', (2, 4)),\n",
       "   ('training process', (6, 8)),\n",
       "   ('0.001 initial learning rate', (9, 13))]],\n",
       " [[('to avoid', (2, 4)), ('use', (7, 8)), ('set', (11, 12)), ('to', (14, 15))],\n",
       "  [('overfitting', (4, 5)),\n",
       "   ('dropout regularization', (8, 10)),\n",
       "   ('drop rate', (12, 14)),\n",
       "   ('0.5', (15, 16))]],\n",
       " [[('runs for', (1, 3)),\n",
       "   ('with', (7, 8)),\n",
       "   ('if', (10, 11)),\n",
       "   ('within', (17, 18))],\n",
       "  [('Training', (0, 1)),\n",
       "   ('up to 100 epochs', (3, 7)),\n",
       "   ('early stopping', (8, 10)),\n",
       "   ('validation loss', (12, 14)),\n",
       "   (\"does n't improve\", (14, 17)),\n",
       "   ('last 10 epochs', (19, 22))]],\n",
       " [[], [('Knowledge - based Systems', (0, 4))]],\n",
       " [[('exploits', (2, 3)),\n",
       "   ('from', (7, 8)),\n",
       "   ('builds', (10, 11)),\n",
       "   ('for', (17, 18))],\n",
       "  [('Babelfy', (0, 1)),\n",
       "   ('semantic network structure', (4, 7)),\n",
       "   ('BabelNet', (8, 9)),\n",
       "   ('unified graph - based architecture', (12, 17)),\n",
       "   ('WSD and Entity Linking', (18, 22))]],\n",
       " [[], [('Supervised Systems', (0, 2))]],\n",
       " [[('selects', (8, 9)),\n",
       "   ('as', (17, 18)),\n",
       "   ('makes use of', (21, 24)),\n",
       "   ('surrounding', (28, 29)),\n",
       "   ('within', (32, 33)),\n",
       "   ('such as', (37, 39))],\n",
       "  [('linear Support Vector Machine ( SVM )', (10, 17)),\n",
       "   ('classifier', (19, 20)),\n",
       "   ('set of features', (25, 28)),\n",
       "   ('target word', (30, 32)),\n",
       "   ('limited window', (34, 36)),\n",
       "   ('POS tags', (39, 41)),\n",
       "   ('local words', (42, 44)),\n",
       "   ('local collocations', (45, 47))]],\n",
       " [[('selects', (3, 4)),\n",
       "   ('as', (5, 6)),\n",
       "   ('makes use of', (10, 13)),\n",
       "   ('as', (15, 16))],\n",
       "  [('IMS +emb', (0, 2)),\n",
       "   ('IMS', (4, 5)),\n",
       "   ('underlying framework', (7, 9)),\n",
       "   ('word embeddings', (13, 15)),\n",
       "   ('features', (16, 17))]],\n",
       " [[], [('Neural - based Systems', (0, 4))]],\n",
       " [[('leverages', (3, 4)), ('which shares', (8, 10)), ('among', (12, 13))],\n",
       "  [('Bi- LSTM', (0, 2)),\n",
       "   ('model parameters', (10, 12)),\n",
       "   ('all words', (13, 15))]],\n",
       " [[], [('Bi-LSTM +att.+ LEX', (0, 3)), ('WSD', (29, 30))]],\n",
       " [[], [('English all - words results', (0, 5))]],\n",
       " [[('achieves', (4, 5)), ('on', (12, 13))],\n",
       "  [('GAS and GAS ext', (0, 4)),\n",
       "   ('state - of - theart performance', (6, 12)),\n",
       "   ('concatenation of all test datasets', (14, 19))]],\n",
       " [[('find that', (18, 20)),\n",
       "   ('with', (22, 23)),\n",
       "   ('achieves', (27, 28)),\n",
       "   ('on', (32, 33))],\n",
       "  [('GAS ext', (20, 22)),\n",
       "   ('concatenation memory updating strategy', (23, 27)),\n",
       "   ('best results 70.6', (29, 32)),\n",
       "   ('concatenation of the four test datasets', (34, 40))]],\n",
       " [[('shows', (1, 2)),\n",
       "   ('can', (7, 8)),\n",
       "   ('as well as avoid', (11, 15)),\n",
       "   ('of', (18, 19))],\n",
       "  [('appropriate number of passes', (3, 7)),\n",
       "   ('boost', (8, 9)),\n",
       "   ('performance', (10, 11)),\n",
       "   ('over - fitting', (15, 18)),\n",
       "   ('model', (20, 21))]],\n",
       " [[], [('Multiple Passes Analysis', (0, 3))]],\n",
       " [[('shows', (1, 2)), ('performs', (6, 7)), ('than', (8, 9))],\n",
       "  [('multiple passes operation', (3, 6)),\n",
       "   ('better', (7, 8)),\n",
       "   ('one pass', (9, 11))]],\n",
       " [[('with', (4, 5))],\n",
       "  [('increasing number of passes', (6, 10)),\n",
       "   ('F1 - score', (12, 15)),\n",
       "   ('increases', (15, 16))]],\n",
       " [[('larger than', (8, 10)), ('due to', (20, 22))],\n",
       "  [('number of passes', (4, 7)),\n",
       "   ('3', (10, 11)),\n",
       "   ('F1- score', (13, 15)),\n",
       "   ('stops increasing', (15, 17)),\n",
       "   ('over-fitting', (22, 23))]],\n",
       " [[], [('Word Sense Disambiguation', (0, 3))]],\n",
       " [[], [('word sense disambiguation ( WSD )', (25, 31))]],\n",
       " [[], [('Improved WSD', (0, 2))]],\n",
       " [[('modeling', (10, 11)),\n",
       "   ('surrounding', (15, 16)),\n",
       "   ('represent', (36, 37)),\n",
       "   ('using', (39, 40))],\n",
       "  [('sequence of words', (12, 15)),\n",
       "   ('target word', (17, 19)),\n",
       "   ('words', (38, 39)),\n",
       "   ('real valued vector representation', (40, 44))]],\n",
       " [[('implemented using', (4, 6)), ('released as', (10, 12))],\n",
       "  [('source code', (1, 3)),\n",
       "   ('TensorFlow', (6, 7)),\n",
       "   ('open source 1', (12, 15))]],\n",
       " [[('initialized using', (3, 5)), ('of', (7, 8)), ('trained on', (14, 16))],\n",
       "  [('embeddings', (1, 2)),\n",
       "   ('set', (6, 7)),\n",
       "   ('freely available 2 Glo Ve vectors', (8, 14)),\n",
       "   ('Wikipedia and Gigaword', (16, 19))]],\n",
       " [[('initialized from', (7, 9))],\n",
       "  [('Words', (0, 1)), ('N ( 0 , 0.1 )', (9, 15))]],\n",
       " [[('achieves', (3, 4)), ('on', (7, 8)), ('tied with', (11, 13))],\n",
       "  [('Our proposed model', (0, 3)),\n",
       "   ('top score', (5, 7)),\n",
       "   ('SE2', (8, 9)),\n",
       "   ('IMS + adapted CW', (13, 17)),\n",
       "   ('SE3', (18, 19))]],\n",
       " [[('see', (3, 4)), ('consistently', (6, 7)), ('on', (10, 11))],\n",
       "  [('dropword', (5, 6)),\n",
       "   ('results', (9, 10)),\n",
       "   ('both', (11, 12)),\n",
       "   ('SE2 and SE3', (12, 15))]],\n",
       " [[('Randomizing', (0, 1)),\n",
       "   ('of', (3, 4)),\n",
       "   ('yields', (7, 8)),\n",
       "   ('are', (25, 26))],\n",
       "  [('input words', (5, 7)),\n",
       "   ('substantially worse result', (9, 12)),\n",
       "   ('order of the words', (21, 25)),\n",
       "   ('significant', (26, 27))]],\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all=[]\n",
    "for k in range(len(pred_label)):\n",
    "    all.append(get_entity_spans(k, pred_label[k]))\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "final-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('have', (14, 15)),\n",
       "   ('on', (17, 18)),\n",
       "   ('looks', (36, 37)),\n",
       "   ('in terms of', (38, 41))],\n",
       "  [('Larger layer size', (3, 6)),\n",
       "   ('little impact', (15, 17)),\n",
       "   ('performance', (19, 20)),\n",
       "   ('our', (21, 22)),\n",
       "   ('adequate', (37, 38)),\n",
       "   ('speed / performance trade - off', (41, 47))]],\n",
       " [[('using', (8, 9)),\n",
       "   ('as', (11, 12)),\n",
       "   ('instead of', (15, 17)),\n",
       "   ('potential', (22, 23))],\n",
       "  [('subword split', (9, 11)),\n",
       "   ('input token unit', (12, 15)),\n",
       "   ('standard tokenized word unit', (17, 21)),\n",
       "   ('performance', (26, 27))]],\n",
       " [[('using', (2, 3)),\n",
       "   ('as', (5, 6)),\n",
       "   ('is', (7, 8)),\n",
       "   ('for leveraging', (11, 13)),\n",
       "   ('into', (15, 16))],\n",
       "  [('subword information', (3, 5)),\n",
       "   ('features', (6, 7)),\n",
       "   ('promising approach', (9, 11)),\n",
       "   ('subword information', (13, 15)),\n",
       "   ('constituency parsing', (16, 18))]],\n",
       " [[('successfully achieved', (3, 5)), ('as', (8, 9))],\n",
       "  [('Our Seq2seq approach', (0, 3)),\n",
       "   ('competitive level', (6, 8)),\n",
       "   ('current', (10, 11)),\n",
       "   ('RNNG', (16, 17))]],\n",
       " [[], [('Syntactic constituency parsing', (0, 3))]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all[35:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "maritime-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=pd.read_csv('pos_sent.csv')\n",
    "#dt=dt['labels','text']\n",
    "a=pd.DataFrame(all)\n",
    "a.columns=['predicates', 'subj/obj']\n",
    "a=pd.concat([dt[['labels','text']],a],axis=1)\n",
    "a['triple_A']='[]'\n",
    "a['triple_B']='[]'\n",
    "a['triple_C']='[]'\n",
    "a['triple_D']='[]'\n",
    "a=a.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "conventional-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('triples.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-bottom",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
